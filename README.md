# Daily AI Papers

These summaries are automatically generated from [HuggingFace's Daily Papers](https://huggingface.co/papers), using Gemini and GitHub actions.  All credits go to the research community for sharing and the HuggingFace community for curating these papers.

Please note:
- Authors may be listed by their HuggingFace user id. This will be rectified soon. 
- These summaries are entirely generated by the LLM. You can refer to the basic prompt [here](templates/prompt_template.md).

Last updated: 2024-09-26 
 


## Papers for 2024-09-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale](https://arxiv.org/abs/2409.17115) | Qian Liu, Pengfei, lockon, SinclairWang, koalazf99 | This paper introduces Programming Every Example (PROX), a novel framework for pre-training language models that leverages small language models to refine training corpora by generating and executing fine-grained data refining operations. PROX outperforms both original and other data selection methods by more than 2% across various benchmarks. The method also significantly saves training FLOPs, offering a promising path for efficient LLM pre-training. It can be applied across various pre-training corpora, model sizes, and specific domains. PROX consistently outperforms existing methods, achieving average accuracy improvements of over 2% across ten downstream benchmarks and significantly improving the performance of language models trained on OpenWebMath in 20x fewer tokens. | ['Natural Language Processing'] | [Link](https://github.com/GAIR-NLP/ProX) | [Link](https://huggingface.co/gair-prox) |
| [Boosting Healthcare LLMs Through Retrieved Context](https://arxiv.org/abs/2409.15127) | Ashwin Kumar Gururajan, dariog, JordiBayarri | The paper proposes a new context retrieval method called OpenMedPrompt for improving the accuracy of large language models (LLMs) in medical question answering. The authors show that their method achieves performance comparable to closed-source models such as MedPalm-2 and GPT-4.  OpenMedPrompt consists of two strategies: Ensemble Refining (OM-ER) and Self-Reflection (OM-SR), which both leverage the diversity of multiple generated answers to produce a refined and more accurate final response. The authors demonstrate that OM-SR generally outperforms OM-ER across most configurations. This suggests that the iterative feedback loop and incorporation of reward model scores provide a more effective mechanism for refining the generated answers. | ['Question Answering'] | N/A | N/A |


## Papers for 2024-09-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|


