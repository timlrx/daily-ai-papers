# Daily AI Papers

These summaries are automatically generated from [HuggingFace's Daily Papers](https://huggingface.co/papers), using Gemini and GitHub actions based on the following categories of interest: Multimodal, Text Generation, Text Classification, Text2Text Generation, Summarization, Question Answering, Natural Language Processing, Audio, Text-to-Speech, Audio-to-Audio. All credits go to the research community for sharing and the HuggingFace community for curating these papers.

Please note:
- Authors may be listed by their HuggingFace user id. This will be rectified soon. 
- These summaries are entirely generated by the LLM. You can refer to the basic prompt [here](templates/prompt_template.md).

Last updated: 2024-10-07 
 


## Papers for 2024-10-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Addition is All You Need for Energy-efficient Language Models](https://arxiv.org/abs/2410.00907) | Wei Sun, luohy | - The paper proposes a novel linear-complexity multiplication (L-Mul) algorithm to approximate floating-point multiplication with integer addition, aiming to reduce energy consumption in large language models (LLMs). - L-Mul replaces expensive floating-point multiplications with less energy-intensive integer additions and introduces an offset to maintain accuracy. - The authors claim L-Mul achieves higher precision and requires less computation compared to 8-bit floating-point multiplications and 80% energy reduction for dot products. - Experiments on various LLMs and tasks (MMLU, BBH, GSM8k, visual question answering) showed that L-Mul in attention layers maintained or even slightly improved performance compared to standard multiplication and outperformed float8 with training free setting. - Fine-tuning models with all multiplications replaced by 3-bit L-Mul achieved comparable results to models using float8_e4m3 accumulation, showcasing its potential for efficient LLM training and deployment. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [NL-Eye: Abductive NLI for Images](https://arxiv.org/abs/2410.02613) | Zorik Gekhman, yonatanbitton, nitay, tokeron, MorVentura |  - NL-EYE, a benchmark designed to evaluate the visual abductive reasoning skills of Visual Language Models (VLMs), is introduced. - NL-EYE tasks models with evaluating the plausibility of hypothesis images given a premise image, requiring explanations for their choices and consisting of 350 image triplets across six reasoning categories: physical, functional, logical, emotional, cultural, and social. - Results show that while humans perform well, VLMs struggle, often failing to surpass random baselines in plausibility prediction. - Even with correct predictions, VLM explanations are frequently unhelpful, indicating weaknesses in visual interpretation and accurate representation generation for reasoning. - Further analysis suggests that VLMs face challenges with temporal reasoning, absolute judgments, and non-correlational tasks, particularly emotional reasoning. | ['Multimodal', 'Computer Vision'] | N/A | N/A |
| [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703) | Yossi Matias, Matan Kalman, yanivle | -"Selective Attention" is introduced; a parameter-free adjustment to the standard attention mechanism in Transformers, enabling a token to deem another as no longer relevant for future tokens and masking it, improving language modelling performance across various model sizes and context lengths. -It allows for reduction in the attention context buffer size without quality loss, resulting in significant memory and compute savings during inference, achieving up to 16X, 25X, and 47X memory reduction for context sizes of 512, 1024, and 2048 respectively with a 100M parameter model trained on C4. -Selective attention transformers often outperform standard transformers with ~2X more parameters and heads in their attention module. -Visualizations show selective attention exhibiting dynamic context pruning behavior; masking previous assignments to the same variable in variable assignment, masking ambiguous inputs until ambiguity resolution, and retaining only necessary elements in tasks like Parity and Copy. -Evaluation on C4 dataset shows consistent perplexity improvements across different model sizes and context lengths; further improvements via explicit loss to encourage masking, and HellaSwag benchmark reveals consistent accuracy gains across various model sizes using selective attention. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise](https://arxiv.org/abs/2410.03017) | Susanna Loeb, ddemszky, carlycodes, Analu, rose-e-wang |  - This paper introduces Tutor CoPilot, a Human-AI system designed to enhance real-time tutoring in K-12 education by providing expert-like guidance to tutors as they interact with students.  - Tutor CoPilot leverages the Bridge method, which captures expert decision-making patterns and adapts Large Language Models (LLMs) to generate contextually relevant suggestions for tutors during live sessions.  - A randomized controlled trial involving 900 tutors and 1,800 K-12 students demonstrates that Tutor CoPilot significantly improves student learning outcomes, particularly for students with lower-rated tutors.  - Analysis of over 550,000 chat messages reveals that tutors using Tutor CoPilot are more likely to employ high-quality pedagogical strategies that foster student understanding and less likely to simply provide answers.  - Tutor CoPilot offers a scalable and cost-effective solution ( $20 per tutor annually) for enhancing tutoring quality, especially in under-served communities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760) | David Bau, Samuel Marks, sfeucht, RohitGandikota | - This research introduces Erasure of Language Memory (ELM), a novel method for removing specific concepts from large language models (LLMs) while preserving fluency and general knowledge. - ELM employs a multi-objective fine-tuning approach with targeted low-rank updates (LoRA). - The method optimizes for erasure of the target concept, retention of unrelated information, and generation fluency when prompted with the erased concept. - Experiments on biosecurity, cybersecurity, and literary domains demonstrate ELM’s efficacy in achieving near-random performance on erased topics while maintaining high scores on general knowledge benchmarks and generating more fluent text than baseline methods. - ELM also exhibits robustness against adversarial attacks, further highlighting its potential for safe and controlled LLM editing. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/rohitgandikota/erasing-llm) | [Link](https://huggingface.co/cais/Zephyr_RMU) |
| [CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction](https://arxiv.org/abs/2410.01273) | wpiioos, Unmanned-YuBeen, lastdefiance20, PurpleSand, MilkClouds |  - CANVAS, a novel framework for intuitive human-robot interaction, is introduced for commonsense-aware navigation. It combines visual and linguistic instructions to generate robot actions, leveraging pre-trained vision-language models (VLMs) to achieve this. - A new dataset called COMMAND, containing 48 hours of driving data over 219 kilometers with human-annotated instructions and navigation outcomes across office, street and orchard simulated environments, was collected to train and test the model. - Experimental results show that CANVAS consistently outperforms the rule-based ROS NavStack in all environments, especially in challenging scenarios like uneven terrain or misleading instructions, with higher success and lower collision rates. - CANVAS achieves successful Sim2Real transfer with a 69% success rate in a real-world office setting, demonstrating its robustness beyond simulated data. - Ablation study confirms that using pre-trained VLM weights improves performance considerably, indicating the usefulness of existing knowledge for navigation tasks. | ['Robotics', 'Multimodal'] | N/A | N/A |


## Papers for 2024-10-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](https://arxiv.org/abs/2410.02740) | Chen Chen, Vasileios Saveris, haotiz, Hong-You, jefflai | This paper investigates the role of large-scale image-caption data in pre-training multimodal foundation models, particularly focusing on the interplay between synthetic captions and original AltText. - It proposes a controllable and scalable captioning pipeline capable of generating diverse caption formats (short, descriptive, dense, AltText-fused). - Experiments across CLIP, multimodal LLMs, and diffusion models reveal that a hybrid approach, combining synthetic captions and AltText, often outperforms using synthetic captions alone.  - Different model types exhibit preferences for specific caption formats: shorter captions for CLIP, descriptive for multimodal LLMs and diffusion models. - Combining AltText with synthetic captions enhances performance, likely due to improved image-text alignment from synthetic captions and increased data diversity from AltText. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification'] | N/A | N/A |
| [Video Instruction Tuning With Synthetic Data](https://arxiv.org/abs/2410.02713) | Wei Li, Chunyuan24, liuziwei7, kimingng, ZhangYuanhan |  - This paper introduces LLaVA-Video, a large multimodal model for video understanding, and LLaVA-Video-178K, a synthetic dataset created for video instruction following. - LLaVA-Video-178K consists of 178,510 videos with 1.3 million instruction samples including detailed captions generated with a recurrent, multi-level approach, along with open-ended and multiple-choice question answering generated using GPT-4. - The model leverages a SlowFast video representation technique to optimize the balance between frame count and limited GPU memory, enabling processing of three times more frames than traditional methods. - LLaVA-Video achieves state-of-the-art results on various video benchmarks, outperforming existing open-source models and demonstrating the effectiveness of the proposed synthetic dataset and training approach. - The dataset, codebase, model checkpoints, and a visual chat demo are publicly released to foster development of general-purpose visual assistants. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/datasets/lmms-lab/VideoDetailCaption) |
| [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712) | Chunyuan24, henghuang, thughost, russwang, txiong23 | **-** LLaVA-Critic is the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess the performance of other multimodal models across various tasks.  **-** It leverages a new high-quality critic instruction-following dataset incorporating diverse evaluation criteria and scenarios, including pointwise scoring and pairwise ranking.  **-** The model shows strong performance as an LMM-as-a-Judge, generating evaluation scores and rankings comparable to commercial GPT models.  **-** In preference learning, LLaVA-Critic generates effective reward signals for iterative Direct Preference Optimization (DPO), surpassing rewards from human feedback as seen in LLaVA-RLHF.  **-** LLaVA-Critic is open-sourced, including its data, code, checkpoints, and demo. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [Contrastive Localized Language-Image Pre-Training](https://arxiv.org/abs/2410.02746) | Marcin Eichner, Xinze Wang, haotiz, jefflai, Hong-You | - CLOC is a new pre-training framework for vision encoders with enhanced localization capabilities. - It augments the CLIP loss with a region-text contrastive loss and a lightweight prompter module that extracts region embeddings from the image embedding given spatial hints. - A visually-enriched and spatially-localized captioning pipeline is designed to generate region-text pseudo-labels at scale, resulting in a two-billion image-text dataset with fine-grained region-text annotations. - CLOC consistently outperforms CLIP on 31 evaluation tasks, including standard image-text tasks, newly constructed region-text tasks, and downstream evaluations with MLLMs, particularly on referring and grounding tasks. - The enhanced localization capabilities of CLOC enable it to be a drop-in replacement of CLIP to enhance MLLMs. | ['Multimodal', 'Image Classification', 'Image Feature Extraction', 'Visual Question Answering', 'Zero-Shot Image Classification'] | N/A | [Link](https://huggingface.co/datasets/zzliang/GRIT) |
| [Large Language Models as Markov Chains](https://arxiv.org/abs/2410.02724) | Abdelhakim Benechehab, Oussama Zekri, ievred, NBoulle, ambroiseodt |  - This paper draws an equivalence between large language models (LLMs) and Markov chains, offering a new theoretical framework to analyze LLM inference.  - By representing LLMs with vocabulary size *T* and context window *K* as Markov chains on a state space of size O(*T*<sup>*K*</sup>), the authors derive findings on stationary distribution, convergence speed, and temperature influence.  - The paper derives generalization bounds for pre-training and in-context learning under minimal assumptions, using concentration inequalities for dependent random variables and leveraging insights from the Markov chain equivalence.  - The theoretical analysis predicts in-context scaling laws that are experimentally validated on recent LLMs (2023-2024), showing that LLMs outperform minimax optimal frequentist Markov chain learning.  - Experimental results on various Markov chains and dynamical systems further support the theoretical findings and demonstrate the practical implications of the proposed framework. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling](https://arxiv.org/abs/2409.19291) | Yu Cheng, Jihai Zhang, Spico, Xiaoye08 | -  This paper introduces Diversified Multiplet Upcycling (DMU), a novel method for enhancing the Contrastive Language-Image Pre-training (CLIP) model by integrating it with a Mixture of Experts (MoE) architecture. DMU fine-tunes multiple CLIP models from a pre-trained checkpoint using Multistage Contrastive Learning (MCL) to capture diverse feature distributions. These fine-tuned models, sharing parameters except for the Feed-Forward Network, are then used to initialize a CLIP-MoE. The approach significantly improves CLIP's performance on various zero-shot tasks, including retrieval and image classification, as well as in downstream Multimodal Large Language Model (MLLM) benchmarks when serving as a vision encoder. Notably, CLIP-MoE surpasses the base OpenAI CLIP model by approximately 20% on retrieval tasks and exhibits minimal additional training overhead, using only 2% of the computational resources required to train a CLIP from scratch. This method provides a model-agnostic and computationally efficient way to scale CLIP and enhance its ability to capture rich, fine-grained information for improved performance in various multimodal applications. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/OpenSparseLLMS/CLIP-MOE) | N/A |
| [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367) | Jun Zhu, Pengle Zhang, Jia wei, Jintao Zhang, surfingtomchen | - SageAttention, a novel post-training quantization method designed to accelerate attention in Transformer models by quantizing tensors to 8-bit integers. - It overcomes the challenges of accuracy degradation in existing methods by smoothing the K matrix to mitigate outlier effects and employing a low-precision FP16 accumulator for the PV matrix multiplication. - It integrates effective kernel fusion with ROPE and an online softmax inspired by FlashAttention. - Comprehensive experiments demonstrate a 2.1x speed improvement over FlashAttention2 and 2.7x over xFormers on an RTX 4090. - It maintains comparable end-to-end metrics across diverse applications, including language, image, and video generation models. | ['Text-to-Image', 'Text-to-Video', 'Text2Text Generation', 'Image Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?](https://arxiv.org/abs/2410.02115) | Jianye Hou, Baibei Ji, Juntao Li, Keyan Zhou, ZetangForward | • L-CiteEval, a new multi-task benchmark for evaluating long-context understanding with citations in large language models (LLMs) is introduced. • The benchmark comprises 11 diverse tasks with context lengths ranging from 8K to 48K tokens and employs automatic evaluation metrics for reproducibility. • Evaluation of 11 LLMs reveals that open-source models lag significantly behind closed-source counterparts in citation accuracy, suggesting reliance on inherent knowledge rather than provided context. • Retrieval-Augmented Generation (RAG) improves faithfulness in open-source LLMs but slightly diminishes generation quality. • A strong correlation is observed between LLMs' attention mechanisms and citation generation process, offering insight into LLM evaluation and development. | ['Question Answering', 'Summarization', 'Natural Language Processing'] | [Link](https://github.com/ZetangForward/L-CITEEVAL.git) | N/A |
| [Training Language Models on Synthetic Edit Sequences Improves Code Synthesis](https://arxiv.org/abs/2410.02749) | Rob Fergus, lerrel, upiter | - LintSeq, a synthetic data generation algorithm, refactors existing code into edit sequences to improve code synthesis in large language models (LLMs). - LLMs trained on this data produce more diverse programs, resulting in better inference-time scaling for benchmark pass rate. - Tiny (150M parameter) edit sequence LMs achieve state-of-the-art performance for their model class, matching or outperforming models twice their size. - Repeated sampling from smaller edit sequence finetuned LLMs achieves HumanEval coverage competitive with GPT-4 at similar cumulative inference cost to single samples from large open-source LLMs. - Ablating linter guidance from LintSeq degrades downstream performance. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Feature Extraction'] | [Link](https://github.com/upiterbarg/lintseq) | N/A |
| [Distilling an End-to-End Voice Assistant Without Instruction Training Data](https://arxiv.org/abs/2410.02678) | Michael Ryan, Ella Li, zyanzhe, missblanchett, WillHeld | **Summary of "Distilling an End-to-End Voice Assistant Without Instruction Training Data"**  - This paper introduces DiVA, a new speech large language model (LLM) trained through knowledge distillation from a text-based LLM, eliminating the need for explicit instruction-following data. DiVA utilizes a novel cross-modal context distillation method, which uses a frozen text-based LLM to guide the audio model's training by matching the output distribution from text transcripts of the audio. The audio input is processed using Whisper for feature extraction and a Q-Former initialized from Whisper's decoder to achieve audio-text feature alignment. - DiVA generalizes well to various spoken language tasks such as Spoken Question Answering, Classification (emotion, humor, and sarcasm detection), and Translation, using only ASR data for training. - In evaluation benchmarks, DiVA outperforms other open-access Speech and Audio LLMs on question answering by a significant margin despite using substantially less compute for training. - DiVA excels in following text-based instructions provided through prompts and user's speech, addressing the "forgetting" issue observed in other models trained using supervised fine-tuning.  - In a user study, DiVA received a 72% preference rate compared to Qwen 2 Audio, demonstrating its effectiveness in real-world scenarios despite some limitations like inheriting the base LLM's bias. | ['Multimodal', 'Audio', 'Automatic Speech Recognition', 'Question Answering', 'Translation'] | N/A | N/A |
| [Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos](https://arxiv.org/abs/2410.02763) | Jianrui Zhang, yjlee0222, mucai |  - This paper introduces Vinoground, a novel temporal counterfactual benchmark for evaluating Large Multimodal Models (LMMs) on dense temporal reasoning in short videos. - Vinoground contains 1000 short video and caption pairs with captions containing the same words but in different orders to create temporal counterfactuals. - The benchmark evaluates an LMM’s ability to distinguish temporal differences between actions and object transformations (e.g., "water turning into ice” vs. “ice turning into water”). - Experimental results show that even state-of-the-art LMMs struggle with temporal reasoning, with the best model (GPT-40) achieving only 54% accuracy on text score and much worse on other metrics, while human performance is around 90%. - All open-source models and CLIP-based models perform much worse, suggesting that existing methods struggle at fully understanding video temporality. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://vinoground.github.io) | N/A |
| [Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data](https://arxiv.org/abs/2410.02056) | manocha, ctnzr, rafaelvalle, ZhifengKong, SreyanG-NVIDIA | Synthio is a novel approach to augment small-scale audio classification datasets using synthetic data generated from text-to-audio (T2A) diffusion models, aligning the generated data with the target dataset's acoustic characteristics through preference optimization. - It addresses the challenge of creating diverse synthetic augmentations by introducing MixCap, a technique that leverages Large Language Models (LLMs) to generate and refine meaningful audio captions used for prompting the T2A model. - Synthio's evaluation across ten datasets and four limited-data settings demonstrates consistent outperformance of existing baselines, improving classification accuracy by 0.1% to 39% using a T2A model trained solely on weakly-captioned AudioSet. - Ablation studies show the vital role of preference optimization and MixCap in achieving optimal results. - Additional analysis demonstrates effectiveness of Synthio in enhancing captioning tasks and addressing long-tail categories. | ['Audio', 'Audio Classification', 'Text-to-Audio'] | [Link](https://github.com/Sreyan88/Synthio) | N/A |


## Papers for 2024-10-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging](https://arxiv.org/abs/2410.01215) | Xiaodong Gu, Chengcheng Wan, Songsong Wang, YerbaPage |  - MGDebugger, a hierarchical code debugger, is introduced to improve the pass rate of LLM-generated code by addressing bugs at multiple levels of granularity.   - MGDebugger decomposes code into subfunctions, debugs them iteratively in a bottom-up manner, and uses an LLM-simulated Python executor to track variable states for precise error identification.   - Experiments show that MGDebugger significantly outperforms existing debugging systems, achieving an 18.9% accuracy improvement over seed generations in HumanEval and a 97.6% repair success rate in HumanEval-Fix.  - Ablation studies confirm the effectiveness of hierarchical debugging, and further analysis highlights the robustness of MGDebugger across diverse bug types, code lengths, and debugging attempts.  - MGDebugger leverages pretrained LLMs for debugging, eliminating task-specific retraining for a lightweight and scalable solution. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/YerbaPage/MGDebugger) | N/A |
| [Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis](https://arxiv.org/abs/2409.20059) | nunonmg, PierreColombo, CelineH, emmanuelmalherbe, hgissbkh | This paper conducts an empirical analysis of preference-based alignment techniques for enhancing large language model (LLM)-based translation, focusing on Contrastive Preference Optimization (CPO). - CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data regarding alignment metrics, like xCOMET-QE. - Preference-based alignment is highly sensitive to the choice of candidate translation systems used for generating preference data, affecting both the alignment metric and downstream metric consistency. - Aligning a model using its own translations achieves performance comparable to employing multiple external systems, ensuring better metric consistency.  - The paper also finds that preference-based lexical alignment using the gold reference as the preferred translation performs poorly.  - Optimizing preference data in a mono-system setting, specifically setting the quality of the chosen and rejected translations, allows the model to match the performance of multi-system settings. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://huggingface.co/collections/artefactory/translation-alignment-analysis) |
| [LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/abs/2410.01744) | Zhihan Zhang, Tianqing Fang, Mengzhao Jia, kaixinm, wyu1 | - LEOPARD, a Multimodal Large Language Model (MLLM), specializes in handling text-rich, multi-image tasks, addressing the limitations of existing MLLMs in this area by focusing on high-quality instruction tuning data and image resolution. - A new dataset, LEOPARD-INSTRUCT, comprising 925K samples, including 739K designed for text-rich, multi-image scenarios, is introduced to train the model. The dataset focuses on real-world domains like multi-page documents, multi-charts, and webpage snapshots. - An adaptive, high-resolution, multi-image encoding module dynamically optimizes the visual sequence length based on image dimensions using pixel shuffling for compression, enabling processing of multiple high-resolution images without information loss. - Experiments conducted on 13 benchmarks demonstrate LEOPARD's superior performance in text-rich multi-image benchmarks with a +9.61 point improvement over other open-source MLLMs. - The model remains competitive on single image and general-domain tasks, highlighting the benefits of training on high-quality, tailored multi-image datasets | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | [Link](https://github.com/Jill0001/Leopard) | N/A |
| [Not All LLM Reasoners Are Created Equal](https://arxiv.org/abs/2410.01748) | Aaron Courville, Daniel Toyama, Alessandro Sordoni, agarwl, arianhosseini |  - This paper investigates Large Language Models' (LLMs) reasoning abilities on grade-school math (GSM) problems, specifically focusing on compositional GSM problems, where the answer to the first question is a variable in the second question. - The study reveals a significant reasoning gap in most LLMs, indicated by a performance difference between solving compositional question pairs and solving each question independently. - This gap is more pronounced in smaller, more cost-efficient, and math-specialized models, suggesting potential limitations in reasoning abilities. -  Instruction-tuning, code generation, and finetuning have varying effects across LLMs, while finetuning can lead to overfitting. - Large reasoning gaps stem from distraction from additional context and poor second-hop reasoning, rather than dataset leakage, impacting performance despite high scores on standard GSM benchmarks. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [HelpSteer2-Preference: Complementing Ratings with Preferences](https://arxiv.org/abs/2410.01257) | okuchaiev, gshennvm, trias702, odelalleau, alexwb |   - This paper introduces HelpSteer2-Preference, a novel dataset of preference annotations designed to complement the existing ratings in the HelpSteer2 dataset, enabling a head-to-head comparison of Bradley-Terry and Regression style reward models. - The authors propose a novel approach combining Bradley-Terry and Regression reward modeling, leading to a Llama 3.1 70B Instruct model that achieved a state-of-the-art 94.1 score on RewardBench as of October 1, 2024. - The preference annotations are accompanied by human-written justifications, enhancing data interpretability and providing insights into annotator decision-making. - The research demonstrates that data format (regression vs. preference) is less critical than the model's ability to capture annotation information, with preference magnitude being key for Bradley-Terry models.  - The combined reward model effectively aligns language models to follow instructions using online Reinforcement Learning from Human Feedback (RLHF), particularly with the REINFORCE algorithm. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/nvidia/HelpSteer2), [Link](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) |
| [RATIONALYST: Pre-training Process-Supervision for Improving Reasoning](https://arxiv.org/abs/2410.01044) | Guoxuan Wang, danyaljj, ChuyuLiu, ylu610, Dongwei | - RATIONALYST, a model pre-trained on implicit rationales extracted from unlabeled text and existing reasoning datasets, is introduced for process-supervision of reasoning. - RATIONALYST leverages these implicit rationales during inference to guide the reasoning process of large language models, enhancing both interpretability and performance. - It consistently generalizes across various reasoning tasks, demonstrating an average 3.9% accuracy improvement on 7 representative reasoning benchmarks when fine-tuned from LLaMa-3-8B. - RATIONALYST outperforms both stronger general-purpose verifiers like GPT-4 and similarly sized models trained on matching datasets, showcasing the efficacy of its process supervision approach. - An ablation study shows that rationales from web-scale data enhance performance, while implicit supervision proves more robust than explicit supervision due to tolerance for imperfect rationales. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JHU-CLSP/Rationalyst) | N/A |
| [Quantifying Generalization Complexity for Large Language Models](https://arxiv.org/abs/2410.01769) | maxtiktok, Nrain, zhuokai, Xulianghuang, luohy | This paper introduces SCYLLA, a dynamic evaluation framework designed to measure the generalization ability of Large Language Models (LLMs) and disentangle it from memorization. - SCYLLA evaluates LLMs across 20 tasks and 5 complexity levels, generating in-distribution and out-of-distribution data to assess generalization. - The study reveals a "generalization valley," where the performance gap between in-distribution and out-of-distribution data is non-monotonic with task complexity. - The peak of this valley, the "critical complexity," represents the upper bound of an LLM's generalization and shifts to higher complexity levels with increasing model size. - The benchmark results covering 28 LLMs show that closed-source models generally exhibit stronger generalization abilities and higher critical complexity than their open-sourced counterparts. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zhentingqi/scylla) | N/A |
| [E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding](https://arxiv.org/abs/2409.18111) | Ying Shan, Yang Wu, Zhongang Qi, Zongyang Ma, Ye Liu | -"E.T. Bench", a large-scale benchmark designed for open-ended, event-level video understanding. - The benchmark comprises 7.3K samples across 12 tasks, spanning 8 domains and featuring 7K videos totaling 251.4 hours. -A novel Video-LLM called "E.T. Chat" is introduced, which excels in event-level understanding by treating timestamp prediction as an embedding matching problem. - A dedicated instruction-tuning dataset, "E.T. Instruct 164K", tailored for multi-event, time-sensitive videos is created. - State-of-the-art models on existing video question answering benchmarks struggle with this new benchmark indicating that current methods struggle with fine-grained time-sensitive video understanding. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling](https://arxiv.org/abs/2410.01440) | Jiazhong Yu, Cao Sheng, Fei Li, feifeiobama, ljh0104 |  - This paper introduces equilibrium sequence modeling, a novel method for training large language models (LLMs) to perform long-horizon robotic planning by iteratively refining plans based on environmental feedback through a self-refinement process. - The approach formulates self-refinement as a fixed-point problem, allowing for end-to-end supervised training without needing external verifiers or reward models, simplifying training compared to reinforcement learning methods. - A nested equilibrium sequence modeling procedure enables efficient closed-loop planning, leveraging feedback from the environment (or a world model) and accelerating plan refinement by reusing previously computed equilibrium solutions. - Evaluations on VirtualHome-Env benchmark demonstrate state-of-the-art performance in most metrics, especially when incorporating environmental feedback, and show advantageous scaling of performance with increased inference computation. - Ablation studies highlight the effectiveness of equilibrium sequence modeling, reuse of previous solutions, and dynamic computation allocation in improving plan quality and computational efficiency. | ['Robotics', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/Singularity0104/equilibrium-planner) | N/A |
| [Selective Aggregation for Low-Rank Adaptation in Federated Learning](https://arxiv.org/abs/2410.01463) | Huijie Fan, Liangqiong-QU, yanranw1, stevezs, gpx333 |  - This research paper introduces FedSA-LoRA, a new method for federated learning that selectively aggregates learned A and B matrices from LoRA. - It asserts that A matrices learn general knowledge while B matrices capture client-specific information, leading to only sharing A matrices for aggregation. - Experimental validation across language understanding and generation tasks on benchmarks like GLUE and GSM8K demonstrates FedSA-LoRA outperforms other methods.  - The authors extend this approach to other LoRA variants (rsLoRA and VeRA), creating FedSA-rsLoRA and FedSA-VeRA, and show consistent improvements. - The findings provide insights into LoRA in federated settings and a general framework for using future LoRA adaptations. | ['Natural Language Processing', 'Text Classification', 'Text Generation', 'Question Answering'] | N/A | N/A |


## Papers for 2024-10-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Law of the Weakest Link: Cross Capabilities of Large Language Models](https://arxiv.org/abs/2409.19951) | xwhan, ruihou16, xwwang, astonzhang, MingZhong |  - This research paper explores the intersection of multiple abilities, termed "cross capabilities," in Large Language Models (LLMs), which are essential for real-world tasks but often overlooked in current evaluations that focus on individual capabilities. - It introduces CROSSEVAL, a benchmark with 1,400 human-annotated prompts and 8,400 human ratings, designed to evaluate both individual and cross capabilities, revealing that current LLMs underperform in cross-capability tasks. - The study finds that LLM cross-capability performance adheres to the "Law of the Weakest Link," being significantly limited by the weakest individual capability, regardless of improvements in other areas. - The results highlight that tool use is a major challenge for LLMs and suggest that prioritizing the enhancement of weaker capabilities is more crucial for improving overall performance than focusing on already strong ones. -  The work emphasizes the importance of shifting focus towards cross-capability evaluation and development to improve LLM effectiveness in complex, real-world scenarios rather than just on individual capabilities. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/facebookresearch/llm-cross-capabilities) | N/A |
| [TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices](https://arxiv.org/abs/2410.00531) | Hongfang Yu, Mohsen Guizani, Jiaoshen, LIKirin | TPI-LLM is a tensor parallel inference system designed for serving 70B-scale LLMs efficiently on low-resource edge devices. - It addresses memory limitations by introducing a sliding window memory scheduler that dynamically manages layer weights during inference, overlapping disk I/O with computation and communication. - TPI-LLM prioritizes tensor parallelism over pipeline parallelism for single-user scenarios on edge devices and implements a star-based allreduce algorithm to minimize link latency. - Experimental results show significant reductions in time-to-first-token, token latency, and peak memory footprint compared to benchmarks like Transformers, Accelerate, and Galaxy. - TPI-LLM successfully runs Llama 2-70B with a peak memory footprint of 3.1GB across 8 low-resource devices, enabling larger models to run on edge devices while preserving user privacy. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Lizonghang/TPI-LLM) | N/A |
| [Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect](https://arxiv.org/abs/2409.17912) | imomayiz, amr-mohamed, khoubrane-yousef, habdine, guokan-shang | Atlas-Chat introduces the first Large Language Models (LLMs) for Moroccan Arabic, a low-resource dialectal Arabic (DA) variant also known as Darija. - A new instruction dataset, Darija-SFT-Mixture, was created by combining existing and new manually and synthetically created Darija resources, as well as translated English instructions. - Atlas-Chat-9B and 2B models, fine-tuned on this dataset, outperform existing LLMs, including Arabic-specific and state-of-the-art models like LLaMa, Jais, and AceGPT, achieving a 13% improvement over a 13B model on a new Darija benchmark. - A new evaluation suite, including DarijaMMLU, DarijaHellaSwag, and DarijaBench, was developed for comprehensive LLM assessment in Darija, focusing on discriminative and generative tasks.  - An experimental analysis was conducted on fine-tuning strategies and base model choices, finding that instruction-tuned Gemma 2 models with LoRA performed optimally. | ['Natural Language Processing', 'Translation', 'Summarization'] | N/A | [Link](https://hf.co/MBZUAI-Paris/Atlas-Chat-9B), [Link](https://hf.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture) |
| [ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer](https://arxiv.org/abs/2410.00086) | Jingren, chenweix7, chaojiemao, jingfengzhang, jiangzeyinzi |  - ACE, a unified framework based on a Diffusion Transformer, supports a wide range of visual generation and editing tasks through natural language instructions, including text-guided generation, low-level visual analysis, controllable generation, semantic editing, element editing, repainting, layer editing, and reference generation. - ACE introduces the Long-context Condition Unit (LCU) to incorporate historical information from previous generation rounds, enabling multi-turn and long-context generation. - A meticulous data collection workflow is established to construct a 0.7 billion-scale dataset covering various generation and editing tasks. - Evaluation on benchmarks such as MagicBrush and a user study on a manually curated benchmark demonstrates ACE’s superior performance in various visual generation tasks. - ACE can be easily integrated into a multimodal chat system to streamline image creation and editing, avoiding cumbersome pipelines typically employed in visual agents. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | [Link](https://huggingface.co/runwayml/stable-diffusion-v1-5), [Link](https://huggingface.co/runwayml/stable-diffusion-inpainting) |
| [Visual Context Window Extension: A New Perspective for Long Video Understanding](https://arxiv.org/abs/2409.20018) | Zhenzhong Chen, hcwei | This research paper proposes a novel approach to enhance long video understanding by extending the visual context window of Large Multimodal Models (LMMs). - It redefines the context window in LMMs as two distinct windows: visual and language, addressing the discrepancies between these modalities. - The study introduces a method to extend positional embeddings within the visual context window, enabling LMMs to handle lengthy videos without retraining on large video-text datasets. - A progressive pooling strategy is implemented to reduce memory consumption by selectively adjusting the spatial resolution of frame embeddings. - Experimental results on benchmarks like MLVU, VideoMME, and LongVideoBench demonstrate consistent performance improvements with increasing video frames, outperforming models like GPT-40 and achieving memory savings of approximately 45%. | ['Multimodal', 'Video-Text-to-Text'] | N/A | N/A |


## Papers for 2024-10-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566) | nm-w, pdufter, zhegan27, fly6464, haotiz |  - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. - MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training. - It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks. - MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI). -  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [DiaSynth -- Synthetic Dialogue Generation Framework](https://arxiv.org/abs/2409.19020) | Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl |  - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning. - It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues. - Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks. - The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data. - DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [Hyper-Connections](https://arxiv.org/abs/2409.19606) | banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder | This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse. - Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters. - Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections. - Hyper-connections are also effective in vision tasks. - Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections. - The authors anticipate this method's broad applicability across various AI problems. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Image-to-Text', 'Unconditional Image Generation', 'Text2Text Generation'] | N/A | N/A |
| [Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models](https://arxiv.org/abs/2409.18943) | yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li | - RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs). - Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation. - RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs. - Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments.  - RULER maintains overall performance on various other benchmarks without affecting non-length based generation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Geaming2002/Ruler) | N/A |
| [Cottention: Linear Transformers With Cosine Attention](https://arxiv.org/abs/2409.18747) | Eric C. Larson, TrevorDohm, gmongaras | This study introduces "Cottention," a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/gmongaras/Cottention_Transformer) | N/A |
| [Can Models Learn Skill Composition from Examples?](https://arxiv.org/abs/2409.19808) | Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu | This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4. - Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills. - Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill. - The study shows that incorporating skill-rich synthetic text improves compositional capabilities. - Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency. - Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code](https://arxiv.org/abs/2409.19715) | Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae | COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym) |
| [IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding](https://arxiv.org/abs/2409.19627) | Jianzong Wang, Jing Xiao, zhangxulong, Pechola | - IDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization. - It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating. - A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability. - IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods. - Experimental results show its ability to withstand various attacks while maintaining good imperceptibility. | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/PecholaL/IDEAW) | N/A |


## Papers for 2024-09-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692) | Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang |  - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video. - It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning. - Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines. - It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning. - MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs. | ['Multimodal', 'Any-to-Any', 'Text-to-Image', 'Image-to-Text', 'Text-to-Speech', 'Automatic Speech Recognition', 'Video-Text-to-Text'] | N/A | N/A |
| [VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2409.17066) | Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92 |  - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization. - VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization. - The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization. - They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks. - VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/VPTQ) | N/A |
| [Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult](https://arxiv.org/abs/2409.17545) | fetong | This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs). - MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning. - Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models. - On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B. - MIPO simplifies hyperparameter tuning by using only a single parameter, β, exhibiting robustness across different model architectures and datasets within a specific range. - MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback) |
| [MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making](https://arxiv.org/abs/2409.16686) | Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu | - The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales. - MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights. - Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment. - The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios. - MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents. | ['Robotics', 'Question Answering'] | N/A | N/A |


## Papers for 2024-09-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference. - Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end. - Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models. - MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks. - The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | - LLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings. - This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks. - LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets. - The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks. - Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling. | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-to-Text', 'Image-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | - EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities. - It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control. - The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data. - EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models. - A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Text-to-Audio', 'Audio-to-Audio', 'Visual Question Answering', 'Image-to-Text'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming |  - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs.  - It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference.  - This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV.  - Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilter’s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O.  - Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | [Link](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose.  - Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data).  - Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models.  - Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain.  - A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/john-hewitt/implicit-ins) | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era. - It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations. - The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs. - It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs. -  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security. | ['Natural Language Processing', 'Summarization'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations. - Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets. - It can achieve even further reduction of vector count by 66% with less than 3% performance degradation. - This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models. - The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors. - The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/colbert-ir/colbertv2.0) |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar |  - This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG). - It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness. - Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time. - The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs. - While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.  - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.  -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.  -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.  -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Text-to-Speech', 'Automatic Speech Recognition', 'Any-to-Any'] | N/A | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Image-to-3D'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence. | ['Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt | This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that "response tuning," which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that "single-task finetuning,"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang | • This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field. • The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA. • The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses. • The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach. • The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs). | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This research introduces MaskLLM, a novel learnable pruning method that generates semi-structured (N:M) sparsity in Large Language Models (LLMs) for enhanced inference efficiency. - MaskLLM distinguishes itself from previous methods by directly learning the distribution of N:M sparsity patterns using Gumbel Softmax sampling, enabling end-to-end training on large datasets and addressing limitations of hand-crafted importance criteria. -  Empirical evaluations on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, demonstrate that MaskLLM surpasses state-of-the-art techniques, achieving a perplexity of 6.72 on LLaMA2-7B compared to SparseGPT's 10.42. - The research underscores the efficacy of learning sparsity patterns directly from data, leading to more accurate and efficient compression of LLMs without compromising performance. - The adaptability of MaskLLM to downstream tasks and its ability to achieve lossless compression in certain scenarios highlight its potential for practical applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research proposes LLaVA-3D, a novel framework for building 3D-aware Large Multimodal Models (LMMs) by adapting the existing 2D LLaVA model.  LLaVA-3D introduces the concept of "3D Patches," which inject 3D positional embeddings into 2D image features, enhancing the model's spatial understanding without complex 3D processing pipelines.  Evaluations demonstrate LLaVA-3D's state-of-the-art performance on various 3D tasks, including question answering, dense captioning, and visual grounding, surpassing existing 3D LMMs while maintaining comparable 2D image understanding capabilities to its 2D counterpart. The research highlights the advantages of leveraging pre-trained 2D LMMs for 3D scene understanding and the benefits of integrating 3D spatial information into 2D visual features. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image-to-Text', 'Text-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal large language model capable of perceiving and generating images, text, and speech with emotional expressiveness. EMOVA utilizes a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for end-to-end speech processing.  The model achieves state-of-the-art performance on both vision-language and speech benchmarks, outperforming models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks and surpassing the speech LLM Mini-Omni in ASR tasks. EMOVA also enables emotional spoken dialogue by explicitly predicting speech style labels (emotions and pitches) and leveraging a lightweight style module for controllable speech synthesis. This is achieved through a novel text-centric multimodal alignment approach, which leverages publicly available bimodal data and eliminates the reliance on scarce trimodal data. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach to reduce the computational cost and latency of processing long context inputs with Large Language Models (LLMs). GemFilter leverages the ability of early LLM layers to identify relevant tokens and compresses the input sequence by a factor of 1000x for subsequent processing by the full model. Empirical evaluations show that GemFilter achieves a 2.4x speedup and 30% reduction in GPU memory consumption compared to state-of-the-art methods, while maintaining comparable performance on benchmarks like LongBench and outperforming them on the Needle in a Haystack task. GemFilter is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting the selected input sequence. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This research paper investigates implicit instruction tuning, demonstrating that instruction following can emerge without explicit instruction-response training.  - The authors show that training solely on responses (response tuning) and on narrow-domain data (single-task finetuning) leads to broad instruction-following abilities in language models.  - For instance, response-tuned models achieve a 43% win rate against explicitly instruction-tuned models in head-to-head evaluations.  - Furthermore, they introduce a simple rule-based language model that, when combined with a pretrained model, exhibits instruction-following behavior.  - These findings highlight that adaptation methods not explicitly designed for instruction following can implicitly induce such capabilities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This research paper introduces a novel technique named "TOKEN POOLING" for enhancing the efficiency of multi-vector retrieval models, especially focusing on ColBERT, without significantly affecting performance.  The method uses clustering techniques to group together similar token representations and then applies mean pooling to create a single, representative vector, effectively reducing the overall storage footprint. Experiments show that this approach reduces the required vector count by 50% without compromising accuracy, and a 66% reduction still yields strong performance. The paper also demonstrates that TOKEN POOLING can be effectively combined with existing quantization methods, leading to even more significant compression rates while maintaining reasonable retrieval performance.  | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper presents a comprehensive review of the emerging field of Conversation Analysis (CA), a process designed to extract critical information from conversational data and leverage it for system optimization and decision-making.  - The paper systematically defines CA as a four-step procedure: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, and discusses the challenges and trends within each step. - The paper further argues that while previous CA efforts focused on atomic tasks with limited business impact, the rise of Large Language Models (LLMs) enables deeper, more insightful analysis and strategic decision-making from conversations.  - The authors compile and categorize existing benchmark datasets for CA but highlight a significant gap in comprehensive benchmarks containing fine-grained conversation elements and long-context modeling capabilities.  - The paper concludes by outlining future research directions, including the development of LLM-based conversation simulators, fine-grained CA benchmarks, long-context conversation modeling, in-depth attribution analysis, and advanced goal-directed optimization and evaluation methods. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging Knowledge Graphs (KGs) and graph-based architectures. The authors demonstrate the effectiveness of their framework by applying it to the SoccerNet dataset, a large dataset of soccer videos. Their findings show that Structured-GraphRAG significantly improves query processing efficiency, reduces response times, and enhances accuracy compared to traditional RAG methods. The structured nature of KGs reduces hallucinations in LLMs, making the responses more consistent and reliable. The authors highlight that their framework can be applied to a broad range of applications due to its flexible design. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


