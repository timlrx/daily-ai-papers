# Daily AI Papers

These summaries are automatically generated from [HuggingFace's Daily Papers](https://huggingface.co/papers), using Gemini and GitHub actions based on the following categories of interest: Multimodal, Text Generation, Text Classification, Text2Text Generation, Summarization, Question Answering, Natural Language Processing, Audio, Text-to-Speech, Audio-to-Audio. All credits go to the research community for sharing and the HuggingFace community for curating these papers.

Please note:
- Authors may be listed by their HuggingFace user id. This will be rectified soon. 
- These summaries are entirely generated by the LLM. You can refer to the basic prompt [here](templates/prompt_template.md).

Last updated: 2024-10-02 
 


## Papers for 2024-10-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Law of the Weakest Link: Cross Capabilities of Large Language Models](https://arxiv.org/abs/2409.19951) | xwhan, ruihou16, xwwang, astonzhang, MingZhong |  - This research paper explores the intersection of multiple abilities, termed "cross capabilities," in Large Language Models (LLMs), which are essential for real-world tasks but often overlooked in current evaluations that focus on individual capabilities. - It introduces CROSSEVAL, a benchmark with 1,400 human-annotated prompts and 8,400 human ratings, designed to evaluate both individual and cross capabilities, revealing that current LLMs underperform in cross-capability tasks. - The study finds that LLM cross-capability performance adheres to the "Law of the Weakest Link," being significantly limited by the weakest individual capability, regardless of improvements in other areas. - The results highlight that tool use is a major challenge for LLMs and suggest that prioritizing the enhancement of weaker capabilities is more crucial for improving overall performance than focusing on already strong ones. -  The work emphasizes the importance of shifting focus towards cross-capability evaluation and development to improve LLM effectiveness in complex, real-world scenarios rather than just on individual capabilities. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/facebookresearch/llm-cross-capabilities) | N/A |
| [TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices](https://arxiv.org/abs/2410.00531) | Hongfang Yu, Mohsen Guizani, Jiaoshen, LIKirin | TPI-LLM is a tensor parallel inference system designed for serving 70B-scale LLMs efficiently on low-resource edge devices. - It addresses memory limitations by introducing a sliding window memory scheduler that dynamically manages layer weights during inference, overlapping disk I/O with computation and communication. - TPI-LLM prioritizes tensor parallelism over pipeline parallelism for single-user scenarios on edge devices and implements a star-based allreduce algorithm to minimize link latency. - Experimental results show significant reductions in time-to-first-token, token latency, and peak memory footprint compared to benchmarks like Transformers, Accelerate, and Galaxy. - TPI-LLM successfully runs Llama 2-70B with a peak memory footprint of 3.1GB across 8 low-resource devices, enabling larger models to run on edge devices while preserving user privacy. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Lizonghang/TPI-LLM) | N/A |
| [Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect](https://arxiv.org/abs/2409.17912) | imomayiz, amr-mohamed, khoubrane-yousef, habdine, guokan-shang | Atlas-Chat introduces the first Large Language Models (LLMs) for Moroccan Arabic, a low-resource dialectal Arabic (DA) variant also known as Darija. - A new instruction dataset, Darija-SFT-Mixture, was created by combining existing and new manually and synthetically created Darija resources, as well as translated English instructions. - Atlas-Chat-9B and 2B models, fine-tuned on this dataset, outperform existing LLMs, including Arabic-specific and state-of-the-art models like LLaMa, Jais, and AceGPT, achieving a 13% improvement over a 13B model on a new Darija benchmark. - A new evaluation suite, including DarijaMMLU, DarijaHellaSwag, and DarijaBench, was developed for comprehensive LLM assessment in Darija, focusing on discriminative and generative tasks.  - An experimental analysis was conducted on fine-tuning strategies and base model choices, finding that instruction-tuned Gemma 2 models with LoRA performed optimally. | ['Natural Language Processing', 'Translation', 'Summarization'] | N/A | [Link](https://hf.co/MBZUAI-Paris/Atlas-Chat-9B), [Link](https://hf.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture) |
| [ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer](https://arxiv.org/abs/2410.00086) | Jingren, chenweix7, chaojiemao, jingfengzhang, jiangzeyinzi |  - ACE, a unified framework based on a Diffusion Transformer, supports a wide range of visual generation and editing tasks through natural language instructions, including text-guided generation, low-level visual analysis, controllable generation, semantic editing, element editing, repainting, layer editing, and reference generation. - ACE introduces the Long-context Condition Unit (LCU) to incorporate historical information from previous generation rounds, enabling multi-turn and long-context generation. - A meticulous data collection workflow is established to construct a 0.7 billion-scale dataset covering various generation and editing tasks. - Evaluation on benchmarks such as MagicBrush and a user study on a manually curated benchmark demonstrates ACEâ€™s superior performance in various visual generation tasks. - ACE can be easily integrated into a multimodal chat system to streamline image creation and editing, avoiding cumbersome pipelines typically employed in visual agents. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | [Link](https://huggingface.co/runwayml/stable-diffusion-v1-5), [Link](https://huggingface.co/runwayml/stable-diffusion-inpainting) |
| [Visual Context Window Extension: A New Perspective for Long Video Understanding](https://arxiv.org/abs/2409.20018) | Zhenzhong Chen, hcwei | This research paper proposes a novel approach to enhance long video understanding by extending the visual context window of Large Multimodal Models (LMMs). - It redefines the context window in LMMs as two distinct windows: visual and language, addressing the discrepancies between these modalities. - The study introduces a method to extend positional embeddings within the visual context window, enabling LMMs to handle lengthy videos without retraining on large video-text datasets. - A progressive pooling strategy is implemented to reduce memory consumption by selectively adjusting the spatial resolution of frame embeddings. - Experimental results on benchmarks like MLVU, VideoMME, and LongVideoBench demonstrate consistent performance improvements with increasing video frames, outperforming models like GPT-40 and achieving memory savings of approximately 45%. | ['Multimodal', 'Video-Text-to-Text'] | N/A | N/A |


## Papers for 2024-10-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566) | nm-w, pdufter, zhegan27, fly6464, haotiz |  - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. - MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training. - It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks. - MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI). -  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [DiaSynth -- Synthetic Dialogue Generation Framework](https://arxiv.org/abs/2409.19020) | Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl |  - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning. - It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues. - Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks. - The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data. - DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [Hyper-Connections](https://arxiv.org/abs/2409.19606) | banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder | This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse. - Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters. - Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections. - Hyper-connections are also effective in vision tasks. - Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections. - The authors anticipate this method's broad applicability across various AI problems. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Image-to-Text', 'Unconditional Image Generation', 'Text2Text Generation'] | N/A | N/A |
| [Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models](https://arxiv.org/abs/2409.18943) | yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li | - RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs). - Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation. - RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs. - Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments.  - RULER maintains overall performance on various other benchmarks without affecting non-length based generation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Geaming2002/Ruler) | N/A |
| [Cottention: Linear Transformers With Cosine Attention](https://arxiv.org/abs/2409.18747) | Eric C. Larson, TrevorDohm, gmongaras | This study introduces "Cottention," a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/gmongaras/Cottention_Transformer) | N/A |
| [Can Models Learn Skill Composition from Examples?](https://arxiv.org/abs/2409.19808) | Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu | This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4. - Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills. - Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill. - The study shows that incorporating skill-rich synthetic text improves compositional capabilities. - Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency. - Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code](https://arxiv.org/abs/2409.19715) | Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae | COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym) |
| [IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding](https://arxiv.org/abs/2409.19627) | Jianzong Wang, Jing Xiao, zhangxulong, Pechola | - IDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization. - It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating. - A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability. - IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods. - Experimental results show its ability to withstand various attacks while maintaining good imperceptibility. | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/PecholaL/IDEAW) | N/A |


## Papers for 2024-09-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692) | Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang |  - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video. - It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning. - Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines. - It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning. - MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs. | ['Multimodal', 'Any-to-Any', 'Text-to-Image', 'Image-to-Text', 'Text-to-Speech', 'Automatic Speech Recognition', 'Video-Text-to-Text'] | N/A | N/A |
| [VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2409.17066) | Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92 |  - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization. - VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization. - The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization. - They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks. - VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/VPTQ) | N/A |
| [Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult](https://arxiv.org/abs/2409.17545) | fetong | This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs). - MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning. - Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models. - On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B. - MIPO simplifies hyperparameter tuning by using only a single parameter, Î², exhibiting robustness across different model architectures and datasets within a specific range. - MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback) |
| [MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making](https://arxiv.org/abs/2409.16686) | Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu | - The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales. - MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights. - Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment. - The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios. - MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents. | ['Robotics', 'Question Answering'] | N/A | N/A |


## Papers for 2024-09-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference. - Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end. - Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models. - MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks. - The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | - LLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings. - This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks. - LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets. - The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks. - Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling. | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-to-Text', 'Image-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | - EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities. - It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control. - The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data. - EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models. - A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Text-to-Audio', 'Audio-to-Audio', 'Visual Question Answering', 'Image-to-Text'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming |  - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs.  - It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference.  - This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV.  - Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilterâ€™s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O.  - Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | [Link](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose.  - Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data).  - Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models.  - Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain.  - A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/john-hewitt/implicit-ins) | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era. - It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations. - The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs. - It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs. -  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security. | ['Natural Language Processing', 'Summarization'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin ClaviÃ©, NohTow | This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations. - Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets. - It can achieve even further reduction of vector count by 66% with less than 3% performance degradation. - This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models. - The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors. - The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/colbert-ir/colbertv2.0) |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | PÃ¥l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar |  - This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG). - It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness. - Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time. - The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs. - While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.  - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.  -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.  -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.  -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Text-to-Speech', 'Automatic Speech Recognition', 'Any-to-Any'] | N/A | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Image-to-3D'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence. | ['Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt | This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that "response tuning," which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that "single-task finetuning,"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin ClaviÃ©, NohTow | This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang | â€¢ This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field. â€¢ The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA. â€¢ The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses. â€¢ The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach. â€¢ The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs). | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | PÃ¥l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This research introduces MaskLLM, a novel learnable pruning method that generates semi-structured (N:M) sparsity in Large Language Models (LLMs) for enhanced inference efficiency. - MaskLLM distinguishes itself from previous methods by directly learning the distribution of N:M sparsity patterns using Gumbel Softmax sampling, enabling end-to-end training on large datasets and addressing limitations of hand-crafted importance criteria. -  Empirical evaluations on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, demonstrate that MaskLLM surpasses state-of-the-art techniques, achieving a perplexity of 6.72 on LLaMA2-7B compared to SparseGPT's 10.42. - The research underscores the efficacy of learning sparsity patterns directly from data, leading to more accurate and efficient compression of LLMs without compromising performance. - The adaptability of MaskLLM to downstream tasks and its ability to achieve lossless compression in certain scenarios highlight its potential for practical applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research proposes LLaVA-3D, a novel framework for building 3D-aware Large Multimodal Models (LMMs) by adapting the existing 2D LLaVA model.  LLaVA-3D introduces the concept of "3D Patches," which inject 3D positional embeddings into 2D image features, enhancing the model's spatial understanding without complex 3D processing pipelines.  Evaluations demonstrate LLaVA-3D's state-of-the-art performance on various 3D tasks, including question answering, dense captioning, and visual grounding, surpassing existing 3D LMMs while maintaining comparable 2D image understanding capabilities to its 2D counterpart. The research highlights the advantages of leveraging pre-trained 2D LMMs for 3D scene understanding and the benefits of integrating 3D spatial information into 2D visual features. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image-to-Text', 'Text-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal large language model capable of perceiving and generating images, text, and speech with emotional expressiveness. EMOVA utilizes a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for end-to-end speech processing.  The model achieves state-of-the-art performance on both vision-language and speech benchmarks, outperforming models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks and surpassing the speech LLM Mini-Omni in ASR tasks. EMOVA also enables emotional spoken dialogue by explicitly predicting speech style labels (emotions and pitches) and leveraging a lightweight style module for controllable speech synthesis. This is achieved through a novel text-centric multimodal alignment approach, which leverages publicly available bimodal data and eliminates the reliance on scarce trimodal data. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach to reduce the computational cost and latency of processing long context inputs with Large Language Models (LLMs). GemFilter leverages the ability of early LLM layers to identify relevant tokens and compresses the input sequence by a factor of 1000x for subsequent processing by the full model. Empirical evaluations show that GemFilter achieves a 2.4x speedup and 30% reduction in GPU memory consumption compared to state-of-the-art methods, while maintaining comparable performance on benchmarks like LongBench and outperforming them on the Needle in a Haystack task. GemFilter is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting the selected input sequence. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This research paper investigates implicit instruction tuning, demonstrating that instruction following can emerge without explicit instruction-response training.  - The authors show that training solely on responses (response tuning) and on narrow-domain data (single-task finetuning) leads to broad instruction-following abilities in language models.  - For instance, response-tuned models achieve a 43% win rate against explicitly instruction-tuned models in head-to-head evaluations.  - Furthermore, they introduce a simple rule-based language model that, when combined with a pretrained model, exhibits instruction-following behavior.  - These findings highlight that adaptation methods not explicitly designed for instruction following can implicitly induce such capabilities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin ClaviÃ©, NohTow | This research paper introduces a novel technique named "TOKEN POOLING" for enhancing the efficiency of multi-vector retrieval models, especially focusing on ColBERT, without significantly affecting performance.  The method uses clustering techniques to group together similar token representations and then applies mean pooling to create a single, representative vector, effectively reducing the overall storage footprint. Experiments show that this approach reduces the required vector count by 50% without compromising accuracy, and a 66% reduction still yields strong performance. The paper also demonstrates that TOKEN POOLING can be effectively combined with existing quantization methods, leading to even more significant compression rates while maintaining reasonable retrieval performance.  | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper presents a comprehensive review of the emerging field of Conversation Analysis (CA), a process designed to extract critical information from conversational data and leverage it for system optimization and decision-making.  - The paper systematically defines CA as a four-step procedure: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, and discusses the challenges and trends within each step. - The paper further argues that while previous CA efforts focused on atomic tasks with limited business impact, the rise of Large Language Models (LLMs) enables deeper, more insightful analysis and strategic decision-making from conversations.  - The authors compile and categorize existing benchmark datasets for CA but highlight a significant gap in comprehensive benchmarks containing fine-grained conversation elements and long-context modeling capabilities.  - The paper concludes by outlining future research directions, including the development of LLM-based conversation simulators, fine-grained CA benchmarks, long-context conversation modeling, in-depth attribution analysis, and advanced goal-directed optimization and evaluation methods. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | PÃ¥l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging Knowledge Graphs (KGs) and graph-based architectures. The authors demonstrate the effectiveness of their framework by applying it to the SoccerNet dataset, a large dataset of soccer videos. Their findings show that Structured-GraphRAG significantly improves query processing efficiency, reduces response times, and enhances accuracy compared to traditional RAG methods. The structured nature of KGs reduces hallucinations in LLMs, making the responses more consistent and reliable. The authors highlight that their framework can be applied to a broad range of applications due to its flexible design. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


