# Daily AI Papers

These summaries are automatically generated from [HuggingFace's Daily Papers](https://huggingface.co/papers), using Gemini and GitHub actions based on the following categories of interest: Multimodal, Text Generation, Text Classification, Text2Text Generation, Summarization, Question Answering, Natural Language Processing, Audio, Text-to-Speech, Audio-to-Audio. All credits go to the research community for sharing and the HuggingFace community for curating these papers.

Please note:
- Authors may be listed by their HuggingFace user id. This will be rectified soon. 
- These summaries are entirely generated by the LLM. You can refer to the basic prompt [here](templates/prompt_template.md).

Last updated: 2024-11-21 
 


## Papers for 2024-11-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2411.10958) | Jun Zhu, Jia Wei, Pengle Zhang, Haofeng Huang, jt-zhang | - SageAttention2 is a new attention mechanism that uses 4-bit matrix multiplication and additional precision-enhancing techniques to accelerate attention computation while maintaining precision. - It quantizes matrices Q and K to INT4 in warp-level granularity, and matrices P and V to FP8, along with smoothing techniques for Q and V to enhance accuracy. - An adaptive mixed-precision method employs 8-bit attention for problematic layers/timesteps and 4-bit attention for others, further improving accuracy. - On an RTX 4090, SageAttention2 achieves a peak performance of 485 TOPS, surpassing FlashAttention2 and xformers by approximately 3.1x and 5.4x, respectively. - Comprehensive experiments across diverse models, including large language, image generation, and video generation models, demonstrate negligible end-to-end metric loss with SageAttention2. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Image-to-Video', 'Text-to-Video', 'Image Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation](https://arxiv.org/abs/2411.13281) | Mohan Kankanhalli, Jing Ma, Dongxu Li, teowu, Ziyang | - Introduces VideoAutoArena, an automated arena-style benchmark for evaluating Large Multimodal Models (LMMs) in video analysis using simulated user interactions and open-ended questions. - Employs user simulation with role-playing by LMM agents to generate diverse, realistic questions based on video content and user personas with varying degrees of relevance to the video. - Implements a fault-driven evolution strategy to progressively increase question complexity, challenging models to address increasingly difficult video analysis scenarios. - Leverages an automated judging system based on the LMM-as-a-Judge paradigm, comparing responses and ranking models using a modified ELO rating system, showing strong alignment with human judgment (87.29%). - Introduces an auxiliary benchmark, VideoAutoBench, streamlining LMM evaluation by comparing model responses against human-selected winners from a subset of VideoAutoArena battles, demonstrating consistent ranking results between the arena and the bench. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text'] | N/A | [Link](https://videoautoarena.github.io/) |
| [When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training](https://arxiv.org/abs/2411.13476) | Cunxiao Du, Tongyao Zhu, Chao Du, Qian Liu, haonan3 | - This paper introduces AnchorAttention, a novel attention mechanism designed to improve long-context training of Large Language Models (LLMs) by addressing the numerical instability of Rotary Position Embedding (RoPE) when using BFloat16 precision. - AnchorAttention mitigates the issue by treating the first token in the input sequence as a shared anchor with a consistent position ID, visible to all documents within the context window, but masking out attention across different documents. - This reduces attention computations and the accumulation of numerical errors while ensuring the model learns a full spectrum of rotational angles in RoPE.  - Experiments on the RULER benchmark and real-world long-context datasets like LongBench show that AnchorAttention outperforms standard full attention and intra-document attention, boosting performance and reducing training time by over 50%. - AnchorAttention maintains performance on medium and short context benchmarks such as MMLU and HellaSwag while excelling in long context settings. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/haonan3/AnchorContext) | N/A |
| [ORID: Organ-Regional Information Driven Framework for Radiology Report Generation](https://arxiv.org/abs/2411.13025) | Dongnan Liu, Ziyong Feng, Xiang An, Tiancheng Gu, Kaichengalex | - This paper introduces ORID, an Organ-Regional Information Driven framework, for generating radiology reports from medical images.  - ORID leverages a fine-tuned multi-modal large language model (LLaVA-Med-RRG) to generate organ-specific diagnostic descriptions. - It employs an organ-based cross-modal fusion module and an organ importance coefficient analysis module to integrate image and text features effectively and to reduce the influence of unrelated organ regions, respectively.  - The framework is evaluated on IU-Xray and MIMIC-CXR datasets, achieving state-of-the-art performance in NLG metrics on both datasets.  - ORID also demonstrates superior clinical efficacy on MIMIC-CXR compared to other models. | ['Image-to-Text', 'Multimodal'] | N/A | N/A |


## Papers for 2024-11-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [RedPajama: an Open Dataset for Training Large Language Models](https://arxiv.org/abs/2411.12372) | Shane Adams, Yonatan Oren, Quentin Anthony, Daniel Fu, Maurice Weber | - The paper releases RedPajama-V1, an open reproduction of the LLaMA training dataset, and RedPajama-V2, a new web-based dataset comprising over 100 trillion tokens with quality signals for filtering. - RedPajama-V2 dataset emphasizes transparency by documenting its creation process, offering data at scale, and includes artifacts and quality signals for filtering and creating higher quality datasets. - This dataset has been instrumental in training production-ready large language models, such as Snowflake Arctic, Salesforce's XGen, and AI2's OLMo. - Ablation studies are conducted using decoder-only transformer models with up to 1.6B parameters, demonstrating how quality signals enhance dataset curation. - The study emphasizes the potential of RedPajama in building more transparent, high-performing large language models. | ['Natural Language Processing'] | [Link](github.com/togethercomputer/RedPajama-Data) | [Link](huggingface.co/datasets/togethercomputer/RedPajama-Data-1T), [Link](huggingface.co/datasets/togethercomputer/RedPajama-Data-V2) |
| [Building Trust: Foundations of Security, Safety and Transparency in AI](https://arxiv.org/abs/2411.12275) | Huamin Chen, Mark Bestavros, Emily Fox, Garth Mollett, huzaifas-sidhpurwala | - This paper explores the security and safety implications of publicly available AI models, particularly large language models (LLMs). - It reviews current security and safety scenarios, highlighting challenges like issue tracking, remediation, and the lack of established lifecycle and ownership processes for AI models. - The paper proposes comprehensive strategies to enhance security and safety for both model developers and end-users. - It discusses the distinction between AI security (protecting systems from threats) and AI safety (preventing harm from the system's operation), emphasizing the need for a holistic approach to AI risk management. - The paper also suggests adapting existing vulnerability disclosure processes for AI security flaws and proposes the establishment of a central body for tracking safety hazards. | ['Natural Language Processing'] | N/A | N/A |
| [Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages](https://arxiv.org/abs/2411.12240) | D. J. Bora, tamang0000 | - This research paper evaluates the performance of tokenizers used by 12 Large Language Models (LLMs) across all 22 official languages of India. - The study uses Normalized Sequence Length (NSL) as the key metric for evaluation and finds that the SUTRA tokenizer outperforms other models, including Indic-specific models, in 14 out of 22 languages. - Notable findings include SUTRA's superior performance with Indic languages, GPT-40's improvement over GPT-4 in processing Indian languages, and the comparatively limited performance of Project Indus. - This suggests that Project Indus' better performance on some Indian language is tied to the training on common scripts (Devanagari) between the languages and not to the linguistic understanding. - The study highlights the importance of developing targeted tokenization strategies for multilingual and Indic-centric LLMs. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |


## Papers for 2024-11-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices](https://arxiv.org/abs/2411.10640) | wolf1110, AJZhou, liuyangbian, yina0, lucky-lance | - BlueLM-V-3B is a 3 billion parameter multimodal large language model designed for mobile devices, featuring a 2.7B parameter language model and a 400M parameter vision encoder (SigLIP). - It introduces a relaxed aspect ratio matching method for dynamic image resolution, reducing the number of image tokens processed by the vision encoder without sacrificing model performance. - System optimizations include batched image encoding, pipeline parallelism for image processing, token downsampling, chunked computing of input tokens, and mixed-precision quantization. - BlueLM-V-3B achieves state-of-the-art performance on the OpenCompass benchmark, with an average score of 66.1, surpassing larger models like MiniCPM-V-2.6 and InternVL2-8B. - On a MediaTek Dimensity 9300 processor, it uses 2.2GB of memory, encodes 768x1536 images in 2.1 seconds, and generates text at 24.4 tokens/second. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Generative World Explorer](https://arxiv.org/abs/2411.11844) | Daniel Khashabi, Alan Yuille, Tianmin Shu, jienengchen, TaiMingLu | - The paper introduces Generative World Explorer (Genex), a novel egocentric video generation model that allows embodied agents to "mentally" explore 3D environments (e.g., urban scenes) by generating imagined future observations, and then use these observations to update the agent's beliefs about the world, which enable the agent to make more informed decisions. - The architecture is based on a video diffusion model that takes an initial egocentric view (represented as a panoramic image), the intended movement direction as action input, and then streams a video of future egocentric observations.  - A spherical-consistent learning objective is introduced to ensure that generated pixels are continuous in the spherical space and improve the consistency and coherence of generated videos during long imaginative exploration. - The experimental evaluation, with a new synthetic urban scene dataset (Genex-DB) and new Embodied QA benchmark (Genex-EQA), shows that Genex generates high-quality and consistent observations during imaginative exploration and improves an existing LLM agent’s decision-making process. - The authors extend Genex to multi-agent scenarios, where an agent infers the perspectives of other agents to make decisions based on a more complete understanding of the situation. | ['Text-to-Video', 'Computer Vision', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/Beckschen/genex) | [Link](https://generative-world-explorer.github.io/) |
| [Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering](https://arxiv.org/abs/2411.11504) | Ben He, Boxi Cao, Xinyu Lu, Yanjiang Liu, Xinyan Guan | This paper introduces verifier engineering, a novel post-training paradigm for foundation models.  It leverages automated verifiers to perform verification tasks and deliver feedback to the models, enhancing their capabilities.  The framework systematically categorizes this process into three stages: search, verify, and feedback.  The authors provide a comprehensive overview of state-of-the-art research in each stage. This approach is presented as a fundamental pathway toward achieving Artificial General Intelligence. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/icip-cas/Verifier-Engineering) | N/A |
| [SlimLM: An Efficient Small Language Model for On-Device Document Assistance](https://arxiv.org/abs/2411.09944) | Viet Dac Lai, Seunghyun Yoon, Phat T. Nguyen, Thang M. Pham, Franck-Dernoncourt | - SlimLM, a series of small language models (SLMs) optimized for document assistance tasks on mobile devices like smartphones, is introduced. - The models range from 125M to 1B parameters and are pre-trained on SlimPajama-627B and fine-tuned on DocAssist, a new dataset constructed for summarization, question answering, and question suggestion. - SlimLM models demonstrate comparable or superior performance to existing SLMs of similar sizes on a Samsung Galaxy S24, efficiently handling contexts up to 800 tokens. - An Android application demonstrates SlimLM's real-world applicability. - The models offer a balance between performance, efficiency, and privacy for on-device document processing, potentially reducing reliance on server-based APIs. | ['Natural Language Processing', 'Document Question Answering', 'Question Answering', 'Summarization'] | N/A | N/A |
| [Top-$nσ$: Not All Logits Are You Need](https://arxiv.org/abs/2411.07641) | Liusheng Huang, Hongli Xu, Jianchun Liu, tomorrowdawn | - This paper introduces top- σ, a novel sampling method for large language models (LLMs) that operates directly on pre-softmax logits by leveraging a statistical threshold. - The method distinguishes between a Gaussian-distributed noisy region and a distinct informative region in the logits, enabling efficient token filtering without complex probability manipulations. - Unlike existing methods, top- σ maintains a stable sampling space regardless of temperature scaling, making it suitable for test-time scaling techniques. - Experimental results across four reasoning-focused datasets demonstrate that top- σ outperforms existing sampling approaches and greedy decoding, while maintaining consistent performance at high temperatures. - The theoretical analysis of top-no provides further insights into its behavior and temperature invariance property. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts](https://arxiv.org/abs/2411.10669) | Nanyi Fei, Hongpeng Lin, Guoxing Yang, Yanqi Dai, Jinqiang Long | - This paper introduces Awaker2.5-VL, a Mixture of Experts (MoE) architecture designed for Multimodal Large Language Models (MLLMs) to address the "multi-task conflict" issue, where mixing data from various tasks leads to performance degradation. - Awaker2.5-VL utilizes multiple sparsely activated expert models, each specializing in a specific task, along with a global expert for general capabilities, and a gating network to control expert activation. - The model employs Low-Rank Adaptation (LoRA) for each expert to reduce training and inference costs and incorporates a simplified routing strategy for enhanced training stability. - The paper uses Qwen2-VL-7B-Instruct as its base model and evaluates performance on benchmarks such as MME-Realworld, MME-Realworld-CN, and MMBench. - Awaker2.5-VL achieves state-of-the-art results on the MME-Realworld-CN benchmark and shows significant improvements over the base model on other benchmarks, including a 5-point improvement in overall score on MME-Realworld-CN. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Object Detection'] | [Link](https://github.com/MetabrainAGI/Awaker) | N/A |
| [LLäMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171) | Andreas Hotho, Julia Wunderle, Jan Pfister | - This paper introduces LLäMmlein, two new German-only decoder-only LLMs (120M and 1B parameters), trained from scratch on a filtered and preprocessed version of the RedPajama dataset. - A new German tokenizer with a 32,000 token vocabulary was created and models were evaluated on the SuperGLEBer and lm-evaluation-harness-de benchmarks, showing competitive performance against similarly sized models and even some larger models. - The 1B model matches the performance of much larger models, like the German finetuned Llama 8B on the SuperGLEBer benchmark and its instruction-tuned version outperforms all other 1B models on TruthfulQA by at least 6%. - Intermediate checkpoints were analyzed to track the learning dynamics, revealing varying rates of improvement across different tasks. - All artifacts, including the models, code, and data, will be open-sourced to promote transparency and future research within the German NLP community. | ['Natural Language Processing', 'Text Generation', 'Question Answering', 'Token Classification', 'Sentence Similarity'] | N/A | [Link](https://huggingface.co/cis-lmu/bavarian_to_english), [Link](https://huggingface.co/LSX-UniWue/Guanako), [Link](https://huggingface.co/FreedomIntelligence/alpaca-gpt4-deutsch), [Link](https://huggingface.co/FreedomIntelligence/evol-instruct-deutsch), [Link](https://huggingface.co/FreedomIntelligence/sharegpt-deutsch), [Link](https://huggingface.co/DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1) |
| [Adaptive Decoding via Latent Preference Optimization](https://arxiv.org/abs/2411.09661) | Jason Weston, Asli Celikyilmaz, Ping Yu, Ilia Kulikov, Shehzaad Dhuliawala | - This paper introduces Adaptive Decoding, a method for dynamically adjusting the sampling temperature during language model generation, leading to improved performance on tasks requiring varying levels of creativity and factual accuracy. - The core component is the Adaptive Decoder module, a small neural network added to the LLM's architecture, predicting the optimal temperature at either the token or sequence level. - Latent Preference Optimization (LPO), a novel training approach based on Direct Preference Optimization, trains the Adaptive Decoder by sampling multiple responses with varying temperatures, scoring them with a reward model, and learning to prefer temperatures associated with higher-ranked responses. - Experiments on a combined dataset (UltraMathStories) of math, creative writing, and general instructions show that Adaptive Decoding outperforms fixed temperature baselines. - Additionally, the method demonstrates success in constrained creative writing, where it learns to use low temperatures for constrained parts and higher temperatures for creative parts of the text, outperforming greedy and high-temperature baselines. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2024-11-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440) | LiYuan, sunlichao137, Yibing, Pengjin, Xkev | - This paper introduces LLaVA-01, a Vision Language Model (VLM) designed for improved multi-stage reasoning in visual question answering. - LLaVA-01 uses a structured approach with four stages: summarization, visual interpretation, logical reasoning, and conclusion generation, unlike traditional chain-of-thought prompting. - A new dataset, LLaVA-01-100k, was created with structured reasoning annotations from various visual question answering sources, used to fine-tune the Llama-3.2-11B-Vision-Instruct model. - A novel stage-level beam search method enables efficient inference-time scaling. - LLaVA-01 surpasses larger and closed-source models like Gemini-1.5-pro and GPT-40-mini on multiple multimodal reasoning benchmarks. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use](https://arxiv.org/abs/2411.10323) | Mingyu Ouyang, AnalMom, QuStar, SiyuanH | - This paper presents a preliminary case study on Claude 3.5 Computer Use, a new large language model (LLM) designed for GUI automation. - The study evaluates the model's abilities in planning, action execution, and critic feedback across various desktop environments, including web search, office productivity software, workflow applications, and video games. - The researchers propose a novel framework, Computer Use Out-of-the-Box (OOTB), which offers a cross-platform solution for easy implementation and benchmarking of GUI automation models. - The study identifies some limitations in Claude 3.5's handling of tasks, including scrolling-based navigation and text selection precision, as well as occasional inaccuracies in final outcome evaluations. - The findings suggest the need for future research to focus on improved selection capabilities, more accurate validation feedback, and enhanced adaptability to real-world application complexities. | ['Multimodal'] | [Link](https://github.com/showlab/computer_use_ootb) | N/A |


## Papers for 2024-11-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models](https://arxiv.org/abs/2411.09595) | Jun Zhu, Hang Su, Yikai Wang, Jonathan Lorraine, Zhengyi Wang | - LLaMA-Mesh is a novel approach that unifies 3D mesh generation with Large Language Models (LLMs) by representing meshes as plain text, allowing seamless integration without tokenizer or vocabulary modifications. - The model leverages the OBJ file format, treating vertex coordinates and face definitions as text sequences, enabling LLMs to process 3D data directly. - A supervised fine-tuning (SFT) dataset with text-3D pairs and interleaved dialogues is used to train a pretrained LLaMA model, enabling it to generate 3D meshes from text prompts, understand 3D meshes, and maintain conversational abilities. - LLaMA-Mesh achieves mesh generation quality comparable to specialized 3D generation models while preserving the LLMs' language capabilities, as demonstrated by qualitative and quantitative results. - The work represents a significant step towards integrating multi-modal content generation within a unified language model. | ['Text-to-3D', 'Multimodal', 'Natural Language Processing'] | N/A | N/A |
| [Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/abs/2411.09009) | Philipp Krähenbühl, Vladlen Koltun, Alexander Hertzberg, Brody Huval, erikwijmans | - This paper introduces Cut Cross-Entropy (CCE), a novel method to compute cross-entropy loss and its gradients without materializing the full logits matrix in memory. - CCE performs matrix multiplications and log-sum-exp operations within flash memory, reducing the memory footprint for loss calculation from gigabytes to megabytes. - It leverages softmax sparsity to further improve throughput by skipping negligible gradient computations. - Experiments on large language models like Gemma 2 and Llama 3 demonstrate significant memory reduction and increased batch size without impacting training speed or convergence. - The proposed method facilitates training larger language models with extended vocabulary sizes under memory constraints. | ['Natural Language Processing'] | [Link](https://github.com/apple/ml-cross-entropy) | N/A |
| [ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?](https://arxiv.org/abs/2411.06469) | Zhongwei Wan, Che Liu, Shan Chen, Jian Yu, canyuchen | - ClinicalBench, a new benchmark, was introduced to evaluate the performance of LLMs and traditional machine learning models on clinical prediction tasks. - The benchmark includes three common clinical prediction tasks (Length-of-Stay, Mortality, and Readmission), two real-world clinical databases (MIMIC-III and MIMIC-IV), 22 LLMs with varying sizes, and 11 traditional ML models. - Through empirical studies, traditional machine learning models outperformed both general-purpose and medical LLMs across all tasks and datasets, even with varying model sizes, prompting, or fine-tuning strategies. - This suggests a potential deficiency in the clinical reasoning and decision-making capabilities of current LLMs. - The benchmark and code are publicly available to facilitate further research in this domain. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3), [Link](https://huggingface.co/google/gemma-2-9b-it), [Link](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2-7B-Instruct), [Link](https://huggingface.co/01-ai/Yi-1.5-6B-Chat), [Link](https://huggingface.co/01-ai/Yi-1.5-9B-Chat), [Link](https://huggingface.co/01-ai/Yi-1.5-34B-Chat), [Link](https://huggingface.co/lmsys/vicuna-7b-v1.5), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), [Link](https://huggingface.co/internlm/internlm2_5-7b-chat), [Link](https://huggingface.co/openbmb/MiniCPM3-4B), [Link](https://huggingface.co/epfl-llm/meditron-7b), [Link](https://huggingface.co/epfl-llm/meditron-70b), [Link](https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20), [Link](https://huggingface.co/BioMistral/BioMistral-7B), [Link](https://huggingface.co/m42-health/Llama3-Med42-8B), [Link](https://huggingface.co/m42-health/Llama3-Med42-70B), [Link](https://huggingface.co/PharMolix/BioMedGPT-LM-7B), [Link](https://huggingface.co/internistai/base-7b-v0.2), [Link](https://huggingface.co/docs/transformers/en/index) |
| [Hermes: A Large Language Model Framework on the Journey to Autonomous Networks](https://arxiv.org/abs/2411.06490) | Merouane Debbah, Antonio De Domenico, Ali Maatouk, Fadhel Ayed, nicopi | - Hermes is a novel chain-of-agent LLM framework designed to automate the creation of Network Digital Twins (NDTs) using blueprints, enhancing the path towards autonomous network management. - Unlike existing NDT approaches that require distinct architectures for each use case, Hermes uses LLMs to generate step-by-step logical blocks (blueprints) for NDT construction, improving flexibility and scalability. - The framework consists of a Designer agent for creating and refining the blueprint based on network data and policies, and a Coder agent for translating the blueprint into executable Python code. - Experimental results across four autonomous network tasks demonstrated that Hermes with GPT-40 as LLM consistently outperforms chain-of-thought prompting and direct code generation, achieving success rates up to 80%. - While open-source LLMs show limited performance independently, integrating them with a library of expert-designed models significantly improves their effectiveness in NDT blueprint design, highlighting the potential for broader application. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2024-11-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Large Language Models Can Self-Improve in Long-context Reasoning](https://arxiv.org/abs/2411.08147) | Mo Yu, Lemao Liu, Zesen Cheng, Cheng Yang, Siheng99 | - This paper introduces SEALONG, a self-improving method for Large Language Models (LLMs) to enhance long-context reasoning. - SEALONG samples multiple reasoning trajectories from the LLM, scores them using Minimum Bayes Risk (MBR), and fine-tunes using either supervised learning or preference optimization. - Experiments show improvement on LLMs like Llama-3.1-8B-Instruct by 4.2 points. - SEALONG outperforms existing methods reliant on human annotations or expert models by leveraging the LLM's own generated outputs. - The results demonstrate substantial potential for LLMs to self-improve in long-context reasoning and suggests promise for further research. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/SihengLi99/SEALONG) | N/A |
| [Direct Preference Optimization Using Sparse Feature-Level Constraints](https://arxiv.org/abs/2411.07618) | Hanqi Yan, Minjun Zhu, Hongbo Zhang, Chak Tou Leong, Qingyu Yin | - This paper introduces Feature-level constrained Preference Optimization (FPO), a novel method for aligning Large Language Models (LLMs) with human preferences. - FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment, simplifying the alignment process while ensuring stability. - FPO achieves an above 5% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines on benchmark datasets such as AlpacaEval-2 and Arena-Hard. - By constraining the shifts of sparse features during training, FPO achieves results that meet or exceed the effectiveness of sequential KL divergence with lower computational cost (17.6% reduction compared to TDPO2). - FPO combines the efficiency of SimPO with the constraint quality of sequential KL, making it a promising solution for efficient and controllable LLM alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/abs/2411.08868) | Benoît Sagot, Éric de la Clergerie, Rian Touchent, Francis Kulumba, Wissam Antoun | - This paper introduces two new versions of the French language model CamemBERT: CamemBERTav2 and CamemBERTv2. - CamemBERTav2 is based on the DeBERTaV3 architecture and uses a Replaced Token Detection (RTD) training objective, while CamemBERTv2 uses the RoBERTa architecture with a Masked Language Modeling (MLM) objective. - Both models are trained on a significantly larger and more up-to-date dataset than their predecessors, with an updated tokenizer to better handle the nuances of modern French text. - Evaluations on various NLP tasks, including question answering, named entity recognition, and text classification, show that both models significantly outperform previous versions, particularly CamemBERTav2 which yielded the best performance in most cases. - All models and checkpoints are publicly available on Hugging Face. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification'] | N/A | [Link](https://huggingface.co/almanach?search_models=camembert+v2) |
| [Can sparse autoencoders be used to decompose and interpret steering vectors?](https://arxiv.org/abs/2411.08790) | Adam Mahdi, Yushi Yang, Harry Mayne | - This paper investigates why applying sparse autoencoders (SAEs) directly to steering vectors results in inaccurate decompositions, hindering interpretability. - Two primary reasons are identified: steering vectors fall outside the training distribution of SAEs, and SAEs cannot accommodate meaningful negative projections present in steering vectors. - The study uses the corrigibility steering vector as a case study, demonstrating that the SAE decomposition is largely driven by the encoder bias, overshadowing the steering vector's contribution. - Scaling the steering vector's L2-norm does not resolve the out-of-distribution issue, as default components present in model activations are absent in steering vectors due to the subtraction process involved in their creation. -  The restriction of SAEs to non-negative reconstruction coefficients leads to misinterpretations, as it overlooks negative projections in feature directions, and these negative projections can also cause spurious positive activations in other features due to negative cosine similarity between features. | ['Natural Language Processing'] | [Link](https://github.com/HarryMayne/SV_interpretability) | [Link](https://huggingface.co/google/gemma-scope-2b-pt-res/tree/main/layer_14/width_16k/average_10_173) |


## Papers for 2024-11-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) | Chengyue Wu, Wen Liu, Xiaokang Chen, Xingchao Liu, Yiyang Ma | - JanusFlow is a unified multimodal model that integrates autoregressive language models with rectified flow for both image understanding and generation tasks. - The model uses a minimalist architecture with a lightweight encoder and decoder to adapt the LLM for rectified flow operations, and employs two key strategies: decoupling understanding and generation encoders and aligning their representations during training. - On text-to-image generation, JanusFlow surpasses existing models on benchmarks like MJHQ FID-30k, GenEval, and DPG-Bench. - It also achieves state-of-the-art performance in multimodal comprehension tasks on benchmarks like MMBench, SeedBench, and GQA, exceeding specialized models. - These results are achieved with a compact 1.3B parameter LLM, showing the potential for efficient and versatile vision-language models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | [Link](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) |
| [Stronger Models are NOT Stronger Teachers for Instruction Tuning](https://arxiv.org/abs/2411.07133) | Radha Poovendran, Luyao Niu, Fengqing Jiang, Zhangchen Xu, yuchenlin | - This paper challenges the assumption that larger language models (LLMs) are always better teachers for instruction tuning of smaller LLMs, a phenomenon termed the "Larger Models' Paradox." - Through experiments across five base models and twenty response generators, they demonstrate that larger models within a model family don't always lead to better instruction-following performance in smaller models after fine-tuning. - Existing metrics for data selection, such as quality, difficulty, and response length, fail to predict response generator effectiveness because they don't account for teacher-student model compatibility. - They propose a new metric, Compatibility-Adjusted Reward (CAR), that considers both the reward of generated responses and their compatibility with the base model (measured by loss on the base model), which outperforms baseline metrics in predicting response generator effectiveness. - Open-source models like Gemma-2-9b-it and Qwen2.5-72B-Instruct were found to be highly effective as response generators, outperforming even closed-source models like GPT-4 in some cases. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-100K-Generator-Zoo) |


## Papers for 2024-11-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models](https://arxiv.org/abs/2411.07140) | Hui Huang, Yingshui Tan, Jiaheng Liu, Shilong Li, Yancheng He | - This paper introduces Chinese SimpleQA, a benchmark designed to evaluate the factuality of Large Language Models (LLMs) in answering short questions in Chinese. - The dataset consists of 3000 high-quality question-answer pairs across six diverse topics, with a focus on static answers that do not change over time. - The questions and answers in Chinese SimpleQA are short, simplifying evaluation with existing LLMs, and the grading process employs the OpenAI API. - Initial findings indicate that Chinese SimpleQA is challenging for existing LLMs, with only a few achieving passing scores. - The research demonstrates the importance of model size, calibration, and the effectiveness of Retrieval-Augmented Generation (RAG) strategies in improving LLM performance in Chinese factuality. | ['Question Answering'] | N/A | N/A |
| [IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization](https://arxiv.org/abs/2411.06208) | Yongbin Li, Fei Huang, Cheng Fu, Haiyang Yu, Xinghua Zhang | - This paper introduces IOPO (Input-Output Preference Optimization), a new alignment method for Large Language Models (LLMs) that aims to improve complex instruction following. - Unlike existing methods like DPO that focus on output preference, IOPO considers both input and output preferences, enabling LLMs to better understand fine-grained constraints within complex instructions. - A new benchmark called TRACE, comprising 120K training and 1K evaluation instances with varying constraint complexities, is also introduced to facilitate training and evaluation of complex instruction following abilities. - Experiments on TRACE, IFEval, and CFBench demonstrate IOPO's effectiveness, showing improvements of 2.18% and 3.13% over DPO on in-domain and out-of-domain datasets, respectively. - Ablation studies confirm that both input and output preference modeling are crucial for instruction following, especially in complex scenarios. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework](https://arxiv.org/abs/2411.06176) | Maojia Song, Chaoqun Liu, Hou Pong Chan, Liying Cheng, Yew Ken Chia | - Introduces M-LongDoc, a benchmark dataset with 851 samples for evaluating multimodal long document understanding. - Proposes a retrieval-aware tuning approach to improve model performance in document question answering. - Presents an automated evaluation framework leveraging multiple judge models to assess the correctness of open-ended solutions. - Demonstrates that the retrieval-aware tuning approach achieves a 4.6% relative improvement in answer correctness compared to baseline open-source models. - Finds that existing models struggle with figure and table-based questions, highlighting a multimodal bias. | ['Multimodal', 'Document Question Answering'] | N/A | N/A |
| [Counterfactual Generation from Language Models](https://arxiv.org/abs/2411.07180) | Ryan Cotterell, Anej Svete, vesteinn, Shauli | - This paper proposes a framework for generating counterfactual text from language models (LMs) by treating them as Generalized Structural Equation Models (GSEMs) and using the Gumbel-max trick. - This approach allows for modeling the joint distribution of original and counterfactual strings, enabling investigation of cause-and-effect relationships within LMs. - An algorithm based on hindsight Gumbel sampling infers the noise variables that produced an observed string, facilitating the generation of its counterfactual. - Experiments on GPT2-XL and LLaMA3-8b, with interventions like knowledge editing and linear steering, reveal unintended side effects, showing that these techniques are not as surgical as intended. - This work highlights the need for more precise intervention methods in LMs to minimize unintended changes in generated text. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/shauli-ravfogel/lm-counterfactuals) | [Link](https://huggingface.co/intfloat/e5-base-v2), [Link](https://huggingface.co/jujipotle/honest_llama3_8B_instruct), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |
| [Game-theoretic LLM: Agent Workflow for Negotiation Games](https://arxiv.org/abs/2411.05990) | Julie Chen, Alfonso Amayuelas, Lingyao Li, Ollie Liu, Wenyue Hua | - This paper introduces game-theory-inspired workflows to enhance the rationality of Large Language Models (LLMs) in strategic decision-making, particularly within negotiation games. - The authors evaluate several state-of-the-art LLMs across various complete and incomplete information games and find that LLMs often deviate from rational strategies, especially in complex games. - They design distinct workflows incorporating principles like dominant strategy search, backward induction, and Bayesian belief updating to guide LLMs' reasoning and improve decision-making. - Experimental results demonstrate that these workflows significantly improve LLM performance in identifying optimal strategies, achieving near-optimal allocations, and reducing susceptibility to exploitation during negotiations. -  The paper further explores the meta-strategic considerations of workflow adoption, highlighting a novel research direction in analyzing the rationality of using such workflows in different scenarios. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/Wenyueh/game_theory) | N/A |
| [Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models](https://arxiv.org/abs/2411.06272) | Yiyan Qi, Zhouchi Lin, Huanyi Su, Junxi Liu, Xiaojun Wu | - This paper introduces Golden Touchstone, a bilingual (English and Chinese) benchmark for evaluating Financial Large Language Models (FinLLMs). - The benchmark covers eight core financial NLP tasks, including sentiment analysis, question answering, and summarization, with 22 datasets in total.  - The authors also release Touchstone-GPT, a FinLLM trained using continuous pre-training and financial instruction tuning on a 100B token dataset and 300k instruction-response pairs. - Evaluation results on Golden Touchstone show that while existing LLMs and FinLLMs perform reasonably well on some tasks like sentiment analysis, they struggle with more complex tasks like relation extraction and stock prediction. - Touchstone-GPT shows competitive performance compared to the other models. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Summarization'] | [Link](https://github.com/IDEA-FinAI/Golden-Touchstone) | N/A |
| [Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction](https://arxiv.org/abs/2411.06424) | Adam Mahdi, Harry Mayne, Filip Sondej, Yushi Yang | - This paper investigates the internal mechanisms of Direct Preference Optimization (DPO) for toxicity reduction in language models, challenging the existing explanation that it primarily works by dampening the most toxic neurons. - By ablating toxic neurons and applying activation patching, the study finds that only 31.8% of toxicity reduction stems from dampened toxic neurons. - Instead, DPO reduces toxicity through a more complex process involving multiple neuron groups, accumulating effects by both reducing writing in the toxic direction and actively promoting anti-toxicity in the residual stream. - DPO's adjustments to neuron activations are noisy, with many neurons actually increasing toxicity, suggesting a balancing process between opposing neuron effects to achieve overall toxicity reduction. - The research provides a more nuanced understanding of DPO's mechanism, suggesting potential for targeted interventions to replicate its effects and improve safety in language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Yushi-Y/dpo-toxic-neurons) | N/A |
| [KMM: Key Frame Mask Mamba for Extended Motion Generation](https://arxiv.org/abs/2411.06481) | Feng Chen, Qi Chen, Akide Liu, Zeyu Zhang, Ha0Tang | - This paper introduces Key Frame Mask Mamba (KMM), a novel architecture for extended motion generation that addresses the challenges of memory decay and poor text-motion alignment in previous methods by strategically masking key frames based on local density and minimum distances between high density motion embeddings within the latent space. - Using a customized contrastive learning strategy to learn dynamic text embeddings, the approach improves motion-text alignment by dynamically learning text encodings and enabling the generation of more accurate and aligned motion sequences. - On the BABEL dataset, KMM achieves state-of-the-art performance with a reduction of more than 57% in FID and 70% in parameters compared to existing methods. - A newly introduced benchmark, BABEL-D, focuses on evaluating text-motion alignment for directional instructions, and demonstrates KMM’s superior performance. - User studies further confirmed the efficacy of the proposed KMM model through qualitative and quantitative analysis of the generated motions across a diversity of prompts. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |


## Papers for 2024-11-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Balancing Pipeline Parallelism with Vocabulary Parallelism](https://arxiv.org/abs/2411.05288) | Min Lin, Penghui Qi, Man Tsung Yeung, ufotalent | - This paper introduces Vocabulary Parallelism (VP), a novel technique designed to address computational and memory imbalances stemming from vocabulary layers in pipeline parallelism, a common strategy for training large language models. - VP partitions vocabulary layers across all pipeline devices and integrates the computation as passes within the pipeline schedule, similar to the handling of transformer layers.  The approach involves algorithms to reduce communication barriers within these vocabulary passes, thereby minimizing activation memory overhead. - When combined with memory-balanced pipeline schedules like V-Half, the proposed method achieves near-perfect balance in both memory and computation. - Experimental results show improvements in throughput ranging from 5% to 51% compared to existing methods, particularly under large vocabulary scenarios, with significant reductions in peak memory usage. - Notably, the benefits extend to various vocabulary and model sizes, demonstrating the robustness and generalizability of the approach. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/sail-sg/VocabularyParallelism) | N/A |
| [DELIFT: Data Efficient Language model Instruction Fine Tuning](https://arxiv.org/abs/2411.04425) | Marina Danilevksy, Lucian Popa, Krishna Killamsetty, ishikaa | - DELIFT (Data Efficient Language Model Instruction Fine-Tuning) is a novel algorithm that optimizes data selection across three key stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning. - It utilizes a pairwise utility metric to quantify the value of a data sample in improving the model's responses to other samples and leverages submodular functions for optimal subset selection. - DELIFT reduces fine-tuning data size by up to 70% without compromising performance, leading to significant computational savings.  - It outperforms current data selection techniques by up to 26% across diverse tasks and model scales. - Experiments show minimal performance drops compared to using full datasets and even surpasses full dataset performance in niche tasks such as query rewriting. | ['Natural Language Processing', 'Question Answering'] | [Link](https://anonymous.4open.science/r/optimizing-data-selection-0CD0) | N/A |
| [Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study](https://arxiv.org/abs/2411.02462) | Jingyue Li, andstor | - This paper conducts an empirical study on Parameter-Efficient Fine-Tuning (PEFT) methods for Large Language Models (LLMs) in generating unit tests for code. - It evaluates full fine-tuning, LoRA, (IA)³, and prompt tuning across various open-source LLMs (CodeGen, Code Llama, StarCoder) ranging from 350 million to 16 billion parameters and use well-established unit-test datasets for evaluation. - The findings indicate that LoRA generally performs best, matching or exceeding full fine-tuning's performance in several cases while using fewer parameters, whereas prompt tuning is the most resource-efficient but has more variable performance. - They evaluate using codebleu to evaluate the similarity between the generated tests and the reference tests. -  The results also show that both full fine-tuning and PEFT methods are mostly resistant to catastrophic forgetting, sometimes even improving the code generation capabilities. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/andstor/methods2test_small), [Link](https://huggingface.co/andstor/peft-unit-test-generation-experiments) |
| [LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation](https://arxiv.org/abs/2411.04997) | Yuqing Yang, Xufang Luo, Aoqi Wu, Weiquan Huang, Yif29 | - LLM2CLIP is a novel approach that integrates Large Language Models (LLMs) into Contrastive Language-Image Pre-training (CLIP) to enhance visual representation learning. - It addresses the limitations of LLMs' output features by applying a caption contrastive fine-tuning strategy, which increases their discriminability and enables them to act as a more powerful teacher for CLIP's visual encoder. - This approach improves CLIP's ability to handle longer and more complex captions and incorporate richer knowledge from LLMs, without significant computational overhead. - LLM2CLIP demonstrates significant improvements across various cross-modal tasks, exceeding previous state-of-the-art models like EVA02 by a substantial margin on text and image retrieval benchmarks, and even enabling cross-lingual capabilities for models trained solely on English data. - It also shows promising results in multimodal training with models like LLaVA 1.5, consistently outperforming the original CLIP across several benchmarks. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | N/A | [Link](https://aka.ms/llm2clip) |
| [Improving the detection of technical debt in Java source code with an enriched dataset](https://arxiv.org/abs/2411.05457) | Rick Kazman, Davide Di Ruscio, Phuong T. Nguyen, Anh M. T. Bui, Nam Le Hai | - This paper introduces TESORO, a new dataset for detecting technical debt (TD) in Java source code, which includes both comments and the corresponding source code. - A pipeline is proposed for enriching technical debt data by extracting Self-Admitted Technical Debts (SATD) comments and their associated source code units. - The study demonstrates that incorporating source code context enhances the performance of state-of-the-art SATD detection models, and an ensemble approach combining predictions from different code context lengths yields even better results. - The paper investigates the accuracy of different pre-trained language models (PLMs) in detecting TD solely from source code, revealing the superior performance of CodeBERT and its variant GraphCodeBERT, and highlighting the potential of LLMs in this task. - The curated TESORO dataset is expected to catalyze future research in the domain of TD detection and facilitate the identification of other software artifacts such as code smells. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/NamCyan/tesoro) | N/A |


## Papers for 2024-11-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/abs/2411.04905) | Jiaran Hao, Jason Klein Liu, Tianhao Cheng, Siming Huang, Zenithwang | - OpenCoder, a top-tier code large language model (LLM) designed for code generation, reasoning, and agent systems, is introduced, boasting performance comparable to leading models while offering full transparency through the release of its training data, processing pipeline, and protocols. - OpenCoder's key ingredients for success include code-optimized heuristic rules for data cleaning and deduplication, incorporation of code-related text corpora, and utilization of high-quality synthetic data in annealing and fine-tuning. - The model architecture for OpenCoder is available in 1.5B and 8B parameter sizes, leveraging SwiGLU activation, RoPE positional embedding, and a vocabulary size of 96,640, with variations in layers, attention heads, and context window size between the two. - OpenCoder's training involves a two-stage instruction-tuning process: the first focuses on theoretical computer science question-answer pairs, while the second refines practical coding skills using high-quality code from GitHub. - Evaluation on benchmarks like HumanEval, MBPP, BigCodeBench, LiveCodeBench, and MultiPL-E reveals OpenCoder surpasses all previous fully open models and other open-access models at the 6B+ parameter scale. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/yuxiang630/hqcode) |
| [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965) | Furu Wei, Shuming Ma, Hongyu Wang | - BitNet a4.8 introduces 4-bit activations and a hybrid quantization and sparsification strategy for 1-bit Large Language Models (LLMs), aiming to reduce inference costs while maintaining performance comparable to the 1.58-bit BitNet b1.58 model. - The model employs 4-bit activations for inputs to attention and feed-forward network layers, and sparsifies intermediate states with 8-bit quantization to mitigate quantization errors caused by outlier channels.  - It also incorporates a two-stage training approach (from 8-bit to 4-bit activations) for efficiency.  - Experimental results show BitNet a4.8 achieves comparable performance to BitNet b1.58 with equivalent training costs but faster inference due to enabling INT4/FP4 kernels and supporting 3-bit KV cache. - Additionally, BitNet a4.8 activates only 55% of the parameters, further enhancing efficiency for large-scale LLM deployment and inference. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996) | Ning Dong, Srinivasan Iyer, Liang Luo, Lili Yu, WxWx | - This paper introduces Mixture-of-Transformers (MoT), a sparse multimodal transformer architecture designed to reduce the computational costs of pretraining large multimodal models. - MoT decouples the non-embedding parameters of the model by modality, including feed-forward networks, attention matrices, and layer normalization, enabling modality-specific processing with global self-attention over the full input sequence. - In the Chameleon 7B setting (autoregressive text and image generation), MoT matches the dense baseline performance using only 55.8% of the FLOPs.  With speech added, MoT reaches comparable speech performance using 37.2% of the FLOPs.  - In the Transfusion setting, which uses multi-objective training with autoregressive text and diffusion-based image generation, a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics; a 7B MoT matches the image performance of the dense baseline with one-third of the FLOPs.  - System profiling shows MoT achieves dense baseline image quality in 47.2% of the wall-clock time and text quality in 75.6% of the wall-clock time. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Text Generation', 'Image-to-Text'] | N/A | N/A |
| [TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation](https://arxiv.org/abs/2411.04709) | Yi Yang, Wenhao Wang | - This paper introduces TIP-I2V, a large-scale dataset of over 1.7 million text and image prompts specifically designed for image-to-video generation, accompanied by generated videos from five state-of-the-art models. - The dataset is sourced from Pika Discord channels and includes additional information like UUIDs, timestamps, embeddings, subjects, and NSFW scores. - Analysis reveals that TIP-I2V's prompts, which focus on animating existing image content, differ semantically from those in text-to-video and text-to-image datasets. - This dataset enables research into user preferences, improved model evaluation, misinformation detection, and source image tracing. - Initial benchmarks using TIP-I2V indicate that even early commercial image-to-video models can outperform open-source alternatives in key areas, emphasizing the importance of real-world user data. | ['Image-to-Video', 'Multimodal', 'Computer Vision'] | [Link](https://tip-i2v.github.io) | N/A |
| [Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model](https://arxiv.org/abs/2411.04496) | Ho-Jin Choi, Kyeongjin Oh, Junyoung Youn, Dokyong Lee, Young-Jun Lee | - This paper introduces THANOS, a family of large language models (LLMs) designed to improve the quality of responses generated by conversational agents by infusing them with "skill-of-mind." - Skill-of-mind is a process that involves considering social context, interpreting dialogue situations, planning an appropriate skill strategy, and selecting the most effective conversational skill for a given response.  - The authors also created MULTIFACETED SKILL-OF-MIND, a dataset of 100k conversations annotated with explanations and conversational skills, to train THANOS.  - Experimental results indicate that THANOS effectively predicts conversational skills and enhances response quality in various scenarios, promoting prosocial behavior in human evaluations.  - This improvement is demonstrated by incorporating the generated skill-of-mind as input for LLM-based conversational agents, leading to better response quality. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/passing2961/Thanos) | N/A |
| [DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation](https://arxiv.org/abs/2411.04999) | Chris Paxton, Soumith Chintala, Mohit Warke, Zhanqiu Guo, Peiqi Liu | - DynaMem, a novel dynamic spatio-semantic memory architecture for open-world mobile manipulation, is introduced, enabling robots to adapt to changing environments. - DynaMem maintains and updates a voxelized pointcloud of the environment, incorporating object additions and removals, and supports object localization queries using both Vision-Language Model features and Multimodal Large Language Model question answering. - In real-world experiments on Stretch SE3 robots across various dynamic scenes, DynaMem achieves a 70% pick-and-drop success rate for non-stationary objects, more than double the performance of static systems. - A new dynamic benchmark, DynaBench, is introduced to evaluate dynamic spatio-semantic memory algorithms in 9 changing environments, and ablation studies demonstrate the effectiveness of key design choices. - DynaMem handles object permanence, going beyond existing systems that often return incorrect matches when the queried object is absent. | ['Robotics', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/abs/2411.05000) | Samuel Albanie, Kai Han, Jonathan Roberts | - This paper introduces a new set of retrieval experiments to evaluate the long-context capabilities of Large Language Models (LLMs), called needle threading tasks. - These tasks involve following threads of linked information across different parts of the context and retrieving the final value, including single and multiple needle retrieval, conditional needle retrieval, threading and multi-threading, and branched threading variations. - The study evaluates 17 LLMs on these tasks using synthetically generated key-value pairs of UUIDs and finds that increased context length negatively impacts performance but concurrent threading is largely unaffected by concurrent queries. - It suggests the LLMs' "effective" context limit is shorter than stated due to performance degradation at longer context lengths.   - The paper introduces a task-specific and model-agnostic effective context limit metric and publicly releases the code and experimental data. | ['Question Answering'] | N/A | N/A |
| [RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval](https://arxiv.org/abs/2411.04752) | Subhankar Maity, Aniket Deroy | - This paper introduces RetrieveGPT, a novel approach for enhancing information retrieval from code-mixed conversations, particularly focusing on Roman transliterated Bengali mixed with English. - The approach uses GPT-3.5 Turbo with carefully designed prompts to evaluate document relevance to a given query, considering the sequential nature of conversations. - A mathematical model integrates GPT-3.5 Turbo's output, accounting for sequential dependencies among documents to determine relevance. - The model treats relevance detection as a problem of finding the optimal relevance chain across a sequence of documents. - Experiments on a Facebook dataset with Query Relevance files (QRels) demonstrate the effectiveness of the approach in extracting information from complex, code-mixed conversations. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos](https://arxiv.org/abs/2411.04923) | Eric Xing, Jiale Cao, Wenqi Zhu, Hanan Gani, Shehan Munasinghe | - VideoGLaMM, a large multimodal model designed for pixel-level visual grounding in videos, is introduced. - The model architecture comprises a Large Language Model (LLM), dual vision encoders (for spatial and temporal features), and a spatio-temporal decoder connected via tunable Vision-to-Language (V→L) and Language-to-Vision (L→V) adapters.  - VideoGLaMM is trained on a new dataset of grounded conversation video question-answer triplets which include segmentation masks generated using a semi-automatic annotation pipeline.  - The new dataset includes 38k video-QA triplets, 83k objects and 671k masks.  - Experimental results demonstrate state-of-the-art performance on grounded conversation generation, visual grounding, and referring video segmentation tasks, outperforming existing approaches. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Mask Generation'] | N/A | [Link](https://mbzuai-oryx.github.io/VideoGLaMM) |


## Papers for 2024-11-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level](https://arxiv.org/abs/2411.03562) | Albert Thomas, Giuseppe Paolo, James Doran, Alexandre Maraval, Antoine Grosnit | - Introduced Agent K v1.0, an end-to-end autonomous data science agent capable of automating the entire data science lifecycle, including task setup, solution generation, and submission to Kaggle competitions. - Employs a structured reasoning framework with a memory module for experience-based learning, enabling adaptation without retraining or backpropagation. - Achieved a 92.5% success rate in automating tasks across multiple modalities (tabular, computer vision, NLP, multimodal). - Ranked in the top 38% when compared against almost 6000 human competitors on Kaggle and reached a performance equivalent to Kaggle Grandmaster, winning 6 gold, 3 silver, and 7 bronze medals across diverse challenges. - Proposed a novel evaluation methodology and competitive benchmark using real-world Kaggle competitions to rigorously assess agent capabilities. | ['Natural Language Processing', 'Tabular', 'Computer Vision', 'Multimodal', 'Image Classification', 'Tabular Classification', 'Tabular Regression', 'Time Series Forecasting'] | N/A | N/A |
| [Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination](https://arxiv.org/abs/2411.03823) | Benyou Wang, Lichao Sun, Shunian Chen, Sicheng Lai, Dingjie Song | - This paper introduces MM-Detect, a framework for detecting data contamination in Multimodal Large Language Models (MLLMs). - MM-Detect employs two methods: Option Order Sensitivity Test for multiple-choice questions and Slot Guessing for Perturbation Captions for caption-based questions.  - Experiments on eleven MLLMs and five VQA datasets reveal varying degrees of contamination across models, impacting performance.  - The study also finds that data leakage can originate from both the pre-training phase of the base LLMs and the multimodal fine-tuning phase.  -  The results indicate that MM-Detect can identify contamination and demonstrate that training set leakage leads to performance inflation, creating unfair comparisons. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](https://github.com/MLLM-Data-Contamination/MM-Detect) | N/A |


## Papers for 2024-11-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems](https://arxiv.org/abs/2411.02959) | Weipeng Chen, Mang Wang, Wen Wang, Zhicheng Dou, Jiejun Tan | - HtmlRAG is a novel Retrieval-Augmented Generation (RAG) system that utilizes HTML, instead of plain text, as the format for retrieved knowledge, aiming to preserve structural and semantic information often lost during HTML-to-text conversion. - HtmlRAG incorporates HTML cleaning, compression, and a two-step pruning process, involving an embedding model for coarse-grained pruning and a generative model for fine-grained pruning, to address challenges of long input sequences and noisy content. - This approach outperforms existing RAG systems, utilizing text-based and Markdown-based post-retrieval processes on six QA datasets, including ASQA, HotpotQA, NQ, TriviaQA, MuSiQue, and ELI5 datasets.  - Specifically, it improves exact match by up to 4.5% on the NQ Dataset and 8.7% on the MuSiQue dataset when using Llama 3.1 70B instruct model. - The results demonstrate the effectiveness of utilizing HTML for knowledge modeling in RAG systems, particularly with powerful LLMs capable of handling complex HTML structures. | ['Question Answering'] | [Link](https://github.com/plageon/HtmlRAG) | N/A |
| [LLaMo: Large Language Model-based Molecular Graph Assistant](https://arxiv.org/abs/2411.00871) | Hyunwoo J. Kim, Dohwan Ko, Minseong Bae, Jinyoung Park | - LLaMo, a Large Language Model-based Molecular graph assistant, integrates a molecular graph encoder and a large language model for instruction-following response generation in the molecular domain. - LLaMo uses a multi-level graph projector to abstract representations of each GNN layer and motif representations, bridging the graph encoder and language model. - Machine-generated molecular graph instruction data, created through a multi-turn conversation format from molecular descriptions and IUPAC names, are used for instruction tuning. - Experimental results demonstrate LLaMo's superior performance in molecular description generation, property prediction, and IUPAC name prediction, outperforming LLM-based models like GPT-4. - Ablation studies validate the contribution of the multi-level graph projector and the instruction tuning process. | ['Graph Machine Learning', 'Multimodal', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/mlvlab/LLaMo) | N/A |
| [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | Shenzhi Wang, Yizeng Han, Bingyi Kang, Yulin Wang, Yang Yue | - DeeR-VLA dynamically adjusts the size of activated Multimodal Large Language Models (MLLMs) based on situation complexity, improving computational efficiency for robotic tasks. - DeeR leverages a multi-exit MLLM architecture allowing early termination of processing once sufficient model capacity is reached for a given input, avoiding redundant computation. - The framework includes algorithms to set early-exit criteria based on predefined computational budgets (average/peak FLOPs, GPU memory), enabling adaptability to resource constraints. - A tailored training method integrates temporal information within the multi-exit architecture to ensure reasonable action predictions. - Evaluation on the CALVIN benchmark shows 5.2-6.5x reduction in LLM computational costs and 2-6x reduction in LLM GPU memory usage without compromising task performance. | ['Robotics', 'Multimodal'] | [Link](https://github.com/yueyang130/DeeR-VLA) | N/A |
| [Sample-Efficient Alignment for LLMs](https://arxiv.org/abs/2411.01493) | Min Lin, Wee Sun Lee, Chao Du, Changyu Chen, Zichen Liu | - This paper introduces SEA (Sample-Efficient Alignment), a Thompson sampling-based algorithm, for aligning Large Language Models (LLMs) with human preferences efficiently, addressing the bottleneck of extensive human feedback requirements in current alignment methods. - The approach frames LLM alignment as a contextual dueling bandit problem and emphasizes two key properties for sample efficiency: online interaction and active exploration. - SEA leverages an epistemic reward model (deep ensemble of reward models) for posterior sampling, policy-guided search for efficient response selection, and mixed preference learning (combining online user feedback and synthetic feedback from the reward model) to update the LLM policy online. - Experimental results across various model scales (1B, 2.8B, 6.9B parameters) and direct preference optimization methods (DPO, IPO, SLiC) show SEA achieves higher win rates against reference responses and significantly better sample efficiency compared to existing baselines, including passive online learning and other active exploration methods.  - The authors release `oat`, an open-source, distributed learning system designed for online LLM alignment research, aiming to facilitate further studies and fair comparisons in the field. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/sail-sg/oat) | [Link](https://huggingface.co/docs/trl/main/en/online_dpo_trainer), [Link](https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B) |
| [Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge](https://arxiv.org/abs/2411.02657) | Lashaw Salta, Chinmay Agrawal, Catalina Villouta, Andrew Langdon, ksoman | - Zebra-Llama, a context-aware large language model specializing in Ehlers-Danlos Syndrome (EDS) information, was developed using a novel context-aware fine-tuning methodology. - The model leverages Retrieval-Augmented Generation (RAG) and is trained on a diverse dataset comprising medical literature, patient forums, and clinical resources, structured as question-context-answer triplets. - Evaluation on real-world questions from EDS patients and clinicians demonstrated Zebra-Llama's superior performance compared to the base Llama model across thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%), and citation reliability (70.6% vs. 52.3%). - A custom RAG API and Jupyter Notebook demo are also released. - The model and code are open-sourced to democratize expert-level knowledge in rare disease management. | ['Question Answering', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/karthiksoman/zebra-llama) | [Link](https://huggingface.co/zebraLLAMA/zebra-Llama-v0.2) |
| [Controlling Language and Diffusion Models by Transporting Activations](https://arxiv.org/abs/2410.23054) | Nicholas Apostoloff, Luca Zappella, Michal Klein, Arno Blaas, Pau Rodriguez | - This paper introduces Activation Transport (ACT), a framework to steer activations in generative models (GMs) using optimal transport theory. - ACT generalizes existing activation steering methods by applying univariate maps to activations while preserving target distributions, improving controllability and robustness.  - Linear-ACT, an inference-time intervention based on ACT, matches or outperforms other methods in toxicity mitigation, concept induction, and truthfulness in LLMs. - ACT effectively controls text-to-image diffusion models for fine-grained style control and concept negation.  - The authors adapt ITI (Li et al., 2024) for text-to-image and find that ACT with a strength parameter of 1 consistently achieves strong conditioning across tasks and models. | ['Natural Language Processing', 'Text-to-Image', 'Text Generation'] | N/A | N/A |


## Papers for 2024-11-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models](https://arxiv.org/abs/2411.00836) | Bin Hu, Junyu Zhang, Xingang Guo, Chengke Zou, Ray2333 | - DYNAMATH, a dynamic visual benchmark, is introduced to evaluate the robustness of Vision Language Models (VLMs) in mathematical reasoning. - The benchmark consists of 501 seed questions represented as Python programs, enabling automatic generation of diverse concrete questions with variations in visual and textual content. - An evaluation of 14 state-of-the-art VLMs on 5,010 generated questions revealed a significant gap between average-case and worst-case accuracy, indicating current VLMs' lack of robustness in handling question variations. - The analysis also found high repetition consistency in many models, suggesting that incorrect answers on certain variants are due to consistent errors rather than inherent randomness. - DYNAMATH provides insights to guide development of more robust VLMs and the paper suggests using adversarial training or reinforcement learning from human feedback with fine-grained process rewards as potential improvement strategies. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265) | Jiaqi Zhu, Xingwu Sun, Ruobing-Xie, Mimosa77, YanfengChen | - Tencent introduces Hunyuan-Large, a 389 billion parameter (52 billion activated) open-source Mixture-of-Experts (MoE) model based on the Transformer architecture and capable of handling up to 256K tokens. - The model outperforms LLama3.1-70B on various benchmarks, including language understanding, generation, logical reasoning, mathematics, coding, and long-context tasks, and exhibits performance comparable to the much larger LLama3.1-405B model. - Key innovations include using large-scale synthetic data, a mixed expert routing strategy combining shared and specialized experts with recycle routing for discarded tokens, KV cache compression by grouped-query attention and cross-layer attention, and an expert-specific learning rate scaling strategy. - The model is pre-trained on 7 trillion tokens, including 1.5 trillion synthetic tokens, followed by post-training stages involving supervised fine-tuning and reinforcement learning from human feedback using direct preference optimization. - Both pre-trained and post-trained versions of Hunyuan-Large are released to the open-source community. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Tencent/Tencent-Hunyuan-Large) | [Link](https://huggingface.co/tencent/Tencent-Hunyuan-Large) |
| [Survey of Cultural Awareness in Language Models: Text and Beyond](https://arxiv.org/abs/2411.00860) | Junho Myung, Arnav Arora, Junyeong Park, jinjh0123, sidicity | - This paper surveys efforts to incorporate cultural awareness into text-based and multimodal large language models (LLMs). - It defines cultural awareness in LLMs based on definitions from psychology and anthropology and examines methodologies for creating cross-cultural datasets and benchmarks, strategies for cultural inclusion in downstream tasks, and benchmarks for evaluating cultural awareness in LLMs. - The survey also discusses ethical implications of cultural alignment, the role of Human-Computer Interaction, and cultural alignment's role in social science research. - The paper identifies research gaps in current literature and provides suggestions for future research in areas such as cross-cultural LLMs and automatic context detection. - It organizes and compares efforts in incorporating culture into NLP and spans several modalities like image, video, audio and text. | ['Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models](https://arxiv.org/abs/2411.00918) | Quang Pham, Van Nguyen, Luong Tran, doantienthongbku, DavidNguyen | - This paper introduces LibMoE, a comprehensive and modular framework designed to streamline the research, training, and evaluation of Mixture of Experts (MoE) algorithms in Large Language Models (LLMs). - LibMoE facilitates easier access to MoE research for a wider range of researchers by standardizing the training and evaluation process, and by reducing the computational cost via sparse upcycling from pre-trained LLMs. - The authors benchmark five state-of-the-art MoE algorithms with three model configurations across eleven datasets under a zero-shot setting. - Results show that all MoE algorithms achieve roughly similar performance when averaged across a variety of tasks. - Further analysis suggests the potential benefits of early stopping and the importance of balanced expert utilization in MoE models. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/abs/2411.02335) | Chaojun Xiao, Yingfa Chen, Chenyang Song, Yuqi Luo, SillyXu | - This paper investigates activation sparsity in Large Language Models (LLMs), proposing a new metric called PPL-p% sparsity. - PPL-p% sparsity is performance-aware, versatile across activation functions, and precisely identifies weakly contributing neurons, improving upon existing metrics like CETT. - Through extensive experiments, the research reveals scaling laws relating activation sparsity to training data, activation function, width-depth ratio, and parameter scale. - ReLU activation is found to be superior to SiLU due to greater sparsity and comparable performance, with deeper models exhibiting higher sparsity below a certain bottleneck. - Notably, the limit of activation sparsity shows weak correlation with parameter scale, suggesting that activation patterns in LLMs are scale-insensitive. | ['Natural Language Processing'] | [Link](https://github.com/thunlp/SparsingLaw) | N/A |
| [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747) | Ryan A. Rossi, Seunghyun Yoon, Viet Dac Lai, Dang Nguyen, Franck-Dernoncourt | - DynaSaur is a novel LLM agent framework that dynamically creates and composes actions, represented as Python functions, enabling the agent to operate beyond a predefined action set. - At each step, the agent generates Python code to perform an action, accumulating these generated actions for reuse in future steps, enhancing flexibility and efficiency. - This framework outperforms existing methods on the GAIA benchmark, demonstrating its effectiveness in complex, long-horizon tasks. - Notably, DynaSaur allows the agent to recover from scenarios where the predefined action set is insufficient or existing actions fail due to unforeseen circumstances. - The dynamic action creation and accumulation capabilities enable the LLM agent to interact with various tools and systems, enhancing its ability to solve a diverse range of tasks. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/adobe-research/dynasaur) | N/A |
| [Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models](https://arxiv.org/abs/2411.00743) | Virginia Smith, Mona Diab, Aashiq Muhamed | - Introduces Specialized Sparse Autoencoders (SSAEs), which are designed to capture rare or infrequent features (tail concepts) within specific domains of foundation models. - Employs dense retrieval and TracIn reranking as effective methods for selecting training data relevant to the target domain, enabling targeted feature extraction without needing to scale to billions of features. - Utilizes Tilted Empirical Risk Minimization (TERM) as a training objective, demonstrating its effectiveness in enhancing tail concept representation in SSAEs compared to standard Empirical Risk Minimization (ERM). - Demonstrates through experiments on the Bias in Bios dataset that SSAEs improve interpretability by capturing rare features and significantly increase worst-group classification accuracy (12.5%) when used to remove spurious gender information. - Evaluation on downstream perplexity and Lo sparsity metrics shows SSAEs effectively capture domain-specific tail concepts, outperforming standard SAEs trained on general-purpose data. | ['Natural Language Processing', 'Feature Extraction'] | N/A | N/A |
| [Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks](https://arxiv.org/abs/2411.01192) | Muhammad Abdul-Mageed, Fakhraddin Alwajih, Abdellah El Mekki, El Moatez Billah Nagoudi, Gagan Bhatia | - This paper introduces Swan, a family of dialect-aware, Arabic-centric, cross-lingual, and cross-cultural embedding models. - Swan includes Swan-Small, based on ARBERTv2, and Swan-Large, based on the pretrained Arabic large language model ArMistral. - A new comprehensive benchmark suite, ArabicMTEB, is proposed to evaluate the models, covering eight tasks and 94 datasets, including cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance. - Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks while maintaining monetary efficiency. - Swan-Small also shows strong performance, consistently surpassing Multilingual-E5-base on most Arabic tasks. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | N/A | N/A |


## Papers for 2024-11-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OS-ATLAS: A Foundation Action Model for Generalist GUI Agents](https://arxiv.org/abs/2410.23218) | Fangzhi Xu, Zhenyu Wu, Zhiyong Wu, heroding77, QiushiSun | - This paper introduces OS-Atlas, a large action model designed for generalist GUI agents, focusing on GUI grounding and out-of-distribution (OOD) generalization. - It leverages a novel multi-platform data synthesis toolkit, enabling the creation of a 13 million GUI element dataset spanning Windows, macOS, Linux, Android, and web interfaces. - OS-Atlas employs a two-stage training process: GUI grounding pre-training on the large dataset and action fine-tuning on existing agent datasets with a unified action space to mitigate conflicts. - Evaluations across six benchmarks and three platforms (mobile, desktop, web) show significant performance improvements over state-of-the-art models. - OS-Atlas-Base, the pre-trained model, serves as an open-source alternative to commercial VLMs for building GUI agents, achieving comparable performance in some settings. | ['Multimodal'] | N/A | N/A |
| [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/abs/2411.00412) | Leon Bergen, Duncan Watson-Parris, Yadi Cao, yuqirose, Bohan22 | - This paper introduces a novel two-stage training method called Adapting While Learning (AWL) to enhance Large Language Models (LLMs) for solving scientific problems by incorporating tool usage and direct reasoning. - AWL consists of World Knowledge Distillation (WKD) which fine-tunes LLMs to internalize domain knowledge from solutions generated using tools and Tool Usage Adaptation (TUA) which trains LLMs to choose between direct answering and tool usage based on problem complexity. - Evaluation across six scientific benchmarks demonstrate average improvements of 28.18% in answer accuracy and 13.89% in tool usage precision compared to baselines and state-of-the-art models such as GPT-4 and Claude-3.5. - The model surpasses existing approaches on custom datasets that include complex and specialized scientific questions not commonly seen during pre-training. - It also showcases improved robustness in noisy data scenarios and adaptability to open-ended questions through integration with preference learning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027) | Yijia Shao, Branislav Kveton, Ryan A. Rossi, Zhehao Zhang, Franck-Dernoncourt | - This survey paper provides a comprehensive overview of personalized Large Language Models (LLMs), unifying research on personalized text generation and downstream task personalization. - It introduces a taxonomy for personalized LLM usage, formalizing foundations, and analyzing personalization granularity (user-level, persona-level, global preference). - The paper surveys techniques for personalization, including retrieval-augmented generation, prompting, representation learning, and reinforcement learning from human feedback (RLHF). - It also covers evaluation metrics and datasets for personalized LLMs, along with various applications like AI assistants, recommendation systems, and search engines. - Finally, it discusses open problems such as benchmarks, cold-start issues, bias, privacy, and multimodality in personalized LLMs. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation](https://arxiv.org/abs/2410.21157) | Shukai Liu, Jian Yang, Congnan Liu, Ken Deng, Jiaheng Liu | - This paper introduces M2RC-EVAL, a massively multilingual repository-level code completion benchmark encompassing 18 programming languages. - It offers two types of fine-grained annotations: bucket-level (based on abstract syntax tree depth) and semantic-level (categorizing completion scenarios). - The authors also present M2RC-INSTRUCT, a multilingual instruction dataset designed to improve repository-level code completion abilities in LLMs. - Experimental results show that incorporating cross-file context and fine-tuning on M2RC-INSTRUCT significantly enhances performance across various languages. - Code Llama with fine-tuning outperforms non-finetuned StarCoder after fine-tuning.  | ['Natural Language Processing', 'Text2Text Generation', 'Text Generation'] | [Link](https://github.com/M2RC-Eval-Team/M2RC-Eval) | N/A |
| [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030) | Pierre-François Marteau, Nicolas Béchet, Danrun Cao | - This paper introduces WikiNER-fr-gold, a manually revised gold-standard version of the French portion of the WikiNER corpus, aimed at improving the quality of Named Entity Recognition (NER) resources. - The corpus consists of 20% of the original WikiNER-fr (26,818 sentences, ~700k tokens), randomly sampled and corrected for inconsistencies in annotation stemming from the semi-supervised nature of the original WikiNER. - The correction process focused on standardizing entity boundaries and categories, resolving ambiguous hyperlinks, and addressing inconsistencies in the application of annotation guidelines. - The authors analyzed the errors in the silver-standard WikiNER-fr, categorized them, and described the correction strategies employed. - The paper also discusses future work, including a broader assessment of entity categorization and potential automation of the correction process for the remaining WikiNER data. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |
| [GRS-QA -- Graph Reasoning-Structured Question Answering Dataset](https://arxiv.org/abs/2411.00369) | Jincen Shuai, Devasha Trivedi, Anish Pahilajani, Franck-Dernoncourt, namyongp | - This paper introduces GRS-QA, a new multi-hop question answering dataset that includes explicit reasoning structures in the form of graphs for enhanced reasoning analysis of LLMs. - Unlike existing datasets that lack clear reasoning pathways, GRS-QA captures intricate structures by constructing reasoning graphs, where nodes denote textual contexts and edges signify logical flow. - GRS-QA offers benefits such as providing transparent reasoning steps for answer derivation, allowing fine-grained evaluation of LLM reasoning capabilities across diverse structures. - It also includes negative reasoning graphs, created by perturbing the structure of positive graphs, to isolate the impact of reasoning structures compared to content on question answering performance. - The authors benchmark state-of-the-art models on GRS-QA from retrieval, direct question answering, and retrieval-augmented generation perspectives, revealing that LLM performance degrades with increasing reasoning complexity and highlighting the importance of GRS-QA in pushing the limits of current QA models. | ['Question Answering', 'Natural Language Processing', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-11-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743) | Tianyi Zhou, Yanhong Li, MingLiiii | - This research paper investigates the layer-wise gradient patterns in Large Language Models (LLMs) during instruction-tuning with different reasoning approaches (fast vs. slow thinking) and response types. - It uses spectral analysis, specifically Singular Value Decomposition (SVD) and nuclear norm, to characterize gradient behaviors across LLM layers for tasks involving math, commonsense reasoning, and knowledge learning. - Slow thinking, using detailed Chain-of-Thought (CoT), results in more stable and uniform gradient norms across layers compared to fast thinking, suggesting improved learning stability. - The gradients associated with slow thinking effectively differentiate correct from irrelevant responses in reasoning tasks, while in knowledge learning tasks, gradient norms are sensitive to knowledge popularity but not correctness.  - The study also finds that instruction-tuned LLMs do not show significant advantages over pre-trained LLMs in identifying incorrect reasoning and have different gradient patterns for fast thinking responses, suggesting challenges in aligning with the instruction-tuning data. | ['Natural Language Processing'] | [Link](https://github.com/MingLiiii/Layer_Gradient) | N/A |
| [A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents](https://arxiv.org/abs/2410.22476) | Pawan Goyal, Gajula Sai Chaitanya, Abhilash Nandy, Sombit Bose, Ankan Mullick | - This paper introduces MLMCID, a pointer network-based architecture for joint extraction and detection of multi-label multi-class intents in task-oriented dialogue systems. - The MLMCID model uses an encoder-decoder framework with a pointer network and LSTM-based sequence generator to identify multiple intent spans within a sentence, along with their corresponding coarse- and fine-grained intent labels. - A new multilingual multi-label intent dataset (MLMCID-dataset) is also created from existing benchmark datasets.  - The model outperforms baseline approaches, including large language models (LLMs) like Llama2 and GPT, on various MLMCID datasets in terms of accuracy and F1-score. -  The approach is also effective in few-shot settings and demonstrates the importance of multi-intent modeling for real-world conversational AI. | ['Natural Language Processing', 'Text Classification', 'Question Answering'] | [Link](https://github.com/ankan2/multi-intent-pointer-network) | N/A |
| [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175) | Lei Hou, Bin Xu, Xiaozhi Wang, Hao Peng, Yunjia Qi | - This paper introduces constraint back-translation, a novel data generation technique for improving complex instruction following in Large Language Models (LLMs). - The technique involves taking existing instruction-response pairs and using an LLM to generate constraints that are already implicitly satisfied by the response.  - This method is used to create CRAB, a high-quality complex instruction-response dataset. - The method improves the performance of LLMs on complex instruction-following tasks, as measured by IFEval and FollowBench benchmarks. - It also serves as a useful auxiliary training objective during post-training by enhancing the model's understanding of complex constraints. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Language Models can Self-Lengthen to Generate Long Texts](https://arxiv.org/abs/2410.23933) | Dayiheng Liu, An Yang, Bowen Yu, Tianyi Tang, Shanghaoran Quan | - This paper introduces Self-Lengthen, an iterative training framework to improve the long text generation capabilities of Large Language Models (LLMs). - Self-Lengthen employs two roles: a Generator to produce initial responses and an Extender to lengthen these responses iteratively. - This method leverages the intrinsic knowledge of LLMs without needing additional data or proprietary models, addressing the training gap in current LLMs for long text generation. - Experimental results on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long text generation using open-source LLMs like Qwen2 and LLaMA3. - Notably, Self-Lengthen enhances the output length while preserving the quality, boosting output from 1,000 words to 8,000 in Qwen2.5. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/QwenLM/Self-Lengthen) | N/A |
| [BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays](https://arxiv.org/abs/2410.21969) | Xinxing Xu, Sicong Leng, Yanyu Xu, Tan Li Hui Faith, youngzhou12 | - BenchX, a unified benchmark framework, is proposed for evaluating Medical Vision-Language Pretraining (MedVLP) methods on chest X-rays, enabling head-to-head comparison and systematic analysis. - BenchX comprises three components: comprehensive datasets covering nine datasets and four medical tasks, benchmark suites to standardize data preprocessing and experimental setups, and unified fine-tuning protocols that accommodate heterogeneous MedVLP methods. - Baselines for nine state-of-the-art MedVLP methods are established using BenchX, revealing that some early methods can outperform recent ones with proper training strategies. - In particular, MGCA and MRM consistently demonstrate strong performance in most cases.  - MedCLIP-ViT also delivers good performance on multi-label image classification tasks. | ['Computer Vision', 'Image Classification', 'Image Segmentation', 'Image-to-Text', 'Multimodal'] | [Link](https://github.com/yangzhou12/BenchX) | N/A |
| [BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments](https://arxiv.org/abs/2410.23918) | Yunhua Zhou, Dong Zhang, Bo Wang, Pengyu Wang, Xinghao Wang | BitStack is a novel, training-free weight compression approach for large language models (LLMs) that enables megabyte-level trade-offs between memory usage and model performance.  It achieves this through iterative weight decomposition and considers parameter significance, resulting in approximately 1-bit per parameter residual blocks.  These blocks are sorted and stacked for dynamic loading based on memory availability.  Extensive experiments show BitStack matches or surpasses existing quantization baselines across various tasks, particularly at extreme compression ratios. | ['Natural Language Processing'] | [Link](https://github.com/xinghaow99/BitStack) | N/A |
| [Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks](https://arxiv.org/abs/2410.24032) | Qingwei Lin, Jue Zhang, Zhiyang Zhang, Xiaoting Qin, Yingzhe Peng | The paper introduces CARE, a collaborative chatbot system for personalized exploratory tasks that combines a multi-agent LLM framework with a structured UI.  CARE addresses limitations of existing LLM chatbots by extracting both explicit and implicit user needs through iterative query refinement and dynamic solution generation. A within-subject user study with 22 participants showed CARE was consistently preferred over a baseline LLM chatbot, reducing cognitive load and inspiring creativity.  CARE transforms LLM-based systems from passive information retrievers into proactive partners in personalized problem-solving and exploration. The study also revealed CARE's impact on facilitating better user experiences in complex tasks, by improving solution comprehensiveness and personalization. | ['Natural Language Processing'] | [Link](null) | [Link](null) |


## Papers for 2024-10-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation](https://arxiv.org/abs/2410.23090) | Hongjin Qian, Ziliang Zhao, Kelong Mao, dongguanting, ariya2357 | - This paper introduces CORAL, a large-scale benchmark designed to evaluate Retrieval-Augmented Generation (RAG) systems in multi-turn conversational settings. - CORAL is derived from Wikipedia, containing 8,000 information-seeking conversations covering various topics with citation labels. - It includes three tasks: passage retrieval, response generation, and citation labeling and proposes a framework to standardize different RAG methods. - Evaluations show that fine-tuned open-source LLMs outperform commercial closed-source LLMs in retrieval, and that input length reduction maintains response quality and improves citation accuracy. - The benchmark addresses challenges in multi-turn conversational RAG, such as redundant information and topic shifts, paving the way for evaluating and improving multi-turn conversational RAG systems. | ['Question Answering'] | [Link](https://github.com/Ariya12138/CORAL) | N/A |
| [Stealing User Prompts from Mixture of Experts](https://arxiv.org/abs/2410.22884) | Nicholas Carlini, Jamie Hayes, Ilia Shumailov, Itay Yona | - This paper introduces a novel attack, called MoE Tiebreak Leakage, which exploits a vulnerability in Mixture-of-Experts (MoE) models to extract user prompts. - The attack leverages the Expert-Choice-Routing (ECR) strategy, manipulating the order of inputs within a batch to cause predictable token dropping, thereby revealing information about a victim's prompt. - The authors successfully demonstrate the attack on a two-layer Mixtral model, extracting almost all secret messages (996/1000) across varying lengths (1-11 tokens), and achieving 99.9% success in recovering individual tokens (4833/4838). - The attack's complexity is O(VM²) for the number of queries to the target model and O(2DNVM²) for queries to a local model copy, where V is vocabulary size, M is prompt length, D is the number of layers, and N is the number of experts. - The paper discusses potential mitigations, including preserving in-batch data independence and introducing stochasticity into the model's routing or capacity parameters. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels](https://arxiv.org/abs/2410.20050) | Xiao Zhou, Xiangxu Zhang, Lei Li, zl101 | - This paper introduces SL-HyDE (Self-Learning Hypothetical Document Embeddings), a novel approach for zero-shot medical information retrieval that eliminates the need for labeled data. - SL-HyDE leverages LLMs to generate hypothetical documents from user queries, and utilizes a retriever to find relevant documents based on these hypothetical documents. - It employs a self-learning mechanism to enhance both LLM document generation and retriever performance without relying on labeled medical data. - A new benchmark for Chinese Medical Information Retrieval (CMIRB) is introduced, consisting of five tasks and ten datasets derived from real-world scenarios. - Experimental results on CMIRB show SL-HyDE surpasses HYDE by 4.9% in NDCG@10 and demonstrates performance gains across various LLM and retriever combinations. | ['Question Answering'] | [Link](https://github.com/CMIRB-benchmark/CMIRB) | N/A |
| [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://arxiv.org/abs/2410.23168) | Jan Eric Lenssen, Yongqin Xian, Muhammad Ferjad Naeem, Yue Fan, Haiyang Wang | - TokenFormer, a novel fully attention-driven neural network architecture, is introduced, which treats model parameters as tokens, enhancing flexibility in token-parameter computations. - By utilizing a cross-attention mechanism between input tokens and learnable parameter tokens, TokenFormer allows for scaling model parameters without altering input or output dimensions, enabling progressive scaling by adding new key-value parameter pairs. - This approach facilitates efficient scaling by reusing pre-trained models, thereby significantly reducing training costs compared to training large transformer models from scratch. - Experimental results demonstrate that TokenFormer achieves comparable perplexity to Transformers trained from scratch on language modeling tasks while substantially reducing the training budget, and maintains similar performance in visual modeling and zero-shot classification tasks. - TokenFormer offers controllable computational costs for long-context modeling, preserves learned distributions during scaling, and shows potential for integration into Mixture-of-Experts frameworks and parameter-efficient tuning strategies. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Zero-Shot Classification'] | [Link](https://github.com/Haiyang-W/TokenFormer) | N/A |


## Papers for 2024-10-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CLEAR: Character Unlearning in Textual and Visual Modalities](https://arxiv.org/abs/2410.18057) | Denis Bobkov, Boris Mikheev, Alexey Zhavoronkin, Dmitrii Korzh, therem | - Introduces CLEAR, a multimodal benchmark for evaluating machine unlearning (MU) in textual and visual modalities, focusing on removing information about specific individuals. - The benchmark includes a synthetic dataset of 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs, along with real-world face and visual question answering datasets for evaluating model retention. - Evaluates 10 existing MU methods adapted for multimodal unlearning, revealing that current state-of-the-art algorithms struggle in multimodal settings. - Demonstrates that simple L1 regularization on LoRA adapter weights during unlearning significantly mitigates catastrophic forgetting, improving the preservation of model performance on retained data. - Makes the dataset publicly available to encourage further research in the field. | ['Multimodal', 'Computer Vision', 'Natural Language Processing', 'Image-to-Text', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/therem/CLEAR) |
| [SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization](https://arxiv.org/abs/2410.21411) | Chuang Gan, Donglai Wei, Jiawei Zhou, zmeng0116, EthanTaylor | - SocialGPT is a novel modular framework that leverages Vision Foundation Models (VFMs) and Large Language Models (LLMs) for social relation reasoning. - It employs VFMs to translate image content into a textual "social story" and utilizes LLMs for reasoning based on the generated story and provided bounding boxes. - This framework incorporates systematic design principles to enhance the collaboration between VFMs and LLMs, including comprehensive and domain-specific visual information extraction and a structured reasoning prompt named SocialPrompt. - SocialGPT achieves competitive zero-shot performance on PIPA and PISC datasets, outperforming previous state-of-the-art supervised methods on PIPA by 1.4%. - The framework also introduces Greedy Segment Prompt Optimization (GSPO) for automatic prompt tuning, resulting in significant performance improvements across various LLMs. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Classification'] | [Link](https://github.com/Mengzibin/SocialGPT) | N/A |
| [OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization](https://arxiv.org/abs/2410.19609) | Hongming Zhang, Wenhao Yu, Kaixin Ma, Wenlin Yao, Hongliang He | - This paper introduces OpenWebVoyager, an open-source framework for building multimodal web agents that can explore real-world websites, receive feedback, and iteratively optimize their performance. - The agent architecture adapts the Idefics2-8b-instruct model, processing observations consisting of webpage screenshots and accessibility trees. -  OpenWebVoyager combines imitation learning from a GPT-40 powered web agent with an exploration-feedback-optimization cycle, where GPT-40 evaluates the agent's trajectory success. - Across multiple iterations on the WebVoyager and Mind2Web datasets, the agent shows improvement in task success rate, starting from 19.9% to 25.8% on the WebVoyager test set and 6.3% to 19.6% on the Mind2Web cross-task test set. - The results indicate that the iterative real-world exploration and optimization method is an effective way to improve the agent's real-world performance. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/MinorJerry/OpenWebVoyager) | N/A |
| [Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning](https://arxiv.org/abs/2410.22304) | Paul Mineiro, ydeng9 | - This paper introduces Flow-DPO, a novel approach to improve Large Language Model (LLM) mathematical reasoning by generating high-quality reasoning traces through online multi-agent learning flows. - The method employs an incremental output production flow composed of multiple LLMs that iteratively communicate to construct solutions, similar to a multi-agent system. - The flow is trained using online Direct Preference Optimization (DPO) with rollouts, generating DPO pairs for each training example during answer chunk generation and updating the models in real-time. - Experimental results on MetaMath, GSM8K, and MATH datasets demonstrate that Flow-DPO generates higher-quality reasoning traces compared to direct model inference, leading to improved performance in mathematical reasoning tasks after supervised fine-tuning. - This improvement is particularly significant for the Llama-3-8B-instruct model, achieving a 20% improvement in validation accuracy on mathematical reasoning tasks during training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465) | Ningxin Zheng, Size Zheng, Wenlei Bao, Li-Wen Chang, preminstrel | - SHADOWKV is a novel LLM inference system designed for enhanced throughput in long-context scenarios by storing a low-rank representation of the key cache on the GPU and offloading the value cache to the CPU. - It employs a precise KV selection strategy during decoding, utilizing landmarks and static outliers to minimize the sparse KV cache budget while maintaining accuracy. - Evaluations on benchmarks like RULER, LongBench, and Needle in a Haystack with various LLMs (Llama, GLM, Yi, Phi, Qwen) show that SHADOWKV can handle contexts up to 1M tokens. - It achieves up to a 6x increase in batch size and a 3.04x boost in throughput compared to full attention on an A100 GPU. - SHADOWKV's performance even surpasses the theoretical throughput of infinite batch size with full attention, assuming infinite GPU memory. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/bytedance/ShadowKV) | N/A |


## Papers for 2024-10-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation](https://arxiv.org/abs/2410.18565) | Remek, adgw, djstrong, lflis, chrisociepa | - This paper introduces Bielik 7B v0.1, a 7-billion parameter generative text model based on the Mistral 7B v0.1 architecture and trained on a curated Polish corpora. - The model utilizes techniques such as Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, and incorporates architectural features like Sliding Window Attention and SwiGLU activation function for enhanced performance. - To evaluate the model, new benchmark frameworks, the Open PL LLM Leaderboard and Polish MT-Bench, were created for assessing NLP tasks and conversational abilities. - Bielik 7B v0.1 showed a significant improvement of 9 percentage points in the RAG Reader task compared to Mistral-7B-v0.1. - In subjective conversational evaluations, Bielik outperformed models with higher average scores on the Open PL LLM Leaderboard benchmarks. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/spaces/speakleash/mt-bench-pl), [Link](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), [Link](https://huggingface.co/datasets/teknium/OpenHermes-2.5), [Link](https://huggingface.co/spaces/speakleash/open_pl_llm_leaderboard) |
| [AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant](https://arxiv.org/abs/2410.18603) | Fangzhi Xu, Qiushi Sun, Zhuohang Dang, Minnan Luo, Chengyou Jia | - AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents, has been introduced for automating diverse computer tasks. - It leverages a novel MetaAgent with an AgentToken strategy, enabling efficient management of diverse agents by representing each agent as a learnable token embedding and predicting the appropriate token(s) for task execution. - AgentStore allows for seamless third-party agent integration, enabling adaptability to evolving operating systems. - Evaluation on OSWorld and a mobile environment demonstrate its ability to improve performance in automating computer tasks, achieving a success rate of 23.85% on OSWorld—more than double the previous best (11.21%). - AgentStore's ability to integrate agents and specialize them for specific tasks while maintaining general capabilities demonstrates significant improvement over single generalist or specialized agents in handling complex tasks within real-world OS environments. | ['Multimodal'] | N/A | N/A |
| [GPT-4o System Card](https://arxiv.org/abs/2410.21276) | Adam Perelman, Adam P. Goucher, Adam Lerer, Aaron Hurst, OpenAI |  - OpenAI's GPT-40 is an "omni" autoregressive model that accepts and generates combinations of text, audio, image, and video, trained end-to-end across these modalities. - GPT-40 matches GPT-4 Turbo's performance on English text and code, surpasses it in non-English languages, and demonstrates significant improvement on vision and audio understanding. - The model's training data includes publicly available data, code and math data, multimodal data (images, audio, and video), and proprietary data from partnerships, with a cutoff date of October 2023. - Prior to deployment, OpenAI performed risk assessments and mitigations with methods including safety classifiers, content filtering, and preference alignment to reduce harms such as information hazards, bias, and policy violations. - Deployment preparation encompassed a four-phased external red teaming process with over 100 participants to evaluate risks and test mitigations across multiple modalities and potential harms such as disallowed content and misinformation. | ['Multimodal', 'Any-to-Any', 'Audio', 'Automatic Speech Recognition', 'Text-to-Speech', 'Text-to-Audio', 'Computer Vision', 'Image-to-Text', 'Image Classification', 'Object Detection', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169) | Zhengren Wang, Junyuan Zhang, Bin Wang, Victor Shea-Jay Huang, Qintong Zhang |  - This survey paper provides a comprehensive overview of document parsing, consolidating recent advancements in modular pipeline systems and end-to-end models driven by large vision-language models (VLMs) and covering key methodologies, challenges, and future research directions. - The paper discusses core document parsing components, including layout detection, content extraction (text, tables, mathematical expressions), and multimodal data integration, examining algorithms for each stage. - It addresses the challenges faced by modular document parsing systems and VLMs in handling complex layouts, integrating modules, and recognizing high-density text. - The survey consolidates widely used datasets and evaluation metrics for document parsing tasks, providing valuable resources for researchers and practitioners. - Finally, the paper emphasizes the importance of developing larger, more diverse datasets and outlines future research directions in the field, such as handling complex layouts and improving OCR for densely packed text. | ['Natural Language Processing', 'Document Question Answering', 'Computer Vision', 'Object Detection'] | N/A | N/A |
| [LongReward: Improving Long-context Large Language Models with AI Feedback](https://arxiv.org/abs/2410.21252) | Zhenyu Hou, Shulin Cao, Xin Lv, Zhongni Hou, Jiajie Zhang | - LongReward, a novel method to improve long-context large language models (LLMs) using AI feedback, is introduced. - It uses an off-the-shelf LLM to assign rewards to model responses based on four dimensions: helpfulness, logicality, faithfulness, and completeness. - When combined with the reinforcement learning algorithm Direct Preference Optimization (DPO), LongReward significantly boosts the performance of long-context SFT models, outperforming baseline methods. - Experiments show improvements on long-context question answering and summarization and a positive impact on short instruction following. - LongReward enhances model capabilities by mitigating common issues like hallucinations and ineffective context utilization in long-context scenarios. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/THUDM/LongReward) | N/A |
| [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011) | Samyadeep Basu, Yu Xia, Ryan Aponte, Xuan Shen, Chien Van Nguyen | - This paper presents a comprehensive survey of Small Language Models (SLMs), focusing on architectures, training techniques, and model compression methods. - The authors introduce a novel taxonomy to categorize SLM optimization methods, considering techniques used in pre-processing, training, post-processing, and the constraints being optimized (e.g., inference compute, training time). - The survey covers lightweight architectures, efficient self-attention approximations, neural architecture search for model building, efficient pre-training and fine-tuning strategies, and model compression techniques like pruning, quantization, and knowledge distillation. - Additionally, it summarizes benchmark datasets and evaluation metrics commonly used for assessing SLM performance and lists various real-world applications enabled by SLMs, categorized by constraints like real-time interaction, content generation, edge inference, and privacy. - Lastly, the paper highlights important open challenges and future research directions for SLMs, such as hallucination, bias, inference-time energy efficiency, and data privacy. | ['Natural Language Processing'] | N/A | N/A |
| [COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training](https://arxiv.org/abs/2410.19313) | Kurt Keutzer, Yao Lu, Ligeng Zhu, Han Cai, Haocheng Xi | - COAT is a novel FP8 training framework designed to reduce memory footprint and increase training speed for large models by compressing both optimizer states and activations. - It introduces Dynamic Range Expansion, aligning optimizer state distributions with FP8's range, thereby minimizing quantization error. - For activations, COAT proposes Mixed-Granularity Activation Quantization, using fine-grained quantization for non-linear layers and per-tensor quantization for linear layers. - COAT achieves nearly lossless performance while decreasing memory by 1.54x and increasing training speed by 1.43x on Llama 7B, 13B, and 30B models compared to BF16. - COAT facilitates training larger models on fewer GPUs by enabling full-parameter training of 7B models on a single GPU and supports doubling the micro-batch size for distributed training. | ['Natural Language Processing', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/NVlabs/COAT) | N/A |
| [Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines](https://arxiv.org/abs/2410.21220) | Xiangyu Yue, Xiaohan Ding, Yiyuan Zhang, Zhixin Zhang | - Vision Search Assistant, a novel framework to address the limitation of traditional methods in understanding unfamiliar visual content.  - The framework facilitates collaboration between VLMs and web agents, leveraging VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. - By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system.  - It involves Visual Content Formulation to represent visual content with correlated formulations, Web Knowledge Search with Chain of Search algorithm to obtain comprehensive web knowledge, and Collaborative Generation to generate the final answer.  - Extensive experiments on open-set and closed-set QA benchmarks demonstrate that Vision Search Assistant significantly outperforms other models. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/cnzzx/VSA) | [Link](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard), [Link](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b) |
| [Fast Best-of-N Decoding via Speculative Rejection](https://arxiv.org/abs/2410.20290) | Jiahao Qiu, Huitao Yang, Ruiqi Zhang, Momin Haider, Hanshi Sun | - This paper introduces Speculative Rejection, a novel inference-time alignment algorithm designed to improve the efficiency of Best-of-N decoding for large language models (LLMs). - The key idea is to dynamically reduce the batch size during generation by halting the generation of unpromising responses early, based on partial reward scores. - The algorithm starts with a large batch size, effectively simulating Best-of-N with large N and leverages a reward model to rank partial utterances and terminate low-scoring ones. - The results on the AlpacaFarm dataset demonstrate that Speculative Rejection can achieve higher rewards with similar latency while requiring significantly fewer GPUs (16-32 times less compute power) compared to standard Best-of-N.  - The method is also shown to be effective in maximizing the probability of generated utterances. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Zanette-Labs/SpeculativeRejection) | N/A |
| [Language Models And A Second Opinion Use Case: The Pocket Professional](https://arxiv.org/abs/2410.20636) | David Noever |  - This research assesses Large Language Models (LLMs) as second opinion tools in complex medical and legal scenarios. - Evaluated LLM performance on 183 medical cases from Medscape and 21 Supreme Court legal cases, comparing responses with crowd-sourced physician opinions and documented legal votes respectively. - Found high accuracy in straightforward medical cases (>81%) but reduced performance (43%) in complex scenarios, suggesting LLMs may be valuable for generating differential diagnoses rather than as primary diagnostic tools. - Developed novel benchmarks for others to assess the reliability of responses by both LLMs and human practitioners, revealing high contestation among human experts. - Suggests that using LLMs as specialized agents for second opinions in medicine, especially in challenging cases, might be more appropriate than current approaches that emphasize automation of routine tasks. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/reveondivad/certify) | N/A |


## Papers for 2024-10-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting](https://arxiv.org/abs/2410.17856) | Xiaojian Ma, Zhancun Mu, Zihao Wang, kevinLian, phython96 | ROCKET-1 is a novel, low-level policy that leverages visual-temporal context prompting, a communication protocol using object segmentation masks and interaction types from past and present observations to guide policy-environment interactions. ROCKET-1 uses a causal transformer architecture that processes observations and object segmentations concatenated into a 4-channel image along with interaction types as conditions. Experiments in Minecraft demonstrate that agents using this approach achieve higher success rates on complex tasks, outperforming methods based on language, future image, or latent code prompting. A backward trajectory relabeling method efficiently generates segmentation annotations, enabling automatic dataset creation for training ROCKET-1. The approach allows for spatial understanding in embodied decision-making, leading to agents accomplishing previously unattainable tasks like “place oak door on diamond block” with a 91% success rate and others requiring long-horizon planning such as obtaining obsidian with a 70% success rate. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [Continuous Speech Synthesis using per-token Latent Diffusion](https://arxiv.org/abs/2410.16048) | Hagai Aronowitz, Slava Shechtman, Arnon Turetzky, Avihu, NimrodShabtay1986 |  - This paper introduces SALAD, a per-token latent diffusion model for zero-shot text-to-speech that operates on continuous representations, inspired by the per-token diffusion head for image generation. - It extends the image generation method to handle variable-length outputs, uses semantic tokens for context and stopping conditions, and doesn't require text-audio alignment. - Three SALAD variants are proposed: T2A (Text2Acoustic), S2A-AR (Semantic2Acoustic Autoregressive), and S2A-NAR (Semantic2Acoustic Non-Autoregressive), along with corresponding discrete baseline models for comparison. -  Evaluations on speech quality, intelligibility, and speaker similarity show that SALAD's T2A model achieves the highest intelligibility score. -  It also maintains speech quality and speaker similarity comparable to ground-truth audio based on subjective listening tests. | ['Text-to-Speech', 'Audio'] | N/A | N/A |
| [Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data](https://arxiv.org/abs/2410.18558) | Jialing Zhang, Shuhao Gu, ZacLiu, bowen92, ldwang | {- Introduced Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through quality filtering and deduplication.  - Proposed a synthetic instruction generation method using open-source VLMs, detailed image annotations, and diverse question generation to improve data quality and scale.  - Trained Aquila-VL-2B, a 2-billion parameter VLM based on the LLaVA-OneVision architecture, using Infinity-MM and synthetic data.  - Aquila-VL-2B achieved state-of-the-art performance for models of similar scale on various visual benchmarks, including MMBench, MMStar, and MathVista.  - Demonstrated that scaling instruction data and generating synthetic data can significantly improve the performance of open-source multimodal models.} | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main/scripts/train) | N/A |
| [Teach Multimodal LLMs to Comprehend Electrocardiographic Images](https://arxiv.org/abs/2410.19008) | Ping Zhang, Xiang Yue, Yuelin Bai, Ruoqi Liu | - This paper introduces PULSE, a new Multimodal Large Language Model (MLLM) tailored for electrocardiogram (ECG) image comprehension.  - It also presents ECGInstruct, a new instruction tuning dataset of over one million ECG image-text samples featuring realistic image synthesis and a diverse range of ECG-related tasks.   - A new evaluation benchmark, ECGBench, covering four key ECG image interpretation tasks across nine different datasets is also constructed.  - PULSE achieves state-of-the-art results, significantly outperforming proprietary MLLMs such as GPT-40 by 15-30% accuracy on out-of-domain datasets. - Ablation studies highlight the importance of diverse data sources and incorporating instruction tasks for ECG image comprehension. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image-to-Text', 'Computer Vision'] | N/A | [Link](https://aimedlab.github.io/PULSE/) |
| [MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark](https://arxiv.org/abs/2410.19168) | Ramaneswaran Selvakumar, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, S Sakshi | - MMAU, a massive multi-task audio understanding and reasoning benchmark, is introduced to evaluate expert-level reasoning and knowledge retrieval abilities in Large Audio-Language Models (LALMs).  - It consists of 10,000 expertly annotated audio-question-response pairs across speech, sound, and music domains, covering 27 distinct tasks, including 16 for reasoning and 11 for information extraction.  - Evaluations of 18 open-source and proprietary LALMs reveal that even the best-performing model only achieves 53% accuracy on MMAU, with human performance at 82%, highlighting significant room for improvement. - Models performed best on sound-based tasks but struggled the most with music. Cascaded models employing audio captioning followed by an LLM achieved the best performance, suggesting the potential for independent advancements in audio perception and text-based reasoning. - A detailed error analysis highlights perceptual errors as the most common mistake, emphasizing the need for better audio processing capabilities in current models. | ['Audio', 'Multimodal'] | N/A | N/A |
| [Counting Ability of Large Language Models and Impact of Tokenization](https://arxiv.org/abs/2410.19730) | Chenyu You, Juntai Cao, Wyattz23 | This paper investigates the impact of tokenization on the counting abilities of Large Language Models (LLMs), demonstrating that tokenization choices significantly influence model performance on counting tasks. - The study adopts a model-agnostic approach, manipulating input string formats to control tokenization in both open and closed-source LLMs. - It is observed that byte-pair encoding (BPE), commonly used in LLMs, can severely degrade counting accuracy due to a mismatch between the unit being counted (letters) and the unit processed (tokens). - The research reveals that Chain-of-Thought (CoT) prompting significantly improves counting abilities by enabling iterative inductive reasoning in the text space, partially overcoming the inherent limitations of Transformer models in sequential computations. - Through extensive experiments, the study finds that clear item-separated tokenization, as opposed to letter-grouped tokenization, enhances counting accuracy. Furthermore, the experiments showed that lower-frequency characters are easier to count compared to higher-frequency ones. | ['Natural Language Processing'] | N/A | N/A |
| [Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning](https://arxiv.org/abs/2410.19290) | Yang Zhang, Tommi Jaakkola, code-terminator, yujianll | - PREREQ-TUNE, a novel fine-tuning strategy designed to mitigate LLM hallucinations, is introduced. - PREREQ-TUNE incorporates a two-stage process: a prerequisite learning stage where a knowledge LoRA is trained to acquire necessary knowledge, followed by a supervised fine-tuning (SFT) stage where a skill LoRA focuses solely on learning task-specific skills.  The prerequisite learning stage enhances factuality by equipping the LLM with the required knowledge for subsequent fine-tuning, thereby reducing reliance on generating incorrect information. - The method also utilizes fictitious synthetic data for multi-version training, further improving the grounding of LLM outputs to internal knowledge.  This decoupling of knowledge and skill learning allows for more robust factual generation and control. - Experiments on long-form generation (biography and medical QA) and short QA tasks demonstrate PREREQ-TUNE's superior performance compared to baselines, including those utilizing reinforcement learning and direct preference optimization. - Analysis confirms the effectiveness of PREREQ-TUNE's disentanglement mechanism, even when trained solely on fictitious data, opening possibilities for new retrieval augmented generation (RAG) paradigms and knowledge-controlled text generation. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/UCSB-NLP-Chang/Prereq_tune.git) | N/A |
| [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133) | Valentina Pyatkin, Sachin Kumar, Yanai Elazar, Yizhong Wang, ljvmiranda921 |  - This paper introduces a routing framework for preference learning that dynamically allocates instances to either human or LM annotators, creating a hybrid approach to data annotation. - The framework employs a performance prediction model (PPM) to estimate the performance of reward models trained on different mixes of human and LM annotations and uses this to strategically select an optimal combination. - Results on the MULTIPREF dataset and others show that the proposed hybrid preference approach significantly outperforms using either human or LM preferences exclusively, as well as random combinations, across several benchmarks. - Analysis of the framework highlights that instances with moderate semantic similarity, safety concerns, or intent complexity tend to benefit the most from human annotation. - The authors release the code, data, and annotation platform used to promote further research in efficient and effective preference data collection. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/allenai/hybrid-preferences) | [Link](https://hf.co/datasets/allenai/multipref) |
| [Reflection-Bench: probing AI intelligence with reflection](https://arxiv.org/abs/2410.16270) | Yan Teng, Shuqi Kong, Haiquan Zhao, Yixu Wang, LingyuLi | This paper introduces Reflection-Bench, a new benchmark designed to evaluate the reflection capabilities of Large Language Models (LLMs). - Reflection is defined as the ability of an intelligent system to adapt its beliefs or behaviors in response to unexpected outcomes, encompassing core cognitive functions such as perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. - The benchmark comprises seven tasks adapted from cognitive science paradigms, including the oddball paradigm, n-back task, probabilistic reversal learning task, Wisconsin card sorting test, weather prediction task, double-choice Iowa gambling task, and meta-bandit task. - An evaluation of 13 prominent LLMs reveals that current models still fall short of human-level reflection abilities, particularly lacking meta-reflection capabilities.  - The authors argue that reflection is a crucial aspect of intelligence and propose Reflection-Bench as a valuable tool for evaluating and furthering the development of more sophisticated AI systems. | ['Natural Language Processing'] | [Link](https://github.com/YabYum/ReflectionBench) | N/A |


## Papers for 2024-10-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss](https://arxiv.org/abs/2410.17243) | Kehan Li, Hang Zhang, LidongBing, Zhiqiang007, ClownRat | - Inf-CL, a novel tile-based contrastive loss implementation, is introduced to address the GPU memory limitations in scaling batch sizes for contrastive learning.  - By partitioning the log-sum-exp (LSE) calculation into smaller tiles and iteratively accumulating the LSE term, Inf-CL avoids full materialization of the similarity matrix, significantly reducing memory overhead and enabling training with near-infinite batch sizes. - A multi-level tiling strategy further enhances practical efficiency by distributing computations across multiple GPUs with ring-based communication and within each GPU across CUDA cores with fused kernels. - Experimental results show that Inf-CL achieves unprecedented batch sizes (e.g., 12M for CLIP-ViT-L/14 on 32 A800 80GB GPUs) without sacrificing accuracy. - Compared to state-of-the-art memory-efficient solutions, Inf-CL demonstrates a two-order-of-magnitude reduction in memory while maintaining comparable speed. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification'] | [Link](https://github.com/DAMO-NLP-SG/Inf-CLIP) | N/A |
| [LOGO -- Long cOntext aliGnment via efficient preference Optimization](https://arxiv.org/abs/2410.18533) | Min Zhang, Qiaoming Zhu, Zechen Sun, douvleplus, ZetangForward | This paper introduces LOGO (Long cOntext aliGnment via efficient preference Optimization), a novel training strategy to enhance the generation ability of Long-Context Models (LCMs) and address issues like hallucinations and instruction unfollowing. - LOGO employs reference-free preference optimization, guiding the model to distinguish between preferred and dis-preferred outputs, and a data construction pipeline leveraging open-source models. - It incorporates a position synthesis method, enabling training with a substantial 0.3B dataset on a single 8xA800 GPU within 16 hours. - Experimental results on LongBench show that LOGO significantly improves LCM performance, outperforming existing methods and approaching top closed-source models like GPT-4. - LOGO effectively scales context window size for short-context models and maintains performance on short-context tasks like MMLU, indicating its adaptability and minimal alignment tax. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ZetangForward/LCM_Stack.git) | [Link](https://huggingface.co/datasets/namespace-Pt/long-llm-data) |
| [Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch](https://arxiv.org/abs/2410.18693) | Qiaoming Zhu, Xiaobo Liang, douvleplus, XinyuShi, dyyyyyyyy | - ScaleQuest, a novel data synthesis method to generate large-scale question-answer pairs by leveraging "small-sized" open-source LLMs. - The method uses a two-stage question-tuning process of Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to improve question quality. - A filtering process for language clarity, solvability, and appropriate difficulty is used along with reward-based filtering for high-quality responses. - Experiments on a dataset of 1 million math problem-solution pairs show improvements of 29.2% to 46.4% on MATH benchmark across mainstream open-source models, outperforming existing datasets and models like GPT-4-Turbo and Claude 3.5. - The data synthesis method also proves to be cost-effective, with 10x reduced cost as compared to GPT-40. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/yyDing1/ScaleQuest) | N/A |
| [Can Knowledge Editing Really Correct Hallucinations?](https://arxiv.org/abs/2410.16251) | kaishu666, apayani, XiongxiaoXu, canyuchen, BaixHuang |  - This paper introduces HalluEditBench, a benchmark designed to evaluate the effectiveness of knowledge editing methods in correcting hallucinations generated by Large Language Models (LLMs). - The benchmark includes a new dataset of over 6,000 verified hallucinations across 9 domains and 26 topics, collected from Llama2-7B, Llama3-8B, and Mistral-v0.3-7B. -  HalluEditBench assesses knowledge editing methods across five dimensions: Efficacy, Generalization, Portability, Locality, and Robustness, offering a more comprehensive evaluation compared to existing datasets. - The evaluation reveals that existing methods struggle with generalization, portability, and robustness, despite showing high performance on standard knowledge editing datasets. For instance, while FT-M and MEMIT achieve near-perfect scores on existing datasets, their efficacy in correcting real-world hallucinations is significantly lower. -  ICE and GRACE show superior performance in correcting hallucinations but have limitations in other aspects, particularly robustness, suggesting that the efficacy of current knowledge editing techniques is highly dependent on domains and LLMs and requires further research. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Unbounded: A Generative Infinite Game of Character Life Simulation](https://arxiv.org/abs/2410.18975) | flavoredquark, mohitbansal, davejacobs, NealWadhwa, yzli |  - UNBOUNDED, a generative infinite game transcending finite, hard-coded video game systems by integrating generative AI models. - It simulates character life in open-ended virtual worlds inspired by sandbox and digital pet games, incorporating unconstrained storytelling of tabletop RPGs. - It uses a specialized, distilled LLM for dynamic game mechanics, narrative, character interactions and IP-Adapter for consistent character visuals across environments. - Evaluations showed improvement in character simulation, instruction following, narrative coherence, and visual consistency compared to traditional related approaches, as well as real-time interactivity (refreshing every second). - It also features a novel regional image prompt adapter that allows consistent and flexible visual generation of character in various environments. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Distill Visual Chart Reasoning Ability from LLMs to MLLMs](https://arxiv.org/abs/2410.18798) | zifeishan, cnxup, zh2001, WooooDyy, hewei2001 |  - This paper introduces Code-as-Intermediary Translation (CIT), a method to improve visual reasoning in Multimodal Large Language Models (MLLMs) by using code to translate visual charts into text, which is then used by LLMs to generate and answer complex questions about the charts. - The authors construct REACHQA, a dataset with 3k reasoning-intensive charts and 20k question-answer pairs, using CIT and leveraging LLMs for data synthesis. - Experiments demonstrate that fine-tuning MLLMs on REACHQA enhances their performance on chart-related benchmarks, improving LLaVA-Next-Llama3-8B by over 30% on average and notably transferring abilities to general mathematical reasoning tasks like MathVista. - The study also suggests that expert rationales distilled from stronger LLMs significantly impact reasoning abilities, and the balance between recognition- and reasoning-oriented data influences model performance. - This work provides valuable insights into improving and evaluating multimodal reasoning in LLMs through the innovative use of code as an intermediary representation. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/hewei2001/ReachQA) | N/A |
| [Why Does the Effective Context Length of LLMs Fall Short?](https://arxiv.org/abs/2410.18745) | Shansan Gong, Lei Li, Ming Zhong, Jun Zhang, Chenxin An | - This paper introduces ShifTed Rotray position embeddING (STRING), a training-free method to improve the effective context length of Large Language Models (LLMs). - STRING addresses the issue of left-skewed position frequency distribution in LLMs by shifting well-trained position indices to overwrite less effective ones during inference. - This allows LLMs to better capture distant information within their existing training lengths, improving long-range dependency modeling. - Experimental results show STRING boosts the performance of LLMs like Llama 3.1 70B and Qwen-2 72B by a significant margin on benchmarks like RULER and InfiniteBench, achieving state-of-the-art results for open-source LLMs. - Notably, Llama 3.1 70B with STRING outperforms commercial models like GPT-4-128K and surpasses Claude 2 and Kimi-chat. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/HKUNLP/STRING) | N/A |
| [Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs](https://arxiv.org/abs/2410.18451) | Jujie He, Rui Yan, Jiacai Liu, zengliangcs, chrisliu298 | - The paper introduces Skywork-Reward, a collection of data-centric methods for enhancing reward modeling in LLMs, along with a new dataset called Skywork-Reward, consisting of 80K curated preference pairs from public sources.  - Skywork-Reward data collection focuses on important domains for RLHF optimization, such as math and code, using a smaller, higher-quality data composition compared to larger datasets like Preference 700K.  - The paper details data selection and filtering strategies designed to prioritize pairs that effectively improve model performance, focusing on maximizing the margin between preferred and rejected responses during training.  - This work also explores various loss functions and finds that the vanilla Bradley-Terry loss consistently outperforms other options.  - As of October 2024, the resulting Skywork-Reward model series holds the top position on the RewardBench leaderboard, demonstrating the effectiveness of their approach. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/collections/Skywork/skywork-reward-model-66d7fbdebae0e60d00a6b60d), [Link](https://huggingface.co/collections/Skywork/skywork-reward-data-collection-66d7fda6a5098dc77035336d) |
| [MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms](https://arxiv.org/abs/2410.18977) | Lei Zhang, Shunlin Lu, Xuan Ju, Wenxun Dai, Ling-Hao Chen | MotionCLR is an attention-based motion diffusion model for interactive human motion generation and editing.  The model architecture is U-Net-like and consists of CLR blocks containing convolutional, self-attention, cross-attention, and feed-forward network layers. MotionCLR supports training-free motion editing including motion (de-)emphasizing, in-place motion replacement, style transfer and example-based motion generation by manipulating self- and cross-attention activations. Experimental results on the HumanML3D dataset demonstrate comparable generation performance to state-of-the-art methods, along with improved explainability and editing control. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Should We Really Edit Language Models? On the Evaluation of Edited Language Models](https://arxiv.org/abs/2410.18785) | Zeyu Li, Peijie Dong, Zhenheng Tang, Qi Li, Dominic789654 |  - This paper evaluates the impact of different model editing methods on the general abilities of Large Language Models (LLMs). - The study finds that existing editing methods lead to inevitable performance deterioration on general benchmarks, especially when the number of edits increases. - The research also reveals that instruction-tuned models are more robust to editing and that larger models are more resistant compared to smaller models. - Additionally, the study finds that editing can significantly weaken the safety of LLMs, even for safety-aligned models. - The results suggest that current editing methods are only suitable for small-scale knowledge updates, motivating further research on more practical and reliable editing methods. | ['Natural Language Processing'] | [Link](https://github.com/lqinfdim/EditingEvaluation) | N/A |
| [ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning](https://arxiv.org/abs/2410.17779) | Han Hu, Yong Luo, Li Shen, Jianyuan Guo, Zhiwei840 | - ADEM-VL is an efficient vision-language (VL) tuning framework based on pre-trained large language models (LLMs) that uses a parameter-free cross-attention mechanism for multimodal fusion. - This approach reduces trainable parameters and improves training and inference speed by embedding visual features into the language space and utilizing multiscale visual feature generation. - An adaptive fusion scheme dynamically discards less relevant visual information based on attention scores, allowing the model to concentrate on more pertinent visual features. - The model outperforms existing methods on ScienceQA by 0.77% with average accuracy and demonstrates comparable performance on image captioning tasks. - The framework suggests more efficient VL model development by utilizing intermediate-layer fusion. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Image Feature Extraction'] | [Link](https://github.com/Hao840/ADEM-VL) | N/A |
| [CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models](https://arxiv.org/abs/2410.18505) | Xiaofeng Shi, Hanyu Zhao, Chengwei Wu, Bo-Wen Zhang, ldwang |   - This paper introduces CCI3.0-HQ, a 500GB high-quality subset of the Chinese Corpora Internet 3.0 (CCI3.0) designed for pre-training large language models (LLMs). - The dataset was created using a novel two-stage hybrid filtering approach: 1. Fundamental Processing(safety filtering, text extraction, deduplication, and initial quality assessment) 2. High-Quality Processing (employs Qwen2-72B-Instruct to identify high-quality samples and train a smaller 0.5B classifier to filter the dataset). - A 0.5B parameter model trained from scratch on CCI3.0-HQ using 100B tokens achieved superior performance on 10 benchmarks compared to CCI3.0, SkyPile, and WanjuanV1 in zero-shot settings.  - The introduced quality classifier (CCI3-HQ) also outperforms existing classifiers like FineWeb-edu, IndustryCorpus2, and ChineseWebText in terms of F1 score. - The dataset and the classifier are open-sourced to benefit the community in developing high-quality Chinese LLMs. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/CCI3-HQ) | [Link](https://huggingface.co/datasets/BAAI/CCI3-HQ), [Link](https://huggingface.co/datasets/BAAI/CCI3-Data), [Link](https://huggingface.co/BAAI/CCI3-HQ-Classifier) |
| [CAMEL-Bench: A Comprehensive Arabic LMM Benchmark](https://arxiv.org/abs/2410.18976) | Ines Riahi, Ali Alharthi, Omkar Thawakar, Sara Ghaboura, ahmedheakl | CAMEL-Bench is a comprehensive Arabic LMM benchmark comprising eight diverse domains and 38 sub-domains, including multi-image understanding, complex visual perception, and video understanding. - It contains around 29,036 questions filtered from a larger pool of samples, and quality is manually verified by native speakers. - Evaluations of both closed-source, including GPT-4 series, and open-source LMMs were conducted. - GPT-4o achieved an overall score of 62%, revealing a need for improvement, especially among the best open-source models. - Closed-source models generally outperformed open-source models in most tests. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [WAFFLE: Multi-Modal Model for Automated Front-End Development](https://arxiv.org/abs/2410.18362) | Lin Tan, Shangshu Qian, jiang719, shanchao | - WAFFLE is a new fine-tuning strategy for Multi-modal Large Language Models (MLLMs) designed to automate front-end development by generating HTML code from UI design images. - It incorporates a structure-aware attention mechanism, enabling MLLMs to better understand HTML structure and a contrastive fine-tuning approach to align the visual understanding of UI designs with the generated HTML code. - The evaluation on WebSight-Test shows improvements of up to +9.00 percentage points in HTML Match, +0.0982 in CW-SSIM, +32.99 in CLIP, and +27.12 percentage points in LLEM. - Similar improvements are observed on Design2Code, another benchmark, demonstrating WAFFLE's effectiveness in bridging the gap between visual UI designs and text-based HTML/CSS code. - WAFFLE, as a fine-tuning method, is model-agnostic and therefore applicable to any MLLMs. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/lt-asset/Waffle) | N/A |
| [Language Models are Symbolic Learners in Arithmetic](https://arxiv.org/abs/2410.15580) | Hanjie Chen, Ruidi Chang, Roy Xie, Zhiqi Li, Chunyuan Deng | This paper investigates how Large Language Models (LLMs) learn arithmetic, specifically focusing on whether they leverage partial products during calculations and how they approach the task symbolically. - It finds that LLMs struggle to leverage partial products to solve multiplications and suggests that improvements in recognizing them arise from their symbol-learning process, not actual partial product calculation. - By decomposing arithmetic tasks into subgroups based on token-level analysis, the paper finds that LLMs treat a collection of different arithmetic operations similarly when subgroup complexity is fixed. - Through position-level accuracy analysis, it's observed that LLM learning follows a U-shaped curve, initially and finally performing well on easy patterns but struggling with harder patterns in between. - Overall, the study concludes that LLMs do not perform true calculations during arithmetic tasks. Rather, they act as symbolic learners by selecting subgroups based on complexity, which provides a novel framework for understanding these models' approach to arithmetic reasoning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Taipan: Efficient and Expressive State Space Language Models with Selective Attention](https://arxiv.org/abs/2410.18572) | Hanieh Deilamsalehy, Ruiyi Zhang, Thang M. Pham, Huy Huu Nguyen, chiennv |  - Taipan, a hybrid architecture for efficient long-context language modeling, combines the efficiency of Mamba-2 with Selective Attention Layers (SALs) to enhance long-range dependency handling. - SALs strategically select tokens requiring long-range interactions, refine their features, and augment them with attention, balancing efficiency and expressiveness. - Taipan scales to billions of parameters and demonstrates superior performance on various tasks, including zero-shot language modeling and memory-intensive tasks like in-context retrieval. - It achieves linear memory scaling, making it applicable for contexts up to 1 million tokens, and significantly outperforms Transformers and other SSM-based models on long sequences. - The ablation study emphasizes the importance of the attention budget and the absence of positional embeddings for efficient and enhanced generalization. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus) |
| [Value Residual Learning For Alleviating Attention Concentration In Transformers](https://arxiv.org/abs/2410.17897) | Zhenzhong Lan, Zhiyun Jiang, Tianyi Wu, Zcchill | • This paper introduces two novel Transformer variants: ResFormer and SVFormer. • ResFormer incorporates a residual connection from the first layer’s value embeddings to subsequent layers’ value embeddings to mitigate attention concentration, which is defined as a model’s attention increasingly focuses on fewer tokens as the network depth increases. • SVFormer shares the first layer’s value embeddings across all layers, reducing KV cache by approximately 50%. • Experimental results on a 20B SlimPajama dataset show ResFormer outperforms vanilla Transformer, DenseFormer, and NeuTRENO in training and downstream tasks. • SVFormer is shown to train faster than vanilla Transformer and perform better than GQA and CLA when sequence length is longer. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Zcchill/Value-Residual-Learning) | [Link](https://huggingface.co/datasets/cerebras/SlimPajama-627B) |
| [Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits](https://arxiv.org/abs/2410.18234) | Roland Memisevic, Arash Behboodi, Hassan Dbouk, Ashish Khisti, mamaj92 | This paper introduces a canonical architecture for multi-draft speculative sampling, where multiple draft models independently generate proposal token sequences. - It demonstrates that the optimal draft selection scheme can be achieved through a two-step process: importance sampling to select an intermediate token and single-draft speculative sampling on the selected token. - For two identical draft models, an analytical expression for optimal acceptance probability is derived, along with a necessary and sufficient condition for achieving an acceptance probability of 1. - A new token selection scheme based on weighted importance sampling is proposed, along with heuristic approaches to reduce computational complexity. - Experimental results on OPT models across various tasks show consistent improvements in block efficiency and token rates compared to baseline methods, especially when using non-identical draft distributions. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2024-10-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models](https://arxiv.org/abs/2410.17637) | conghui, KennyUTC, yhcao, yuhangzang, ziyuliu | **- MIA-DPO: a novel Multi-Image Augmented Direct Preference Optimization (DPO) framework, designed to enhance the multi-image understanding of Large Vision-Language Models (LVLMs).** **- MIA-DPO addresses the scarcity of diverse multi-image training data and high annotation costs by augmenting existing single-image data with noisy or unrelated images arranged in grid collages or pic-in-pic formats, reducing the need for manual annotation of multi-image data.** **- This framework leverages an attention-aware selection mechanism that filters out rejected responses by analyzing the attention value distribution across multiple images, allowing for automated, cost-effective, and scalable DPO data construction without relying on manual annotations or expensive APIs.** **- Experimental results demonstrate that MIA-DPO consistently outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on InternLM-XC2.5.** **- MIA-DPO maintains competitive performance on single-image tasks while boosting the performance on multi-image tasks, demonstrating its robustness across various architectures and its ability to handle both single and multiple images effectively.** | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Liuziyu77/MIA-DPO) | N/A |
| [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891) | Jiacheng Ye, Yizhe Zhang, kiaia, shivamag99, Sansa |  - This paper introduces a novel approach to scaling Diffusion Language Models (DLMs) by adapting pre-trained autoregressive (AR) language models like GPT2 and LLaMA. - The adaptation method bridges the gap between AR and DLM objectives through attention mask annealing to remove causal masking bias and inheriting the shift operation from AR models. - The resulting models, DiffuGPT and DiffuLLaMA (up to 7B parameters), are trained on less than 200B tokens and evaluated on various benchmarks, demonstrating competitive performance with their AR counterparts and state-of-the-art results among existing DLMs. - DiffuLLaMA showcases promising in-context learning and infilling abilities. - The models and training code are released to facilitate further DLM research.   | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/HKUNLP/DiffuLLaMA) | N/A |
| [Lightweight Neural App Control](https://arxiv.org/abs/2410.17883) | Jianye Hao, ShaoKun-HW, Fahren24, gpap, semitable |  - This paper introduces Lightweight Multi-modal App Control (LiMAC), a novel mobile phone control architecture designed for efficient interaction and control across various Android apps.  - LiMAC combines a small Action Transformer (AcT) with a fine-tuned vision-language model (VLM) to process textual goals and past mobile observations (screenshots, UI trees) and generate precise actions.  -  AcT predicts action types (click, scroll, input text) and executes straightforward interactions, while the VLM handles complex text generation tasks (composing messages, search queries).   - Experimental results on two mobile control datasets show LiMAC significantly outperforms fine-tuned open-source VLMs (Florence2, Qwen2-VL) and prompt engineering baselines using GPT-40, increasing overall action accuracy by up to 19% and 42% respectively.   - LiMAC also executes tasks 30 times faster than GPT-40 methods, making it more suitable for real-time mobile applications. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [MedINST: Meta Dataset of Biomedical Instructions](https://arxiv.org/abs/2410.13458) | Zirui Song, Yu Yin, Zihan Zhang, Meng Fang, Wenhan Han | • This paper introduces MEDINST, a large biomedical instruction meta-dataset comprising 133 tasks and over 7 million training examples spanning 12 distinct categories. • The authors curate MEDINST32, a benchmark derived from MEDINST consisting of 32 tasks with varying difficulty to evaluate large language models' (LLMs) generalization abilities in the biomedical domain.  • Several LLMs are fine-tuned on MEDINST and show improved generalization performance across various biomedical tasks, as evaluated on MEDINST32.  • The study finds that instruction fine-tuning is more effective than further pre-training on domain-specific data for adapting general LLMs to the biomedical domain. • Experimental results on MEDINST32 reveal that the models often struggle with generalization to new tasks when only fine-tuned on smaller datasets or in limited task formats, highlighting the value of large, comprehensive datasets. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification', 'Summarization', 'Translation'] | [Link](https://github.com/aialt/MedINST) | N/A |
| [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/abs/2410.15522) | Drishti Sharma, Rishabh Maheshwary, Lester James V. Miranda, shayekh, srishti-hf1110 | • This paper introduces M-REWARDBENCH, a multilingual benchmark for evaluating reward models (RMs) across 23 languages and six tasks. • M-REWARDBENCH consists of 2.87k preference instances covering chat, safety, reasoning, and translation capabilities. • Evaluation results show a significant performance gap between English and non-English languages, with RMs exhibiting higher performance in English and variations across different languages. • The analysis indicates that translation quality positively impacts RM performance, with better translations leading to improved accuracy. • The authors also explore the sensitivity of different RM types to translation quality and analyze the performance variations across different language families and scripts. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://hf.co/datasets/C4AI-Community/multilingual-reward-bench) |
| [TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts](https://arxiv.org/abs/2410.18071) | Tianhua Li, Yuxuan Xie, kpzhang, wqshao126 | TP-Eval is a new evaluation framework for Multimodal Large Language Models (MLLMs) that addresses the issue of prompt sensitivity, where minor prompt variations can lead to significant performance fluctuations, resulting in underestimation or bias in evaluation. - It introduces a prompt customization method using an automatic prompt optimizer, tailored for MLLMs, to generate optimal prompts for each model, tapping their full potential. - This optimizer leverages a scorer, composed of the target MLLM and an answer analyzer, to iteratively refine prompts based on accuracy, semantic similarity to the original prompt, and introspection from incorrect responses. - Experiments on MMT-Bench and MMMU datasets demonstrate that TP-Eval effectively reduces underestimation and bias, revealing models' true capabilities and facilitating fairer comparisons. - TP-Eval also shows promising results in zero-shot settings using in-context learning, enabling prompt optimization even with limited data. | ['Multimodal'] | N/A | N/A |


## Papers for 2024-10-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/abs/2410.17247) | lindahua, jiaqiwang-rex, conghui, yhcao, yuhangzang |  - PyramidDrop is a novel visual redundancy reduction strategy for Large Vision-Language Models (LVLMs) designed to accelerate training and inference. - It partitions the LVLM into stages and progressively drops image tokens at each stage's end based on a lightweight similarity calculation with the instruction's last token.  - This pyramid-like token reduction leverages the observation that token redundancy increases in deeper LVLM layers. - Experiments on LLaVA-NeXT-7B show 40% training time and 55% inference FLOPs reduction without performance loss on 15 vision-language tasks.  - PyramidDrop also allows training with doubled resolution using only 70% of the original training time and serves as a plug-and-play inference acceleration strategy outperforming existing methods. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/Cooperx521/PyramidDrop) | N/A |
| [Aligning Large Language Models via Self-Steering Optimization](https://arxiv.org/abs/2410.17131) | Jingren, xphan, luyaojie, keminglu, sanmusunrise |   - This paper introduces Self-Steering Optimization (SSO), an algorithm designed for automated alignment of large language models (LLMs), eliminating the need for manual annotation.  - SSO autonomously generates preference signals by prompting the policy model with contrastive principles and optimizing based on three objectives: steering the model towards chosen responses, maintaining on-policy behavior, and ensuring a consistent quality gap between responses. -  Experiments conducted on Qwen2 and Llama3.1 demonstrate SSO's ability to generate accurate and learnable signals, leading to significant performance improvements across various benchmarks without manual annotation or external models. - SSO enhanced the training of reward models using data generated during the alignment process, further highlighting its effectiveness. - This work contributes a scalable approach to preference optimization for more efficient and effective automated alignment. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/icip-cas/SSO) | N/A |
| [JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://arxiv.org/abs/2410.17250) | Yuki Imajuku, gneubig, ku21fan, AtsuMiyai, shtapm | JMMMU is a new large-scale Japanese benchmark dataset designed to evaluate Large Multimodal Models (LMMs) focusing on Japanese cultural understanding. - It comprises two subsets: a Culture-Agnostic (CA) subset, containing translations of culture-independent components from the MMMU benchmark, and a Culture-Specific (CS) subset with newly crafted questions related to Japanese culture. - The benchmark is significantly larger than existing culture-aware Japanese benchmarks, totaling 1,320 questions with 1,118 images across a diverse range of 28 subjects. - An evaluation of 15 open-source and 3 proprietary LMMs reveals up to 58.6% overall accuracy, indicating significant room for improvement in utilizing the Japanese context. - The results indicate that many LMMs perform worse on questions in Japanese compared to their English counterparts and highlight the importance of culture-specific evaluation. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/cyberagent/llava-calm2-siglip), [Link](https://huggingface.co/datasets/SakanaAI/JA-Multi-Image-VQA), [Link](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500), [Link](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild) |
| [EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search](https://arxiv.org/abs/2410.14649) | dalistarh, ekurtic, SpiridonSunRotator, OliverSieberling | • EvoPress, a new evolutionary search approach for dynamic compression of large language models (LLMs), is introduced, offering provable convergence and low sample and iteration complexity. • EvoPress challenges the assumption of error monotonicity in LLM compression, demonstrating instances where lower per-layer error sums do not translate to better overall performance. • This method improves upon existing layer dropping, unstructured sparsity, and quantization techniques, setting new state-of-the-art results. • It achieves significant improvements, particularly at higher compression ratios, across various LLM families. • EvoPress converges efficiently, often within hours on a single GPU, even for large models, and a lightweight version is available for faster processing. | ['Natural Language Processing', 'Text Generation', 'Feature Extraction'] | [Link](https://github.com/IST-DASLab/EvoPress) | N/A |
| [MiniPLM: Knowledge Distillation for Pre-Training Language Models](https://arxiv.org/abs/2410.17215) | Minlie Huang, Jie Zhou, Hao Zhou, fandong, t1101675 |  - MINIPLM is a new Knowledge Distillation (KD) framework for pre-training Language Models (LMs) that refines the training data distribution using a teacher LM's knowledge. - It addresses the efficiency, flexibility, and effectiveness challenges of existing KD methods during pre-training through offline teacher inference, corpus-based operation, and a Difference Sampling technique that leverages the discrepancies between large and small LMs. - Experiments across various student LM sizes show that MINIPLM improves performance on 9 downstream tasks, language modeling capabilities, and reduces pre-training computation by 2.2 times compared to Vanilla KD which achieves similar performance but with more compute. - MINIPLM also supports KD across model families with different tokenizations, unlike existing online KD methods. - Further analysis suggests that MINIPLM improves pre-training data utilization, reducing the data demand by 2.4 times. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/MiniPLM) | N/A |
| [Mitigating Object Hallucination via Concentric Causal Attention](https://arxiv.org/abs/2410.15926) | Shijian Lu, Ivan Laptev, Yiheng Li, xing0047 | - This paper introduces Concentric Causal Attention (CCA), a novel position alignment strategy for Large Vision-Language Models (LVLMs) designed to mitigate object hallucination, a phenomenon where LVLMs generate text responses misaligned with image content.  - CCA addresses the limitations of Rotary Position Encoding (ROPE), commonly used in LVLMs, where long-term decay in attention can lead to hallucination.  - The method reorganizes visual tokens in a concentric manner, reducing the relative distance between visual and instruction tokens and improving spatial locality. It also introduces a modified causal attention mask to support the 2-D structure of image data.  - Experimental results on benchmarks like POPE, CHAIR, and MME demonstrate that CCA surpasses existing debiasing methods, improving accuracy and reducing hallucination.  - CCA also enhances the overall perception capability of LVLMs in multiple-choice visual question answering tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/xing0047/cca-llava.git) | N/A |
| [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930) | Thomas Hartvigsen, Jonathan Kropko, Zack Gottesman, Bryan R. Christ |  - MathNeuro is introduced; a method for isolating math-specific parameters in LLMs using forward passes, building upon existing work by calculating parameter importance with weights and activations, but with the key innovation of removing parameters also important for general language tasks measured on non-math datasets. - Pruning MathNeuro-identified parameters eliminates a LLM's math reasoning ability, while the impact on other tasks is similar to pruning random parameters. - Scaling up MathNeuro-identified parameters by a small constant (1.1 for smaller models and 1.01 for larger models) improves performance on GSM8K by 4-17% without affecting non-math performance. - MathNeuro remains effective with a single sample for parameter identification, demonstrating its data efficiency. - Math-specific parameters are distributed across the model's decoder blocks, suggesting math reasoning is not localized. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/bryanchrist/MathNeuro) | N/A |


## Papers for 2024-10-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution](https://arxiv.org/abs/2410.16256) | Hongwei Liu, Maosong Cao, zsytony, KennyUTC, acylam | - CompassJudger-1 is introduced as the first open-source all-in-one judge LLM. - It supports unitary scoring, two-model comparisons, formatted evaluations, critique generation and diverse tasks. - A new benchmark called JudgerBench is created to evaluate judge models. It includes realistic human annotation from the LLM arena and GPT annotations on subjective benchmarks. - Training data for CompassJudger-1 includes several sources, like pair-wise data, critiques and reward data. - Several data filtering and sampling strategies are developed for training. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/CompassJudger) | N/A |
| [PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://arxiv.org/abs/2410.13861) | hsli-cuhk, daijifeng, zengxingyu, gogoduan, LucasFang |   - PUMA, a unified multimodal large language model (MLLM), is introduced, featuring multi-granular visual feature processing for diverse visual tasks.  - The model uses a three-part architecture: a multi-granular image encoder (CLIP), a set of dedicated diffusion-based decoders, and an autoregressive MLLM.  - PUMA is trained in two stages: Multimodal pretraining on large datasets (Laion-2B, Laion-Aesthetics, GRIT, The Pile, OCR-VQA-200K, LLaVAR) followed by task-specific instruction tuning.  - The evaluation shows that PUMA excels in diverse text-to-image generation, image editing, conditional image generation, and understanding, outperforming existing unified MLLMs.  - The multi-granular approach balances diversity and controllability by processing features at multiple levels from coarse-grained abstractions to fine-grained details. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image', 'Visual Question Answering'] | [Link](https://github.com/rongyaofang/PUMA) | N/A |
| [Baichuan Alignment Technical Report](https://arxiv.org/abs/2410.14940) | dongguosheng, YijieZhou, TJU-Tianpengli, zilchshen, lin5547 | • This report introduces Baichuan Alignment, a comprehensive suite of techniques used to align the Baichuan series of large language models (LLMs), including optimization methods, data strategies, and evaluation processes. • Baichuan Alignment consists of three phases: Prompt Augmentation System (PAS) which transforms user queries into actionable instructions, Supervised Fine-Tuning (SFT) which trains LLMs for dialogue and complex tasks, and Preference Alignment which aligns LLMs with human preferences. • The alignment process employs several optimizations such as sample packing and multi-layer gradient checkpointing to increase training efficiency and model merging to improve performance across domains. • Evaluations of Qwen2-Nova-72B and Llama3-PBM-Nova-70B, instruct versions of open-source models optimized with Baichuan Alignment, show significant performance improvements across various benchmarks, outperforming official instruct versions and competing with leading LLMs. • Baichuan-Instruct, an internal model, demonstrates 17% to 28% user experience improvement in core capabilities, highlighting the effectiveness of the proposed alignment techniques. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B) |
| [AutoTrain: No-code training for state-of-the-art models](https://arxiv.org/abs/2410.15735) | abhishek | AutoTrain (AutoTrain Advanced) is an open-source, no-code tool/library for training and fine-tuning machine learning models on a variety of tasks and modalities. - It supports various tasks, including large language model (LLM) fine-tuning, text classification/regression, token classification, sequence-to-sequence tasks, fine-tuning of sentence transformers, visual language model (VLM) fine-tuning, image classification/regression, and tabular data classification/regression. - AutoTrain simplifies the training process by providing a user-friendly interface and automating tasks such as dataset processing, hyperparameter tuning, and model validation. - It offers flexibility by supporting local and cloud-based training, multiple data formats (zip, CSV, JSONL), and various model architectures compatible with Hugging Face Transformers. - AutoTrain is designed for both novice and experienced users, enabling them to build and deploy high-performing models easily. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Object Detection', 'Text Classification', 'Token Classification', 'Tabular Classification', 'Tabular Regression', 'Text2Text Generation', 'Text Generation', 'Multimodal'] | [Link](https://github.com/huggingface/autotrain-advanced) | N/A |
| [RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style](https://arxiv.org/abs/2410.16184) | Rui Min, Yantao Liu, juanli, Nuomei, TranSirius | This paper introduces RM-BENCH, a novel benchmark designed to evaluate reward models for language models.  RM-BENCH focuses on assessing reward models' sensitivity to subtle content differences and resistance to style biases, unlike existing benchmarks.  Extensive experiments show RM-BENCH strongly correlates with policy model performance, making it a reliable tool for selecting effective reward models. Results indicate that current state-of-the-art reward models perform poorly when faced with style bias, showcasing areas for improvement in future model development. The benchmark includes datasets across various domains, including chat, code, math, and safety, with style-controlled variations. | ['Natural Language Processing', 'Text Classification', 'Reinforcement Learning'] | [Link](https://github.com/THU-KEG/RM-Bench) | N/A |
| [Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://arxiv.org/abs/2410.16153) | Nyandwi, seungone, akariasai, yueqis, yuexiang96 | This paper introduces PANGEA, a fully open multilingual multimodal large language model (LLM) trained on a diverse 6M instruction dataset spanning 39 languages.  PANGEA significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts, showing comparable performance to state-of-the-art (SOTA) models in English while substantially exceeding them in multilingual scenarios.  The model's architecture is based on LLaVA-Next, using Qwen2-7B-Instruct as the language model backbone.  PANGEA, along with its associated data and code, is fully open-sourced to promote equitable and accessible access to robust multilingual MLLMs. | ['Multimodal'] | [Link](https://neulab.github.io/Pangea/) | [Link](https://huggingface.co/datasets/cmarkea/table-vqa), [Link](https://huggingface.co/datasets/deepvk/GQA-ru), [Link](https://huggingface.co/datasets/cmarkea/doc-vqa), [Link](https://huggingface.co/datasets/cmarkea/table-vqa), [Link](https://huggingface.co/datasets/BUAADreamer/Chinese-LLaVA-Med-7B), [Link](https://huggingface.co/datasets/LinkSoul-AI/Chinese-LLaVA), [Link](https://huggingface.co/datasets/Toshi456/LLaVA-Japanese-Instruct), [Link](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT) |
| [Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception](https://arxiv.org/abs/2410.12788) | Zhiyuan Ji, jimi888, siminniu, MoCun, Robot2050 |  - This paper introduces Meta-Chunking, a novel text segmentation technique that leverages Large Language Models (LLMs) to divide documents into logically coherent chunks at a granularity between sentences and paragraphs. - Two strategies are proposed: Margin Sampling Chunking, which performs binary classification on consecutive sentences based on probability differences, and Perplexity Chunking, which analyzes perplexity distribution to identify chunk boundaries. - A dynamic merging strategy is also introduced to balance fine-grained and coarse-grained chunking, adjusting chunk sizes based on user-specified length requirements. - Experimental results across eleven datasets and four benchmarks demonstrate that Meta-Chunking significantly improves single-hop and multi-hop question answering performance in Retrieval-Augmented Generation (RAG) systems. - On the 2WikiMultihopQA dataset, for example, Meta-Chunking outperforms similarity chunking by 1.32 in F1 score while requiring only 45.8% of the processing time. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/IAAR-Shanghai/Meta-Chunking) | N/A |
| [Pre-training Distillation for Large Language Models: A Design Space Exploration](https://arxiv.org/abs/2410.16215) | Xin Lv, juanli, NeoZ123, bys0318, Wesleythu | - This paper explores pre-training distillation (PD), a method for transferring knowledge from a larger teacher LLM to a smaller student LLM during the pre-training phase. - The study investigates four key aspects of PD: logits processing, loss selection, scaling law (model and data size), and the use of offline vs. online logits. - A preliminary experiment using GLM-4-9B as the teacher and a 1.9B parameter student model shows a 1.6% average improvement across various datasets with PD. - Further experiments reveal that larger student LLMs benefit more from PD, while larger teacher LLMs don't always guarantee better results. - The best results are achieved by combining Kullback-Leibler divergence loss with language modeling loss using a Warmup-Stable-Decay scheduling strategy. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation](https://arxiv.org/abs/2410.15748) | Ping Wei, opotle, yegong, shuailu, EurekaWu123 |  - Alchemy, a novel framework, synthesizes formal theorems through symbolic mutations to address the data scarcity challenge in Neural Theorem Proving (NTP). - For each candidate theorem, Alchemy identifies invocable theorems from Mathlib and performs mutations by replacing terms with equivalent forms or antecedents. - This method significantly increases the number of theorems in Mathlib from 110k to 6M. - Continual pretraining and supervised finetuning on this augmented dataset for Large Language Models leads to a 5% absolute performance improvement on the Leandojo benchmark and a 2.5% gain on the miniF2F benchmark. - The analysis of synthetic data composition and training paradigms offers valuable insights for developing strong theorem provers. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation](https://arxiv.org/abs/2410.14745) | Wei Ju, Xiao Luo, Shockzipper, XtremSup, luojunyu | - SemiEvol is a semi-supervised fine-tuning framework designed to improve large language model (LLM) performance in scenarios with limited labeled data and abundant unlabeled data. - It employs a bi-level knowledge propagation strategy, transferring knowledge from labeled data to unlabeled data through both model adaptation and context enhancement.  - For unlabeled data utilization, it involves collaborative learning among multiple LLMs with diverse configurations and adaptive data selection. - Experimental results on various datasets, including MMLU, MMLU-Pro, ARC, FPB, USMLE, PubMedQA, and ConvFinQA, demonstrate significant performance improvements compared to SFT and self-evolution methods. - SemiEvol effectively utilizes both labeled and unlabeled data, enabling LLMs to adapt to specific scenarios more economically. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/luo-junyu/SemiEvol) | [Link](https://huggingface.co/Solshine/reflection-llama-3.1-8B), [Link](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B) |
| [Zero-shot Model-based Reinforcement Learning using Large Language Models](https://arxiv.org/abs/2410.11711) | GPaolo, albert9000, Xssama, ambroiseodt, abenechehab |  - This paper introduces Disentangled In-Context Learning (DICL), a novel approach for applying in-context learning (ICL) with large language models (LLMs) to reinforcement learning (RL) environments with continuous state spaces. - DICL addresses the challenges of incorporating action information and handling state-action dimension interdependence by projecting the state-action vector into a latent space using Principal Component Analysis (PCA) where features are linearly uncorrelated, then applying ICL. - The paper demonstrates the effectiveness of DICL in two RL applications: model-based policy evaluation and data-augmented off-policy RL, showing improved sample efficiency in both cases. - A theoretical analysis provides a novel return bound for the policy evaluation algorithm resulting from multi-branch rollouts with the LLM-based dynamics model.  - Additionally, the paper provides empirical evidence suggesting that LLMs offer well-calibrated uncertainty estimations, a desirable property for model-based RL algorithms. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/abenechehab/dicl) | N/A |
| [Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement](https://arxiv.org/abs/2410.15633) | Yunshui Li, Gang Chen, Haozhe Zhao, Shuzheng Si, kaikai1 |  - This paper introduces GATEAU, a novel framework for selecting influential samples to improve long-context alignment in large language models (LLMs).  - GATEAU leverages Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM) to identify high-quality samples exhibiting strong long-range dependencies.  - HMG compares perplexity scores from homologous models with different context windows to assess response generation difficulty, while CAM evaluates whether the model focuses on important input segments.  - Experimental results show that LLMs trained on samples selected by GATEAU outperform those trained on the full dataset and various baselines across multiple benchmarks, including LongBench, LongBench-Chat, and MT-Bench.  - Ablation studies demonstrate that both HMG and CAM contribute significantly to GATEAU's effectiveness. | ['Natural Language Processing', 'Text Generation', 'Summarization', 'Question Answering'] | N/A | N/A |
| [CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy](https://arxiv.org/abs/2410.13218) | Travis Labrum, wangwilliamyang, xz97, Xianjun, billmianz | This paper introduces CBT-BENCH, a new benchmark for evaluating large language models' (LLMs) ability to assist cognitive behavioral therapy (CBT).  CBT-BENCH includes three levels of tasks: basic CBT knowledge acquisition, cognitive model understanding, and therapeutic response generation.  The benchmark uses three new datasets (CBT-QA, CBT-CD, CBT-PC, CBT-FC and CBT-DP) to evaluate the LLMs across these tasks.  The results show that while LLMs perform well on knowledge-based tasks, they struggle with complex tasks that require deep understanding of patient cognitive structures and effective response generation.  This suggests the need for further research into how LLMs can be improved for real-world CBT applications. | ['Natural Language Processing', 'Text Classification', 'Question Answering', 'Text Generation'] | [Link](https://github.com/mianzhang/CBT-Bench) | N/A |
| [Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs](https://arxiv.org/abs/2410.13394) | anoopk, prajdabre, dipsivenkatesh, safikhan, sumanthd | This paper introduces CIA Suite, a framework for cross-lingual auto-evaluation of multilingual LLMs.  The framework includes a novel test set (RECON) with human annotations across six languages and a cross-lingual evaluator LLM (HERCULE). HERCULE leverages English reference answers to evaluate responses in other languages, addressing the scarcity of reference answers in low-resource scenarios. Experiments show HERCULE aligns more closely with human judgments than existing models, exhibiting effectiveness in zero-shot settings.  The CIA suite is publicly available to encourage further research. | ['Natural Language Processing', 'Text Generation', 'Text Classification', 'Zero-Shot Classification'] | [Link](https://github.com/CIA) | [Link](huggingface.co/CIA-Suite) |
| [DM-Codec: Distilling Multimodal Representations for Speech Tokenization](https://arxiv.org/abs/2410.15017) | A K M Mahbubur Rahman, Md Fahim, amanchadha, tasnim, mubtasim | DM-Codec is a novel speech tokenizer that leverages a neural codec architecture with Residual Vector Quantization (RVQ) and incorporates two novel distillation approaches: LM-guided and combined LM and SM-guided distillation. - The LM-guided approach distills contextual representations from a Language Model (LM) and integrates them with acoustic representations, while the combined approach incorporates semantic representations from a Speech Model (SM) along with contextual and acoustic representations.  - DM-Codec adopts a streamlined encoder-decoder framework enhanced by a multi-discriminator setup comprising Multi-Scale, Multi-Period, and Multi-Scale Short-Time Fourier Transform discriminators. - Experimental results on the LibriSpeech benchmark demonstrate that DM-Codec significantly outperforms state-of-the-art models, reducing Word Error Rate (WER) by up to 13.46%, Word Information Lost (WIL) by 9.82%, and improving speech quality and intelligibility.  - The combined distillation approach results in a WER of 4.05 and a WIL of 6.61, surpassing existing speech tokenization models. | ['Audio', 'Automatic Speech Recognition', 'Multimodal'] | [Link](https://github.com/mubtasimahasan/DM-Codec) | N/A |


## Papers for 2024-10-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation](https://arxiv.org/abs/2410.13232) | jihoonkim25, Gwanwoo, ktio, kimnamssya, hyungjoochae | • This paper introduces World-Model-Augmented (WMA) web agents, which leverage world models to simulate the outcomes of actions for enhanced decision-making in web navigation. • WMA agents address the limitations of Large Language Models (LLMs) in long-horizon web navigation tasks by incorporating a world model that predicts the effects of actions, enabling the agent to foresee potential outcomes. • The authors propose a transition-focused observation abstraction method to overcome training challenges, where the world model is trained to generate natural language descriptions of state differences between time steps, rather than predicting the entire next observation. • The WMA agent employs a value function to estimate rewards for simulated next observations, guiding the policy model to select optimal actions. • Experimental results on WebArena and Mind2Web show that WMA agents improve policy selection, achieve state-of-the-art performance on Mind2Web, and demonstrate superior cost and time efficiency compared to tree-search-based agents. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/kyle8581/WMA-Agents) | N/A |
| [UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models](https://arxiv.org/abs/2410.14059) | Yilin Guo, Yan Hu, wittenberg, amstrongzyf, TobyYang7 | - UCFE, a User-Centric Financial Expertise benchmark, is introduced to evaluate LLMs' ability to handle complex, real-world financial tasks using dynamic, task-specific interactions in a hybrid approach combining human and LLM evaluations. - Based on a user study with 804 participants, a dataset is created that incorporates various user intents and interactions across different user groups, serving as a foundation for benchmarking 12 LLMs using LLM-as-Judge methodology. - Results demonstrate a strong correlation (Pearson coefficient 0.78) between benchmark scores and human preferences, validating the UCFE dataset and evaluation method. - Mid-sized LLMs (7B-14B parameters), fine-tuned on financial texts, achieve a balance between performance and resource efficiency. - The user-centric design highlights the necessity of aligning AI systems with diverse user requirements in finance, setting the stage for enhanced, reliable AI-driven solutions. | ['Natural Language Processing'] | [Link](https://github.com/TobyYang7/UCFE-Benchmark) | N/A |
| [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669) | Daniel Jiang, Wenxuan Peng, Zhiqiu Lin, Nyandwi, BaiqiL | • NaturalBench is a new benchmark designed for evaluating vision-language models (VLMs) on natural adversarial samples. • These are image-question pairs derived from real-world images and questions, which are easily answered by humans but pose a challenge for current VLMs. • The authors use a semi-automated approach to curate the benchmark, making use of CLIP and ChatGPT to source and filter questions from image caption datasets. • The benchmark comprises 10,000 human-verified question-answer samples, categorized by visual reasoning skill. • Evaluation results of 53 state-of-the-art VLMs demonstrate a significant performance gap compared to humans, suggesting the benchmark's efficacy in revealing areas for improvement. | ['Visual Question Answering', 'Multimodal'] | [Link](https://linzhiqiu.github.io/papers/naturalbench) | N/A |
| [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](https://arxiv.org/abs/2410.13276) | Hayden Kwok-Hay So, Dayou Du, Shijie, CharyZeng, Retromonic |  - This paper introduces SeerAttention, a novel attention mechanism designed to improve the efficiency and scalability of Large Language Models (LLMs), especially those with long context windows, by learning intrinsic sparse attention rather than using predefined patterns. - SeerAttention augments conventional attention with a learnable gate, called Attention Gate (AttnGate), to dynamically select important blocks in an attention map and treat the rest as sparse. - It employs a customized FlashAttention kernel to extract the block-level ground truth of attention maps for efficient training of the gating network, minimizing overhead. - Evaluations show SeerAttention outperforms existing sparse attention methods in post-training and achieves near-lossless accuracy with high sparsity (up to 90%) during fine-tuning for long context extension using YaRN. -  With a block-sparse pattern, the attention kernel achieves up to a 5.67x speedup over the FlashAttention-2 dense baseline on a single A100 GPU. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/SeerAttention) | N/A |
| [Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts](https://arxiv.org/abs/2410.14677) | Yury Chekhovich, Anastasia Voznyuk, German Gritsai, andriygav |  - This paper presents a systematic review of datasets used in competitions and research papers dedicated to AI-generated content detection and proposes methods for evaluating the quality of such datasets. - The authors argue that the high performance of current detectors on benchmark datasets may be due to the poor quality of the evaluation datasets rather than the true effectiveness of the detectors. - The authors investigate different metrics, such as detecting low-quality generations with the use of metrics based on topological time series, detecting suspicious activation maps, and detecting sensibility to perturbations, such as text modification and sentence shuffling - The paper emphasizes the need for robust and qualitative methods to evaluate generated data to be secure against bias and low generalization ability of future models and provide a more comprehensive understanding of the dynamics between human and machine text. - The paper suggests that the use of high-quality generated data can be used for two purposes: enhancing the training of detection models and refining the training datasets themselves. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828) | Mengdi Wang, Huazheng Wang, Yue Wu, yokey, huiyuan23 |  - This paper identifies a common pitfall in margin-based language model alignment methods used in Reinforcement Learning from Human Feedback (RLHF): the under-specification of ideal behavior on preferred and dispreferred responses.  - This issue leads to two problems as the margin increases: an increase in unsafe responses, and a decrease in preferred, ideal responses.  - The underlying cause is identified as the *gradient entanglement* effect, in which margin-based losses couple the preferred and dispreferred probabilities, thus often preventing ideal changes.  - This effect is characterized by an inner product condition involving the gradients of preferred and dispreferred log-probabilities.  - The theoretical analysis is empirically validated, and suggests potential mitigation through pairwise normalized gradient descent and sparsity regularized token masking. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/HumainLab/Understand_MarginPO) | N/A |
| [DPLM-2: A Multimodal Diffusion Protein Language Model](https://arxiv.org/abs/2410.13782) | Shujian Huang, Dongyu Xue, Fei Ye, Zaixiang Zheng, Xinyou Wang | • DPLM-2 is a multimodal protein foundation model based on a discrete diffusion probabilistic framework that models both protein sequences and structures.  • DPLM-2 employs a lookup-free quantizer (LFQ) to convert 3D coordinates to discrete tokens, facilitating structure learning within the language model.  • It uses an efficient warm-up strategy, leveraging pre-trained sequence-based DPLM and evolutionary data to enhance structural modeling.  • DPLM-2 demonstrates competitive performance in co-generation of structure and sequence, achieving high designability and outperforming ESM3-Open and Multiflow in structure-sequence compatibility.  • DPLM-2 also shows strong results in various conditional generation tasks like folding, inverse folding, and motif scaffolding and structure-aware representations for predictive tasks. | ['Multimodal', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media](https://arxiv.org/abs/2410.12791) | Mette Thunø, Rebecca M. M. Hicke, Ross Deans Kristensen-McLachlan, kardosdrur | This paper introduces KeyNMF, a novel approach to topic modeling that leverages contextual embeddings and Non-negative Matrix Factorization (NMF). - KeyNMF extracts keywords from documents using contextual embeddings and then applies NMF to these embeddings to generate topics.  - It is evaluated on Chinese news data and demonstrates competitive performance compared to other contextual topic models, especially in terms of external coherence. - KeyNMF is integrated with existing methods for analyzing information dynamics to study Chinese diaspora media's coverage of the 2024 European parliamentary elections.  -  The pipeline identifies trends in novelty and resonance signals that correlate with key political events, demonstrating its effectiveness in capturing information dynamics. - The researchers find that KeyNMF enables nuanced analysis of information flow and agenda-setting within Chinese diaspora media during the election period. | ['Natural Language Processing', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) |


## Papers for 2024-10-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures](https://arxiv.org/abs/2410.13754) | kcz358, fuzhao, Junhao233, dghosal, jinjieni |  - MixEval-X is a benchmark for evaluating multimodal models across various input-output modalities, including image, video, audio, text, and actions. - The benchmark covers eight input-output modality combinations and uses a mixture of existing datasets and real-world web data to construct evaluations.  - MixEval-X employs a novel multi-modal benchmark mixture and adaptation-rectification pipeline to optimize evaluation tasks by aligning them with real-world task distributions and mitigating biases.  - Meta-evaluations demonstrate that MixEval-X effectively aligns benchmark samples with real-world distributions, with model rankings correlating strongly (up to 0.98) with crowd-sourced real-world evaluations.  - The benchmark offers comprehensive leaderboards to rerank existing models and organizations across modalities. | ['Multimodal', 'Any-to-Any', 'Image-to-Text', 'Video-Text-to-Text', 'Text-to-Image', 'Text-to-Video', 'Text-to-Audio'] | [Link](https://mixeval-x.github.io/) | N/A |
| [Harnessing Webpage UIs for Text-Rich Visual Understanding](https://arxiv.org/abs/2410.13824) | Yuxiao Qu, Yifan Song, yuexiang96, oottyy, jeepliu |  - This paper introduces MultiUI, a 7.3 million sample dataset synthesized from 1 million web page UIs using LLMs, for training multimodal models in text-rich visual understanding. - MultiUI covers nine diverse tasks across three categories (visual understanding and reasoning, text recognition, and grounding), enhancing model perception, comprehension, grounding, and reasoning capabilities.  - Models trained on MultiUI demonstrate significant improvement, up to 48% on VisualWebBench and 19.1% on Mind2Web, outperforming larger models like LLaVA 1.6 34B and GPT-4V in GUI tasks.  - MultiUI also generalizes well to non-web UI tasks like document understanding, OCR, and chart interpretation, showing strong cross-domain generalization. - This highlights the value of structured web UI data for advancing text-rich visual understanding in MLLMs.  | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | [Link](https://neulab.github.io/MultiUI/) |
| [MobA: A Two-Level Agent System for Efficient Mobile Task Automation](https://arxiv.org/abs/2410.13757) | Yixuan Jiang, Kunyao Lan, Yansi Li, Hao Tang, JamesZhutheThird | - MobA, a novel two-level agent architecture designed to enhance the abilities of mobile phone assistants, using Multimodal Large Language Models (MLLMs). -  Composed of a higher-level Global Agent for tasks such as command interpretation and task planning, and a lower-level Local Agent to select and execute actions based on current screen information and historical data. -  A double reflection mechanism allowing the system to correct errors quickly and avoid sub-optimal operations, as well as an integrated memory module to track actions and optimize execution. -  Evaluation performed on the Mobbench dataset containing 50 mobile tasks across 10 applications of varying difficulty, outperforming other mobile agents, achieving the highest milestone score of 66.2%. | ['Multimodal', 'Computer Vision', 'Natural Language Processing'] | N/A | N/A |
| [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) | zdaxie, zizhpan, XCLiu, CNMaxwell, WuChengyue | - Janus is an autoregressive multimodal model that decouples visual encoding pathways for understanding and generation tasks using a shared transformer architecture.  - For understanding, it uses a SigLIP encoder for high-level semantic information, while for generation, it utilizes a VQ tokenizer focusing on fine-grained visual details.  - This approach addresses the conflicting representational needs of the two tasks, enabling both strong performance and model flexibility.  - Experimental results demonstrate that Janus outperforms other unified models of comparable size and matches or exceeds task-specific models on benchmarks like MMBench, SEED-Bench, POPE, MSCOCO, and GenEval.  - The model's performance and flexibility make it a potential candidate for the next generation of unified multimodal models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/deepseek-ai/Janus) | N/A |
| [MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://arxiv.org/abs/2410.13085) | Weijia Shi, Tianze Wang, Haoran Li, Kangyu Zhu, richardxp888 | MMed-RAG is a new multimodal retrieval-augmented generation (RAG) system designed to improve the factuality of Medical Large Vision-Language Models (Med-LVLMs). - It incorporates a domain-aware retrieval mechanism, adaptive context selection, and RAG-based preference fine-tuning to address misalignment issues and enhance alignment with ground truth. - The model achieves an average improvement of 43.8% in factual accuracy across five medical datasets and two tasks (medical VQA and report generation) compared to the original Med-LVLM.  - It outperforms other decoding-based and RAG-based approaches on medical VQA and report generation tasks. - MMed-RAG demonstrates strong generalizability, achieving consistent improvements across various medical image modalities (radiology, ophthalmology, and pathology). - Through ablation studies, the contribution of each proposed component is validated, demonstrating its effectiveness in enhancing the factuality and performance of Med-LVLMs in different medical domains. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/richard-peng-xia/MMed-RAG) | N/A |
| [A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models](https://arxiv.org/abs/2410.13841) | Keming Lu, Hongyu Lin, Bowen Yu, Le Yu, TangQiaoYu | - This paper introduces a unified perspective on delta parameter editing in post-trained large-scale models, formulating editing operations based on Riemann sum approximation of the loss difference. - This analysis categorizes existing methods into three performance classes: competitive (e.g., DARE, DELLA-Merging), decreased (e.g., BitDelta, Twin-Merging, TIES-Merging), and improved (e.g., EXPO), explaining their impact on model performance through the lens of Riemann sum approximation. - Extensive experiments on visual and language models (ViT, LLaMA 3, Qwen 2, Mistral) support the theoretical findings. - The paper further proposes extensions to existing techniques like DARE and BitDelta, generalizing their formats and improving applicability. - For example, introducing a factor *k* to DARE handles dropped parameters more effectively and expanding BitDelta to use multiple bits improves performance beyond the original post-trained model. | ['Natural Language Processing', 'Computer Vision'] | N/A | N/A |
| [PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment](https://arxiv.org/abs/2410.13785) | Ke Xu, Jiaheng Liu, Shawn Wang, Zekun Moore Wang, kangz | PopAlign is a framework for aligning large language models (LLMs) by diversifying contrasting patterns across prompt, model, and pipeline levels. - It integrates six distinct contrasting strategies: Prefix Contrast, Demon Contrast, Elicitive Contrast, NParam Contrast, Leaderboard Contrast, and Refine Contrast. - These strategies synthesize preference-contrastive data without requiring additional feedback labeling. - Experimental results demonstrate that PopAlign significantly outperforms existing methods on various alignment tasks and leaderboards. - Notably, PopAlign achieves higher scores than strong baselines trained on original labels, indicating its effectiveness in preference modeling and comprehensive alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [MoH: Multi-Head Attention as Mixture-of-Head Attention](https://arxiv.org/abs/2410.11842) | Shuicheng Yan, Li Yuan, Bo Zhu, Chat-UniVi | - Mixture-of-Head attention (MoH) is proposed, which integrates multi-head attention with a Mixture-of-Experts (MoE) mechanism by treating attention heads as experts. - MoH employs a router to select the top-k heads for each token, improving inference efficiency, and uses a weighted sum of outputs rather than standard summation, potentially enhancing performance. - Shared heads in MoH retain constant activation, capturing general knowledge. - Evaluations on ViT, DiT, and LLMs show MoH outperforms multi-head attention using only 50%~90% of heads. - Pre-trained models like LLaMA3-8B can be continue-tuned into MoH models, with MoH-LLaMA3-8B showing improved accuracy with fewer heads. | ['Computer Vision', 'Image Classification', 'Unconditional Image Generation', 'Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/SkyworkAI/MoH) | N/A |
| [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852) | Anne Wu, Gloria Geng, Yiwei Chen, Mustafa Omer Gul, Zizhao Chen |  - This paper introduces RESPECT, a novel method for improving large language models (LLMs) through retrospective learning from implicit feedback signals in multi-turn interactions. - RESPECT leverages user responses such as rephrased requests, expressions of frustration, or task pivots as implicit feedback signals, eliminating the need for explicit annotations or feedback solicitation.  - The method involves decoding feedback from past interactions by prompting the LLM to analyze interaction contexts and follow-up utterances. -  This decoded feedback is then used to re-train the LLM, resulting in continual improvement over multiple rounds of interaction and training. -  In a new multimodal interaction scenario called MULTIREF, where humans instruct an LLM to solve an abstract reasoning task, RESPECT demonstrates significant improvement, boosting task completion rate from 31% to 82% without external annotations. | ['Multimodal', 'Natural Language Processing', 'Reinforcement Learning'] | [Link](https://lil-lab.github.io/respect) | N/A |
| [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426) | Kang Zhao, Han Bao, Haoli Bai, Yuxuan Sun, lianlio |  - FLATQUANT, a novel post-training quantization approach, enhances the flatness of Large Language Model (LLM) weights and activations through fast and learnable affine transformations, improving quantization accuracy and reducing error propagation. - FLATQUANT employs a lightweight, block-wise training strategy over calibration data and utilizes Kronecker decomposition for efficient affine transformations, minimizing memory and computational demands. - A single kernel fusing affine transformations and quantization reduces transformation overhead, resulting in inference speedups of up to 2.3x for prefill and 1.7x for decoding compared to the FP16 baseline.  - FLATQUANT achieves state-of-the-art quantization results, including less than 1% accuracy drop for W4A4 quantization on LLaMA-3-70B, outperforming SpinQuant by 7.5%.  - The method's effectiveness is shown on various LLMs (LLaMA-2/3, 7B to 70B parameters) across tasks like language modeling and question answering, demonstrating superior accuracy and inference latency compared to other state-of-the-art techniques. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ruikangliu/FlatQuant) | N/A |
| [MedMobile: A mobile-sized language model with expert-level clinical capabilities](https://arxiv.org/abs/2410.09019) | Eric Karl Oermann, Daniel Alexander Alber, Anton Alaykin, Jaden Stryker, KrithikV | - MedMobile, a fine-tuned 3.8B parameter phi-3-mini language model, demonstrates expert-level clinical reasoning capabilities, achieving a 75.7% accuracy on MedQA (USMLE), surpassing the passing score for physicians and outperforming previous state-of-the-art sub-5B parameter models by over 20%. - MedMobile leverages chain-of-thought prompting, ensemble methods, and supervised fine-tuning, with the latter contributing an 8.4% improvement in accuracy.  - Unlike larger models, techniques such as k-shot prompting and retrieval-augmented generation did not enhance MedMobile's performance, possibly due to context window limitations, leaving potential avenues for future research.  - This model holds promise for low-resource medical settings and democratizes access to advanced language models beyond large technology companies.  - The model can be expanded to vision-language tasks by utilizing Phi-3-vision architecture. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/nyuolab/MedMobile) | [Link](https://huggingface.co/KrithikV/MedMobile) |
| [Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation](https://arxiv.org/abs/2410.13198) | Jian Xue, Peidong Wang, Michael Levit, Mohammad Sadegh Rasooli, Sreyan Ghosh |  - This paper introduces DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach to improve the performance of generative error correction (GEC) models for automatic speech recognition (ASR) systems.  - DARAG addresses limitations of traditional GEC models by augmenting training data with synthetic examples generated by prompting large language models (LLMs) and text-to-speech (TTS) models, simulating realistic ASR errors.  - It also incorporates retrieval augmentation, extracting named entities from the training data and retrieving similar entities during correction to handle novel or unknown named entities more effectively. - Experimental results on various in-domain and out-of-domain settings show that DARAG consistently outperforms baseline methods, with relative word error rate (WER) improvements of 8%-30% in in-domain and 10%-33% in out-of-domain scenarios. -  DARAG improves named entity correction and shows the benefit of using synthetic data in low-resource domain adaptation setting as well. | ['Automatic Speech Recognition', 'Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2410.13618) | Chengwei Sun, Ran Ran, Yujia Wu, Jiwei Wei, Shiym | • LoLDU is a novel Parameter-Efficient Fine-Tuning (PEFT) method that leverages Lower-Diag-Upper (LDU) decomposition to reduce the number of trainable parameters during fine-tuning. • LoLDU initializes low-rank matrices with orthogonal properties using LDU decomposition, focusing on optimizing a diagonal matrix for scaling transformations and dynamic adjustment of a scaling factor to align updates with the target matrix. • LoLDU achieves comparable performance to full fine-tuning and other PEFT methods while drastically reducing trainable parameters, sometimes down to 0.00025% of the original model. • Experimental results across various tasks, including instruction following, natural language understanding, image classification, and image generation, with models ranging from 86 million to 7 billion parameters (LLaMA2, RoBERTa, ViT, and Stable Diffusion) demonstrate LoLDU's effectiveness. • LoLDU excels in preserving pre-trained knowledge and enhancing generalization through the use of orthogonal lower and upper triangular matrices, outperforming LoRA on certain tasks while using significantly fewer parameters. | ['Computer Vision', 'Image Classification', 'Text-to-Image', 'Natural Language Processing', 'Text Generation', 'Text Classification'] | [Link](https://github.com/SKDDJ/LoLDU) | N/A |
| [BenTo: Benchmark Task Reduction with In-Context Transferability](https://arxiv.org/abs/2410.13804) | Lichao Sun, Ming Li, Hongyu Zhao, zhoutianyi | BENTO: Benchmark Task Reduction with In-Context Transferability - This paper introduces a novel benchmark reduction method called BENTO (Benchmark Task Reduction) designed to reduce the evaluation cost of Large Language Models (LLMs).  - BENTO leverages In-Context Transferability (ICT), a training-free approach to estimate the transferability between different tasks using in-context learning.  - By analyzing the ICT matrix and applying spectral clustering, BENTO identifies representative tasks that capture the overall benchmark's essence.  - The paper shows that BENTO can reduce the number of tasks in popular LLM benchmarks like MMLU and FLAN by up to 95% while maintaining evaluation accuracy within a 4% margin of the full benchmark.  - This method is significantly more efficient than existing benchmark reduction techniques as it doesn't rely on computationally expensive fine-tuning or extensive training data. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/tianyi-lab/bento) | N/A |
| [AERO: Softmax-Only LLMs for Efficient Private Inference](https://arxiv.org/abs/2410.13060) | Brandon Reagen, Nandan Kumar Jha |  - AERO, a four-step architectural optimization framework, refines existing large language models (LLMs) for efficient private inference (PI) by removing nonlinearities and reducing FLOPs. - AERO systematically removes nonlinearities such as LayerNorm and GELU, proposes using ReLU in LayerNorm-free models, and designs a Softmax-only architecture tailored for PI. - A novel entropy regularization technique mitigates entropic overload, improving the performance of the Softmax-only model. - AERO achieves up to a 4.23x reduction in communication overhead and a 1.94x speedup in latency compared to the baseline. - Experiments were conducted on GPT-2 and Pythia-70M models, trained from scratch on CodeParrot and Languini datasets, demonstrating improvements across various context sizes and model depths. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant](https://arxiv.org/abs/2410.13360) | Xiangyu Yue, Yu-Feng Li, Changsheng Li, Jiaming Han, Hoar012 |  - This paper introduces Retrieval Augmented Personalization (RAP), a framework for personalizing Multimodal Large Language Models (MLLMs) by integrating user-specific visual concepts without requiring further training. - RAP employs a key-value database to store user-provided concept information (image, name, description), retrieves relevant information using a multimodal retriever based on user input (image and/or text), and feeds both the query and retrieved information to the MLLM for personalized response generation. - A dedicated dataset is created using a pipeline that leverages Gemini to automatically generate personalized captions, descriptions, and question-answer pairs associated with user-provided visual concepts.  - Experimental results show that RAP-MLLMs, trained on this dataset using LLaVA and Phi-3V backbones, achieve superior performance in personalized image captioning and visual question answering compared to finetuning and other personalization methods, while also performing well on standard multimodal benchmarks like MMMU and InfoSeek. - RAP offers real-time concept editing and addition by updating the external database, providing flexibility and eliminating retraining needs, though performance depends on the robustness of the multimodal retriever. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Hoar012/RAP-MLLM) | N/A |
| [MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization](https://arxiv.org/abs/2410.12957) | Shengpeng Ji, Ziang Zhang, Xize Cheng, Siqi Zheng, Ruiqi Li | MuVi is a novel video-to-music generation framework that focuses on semantic alignment and rhythmic synchronization. - MuVi employs a non-autoregressive encoder-decoder architecture, using a pre-trained visual encoder and a flow-matching-based music generator. A visual adaptor connects the two modules and performs efficient compression of high-frame-rate visual features. - A contrastive music-visual pre-training scheme is introduced, utilizing negative samples from temporal shifts and random replacements to enhance rhythmic synchronization. - Experimental results demonstrate MuVi's superior performance over existing methods, achieving improvements in audio quality and temporal synchronization in generated music. | ['Text-to-Audio', 'Multimodal'] | N/A | N/A |
| [Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems](https://arxiv.org/abs/2410.13334) | Isack Lee, hbseong |  - This paper introduces PCJailbreak, a method to analyze how intentional biases in Large Language Models (LLMs), implemented for safety alignment, can be exploited to generate harmful content.  - The method involves using LLM-generated keywords representing contrasting demographic groups in prompts containing harmful requests to assess the model's susceptibility to jailbreak attacks.  - Experiments on various LLMs, including GPT models and open-source alternatives, revealed that intentional biases lead to significant differences in jailbreak success rates between marginalized and privileged groups.  - The paper also proposes PCDefense, a mitigation strategy that uses prompts to adjust biases without the need for additional inference or models, unlike Guard Models.  - The authors advocate for responsible development and deployment of LLMs, emphasizing careful consideration of safety measures to avoid unintended vulnerabilities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation](https://arxiv.org/abs/2410.13293) | Tim Oates, pdx97 |  - This paper introduces SBI-RAG, a Schema-Based Instruction Retrieval-Augmented Generation framework, for enhancing math word problem solving using a Large Language Model (LLM).  - SBI-RAG uses a schema classifier (trained on DistilBERT) to predict the problem's schema, which guides prompt creation for context retrieval using RAG and generates step-by-step solutions using Ollama Llama 3.1.  - The authors evaluate SBI-RAG on GSM8K, comparing it with GPT-4 and GPT-3.5 Turbo, using a "reasoning score" to assess solution quality.  - Results suggest SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially improving student learning.  - The approach incorporates a schema classifier, structured prompt generation, schema-relevant RAG, and a new evaluation metric. | ['Question Answering', 'Text2Text Generation', 'Natural Language Processing'] | N/A | N/A |
| [$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models](https://arxiv.org/abs/2410.13859) | Xiaoshuai Sun, Yiyi Zhou, Jiayi Ji, Gen Luo, YaxinLuo |  - This paper introduces $\gamma$-MoD, a novel mixture-of-depth (MoD) adaptation strategy for enhancing the computational efficiency of existing Multimodal Large Language Models (MLLMs). - $\gamma$-MoD employs a new metric called Rank of Attention Maps (ARank) to identify and replace redundant MLLM layers with MoD layers, dynamically allocating computational resources based on token relevance. - Two key designs, shared vision-language router and masked routing learning, are incorporated to maximize sparsity while preserving performance. - The shared router applies routing to the entire multimodal sequence for better optimization, and masked routing learning prevents critical tokens from being skipped during training. - Experiments on nine benchmarks show that $\gamma$-MoD notably reduces training and inference time while maintaining competitive performance compared to existing dense and sparse MLLMs. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment](https://arxiv.org/abs/2410.09347) | Jun Zhu, Peize Sun, Hang Su, ChenDRAG | - This paper introduces Condition Contrastive Alignment (CCA), a fine-tuning technique for autoregressive (AR) visual generation models to improve sample quality without relying on guided sampling methods like Classifier-Free Guidance (CFG). - CCA fine-tunes pre-trained models by contrasting positive and negative image-condition pairs, directly optimizing the model to achieve the desired target distribution, similar to alignment techniques used in language models. - Experimental results on LlamaGen and VAR models demonstrate significant improvement in guidance-free FID and IS scores after just one epoch of fine-tuning with CCA, achieving performance comparable to CFG while reducing sampling costs. - CCA offers a controllable trade-off between image diversity and fidelity similar to CFG by adjusting a training hyperparameter (λ), further confirming their theoretical connection in targeting the same sampling distribution. - Combining CCA with CFG can lead to further performance gains, showcasing its potential as a complementary technique for enhancing visual generation. | ['Text-to-Image', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/thu-ml/CCA) | N/A |
| [Can MLLMs Understand the Deep Implication Behind Chinese Images?](https://arxiv.org/abs/2410.13854) | Xinrun Du, Yuelin Bai, Xi Feng, zhangysk, MING-ZCH |  - This research introduces CII-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) ability to understand the implications behind Chinese images, including those deeply rooted in Chinese traditional culture. - CII-Bench includes 698 images across diverse domains and visual content types, paired with 800 multiple-choice questions to assess comprehension and reasoning abilities. - Experimental findings reveal a notable performance gap between MLLMs and humans, with models achieving a maximum accuracy of 64.4% compared to human accuracy averaging 78.2%. - A custom evaluation metric is designed using GPT-4 to better evaluate Chinese traditional painting comprehension, revealing model limitations in grasping complex cultural nuances. - Models benefit from image emotion hints in prompts, indicating ongoing struggles with emotional understanding crucial for accurate interpretation. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/MING_X/CII-Bench) | [Link](https://cii-bench.github.io/) |
| [Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key](https://arxiv.org/abs/2410.10210) | Yunlin Mao, Jintao Huang, Daoze, wangxingjun778, Yingda |  - This research introduces a technique for enhancing the long-form output generation capabilities of Large Language Models (LLMs) through minimal tuning with high-quality data. - By curating a smaller, higher-quality dataset from the existing LongWriter-6k dataset, and combining it with a small fraction of alignment data, this method demonstrates comparable performance improvements to more compute-intensive training approaches. - Notably, the new dataset requires just 3.74% of the original training data, improving tuning efficiency by effectively addressing issues with data quality such as mismatched output lengths and missing instructions in the original data. - Evaluations based on length-following score (SL) and writing quality score (SQ) show improvements across various models, including the Qwen and GLM families. - This approach provides an efficient method for enhancing long-form output generation while preserving model coherence and alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://www.modelscope.com/models/swift/MS-LongWriter-GLM4-9B-Chat), [Link](https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2-7B-Instruct), [Link](https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2.5-7B-instruct), [Link](https://www.modelscope.com/datasets/ZhipuAI/LongWriter-6k), [Link](https://huggingface.co/datasets/THUDM/LongWriter-6k), [Link](https://huggingface.co/THUDM/LongWriter-glm4-9b), [Link](https://github.com/modelscope/evalscope/tree/main/evalscope/third_party/longbench_write), [Link](https://www.modelscope.com/datasets/swift/longwriter-6k-filtered), [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-Chinese), [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-English), [Link](https://huggingface.co/THUDM/glm-4-9b) |
| [TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration](https://arxiv.org/abs/2410.12183) | Yali Wang, Yu Qiao, Kunchang Li, Shaobin Zhuang, markywg | TransAgent is a novel framework that transfers knowledge from heterogeneous vision, language, and multimodal agents to enhance the generalization of Vision-Language (V-L) foundation models like CLIP. - It leverages 11 different pre-trained agents covering various tasks and modalities, including visual recognition, dense prediction, chatbot, text encoding, multimodal generation, and captioning. - The knowledge transfer is achieved through a unified distillation framework, where a Mixture-of-Agents (MoA) gating mechanism adaptively integrates knowledge from different agents. - TransAgent achieves state-of-the-art performance on 11 visual recognition datasets, outperforming CoOp by approximately 10% on average and 20% on EuroSAT under the same low-shot setting. - All pre-trained agent models can be unloaded after distillation, resulting in efficient deployment with no need for model ensembles in the inference phase. | ['Zero-Shot Image Classification', 'Image Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/markywg/transagent) | N/A |


## Papers for 2024-10-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks](https://arxiv.org/abs/2410.12381) | Xiao Li, Guancheng Lin, Huiyu Bai, Linquan Wu, zfj1998 | - HumanEval-V is introduced; a novel benchmark designed to evaluate the visual understanding and reasoning abilities of Large Multimodal Models (LMMs) through Python code generation tasks.  - The benchmark comprises 108 coding tasks adapted from platforms like CodeForces and Stack Overflow, each requiring LMMs to integrate visual and textual information to generate functional code.  - Evaluation results for 19 state-of-the-art LMMs reveal that even leading proprietary models struggle, with GPT-4o achieving 13% pass@1, highlighting limitations in visual reasoning and coding abilities.  -  Ablation studies indicate current LMMs have limitations in vision reasoning and coding capabilities, showing significant performance improvement when image descriptions are provided. - Further analysis reveals that open-weight LMMs suffer deteriorated coding performance after vision-encoder integration, suggesting areas for future LMM research. | ['Multimodal', 'Computer Vision', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/HumanEval-V/HumanEval-V-Benchmark) | N/A |
| [VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI](https://arxiv.org/abs/2410.11623) | Sicheng Zhou, Yangyang Yu, Kechen Fang, yetian, SijieCheng |  - VidEgoThink is introduced; a benchmark designed to assess egocentric video understanding capabilities for embodied AI, focusing on bridging the gap between Multimodal Large Language Models (MLLMs) and low-level control. - It incorporates four tasks: video question answering, hierarchical planning, visual grounding, and reward modeling.  - Leverages GPT-4 to generate data automatically, which is filtered by human annotators.  This pipeline is based on the Ego4D dataset. - Experimental evaluation of various MLLMs, including GPT-4, open-source image and video-based models, reveals poor performance across all tasks, particularly in sequence and order understanding.  - Findings indicate a need for significant advancements in foundational models for first-person Embodied AI applications. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text', 'Robotics'] | N/A | N/A |
| [The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio](https://arxiv.org/abs/2410.12787) | Hang Zhang, Yang Zhou, Yun Xing, Sicong Leng, ClownRat | - This paper investigates hallucinations in Large Multimodal Models (LMMs) across language, visual, and audio modalities. - Two key contributors to hallucinations are identified: overreliance on unimodal priors and spurious inter-modality correlations. - The Curse of Multi-Modalities (CMM) benchmark is introduced, which provides a detailed analysis of these underlying issues. - CMM converts hallucination evaluation into a binary classification task with object-level and event-level probing across 1200 samples with 2400 probing questions. - Experimental results reveal key vulnerabilities, including imbalances in modality integration and biases from training data. | ['Multimodal'] | [Link](github.com/DAMO-NLP-SG/CMM) | [Link](cmm-damovl.site) |
| [Revealing the Barriers of Language Agents in Planning](https://arxiv.org/abs/2410.12409) | Kai Zhang, Siyu Yuan, jiangjiechen, kexunz, hsaest |  - This paper investigates the limitations of current large language models (LLMs) in planning tasks using feature attribution analysis.  - It identifies two key weaknesses: a limited understanding of constraints and the diminishing influence of questions as the planning horizon expands. - The study explores episodic and parametric memory updating strategies, finding that while they improve constraint and question utilization, they do not fully resolve the core issues. - The episodic memory updating reiterates constraints, making them easier for agents to recognize, but agents primarily understand it on a global level. -  Parametric memory updating enhances the impact of questions, yet agents still lose focus on them as the horizon increases; both strategies resemble shortcut learning and are insufficient for high-level planning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Exploring Model Kinship for Merging Large Language Models](https://arxiv.org/abs/2410.12613) | Huajun Chen, Shumin Deng, Ningyu Zhang, Yunzhi Yao, Yedi Hu |  - This paper introduces "model kinship", a metric to assess the similarity between Large Language Models (LLMs), drawing an analogy to biological kinship, for enhanced model merging. - It is shown empirically that model kinship correlates with performance gains after merging, which helps guide the selection of candidate models for merging and escape local optima. - A novel merging strategy, "Top-k Greedy Merging with Model Kinship", is proposed, demonstrating improved performance on benchmark datasets by mitigating performance degradation and avoiding local optima during model evolution. - The analysis of model evolution through iterative merging reveals two distinct stages: a learning stage with rapid performance improvement and a saturation stage where improvements plateau, with the latter attributed to weight space convergence and high kinship values. - Model kinship is further suggested as a criterion for early stopping in the merging process, which improves efficiency without compromising performance gains.  | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/ModelKinship) | N/A |
| [Large Language Model Evaluation via Matrix Nuclear-Norm](https://arxiv.org/abs/2410.10672) | Yi Chang, Yahan Li, WhiteCatY, xiatingyu | • This paper introduces Matrix Nuclear-Norm, a novel metric for evaluating the information compression and redundancy reduction capabilities of Large Language Models (LLMs). • The metric leverages the nuclear norm and its L1,2-norm approximation to quantify the data compression proficiency of LLMs. • Matrix Nuclear-Norm addresses the computational limitations of existing metrics like Matrix Entropy by reducing the time complexity from O(n³) to O(n²), eliminating the need for Singular Value Decomposition (SVD). • Experimental results on various LLMs, including Cerebras-GPT and Pythia, demonstrate that Matrix Nuclear-Norm effectively captures compression capabilities with values decreasing as model size increases. • Evaluations on benchmark datasets like AlpacaEval and Chatbot Arena confirm that the proposed metric reliably assesses and ranks model performance, achieving a balance between accuracy and computational efficiency. | ['Natural Language Processing'] | [Link](https://github.com/MLGroupJLU/MatrixNuclearNorm) | N/A |
| [ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs](https://arxiv.org/abs/2410.12405) | Dahua Lin, Xinyu Fang, KennyUTC, zsytony, JingmingZ | • ProSA, a framework designed to evaluate and understand prompt sensitivity in LLMs, is introduced, incorporating a novel sensitivity metric, PromptSensiScore (PSS), and leveraging decoding confidence. • PSS quantifies the average discrepancy in LLM responses when given different semantic variants of the same instruction. • The study, spanning multiple tasks and models, reveals that prompt sensitivity varies across datasets and models, with larger models generally exhibiting better robustness, and few-shot examples, especially for larger models, mitigate sensitivity. • Subjective evaluations highlight increased sensitivity in complex reasoning tasks compared to straightforward ones, with higher model confidence correlating with increased prompt robustness. • Prompt sensitivity is linked to decoding confidence, where greater confidence corresponds to higher robustness against prompt variations. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/ProSA) | N/A |
| [ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression](https://arxiv.org/abs/2410.08584) | Wenqi Shao, Jing Liu, Feng Chen, Yefei He, kpzhang996 | - ZipVL is an efficient inference framework for Large Vision-Language Models (LVLMs) that addresses computational and memory bottlenecks through dynamic token sparsification and KV cache compression. - It employs a layer-wise adaptive ratio assignment for important tokens based on attention score distribution, optimizing both prefill and decoding phases. - The prefill phase is accelerated by performing attention only on important tokens, seamlessly integrating with existing attention implementations. - Mixed-precision quantization is applied to the KV cache, using higher bit-width for important tokens and lower bit-width for others, reducing memory usage without significant performance loss. - Experiments show ZipVL accelerates prefill by 2.6x and reduces GPU memory by 50% with minimal accuracy reduction on Video-MME, outperforming fixed-ratio methods like FastV. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Video-Text-to-Text'] | N/A | N/A |
| [Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL](https://arxiv.org/abs/2410.12491) | Sonali Parbhoo, Arjun Jagota, Jared Joselowitz, skrishna | • This paper introduces a novel approach to interpreting Large Language Models (LLMs) by applying Inverse Reinforcement Learning (IRL) to recover their implicit reward functions, focusing on toxicity-aligned LLMs. • Experiments conducted on toxicity-aligned LLMs of varying sizes extracted reward models that achieved up to 80.40% accuracy in predicting human preferences. • The analysis reveals insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the Reinforcement Learning from Human Feedback (RLHF) process. • The study demonstrates that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. • The paper proposes that this work provides a new perspective for understanding and improving LLM alignment, with implications for responsible development. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |
| [WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation](https://arxiv.org/abs/2410.12722) | Juan Carlos Climent Pardo, Yingya Li, Siena Placino, João Matos, shanchen |  - WorldMedQA-V is a new multilingual and multimodal dataset designed to evaluate the performance of multimodal language models (VLMs) on medical question answering tasks. - The dataset consists of 568 multiple-choice questions with images from real medical exams in Brazil, Israel, Japan, and Spain. - Evaluations of several popular open and closed-source VLMs reveal that GPT4o achieved the best performance, generally exceeding passing thresholds across countries and both local languages and English translations. - Including the associated image with the medical question generally improves the model performance, particularly for models with lower baseline accuracies. - The results also highlight persistent language disparities, where models showed relatively lower performance on Hebrew, potentially due to underrepresentation in pre-training datasets. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/WorldMedQA/V) | [Link](https://huggingface.co/datasets/WorldMedQA/V) |
| [OMCAT: Omni Context Aware Transformer](https://arxiv.org/abs/2410.12109) | Andrew Tao, Rafael Valle, Matthieu Le, Karan Sapra, goarushi27 |  - The paper introduces OMCAT (Omni Context Aware Transformer), a novel multimodal large language model designed for enhanced temporal understanding and cross-modal alignment in audio-visual contexts.  - OMCAT leverages ROTE (Rotary Time Embeddings), a modification of RoPE (Rotary Position Embeddings), to encode absolute and relative temporal information, improving performance on time-anchored tasks. -  A new dataset, OCTAV (Omni Context and Temporal Audio Video), is also introduced, focusing on event transitions within videos and their correlation with audio cues, facilitating training for fine-grained temporal reasoning.  -  OMCAT undergoes a three-stage training process: feature alignment, instruction tuning, and OCTAV-specific training, achieving state-of-the-art results on Audio-Visual Question Answering (AVQA) and temporal video grounding benchmarks, surpassing existing models on the OCTAV dataset by a significant margin.  - The paper's contributions include a new model and dataset, demonstrating significant advancements in multimodal LLMs' capacity for fine-grained temporal and cross-modal understanding. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Tracking Universal Features Through Fine-Tuning and Model Merging](https://arxiv.org/abs/2410.12391) | Desmond Elliott, nilq |  - This paper investigates the evolution of features in one-layer Transformer language models during fine-tuning and merging. - The study uses sparse autoencoders to extract and track features across models trained on different domains (English text, Python, Lua, TinyStories). - Findings reveal that few features persist across models, but those that do are often interpretable, relating to code-related elements like punctuation and formatting. - Case studies highlight a persistent variable assignment feature and a disappearing Python exception-handling feature. - The paper contributes to understanding feature dynamics in transfer learning scenarios. | ['Natural Language Processing', 'Feature Extraction'] | N/A | N/A |
| [DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities](https://arxiv.org/abs/2410.07722) | Jeff Dalton, Iain Mackie, Sean MacAvaney, Shubham Chatterjee, Thong Nguyen | - DyVo, a novel dynamic vocabulary model, is introduced to enhance Learned Sparse Retrieval (LSR) by incorporating Wikipedia entities into the vocabulary. - The model utilizes a Dynamic Vocabulary (DyVo) head which leverages existing entity embeddings and an entity retrieval component to generate entity weights. - These weights are merged with word piece weights and used for efficient indexing and retrieval using an inverted index. - Experiments on three entity-rich document ranking datasets show DyVo consistently outperforms state-of-the-art baselines, demonstrating significant improvements over traditional LSR models by incorporating entities. - A few-shot generative entity retrieval approach using LLMs like Mixtral and GPT-4 is introduced, generating highly relevant entity candidates leading to superior performance compared to using linked entities or entities found by human annotators. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/thongnt99/DyVo) | N/A |


## Papers for 2024-10-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation](https://arxiv.org/abs/2410.11779) | Haoming Xu, Bozhong Tian, Xiang Chen, Chenxi Wang, Ningyu | This paper introduces DeCo, a novel dynamic correction decoding method for Multimodal Large Language Models (MLLMs) designed to mitigate hallucinations by leveraging information from preceding layers. - DeCo dynamically selects an appropriate preceding layer ("anchor layer") based on the probabilities of candidate tokens, and integrates its knowledge into the final layer to adjust the output logits, thereby correcting potential hallucinations. - The method is training-free and model-agnostic, compatible with various decoding strategies (greedy search, nucleus sampling, beam search) and applicable to different MLLMs. - Experimental results on benchmarks like CHAIR and POPE demonstrate that DeCo significantly reduces hallucination rates compared to baselines and existing methods like OPERA and VCD, with an average suppression rate of 10.8% on image captioning datasets. - Empirical analysis suggests that MLLMs can recognize visual objects in earlier layers, but this recognition is suppressed in later layers due to language model priors, leading to hallucinations. DeCo addresses this by correcting final-layer logits using more accurate information from preceding layers. - Further analysis shows DeCo is also effective in reducing snowballing hallucinations, where an initial hallucination leads to a cascade of errors. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/zjunlp/DeCo) | N/A |
| [MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models](https://arxiv.org/abs/2410.11710) | Xiaoshuai Song, Jiaheng Liu, Zekun Wang, Yanan Wu, Pei Wang |  - This paper introduces MTU-Bench, a multi-granularity tool-use benchmark designed to evaluate large language models' (LLMs) ability to interact with external tools.  - MTU-Bench consists of two main components: MTU-Instruct, a diverse instruction tuning dataset for training LLMs on tool usage, and MTU-Eval, a comprehensive evaluation framework featuring fine-grained metrics that assess various tool-use scenarios without relying on GPT-based evaluation.  - The authors propose a novel automated data synthesis pipeline based on existing task-oriented dialogue datasets to create MTU-Bench.  - The benchmark covers five tool usage scenes: single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks.  - Experiments demonstrate that fine-tuning LLaMA on MTU-Bench yields a robust model, MTU-LLaMA, with improved performance in various tool-use scenarios, outperforming the baseline model and demonstrating the efficacy of the MTU-Instruct dataset. | ['Natural Language Processing'] | [Link](https://github.com/MTU-Bench-Team/MTU-Bench.git) | N/A |
| [SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI](https://arxiv.org/abs/2410.11096) | Wenbo Guo, Yuheng Tang, Zhun Wang, Yuzhou Nie, yuyangy |  - This paper introduces SECCODEPLT, a unified evaluation platform designed to assess the security risks of code generation AI models concerning insecure coding and cyberattack helpfulness.  - For insecure coding, a two-stage data creation pipeline is employed, combining expert-crafted seed examples with LLM-based mutation and dynamic testing to ensure benchmark quality and scalability, covering 27 critical Python CWEs compared to existing benchmarks' 8.  - For cyberattack helpfulness, a real-world attack environment with dynamic metrics is designed to evaluate models' capabilities across different attack stages based on MITRE ATT&CK.  - Experimental results indicate SECCODEPLT outperforms CYBERSECEVAL in benchmark quality and reveals higher risks in SOTA models, including GPT-40 and Claude’s capability to generate end-to-end attacks, also uncovering risks in the code agent Cursor where it failed on code injection, access control and data leakage prevention CWEs.  - Additionally, providing security policy reminders significantly improves model performance in secure coding by 30%. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/Virtue-AI-HUB/SecCodePLT) |
| [LVD-2M: A Long-take Video Dataset with Temporally Dense Captions](https://arxiv.org/abs/2410.10816) | Zhijie Lin, Daquan Zhou, Yuqing Wang, XihuiLiu, YuuTennYi |  - This paper introduces LVD-2M, a large-scale dataset of 2 million long-take videos with temporally dense captions, designed to address the limitations of existing datasets for training long video generation models. - The dataset creation involved an automatic pipeline with low-level filtering (scene cut detection, optical flow) and semantic-level filtering (video LLMs) to select high-quality videos, and a hierarchical captioning approach combining LLaVA and Claude3-Haiku to generate detailed descriptions of video content over time. - Human evaluations show LVD-2M surpasses other datasets in long-take consistency, dynamic degree, and caption quality. - Fine-tuning experiments with both diffusion-based and LM-based video generation models demonstrate that LVD-2M improves their ability to generate longer, more dynamic videos with smoother transitions and camera motions. - The authors argue that LVD-2M will significantly benefit future research in long video generation. | ['Text-to-Video', 'Video-Text-to-Text', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/SilentView/LVD-2M) | N/A |
| [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786) | Zheyu Shen, Guoheng Sun, Shwai He, charleslipku | -"What Matters in Transformers? Not All Attention is Needed" introduces a method called "Joint Layer Drop" to efficiently prune redundant attention and MLP layers in Transformer-based language models. -The method identifies these layers using a similarity-based metric, removing those with minimal transformation between input and output, and prioritizes dropping attention layers due to their observed higher redundancy. -Experiments demonstrate that removing a substantial portion of attention layers (e.g., 50% in Llama-2-70B) leads to minimal performance degradation while significantly improving inference speed (48.4% speedup with a 2.4% performance drop). -The redundancy in attention layers is found to be consistent throughout training, suggesting it's an inherent property, and Joint Layer Drop allows for even more aggressive pruning by targeting both attention and MLP layers for increased efficiency. -This work reveals that not all attention layers are equally important in transformers, leading to potential improvements in future network architecture designs and training techniques. | ['Natural Language Processing', 'Feature Extraction'] | [Link](https://github.com/Shwai-He/LLM-Drop) | [Link](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k) |


## Papers for 2024-10-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2410.10139) | WendellZwh, wangzhaoyang, StarThomas1002, Lillianwei, richardxp888 | • MMIE is a large-scale benchmark designed to evaluate the interleaved multimodal comprehension and generation capabilities of Large Vision-Language Models (LVLMs). • The benchmark comprises 20K meticulously curated multimodal queries across diverse fields, supporting both interleaved inputs and outputs in multiple-choice and open-ended formats. • An automated evaluation metric is proposed based on a fine-tuned InternVL-2-4B scoring model, which demonstrates strong alignment with human evaluation and mitigates potential biases.  • Experimental results reveal that even state-of-the-art LVLMs and the combination of advanced LLMs with text-to-image models face significant challenges in MMIE, with most achieving moderate performance, indicating substantial room for improvement.  • Error analysis categorizes key challenges into temporal understanding (cross-modality coherence, generation adaptability) and reasoning (multimodal information comprehension, complex reasoning) skills. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://mmie-bench.github.io/) |
| [LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models](https://arxiv.org/abs/2410.09732) | Junan Zhang, Zilong Huang, beccabai, bczhou, Yejy53 | - LOKI, a new benchmark designed to evaluate large multimodal models (LMMs) on synthetic data detection across various modalities (video, image, 3D, text, and audio), has been introduced. - The benchmark includes 18K questions across 26 subcategories, with multi-level annotations including coarse-grained and multiple-choice questions, and fine-grained anomaly selection and explanation tasks. - An evaluation of 22 open-source and 6 closed-source LMMs on LOKI has revealed their potential as synthetic data detectors while also showing limitations such as model biases, a lack of expert domain knowledge, and unbalanced multimodal capabilities. - While LMMs exhibited moderate capabilities with some levels of explainability and generalization, they still lag behind human performance in synthetic data detection tasks. - Chain-of-thought prompting improved the performance of most LMMs, but not GPT-4, suggesting that GPT-4 already exhibits strong reasoning capabilities for this task. | ['Multimodal', 'Computer Vision'] | N/A | N/A |
| [Toward General Instruction-Following Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.09584) | Zhicheng Dou, Runqi Qiao, Yutao Zhu, Xiaoshuai Song, Guanting Dong |   - This paper introduces VIF-RAG, an automated, scalable, and verifiable data synthesis pipeline designed to improve instruction-following alignment in Retrieval-Augmented Generation (RAG) systems.  - VIF-RAG begins with a small set of manually crafted atomic instructions and uses a combination of rule-based composition, supervised rewriting, and code-based verification to generate a large-scale dataset (VIF-RAG-QA) of instruction-following data for RAG.  - It also presents FollowRAG, a new benchmark for evaluating complex instruction-following capabilities in RAG, composed of 2.8K samples covering 22 categories of general instruction constraints and 4 knowledge-intensive QA datasets.  - In experiments, VIF-RAG significantly boosts performance across various LLMs and datasets, demonstrating a remarkable 44% improvement over the Llama3-base model in instruction-following within RAG scenarios.  - The results further indicate that VIF-RAG not only enhances IF capability but also maintains stability in RAG performance across different model sizes and datasets, offering promise for real-world applications. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://arxiv.org/abs/2410.10563) | wenhu, yuexiang96, DongfuJiang, yuanshengni, shermansiu |   - MEGA-BENCH is a multimodal evaluation benchmark comprising over 500 real-world tasks designed to assess the diverse capabilities of contemporary vision-language models.  - The benchmark employs a taxonomy of multimodal tasks and incorporates diverse output formats, moving beyond standard multiple-choice questions to include numbers, phrases, code, LaTeX, and coordinates.  -  A range of over 40 unique evaluation metrics, including rule-based and LLM-assisted options, is used to accommodate these diverse formats.  -  In evaluations, MEGA-BENCH demonstrated GPT-4's superior performance over other flagship models, and Qwen2-VL's leading performance among open-source models.  -  The benchmark facilitates fine-grained capability analysis by offering a breakdown of model performance across various dimensions such as input/output format and required skill.  | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | N/A |
| [LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content](https://arxiv.org/abs/2410.10783) | M. Jehanzeb Mirza, Sivan Doveh, Felipe Maia Polo, Nimrod Shabtay, wlin21at |  - LiveXiv is a novel, fully automated, multimodal live benchmark focusing on scientific domains, designed to address test set contamination and provide an updated evaluation of Large Multi-modal Models (LMMs). - It uses scientific papers from arXiv to generate Visual Question Answering (VQA) and Table Question Answering (TQA) pairs automatically, avoiding human bias and ensuring scalability. - An efficient evaluation pipeline based on Item Response Theory (IRT) allows for performance estimation on new benchmark versions by reevaluating only a small subset of models, significantly reducing computational costs. - The benchmark has been evaluated with 17 prominent open and proprietary LMMs, demonstrating its challenging nature and exposing model capabilities on less-contaminated data. - It provides the first version of the dataset including VQA and TQA pairs, alongside an efficient evaluation methodology and benchmark results, along with its limitations. | ['Visual Question Answering', 'Table Question Answering', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/IBM/LiveXiv) |
| [TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models](https://arxiv.org/abs/2410.10818) | Jianrui Zhang, Reuben Tan, Mu Cai, fengyao1909, BochengZou |  - TemporalBench, a novel video understanding benchmark, is introduced to evaluate the fine-grained temporal understanding abilities of multimodal video models. - The benchmark consists of ~10K video question-answer pairs derived from ~2K human-annotated captions with rich activity details, focusing on long-range dependencies and event progression. -  State-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, significantly lower than human performance (67.9%). - A critical pitfall in multi-choice QA is identified where LLMs can detect subtle changes in negative captions and find a "centralized" description as a cue for prediction. - Multiple Binary Accuracy (MBA) is proposed to correct such bias by decomposing multi-choice QA into multiple binary QAs. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://TemporalBench.github.io/) | N/A |
| [Tree of Problems: Improving structured problem solving with compositionality](https://arxiv.org/abs/2410.06634) | Rachel Bawden, Benoît Sagot, Armel Zebaze | - This research paper proposes Tree of Problems (ToP), a novel prompting approach for enhancing the problem-solving abilities of Large Language Models (LLMs). - ToP decomposes complex problems into a tree structure of simpler, analogous subproblems, leveraging compositionality for efficient problem-solving, and drawing inspiration from techniques like divide-and-conquer. - Empirical results demonstrate that ToP outperforms existing methods like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of Thoughts (GoT) on structured tasks. - Furthermore, ToP excels in out-of-distribution generalization scenarios. - The authors provide evidence of superior performance across various LLMs, including GPT-3.5, on difficult benchmark tasks such as Last Letter Concatenation and Navigate from BIG-Bench Hard. | ['Natural Language Processing'] | [Link](https://github.com/ArmelRandy/tree-of-problems) | N/A |
| [LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813) | Kai-Wei Chang, Yuwei Zhang, Wenhao Yu, Hongwei Wang, xiaowu0162 | - LongMemEval, a comprehensive benchmark designed to evaluate the long-term memory capabilities of chat assistants.  - It focuses on five core abilities: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. - The benchmark consists of 500 meticulously curated questions embedded within freely scalable user-assistant chat histories. - A unified framework is presented that breaks down long-term memory design into four design choices across indexing, retrieval, and reading stages.  - Several memory designs, including session decomposition, fact-augmented key expansion, and time-aware query expansion, are proposed and shown to greatly improve both memory recall and downstream question answering. | ['Question Answering'] | [Link](https://github.com/xiaowu0162/LongMemEval) | N/A |


## Papers for 2024-10-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Baichuan-Omni Technical Report](https://arxiv.org/abs/2410.08565) | kenshinn, dbv, dongguosheng, TJU-Tianpengli, lin5547 |  - Baichuan-Omni is a 7B Multimodal Large Language Model (MLLM) capable of processing image, video, audio, and text modalities concurrently. - The model architecture involves separate encoders for each modality, projectors to map these encodings into the language model's embedding space, and a shared decoder. - The training process consists of two phases: multimodal alignment pre-training and multitask fine-tuning, using a diverse dataset of open-source, synthetic, and internally annotated data. - Evaluation across various benchmarks demonstrates that Baichuan-Omni outperforms existing open-source omni-modal models like VITA and achieves competitive results compared to closed-source models like GPT-4, particularly excelling in Chinese benchmarks and audio tasks. - Real-time interaction is facilitated by predicting audio input boundaries while concurrently processing visual data, enhancing dynamic attention calculation and streaming capabilities. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Video-Text-to-Text', 'Any-to-Any', 'Audio', 'Automatic Speech Recognition', 'Text Generation'] | [Link](https://github.com/westlake-baichuan-mllm/bc-omni) | N/A |
| [SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights](https://arxiv.org/abs/2410.09008) | Joseph E. Gonzalez, Minkai Xu, Tianjun Zhang, Zhaochen Yu, Ling Yang | SuperCorrect is a novel two-stage framework that leverages a large teacher model to supervise and correct the reasoning and reflection processes of a smaller student model, thereby improving mathematical reasoning and self-correction abilities.  The first stage uses hierarchical thought templates extracted from the teacher model to guide the student in generating more fine-grained reasoning thoughts.  The second stage employs cross-model collaborative direct preference optimization (DPO) to refine the student model's self-correction capabilities by following the teacher's correction traces during training.  Experimental results show that SuperCorrect achieves state-of-the-art performance among 7B models on MATH and GSM8K benchmarks, outperforming DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3%, respectively and surpasses models that are larger, such as Llama3-70B. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/YangLing0818/SuperCorrect-llm) | N/A |
| [Mechanistic Permutability: Match Features Across Layers](https://arxiv.org/abs/2410.07656) | Ian Maksimov, kefirski, elephantmipt |  - This paper introduces SAE Match, a data-free method for aligning Sparse Autoencoder (SAE) features across different layers of a neural network, addressing the challenge of understanding feature evolution and polysematicity in large language models (LLMs). - The method involves matching features by minimizing the mean squared error (MSE) between the "folded" parameters of SAEs, a technique that integrates activation thresholds into encoder and decoder weights to account for differences in feature scales. - Experiments on the Gemma 2 language model demonstrate improved feature matching quality compared to methods without parameter folding and provide insights into feature persistence and transformation across layers. - The approach also shows potential for approximating hidden states across layers, effectively skipping intermediate layers with minimal performance loss, especially in later layers where features are more monosemantic.  -  Evaluation using external LLMs and matching scores shows that feature similarity gradually declines over several layers but remains significant for approximately five layers, while initial layers appear to exhibit higher polysematicity, making feature matching more challenging in these layers. | ['Natural Language Processing', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/datasets/loubnabnl/github-small-near-dedup) |
| [Mentor-KD: Making Small Language Models Better Multi-step Reasoners](https://arxiv.org/abs/2410.09037) | SKyii, monocrat23, nokomon |  - Mentor-KD is a novel reasoning distillation framework that improves the multi-step reasoning capabilities of small language models (LLMs) by addressing the limitations of insufficient distillation sets from large LLM teachers. - It introduces a mentor model, an intermediate-sized task-specific model, to augment the distillation sets by generating additional chain-of-thought (CoT) rationales and soft labels for the student model. - Through extensive experiments, Mentor-KD has shown to improve student performance and outperform existing reasoning distillation baselines on complex reasoning tasks, including commonsense, arithmetic, logical, and symbolic reasoning. - Notably, the student models trained with Mentor-KD sometimes even surpassed the performance of the LLM teacher (GPT-3.5) on certain tasks. - The framework also proved effective in low-resource scenarios, offering performance improvements even with limited distillation sets, which showcases its cost-efficiency. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/2hojae/mentor-kd) | N/A |
| [DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models](https://arxiv.org/abs/2410.07331) | Yiming Huang, lx865712528, bjEdward, FangyuLei, Jianwen2003 |  - Introduces DA-Code, a benchmark designed to evaluate Large Language Models (LLMs) on agent-based data science tasks.  - DA-Code features challenging tasks requiring advanced coding skills, diverse real-world data sources, and complex data science programming languages (Python, SQL, Bash).  - A controllable and executable environment simulating real-world scenarios is provided, along with a meticulously designed evaluation suite and a DA-Agent baseline.  - Experimental results show that even state-of-the-art LLMs achieve only 30.5% accuracy on DA-Code, indicating significant room for improvement in LLM-agent capabilities.  - The benchmark and baseline are released to facilitate research in this area. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://da-code-bench.github.io) | N/A |


## Papers for 2024-10-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code](https://arxiv.org/abs/2410.08196) | juntingpan, shiwk20, Houxing, scikkk, AJZhou |  - MathCoder2, a new family of models, enhances mathematical reasoning in Large Language Models (LLMs) through continued pretraining on a 19.2B token dataset named MathCode-Pile, which pairs mathematical code with corresponding natural language reasoning steps. - The MathCode-Pile dataset was constructed by filtering and combining various math-related data sources, including web data, synthetic data, code using math packages, textbooks, and model-translated mathematical code. - A novel method was introduced to extract reasoning steps (conditions, LaTeX expressions, and results) from text using Llama 3.1-70B Instruct, subsequently translated into executable Python snippets. - MathCoder2-Llama-3-8B, a model from the MathCoder2 family, achieves 4-shot accuracies of 38.4% on MATH and 69.9% on GSM8K, improving upon the baseline by 3.1% and 4.1% respectively. - The complete data processing and training code, along with the dataset, is open-sourced for transparency and reproducibility. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/mathllm/MathCoder2) | N/A |
| [PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs](https://arxiv.org/abs/2410.05265) | Yi Bin, Jiahao Wang, Yi Liu, wqshao126, ChenMnZ | • PrefixQuant is a novel quantization technique for Large Language Models (LLMs) that leverages the observation that outlier tokens often appear at predictable locations or have low semantic value.  • The technique involves offline identification and prefixing of these outlier tokens in the key-value cache to prevent their generation during inference, enabling the use of per-tensor static quantization.  • This method enables static quantization to outperform the more computationally expensive per-token dynamic quantization.  • The authors demonstrate PrefixQuant's efficacy on Llama-2, Llama-3, and other LLMs, achieving perplexity improvements and accuracy gains over existing methods like QuaRot while also improving inference speed.  • For example, in a W4A4KV4 quantized Llama-3-8B model, PrefixQuant attains a 7.43 WikiText2 perplexity and 71.08% average accuracy on five common sense reasoning tasks, surpassing QuaRot by 0.98 perplexity and 5.98 accuracy points. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ChenMnZ/PrefixQuant) | N/A |
| [MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents](https://arxiv.org/abs/2410.03450) | Zongqing Lu, Xinru Xu, tellarin, yuejunpengpku | - MART (MLLM As ReTriever) is a new approach for multimodal retrieval in embodied agents, using interactive learning to fine-tune an MLLM retriever to assess trajectory effectiveness. - It leverages interaction data and preference learning to prioritize trajectories that are most beneficial for unseen tasks, addressing limitations of current retrieval methods that focus on surface-level similarities. - It introduces Trajectory Abstraction, a mechanism using MLLMs' summarization capabilities to condense trajectories while preserving key information, improving comprehension and efficiency in long-horizon tasks. - Experimental results across various environments show that MART significantly improves task success rates in unseen scenes compared to baselines, often exceeding 10% improvement. - MART offers a new paradigm for multimodal retrieval, adapting general-purpose MLLMs as retrievers for embodied agents to consider the task-specific relevance of retrieved information. | ['Multimodal', 'Robotics', 'Reinforcement Learning'] | N/A | N/A |
| [DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models](https://arxiv.org/abs/2410.08207) | akashsri, FelixXu, quandao10, ligongh, AristHe | - DICE (Discrete Inversion for Controllable Editing) is introduced as the first method to enable precise inversion for discrete diffusion models such as VQ-Diffusion, Paella, and masked generative models such as RoBERTa. - DICE enhances the editability of these models by recording noise sequences or masking patterns in the reverse sampling process, allowing for accurate reconstruction and controlled editing without reliance on predefined masks or attention manipulations. - The method's effectiveness has been demonstrated in image and text modalities.  For image editing, the experimental results on PIE-Bench using Paella show that the proposed method achieves lower structure distance while preserving background as well as competitive CLIP similarity compared to baselines including DDIM inversion with Stable Diffusion v1.4 and masked inpainting. - For text editing, using RoBERTa as the language model, DICE shows the ability to adjust a sentence’s sentiment without altering its original structure, outperforming masked generation by a large margin based on structure preservation and sentiment correctness evaluation using ChatGPT-4. - A novel text-editing dataset, Sentiment Editing, focusing on controlled sentiment adjustments in sentences while preserving their structure and theme, is presented | ['Text-to-Image', 'Image-to-Image', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Benchmarking Agentic Workflow Generation](https://arxiv.org/abs/2410.07869) | Ningyu, xiaoyuehanbin, consultantQ, Runnaning, GoooDte | WORFBENCH, a unified workflow generation benchmark featuring diverse scenarios and complex graph workflow structures, is introduced to evaluate Large Language Model (LLM) agents' ability to decompose problems into executable workflows. - WORFEVAL, a systematic evaluation protocol employing subsequence and subgraph matching algorithms, is presented to rigorously assess workflow generation capabilities. - Evaluations across different LLMs reveal performance gaps between sequence and graph planning, with GPT-4 showing a 15% gap. - Two open-source models are trained and evaluated, demonstrating improved but limited generalization on held-out tasks. - Generated workflows enhance downstream tasks by serving as Chain-of-Thought augmentation and prior knowledge, enabling superior performance with reduced inference time through parallel and shortened planning steps. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/zjunlp/WorFBench) | N/A |
| [Agent S: An Open Agentic Framework that Uses Computers Like a Human](https://arxiv.org/abs/2410.08164) | Shuyu Gan, Saaket Agashe, xw-eric, jc-y42, Jiuzhouh |  - Agent S is introduced as an open agentic framework designed for autonomous interaction with computers through a GUI, aiming to automate complex multi-step tasks. - The framework utilizes experience-augmented hierarchical planning, learning from both external web knowledge searches and internal experience retrieval to plan and execute subtasks efficiently. - It employs an Agent-Computer Interface (ACI) that improves grounding by using vision-augmented accessibility tree observations and restricts the agent's action space to enhance safety and control. - Evaluation on the OSWorld benchmark demonstrates a significant performance improvement, achieving a 9.37% higher success rate than the baseline and establishing a new state-of-the-art, with consistent improvement across five categories of computer tasks. - Further evaluation on the WindowsAgentArena benchmark reveals the framework's broad generalizability to different operating systems with an improvement from 13.3% to 18.2% on an equivalent setup without explicit adaption. | ['Multimodal'] | [Link](https://github.com/simular-ai/Agent-S) | N/A |
| [Intriguing Properties of Large Language and Vision Models](https://arxiv.org/abs/2410.04751) | Ho-Jin Choi, yechan99, mkmiracle, kobiso, passing2961 | • This paper investigates the intriguing properties of Large Language and Vision Models (LLVMs), focusing on how they perceive and process images.  • The study evaluates the performance of LLaVA-series models across 10 diverse benchmarks, including visual question answering, OCR and mathematical reasoning tasks, revealing that LLVMs process images globally despite using localized visual tokens. • The experiments show that LLVMs can solve math problems even with missing numerical details from the image, and the lower layers of the model are crucial for visual understanding while higher layers focus on text interpretation. • The research highlights LLVMs' struggle to preserve initial visual understanding capabilities after alignment and visual instruction tuning.  • It suggests that future work should focus on developing interactive evaluation benchmarks and new model architectures to improve cross-modal alignment and visual perception. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/passing2961/IP-LLVM) | N/A |
| [Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning](https://arxiv.org/abs/2410.06508) | Ye Tian, haitaominlp, Pluie1503, freesunshine0316, russwang | - ALPHALLM-CPL, a novel pairwise training framework, enhances the reasoning capabilities of Large Language Models (LLMs) through Monte Carlo Tree Search (MCTS) behavior distillation. - It leverages stepwise trajectory pairs from child nodes in the search tree, providing step-level information for effective distillation. - Curriculum preference learning dynamically adjusts the training sequence, prioritizing critical learning steps and mitigating overfitting. - Experiments on mathematical reasoning tasks (GSM8K and MATH) show substantial improvements over existing MCTS distillation methods. - ALPHALLM-CPL boosts LLaMA2-7B's accuracy on GSM8K by 150%, Mistral-7B by 48.8%, and LLaMA3-8B by 17.4% on MATH, demonstrating its effectiveness in LLM self-improvement. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality](https://arxiv.org/abs/2410.05210) | Junmo Kim, In So Kweon, Dong-Jin Kim, Jae Won Cho, ytaek-oh | - FSC-CLIP, a novel fine-tuning framework for Vision-Language Models (VLMs), enhances compositional reasoning without sacrificing performance in zero-shot multi-modal tasks. - It integrates Local Hard Negative (LHN) Loss, which uses dense alignments between image patches and text tokens to compute loss, and Selective Calibrated Regularization (SCR) to regulate hard negative supervision. - Extensive evaluations on 11 compositionality benchmarks and 21 zero-shot classification tasks show that FSC-CLIP achieves comparable compositionality to state-of-the-art methods while better preserving multi-modal capabilities and exceeding pre-trained CLIP's zero-shot classification score by +0.5 points when fine-tuned on 100k LAION-COCO samples, a substantial improvement compared to a drop of -4.9 observed in existing methods. - Additionally, FSC-CLIP demonstrates superior retrieval capabilities, particularly in counterfactual scenarios, showcasing a more nuanced understanding of compositional concepts, as evidenced by qualitative examples on COCO-Counterfactuals. - FSC-CLIP addresses the trade-off between compositionality and multi-modal task performance, common in existing fine-tuning approaches that use global hard negative losses and often lead to degraded performance in tasks like zero-shot classification and retrieval. | ['Multimodal', 'Computer Vision', 'Image-to-Text', 'Zero-Shot Classification'] | [Link](https://github.com/ytaek-oh/fsc-clip) | N/A |
| [SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe](https://arxiv.org/abs/2410.05248) | Sanqiang Zhao, Marzyeh Ghassemi, wzhouad, szhang42, YuxinXiao | - SFTMix is a novel Mixup-based recipe for Large Language Model (LLM) instruction tuning that aims to improve performance without relying on curated datasets.  - SFTMix leverages training dynamics to identify and split the training dataset into confident and unconfident subsets based on the model's perplexity. - A Mixup-based regularization is then applied, interpolating examples between these subsets to mitigate overfitting on confident examples and propagate supervision to unconfident ones. - SFTMix significantly outperforms next-token prediction (NTP) across various instruction-following tasks and healthcare-related benchmarks using different LLMs and dataset sizes.  - Ablation studies confirm the method's robustness and design choices, demonstrating its potential across NLP applications. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models](https://arxiv.org/abs/2410.06154) | aquila147, mdorkenw, paulgavrikov, sivand, kevinmzy | • GLOV is a novel method that utilizes LLMs as implicit optimizers for Vision-Language Models (VLMs), enhancing performance on downstream tasks by optimizing natural language prompts.  • It uses a meta-prompt to guide iterative prompt generation, incorporating ranked in-context examples based on a few-shot training set and explicit guidance in the embedding space using offset vectors.  • This guidance steers the LLM towards positive solutions, improving recognition performance by up to 15% and 57.5% (3.8% and 21.6% average) on dual-encoder and encoder-decoder VLMs.  •  Comprehensive evaluation on 16 diverse datasets using CLIP and LLaVa demonstrates GLOV's ability to consistently improve performance. • The method was shown to be effective even for challenging fine-grained recognition tasks using encoder-decoder models without requiring gradient-based learning. | ['Zero-Shot Image Classification', 'Image Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/jmiemirza/GLOV) | N/A |
| [Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System](https://arxiv.org/abs/2410.08115) | Cheng Yang, Chen Qian, Jiarui Yuan, zibuyu9, weizechen |  - OPTIMA, a novel framework designed to optimize Large Language Model (LLM)-based Multi-Agent Systems (MAS) by enhancing both communication efficiency and task effectiveness through LLM training.  - Employs an iterative "generate, rank, select, and train" paradigm and utilizes a reward function that balances task performance, token efficiency, and communication interpretability.  - Integrates Monte Carlo Tree Search (MCTS)-inspired techniques for DPO data generation, to explore diverse interaction paths during conversations.  - Evaluated on various multi-agent tasks, including information-asymmetric question answering and complex reasoning, OPTIMA consistently outperforms single-agent and vanilla LLM-based MAS baselines, showing significant improvements in token usage and task performance (up to 2.8x performance gain with <10% tokens).  - The efficiency gains also contribute to improved inference-time scaling laws, enhancing the overall capabilities of LLM systems. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Emergent properties with repeated examples](https://arxiv.org/abs/2410.07041) | François Charton, Knykny | This study explores the impact of training example repetition on transformer performance using generated datasets for three mathematical tasks: greatest common divisor (GCD), modular multiplication, and matrix eigenvalues. - For a fixed number of training steps, models trained on smaller datasets with repeated examples outperform models trained on larger datasets with single-use examples. - This "repetition helps" phenomenon sometimes leads to the emergence of properties learned only by models trained on smaller, repeated datasets.  - A "two-set training" approach, where a small random subset of examples is repeated more often alongside normal sampling on the rest of the training set, further improves learning speed and performance.   - The findings suggest that repetition's benefits can outweigh those of data diversity, challenging the common practice of minimizing example reuse. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations](https://arxiv.org/abs/2410.08049) | xyyue, DingXiaoH, Yiyuan |   - UniRepLKNet, a large-kernel Convolutional Neural Network (ConvNet) architecture, is proposed, challenging the dominance of Vision Transformers (ViTs) in multimodal tasks by demonstrating comparable performance with faster inference and reduced complexity. - The architecture employs a few strategically placed large kernels to efficiently capture global context, supplemented by small kernels for detailed spatial feature extraction, achieving a balance between receptive field coverage and computational efficiency. - Design principles for large-kernel ConvNets are introduced, including guidelines for kernel size selection based on task and layer depth, efficient implementation of large kernels using depth-wise convolutions, the vital role of identity shortcuts, and the use of dilated small kernels for re-parameterizing large kernels. - Experiments across diverse modalities like images, audio, video, point clouds, and time series demonstrate UniRepLKNet's superior performance. It achieves state-of-the-art results on ImageNet classification, ADE20K semantic segmentation, and a global weather forecasting task, surpassing both existing large-kernel ConvNets and recent transformer-based models. - When scaled to 1.4B parameters and pretrained on a massive dataset of 10B image-text pairs, UniRepLKNet exhibits exceptional zero-shot image recognition capabilities and competitive performance on large vision-language model benchmarks, showcasing its scalability and potential for broader applications in multimodal learning. | ['Computer Vision', 'Image Classification', 'Object Detection', 'Image Segmentation', 'Zero-Shot Image Classification', 'Audio Classification', 'Time Series Forecasting', 'Multimodal'] | [Link](https://github.com/AILab-CVC/UniRepLKNet) | N/A |


## Papers for 2024-10-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/abs/2410.05254) | Roi Reichart, Samuel Joseph Amouyal, Omer Madmon, ireinman, EilamSha |   - This paper introduces GLEE, a unified framework and benchmark for evaluating Large Language Models (LLMs) in language-based economic games like bargaining, negotiation, and persuasion.   - It parameterizes the space of these games, defines consistent evaluation metrics (self-gain, efficiency, and fairness), and provides an open-source framework for interaction simulation.   - A dataset of 7.15M LLM decisions across various game configurations and an additional human vs. LLM dataset are collected using four different LLMs.   - The framework facilitates controlled experiments across numerous game configurations and LLMs, enabling robust evaluation.   - Demonstrates the framework's utility in evaluating and comparing LLMs to human players and in quantifying the impact of economic environment parameters. | ['Natural Language Processing'] | [Link](https://github.com/eilamshapira/GLEE) | N/A |
| [Personalized Visual Instruction Tuning](https://arxiv.org/abs/2410.07113) | Jipeng Zhang, Tianyang Han, research4pan, Sterzhang, renjiepi |  PVIT (Personalized Visual Instruction Tuning) is a new training paradigm designed to enable Multimodal Large Language Models (MLLMs) to engage in personalized conversations by identifying target individuals within an image. - The framework leverages in-context learning, utilizing a multimodal prefix of <personal image, personal introduction> and personalized wrapper tokens to eliminate ambiguity. - PVIT involves an automatic framework to create training data in three stages: visual concept curation, dual-level textual information extraction and fusion, and dataset generation using LLM reasoning. - A benchmark named P-Bench, with various question types, is introduced to evaluate the personalized capabilities of MLLMs.  - Experimental results on P-Bench demonstrate that current MLLMs have limited ability for personalized conversations. P-LLaVA trained with PVIT significantly improves performance on both answerable and unanswerable question types across all input complexities, achieving an average accuracy of 96.69% for answerable questions and 99.72% for unanswerable questions on the multiple choice questions in P-Bench. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/sterzhang/PVIT) | [Link](https://huggingface.co/datasets/Sterzhang/PVIT-3M) |
| [Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate](https://arxiv.org/abs/2410.07167) | Pan Zhang, Xiaoyi Dong, lindahua, yuhangzang, shikiw | - This paper introduces the Modality Integration Rate (MIR), a new metric for evaluating the cross-modal alignment quality during the pre-training phase of Large Vision-Language Models (LVLMs). - MIR quantifies the domain divergence between vision and language features across all layers of the LLM, thus, correlates strongly with the model's post-SFT multi-modal performance and exhibits convergence behavior during pre-training, offering insights for training optimization. - Furthermore, it is robust to variations in input type and training/evaluation datasets, and generalizes across different pre-training recipes, strategies, and module designs. - A lightweight and learnable calibration module called MoCa is proposed, improving alignment between visual and textual tokens and leading to performance gains when integrated into both pre-training and SFT stages. - Experiments show that MoCa yields a 1.5% average performance increase for LLaVA-1.5 and a 0.9% increase for Mini-Gemini. | ['Multimodal'] | [Link](https://github.com/shikiw/Modality-Integration-Rate) | N/A |
| [Pixtral 12B](https://arxiv.org/abs/2410.07073) | saurabhgarg, devendrachaplot, EmmaBH, Simontwice, pragra |  - Pixtral 12B is a 12-billion parameter multimodal language model trained to understand both images and text. - It utilizes a novel vision encoder trained from scratch, allowing it to process images at native resolution, and a multimodal decoder based on Mistral Nemo 12B. - Pixtral 12B outperforms open models of similar size on multimodal benchmarks, such as Llama 3.2 11B and Qwen-2-VL 7B and even surpasses larger models like Llama 3.2 90B on certain tasks.  - It also achieves strong performance on text-only tasks, demonstrating its capability as a general purpose language model.  - The authors introduce MM-MT-Bench, an open-source benchmark to evaluate vision-language models in practical multi-turn scenarios. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/mistralai/mistral-inference), [Link](https://github.com/mistralai/mistral-evals) | [Link](https://huggingface.co/datasets/mistralai/MM-MT-Bench) |
| [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/abs/2410.05993) | JunnanLi, guoyinwang, sirius-ctrl, teowu, dxli1 | **Summary of Aria: An Open Multimodal Native Mixture-of-Experts Model:** - ARIA is an open-source, multimodal native, mixture-of-experts (MoE) model with 24.9B parameters, trained from scratch and designed for comprehensive understanding across diverse modalities. - With a visual encoder lightweight of only 438M parameters, ARIA's MoE decoder has 3.9B and 3.5B activated parameters per visual and text token, respectively, enabling efficient parameter utilization and leading to faster training and inference. It outperforms Pixtral-12B and Llama3.2-11B and is competitive with top proprietary models on various multimodal tasks. - Trained in a 4-stage pipeline, the model progressively develops capabilities in language understanding, multimodal understanding, long context (64k tokens), and instruction following. This pipeline design ensures that each stage enhances the model's capabilities while preserving the already acquired skills from the previous stages. - ARIA's training data includes 6.4T language tokens and 400B multimodal tokens, with a rigorous curation process employing a combination of rule-based and model-based filtering to maintain data quality. - Qualitative results showcases ARIA is able to integrate information across multiple modalities in complex reasoning tasks involving chart, table, text, and images understanding and show advanced coding, debugging, math, paper reading, video understanding abilities. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Text2Text Generation', 'Video-Text-to-Text', 'Question Answering'] | N/A | N/A |
| [MM-Ego: Towards Building Egocentric Multimodal LLMs](https://arxiv.org/abs/2410.07177) | HaoxuanYou, FrozzZen, edaxberger, haotiz, leoye | **Key Points:** - Introduces MM-Ego, a multimodal large language model (MLLM) designed for egocentric video understanding, featuring a novel "Memory Pointer Prompting" mechanism. This mechanism incorporates a global glimpse step, which extracts compressed visual embeddings from the entire video to gain an overarching understanding, and a fallback step, which uses higher-resolution key visual embeddings identified in the global glimpse stage to respond to questions. - Creates a 7M egocentric QA dataset, generated automatically from human-annotated video narrations from the Ego4D dataset, that ranges from 30 seconds to one hour, representing the largest egocentric QA dataset currently available. - Introduces EgoMemoria, a benchmark to evaluate egocentric video understanding capabilities with 7,026 multiple-choice questions across 629 videos ranging from 30 seconds to one hour in length, alongside a debiased metric to mitigate language bias. - In experiments, MM-Ego outperforms prior state-of-the-art models on the EgoMemoria benchmark and demonstrates competitive results on general video benchmarks like EgoSchema and Video-MME. - The Memory Pointer Prompting and data augmentation strategies show improvements even after the removal of language-biased questions, demonstrating their efficacy for the targeted task. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation](https://arxiv.org/abs/2410.07170) | Marc Peter Deisenroth, Benedikt Alkin, thomasschmied, sirluk, paischer101 |  - This paper introduces Explained Variance Adaptation (EVA), a novel data-driven initialization method for Low-Rank Adaptation (LoRA) used in fine-tuning large foundation models. - EVA computes the Singular Value Decomposition (SVD) on mini-batches of activation vectors derived from downstream data to initialize LoRA weights, maximizing explained variance and enabling adaptive rank allocation across model layers. - Experiments conducted on diverse tasks, including language generation, understanding, image classification, and reinforcement learning, demonstrate EVA's superior performance to existing initialization and rank adaptation techniques. - EVA achieves faster convergence than competitor models across multiple tasks, such as achieving higher average scores on commonsense reasoning with LLMs and even exceeding full fine-tuning performance when combined with DORA on reinforcement learning tasks. - Ablation studies confirm that both the directional components and scale obtained from SVD contribute to EVA's enhanced performance. | ['Natural Language Processing', 'Text Generation', 'Image Classification', 'Reinforcement Learning', 'Robotics'] | [Link](https://github.com/ml-jku/EVA), [Link](https://github.com/BenediktAlkin/vtab1k-pytorch), [Link](https://github.com/sirluk/peft/blob/main/examples/eva_finetuning/eva_finetuning.py) | N/A |
| [Self-Boosting Large Language Models with Synthetic Preference Data](https://arxiv.org/abs/2410.06961) | Zhifang Sui, Li Dong, thegenerality, THU-CHUNXIA, Rsy24 | SynPO, a novel self-boosting paradigm, leverages synthetic preference data for Large Language Model (LLM) alignment, eliminating the need for extensive human preference data. It employs an iterative mechanism where a self-prompt generator creates diverse prompts, and a response improver refines model responses.  After four SynPO iterations, LLMs like Llama2-8B and Mistral-7B demonstrated significant improvements, achieving over 22.1% win rate improvements on benchmarks like AlpacaEval 2.0 and ArenaHard. Moreover, SynPO boosts the general LLM performance, as evidenced by a 3.2 to 5.0 average score increase on the Open LLM leaderboard.  SynPO's self-boosting mechanism dynamically guides LLMs to refine their own outputs, effectively integrating generative rewards for preference learning. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Falcon Mamba: The First Competitive Attention-free 7B Language Model](https://arxiv.org/abs/2410.05355) | Ilyas Chahed, Dhia Eddine Rhaiem, ybelkada, yellowvm, JingweiZuo | - Falcon Mamba 7B is a new large language model based on the Mamba architecture, making it attention-free, trained on 5.8 trillion tokens. - It outperforms other open-source 7B models like Mistral 7B and Llama 3.1 8B, as well as larger models such as Falcon2 11B in benchmarks like the Open LLM Leaderboard. - Falcon Mamba 7B has faster inference speeds and lower memory usage, especially beneficial for long sequence generation due to the Mamba architecture's linear memory scaling. - The model uses an AdamW optimizer with a warmup-stable-decay learning rate schedule and is trained on a dataset mixture of web data, curated content, code, and math data. - Falcon Mamba 7B is available with a permissive license on Hugging Face, supporting functionalities such as inference, quantization, and fine-tuning. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/tiiuae/falcon-mamba-7b), [Link](https://huggingface.co/tiiuae/falcon-mamba-7b-pre-decay) |
| [Temporal Reasoning Transfer from Text to Video](https://arxiv.org/abs/2410.06166) | Chancy, PY007, yaolily, lyx97, tobiaslee | - T3 (Textual Temporal reasoning Transfer) is introduced, a method that enhances Video Large Language Models' (Video LLMs) temporal reasoning by transferring knowledge from the text domain.  - T3 creates diverse temporal reasoning tasks in text format from existing image-text datasets, addressing the lack of video samples with complex temporal scenarios.  - Without using any video data, T3 improves LongVA-7B's performance significantly, achieving a 5.3 absolute accuracy gain on TempCompass, exceeding ShareGPT4Video-8B (trained on 28,000 video samples). - The enhanced LongVA-7B achieves competitive performance on video benchmarks, e.g. 49.7 accuracy on Video-MME's Temporal Reasoning task, outperforming InternVL-Chat-V1.5-20B and VILA1.5-40B.  -  Analysis reveals a strong correlation between textual and video temporal task performance (e.g., Pearson r=0.89 on TempCompass), validating the efficacy of T3. | ['Multimodal', 'Video-Text-to-Text', 'Video Classification'] | N/A | N/A |
| [TRACE: Temporal Grounding Video LLM via Causal Event Modeling](https://arxiv.org/abs/2410.05643) | Xiaoying Tang, Mingda Li, Jingyu Liu, qingbinliu, Yongxin-Guo |  - TRACE, a novel task-interleaved video Large Language Model (LLM), is introduced for Video Temporal Grounding (VTG). It addresses the limitations of current video LLMs that rely solely on natural language generation, which lack the clear structure and information presented in videos.  - TRACE models videos as sequences of events, each with timestamps, salient scores, and captions, and leverages causal event modeling framework to represent the inherent structure of videos. - The TRACE architecture uses an interleaved sequence of task tokens for visual frames, timestamps, salient scores, and text, and employs separate encoders and decoding heads for each task. - The model also incorporates an adaptive head-switching mechanism for improved generation and achieves superior performance on various VTG tasks and datasets, outperforming current video LLMs. - TRACE improves zero-shot performance by 3.1% and 4.9% on Youcook2 (CIDEr and F1 Score), by 6.5% and 3.7% on Charades-STA (Recall with IOU=0.5 and IOU=0.7 respectively), and by 10.3% and 9.2% on QVHighlights (mAP and HIT@1). | ['Video-Text-to-Text', 'Multimodal', 'Question Answering'] | [Link](https://github.com/gyxxyg/TRACE) | N/A |
| [Data Selection via Optimal Control for Language Models](https://arxiv.org/abs/2410.07064) | Li Dong, thegenerality, Rsy24, howang, t1101675 | This paper introduces PMP-based Data Selection (PDS), a framework for selecting high-quality pre-training data for language models (LMs). PDS formulates data selection as an Optimal Control problem and leverages Pontryagin's Maximum Principle (PMP) to derive necessary conditions for optimal data selection. Experiments show that PDS accelerates LM pre-training by 2x and improves performance across various model sizes and downstream tasks, even extrapolating to 400B models trained on 15T tokens. PDS also enhances data utilization in data-constrained settings, reducing pre-training data demand by 1.8 times. This method offers a principled, theory-driven approach to data selection compared to existing heuristics, leading to more efficient and effective LM training. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/microsoft/LMOps/tree/main/data_selection) | N/A |
| [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/abs/2410.07002) | Shijin Wang, Rui Li, Qi Liu, Eviloder, TechxGenus |  - This paper introduces CursorCore, a new framework for AI-assisted programming that integrates various information sources such as coding history, current code, and user instructions for enhanced automation.  - It also presents a new benchmark called APEval (Assist Programming Eval) to evaluate models on this task and a data generation pipeline, Programming-Instruct, to create synthetic training data from diverse sources.  - This pipeline generated 219K samples to fine-tune the CursorCore models.  - The CursorCore models reportedly outperforms other models of comparable size on the APEval benchmark.  - This framework unifies applications like inline chat and automated editing, contributing to the advancement of coding assistants. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/TechxGenus/CursorCore) | N/A |
| [Response Tuning: Aligning Large Language Models without Instruction](https://arxiv.org/abs/2410.02465) | Hyounghun Kim, seokhyun |  - Response Tuning (RT) is proposed, a novel fine-tuning method that omits the instruction-conditioning step of instruction tuning, instead focusing exclusively on the supervision of response space. - RT models, trained solely on responses, exhibit helpfulness and open-ended instruction following capabilities comparable to instruction-tuned models, demonstrating the potential of response space supervision in alignment. - Refining the structural attributes of training responses leads to significant improvements in user preference for RT models, while incorporating contextual refusals into the training data allows RT models to implicitly evaluate and reject unsafe queries.  - These findings emphasize the importance of controlling response distribution in safety alignment and suggest that large language models inherently acquire many capabilities during pre-training. - In-context learning with response demonstrations only yields effective instruction-following and refusal behaviors, further strengthening the argument for the power of response supervision and highlighting the inherent potential of pretrained large language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/seokhyunan/response-tuning) | N/A |
| [ING-VP: MLLMs cannot Play Easy Vision-based Games Yet](https://arxiv.org/abs/2410.06555) | Haoran Zhang, zhangysk, CheeryLJH, EZ-hwh, Rosiness |  - This research introduces ING-VP, a novel interactive game-based vision planning benchmark designed to evaluate the spatial imagination and multi-step reasoning capabilities of Multimodal Large Language Models (MLLMs). - ING-VP comprises six distinct games with varying complexity, offering 300 levels and six unique configurations per level, leading to over 60,000 interaction rounds for a single model. - The benchmark incorporates image-text and text-only input modalities, single and multi-step reasoning settings, and conditions with and without interaction history, facilitating a comprehensive evaluation of MLLM performance. - Initial evaluations using ING-VP demonstrate that current state-of-the-art MLLMs struggle with these seemingly simple game tasks. The highest performing model, Claude-3.5 Sonnet, only achieves an average accuracy of 3.37%, significantly below human performance. - This underscores the need for further research and development to enhance MLLMs' capacity for complex spatial reasoning and planning, a crucial aspect of achieving robust artificial general intelligence. | ['Multimodal'] | [Link](https://github.com/Thisisus7/ING-VP.git) | N/A |
| [Mixed-Session Conversation with Egocentric Memory](https://arxiv.org/abs/2410.02503) | Taeyoung Kim, khh3323, jihyoung | • The paper introduces Mixed-Session Conversation, a new dialogue paradigm where a main speaker interacts with different partners across multiple sessions, promoting deeper layered interactions and complex dynamics.  • MISC, a new dataset comprising 8.5K episodes with 6 sessions and 4 speakers per episode is presented, implementing Mixed-Session Conversation and managing memories across sessions and partners from the main speaker's perspective.   • EMMA (Egocentric Memory Enhanced Mixed-session Conversation Agent), a novel dialogue model trained on MISC, facilitates seamless conversation continuity using Egocentric Memory, and allows retention of all conversational contexts across sessions and partners.   • Human evaluations validate that dialogues in MISC demonstrate seamless conversational flow even with changing partners, with EMMA exhibiting high humanness, engagingness, and memorability.  • EMMA's use of Egocentric memory retains high memorability without contradiction by connecting instances within and across sessions and tagging memory to each utterance. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://mixed-session.github.io/) |
| [FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance](https://arxiv.org/abs/2410.05791) | C. Karen Liu, Elizabeth Schumann, Haochen Shi, Pei Xu, rcwang | -   This paper introduces FürElise, a large-scale dataset of 3D hand motions and audio from 15 pianists playing 153 classical music pieces, captured using a markerless multi-view video setup and refined with MIDI data from a Disklavier piano.   - A new model is proposed to synthesize physically plausible piano playing motions from sheet music, combining a diffusion model for initial motion generation, a music-based motion retrieval method for enhancing accuracy, and reinforcement learning for physics-based bimanual control.  - The diffusion model, trained on FürElise, generates kinematic hand trajectories conditioned on sheet music, providing high-level guidance and fingering information.  - Motion retrieval augments the diffusion model's output by retrieving similar motions from FürElise based on musical similarity, improving the precision of key presses.  - The reinforcement learning policy learns to control simulated hands interacting with a piano keyboard, optimizing a combination of imitation and goal-based rewards to achieve realistic and musically accurate performance. | ['Computer Vision', 'Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs](https://arxiv.org/abs/2410.05295) | Edward Suh, huansun, someshjha, peiranli0930, ShletonLiu-N | - AutoDAN-Turbo, a novel black-box jailbreak method for Large Language Models (LLMs), automatically discovers and combines diverse jailbreak strategies using a lifelong learning approach. - This method leverages three core modules: an Attack Generation and Exploration Module, a Strategy Library Construction Module, and a Jailbreak Strategy Retrieval Module, allowing for continuous strategy discovery, evolution, and integration of human-designed strategies. - Evaluation on Harmbench and StrongREJECT benchmarks shows that AutoDAN-Turbo significantly outperforms existing methods, achieving a 74.3% higher average attack success rate and a 92.3% higher StrongREJECT score than the runner-up. - Notably, it demonstrates exceptional effectiveness on GPT-4-1106-turbo, reaching an 88.5% attack success rate, which further increases to 93.4% with the integration of human-designed strategies. - The learned strategy library exhibits strong transferability across different target models and datasets, demonstrating its robustness and adaptability in various attack scenarios. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SaFoLab-WISC/AutoDAN-Turbo) | N/A |
| [Multimodal Situational Safety](https://arxiv.org/abs/2410.06172) | xw-eric, dawnsong, acompalas, Xuandong, LCZZZZ |  - This paper introduces the novel problem of Multimodal Situational Safety, which focuses on evaluating a multimodal model's ability to consider safety aspects based on visual context. - A new benchmark called MSSBench is created to evaluate the situational safety performance of current Multimodal Large Language Models (MLLMs). - The benchmark comprises 1820 language query-image pairs across two scenarios: chat and embodied assistants, where half the images depict safe situations and the other half unsafe. - An evaluation framework analyzes key safety aspects, including explicit safety reasoning, visual understanding, and situational safety reasoning. - Results show current MLLMs struggle with recognizing unsafe situations, especially open-source models which frequently ignore safety clues.  | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](mssbench.github.io) | N/A |
| [T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design](https://arxiv.org/abs/2410.05677) | wangwilliamyang, wenhu, rpiramuthu, xfgao, jiachenli-ucsb | • T2V-Turbo-v2, a novel text-to-video (T2V) generation model, enhances post-training through incorporating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance into the consistency distillation process.  • It eliminates the target network from T2V-Turbo for improved memory efficiency and enables full model training, rather than just LORA. • It leverages motion guidance from training videos to formulate an energy function that augments the ODE solver, improving motion quality.  • Evaluated on VBench, T2V-Turbo-v2 achieves state-of-the-art performance with a Total Score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.  • Ablation studies confirm the benefits of curating specialized datasets, utilizing diverse reward models and employing motion guidance. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | [Link](https://huggingface.co/spaces/Vchitect/VBench_Leaderboard) |
| [Collective Critics for Creative Story Generation](https://arxiv.org/abs/2410.02428) | Hyounghun Kim, minwook | CRITICS is a novel framework for long-form story generation that integrates a collaborative critique mechanism to enhance story creativity and expressiveness. - It consists of two stages: CRPLAN for refining story plans and CRTEXT for enhancing story expressiveness. - Multiple LLM critics and a leader collaborate to refine story plans and enhance story texts based on criteria for creativity. - Human evaluation shows that CRITICS significantly improves story creativity and reader engagement while maintaining coherence. - It supports interactive writing, where humans can participate as any player within the framework. | ['Text Generation'] | [Link](https://github.com/EMNLP-2024-CritiCS/Collective-Critics-for-Creative-Story-Generation) | N/A |
| [Diversity-Rewarded CFG Distillation](https://arxiv.org/abs/2410.06084) | alexrame, Sper42, bachem, ferretj, aagostinelli86 |  - This paper introduces diversity-rewarded CFG distillation, a novel finetuning strategy to enhance the quality-diversity trade-off in generative models, specifically for text-to-music generation. - It combines distillation and reinforcement learning (RL) to optimize two complementary objectives; a novel CFG distillation objective and an RL with diversity reward objective. - By interpolating between the weights of two models(quality-focused and diversity-focused model), the strategy controls the quality-diversity trade-off at deployment time, further boosting performance. - Experiments on MusicLM using human evaluation validate that the model generates more diverse music samples while maintaining high quality. - The finetuned-then-merged model outperforms CFG augmentation in terms of Pareto-optimal quality and diversity, generating high-quality samples with improved diversity. | ['Text-to-Audio', 'Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [TinyEmo: Scaling down Emotional Reasoning via Metric Projection](https://arxiv.org/abs/2410.07062) | ggcristian |  - TinyEmo, a family of small Multimodal Large Language Models (MM-LLMs), is introduced for enhanced emotional reasoning and classification, integrating a synthetic emotional instruction dataset, a Metric Projector for classification, and a conditional reasoning approach. - The architecture includes a vision encoder (CLIP ViT-L/14), two projectors for classification and reasoning respectively and different LLM backbones (OpenELM, TinyLlama, Phi-2) ranging from 0.7B to 3.21B parameters. The Metric Projector is trained separately with metric learning, detaching classification from the LLM to improve efficiency and performance. - TinyEmo-700M outperforms larger state-of-the-art models like EmoVIT (7.91B parameters) with only 700M parameters on emotion classification and achieves a Zero-Shot accuracy of 57.62% when trained with data augmentation, outperforming EmoVIT's 55.57%. - A Conditional Reasoning approach, where the predicted emotion label from the Metric Projector is inserted into the prompt, leads to more accurate reasoning compared to the standard approach. - A semi-automated framework is proposed which uses the Metric Projector for interpretability and bias detection by analyzing neuron activations and embedding space robustness, showing the potential for mitigating bias and improving understanding of model behavior. | ['Multimodal', 'Image Classification', 'Visual Question Answering', 'Text Generation', 'Zero-Shot Classification', 'Zero-Shot Image Classification'] | [Link](https://github.com/ggcr/TinyEmo) | N/A |
| [F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](https://arxiv.org/abs/2410.06885) | Zhikang Niu, kaiyu-hf, ChunHuiWangFN, D-Keqi, SWivid | • F5-TTS is a fully non-autoregressive text-to-speech model based on flow matching with Diffusion Transformer (DiT) and ConvNeXt V2. • It simplifies the pipeline by removing the need for a duration model, text encoder, phoneme alignment, and semantically infused codec, using padded character sequences as input. • The model employs a novel Sway Sampling strategy during inference, improving performance and allowing for faster inference with fewer function evaluations. • Evaluation on LibriSpeech-PC, Seed-TTS test-en, and test-zh demonstrates that F5-TTS achieves state-of-the-art zero-shot performance with a real-time factor (RTF) of 0.15, outperforming existing methods in terms of both speed and quality. • Ablation studies highlight the robustness of F5-TTS, especially in handling challenging scenarios where the alignment between text and speech is crucial. | ['Text-to-Speech', 'Audio'] | [Link](https://github.com/SWivid/F5-TTS) | N/A |
| [MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders](https://arxiv.org/abs/2410.06845) | Chi Han, Qingyun Wang, May Fung, jindongwang, Cheng228 | - MentalArena is a novel self-play training framework for LLMs to improve their ability to diagnose and treat mental health disorders by generating personalized training data. - It consists of three modules: Symptom Encoder simulates realistic mental health patients, Symptom Decoder mitigates intent bias in patient-therapist dialogues, and Model Optimizer fine-tunes the LLM on the generated data. - The Symptom Encoder uses cognitive models and behavior principles of patients to produce realistic symptom descriptions. - The framework significantly outperformed several state-of-the-art and mental-health-specific LLMs, including GPT-4, on six benchmark datasets, demonstrating improvement over base models by 20.7% for GPT-3.5-turbo and 6.6% for Llama-3-8b. - Further analysis revealed a strong correlation between model performance and perplexity of the training data, and that maintaining data diversity above a certain threshold during training contributes to improved model performance. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/Scarelette/MentalArena/tree/main) | N/A |
| [Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning](https://arxiv.org/abs/2410.04223) | Jie Chen, Wojciech Matusik, Michael Sun, Gang Liu, mjiang89 | This research paper presents Llamole, a multimodal large language model (MLLM) for controllable and synthesizable molecular generation and retrosynthetic planning. Llamole integrates a base LLM with a graph diffusion transformer, graph neural networks, and A* search, allowing for the seamless generation of text, molecules, and reactions. Benchmarks on 14 LLMs of various sizes reveal the limitations of existing models in controllable molecular design and synthetic planning. Llamole shows significant improvement, increasing success rates from 5.5% to 35% and enhancing controllability by up to 80.9% across various metrics. | ['Multimodal', 'Graph Machine Learning', 'Text Generation'] | N/A | N/A |
| [Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach](https://arxiv.org/abs/2410.06949) | Minlie Huang, Yuan Yuan, Yuxuan Chen, XUANMINGZHANG |  - Seeker, a multi-agent framework leverages LLMs to enhance exception handling in code by addressing three key issues: insensitive detection of fragile code, inaccurate capture of exception types, and distorted handling solutions. - Seeker employs five agents—Scanner, Detector, Predator, Ranker, and Handler—inspired by expert developer strategies. -  A Common Exception Enumeration (CEE) document, built from trusted external experience and exception practices, is used to improve retrieval and handling. - A deep retrieval-augmented generation (Deep-RAG) algorithm is proposed to handle complex inheritance relationships between exception types. - Experimental results show that Seeker outperforms baselines on various metrics including code quality, coverage, accuracy, and edit similarity. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/XMZhangAI/Seeker) | N/A |
| [Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA](https://arxiv.org/abs/2410.06524) | Jordan Boyd-Graber, Hal Daumé III, zhoutianyi, mgor |  - This paper introduces CAIMIRA, a novel framework based on Item Response Theory (IRT) for evaluating and comparing the question-answering abilities of humans and AI systems. - CAIMIRA uses question text to infer characteristics, enabling generalization to new questions without needing prior responses and allowing for analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions. - The study finds that humans outperform AI in knowledge-grounded abductive and conceptual reasoning, while LLMs like GPT-4-TURBO excel at targeted information retrieval and fact-based reasoning. - The authors suggest future QA tasks focus on challenging higher-order reasoning, scientific thinking, nuanced linguistic interpretation, and cross-contextual knowledge application. - The implementation can be found at https://github.com/maharshi95/neural-irt | ['Question Answering'] | [Link](https://github.com/maharshi95/neural-irt) | [Link](mgor/protobowl-11-13) |
| [Does Spatial Cognition Emerge in Frontier Models?](https://arxiv.org/abs/2410.06468) | vkoltun, philkra, erikwijmans, sramakrishnan |  - The paper introduces SPACE, a benchmark for evaluating spatial cognition in large language models (LLMs) and large multimodal models.   - SPACE evaluates large-scale mapping abilities and smaller-scale reasoning about object shapes and layouts.   - The benchmark includes tasks from cognitive science, instantiated in parallel via text and images.   - Results indicate that current frontier models fall short of animal spatial intelligence, performing near chance level on several classic tests.   - The authors suggest that spatial cognition is a crucial form of intelligence, and its emergence in models is worthy of further investigation. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Zero-Shot Image Classification', 'Zero-Shot Object Detection', 'Computer Vision', 'Image Classification', 'Image Segmentation', 'Video Classification', 'Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |


## Papers for 2024-10-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LongGenBench: Long-context Generation Benchmark](https://arxiv.org/abs/2410.04199) | Peijie Dong, wenxinsiju, xuminghui, Dominic789654 | - LongGenBench, a synthetic benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs), focusing on consistency and logical flow. - It redesigns question formats, requiring LLMs to provide single, cohesive long-context answers encompassing multiple questions within a single query. - Evaluation on LongGenBench reveals performance degradation across both API-accessed and open-source LLMs in long-context scenarios, ranging from 1.2% to 47.1%. - Different LLM series show varying degradation trends, with Gemini-1.5-FLASH exhibiting minimal degradation among API-accessed models, and QWEN2 series showing minimal degradation among open-source models. - Model size influences performance decline, with larger models within a series generally demonstrating less degradation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization](https://arxiv.org/abs/2410.04717) | Francois Charton, Justin Wang, shizhuo2 | - This paper investigates the impact of instruction diversity on the generalization ability of Large Language Models (LLMs), focusing solely on instruction-following capabilities and isolating them from reasoning and knowledge retrieval. - Through controlled string rewriting experiments inspired by the Turing-complete Markov algorithm and mathematical deduction tasks, the study demonstrates that generalization to unseen instructions emerges only when training data is sufficiently diverse across semantic domains. - Findings reveal that diversifying data within limited domains does not guarantee robust generalization, while cross-domain diversification significantly enhances adaptability to new instructions. - The research further shows that increasing the diversity of training data can lead to performance improvements in real-world scenarios, including code generation and reasoning tasks with both specialized and generalist models.  - The results underscore the importance of strategic data diversification over simply increasing data size, offering guidelines for improving instruction-tuning datasets and enhancing model performance across various domains. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://arxiv.org/abs/2410.05193) | lifengshang, YuxinJiang, Tiezheng, yufeiwang201217a, DonJoey | • REVISEVAL, a novel evaluation paradigm, leverages the revision capabilities of Large Language Models (LLMs) to generate response-adapted references for evaluating text generation quality.  • It revises the generated response based on the given instruction and evaluation rubric, then uses the revised text as a reference for subsequent evaluation by either LLM-as-a-Judge or classic text evaluation metrics. • REVISEVAL outperforms reference-free and reference-based evaluation methods across various NLG and instruction-following tasks using both open-source and proprietary LLMs.  • Response-adapted references enhance the performance of classic metrics, sometimes even rivaling LLM-as-a-Judge.  • REVISEVAL effectively reduces bias in evaluation, such as verbosity and positional biases, and its effectiveness is linked to the relevance of the generated references. | ['Natural Language Processing'] | N/A | N/A |
| [MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions](https://arxiv.org/abs/2410.02743) | Yu Sun, Shuohuan Wang, Huang Fang, Haoran Sun, Yekun Chai |  - MA-RLHF, a new Reinforcement Learning from Human Feedback (RLHF) framework, is introduced to improve large language model alignment with human preferences.  It leverages "macro actions" which are sequences of tokens or higher-level language constructs.  - This approach reduces the temporal distance between actions and rewards, addressing the credit assignment problem in token-level RLHF, and facilitates faster and more accurate credit assignment.  - The model achieves substantial performance improvements across various tasks, including up to a 30% gain in summarization, an 18% gain in dialogue, and an 8% gain in question answering, while demonstrating a 1.7x-2x faster convergence compared to standard RLHF.  - MA-RLHF's robustness is highlighted through experiments conducted with different model sizes (2B to 27B) on various tasks, such as text summarization with the TL;DR dataset and dialogue generation with the HH-RLHF dataset.  - Further analysis explores termination strategies for macro actions, demonstrating the effectiveness of n-gram and parsing-based approaches in improving model performance. | ['Reinforcement Learning', 'Natural Language Processing', 'Summarization', 'Text2Text Generation', 'Question Answering', 'Text Generation'] | [Link](https://github.com/ernie-research/MA-RLHF) | [Link](https://huggingface.co/datasets/Dahoas/full-hh-rlhf) |
| [Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models](https://arxiv.org/abs/2410.03290) | Yufan Zhou, Shizhe Diao, Yu Cheng, Zhiyang Xu, WHB139426 | **-** This paper introduces Grounded-VideoLLM, a novel Video Large Language Model (Video-LLM) designed for fine-grained temporal grounding in videos.  **-** Grounded-VideoLLM uses a two-stream architecture, encoding spatial information from keyframes and temporal dynamics from multiple frames using a video encoder, to create a temporally-aware video representation. **-**  It introduces discrete temporal tokens into the LLM's vocabulary for representing timestamps efficiently, avoiding tokenization of numerical text and integrating time representations directly into the LLM.   **-** A multi-stage training approach is employed, progressing from video-caption alignment to temporal token alignment and finally multi-task instruction tuning on datasets incorporating temporal grounding tasks. **-** Experimental results demonstrate that Grounded-VideoLLM achieves state-of-the-art performance on various fine-grained temporal grounding tasks including Temporal Sentence Grounding, Dense Video Captioning and Grounded VideoQA, as well as general video understanding benchmarks. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/WHB139426/Grounded-Video-LLM) | N/A |
| [Hyper-multi-step: The Truth Behind Difficult Long-context Tasks](https://arxiv.org/abs/2410.04422) | yuyijiong |  - This paper investigates the underlying reasons why Long Context Language Models (LCLMs) struggle with complex tasks, despite their ability to handle extensive text.  - Through experiments with synthetic datasets, the study identifies "multi-matching retrieval" (retrieving multiple items simultaneously) and "logic-based retrieval" (using logic within retrieval criteria) as the core challenges, and further defines them as "hyper-multi-step" problems. - "Hyper-multi-step" implies that these seemingly simple tasks actually comprise a large number of indivisible sub-steps, which increases with context length and exceeds the processing capacity of current LCLMs.  - The paper provides empirical evidence through linear probing of hidden states and analysis of attention weights, demonstrating that these problems are more akin to complex arithmetic tasks, rather than traditional retrieval, and are therefore not adequately addressed by existing techniques such as Retrieval-Augmented Generation (RAG) or Chain-of-Thought (CoT) prompting.  - The study concludes that simply increasing the context window size of LCLMs may not suffice; instead, future research should focus on addressing the numerous steps involved and explore alternative solutions, such as using external tools. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2024-10-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Differential Transformer](https://arxiv.org/abs/2410.05258) | Li Dong, thegenerality, sunyt32, yuqxia, ytz20 | • This paper introduces the Differential Transformer (DIFF Transformer), a novel architecture for large language models (LLMs) designed to improve attention to relevant context and mitigate noise. • The core innovation is the differential attention mechanism, which calculates attention scores as the difference between two separate softmax attention maps, thus canceling noise and promoting sparse attention patterns. • Experimental results on language modeling demonstrate that DIFF Transformer outperforms standard Transformer models in various scaling settings, requiring only about 65% of the model size or training tokens to achieve comparable performance. • The model also exhibits advantages in downstream tasks such as long-context modeling, key information retrieval, hallucination mitigation, and in-context learning. • Additionally, DIFF Transformer demonstrates increased robustness to order permutation in in-context learning and a reduction in activation outliers, which presents opportunities for model quantization. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | N/A | N/A |
| [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707) | Roi Reichart, Zorik Gekhman, belinkov, tokeron, hadasor |   - This paper investigates the internal representations of large language models (LLMs) and their connection to the phenomenon of hallucinations. - The research finds that truthfulness information is highly localized within exact answer tokens, leading to improved error detection when probing these specific tokens. - The study demonstrates that while error detection is enhanced by focusing on these tokens, probing classifiers trained on one dataset often fail to generalize effectively to others, indicating that truthfulness mechanisms are skill-specific. - The authors further categorize LLM errors based on repeated sampling, showing that error types are predictable from internal representations. - Finally, they highlight a discrepancy between LLM internal encoding and external behavior, revealing that models may internally identify the correct answer but consistently generate an incorrect one, suggesting the potential for harnessing this existing knowledge to reduce errors. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/technion-cs-nlp/LLMsKnow) | N/A |
| [FAN: Fourier Analysis Networks](https://arxiv.org/abs/2410.02675) | Yongding Tao, Ge Li, Jingjingxu, zkcpku, dongyh | - This paper introduces the Fourier Analysis Network (FAN), a novel neural network architecture designed to effectively model and reason about periodic phenomena by incorporating Fourier Series into its structure and computational process. - FAN aims to address the limitations of existing neural networks, such as MLPs and Transformers, which often struggle to generalize periodic functions beyond the training data domain. - The architecture consists of stacking FAN layers where each layer outputs a concatenation of cosine, sine transformations, and an activation function applied to a linear transformation of the input. - Experimental results demonstrate FAN's superior performance compared to MLP, KAN, and Transformer on various tasks, including symbolic formula representation, time series forecasting, and language modeling tasks. - By seamlessly replacing MLP layers with FAN layers, models achieve improved generalization while reducing parameters and FLOPs. | ['Time Series Forecasting', 'Natural Language Processing'] | [Link](https://github.com/YihongDong/FAN) | N/A |
| [Presto! Distilling Steps and Layers for Accelerating Music Generation](https://arxiv.org/abs/2410.05167) | Jonah Casebeer, Ge Zhu, Njb, tberg12, ZacharyNovack |  - Presto! is a new dual-faceted distillation approach for accelerating score-based diffusion transformers by reducing sampling steps and the cost per step. - Presto includes score-based distribution-matching distillation for continuous-time diffusion (EDM) using a GAN, improved conditional layer distillation with better-preserved hidden-state variance, and combined layer-step distillation. - For step distillation, Presto-S achieves best-in-class performance among step distillation techniques and matches the original model quality with 4-step inference. - When combined with the novel layer distillation Presto-L, which independently outperforms SOTA layer dropping and base diffusion sampling, the resulting Presto-LS approach accelerates the model by 10-18x, generating 32-second mono audio in 230ms and stereo audio in 435ms on an A100 40GB GPU, outperforming Stable Audio Open by 15x. | ['Audio', 'Text-to-Audio'] | [Link](https://presto-music.github.io/web/) | N/A |
| [Named Clinical Entity Recognition Benchmark](https://arxiv.org/abs/2410.05046) | Clément Christophe, Tathagata Raha, Muhammad Umar Salman, Marco AF Pimentel, Wadood M Abdul | - This paper introduces a Named Clinical Entity Recognition (NER) benchmark designed for evaluating language models in healthcare. - This benchmark encompasses a curated selection of publicly accessible medical datasets with standardized entities adhering to the Observational Medical Outcomes Partnership (OMOP) Common Data Model. - The leaderboard accommodates various language model architectures, including encoder, decoder, and GLiNER models, and employs standardized evaluation metrics, predominantly the F1-score, to ensure consistent performance comparisons. - Initial findings from the leaderboard indicate superior performance by GLiNER-based models over decoder-only architectures, commonly used in Large Language Models (LLMs). - The choice of evaluation strategy, token-based or span-based, has been found to influence model ranking. | ['Natural Language Processing', 'Token Classification'] | [Link](https://github.com/WadoodAbdul/clinical_ner_benchmark) | [Link](https://huggingface.co/m42-health/clinical_ner_leaderboard), [Link](https://huggingface.co/spaces/m42-health/clinical_ner_leaderboard) |
| [TLDR: Token-Level Detective Reward Model for Large Vision Language Models](https://arxiv.org/abs/2410.04734) | Rui Wang, Tong Xiao, tbpangolin, pzzhang, deqing |  - This paper introduces TLDR, a novel token-level reward model designed to improve the performance and interpretability of large vision-language models (VLMs). - The TLDR model assigns rewards to individual tokens rather than entire sequences, enabling finer-grained feedback and more precise identification of errors, like hallucinations. - A perturbation-based method is used to generate synthetic hard negatives for training TLDR, enhancing its robustness. - Experiments demonstrate that TLDR significantly improves VLM performance in various tasks and reduces human annotation time by approximately threefold. - The study shows that the proposed model speeds up human annotation by 3 times in acquiring high-quality vision-language data. | ['Multimodal', 'Image-Text-to-Text', 'Reinforcement Learning'] | N/A | N/A |
| [UniMuMo: Unified Text, Music and Motion Generation](https://arxiv.org/abs/2410.04534) | Yutong Zhang, Kun Su, Han Yang, auspicious3000, Jiaben |   - UniMuMo is a unified multimodal model that uses a transformer-based encoder-decoder architecture to generate music, motion, and text from any combination of the three modalities as input. - The model bridges the modalities through a unified encoder-decoder architecture after converting inputs to a token-based representation and addresses the lack of time-synchronized data by aligning unpaired music and motion data based on rhythmic patterns and using existing large-scale datasets of single modalities.  - It utilizes a music codebook to encode motion and introduces a music-motion parallel generation scheme. - This design unifies all music and motion generation tasks into a single transformer decoder architecture with one training task of music-motion joint generation and can be efficiently achieved by fine-tuning existing pre-trained single-modality models. - Extensive evaluations shows that UniMuMo achieves competitive results across all unidirectional generation benchmarks including text-to-music, music-to-motion, motion-to-music, music captioning and motion captioning. | ['Multimodal', 'Text-to-Audio', 'Text-to-Video', 'Audio-to-Audio', 'Audio-to-Audio', 'Video-Text-to-Text'] | [Link](https://hanyangclarence.github.io/unimumo_demo/) | N/A |
| [LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning](https://arxiv.org/abs/2410.02884) | Tong Che, Jingdi Lei, schrodingers-tiger, jwu323, qq8933 | LLaMA-Berry is a new framework for enhancing the mathematical reasoning ability of Large Language Models (LLMs) by combining Monte Carlo Tree Search (MCTS) with iterative Self-Refine and a pairwise reward model. - The framework uses Self-Refine applied to MCTS (SR-MCTS) to optimize the reasoning path by leveraging the self-critic and rewriting capabilities of LLMs. - A Pairwise Preference Reward Model (PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is used to evaluate different reasoning paths globally. - An Enhanced Borda Count (EBC) method synthesizes pairwise preferences between solutions into a global ranking score to identify better answers. - Experimental results on benchmarks like GSM8K, MATH, AIME24, AMC23, and GPQA Diamond demonstrate that LLaMA-Berry significantly improves the performance of LLaMA-3.1-8B, achieving results competitive with GPT-4 Turbo without additional training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs](https://arxiv.org/abs/2410.04698) | cxiong, lunshi, hendrydong, yuhuixu, demolei | **- MATHHAY: An automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs.** **- Unlike previous benchmarks, MATHHAY requires both information retrieval and complex mathematical reasoning, focusing on real-world scenarios within a specified time period.** **- Includes questions of varying difficulty levels across different input lengths (32K, 64K, 128K) and utilizes a combination of rule-based exact matching and LLM-based judgment for evaluation.** **- Experimental results reveal that even top-performing LLMs like Gemini struggle with long contexts in mathematical reasoning, indicating room for improvement.** **- Open-source models significantly underperform compared to closed-source models.** | ['Question Answering'] | N/A | N/A |
| [TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles](https://arxiv.org/abs/2410.05262) | siminniu, fan2goa1, WinfredShi, Ki-Seki, Duguce |  - TurtleBench is a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) using real user guesses from an online Turtle Soup Puzzle game. - This dynamic approach creates a bilingual dataset (Chinese and English) with 1532 annotated user guesses, which are then used to test the reasoning abilities of the LLMs.  - The benchmark emphasizes reasoning ability and minimizes reliance on memorization and background knowledge.  - Nine advanced LLMs, including open and closed-source models, were tested on TurtleBench.  - The results show that Claude-3.5-Sonnet and GPT-4 performed best but that OpenAI's o1 series models performed sub-optimally. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/mazzzystar/TurtleBench) | N/A |
| [Grounding Language in Multi-Perspective Referential Communication](https://arxiv.org/abs/2410.03959) | alsuhr, mao1207, ZinengTang | This paper introduces a new task and dataset for evaluating referring expression generation and comprehension in multi-agent embodied environments. The dataset, comprising 2,970 human-written referring expressions, requires agents to consider each other's perspective when generating and understanding references to objects.  The authors find that model performance lags behind that of human agents in both generation and comprehension tasks.  A speaker model fine-tuned using communicative success significantly improves performance, surpassing even a strong proprietary model (GPT-40). The contributions include a novel platform for generating 3D scenes, a new dataset, and analysis of language strategies in embodied referential communication. | ['Multimodal'] | [Link](https://github.com/zinengtang/MulAgentRef) | N/A |


## Papers for 2024-10-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Addition is All You Need for Energy-efficient Language Models](https://arxiv.org/abs/2410.00907) | Wei Sun, luohy | - The paper proposes a novel linear-complexity multiplication (L-Mul) algorithm to approximate floating-point multiplication with integer addition, aiming to reduce energy consumption in large language models (LLMs). - L-Mul replaces expensive floating-point multiplications with less energy-intensive integer additions and introduces an offset to maintain accuracy. - The authors claim L-Mul achieves higher precision and requires less computation compared to 8-bit floating-point multiplications and 80% energy reduction for dot products. - Experiments on various LLMs and tasks (MMLU, BBH, GSM8k, visual question answering) showed that L-Mul in attention layers maintained or even slightly improved performance compared to standard multiplication and outperformed float8 with training free setting. - Fine-tuning models with all multiplications replaced by 3-bit L-Mul achieved comparable results to models using float8_e4m3 accumulation, showcasing its potential for efficient LLM training and deployment. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [NL-Eye: Abductive NLI for Images](https://arxiv.org/abs/2410.02613) | Zorik Gekhman, yonatanbitton, nitay, tokeron, MorVentura |  - NL-EYE, a benchmark designed to evaluate the visual abductive reasoning skills of Visual Language Models (VLMs), is introduced. - NL-EYE tasks models with evaluating the plausibility of hypothesis images given a premise image, requiring explanations for their choices and consisting of 350 image triplets across six reasoning categories: physical, functional, logical, emotional, cultural, and social. - Results show that while humans perform well, VLMs struggle, often failing to surpass random baselines in plausibility prediction. - Even with correct predictions, VLM explanations are frequently unhelpful, indicating weaknesses in visual interpretation and accurate representation generation for reasoning. - Further analysis suggests that VLMs face challenges with temporal reasoning, absolute judgments, and non-correlational tasks, particularly emotional reasoning. | ['Multimodal', 'Computer Vision'] | N/A | N/A |
| [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703) | Yossi Matias, Matan Kalman, yanivle | -"Selective Attention" is introduced; a parameter-free adjustment to the standard attention mechanism in Transformers, enabling a token to deem another as no longer relevant for future tokens and masking it, improving language modelling performance across various model sizes and context lengths. -It allows for reduction in the attention context buffer size without quality loss, resulting in significant memory and compute savings during inference, achieving up to 16X, 25X, and 47X memory reduction for context sizes of 512, 1024, and 2048 respectively with a 100M parameter model trained on C4. -Selective attention transformers often outperform standard transformers with ~2X more parameters and heads in their attention module. -Visualizations show selective attention exhibiting dynamic context pruning behavior; masking previous assignments to the same variable in variable assignment, masking ambiguous inputs until ambiguity resolution, and retaining only necessary elements in tasks like Parity and Copy. -Evaluation on C4 dataset shows consistent perplexity improvements across different model sizes and context lengths; further improvements via explicit loss to encourage masking, and HellaSwag benchmark reveals consistent accuracy gains across various model sizes using selective attention. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise](https://arxiv.org/abs/2410.03017) | Susanna Loeb, ddemszky, carlycodes, Analu, rose-e-wang |  - This paper introduces Tutor CoPilot, a Human-AI system designed to enhance real-time tutoring in K-12 education by providing expert-like guidance to tutors as they interact with students.  - Tutor CoPilot leverages the Bridge method, which captures expert decision-making patterns and adapts Large Language Models (LLMs) to generate contextually relevant suggestions for tutors during live sessions.  - A randomized controlled trial involving 900 tutors and 1,800 K-12 students demonstrates that Tutor CoPilot significantly improves student learning outcomes, particularly for students with lower-rated tutors.  - Analysis of over 550,000 chat messages reveals that tutors using Tutor CoPilot are more likely to employ high-quality pedagogical strategies that foster student understanding and less likely to simply provide answers.  - Tutor CoPilot offers a scalable and cost-effective solution ( $20 per tutor annually) for enhancing tutoring quality, especially in under-served communities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760) | David Bau, Samuel Marks, sfeucht, RohitGandikota | - This research introduces Erasure of Language Memory (ELM), a novel method for removing specific concepts from large language models (LLMs) while preserving fluency and general knowledge. - ELM employs a multi-objective fine-tuning approach with targeted low-rank updates (LoRA). - The method optimizes for erasure of the target concept, retention of unrelated information, and generation fluency when prompted with the erased concept. - Experiments on biosecurity, cybersecurity, and literary domains demonstrate ELM’s efficacy in achieving near-random performance on erased topics while maintaining high scores on general knowledge benchmarks and generating more fluent text than baseline methods. - ELM also exhibits robustness against adversarial attacks, further highlighting its potential for safe and controlled LLM editing. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/rohitgandikota/erasing-llm) | [Link](https://huggingface.co/cais/Zephyr_RMU) |
| [CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction](https://arxiv.org/abs/2410.01273) | wpiioos, Unmanned-YuBeen, lastdefiance20, PurpleSand, MilkClouds |  - CANVAS, a novel framework for intuitive human-robot interaction, is introduced for commonsense-aware navigation. It combines visual and linguistic instructions to generate robot actions, leveraging pre-trained vision-language models (VLMs) to achieve this. - A new dataset called COMMAND, containing 48 hours of driving data over 219 kilometers with human-annotated instructions and navigation outcomes across office, street and orchard simulated environments, was collected to train and test the model. - Experimental results show that CANVAS consistently outperforms the rule-based ROS NavStack in all environments, especially in challenging scenarios like uneven terrain or misleading instructions, with higher success and lower collision rates. - CANVAS achieves successful Sim2Real transfer with a 69% success rate in a real-world office setting, demonstrating its robustness beyond simulated data. - Ablation study confirms that using pre-trained VLM weights improves performance considerably, indicating the usefulness of existing knowledge for navigation tasks. | ['Robotics', 'Multimodal'] | N/A | N/A |


## Papers for 2024-10-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](https://arxiv.org/abs/2410.02740) | Chen Chen, Vasileios Saveris, haotiz, Hong-You, jefflai | This paper investigates the role of large-scale image-caption data in pre-training multimodal foundation models, particularly focusing on the interplay between synthetic captions and original AltText. - It proposes a controllable and scalable captioning pipeline capable of generating diverse caption formats (short, descriptive, dense, AltText-fused). - Experiments across CLIP, multimodal LLMs, and diffusion models reveal that a hybrid approach, combining synthetic captions and AltText, often outperforms using synthetic captions alone.  - Different model types exhibit preferences for specific caption formats: shorter captions for CLIP, descriptive for multimodal LLMs and diffusion models. - Combining AltText with synthetic captions enhances performance, likely due to improved image-text alignment from synthetic captions and increased data diversity from AltText. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification'] | N/A | N/A |
| [Video Instruction Tuning With Synthetic Data](https://arxiv.org/abs/2410.02713) | Wei Li, Chunyuan24, liuziwei7, kimingng, ZhangYuanhan |  - This paper introduces LLaVA-Video, a large multimodal model for video understanding, and LLaVA-Video-178K, a synthetic dataset created for video instruction following. - LLaVA-Video-178K consists of 178,510 videos with 1.3 million instruction samples including detailed captions generated with a recurrent, multi-level approach, along with open-ended and multiple-choice question answering generated using GPT-4. - The model leverages a SlowFast video representation technique to optimize the balance between frame count and limited GPU memory, enabling processing of three times more frames than traditional methods. - LLaVA-Video achieves state-of-the-art results on various video benchmarks, outperforming existing open-source models and demonstrating the effectiveness of the proposed synthetic dataset and training approach. - The dataset, codebase, model checkpoints, and a visual chat demo are publicly released to foster development of general-purpose visual assistants. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/datasets/lmms-lab/VideoDetailCaption) |
| [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712) | Chunyuan24, henghuang, thughost, russwang, txiong23 | **-** LLaVA-Critic is the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess the performance of other multimodal models across various tasks.  **-** It leverages a new high-quality critic instruction-following dataset incorporating diverse evaluation criteria and scenarios, including pointwise scoring and pairwise ranking.  **-** The model shows strong performance as an LMM-as-a-Judge, generating evaluation scores and rankings comparable to commercial GPT models.  **-** In preference learning, LLaVA-Critic generates effective reward signals for iterative Direct Preference Optimization (DPO), surpassing rewards from human feedback as seen in LLaVA-RLHF.  **-** LLaVA-Critic is open-sourced, including its data, code, checkpoints, and demo. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [Contrastive Localized Language-Image Pre-Training](https://arxiv.org/abs/2410.02746) | Marcin Eichner, Xinze Wang, haotiz, jefflai, Hong-You | - CLOC is a new pre-training framework for vision encoders with enhanced localization capabilities. - It augments the CLIP loss with a region-text contrastive loss and a lightweight prompter module that extracts region embeddings from the image embedding given spatial hints. - A visually-enriched and spatially-localized captioning pipeline is designed to generate region-text pseudo-labels at scale, resulting in a two-billion image-text dataset with fine-grained region-text annotations. - CLOC consistently outperforms CLIP on 31 evaluation tasks, including standard image-text tasks, newly constructed region-text tasks, and downstream evaluations with MLLMs, particularly on referring and grounding tasks. - The enhanced localization capabilities of CLOC enable it to be a drop-in replacement of CLIP to enhance MLLMs. | ['Multimodal', 'Image Classification', 'Image Feature Extraction', 'Visual Question Answering', 'Zero-Shot Image Classification'] | N/A | [Link](https://huggingface.co/datasets/zzliang/GRIT) |
| [Large Language Models as Markov Chains](https://arxiv.org/abs/2410.02724) | Abdelhakim Benechehab, Oussama Zekri, ievred, NBoulle, ambroiseodt |  - This paper draws an equivalence between large language models (LLMs) and Markov chains, offering a new theoretical framework to analyze LLM inference.  - By representing LLMs with vocabulary size *T* and context window *K* as Markov chains on a state space of size O(*T*<sup>*K*</sup>), the authors derive findings on stationary distribution, convergence speed, and temperature influence.  - The paper derives generalization bounds for pre-training and in-context learning under minimal assumptions, using concentration inequalities for dependent random variables and leveraging insights from the Markov chain equivalence.  - The theoretical analysis predicts in-context scaling laws that are experimentally validated on recent LLMs (2023-2024), showing that LLMs outperform minimax optimal frequentist Markov chain learning.  - Experimental results on various Markov chains and dynamical systems further support the theoretical findings and demonstrate the practical implications of the proposed framework. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling](https://arxiv.org/abs/2409.19291) | Yu Cheng, Jihai Zhang, Spico, Xiaoye08 | -  This paper introduces Diversified Multiplet Upcycling (DMU), a novel method for enhancing the Contrastive Language-Image Pre-training (CLIP) model by integrating it with a Mixture of Experts (MoE) architecture. DMU fine-tunes multiple CLIP models from a pre-trained checkpoint using Multistage Contrastive Learning (MCL) to capture diverse feature distributions. These fine-tuned models, sharing parameters except for the Feed-Forward Network, are then used to initialize a CLIP-MoE. The approach significantly improves CLIP's performance on various zero-shot tasks, including retrieval and image classification, as well as in downstream Multimodal Large Language Model (MLLM) benchmarks when serving as a vision encoder. Notably, CLIP-MoE surpasses the base OpenAI CLIP model by approximately 20% on retrieval tasks and exhibits minimal additional training overhead, using only 2% of the computational resources required to train a CLIP from scratch. This method provides a model-agnostic and computationally efficient way to scale CLIP and enhance its ability to capture rich, fine-grained information for improved performance in various multimodal applications. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/OpenSparseLLMS/CLIP-MOE) | N/A |
| [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367) | Jun Zhu, Pengle Zhang, Jia wei, Jintao Zhang, surfingtomchen | - SageAttention, a novel post-training quantization method designed to accelerate attention in Transformer models by quantizing tensors to 8-bit integers. - It overcomes the challenges of accuracy degradation in existing methods by smoothing the K matrix to mitigate outlier effects and employing a low-precision FP16 accumulator for the PV matrix multiplication. - It integrates effective kernel fusion with ROPE and an online softmax inspired by FlashAttention. - Comprehensive experiments demonstrate a 2.1x speed improvement over FlashAttention2 and 2.7x over xFormers on an RTX 4090. - It maintains comparable end-to-end metrics across diverse applications, including language, image, and video generation models. | ['Text-to-Image', 'Text-to-Video', 'Text2Text Generation', 'Image Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?](https://arxiv.org/abs/2410.02115) | Jianye Hou, Baibei Ji, Juntao Li, Keyan Zhou, ZetangForward | • L-CiteEval, a new multi-task benchmark for evaluating long-context understanding with citations in large language models (LLMs) is introduced. • The benchmark comprises 11 diverse tasks with context lengths ranging from 8K to 48K tokens and employs automatic evaluation metrics for reproducibility. • Evaluation of 11 LLMs reveals that open-source models lag significantly behind closed-source counterparts in citation accuracy, suggesting reliance on inherent knowledge rather than provided context. • Retrieval-Augmented Generation (RAG) improves faithfulness in open-source LLMs but slightly diminishes generation quality. • A strong correlation is observed between LLMs' attention mechanisms and citation generation process, offering insight into LLM evaluation and development. | ['Question Answering', 'Summarization', 'Natural Language Processing'] | [Link](https://github.com/ZetangForward/L-CITEEVAL.git) | N/A |
| [Training Language Models on Synthetic Edit Sequences Improves Code Synthesis](https://arxiv.org/abs/2410.02749) | Rob Fergus, lerrel, upiter | - LintSeq, a synthetic data generation algorithm, refactors existing code into edit sequences to improve code synthesis in large language models (LLMs). - LLMs trained on this data produce more diverse programs, resulting in better inference-time scaling for benchmark pass rate. - Tiny (150M parameter) edit sequence LMs achieve state-of-the-art performance for their model class, matching or outperforming models twice their size. - Repeated sampling from smaller edit sequence finetuned LLMs achieves HumanEval coverage competitive with GPT-4 at similar cumulative inference cost to single samples from large open-source LLMs. - Ablating linter guidance from LintSeq degrades downstream performance. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Feature Extraction'] | [Link](https://github.com/upiterbarg/lintseq) | N/A |
| [Distilling an End-to-End Voice Assistant Without Instruction Training Data](https://arxiv.org/abs/2410.02678) | Michael Ryan, Ella Li, zyanzhe, missblanchett, WillHeld | **Summary of "Distilling an End-to-End Voice Assistant Without Instruction Training Data"**  - This paper introduces DiVA, a new speech large language model (LLM) trained through knowledge distillation from a text-based LLM, eliminating the need for explicit instruction-following data. DiVA utilizes a novel cross-modal context distillation method, which uses a frozen text-based LLM to guide the audio model's training by matching the output distribution from text transcripts of the audio. The audio input is processed using Whisper for feature extraction and a Q-Former initialized from Whisper's decoder to achieve audio-text feature alignment. - DiVA generalizes well to various spoken language tasks such as Spoken Question Answering, Classification (emotion, humor, and sarcasm detection), and Translation, using only ASR data for training. - In evaluation benchmarks, DiVA outperforms other open-access Speech and Audio LLMs on question answering by a significant margin despite using substantially less compute for training. - DiVA excels in following text-based instructions provided through prompts and user's speech, addressing the "forgetting" issue observed in other models trained using supervised fine-tuning.  - In a user study, DiVA received a 72% preference rate compared to Qwen 2 Audio, demonstrating its effectiveness in real-world scenarios despite some limitations like inheriting the base LLM's bias. | ['Multimodal', 'Audio', 'Automatic Speech Recognition', 'Question Answering', 'Translation'] | N/A | N/A |
| [Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos](https://arxiv.org/abs/2410.02763) | Jianrui Zhang, yjlee0222, mucai |  - This paper introduces Vinoground, a novel temporal counterfactual benchmark for evaluating Large Multimodal Models (LMMs) on dense temporal reasoning in short videos. - Vinoground contains 1000 short video and caption pairs with captions containing the same words but in different orders to create temporal counterfactuals. - The benchmark evaluates an LMM’s ability to distinguish temporal differences between actions and object transformations (e.g., "water turning into ice” vs. “ice turning into water”). - Experimental results show that even state-of-the-art LMMs struggle with temporal reasoning, with the best model (GPT-40) achieving only 54% accuracy on text score and much worse on other metrics, while human performance is around 90%. - All open-source models and CLIP-based models perform much worse, suggesting that existing methods struggle at fully understanding video temporality. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://vinoground.github.io) | N/A |
| [Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data](https://arxiv.org/abs/2410.02056) | manocha, ctnzr, rafaelvalle, ZhifengKong, SreyanG-NVIDIA | Synthio is a novel approach to augment small-scale audio classification datasets using synthetic data generated from text-to-audio (T2A) diffusion models, aligning the generated data with the target dataset's acoustic characteristics through preference optimization. - It addresses the challenge of creating diverse synthetic augmentations by introducing MixCap, a technique that leverages Large Language Models (LLMs) to generate and refine meaningful audio captions used for prompting the T2A model. - Synthio's evaluation across ten datasets and four limited-data settings demonstrates consistent outperformance of existing baselines, improving classification accuracy by 0.1% to 39% using a T2A model trained solely on weakly-captioned AudioSet. - Ablation studies show the vital role of preference optimization and MixCap in achieving optimal results. - Additional analysis demonstrates effectiveness of Synthio in enhancing captioning tasks and addressing long-tail categories. | ['Audio', 'Audio Classification', 'Text-to-Audio'] | [Link](https://github.com/Sreyan88/Synthio) | N/A |


## Papers for 2024-10-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging](https://arxiv.org/abs/2410.01215) | Xiaodong Gu, Chengcheng Wan, Songsong Wang, YerbaPage |  - MGDebugger, a hierarchical code debugger, is introduced to improve the pass rate of LLM-generated code by addressing bugs at multiple levels of granularity.   - MGDebugger decomposes code into subfunctions, debugs them iteratively in a bottom-up manner, and uses an LLM-simulated Python executor to track variable states for precise error identification.   - Experiments show that MGDebugger significantly outperforms existing debugging systems, achieving an 18.9% accuracy improvement over seed generations in HumanEval and a 97.6% repair success rate in HumanEval-Fix.  - Ablation studies confirm the effectiveness of hierarchical debugging, and further analysis highlights the robustness of MGDebugger across diverse bug types, code lengths, and debugging attempts.  - MGDebugger leverages pretrained LLMs for debugging, eliminating task-specific retraining for a lightweight and scalable solution. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/YerbaPage/MGDebugger) | N/A |
| [Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis](https://arxiv.org/abs/2409.20059) | nunonmg, PierreColombo, CelineH, emmanuelmalherbe, hgissbkh | This paper conducts an empirical analysis of preference-based alignment techniques for enhancing large language model (LLM)-based translation, focusing on Contrastive Preference Optimization (CPO). - CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data regarding alignment metrics, like xCOMET-QE. - Preference-based alignment is highly sensitive to the choice of candidate translation systems used for generating preference data, affecting both the alignment metric and downstream metric consistency. - Aligning a model using its own translations achieves performance comparable to employing multiple external systems, ensuring better metric consistency.  - The paper also finds that preference-based lexical alignment using the gold reference as the preferred translation performs poorly.  - Optimizing preference data in a mono-system setting, specifically setting the quality of the chosen and rejected translations, allows the model to match the performance of multi-system settings. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://huggingface.co/collections/artefactory/translation-alignment-analysis) |
| [LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/abs/2410.01744) | Zhihan Zhang, Tianqing Fang, Mengzhao Jia, kaixinm, wyu1 | - LEOPARD, a Multimodal Large Language Model (MLLM), specializes in handling text-rich, multi-image tasks, addressing the limitations of existing MLLMs in this area by focusing on high-quality instruction tuning data and image resolution. - A new dataset, LEOPARD-INSTRUCT, comprising 925K samples, including 739K designed for text-rich, multi-image scenarios, is introduced to train the model. The dataset focuses on real-world domains like multi-page documents, multi-charts, and webpage snapshots. - An adaptive, high-resolution, multi-image encoding module dynamically optimizes the visual sequence length based on image dimensions using pixel shuffling for compression, enabling processing of multiple high-resolution images without information loss. - Experiments conducted on 13 benchmarks demonstrate LEOPARD's superior performance in text-rich multi-image benchmarks with a +9.61 point improvement over other open-source MLLMs. - The model remains competitive on single image and general-domain tasks, highlighting the benefits of training on high-quality, tailored multi-image datasets | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | [Link](https://github.com/Jill0001/Leopard) | N/A |
| [Not All LLM Reasoners Are Created Equal](https://arxiv.org/abs/2410.01748) | Aaron Courville, Daniel Toyama, Alessandro Sordoni, agarwl, arianhosseini |  - This paper investigates Large Language Models' (LLMs) reasoning abilities on grade-school math (GSM) problems, specifically focusing on compositional GSM problems, where the answer to the first question is a variable in the second question. - The study reveals a significant reasoning gap in most LLMs, indicated by a performance difference between solving compositional question pairs and solving each question independently. - This gap is more pronounced in smaller, more cost-efficient, and math-specialized models, suggesting potential limitations in reasoning abilities. -  Instruction-tuning, code generation, and finetuning have varying effects across LLMs, while finetuning can lead to overfitting. - Large reasoning gaps stem from distraction from additional context and poor second-hop reasoning, rather than dataset leakage, impacting performance despite high scores on standard GSM benchmarks. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [HelpSteer2-Preference: Complementing Ratings with Preferences](https://arxiv.org/abs/2410.01257) | okuchaiev, gshennvm, trias702, odelalleau, alexwb |   - This paper introduces HelpSteer2-Preference, a novel dataset of preference annotations designed to complement the existing ratings in the HelpSteer2 dataset, enabling a head-to-head comparison of Bradley-Terry and Regression style reward models. - The authors propose a novel approach combining Bradley-Terry and Regression reward modeling, leading to a Llama 3.1 70B Instruct model that achieved a state-of-the-art 94.1 score on RewardBench as of October 1, 2024. - The preference annotations are accompanied by human-written justifications, enhancing data interpretability and providing insights into annotator decision-making. - The research demonstrates that data format (regression vs. preference) is less critical than the model's ability to capture annotation information, with preference magnitude being key for Bradley-Terry models.  - The combined reward model effectively aligns language models to follow instructions using online Reinforcement Learning from Human Feedback (RLHF), particularly with the REINFORCE algorithm. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/nvidia/HelpSteer2), [Link](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) |
| [RATIONALYST: Pre-training Process-Supervision for Improving Reasoning](https://arxiv.org/abs/2410.01044) | Guoxuan Wang, danyaljj, ChuyuLiu, ylu610, Dongwei | - RATIONALYST, a model pre-trained on implicit rationales extracted from unlabeled text and existing reasoning datasets, is introduced for process-supervision of reasoning. - RATIONALYST leverages these implicit rationales during inference to guide the reasoning process of large language models, enhancing both interpretability and performance. - It consistently generalizes across various reasoning tasks, demonstrating an average 3.9% accuracy improvement on 7 representative reasoning benchmarks when fine-tuned from LLaMa-3-8B. - RATIONALYST outperforms both stronger general-purpose verifiers like GPT-4 and similarly sized models trained on matching datasets, showcasing the efficacy of its process supervision approach. - An ablation study shows that rationales from web-scale data enhance performance, while implicit supervision proves more robust than explicit supervision due to tolerance for imperfect rationales. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JHU-CLSP/Rationalyst) | N/A |
| [Quantifying Generalization Complexity for Large Language Models](https://arxiv.org/abs/2410.01769) | maxtiktok, Nrain, zhuokai, Xulianghuang, luohy | This paper introduces SCYLLA, a dynamic evaluation framework designed to measure the generalization ability of Large Language Models (LLMs) and disentangle it from memorization. - SCYLLA evaluates LLMs across 20 tasks and 5 complexity levels, generating in-distribution and out-of-distribution data to assess generalization. - The study reveals a "generalization valley," where the performance gap between in-distribution and out-of-distribution data is non-monotonic with task complexity. - The peak of this valley, the "critical complexity," represents the upper bound of an LLM's generalization and shifts to higher complexity levels with increasing model size. - The benchmark results covering 28 LLMs show that closed-source models generally exhibit stronger generalization abilities and higher critical complexity than their open-sourced counterparts. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zhentingqi/scylla) | N/A |
| [E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding](https://arxiv.org/abs/2409.18111) | Ying Shan, Yang Wu, Zhongang Qi, Zongyang Ma, Ye Liu | -"E.T. Bench", a large-scale benchmark designed for open-ended, event-level video understanding. - The benchmark comprises 7.3K samples across 12 tasks, spanning 8 domains and featuring 7K videos totaling 251.4 hours. -A novel Video-LLM called "E.T. Chat" is introduced, which excels in event-level understanding by treating timestamp prediction as an embedding matching problem. - A dedicated instruction-tuning dataset, "E.T. Instruct 164K", tailored for multi-event, time-sensitive videos is created. - State-of-the-art models on existing video question answering benchmarks struggle with this new benchmark indicating that current methods struggle with fine-grained time-sensitive video understanding. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling](https://arxiv.org/abs/2410.01440) | Jiazhong Yu, Cao Sheng, Fei Li, feifeiobama, ljh0104 |  - This paper introduces equilibrium sequence modeling, a novel method for training large language models (LLMs) to perform long-horizon robotic planning by iteratively refining plans based on environmental feedback through a self-refinement process. - The approach formulates self-refinement as a fixed-point problem, allowing for end-to-end supervised training without needing external verifiers or reward models, simplifying training compared to reinforcement learning methods. - A nested equilibrium sequence modeling procedure enables efficient closed-loop planning, leveraging feedback from the environment (or a world model) and accelerating plan refinement by reusing previously computed equilibrium solutions. - Evaluations on VirtualHome-Env benchmark demonstrate state-of-the-art performance in most metrics, especially when incorporating environmental feedback, and show advantageous scaling of performance with increased inference computation. - Ablation studies highlight the effectiveness of equilibrium sequence modeling, reuse of previous solutions, and dynamic computation allocation in improving plan quality and computational efficiency. | ['Robotics', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/Singularity0104/equilibrium-planner) | N/A |
| [Selective Aggregation for Low-Rank Adaptation in Federated Learning](https://arxiv.org/abs/2410.01463) | Huijie Fan, Liangqiong-QU, yanranw1, stevezs, gpx333 |  - This research paper introduces FedSA-LoRA, a new method for federated learning that selectively aggregates learned A and B matrices from LoRA. - It asserts that A matrices learn general knowledge while B matrices capture client-specific information, leading to only sharing A matrices for aggregation. - Experimental validation across language understanding and generation tasks on benchmarks like GLUE and GSM8K demonstrates FedSA-LoRA outperforms other methods.  - The authors extend this approach to other LoRA variants (rsLoRA and VeRA), creating FedSA-rsLoRA and FedSA-VeRA, and show consistent improvements. - The findings provide insights into LoRA in federated settings and a general framework for using future LoRA adaptations. | ['Natural Language Processing', 'Text Classification', 'Text Generation', 'Question Answering'] | N/A | N/A |


## Papers for 2024-10-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Law of the Weakest Link: Cross Capabilities of Large Language Models](https://arxiv.org/abs/2409.19951) | xwhan, ruihou16, xwwang, astonzhang, MingZhong |  - This research paper explores the intersection of multiple abilities, termed "cross capabilities," in Large Language Models (LLMs), which are essential for real-world tasks but often overlooked in current evaluations that focus on individual capabilities. - It introduces CROSSEVAL, a benchmark with 1,400 human-annotated prompts and 8,400 human ratings, designed to evaluate both individual and cross capabilities, revealing that current LLMs underperform in cross-capability tasks. - The study finds that LLM cross-capability performance adheres to the "Law of the Weakest Link," being significantly limited by the weakest individual capability, regardless of improvements in other areas. - The results highlight that tool use is a major challenge for LLMs and suggest that prioritizing the enhancement of weaker capabilities is more crucial for improving overall performance than focusing on already strong ones. -  The work emphasizes the importance of shifting focus towards cross-capability evaluation and development to improve LLM effectiveness in complex, real-world scenarios rather than just on individual capabilities. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/facebookresearch/llm-cross-capabilities) | N/A |
| [TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices](https://arxiv.org/abs/2410.00531) | Hongfang Yu, Mohsen Guizani, Jiaoshen, LIKirin | TPI-LLM is a tensor parallel inference system designed for serving 70B-scale LLMs efficiently on low-resource edge devices. - It addresses memory limitations by introducing a sliding window memory scheduler that dynamically manages layer weights during inference, overlapping disk I/O with computation and communication. - TPI-LLM prioritizes tensor parallelism over pipeline parallelism for single-user scenarios on edge devices and implements a star-based allreduce algorithm to minimize link latency. - Experimental results show significant reductions in time-to-first-token, token latency, and peak memory footprint compared to benchmarks like Transformers, Accelerate, and Galaxy. - TPI-LLM successfully runs Llama 2-70B with a peak memory footprint of 3.1GB across 8 low-resource devices, enabling larger models to run on edge devices while preserving user privacy. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Lizonghang/TPI-LLM) | N/A |
| [Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect](https://arxiv.org/abs/2409.17912) | imomayiz, amr-mohamed, khoubrane-yousef, habdine, guokan-shang | Atlas-Chat introduces the first Large Language Models (LLMs) for Moroccan Arabic, a low-resource dialectal Arabic (DA) variant also known as Darija. - A new instruction dataset, Darija-SFT-Mixture, was created by combining existing and new manually and synthetically created Darija resources, as well as translated English instructions. - Atlas-Chat-9B and 2B models, fine-tuned on this dataset, outperform existing LLMs, including Arabic-specific and state-of-the-art models like LLaMa, Jais, and AceGPT, achieving a 13% improvement over a 13B model on a new Darija benchmark. - A new evaluation suite, including DarijaMMLU, DarijaHellaSwag, and DarijaBench, was developed for comprehensive LLM assessment in Darija, focusing on discriminative and generative tasks.  - An experimental analysis was conducted on fine-tuning strategies and base model choices, finding that instruction-tuned Gemma 2 models with LoRA performed optimally. | ['Natural Language Processing', 'Translation', 'Summarization'] | N/A | [Link](https://hf.co/MBZUAI-Paris/Atlas-Chat-9B), [Link](https://hf.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture) |
| [ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer](https://arxiv.org/abs/2410.00086) | Jingren, chenweix7, chaojiemao, jingfengzhang, jiangzeyinzi |  - ACE, a unified framework based on a Diffusion Transformer, supports a wide range of visual generation and editing tasks through natural language instructions, including text-guided generation, low-level visual analysis, controllable generation, semantic editing, element editing, repainting, layer editing, and reference generation. - ACE introduces the Long-context Condition Unit (LCU) to incorporate historical information from previous generation rounds, enabling multi-turn and long-context generation. - A meticulous data collection workflow is established to construct a 0.7 billion-scale dataset covering various generation and editing tasks. - Evaluation on benchmarks such as MagicBrush and a user study on a manually curated benchmark demonstrates ACE’s superior performance in various visual generation tasks. - ACE can be easily integrated into a multimodal chat system to streamline image creation and editing, avoiding cumbersome pipelines typically employed in visual agents. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | [Link](https://huggingface.co/runwayml/stable-diffusion-v1-5), [Link](https://huggingface.co/runwayml/stable-diffusion-inpainting) |
| [Visual Context Window Extension: A New Perspective for Long Video Understanding](https://arxiv.org/abs/2409.20018) | Zhenzhong Chen, hcwei | This research paper proposes a novel approach to enhance long video understanding by extending the visual context window of Large Multimodal Models (LMMs). - It redefines the context window in LMMs as two distinct windows: visual and language, addressing the discrepancies between these modalities. - The study introduces a method to extend positional embeddings within the visual context window, enabling LMMs to handle lengthy videos without retraining on large video-text datasets. - A progressive pooling strategy is implemented to reduce memory consumption by selectively adjusting the spatial resolution of frame embeddings. - Experimental results on benchmarks like MLVU, VideoMME, and LongVideoBench demonstrate consistent performance improvements with increasing video frames, outperforming models like GPT-40 and achieving memory savings of approximately 45%. | ['Multimodal', 'Video-Text-to-Text'] | N/A | N/A |


## Papers for 2024-10-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566) | nm-w, pdufter, zhegan27, fly6464, haotiz |  - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. - MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training. - It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks. - MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI). -  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [DiaSynth -- Synthetic Dialogue Generation Framework](https://arxiv.org/abs/2409.19020) | Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl |  - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning. - It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues. - Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks. - The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data. - DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [Hyper-Connections](https://arxiv.org/abs/2409.19606) | banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder | This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse. - Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters. - Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections. - Hyper-connections are also effective in vision tasks. - Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections. - The authors anticipate this method's broad applicability across various AI problems. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Image-to-Text', 'Unconditional Image Generation', 'Text2Text Generation'] | N/A | N/A |
| [Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models](https://arxiv.org/abs/2409.18943) | yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li | - RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs). - Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation. - RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs. - Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments.  - RULER maintains overall performance on various other benchmarks without affecting non-length based generation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Geaming2002/Ruler) | N/A |
| [Cottention: Linear Transformers With Cosine Attention](https://arxiv.org/abs/2409.18747) | Eric C. Larson, TrevorDohm, gmongaras | This study introduces "Cottention," a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/gmongaras/Cottention_Transformer) | N/A |
| [Can Models Learn Skill Composition from Examples?](https://arxiv.org/abs/2409.19808) | Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu | This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4. - Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills. - Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill. - The study shows that incorporating skill-rich synthetic text improves compositional capabilities. - Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency. - Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code](https://arxiv.org/abs/2409.19715) | Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae | COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym) |
| [IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding](https://arxiv.org/abs/2409.19627) | Jianzong Wang, Jing Xiao, zhangxulong, Pechola | - IDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization. - It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating. - A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability. - IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods. - Experimental results show its ability to withstand various attacks while maintaining good imperceptibility. | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/PecholaL/IDEAW) | N/A |


## Papers for 2024-09-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692) | Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang |  - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video. - It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning. - Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines. - It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning. - MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs. | ['Multimodal', 'Any-to-Any', 'Text-to-Image', 'Image-to-Text', 'Text-to-Speech', 'Automatic Speech Recognition', 'Video-Text-to-Text'] | N/A | N/A |
| [VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2409.17066) | Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92 |  - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization. - VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization. - The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization. - They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks. - VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/VPTQ) | N/A |
| [Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult](https://arxiv.org/abs/2409.17545) | fetong | This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs). - MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning. - Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models. - On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B. - MIPO simplifies hyperparameter tuning by using only a single parameter, β, exhibiting robustness across different model architectures and datasets within a specific range. - MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback) |
| [MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making](https://arxiv.org/abs/2409.16686) | Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu | - The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales. - MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights. - Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment. - The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios. - MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents. | ['Robotics', 'Question Answering'] | N/A | N/A |


## Papers for 2024-09-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference. - Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end. - Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models. - MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks. - The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | - LLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings. - This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks. - LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets. - The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks. - Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling. | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-to-Text', 'Image-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | - EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities. - It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control. - The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data. - EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models. - A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Text-to-Audio', 'Audio-to-Audio', 'Visual Question Answering', 'Image-to-Text'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming |  - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs.  - It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference.  - This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV.  - Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilter’s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O.  - Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | [Link](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose.  - Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data).  - Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models.  - Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain.  - A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/john-hewitt/implicit-ins) | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era. - It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations. - The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs. - It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs. -  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security. | ['Natural Language Processing', 'Summarization'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations. - Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets. - It can achieve even further reduction of vector count by 66% with less than 3% performance degradation. - This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models. - The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors. - The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/colbert-ir/colbertv2.0) |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar |  - This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG). - It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness. - Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time. - The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs. - While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.  - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.  -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.  -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.  -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Text-to-Speech', 'Automatic Speech Recognition', 'Any-to-Any'] | N/A | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Image-to-3D'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence. | ['Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt | This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that "response tuning," which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that "single-task finetuning,"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang | • This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field. • The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA. • The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses. • The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach. • The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs). | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This research introduces MaskLLM, a novel learnable pruning method that generates semi-structured (N:M) sparsity in Large Language Models (LLMs) for enhanced inference efficiency. - MaskLLM distinguishes itself from previous methods by directly learning the distribution of N:M sparsity patterns using Gumbel Softmax sampling, enabling end-to-end training on large datasets and addressing limitations of hand-crafted importance criteria. -  Empirical evaluations on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, demonstrate that MaskLLM surpasses state-of-the-art techniques, achieving a perplexity of 6.72 on LLaMA2-7B compared to SparseGPT's 10.42. - The research underscores the efficacy of learning sparsity patterns directly from data, leading to more accurate and efficient compression of LLMs without compromising performance. - The adaptability of MaskLLM to downstream tasks and its ability to achieve lossless compression in certain scenarios highlight its potential for practical applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research proposes LLaVA-3D, a novel framework for building 3D-aware Large Multimodal Models (LMMs) by adapting the existing 2D LLaVA model.  LLaVA-3D introduces the concept of "3D Patches," which inject 3D positional embeddings into 2D image features, enhancing the model's spatial understanding without complex 3D processing pipelines.  Evaluations demonstrate LLaVA-3D's state-of-the-art performance on various 3D tasks, including question answering, dense captioning, and visual grounding, surpassing existing 3D LMMs while maintaining comparable 2D image understanding capabilities to its 2D counterpart. The research highlights the advantages of leveraging pre-trained 2D LMMs for 3D scene understanding and the benefits of integrating 3D spatial information into 2D visual features. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image-to-Text', 'Text-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal large language model capable of perceiving and generating images, text, and speech with emotional expressiveness. EMOVA utilizes a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for end-to-end speech processing.  The model achieves state-of-the-art performance on both vision-language and speech benchmarks, outperforming models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks and surpassing the speech LLM Mini-Omni in ASR tasks. EMOVA also enables emotional spoken dialogue by explicitly predicting speech style labels (emotions and pitches) and leveraging a lightweight style module for controllable speech synthesis. This is achieved through a novel text-centric multimodal alignment approach, which leverages publicly available bimodal data and eliminates the reliance on scarce trimodal data. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach to reduce the computational cost and latency of processing long context inputs with Large Language Models (LLMs). GemFilter leverages the ability of early LLM layers to identify relevant tokens and compresses the input sequence by a factor of 1000x for subsequent processing by the full model. Empirical evaluations show that GemFilter achieves a 2.4x speedup and 30% reduction in GPU memory consumption compared to state-of-the-art methods, while maintaining comparable performance on benchmarks like LongBench and outperforming them on the Needle in a Haystack task. GemFilter is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting the selected input sequence. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This research paper investigates implicit instruction tuning, demonstrating that instruction following can emerge without explicit instruction-response training.  - The authors show that training solely on responses (response tuning) and on narrow-domain data (single-task finetuning) leads to broad instruction-following abilities in language models.  - For instance, response-tuned models achieve a 43% win rate against explicitly instruction-tuned models in head-to-head evaluations.  - Furthermore, they introduce a simple rule-based language model that, when combined with a pretrained model, exhibits instruction-following behavior.  - These findings highlight that adaptation methods not explicitly designed for instruction following can implicitly induce such capabilities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This research paper introduces a novel technique named "TOKEN POOLING" for enhancing the efficiency of multi-vector retrieval models, especially focusing on ColBERT, without significantly affecting performance.  The method uses clustering techniques to group together similar token representations and then applies mean pooling to create a single, representative vector, effectively reducing the overall storage footprint. Experiments show that this approach reduces the required vector count by 50% without compromising accuracy, and a 66% reduction still yields strong performance. The paper also demonstrates that TOKEN POOLING can be effectively combined with existing quantization methods, leading to even more significant compression rates while maintaining reasonable retrieval performance.  | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper presents a comprehensive review of the emerging field of Conversation Analysis (CA), a process designed to extract critical information from conversational data and leverage it for system optimization and decision-making.  - The paper systematically defines CA as a four-step procedure: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, and discusses the challenges and trends within each step. - The paper further argues that while previous CA efforts focused on atomic tasks with limited business impact, the rise of Large Language Models (LLMs) enables deeper, more insightful analysis and strategic decision-making from conversations.  - The authors compile and categorize existing benchmark datasets for CA but highlight a significant gap in comprehensive benchmarks containing fine-grained conversation elements and long-context modeling capabilities.  - The paper concludes by outlining future research directions, including the development of LLM-based conversation simulators, fine-grained CA benchmarks, long-context conversation modeling, in-depth attribution analysis, and advanced goal-directed optimization and evaluation methods. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging Knowledge Graphs (KGs) and graph-based architectures. The authors demonstrate the effectiveness of their framework by applying it to the SoccerNet dataset, a large dataset of soccer videos. Their findings show that Structured-GraphRAG significantly improves query processing efficiency, reduces response times, and enhances accuracy compared to traditional RAG methods. The structured nature of KGs reduces hallucinations in LLMs, making the responses more consistent and reliable. The authors highlight that their framework can be applied to a broad range of applications due to its flexible design. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


