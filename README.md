# Daily AI Papers

These summaries are automatically generated from [HuggingFace's Daily Papers](https://huggingface.co/papers), using Gemini and GitHub actions based on the following categories of interest: Multimodal, Text Generation, Text Classification, Text2Text Generation, Summarization, Question Answering, Natural Language Processing, Audio, Text-to-Speech, Audio-to-Audio. All credits go to the research community for sharing and the HuggingFace community for curating these papers.

Please note:
- Authors may be listed by their HuggingFace user id. This will be rectified soon. 
- These summaries are entirely generated by the LLM. You can refer to the basic prompt [here](templates/prompt_template.md).

Last updated: 2025-08-26 
 


## Papers for 2025-08-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,
  Reasoning, and Efficiency](https://arxiv.org/abs/2508.18265) | jinglinglin, WesKwong, MIASANMIA, gulixin0922, Weiyun1025 |  - InternVL 3.5 is a new family of open-source multimodal models that significantly improves versatility, reasoning, and inference efficiency.  - A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment.  - To optimize efficiency, a Visual Resolution Router (ViR) dynamically adjusts the resolution of visual tokens and a Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs.  - InternVL3.5-241B-A28B achieves state-of-the-art results among open-source MLLMs across various tasks and narrows the performance gap with leading commercial models like GPT-5.  - All models and code are publicly released. | ['Multimodal'] | [Link](https://github.com/OpenGVLab/InternVL) | [Link](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B) |
| [Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory
  and Test-Time Compute Scaling](https://arxiv.org/abs/2508.16745) | Daniil Orel, mbur, yurakuratov, b1l4lx1, irodkin | - This paper introduces a novel benchmark, 1dCA-Reasoning, for evaluating the multi-step reasoning capabilities of neural models, focusing on rule generalization rather than memorization. - The authors conduct a comprehensive study across diverse neural architectures (Transformers, LSTMs, state-space models, and ARMT), demonstrating that increasing model depth significantly improves multi-step reasoning performance. - They investigate several depth-extension strategies, including recurrence, memory, and test-time compute scaling (ACT), finding that ACT yields a noticeable improvement while preserving parameter efficiency. -  Reinforcement learning with GRPO allows for successful multi-step reasoning without intermediate supervision, and chain-of-thought prompting achieves near-perfect accuracy up to k=4. - The study highlights the importance of both architectural inductive biases and training objectives in determining the reasoning capabilities of neural models. | ['Natural Language Processing'] | [Link](https://github.com/) | [Link](https://huggingface.co/datasets/irodkin/1dCA_r2s20T20) |
| [UQ: Assessing Language Models on Unsolved Questions](https://arxiv.org/abs/2508.17580) | Wei Liu, Rui Sun, Zihao Wang, Fan Nie, kzliu | This paper introduces UQ, a new benchmark for evaluating language models on unsolved questions sourced from Stack Exchange.  UQ addresses the limitations of existing benchmarks by focusing on challenging, open-ended problems with inherent real-world value. The benchmark is composed of 500 questions across diverse topics, curated through a multi-stage pipeline employing rule-based filters, LLM judges, and human review.  The evaluation of models on UQ is performed asynchronously and uses validator strategies that leverage the generator-validator gap to provide evaluation signals before human verification. A platform supports community-driven evaluation and asynchronous verification of solutions. | ['Question Answering'] | [Link](https://github.com/) | [Link](https://huggingface.co/) |
| [MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for
  N-level Assessment](https://arxiv.org/abs/2508.17290) | Doratossadat Dastgheib, Seyed Mohammad Hadi Hosseini, Marzia Nouri, Arshia Hemmat, omidgh |  - This paper introduces MEENA, a new multimodal-multilingual dataset designed to evaluate Persian VLMs across a wide range of educational tasks.   - MEENA includes approximately 7,500 Persian and 3,000 English questions, covering diverse subjects and educational levels.   - The dataset incorporates various question formats, including multiple-choice, mathematical problem solving and visual reasoning.   - MEENA also includes rich metadata, such as difficulty levels, descriptive answers, and human performance data.   - Experimental results demonstrate the challenges that current VLMs face in handling both reasoning and complex multimodal tasks, especially in Persian. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | N/A |
| [Explain Before You Answer: A Survey on Compositional Visual Reasoning](https://arxiv.org/abs/2508.17298) | Xin Zheng, Zixian Ma, Joy Hsu, Fucai Ke, ControlNet | This survey paper comprehensively reviews the field of compositional visual reasoning (CVR), focusing on works from 2023 to 2025.  It identifies key advantages of CVR over monolithic approaches, including improved cognitive alignment, semantic fidelity, robustness, and data efficiency. The paper traces the evolution of CVR paradigms across five stages: prompt-enhanced language-centric methods, tool-enhanced LLMs and VLMs, chain-of-thought reasoning VLMs, and unified agentic VLMs.  It catalogs benchmarks and metrics for evaluating CVR systems, highlighting challenges such as LLM limitations, hallucinations, and bias toward deductive reasoning.  Finally, the survey proposes future directions for CVR research, including world-model integration, human-AI collaborative reasoning, and improved evaluation protocols. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | N/A |
| [German4All - A Dataset and Model for Readability-Controlled Paraphrasing
  in German](https://arxiv.org/abs/2508.17973) | Cristian-George Craciun, Maximilian Müller, Eslam Nasrallah, Thanh Mai Pham, Miriam Anschütz | - The paper introduces German4All, the first large-scale German dataset of aligned readability-controlled, paragraph-level paraphrases, spanning five readability levels and comprising over 25,000 samples. - German4All was automatically synthesized using GPT-4 and rigorously evaluated through human and LLM-based judgments to ensure data quality and usefulness. - An open-source, readability-controlled paraphrasing model trained on German4All achieves state-of-the-art performance in German text simplification, allowing for nuanced reader-specific adaptations. - The dataset and model are open-sourced to encourage further research on multi-level paraphrasing and readability control in German. - The study also shows how the model outperforms existing German ATS systems on existing text simplification datasets in terms of SARI scores and FRE, demonstrating its effectiveness in handling different complexity levels. | ['Text2Text Generation'] | [Link](https://github.com/MiriUll/German4All) | N/A |
| [Limitations of Normalization in Attention Mechanism](https://arxiv.org/abs/2508.17821) | Radu State, Tatiana Petrova, mbur, opensapce | - This paper investigates the limitations of normalization in attention mechanisms, focusing on the softmax function. - The authors provide a theoretical framework to analyze the model's selective ability and geometric separation in token selection, deriving explicit bounds on distances and separation criteria. - Through experiments with pre-trained GPT-2 models, they validate their theoretical results and demonstrate that the model's ability to distinguish informative tokens declines as the number of selected tokens increases. - They also show that gradient sensitivity under softmax normalization poses challenges during training, especially at low temperatures. - This work advances the understanding of softmax-based attention mechanisms and motivates the development of more robust normalization and selection strategies for future attention architectures. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language
  Modeling](https://arxiv.org/abs/2508.16790) | Jiaqi Li, Junan Zhang, Xueyao Zhang, Dekun Chen, Yuancheng Wang | - TaDiCodec, a novel text-aware diffusion speech tokenizer, is proposed to address limitations of existing methods by employing end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance.- TaDiCodec achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps, outperforming existing methods in terms of compression while maintaining competitive performance on speech generation evaluation metrics.- The model architecture uses a diffusion autoencoder with a single-layer codebook and incorporates text guidance into the diffusion decoder to improve reconstruction quality.- TaDiCodec demonstrates compatibility with language model-based zero-shot text-to-speech using both autoregressive and masked generative modeling, showcasing its effectiveness for speech language modeling.- The authors will open-source the code and model checkpoints, and audio samples are available on the project website. | ['Text-to-Speech'] | [Link](https://github.com/HeCheng0625/Diffusion-Speech-Tokenizer) | N/A |
| [Neither Valid nor Reliable? Investigating the Use of LLMs as Judges](https://arxiv.org/abs/2508.18076) | Golnoosh Farnadi, Jackie Chi Kit Cheung, Mohammed Haddou, Khaoula Chehbouni | This paper investigates the use of Large Language Models (LLMs) as judges for evaluating Natural Language Generation (NLG) systems.  It critically examines four core assumptions underlying the use of LLMs as evaluators: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. The authors identify and assess challenges to these assumptions, drawing on measurement theory from the social sciences. Three applications of LLMs as judges are explored: text summarization, data annotation, and safety alignment. Finally, the paper highlights the need for more responsible evaluation practices to ensure that the growing role of LLMs in NLG evaluation supports, rather than undermines, progress in the field. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-08-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153) | Xue Yan, Siyuan Guo, Yihang Chen, linyiyang2023, Zhouhc | AgentFly is a novel memory-based learning paradigm for Large Language Models that eliminates the need for fine-tuning LLMs.  It leverages online reinforcement learning with a memory-augmented Markov Decision Process (M-MDP), incorporating a neural case-selection policy. AgentFly achieves top performance on various benchmarks, including GAIA, outperforming state-of-the-art methods.  Its continual learning capabilities allow for adaptation without gradient updates, demonstrating efficiency and scalability. The method's performance gains are attributed to case-based reasoning, enhancing adaptability and generalization. | ['Reinforcement Learning', 'Question Answering', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/Agent-on-the-Fly/AgentFly) | N/A |
| [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650) | Yonatan Belinkov, Martin Tutek, Aaron Mueller, Dana Arad, Tomertech | - This paper introduces CRISP, a novel parameter-efficient method for persistent concept unlearning in large language models (LLMs) that uses sparse autoencoders (SAEs). - CRISP outperforms existing methods on safety-critical unlearning tasks by automatically identifying and suppressing salient SAE features across multiple layers, achieving better trade-offs between unlearning efficacy and preserving model utility. - The method is shown to be effective on two LLMs and outperforms previous approaches on the WMDP benchmark, indicating its robustness and generalizability. - Feature-level analysis demonstrates that CRISP achieves semantically coherent separation between target and benign concepts, ensuring precise suppression of the target features while maintaining coherence. - The method is parameter-efficient and is thus suited for open-source deployment, and it addresses the issue of inference-time interventions that can be bypassed by malicious actors. | ['Natural Language Processing', 'Text Generation', 'Feature Extraction'] | N/A | N/A |
| [AetherCode: Evaluating LLMs' Ability to Win In Premier Programming
  Competitions](https://arxiv.org/abs/2508.16402) | Yidi Du, Markus Mak, Zhicheng Liu, Jiaze Chen, zhwang01 | - AetherCode, a new benchmark for evaluating LLMs' coding and reasoning capabilities, is introduced.  It uses problems from premier programming competitions (IOI and ICPC) offering broader coverage and higher difficulty than existing benchmarks. - The benchmark addresses shortcomings of existing benchmarks by incorporating comprehensive, expert-validated test suites, combining automated generation and human curation to ensure rigorous assessment. - AetherCode's hybrid methodology combines automated test case generation with expert annotation to achieve 100% TPR and 100% TNR, ensuring high-quality test cases. - Evaluation reveals a significant performance gap between reasoning and non-reasoning models, with reasoning models demonstrating superior performance, particularly on complex algorithmic problems. - The results highlight the remaining challenges for LLMs in competitive programming and the need for continued improvement in reasoning and coding capabilities. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/m-a-p/AetherCode) |
| [End-to-End Agentic RAG System Training for Traceable Diagnostic
  Reasoning](https://arxiv.org/abs/2508.15746) | Pengcheng Qiu, Chaoyi Wu, Yuze Sun, Qiaoyu Zheng, Angelakeke | This paper introduces Deep-DxSearch, a novel agentic RAG system for medical diagnosis trained end-to-end with reinforcement learning.  The model uses a large-scale medical retrieval corpus and a tailored reward function to improve diagnostic accuracy and traceability. Deep-DxSearch significantly outperforms several strong baselines across various datasets, achieving substantial gains in both in-distribution and out-of-distribution settings.  The model's performance is further enhanced by its ability to adapt retrieval strategies, perform effective differential diagnosis, and filter out irrelevant information.  The authors provide code and data for reproducibility. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/MAGIC-AI4Med/Deep-DxSearch) | N/A |
| [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated
  Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881) | Di Yin, Yuxuan Wang, Pingzhi Tang, Fanxu Meng, xiaojuan0920 | - This paper introduces Tensor Parallel Latent Attention (TPLA), a novel technique designed to enhance the efficiency of disaggregated prefill and decode inference in large language models (LLMs). - TPLA addresses the limitations of existing methods like Multi-Head Latent Attention (MLA) in tensor parallel settings by partitioning both the latent representation and each head's input dimension across devices. - The proposed method preserves the benefits of compressed KV caching while achieving significant speedups (1.79x and 1.93x for DeepSeek-V3 and Kimi-K2, respectively) at a 32K-token context length. - TPLA maintains strong performance on commonsense and LongBench benchmarks and is compatible with FlashAttention-3 for end-to-end acceleration. - The authors demonstrate that TPLA is drop-in compatible with MLA pre-trained models and requires minimal retraining. | ['Natural Language Processing'] | [Link](https://github.com/fxmeng/TransMLA) | N/A |
| [AgentScope 1.0: A Developer-Centric Framework for Building Agentic
  Applications](https://arxiv.org/abs/2508.16279) | Liuyi Yao, Weirui Kuang, Yuexiang Xie, Zitao Li, Dawei Gao | - AgentScope 1.0 is a developer-centric framework for building agentic applications that leverages Large Language Models (LLMs). - It introduces improvements in supporting flexible and efficient tool-based agent-environment interactions, abstracting foundational components and providing unified interfaces. - AgentScope incorporates several built-in agents tailored to specific practical scenarios, along with robust engineering support for developer-friendly experiences. - It features a scalable evaluation module with a visual studio interface, a runtime sandbox for safe agent execution, and facilitates rapid deployment in production environments. - The framework is based on the ReAct paradigm and supports parallel tool calls, asynchronous executions, and real-time steering. | ['Natural Language Processing', 'Reinforcement Learning', 'Other'] | [Link](https://github.com/agentscope-ai/agentscope) | N/A |
| [InMind: Evaluating LLMs in Capturing and Applying Individual Human
  Reasoning Styles](https://arxiv.org/abs/2508.16072) | Diping Song, Qi Chen, Yibin Wang, Chuanhao Li, Zizhen Li | InMind is a novel, cognitively grounded evaluation framework designed to assess LLMs' capacity for individualized reasoning.  It uses social deduction games (SDGs) like Avalon, enhancing structured gameplay data with strategy traces and post-game reflections. InMind evaluates LLMs on four cognitively motivated tasks: Player Identification, Reflection Alignment, Trace Attribution, and Role Inference.  The results reveal key limitations in current LLMs' capacity for individualized reasoning, highlighting the need for further research to bridge the gap between human-like reasoning and current AI capabilities. The InMind-Avalon dataset is also introduced, containing detailed annotations of human gameplay. | ['Natural Language Processing'] | [Link](https://github.com/leroy9472/InMind) | N/A |
| [CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated
  Chain-of-Thought-based Reinforced Fine-Tuning](https://arxiv.org/abs/2508.15868) | Yulun Zhang, Haipang Wu, Rongjuncheng Zhang, Ji Liu, Wenqiao Zhu | - This paper introduces CARFT, a novel contrastive learning approach for enhancing the reasoning capabilities of Large Language Models (LLMs). - CARFT leverages annotated Chain-of-Thought (CoT) and incorporates contrastive signals to guide the fine-tuning process, addressing limitations of existing RL-based methods. - The method uses CoT embeddings to generate contrastive signals, including positive (correct answers) and negative signals (incorrect answers), improving both performance and stability. - Experiments on the SVAMP and GSM8K datasets demonstrate that CARFT significantly outperforms baselines (SFT, ReFT, and Dr.GRPO) in terms of accuracy and robustness, achieving improvements of up to 10.15%. - CARFT's efficiency is also highlighted, showing improvements of up to 30.62% compared to Dr.GRPO. | ['Question Answering'] | [Link](https://github.com/WNQzhu/CARFT) | N/A |
| [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390) | Liming Fang, Jiafei Wu, Xiaogang Xu, Lu Zhou, AlienZhang1996 | - This paper introduces MDH, a malicious content detection framework that combines LLM-based annotation with human oversight to improve the accuracy and efficiency of cleaning red-teaming datasets and identifying jailbroken responses. - Two new jailbreaking strategies, D-Attack and DH-CoT, are proposed.  D-Attack leverages context simulation, while DH-CoT incorporates hijacked chains of thought to improve the success rate of jailbreaks. - The MDH framework achieves over 95% accuracy in detecting malicious content across multiple datasets with less than 10% manual effort. - The proposed jailbreaking methods demonstrate significant improvements in attack success rates compared to existing approaches, particularly on reasoning models with the DH-CoT method. - The paper contributes datasets, judgements, and detection results to a GitHub repository for reproducibility and further research. | ['Natural Language Processing', 'Text Classification', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/AlienZhang1996/DH-CoT) | N/A |


## Papers for 2025-08-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Intern-S1: A Scientific Multimodal Foundation Model](https://arxiv.org/abs/2508.15763) | xuhuang87, ZhouqiHUA, Jerry-hyl, guox18, gaoyang07 | This paper introduces Intern-S1, a 28-billion parameter (241 billion total parameters) multimodal Mixture-of-Experts (MoE) model pre-trained on 5 trillion tokens, including over 2.5 trillion from scientific domains.  Intern-S1 utilizes offline and online reinforcement learning via InternBootCamp, employing a novel Mixture-of-Rewards (MoR) algorithm to handle over 1000 tasks simultaneously.  It achieves top-tier performance on general reasoning benchmarks and significantly surpasses existing open-source models in scientific domains.  The model incorporates several novel techniques, including a dynamic tokenizer for efficient handling of scientific data and an adaptive downsampling module for time-series data. | ['Multimodal'] | N/A | [Link](https://huggingface.co/internlm/Intern-S1) |
| [Deep Think with Confidence](https://arxiv.org/abs/2508.15260) | Xuewei Wang, jiaweizhao, tydsh, Viol2000 | This paper introduces Deep Think with Confidence (DeepConf), a novel method that enhances large language model (LLM) reasoning performance and efficiency. DeepConf utilizes internal confidence signals to filter low-quality reasoning traces, improving accuracy and reducing computational cost.  Evaluations on various reasoning benchmarks demonstrate that DeepConf significantly outperforms baseline methods, achieving up to 99.9% accuracy and reducing generated tokens by up to 84.7%. The method is applicable across various LLMs and integrates seamlessly into existing serving frameworks. | ['Natural Language Processing'] | [Link](https://github.com/jiaweizzhao/deepconf) | [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B) |
| [LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries](https://arxiv.org/abs/2508.15760) | huuuyeah, Ironieser, sileixu, dinghanshen, Kevin355 | - This paper introduces LiveMCP-101, a benchmark containing 101 real-world queries designed to test the ability of AI agents to utilize multiple tools. - The benchmark focuses on multi-step tasks that require coordination between different tools and features a novel evaluation approach using ground-truth execution plans. - Experiments reveal that even state-of-the-art LLMs struggle, achieving success rates below 60%, highlighting the challenges in tool orchestration. - The paper also provides detailed error analysis that reveals distinct failure modes, offering insights for improving future models. - LiveMCP-101 provides a challenging and standardized evaluation approach for real-world AI agent capabilities. | ['Natural Language Processing'] | N/A | N/A |
| [A Survey on Large Language Model Benchmarks](https://arxiv.org/abs/2508.15361) | Siyi Li, Xuanang Chen, Shuaimin Li, Guhong Chen, Shiwen Ni |  - This paper presents a comprehensive survey of Large Language Model (LLM) benchmarks, categorizing 283 benchmarks into three categories: general capabilities, domain-specific, and target-specific.  - The study identifies key issues with current benchmarks such as inflated scores due to data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments.  - It provides a referable design paradigm for future benchmark innovation.   -  The paper offers a detailed taxonomy of LLM benchmarks, which serves as a valuable resource for researchers working on LLM development and evaluation.  - The work is the first to conduct such a systematic review and prospective analysis, highlighting its importance as a foundational contribution to the field. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998) | Yacine Jernite, Giada Pistilli, frimelle | - This paper introduces INTIMA, a benchmark designed to evaluate AI companionship behaviors in language models. - INTIMA uses a taxonomy of 31 behaviors across four categories (Assistant Traits, Emotional Investment, User Vulnerabilities, and Relationship & Intimacy), with 368 targeted prompts. - The benchmark is grounded in psychological theories of parasocial interaction, attachment, and anthropomorphism. - Evaluation of model responses is based on whether they reinforce companionship, maintain boundaries, or are neutral. - Applying INTIMA to four models reveals differences in how they handle emotionally charged interactions, highlighting the need for consistent approaches. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/AI-companionship/INTIMA) |


## Papers for 2025-08-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating
  Financial Large Language Models](https://arxiv.org/abs/2508.13491) | Ziyan Kuang, Effoula, QianqianXie1994, hugai101, 2083L | - This paper introduces FinCDM, the first cognitive diagnosis framework designed for evaluating financial LLMs at the knowledge-skill level, moving beyond traditional score-level evaluations. - FinCDM leverages CPA-QKA, a new cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, rigorously annotated by domain experts. - Experiments on 30 LLMs demonstrate FinCDM's ability to reveal hidden knowledge gaps, identify under-tested areas, and uncover behavioral clusters among models, supporting more trustworthy and targeted model development. - The proposed framework enables interpretable, skill-aware diagnosis, offering a more nuanced understanding of LLM capabilities compared to existing aggregate-score benchmarks. - All datasets and evaluation scripts are publicly released to foster further research in financial LLM evaluation. | ['Question Answering'] | [Link](https://github.com/WHUNextGen/FinCDM) | N/A |
| [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference
  Optimization](https://arxiv.org/abs/2508.14460) | Yu Lu, Yu Bao, Shanbo, ShujianHuang, kevinpro | - The paper introduces DuPO, a dual learning-based preference optimization framework for reliable LLM self-verification, addressing the limitations of RLVR's reliance on costly labels and traditional dual learning's restriction to strictly dual task pairs. - DuPO decomposes a primal task's input into known and unknown components, constructing a dual task to reconstruct the unknown part using the primal output and known information, thus broadening applicability to non-invertible tasks. - The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, achieving substantial gains across diverse tasks such as translation and mathematical reasoning. - Empirical results demonstrate that DuPO enhances translation quality by an average of 2.13 COMET points and boosts mathematical reasoning accuracy by an average of 6.4 points. - DuPO is presented as a scalable, general, and annotation-free paradigm for LLM optimization, showcasing its effectiveness in both training and inference-time reranking. | ['Reinforcement Learning', 'Natural Language Processing', 'Translation', 'Text Generation', 'Question Answering'] | [Link](https://github.com/ByteDance-Seed/Seed-X-7B/tree/main/challenge_set) | N/A |
| [Quantization Meets dLLMs: A Systematic Study of Post-training
  Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896) | Haobo Xu, cityug7353, ZiyuG, chriswyc, Felix1023 | - This paper presents the first systematic study on post-training quantization (PTQ) for diffusion large language models (dLLMs). - The authors identify activation outliers as a key challenge to low-bit quantization in dLLMs and implement state-of-the-art PTQ methods. - Their comprehensive evaluation across various task types and model variants offers practical insights into the quantization behavior of dLLMs. - The results show that 4-bit is the most effective configuration for weight-only quantization, while 8-bit is recommended for weight-activation quantization. - GPTQ consistently outperforms AWQ, and rotation-based methods like DuQuant demonstrate clear advantages over SmoothQuant for weight-activation quantization. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160) | jiangpinliu, CausalLi, maoyunxuan, CircleRadon, RH-Dang |  - RynnEC, a novel video multimodal large language model (MLLM), is introduced for embodied cognition, incorporating a region encoder and a mask decoder for flexible region-level video interaction.  - The model achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning.  - To address the scarcity of annotated 3D datasets, an egocentric video-based pipeline is proposed for embodied cognition data generation.  - A new benchmark, RynnEC-Bench, is introduced for evaluating embodied cognitive capabilities, covering 22 tasks in object and spatial cognition.  - Extensive experiments demonstrate RynnEC's superior performance in embodied cognitive abilities compared to existing general and task-specific MLLMs. | ['Robotics', 'Video Classification', 'Visual Question Answering', 'Multimodal', 'Mask Generation'] | [Link](https://github.com/alibaba-damo-academy/RynnEC) | N/A |
| [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid
  Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444) | abercovich, aditya-malte, adirendu, aklife97, apaithan |  - The paper introduces Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed for reasoning workloads. - The model architecture combines Mamba-2 and self-attention layers, resulting in improved inference speed. - Nemotron-Nano-9B-v2 outperforms similarly-sized models (e.g., Qwen3-8B) by achieving on-par accuracy and up to 6x higher throughput on reasoning benchmarks. - The model was trained using a novel FP8 training recipe on 20 trillion tokens and further aligned with various post-training methods like SFT and RLHF. - The authors are releasing the model, along with the majority of their pre-training and post-training datasets, on Hugging Face. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1) |
| [MCP-Universe: Benchmarking Large Language Models with Real-World Model
  Context Protocol Servers](https://arxiv.org/abs/2508.14704) | Prathyusha Jwalapuram, Zirui Zhao, Wenzhuo Yang, Zhiqi Shen, Ziyang Luo | This paper introduces MCP-Universe, a comprehensive benchmark designed for evaluating large language models (LLMs) interacting with real-world Model Context Protocol (MCP) servers. - MCP-Universe includes six core domains spanning eleven different MCP servers, evaluating LLMs on realistic, challenging tasks. - The benchmark employs execution-based evaluators (format, static, and dynamic) for rigorous evaluation, overcoming limitations of simpler benchmarks. - Extensive evaluation reveals performance limitations of leading LLMs, highlighting challenges such as long-context handling and unfamiliar tools. - The benchmark is open-sourced to facilitate innovation and seamless integration of new agents and MCP servers. | ['Natural Language Processing'] | [Link](https://github.com/SalesforceAIResearch/MCP-Universe) | N/A |
| [ViExam: Are Vision Language Models Better than Humans on Vietnamese
  Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680) | Daeyoung Kim, Duc Dm, Quang Tau, anvo25, tuongvy2603 |  - This paper introduces ViExam, a benchmark dataset consisting of 2,548 multimodal Vietnamese exam questions across 7 academic domains. - ViExam is the first comprehensive multimodal benchmark for evaluating Vision Language Models (VLMs) on Vietnamese educational assessments. - State-of-the-art (SOTA) VLMs achieve only 57.74% mean accuracy on ViExam, while open-source models achieve 27.70%, underperforming human test-takers (66.54%). - The findings reveal that multimodal reasoning and cross-lingual prompting pose significant challenges for VLMs on low-resource languages, and that human-in-the-loop collaboration is effective in improving VLM performance. - The dataset includes a variety of visual elements (charts, diagrams, illustrations, tables) which present additional challenges for VLMs. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [mSCoRe: a Multilingual and Scalable Benchmark for Skill-based
  Commonsense Reasoning](https://arxiv.org/abs/2508.10137) | anoperson, Franck-Dernoncourt, ntnghia1811 |  - mSCoRe, a novel multilingual and scalable benchmark for skill-based commonsense reasoning, is introduced.  The benchmark systematically evaluates large language models (LLMs) using three key components: a taxonomy of reasoning skills, a data synthesis pipeline, and a complexity scaling framework.  - The benchmark incorporates general and cultural commonsense knowledge across multiple languages (English, German, French, Chinese, and Japanese).  - Extensive experiments on eight state-of-the-art LLMs reveal that mSCoRe remains challenging for current models, particularly at higher complexity levels, demonstrating the limitations of current reasoning-reinforced models in handling nuanced multilingual commonsense.   - The results reveal the limitations of current models in handling nuanced multilingual commonsense and suggest future directions for improving these capabilities.   - The paper provides a detailed analysis of the models' reasoning processes, suggesting future directions for improving multilingual commonsense reasoning capabilities. | ['Question Answering'] | N/A | N/A |


## Papers for 2025-08-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent
  Distillation and Agentic RL](https://arxiv.org/abs/2508.13167) | Liam-Liu, hugteste, kangz, wanwan1212, tianyue818 | This paper introduces Chain-of-Agents (CoA), a novel paradigm for large language model (LLM) reasoning that enables end-to-end complex problem solving.  The CoA paradigm supports multi-agent collaboration within a single model by dynamically activating different tool agents and role-playing agents.  To achieve this, a multi-agent distillation framework is used to transfer capabilities from state-of-the-art multi-agent systems into LLM trajectories for agentic supervised fine-tuning, followed by agentic reinforcement learning.  The resulting models, Agent Foundation Models (AFMs), establish new state-of-the-art performance across diverse benchmarks in web and code agent settings. | ['Reinforcement Learning', 'Question Answering', 'Text Generation'] | [Link](https://github.com/huggingface/Math-Verify) | [Link](https://github.com/huggingface/smolagents) |
| [Prompt Orchestration Markup Language](https://arxiv.org/abs/2508.13948) | Yuqing Yang, Nan Chen, Yuge Zhang, Jiahang | POML is a novel markup language designed for prompt engineering, addressing challenges in prompt structure, data integration, format sensitivity, and tooling.  It offers specialized components for diverse data types (documents, tables, images) and a CSS-like styling system, decoupling content from presentation.  Two case studies (PomLink and TableQA) and a user study demonstrate POML's effectiveness in complex applications and real-world development.  The PomLink application showcases POML's support for multimodal data, while the TableQA case study explores the significant impact of prompt styling on model performance.  | ['Table Question Answering', 'Summarization', 'Text Generation', 'Multimodal'] | [Link](https://github.com/microsoft/poml) | N/A |
| [Mind the Generation Process: Fine-Grained Confidence Estimation During
  LLM Generation](https://arxiv.org/abs/2508.12040) | Xinyi Wang, Jie Shi, Shisong Chen, Tingyun Li, JinyiHan | - This paper introduces FineCE, a novel confidence estimation method that provides fine-grained, continuous confidence scores throughout the text generation process. - FineCE first develops a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses. - Then, it trains a model to predict confidence scores for arbitrary text sequences in a supervised manner, and proposes a Backward Confidence Integration (BCI) strategy to enhance confidence estimation. - Extensive experiments demonstrate that FineCE consistently outperforms existing confidence estimation methods on multiple benchmark datasets. - FineCE provides three strategies for identifying optimal positions to perform confidence estimation within the generation process, which balances the trade-off between performance and computational efficiency. | ['Text Generation'] | [Link](https://github.com/JinyiHan99/FineCE) | N/A |
| [Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge](https://arxiv.org/abs/2508.08777) | Alice Wang, Edoardo D'Amico, Gustavo Penha, marcodena, frafabbri | - This paper introduces a novel framework that uses Large Language Models (LLMs) as offline judges to evaluate the quality of podcast recommendations. - The framework employs a two-stage profile-aware approach: it first creates natural-language user profiles from listening history, then uses these profiles to provide context for the LLM to judge the recommendations. - In a controlled study with 47 participants, the proposed method matched human judgments with high fidelity and outperformed a variant using raw listening histories. - The framework enables efficient, profile-aware evaluation, which is useful for iterative testing and model selection in recommender systems. - The contributions of this paper include a new evaluation framework, demonstrating the effectiveness of using LLMs as judges and the benefits of incorporating user profiles into the evaluation process. | ['Natural Language Processing'] | N/A | N/A |
| [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language
  Models](https://arxiv.org/abs/2508.12903) | Zishang Jiang, Tingyun li, Haiquan Zhao, Xinyi Wang, JinyiHan | - This paper introduces ProActive Self-Refinement (PASR), a novel method that allows large language models (LLMs) to refine their outputs during the generation process, unlike traditional methods that refine after generation. - PASR uses reinforcement learning to train the LLM to make proactive refinement decisions based on its internal state and context, addressing the limitations of reactive, post-hoc refinement methods. - Experimental results on ten diverse tasks show that PASR significantly improves problem-solving performance, reducing average token consumption and increasing accuracy, particularly on Qwen3-8B. - The method uses a comparison-based reward strategy to encourage effective and timely refinements, avoiding unnecessary refinements and improving efficiency. - PASR demonstrates strong generalization capabilities, outperforming existing self-refinement methods across various tasks and achieving performance gains without external supervision or task-specific training. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/JinyiHan99/Proactive-Self-Refine-in-LLMs/) | N/A |
| [Advances in Speech Separation: Techniques, Challenges, and Future Trends](https://arxiv.org/abs/2508.10830) | Zhuo Chen, Yi Luo, Wendi Sang, Guo Chen, JusperLee | This paper provides a comprehensive survey of deep neural network-based speech separation techniques, covering various learning paradigms, architectural components, evaluation metrics, and datasets.  It offers a unique perspective by considering higher-level learning paradigms, including scenarios with a known or unknown number of speakers, and by evaluating technological trajectories. The authors critically evaluate existing methods, identify emerging trends, and suggest promising research directions.  The work differentiates itself from previous surveys by providing fair quantitative evaluations across datasets, including benchmark results.  Finally, the paper reviews existing open-source toolkits for the benefit of the research community. | ['Audio'] | N/A | N/A |
| [Embodied-R1: Reinforced Embodied Reasoning for General Robotic
  Manipulation](https://arxiv.org/abs/2508.13998) | Fei Ni, Yibin Chen, Yaoting Huang, Haiqin Cui, Yifu Yuan |  - This paper introduces Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing, which bridges the gap between high-level vision-language comprehension and low-level action primitives.  - Embodied-R1 uses a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design and achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks.  - It demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning.  - The model exhibits high robustness against diverse visual disturbances and uses a pointing-centric representation.  - The core mechanism is to achieve unified anchoring of objects and spatial concepts through “pointing”, thereby mastering general robotic manipulation. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/pickxiguapi/Embodied-R1) | [Link](https://huggingface.co/IffYuan) |
| [Copyright Protection for Large Language Models: A Survey of Methods,
  Challenges, and Trends](https://arxiv.org/abs/2508.11548) | Xixiang Zhao, Qichen Liu, Xubin Yue, Zhenhua Xu, BreynaldDva | This paper presents a comprehensive survey of current Large Language Model (LLM) copyright protection technologies, focusing on model fingerprinting. - It clarifies the conceptual connection from text watermarking to model watermarking and fingerprinting, using a unified terminology. - It provides an overview and comparison of diverse text watermarking techniques, highlighting cases where these methods can function as model fingerprinting. - It systematically categorizes and compares existing model fingerprinting approaches for LLM copyright protection. - It presents techniques for fingerprint transfer and fingerprint removal, which are novel contributions in this area. - It summarizes evaluation metrics for model fingerprints. | ['Natural Language Processing'] | [Link](https://github.com/Xuzhenhua55/awesome-llm-copyright-protection) | N/A |
| [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669) | Abhilash Nandy, Aman Bansal, Rahul Seetharaman, Bishanka Seal | - This research paper introduces a novel approach for predicting human misery scores from natural language descriptions using large language models (LLMs). - The study compares different prompting strategies including zero-shot, few-shot, and retrieval-augmented prompting, demonstrating that few-shot and retrieval-based methods significantly outperform zero-shot baselines. - A gamified evaluation framework, "Misery Game Show", is introduced to assess the model's ability to adapt and refine predictions through feedback-driven reasoning; this shows measurable improvement in adaptive learning. - The paper uses a dataset of 516 real-world and imagined scenarios with human-annotated misery scores (0-100) to evaluate the performance of different LLMs. - The findings suggest that LLMs possess significant potential for subjective emotional reasoning tasks but also reveal limitations requiring further refinement of prompting strategies and model calibration. | ['Natural Language Processing', 'Text Classification', 'Zero-Shot Classification', 'Text Generation', 'Text2Text Generation', 'Tabular Regression'] | [Link](https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub) | N/A |
| [CorrSteer: Steering Improves Task Performance and Safety in LLMs through
  Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535) | Adriano Koshiyama, Zekun Wu, seonglae |  - CorrSteer is a novel method for improving task performance and safety in large language models (LLMs) by leveraging correlation-based sparse autoencoder (SAE) feature selection. - It addresses the limitations of existing SAE-based steering approaches by using only inference-time activations and avoiding the need for contrastive datasets or large activation storage. - CorrSteer demonstrates improved performance on various benchmarks, including question answering, bias mitigation, and safety tasks. - The selected features reveal semantically meaningful patterns that align with each task's requirements, highlighting the approach's interpretability. - The study establishes correlation-based feature selection as an effective and scalable approach for automated SAE steering across LLM applications. | ['Natural Language Processing', 'Text Generation', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/spaces/seonglae/CorrSteer) |
| [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478) | Enrico Palumbo, Edoardo D'Amico, Gustavo Penha, frafabbri, marcodena | - This paper introduces a novel approach to constructing semantic IDs for joint generative search and recommendation models, focusing on creating a unified representation that performs well for both tasks. - The proposed method uses a bi-encoder model fine-tuned on both search and recommendation datasets to generate item embeddings, which are then used to create a unified semantic ID space. - Experimental results demonstrate that the proposed method outperforms existing task-specific approaches, achieving a strong performance trade-off between search and recommendation tasks without sacrificing the effectiveness of either task. - The study also explores several ID construction strategies, including task-specific, cross-task, and embedding combination methods, showing that the proposed unified approach is more effective. - The findings suggest that using a shared representation space for semantic IDs can streamline generative model design without sacrificing quality, especially in multi-task systems. | ['Natural Language Processing', 'Text2Text Generation', 'Other'] | N/A | N/A |
| [Describe What You See with Multimodal Large Language Models to Enhance
  Video Recommendations](https://arxiv.org/abs/2508.09789) | Mounia Lalmas, Andreas Damianou, marcodena | - This paper introduces a novel framework that leverages Multimodal Large Language Models (MLLMs) to enhance video recommendations by generating rich natural-language descriptions of video clips. - The framework is model-agnostic and requires zero-finetuning, making it easily adaptable to existing recommendation systems. - Experiments on the MicroLens-100K dataset demonstrate that MLLM-generated features consistently outperform traditional video, audio, and metadata features across various recommendation models. - The findings highlight the promise of using MLLMs as on-the-fly knowledge extractors to build more intent-aware video recommenders. - The authors make their prompts and generated data publicly available to promote reproducibility. | ['Video-Text-to-Text', 'Video Classification', 'Multimodal', 'Feature Extraction', 'Summarization'] | N/A | [Link](https://huggingface.co/datasets/marcodena/video-recs-describe-what-you-see) |
| [MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic
  Evaluation of Audio General Intelligence](https://arxiv.org/abs/2508.13992) | Fernando López, Vaibhavi Lokegaonkar, Šimon Sedláček, Sonal Kumar, Sreyan88 | - MMAU-Pro, a comprehensive benchmark for evaluating holistic audio intelligence in AI systems, is introduced.  It contains 5,305 instances with expert-generated question-answer pairs spanning speech, sound, and music. - The benchmark assesses auditory intelligence across 49 unique skills and multiple complex dimensions, including long-form audio comprehension and spatial audio reasoning. - Evaluation of 22 leading multimodal AI models reveals significant limitations, with even state-of-the-art models achieving accuracy as low as 51.7%, highlighting areas for future model development. - The benchmark's novel challenges and insights are expected to accelerate the progression of AI systems towards audio general intelligence. - MMAU-Pro surpasses existing benchmarks by including a wider range of audio types, incorporating diverse question formats and requiring more complex reasoning abilities. | ['Audio', 'Audio Classification', 'Question Answering', 'Multimodal'] | [Link](https://sonalkum.github.io/mmau-pro) | N/A |
| [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186) | Jun Dong, Jiaheng Liu, Wenjie Wang, Xingyuan Bu, Shilong Li | This paper introduces MM-BrowseComp, a comprehensive benchmark designed to evaluate the multimodal browsing capabilities of AI agents.  The benchmark includes 224 challenging questions, many incorporating images and videos, requiring agents to retrieve and reason with multimodal content.  MM-BrowseComp reveals that even state-of-the-art models achieve low accuracy (around 29%), highlighting suboptimal multimodal capabilities and limited native multimodal reasoning. The benchmark also features a verified checklist for each question, enabling fine-grained analysis of agent behavior and reasoning paths.  The dataset is designed to address limitations in existing benchmarks that focus primarily on text-based information. | ['Multimodal'] | [Link](https://github.com/MMBrowseComp/MM-BrowseComp) | N/A |
| [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values
  Understanding](https://arxiv.org/abs/2508.13804) | Alina Landowska, maciejskorski | - This paper introduces a novel Bayesian framework for evaluating large language models' (LLMs) understanding of moral values, addressing limitations of previous deterministic approaches. - The framework models annotator disagreements to capture both aleatoric and epistemic uncertainties, leading to more robust and nuanced evaluations. - A large-scale evaluation across three datasets (MFTC, eMFD, MFRC) with over 250K annotations and 1M+ model queries demonstrates that LLMs consistently outperform human annotators in balanced accuracy. - Interestingly, LLMs exhibit significantly fewer false negatives than humans, suggesting a more sensitive moral detection capability. - These findings have important implications for developing ethically aligned AI systems and highlight the potential of LLMs in moral reasoning tasks. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/maciejskorski/moral-foundations-llm-eval) | N/A |


## Papers for 2025-08-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737) | Yang Li, cqgwin, Suikong, xxyyy123, runninglsy |  - Ovis2.5 is a multimodal large language model designed for native resolution visual perception and strong multimodal reasoning. It integrates a native resolution vision transformer and is trained with a five-phase curriculum.  - To enhance reasoning, it performs reflection, including self-checking and revision, exposed as an optional "thinking mode".  - Ovis2.5-9B achieves state-of-the-art results on the OpenCompass multimodal leaderboard, averaging 78.3, and Ovis2.5-2B achieves state-of-the-art results for its size, scoring 73.9.  -  The model shows strong capabilities in grounding and video tasks, and achieves leading results on STEM benchmarks and complex chart analysis.  - Two open-source models, Ovis2.5-9B and Ovis2.5-2B, are released. | ['Multimodal'] | [Link](https://github.com/AIDC-AI/Ovis) | [Link](https://huggingface.co/AIDC-AI/Ovis2.5-9B) |
| [Speed Always Wins: A Survey on Efficient Architectures for Large
  Language Models](https://arxiv.org/abs/2508.09834) | Jusen Du, Yucheng Zhou, Jiaxi Hu, Weigao Sun, landisen | This paper surveys recent advancements in efficient architectures for large language models (LLMs). - The authors systematically examine innovative LLM architectures that address the inherent limitations of transformers and boost efficiency. - The survey covers linear and sparse sequence modeling, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures, and emerging diffusion LLMs. - The authors provide a blueprint of modern efficient LLM architectures and discuss applications of efficient techniques to other modalities. - The study categorizes the reviewed studies for efficient LLM architecture, and hopes to help researchers develop scalable resource-aware foundation models. | ['Natural Language Processing'] | [Link](https://github.com/weigao266/Awesome-Efficient-Arch) | [Link](string) |
| [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142) | Ruisi Wang, Qingping Sun, Yubo Wang, yl-1993, caizhongang | This paper empirically evaluates the spatial intelligence capabilities of GPT-5 and other state-of-the-art multi-modal language models (MLLMs).  A comprehensive taxonomy of spatial tasks is proposed to unify existing benchmarks, ensuring fair evaluations.  GPT-5 demonstrates significant advancements in spatial intelligence, surpassing other models across various benchmarks. However, GPT-5 still falls short of human performance on many challenging spatial reasoning tasks, highlighting ongoing limitations in MLLMs.  The paper identifies areas where further research is needed to advance MLLMs towards achieving true spatial intelligence. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | N/A |
| [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness
  Methods for LLMs](https://arxiv.org/abs/2508.11383) | Elena Tutubalina, Gleb Ershov, Mikhail Chaichuk, apanc, myyycroft | - This paper presents the first systematic evaluation of five methods for improving prompt robustness in large language models (LLMs). - The evaluation is conducted on eight models from Llama, Qwen, and Gemma families across 52 tasks from the Natural Instructions dataset, covering various robustness methods and learning paradigms. - The study analyzes the generalization of these methods against multiple types of distribution shifts and extends the analysis to GPT-4.1 and DeepSeek V3. - The findings offer insights into the effectiveness of different robustness methods, enabling practitioners to make informed decisions. - The code for the study is publicly available on GitHub. | ['Natural Language Processing'] | [Link](https://github.com/AIRI-Institute/when-punctuation-matters) | N/A |
| [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive
  World Model](https://arxiv.org/abs/2508.13009) | Yifan Zhang, Boyang Wang, Zexiang Liu, Chunli Peng, Xianglong He | - Matrix-Game 2.0 is a novel interactive world model that generates high-quality, minute-level videos at 25 FPS using a few-step autoregressive diffusion approach, addressing limitations of existing models that rely on bidirectional attention and lengthy inference steps. - The model architecture consists of three key components: a scalable data production pipeline using Unreal Engine and GTA5, an action injection module for frame-level user inputs, and a few-step distillation technique. - Matrix-Game 2.0 outperforms existing methods in terms of speed and video quality, achieving real-time performance on a single H100 GPU.  Quantitative evaluations show improvements in visual quality, temporal coherence, and action controllability compared to baselines such as Oasis and YUME. - The model's strong generalization capability is demonstrated through experiments on diverse scenes including Minecraft, GTA5 driving scenarios, and TempleRun game environments. - The model weights and codebase are open-sourced to advance research in interactive world modeling. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598) | Daniel L. K. Yamins, Evelina Fedorenko, Greta Tuckute, klemenk | - AuriStream, a novel biologically-inspired model for speech representation, is introduced. It uses a two-stage framework: WavCoch transforms audio into a time-frequency representation based on the human cochlea, and AuriStream, an autoregressive model, predicts cochlear tokens. - AuriStream demonstrates competitive performance on various speech tasks, including phoneme and word decoding, and achieves state-of-the-art results on lexical semantics. - Unlike other models, AuriStream generates audio continuations which are interpretable in a cochleagram space, providing insights into the model's internal representations and predictions.  - The results show that the model efficiently learns short and long-range speech statistics without ground-truth phoneme, word, or task labels. - The proposed framework combines autoregressive prediction with biologically plausible inputs, contributing to advancements in human-like speech models. | ['Audio', 'Automatic Speech Recognition', 'Audio Classification', 'Text-to-Speech'] | [Link](https://tukoresearch.github.io/auristream-speech/) | N/A |
| [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision
  Mapping](https://arxiv.org/abs/2508.12466) | Tyler Derr, xuhuizhan5 | - This paper introduces Inverse-LLaVA, a novel multimodal learning approach that eliminates the need for expensive alignment pre-training by inverting the traditional mapping direction. - Unlike existing methods that project visual features into discrete text token spaces, Inverse-LLaVA maps text embeddings into continuous visual representation space and performs fusion within transformer intermediate layers. - Inverse-LLaVA achieves notable improvements on reasoning-intensive and cognitive tasks while showing expected decreases in perception tasks requiring memorized visual-text associations, providing empirical evidence that alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning tasks. - The proposed method reduces computational requirements by 45% compared to traditional methods. - Inverse-LLaVA's performance is validated across nine multimodal benchmarks demonstrating nuanced performance trade-offs across different tasks. | ['Multimodal'] | [Link](https://inverse-llava.github.io) | N/A |
| [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning
  Models to Ask for Information](https://arxiv.org/abs/2508.11252) | Xi Yang, Duanyu Feng, Chen Huang, Bowen Qin, YouchengHuang |  - This paper introduces CRITIC-math, a new benchmark dataset for evaluating the ability of Large Reasoning Models (LRMs) to ask for information when presented with incomplete mathematical problems.  - The dataset consists of two types of incomplete problems: missing goals and missing premises, with diverse contexts and challenging mathematical problems.  - Experiments on several state-of-the-art LRMs reveal their inability in proactively asking for information, highlighting behaviors related to overthinking and hallucination.  - Supervised fine-tuning (SFT) is explored as a potential solution for improving LRMs' ability to ask for information, though it shows a dilemma between solving problems and asking questions.  - CRITIC-math offers new insights into the development of LRMs with genuine intelligence, going beyond simple problem-solving abilities. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/YouchengHuang/CRITIC-math), [Link](https://huggingface.co/datasets/YouchengHuang/CRITIC-math-sft) |


## Papers for 2025-08-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630) | Wei Chen, Chaoyou Fu, Shukang Yin, Xingyu Lu, Yi-Fan Zhang | This paper introduces Thyme, a novel multimodal large language model that transcends existing "think with images" approaches by autonomously generating and executing image processing and computational operations via executable code.  Thyme uses a two-stage training strategy: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), incorporating the GRPO-ATS algorithm to balance reasoning exploration with code execution precision.  Evaluations on nearly 20 benchmarks demonstrate significant and consistent performance gains, especially in challenging high-resolution perception and complex reasoning tasks.  The datasets, sandbox, and code are publicly available. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/yfzhang114/Thyme) | [Link](https://huggingface.co/Kwai-Keye/Thyme-RL) |
| [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization](https://arxiv.org/abs/2508.10395) | Rishabh Tiwari, Haocheng Xi, Minjae Lee, Coleman Hooper, Aditya Tomar | - This paper introduces XQuant, a novel technique for reducing the memory footprint of large language model (LLM) inference by quantizing and caching layer input activations instead of standard key-value (KV) caching. - XQuant achieves up to 7.7x memory savings with less than 0.1 perplexity degradation compared to the FP16 baseline. - The authors further introduce XQuant-CL, which exploits cross-layer similarity in activations to achieve up to 10x memory savings with minimal perplexity degradation. - XQuant-CL outperforms state-of-the-art KV cache quantization methods despite using standard uniform quantization. - The results demonstrate that XQuant and XQuant-CL effectively alleviate the memory bottleneck in LLM inference while maintaining near FP16 accuracy across a wide range of models. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for
  Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255) | Mu Xu, Fan Jiang, MengChao Wang, wangqiang9 | - This paper introduces Talking-Critic, a multimodal reward model, and Talking-NSQ, a large-scale multi-dimensional human preference dataset containing 410K preference pairs, to address the limitations of existing audio-driven portrait animation methods. - A novel framework called Timestep-Layer adaptive multi-expert Preference Optimization (TLPO) is proposed to align diffusion-based portrait animation models with fine-grained, multidimensional preferences. - TLPO decouples preferences into specialized expert modules and fuses them across timesteps and network layers, enabling comprehensive, fine-grained enhancement across all dimensions. - Experiments demonstrate that Talking-Critic significantly outperforms existing methods in aligning with human preference ratings, and TLPO achieves substantial improvements over baseline models in lip-sync accuracy, motion naturalness, and visual quality. - The proposed methods exhibit superior performance in both qualitative and quantitative evaluations, showcasing the effectiveness of the proposed approach for generating high-fidelity, human-aligned audio-driven portrait animations. | ['Audio-to-Audio', 'Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://fantasyamap.github.io/fantasy-talking2/) | N/A |
| [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616) | Michal Drozdzal, Adriana Romero-Soriano, Koustuv Sinha, Pierluca D'Oro, oscmansan | - This paper introduces a novel method for reward-guided decoding of Multimodal Large Language Models (MLLMs) to improve visual grounding. - The method involves building two separate reward models to independently control object precision and recall, enabling on-the-fly controllability of an MLLM's inference process. - The proposed method allows users to dynamically trade off between object precision and recall, as well as between compute and visual grounding quality. - Evaluation on standard object hallucination benchmarks demonstrates that the method provides significant controllability over MLLM inference while consistently outperforming existing hallucination mitigation methods. - The method is shown to be effective on various MLLMs and robust to changes in the reward model's quality. | ['Multimodal'] | N/A | N/A |


## Papers for 2025-08-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual
  Mathematical Reasoning](https://arxiv.org/abs/2508.10433) | Xiaowan Wang, Yanzi Wang, Peiqing Yang, Qiuna Tan, Runqi Qiao |  - This paper introduces WE-MATH 2.0, a unified system that enhances the mathematical reasoning abilities of Multimodal Large Language Models (MLLMs). - WE-MATH 2.0 integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm. - The system includes three key components: MathBook Knowledge System, MathBook-Standard and Pro, and MathBookEval benchmark covering 491 knowledge points and 1819 fundamental principles. - The proposed MathBook-RL training framework uses a two-stage RL approach (Cold-Start Fine-tuning and Progressive Alignment RL) and outperforms existing baselines on four widely used benchmarks. - The comprehensive MathBookEval benchmark demonstrates that MathBook-RL performs well and shows promising generalization in mathematical reasoning. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://we-math2.github.io/) | N/A |
| [ToonComposer: Streamlining Cartoon Production with Generative
  Post-Keyframing](https://arxiv.org/abs/2508.10881) | Xiaoyu Li, Yaowei Li, Zhaoyang Zhang, Guangzhi Wang, Lingen Li | - ToonComposer is a novel generative model that unifies inbetweening and colorization into a single post-keyframing stage, reducing manual effort in cartoon production. - It uses a diffusion transformer (DiT) architecture with a spatial low-rank adapter (SLRA) to adapt a pre-trained video foundation model to the cartoon domain while preserving temporal consistency. - The model incorporates a sparse sketch injection mechanism, allowing for precise control with as few as a single keyframe sketch and a colored reference frame. - Experiments on synthetic and real-world benchmarks demonstrate that ToonComposer outperforms existing methods in visual quality, motion coherence, and production efficiency. - ToonComposer introduces region-wise control, enabling flexible motion generation without the need for fully drawn sketches, further reducing the workload of artists. | ['Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://lg-li.github.io/project/tooncomposer) | N/A |
| [UI-Venus Technical Report: Building High-performance UI Agents with RFT](https://arxiv.org/abs/2508.10833) | Shuheng Shen, Xingran Zhou, Zhenyu Xu, Zhengwen Zeng, Zhangxuan Gu |  *  This paper introduces UI-Venus, a novel UI agent that leverages a multimodal large language model and reinforcement learning to achieve state-of-the-art performance in UI grounding and navigation tasks.  *  The model uses only screenshots as input and is trained on a relatively small dataset of several hundred thousand samples.  *  UI-Venus outperforms existing baselines on standard grounding benchmarks (Screenspot-V2/Pro) and online UI navigation arenas (AndroidWorld), demonstrating its effectiveness.  *  The authors propose Self-Evolving Trajectory History Alignment & Sparse Action Enhancement, a novel technique that enhances the navigation performance of UI agents.  *  The code and evaluation codes are publicly available on Github, promoting further research and development in the community. | ['Reinforcement Learning', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/antgroup/UI-Venus) | N/A |
| [PRELUDE: A Benchmark Designed to Require Global Comprehension and
  Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848) | Rui Lu, Tong Li, Chulun Zhou, Tsz Ting Chung, Mo Yu | - This paper introduces PRELUDE, a new benchmark designed to evaluate long-context understanding and reasoning capabilities in large language models (LLMs). - The benchmark focuses on the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book, requiring global comprehension and deep reasoning. -  Experimental results demonstrate that state-of-the-art LLMs, even with advanced techniques like retrieval-augmented generation (RAG) and in-domain training, lag significantly behind human performance. - The substantial performance gap highlights the considerable room for improvement in LLMs' long-context understanding and reasoning abilities. -  PRELUDE provides a novel and challenging task for evaluating LLMs, pushing the boundaries of current long-context understanding research. | ['Natural Language Processing'] | [Link](https://gorov.github.io/prelude) | N/A |
| [HumanSense: From Multimodal Perception to Empathetic Context-Aware
  Responses through Reasoning MLLMs](https://arxiv.org/abs/2508.10576) | Yi Yuan, Tianqi Li, Yabing Wang, Ruobing Zheng, Zheng Qin | - The paper introduces HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of Multimodal Large Language Models (MLLMs). - HumanSense focuses on deep understanding of extended multimodal contexts and the formulation of rational feedback, covering 15 progressively challenging tests and 3882 questions. - The evaluation reveals considerable room for improvement in leading MLLMs, particularly for advanced interaction-oriented tasks; supplementing visual input with audio and text yields substantial improvements; and Omni-modal models show advantages. - A multi-stage, modality-progressive reinforcement learning approach enhances the reasoning abilities of an Omni-modal model, achieving substantial gains on evaluation results; successful reasoning processes exhibit highly consistent thought patterns. - The training-free manner prompts further enhance the performance of non-reasoning models. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Question Answering', 'Reinforcement Learning'] | [Link](https://digital-avatar.github.io/ai/HumanSense/) | N/A |
| [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875) | Zhiqiang Shen, Bowei Guo, Mingda Chen, Tianyi Li | This survey paper provides a comprehensive overview of Diffusion Language Models (DLMs), highlighting their advantages over autoregressive models in terms of speed and bidirectional context.  It details the evolution of DLMs, covering both foundational principles and state-of-the-art models, along with pre-training strategies and post-training methods.  The survey also includes a thorough analysis of DLM inference strategies and their optimization, in addition to the latest approaches in multimodal extensions. Finally, it addresses the limitations and challenges of DLMs, while outlining future research directions. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/VILA-Lab/Awesome-DLMs) | N/A |
| [When Explainability Meets Privacy: An Investigation at the Intersection
  of Post-hoc Explainability and Differential Privacy in the Context of Natural
  Language Processing](https://arxiv.org/abs/2508.10482) | Gjergji Kasneci, Florian Matthes, Ege Erdogan, Stephen Meisenbacher, Mahdi Dhaini | - This paper investigates the interplay between post-hoc explainability and differential privacy in natural language processing (NLP). - The authors empirically analyze the privacy-explainability trade-off using differentially private text rewriting methods and various post-hoc explainability techniques. - Their findings reveal a complex relationship between privacy and explainability, highlighting the influence of downstream tasks and method choices. - They also demonstrate that privacy and explainability can coexist synergistically under certain configurations. - The authors propose practical recommendations for researchers and practitioners working at the intersection of privacy and explainability in NLP. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/dmah10/xpnlp) | N/A |


## Papers for 2025-08-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery](https://arxiv.org/abs/2508.08401) | Di Zhang, Junxian Li, Qinggang Zhang, Weida Wang, Jiatong Li | - Mol-R1 is a novel framework designed to enhance the explainability and reasoning performance of explicit long chain-of-thought (CoT) reasoning LLMs in text-based molecule generation. - It introduces Prior Regulation via In-context Distillation (PRID), a distillation strategy to generate paired reasoning traces guided by prior regulations. - Mol-R1 also uses Molecular Iterative Adaptation (MoIA), a training strategy combining supervised fine-tuning (SFT) with reinforced policy optimization (RPO) to improve reasoning performance. - Experiments show Mol-R1 outperforms existing baselines in text-based molecule reasoning generation, achieving a BLEU score that is 354% higher than QWQ-32B and an exact match score 1.5 times better than DeepSeek-R1. - The framework significantly enhances the interpretability and trustworthiness of generated molecules through high-quality reasoning traces. | ['Text Generation'] | N/A | N/A |
| [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust
  GAIA Problem Solving](https://arxiv.org/abs/2508.09889) | Jinjie Gu, Chenyi Zhuang, Chengyue Yu, Qintong Wu, Zhitian Xie | - This paper introduces AWorld, a dynamic multi-agent system (MAS) for robust GAIA problem solving that uses dynamic supervision and maneuvering mechanisms to improve the stability and reliability of agent-based systems. - The MAS architecture consists of an Execution Agent and a Guard Agent; the Guard Agent verifies and corrects the reasoning process of the Execution Agent, reducing errors and improving robustness. - Experiments on the GAIA benchmark show that AWorld significantly outperforms single-agent systems and standard tool-augmented systems, achieving first place among open-source projects. - The dynamic maneuvering mechanism in AWorld improves both the effectiveness and stability of solutions, highlighting the practical value of collaborative agent roles in developing more reliable intelligent systems. - This approach is inspired by principles in control theory, particularly from marine vessel navigation, where continuous and adaptive adjustments are essential to ensure a vessel converges to a desired trajectory. | ['Question Answering'] | [Link](https://github.com/inclusionAI/AWorld) | N/A |
| [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion
  Forcing](https://arxiv.org/abs/2508.09192) | Hao Zhang, Jiachun Jin, Yijie Jin, Chenkai Xu, Xu Wang | - This paper introduces Discrete Diffusion Forcing (D2F), a novel training paradigm for Diffusion Large Language Models (dLLMs) that achieves faster-than-autoregressive (AR) inference speeds. - D2F combines block-wise autoregressive generation with inter-block parallel decoding, enabling efficient KV cache utilization and significantly increased throughput. - The proposed method utilizes an asymmetric distillation process to train D2F dLLMs, leveraging pre-trained dLLMs to avoid the high cost of training from scratch. - Empirical results demonstrate that D2F dLLMs achieve more than 2.5x inference speed compared to similarly sized AR baselines on multiple benchmarks. - The acceleration provided by D2F is shown to be more than 50x faster than other dLLMs while maintaining comparable output quality. | ['Text Generation'] | [Link](https://github.com/zhijie-group/Discrete-Diffusion-Forcing) | N/A |
| [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with
  Long-Term Memory](https://arxiv.org/abs/2508.09736) | Yuan Lin, Yiyuan Pan, Wentao Ye, Yichen He, Lin Long | This paper introduces M3-Agent, a novel multimodal agent framework equipped with long-term memory, which processes real-time visual and auditory inputs to build and update its memory.  M3-Agent outperforms existing methods by achieving 6.7%, 7.7%, and 5.3% higher accuracy on three benchmarks (M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively).  A new benchmark, M3-Bench, is also introduced for evaluating multimodal agent capabilities, including long-video question answering.  M3-Agent's design incorporates both episodic and semantic memory organized in an entity-centric format. | ['Robotics', 'Reinforcement Learning', 'Multimodal', 'Video Classification', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/bytedance-seed/m3-agent) | N/A |
| [Learning to Align, Aligning to Learn: A Unified Approach for
  Self-Optimized Alignment](https://arxiv.org/abs/2508.07750) | Lei Fan, Shuowen Zhang, Zhiling Ye, Yun Yue, Haowen Wang | - This paper introduces GRAO (Group Relative Alignment Optimization), a novel unified framework that synergizes supervised fine-tuning (SFT) and reinforcement learning (RL) for self-optimized language model alignment. - GRAO addresses the limitations of SFT (offline policy trajectory) and RL (low sample efficiency, dependency on high-quality base models) through a multi-sample generation strategy, a novel Group Direct Alignment Loss, and reference-aware parameter updates. - Theoretical analysis demonstrates GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. - Comprehensive evaluations across complex human alignment tasks show GRAO achieves significant relative improvements (57.70%, 17.65%, 7.95%, and 5.18%) over SFT, DPO, PPO, and GRPO baselines respectively. - The results indicate that GRAO enhances both the reasoning ability and alignment of language models by dynamically adjusting imitation learning and exploratory learning. | ['Natural Language Processing'] | N/A | N/A |
| [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math
  Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009) | Zhihan Zhou, Yue Guo, Zhentao Zhang, Zixin Wang, junfeng0288 |  - This paper introduces MATHREAL, a new benchmark dataset containing 2000 real-world mathematical questions with images captured by handheld devices.   -  The dataset is categorized into three primary challenges: image quality degradation, perspective variation, and irrelevant content interference, which are further divided into 14 subcategories.   -  MATHREAL spans five core knowledge and ability categories, three question types, and three difficulty levels, making it comprehensive and nuanced.   - Experiments on MATHREAL reveal a significant performance gap between existing MLLMs' abilities on real-world versus clean or processed images.   - The analysis of error patterns provides insights for future model improvements in multimodal mathematical reasoning. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/junfeng0288/MathReal) | N/A |
| [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456) | Di Zhang, Beining Xu, Junxian Li | - This paper introduces a novel input-aware backdoor attack method, IAG, for manipulating the grounding behavior of Vision-Language Models (VLMs). - IAG forces the model to ground a specific target object regardless of the user's query by embedding semantic information of the attack target's description into the original image using a text-conditional U-Net. - The attack's stealthiness is ensured by utilizing a reconstruction loss to minimize visual discrepancies between poisoned and clean images. - IAG achieves over 65% ASR@0.5 on InternVL-2.5-8B and shows promising results on Ferret-7B and LlaVA-1.5-7B, demonstrating its effectiveness and robustness. - Extensive experiments validate IAG's effectiveness across multiple VLMs and datasets, highlighting its potential threat to the security of VLMs in visual grounding tasks. | ['Multimodal'] | N/A | N/A |
| [VisCodex: Unified Multimodal Code Generation via Merging Vision and
  Coding Models](https://arxiv.org/abs/2508.09945) | Dongdong Zhang, Yixia Li, Xun Wu, Shaohan Huang, Lingjie Jiang | - VisCodex is a unified framework that merges vision and coding language models to achieve strong multimodal code generation capabilities.  The model architecture uses a task vector-based model merging technique to integrate a state-of-the-art coding LLM into a vision-language backbone. - The Multimodal Coding Dataset (MCD), a large-scale and diverse dataset of 598k samples, was introduced to support training and evaluation. MCD includes various modalities such as HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. - InfiBench-V, a challenging benchmark designed to evaluate visually rich, real-world programming questions, was also introduced.  - VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4, highlighting the effectiveness of the model merging strategy and new datasets. - Extensive experiments demonstrate VisCodex's superior performance on various benchmarks, outperforming other open-source models and approaching the performance of GPT-4. | ['Multimodal', 'Image-Text-to-Text', 'Text2Text Generation', 'Text Generation'] | [Link](https://github.com/JackLingjie/VisCodex) | N/A |
| [Can LLM-Generated Textual Explanations Enhance Model Classification
  Performance? An Empirical Study](https://arxiv.org/abs/2508.09776) | Gjergji Kasneci, Zineb Attaoui, Ege Erdogan, Juraj Vladika, Mahdi Dhaini | - This paper introduces a novel LLM-based framework for automatically generating textual explanations for natural language inference (NLI) tasks. - The framework leverages multiple state-of-the-art LLMs to generate high-quality textual explanations, rigorously evaluated using a suite of natural language generation (NLG) metrics. - Experiments demonstrate that these automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance across various pre-trained language models (PLMs) and LLMs. - The findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance. - The research addresses the lack of definitive ground-truth explanations in Explainable NLP and the limitations of traditional human annotation approaches. | ['Natural Language Processing'] | N/A | N/A |
| [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal
  Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944) | Yong Li, Jie Feng, Lixuan He | This paper introduces AMFT, a novel single-stage algorithm for aligning large language model (LLM) reasoners by learning the optimal balance between imitation and exploration.  AMFT uses a meta-gradient adaptive weight controller to dynamically adjust the balance between supervised fine-tuning (SFT) and reinforcement learning (RL).  The proposed method consistently outperforms state-of-the-art methods on various benchmark tasks such as mathematical reasoning, visual reasoning, and vision-language navigation.  AMFT's adaptive controller learns a dynamic training curriculum without the need for manually tuning hyperparameters.  Ablation studies confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance. | ['Reinforcement Learning', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/hlxtsyj/AMFT) | N/A |


## Papers for 2025-08-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent](https://arxiv.org/abs/2508.05748) | zhaoyd, callanwu, zhzhen23, richardxp888, Ornamentt | - This paper introduces WebWatcher, a novel multimodal deep research agent designed for complex information-seeking tasks that leverage both visual and textual data. - WebWatcher is equipped with enhanced visual-language reasoning capabilities and utilizes various tools for deep reasoning, including web search, image search, and code execution. - The model is trained using high-quality synthetic multimodal trajectories for efficient cold-start training and further enhanced via reinforcement learning. - To evaluate WebWatcher, the authors propose BrowseComp-VL, a challenging benchmark requiring complex information retrieval involving visual and textual data. - Experimental results on four benchmarks demonstrate that WebWatcher significantly outperforms existing baselines, including RAG workflow and open-source agents. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Alibaba-NLP/WebAgent) | N/A |
| [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086) | Yuqi Li, Wenhang Ge, Zhongqi Yang, kangfei, dearamy | - The paper introduces Matrix-3D, a novel framework for generating omnidirectional explorable 3D worlds from single images or text prompts.  It leverages a trajectory-guided panoramic video diffusion model and a panoramic 3D reconstruction module for high-quality and wide-coverage 3D world generation. - Matrix-3D uses a two-stage training strategy for 3D reconstruction: an optimization-based pipeline for detailed reconstruction and a feed-forward method for faster generation. The model integrates scene mesh renders as condition for trajectory guidance to address the issue of geometric inconsistencies in existing approaches. - The Matrix-Pano dataset, a large-scale synthetic dataset with 116K high-quality static panoramic video sequences and depth and trajectory annotations is introduced to facilitate effective training and evaluation.  - Extensive experiments demonstrate that Matrix-3D outperforms state-of-the-art methods in panoramic video generation and 3D world reconstruction in terms of visual quality, camera controllability, and reconstruction speed, according to quantitative and qualitative evaluations. - The paper also conducts ablation studies to demonstrate the effectiveness of specific design choices, such as using scene mesh renders, and the two-stage training strategy. | ['Image-to-3D', 'Text-to-3D', 'Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://matrix-3d.github.io) | N/A |
| [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale
  Asynchronous RL](https://arxiv.org/abs/2508.07976) | Chuyi He, Shusheng Xu, Minyang Xie, Wei Fu, Jiaxuan Gao | - This paper introduces ASearcher, an open-source project for large-scale reinforcement learning of search agents that achieves expert-level search intelligence. - ASearcher uses a fully asynchronous RL training method to enable long-horizon search (over 40 turns and 150k output tokens), surpassing existing open-source 32B agents. - A key contribution is a prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset for training. - ASearcher achieves substantial improvements on various benchmarks (46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively) compared to existing methods. - The agent design is simple, using only a search engine and a web browser as tools, without relying on any external LLMs. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/inclusionAI/ASearcher) | N/A |
| [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409) | Fei Shen, Yanhong Zeng, Wenran Liu, LiJiaxing, Gaojunyao | - CharacterShot is a novel framework for controllable and consistent 4D character animation, generating dynamic 3D characters from a single reference image and a 2D pose sequence. - The model architecture uses a DiT-based image-to-video model enhanced with a dual-attention module and camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency, followed by neighbor-constrained 4D Gaussian splatting for optimization. - CharacterShot outperforms existing state-of-the-art methods on the newly constructed CharacterBench benchmark, demonstrating superior performance in generating high-quality and consistent 4D character animations. - A large-scale dataset, Character4D, containing 13,115 unique characters with diverse appearances and motions, was also constructed to facilitate training and evaluation. - The framework is designed to be accessible to individual creators, democratizing the creation of high-quality 4D character animations without specialized hardware or expertise. | ['Image-to-Video', 'Text-to-Video', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/Jeoyal/CharacterShot) | N/A |
| [Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language
  Models](https://arxiv.org/abs/2508.09138) | Chenchen Jing, Bozhen Fang, Wen Wang, qiuyuu, tricktreat | - This paper introduces two novel methods to improve the performance of diffusion language models (DLLMs) by leveraging their temporal dynamics. - The first method is Temporal Self-Consistency Voting, a training-free method that aggregates predictions across multiple denoising steps to select the most consistent output. - The second method is Temporal Consistency Reinforcement, a post-training method that uses a reward signal based on the Temporal Semantic Entropy (TSE) to encourage more stable generations. - Experimental results on multiple benchmarks show that these methods significantly improve the performance of DLLMs, with an average improvement of 24.7% on the Countdown dataset. - The findings underscore the importance of temporal dynamics in DLLMs and provide simple yet effective tools to harness them. | ['Text Generation'] | [Link](https://aim-uofa.github.io/dLLM-MidTruth) | N/A |
| [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating
  Local and Web Searches](https://arxiv.org/abs/2508.08088) | Qiang Ju, Jiehan Cheng, Yan Yu, Zhicheng Dou, zstanjj | - This paper introduces HierSearch, a hierarchical agentic deep search framework that integrates both local and web searches to enhance the retrieval of information. - The framework uses hierarchical reinforcement learning (HRL) to train three agents: a local deep search agent, a web deep search agent, and a planner agent, which coordinates the low-level agents. - To prevent hallucinations and irrelevant information propagation, a knowledge refiner is incorporated to filter the output from the low-level agents, providing a more accurate final answer. - Experiments across six benchmarks show that HierSearch outperforms flat RL and various deep search baselines, demonstrating the effectiveness of its hierarchical architecture and the knowledge refiner. - The superiority of the method is evidenced by improved performance on six benchmark datasets and more effective utilization of both local and web knowledge sources. | ['Question Answering'] | [Link](https://github.com/plageon/HierSearch) | N/A |
| [Aryabhata: An exam-focused language model for JEE Math](https://arxiv.org/abs/2508.08665) | Sandeep Varma, Sachin Dharashivkar, RitvikPW | - Aryabhata 1.0 is a 7B parameter language model designed for solving mathematical problems, specifically focusing on the Indian Joint Entrance Examination (JEE). - The model is built by merging three strong open-weight reasoning models and then fine-tuned using supervised fine-tuning (SFT) with curriculum learning and reinforcement learning with verifiable rewards (RLVR). - Aryabhata outperforms existing models in accuracy and efficiency on both in-distribution (JEE Main 2025) and out-of-distribution benchmarks (MATH, GSM8K), while also providing pedagogically useful step-by-step reasoning. - The model is released as a foundation model to promote the development of exam-centric, open-source small language models. - Future work includes expanding coverage to Physics and Chemistry, scaling to the full JEE syllabus, and developing a family of exam-centric SLMs. | ['Question Answering'] | N/A | [Link](https://huggingface.co/) |
| [Train Long, Think Short: Curriculum Learning for Efficient Reasoning](https://arxiv.org/abs/2508.08940) | Marzyeh Ghassemi, Elie Bou-Zeid, Abed Hammoud, Kumail Alhamoud, Hasan Abed Al Kader Hammoud | - This paper introduces curriculum learning for length-controlled reasoning in large language models (LLMs) to improve efficiency. - It proposes a method that starts with generous token budgets and gradually tightens them over training using Group Relative Policy Optimization (GRPO), allowing the model to discover effective strategies and distill them into concise solutions. - The method is evaluated on several benchmark datasets (GSM8K, MATH500, SVAMP, College Math, and GSM+), demonstrating consistent outperformance of fixed-budget baselines in both accuracy and token efficiency. - Ablation studies show the impact of reward weighting and decay schedule design, highlighting the effectiveness of progressive constraint as an inductive bias. - The code and checkpoints are publicly available on GitHub. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/hammoudhasan/curriculum_grpo) | N/A |
| [OpenCUA: Open Foundations for Computer-Use Agents](https://arxiv.org/abs/2508.09123) | Tianbao Xie, Junlin Yang, Dunjie Lu, Bowen Wang, xywang626 | - OpenCUA is a comprehensive open-source framework for Computer-Use Agents (CUAs) that addresses critical gaps in existing research by providing diverse datasets, effective training recipes, and efficient evaluation benchmarks. - The framework introduces novel modules, including reflective reasoning and context encoding, to improve the reasoning capabilities of CUAs. - OpenCUA achieves state-of-the-art performance on multiple benchmarks, surpassing proprietary models in various aspects, including success rate and efficiency. - The framework's data processing pipeline and novel training recipes significantly enhance the performance and generalizability of CUAs, enabling the community to conduct more rigorous research. - Comprehensive evaluation benchmarks ensure the models' robustness and reliability across diverse tasks and platforms. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/OpenCUA/OpenCUA) | N/A |
| [AutoCodeBench: Large Language Models are Automatic Code Benchmark
  Generators](https://arxiv.org/abs/2508.09101) | Tao Zhang, Zhiying Zeng, Yuchi Deng, Ao Liu, Jason Chou | - AutoCodeBench, a novel multilingual code generation benchmark, is introduced to evaluate the capabilities of large language models (LLMs) in generating code across multiple programming languages. - The benchmark features a fully automated workflow built on an LLM-based sandbox, ensuring high quality, diversity, and practicality for evaluating code generation performance. - AutoCodeBench comprises a large-scale, high-difficulty multilingual benchmark covering 20 programming languages and various programming domains, outperforming existing benchmarks. - Experimental results demonstrate that even the most advanced LLMs still struggle with complex and diverse multilingual code generation problems. - AutoCodeBench provides valuable insights for the future development of code generation benchmarks and helps evaluate the capabilities of LLMs in generating code across multiple programming languages. | ['Text Generation'] | N/A | N/A |
| [BiasGym: Fantastic Biases and How to Find (and Remove) Them](https://arxiv.org/abs/2508.08855) | Arnav Arora, Haeun Yu, Siddhesh Milind Pawar, Nadav Borenstein, sekhcopenlu | - This paper introduces BiasGym, a novel framework for identifying and mitigating biases in large language models (LLMs). - BiasGym uses a two-module framework: the first module identifies biased attention heads, and the second module uses a novel fine-tuning approach to mitigate those biases. - The proposed approach outperforms existing methods across several LLMs and bias types, achieving higher performance in both bias mitigation and downstream task performance. - The framework is shown to generalize well across diverse models and bias types. - BiasGym provides a controlled setup for in-depth analysis of bias mitigation techniques in LLMs, supporting future research on fairness and safety in AI. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via
  General Samples Replay](https://arxiv.org/abs/2508.04676) | Yang Fan, Yuefeng Li, Mengchen Zhao, Shuoran Jiang, Yunan Zhang | - This paper introduces GeRe, a novel framework for efficient anti-forgetting in continual learning of LLMs via general samples replay. - GeRe leverages a set of general samples replayed using a threshold-based margin loss to mitigate catastrophic forgetting in the downstream tasks. - The proposed method demonstrates superior performance compared to existing continual learning methods, achieving better generalization capabilities and robustness. - GeRe shows significant improvements in both MMU and average accuracy across multiple downstream tasks compared to baseline methods. - The authors provide a comprehensive analysis of their method, investigating the effect of various parameters and showcasing the method's effectiveness in various continual learning scenarios. | ['Natural Language Processing'] | [Link](https://github.com/Qznan/GeRe) | N/A |
| [NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech
  Modeling with Paralinguistic Vocalizations](https://arxiv.org/abs/2508.04195) | Haoyue Zhan, Yiheng Lu, Yuancheng Wang, Qinke Ni, Huan Liao | - This paper introduces NVSpeech, an integrated and scalable pipeline for human-like speech modeling that incorporates paralinguistic vocalizations (like laughter, breathing, and interjections). - NVSpeech includes a new, manually annotated dataset of 48,430 utterances with 18 word-level paralinguistic categories and an automatically annotated corpus of 174,179 utterances (573 hours). - The pipeline develops a paralinguistic-aware ASR model that jointly transcribes lexical and non-verbal content. - NVSpeech also uses a fine-tuned zero-shot TTS model to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for human-like speech synthesis. - Experimental results demonstrate that NVSpeech outperforms baseline models on paralinguistic tagging, ASR, and zero-shot TTS tasks, showing the effectiveness of incorporating paralinguistic vocalizations for more natural and expressive speech synthesis. | ['Audio', 'Automatic Speech Recognition', 'Text-to-Speech'] | [Link](https://github.com/nvspeech170k/nvspeech) | N/A |


## Papers for 2025-08-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050) | Yuchen Li, Yutao Zhu, Weiwei Sun, Xinyu Ma, Wenhan Liu |  - ReasonRank is a novel reasoning-intensive passage reranker that significantly improves the performance of passage ranking tasks. - It employs a two-stage training framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance reasoning ability. - ReasonRank uses a multi-view ranking reward that considers both single-turn and multi-turn ranking rewards for effective training. - Experimental results on the BRIGHT and R2MED benchmarks show that ReasonRank outperforms existing state-of-the-art methods. - Ablation studies demonstrate the effectiveness of each component of the proposed framework. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/8421BCD/ReasonRank) | N/A |
| [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings
  and Speaks in Tokens](https://arxiv.org/abs/2508.05305) | Anton Razzhigaev, Andrey Kuznetsov, Elizaveta Goncharova, Temurbek Rahmatullaev, Nikita Dragunov | - This paper introduces SONAR-LLM, a decoder-only transformer that generates text by predicting a sequence of sentence embeddings and using a token-level cross-entropy objective propagated through a frozen SONAR decoder. - The model architecture combines the semantic abstraction of Large Concept Models (LCMs) with the stability of likelihood-based training, eliminating the need for diffusion samplers. - SONAR-LLM achieves competitive generation quality across different model sizes (39M to 1.3B parameters) and exhibits strong scaling properties. - Experimental results on summarization tasks (XSum and CNN/DM) demonstrate that SONAR-LLM matches or exceeds the performance of other sentence-level approaches. - The model demonstrates superior inference efficiency on long sequences compared to standard LLMs, resulting from its operation on compressed sentence embeddings. | ['Text Generation'] | [Link](https://github.com/FusionBrainLab/SONAR-LLM/tree/main) | N/A |
| [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm
  Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407) | Xinhao Yi, Yingxu Wang, Xi Zhang, Yanwen Peng, Jinyuan Fang |  - This paper introduces a novel conceptual framework for understanding and comparing self-evolving AI agents, which highlights four key components: System inputs, Agent System, Environment, and Optimizers.  - It provides a systematic review of existing techniques for self-evolving agentic systems, focusing on foundation models, agent prompts, memory, tools, workflows, and communication mechanisms.  - The paper also investigates domain-specific evolution strategies developed for specialized fields such as biomedicine, programming, and finance.  - A dedicated discussion on evaluation, safety, and ethical considerations for self-evolving agentic systems is included.  - The authors propose a set of guiding principles for safe and effective self-evolution of AI agents, inspired by Isaac Asimov's Three Laws of Robotics. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents) | N/A |
| [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785) | Tieyuan Chen, Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Haoyuan Wu | - This paper introduces Grove MoE, a novel Mixture-of-Experts (MoE) architecture for large language models (LLMs) that uses experts of varying sizes and a dynamic activation mechanism. - The Grove MoE architecture is inspired by the big.LITTLE CPU architecture and features adjugate experts that are shared among groups of experts, improving computational efficiency. - GroveMoE-Base and GroveMoE-Inst are two 33B-parameter LLMs built using the Grove MoE architecture by applying an upcycling strategy to the Qwen-30B-A3B-Base model. - Experiments show that GroveMoE models achieve performance comparable to state-of-the-art open-source LLMs of similar or even larger scales on various benchmarks. - The authors discuss the limitations of the current Grove MoE architecture, including the scarcity of long-CoT data and the exclusive reliance on rejection sampling, and suggest future research directions. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/inclusionAI/GroveMoE) | N/A |
| [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via
  Past-Future](https://arxiv.org/abs/2508.06026) | Qiufeng Wang, Junfeng Fang, Cunxiang Wang, Xin Wang, Yidong Wang | - This paper introduces Temporal Self-Rewarding Language Models, a novel method to address the diminishing preference signals in existing self-rewarding language models. - The core idea is to decouple the chosen and rejected responses by using past model outputs for rejected responses and next-generation model predictions for chosen responses, thus maintaining a clear quality gap. - The proposed two-phase framework (Anchored Rejection and Future-Guided Chosen) is evaluated on three model families (Llama, Qwen, Mistral) and various model sizes. - Experimental results across multiple benchmarks (AlpacaEval 2.0, Arena-Hard-v0.1, MT-Bench) demonstrate significant improvements over existing self-rewarding methods, achieving higher win rates and better scores, even with fewer iterations. - The approach also shows superior out-of-distribution generalization across mathematical reasoning, knowledge-based QA, and code generation tasks. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189) | Qingwei Meng, Kevin Qinghong Lin, Joya Chen, Chen Gao, Weijia Wu | This survey paper provides a comprehensive overview of recent advances in reinforcement learning (RL) applied to vision.  It categorizes over 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models.  The survey examines algorithmic design, reward engineering, and benchmark progress in each pillar.   Key challenges and promising future directions in visual RL are identified, including sample efficiency, generalization, and safe deployment.  Finally, the paper offers a structured overview of visual reinforcement learning to support future research and practical deployment. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning) | N/A |
| [Less Is More: Training-Free Sparse Attention with Global Locality for
  Efficient Reasoning](https://arxiv.org/abs/2508.07101) | Baihong Yuan, Shijie Cao, Arti Jain, Zhihao Zhang, Lijie Yang | - LessIsMore is a novel training-free sparse attention mechanism that improves efficiency and accuracy in reasoning tasks by leveraging global attention patterns and recency locality. - It addresses the limitations of existing sparse attention methods that suffer from accuracy degradation due to accumulated errors during long-generation reasoning by unifying token selection across attention heads. - The model aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. - Evaluations across diverse reasoning tasks and benchmarks show that LessIsMore achieves a 1.1× average decoding speed-up compared to full attention, attends to 2× fewer tokens without accuracy loss and achieves a 1.13× end-to-end speed-up compared to existing sparse attention methods. - LessIsMore consistently outperforms existing sparse attention methods on challenging reasoning benchmarks while maintaining or even improving accuracy. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/DerrickYLJ/LessIsMore) | N/A |
| [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation
  for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493) | Tong Yu, Chenguang Wang, Jihyung Kil, Ming Li, Jian Chen | - This paper introduces VisR-Bench, a multilingual benchmark for question-driven multimodal retrieval in long documents, containing over 35K QA pairs across 1.2K documents spanning sixteen languages. - VisR-Bench enables fine-grained evaluation of multimodal retrieval with three question types (figures, text, and tables), and queries without explicit answers to prevent superficial keyword matching. - The evaluation of various retrieval models, including text-based methods, multimodal encoders, and LLMs, shows that while LLMs significantly outperform other models, they still struggle with structured tables and low-resource languages. - VisR-Bench addresses limitations of existing benchmarks by focusing on QA relevance rather than text-image similarity and including multi-page multilingual documents. - This work highlights key challenges in multilingual visual retrieval and provides insights for improving LLMs. | ['Document Question Answering', 'Multimodal', 'Visual Question Answering', 'Table Question Answering'] | [Link](https://github.com/puar-playground/VisR-Bench) | N/A |
| [MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://arxiv.org/abs/2508.05257) | Jianguo Li, Jing Zhang, Zhenzhong Lan, Mingming Ha, Xiaodong Chen | - This paper introduces MoBE, a novel Mixture-of-Basis-Experts method for compressing MoE-based LLMs. - MoBE factorizes each expert's weight matrix using rank decomposition (W = AB), where matrix A is unique to each expert and matrix B is shared across experts as a linear combination of basis matrices. - Experiments show that MoBE achieves significantly lower accuracy drops compared to previous methods, reducing parameter counts by 24%-30% with only 1%-2% accuracy drop. - MoBE outperforms existing MoE compression methods (MoLAE and D2-MoE) across various benchmarks, demonstrating its effectiveness in compressing large MoE-based LLMs. - The code is open-sourced to encourage further research in efficient MoE architectures. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/inclusionAI/MOBE) | N/A |
| [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662) | Alexander Yavorskyi, Oleksandr Lukashov, Dmytro Vodianytskyi, Mykhailo Shtopko, Ihor Stepanov | - This paper introduces GLiClass, a novel sequence classification model based on the GLiNER uni-encoder architecture, designed for efficient and accurate text classification. - GLiClass addresses the limitations of existing methods by combining the accuracy of advanced architectures with the efficiency of embedding-based methods, achieving comparable or superior performance to cross-encoder baselines. - The model is designed to perform multi-label classification in a single forward pass and achieve non-linear scaling with the number of classes, enabling efficient handling of multiple categories and large-scale applications. - GLiClass utilizes proximal policy optimization (PPO) for multi-label text classification, allowing training in data-sparse conditions or with human feedback. - The experimental results demonstrate that GLiClass achieves state-of-the-art results on standard text classification benchmarks, outperforming strong cross-encoder baselines in terms of both accuracy and efficiency. | ['Text Classification'] | [Link](https://github.com/Knowledgator/GLiClass) | [Link](https://huggingface.co/collections/knowledgator/gliclass-v3-687a2d211b89659da1e3f34a) |
| [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
  Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601) | Robert Kirk, Tomek Korbak, Quentin Anthony, Stephen Casper, Kyle O'Brien | - This paper introduces a multi-stage data filtering pipeline for large language models (LLMs) to enhance their tamper resistance and reduce vulnerabilities related to proxy knowledge.- The pipeline involves filtering by keywords, using classifiers, and combining these techniques to mitigate different types of attacks.- Experiments show that the filtering approach achieves state-of-the-art tamper resistance, defending against fine-tuning attacks up to 10k steps and 500M tokens, and latent-space attacks.- The study also explores challenges with synthetic document training and introduces techniques to address these challenges, further improving the robustness of the LLMs.-The approach improves robustness to latent space attacks and other adversarial attacks, which makes the model more resilient, ultimately providing a significant step towards building more secure and reliable LLMs. | ['Natural Language Processing'] | N/A | N/A |
| [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059) | Reynold Cheng, Dacheng Wen, Bin Benjamin Zhu, Yupeng Li, Haorui He | - This paper introduces FACT2FICTION, the first poisoning attack framework targeting agentic fact-checking systems. - FACT2FICTION mirrors the decomposition strategy of agentic systems and leverages system-generated justifications to craft targeted malicious evidence that compromises sub-claim verification. - Experiments show that FACT2FICTION achieves 8.9%-21.2% higher attack success rates than state-of-the-art attacks across various poisoning budgets. - FACT2FICTION exposes security vulnerabilities in current fact-checking systems and highlights the need for defensive countermeasures. - The framework consists of two LLM-based agents: a Planner and an Executor, which collaboratively create and inject malicious evidence. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with
  Benign Inputs](https://arxiv.org/abs/2508.03365) | Dasol Choi, Taeyoun Kwon, Hiskias Dingeto, Bodam Kim, oneonlee | - This paper introduces WHISPERINJECT, a novel two-stage framework for launching adversarial attacks against audio-language models (ALMs). - Stage 1, Native Target Discovery, uses reinforcement learning with projected gradient descent (RL-PGD) to identify model-native harmful responses, which are then used as targets for Stage 2. - Stage 2, Payload Injection, employs projected gradient descent (PGD) to embed these payloads into benign audio carriers, making the attacks stealthy and effective. - Experiments demonstrate a success rate exceeding 86% across various state-of-the-art ALMs, including Qwen2.5-Omni, and Phi-4-Multimodal. - The work highlights a new class of practical, audio-native threats and emphasizes the need for more robust safety mechanisms in ALMs. | ['Audio', 'Audio Classification', 'Reinforcement Learning'] | [Link](https://github.com/AIM-Intelligence/WhisperInject) | N/A |
| [Compressing Chain-of-Thought in LLMs via Step Entropy](https://arxiv.org/abs/2508.03346) | Zhijian Xu, Xiangyu Wen, Ziyang Zheng, Jianyuan Zhong, Zeju Li | - This paper introduces a novel Chain-of-Thought (CoT) compression framework for Large Language Models (LLMs) based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps. - The proposed method identifies and prunes redundant steps with low entropy, achieving significant compression with minimal accuracy loss. Experiments show up to 80% of low-entropy steps can be pruned across multiple LLMs and benchmarks. - A two-stage training strategy, combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO), is proposed to enable LLMs to autonomously generate compressed CoTs. - The results demonstrate that LLMs can learn to generate compressed CoTs, further improving inference efficiency without significant accuracy degradation. - The findings offer profound implications for practical LLM deployment and a deeper understanding of reasoning structures. | ['Natural Language Processing'] | [Link](https://github.com/staymylove/COT_Compresstion_via_Step_entropy) | N/A |
| [Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations
  and Sentences](https://arxiv.org/abs/2508.03542) | Matvey Skripkin, Elvir Karimov, Artyom Iudin, Dmitrii Tarasov, Dmitrii Korzh | - This paper introduces a novel large-scale, open-source dataset (S2L) for spoken mathematical expressions and sentences, consisting of approximately 66,000 human-annotated and 571,000 synthetic audio samples. - It proposes several speech-to-LaTeX (S2L) methods, combining state-of-the-art ASR models with post-processing via fine-tuned language models (LMs) and end-to-end approaches based on Audio-LLMs. - The best models achieve an equation character error rate (CER) between 27.7% and 30.0% on English data, and a text CER up to 9.6% on mathematical sentences. - The results show that the proposed models outperform the existing MathSpeech model by a substantial margin, particularly on a newly proposed S2L-equations benchmark (27% vs. 64%). - This work establishes the first benchmark for mathematical sentence recognition and lays the groundwork for future advances in multimodal AI focused on mathematical content recognition. | ['Automatic Speech Recognition', 'Text2Text Generation', 'Multimodal'] | N/A | [Link](https://hf.co/datasets/marsianin500/Speech2Latex) |


## Papers for 2025-08-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471) | GLM-4. 5 Team, zixuanlimit, ZAHNGYUXUAN, LiquidAmmonia, Stanislas |  - This paper introduces GLM-4.5, a 355B parameter Mixture-of-Experts (MoE) large language model with a hybrid reasoning method.  - GLM-4.5 achieves strong performance on agentic, reasoning, and coding benchmarks, outperforming several competitors with fewer parameters.  - The model is trained using a multi-stage approach including pre-training, mid-training, and post-training with expert model iteration and reinforcement learning.  - A smaller version, GLM-4.5-Air (106B parameters), is also released to facilitate research.  - Both models are open-sourced along with evaluation tools. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/zai-org/GLM-4.5) | [Link](https://huggingface.co/zai-org/GLM-4.5) |
| [Pruning the Unsurprising: Efficient Code Reasoning via First-Token
  Surprisal](https://arxiv.org/abs/2508.05988) | Chengcheng Wan, Chao Hu, Yaoning Wang, Wenhao Zeng, YerbaPage | - This paper introduces ASAP, a novel framework for compressing Chain-of-Thought (CoT) reasoning in large language models (LLMs) by using an anchor-guided pruning method combined with a surprisal-based refining method.  - The ASAP framework first prunes redundant parts of the CoT using an anchor-guided approach and then refines the pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric.  - Experimental results demonstrate that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs.  - On the LiveCodeBench v4_v5 benchmark, ASAP reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline while achieving competitive accuracy.  - The authors also conduct ablation studies to validate the contribution of each component in ASAP and show the effectiveness of the two-stage pruning method. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Zengwh02/ASAP) | N/A |
| [MELLA: Bridging Linguistic Capability and Cultural Groundedness for
  Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502) | Guohang Yan, Ruirui Chen, Nuo Chen, Jiaying Fei, Yufei Gao | - This paper introduces MELLA, a novel multimodal multilingual dataset designed to improve the performance of Multimodal Large Language Models (MLLMs) in low-resource languages. - MELLA addresses the dual objectives of enhancing linguistic capabilities and cultural groundedness by employing a dual-source data strategy. - The dataset consists of 6.82 million image-text pairs across eight low-resource languages, sourced from native web alt-text and MLLM-generated captions. - Experiments demonstrate that fine-tuning on MELLA leads to significant performance improvements across various MLLM backbones, showcasing the effectiveness of the dual-objective and dual-source strategy. - The authors also provide extensive qualitative analysis to further illustrate MELLA's improvement in bridging the linguistic capability and cultural groundedness gap. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |


## Papers for 2025-08-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning](https://arxiv.org/abs/2508.05405) | Ziming Wang, Börje F. Karlsson, Ye Wang, Pi Bu, Xinrun Xu |  - DeepPHY is a novel benchmark framework designed to systematically evaluate Vision-Language Models' (VLMs) understanding of fundamental physical principles through challenging simulated environments.  - It integrates multiple physical reasoning environments of varying difficulty and incorporates fine-grained evaluation metrics, finding that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise control.  - DeepPHY systematically integrates six challenging physics-based simulation environments: PHYRE, I-PHYRE, Kinetix, Pooltool, Angry Birds, and Cut the Rope.  -  The benchmark utilizes a unified framework and standardized metrics to transform diverse physics simulators into a rigorous and accessible testbed, evaluating VLMs and collecting interaction data.  - DeepPHY reveals the boundaries and core shortcomings of current VLMs, highlighting their limitations in complex physical interaction, long-horizon planning, and dynamic adaptation. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/XinrunXu/DeepPHY) | N/A |
| [Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity](https://arxiv.org/abs/2508.05609) | Zhibing Li, Tong Wu, Ziyang Chu, Long Zhuo, Yuhan Zhang |  - Hi3DEval is a new hierarchical evaluation framework for 3D generation that assesses object-level and part-level quality, along with material evaluation via reflectance cues.  - It introduces a large-scale benchmark (Hi3DBench) with diverse 3D generative models and human-aligned annotations generated via a multi-agent, multi-modal pipeline.  - Hi3DEval uses a hybrid automated scoring system integrating video-based and naive 3D-based representations to enhance evaluations of 3D structure.  - Experiments demonstrate that Hi3DEval outperforms existing image-based metrics in modeling 3D characteristics and shows superior alignment with human preference.  - This framework provides a scalable alternative to manual evaluations and detailed diagnostic analysis capabilities. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/anonymous-mY2nG5/H3DBench) |
| [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990) | Huan Liu, Chengshuai Zhao, Zhen Tan, Dawei Li, Bohan Jiang | - This paper introduces a novel large-scale dataset containing 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. - A principle-guided LLM-as-a-judge evaluation framework is proposed, employing dual judges to assess explanation quality, which aligns well with human evaluations. - Fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) significantly improves the quality of generated explanations. - The study reveals that explanation quality varies significantly across models, audiences, and categories, with DPO- and SFT-finetuned models outperforming their larger counterparts. - This research pioneers a large-scale, systematic investigation of LLMs' capabilities in explaining well-being concepts and provides insights into the strengths and weaknesses of current LLMs in this specific task. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Can Large Multimodal Models Actively Recognize Faulty Inputs? A
  Systematic Evaluation Framework of Their Input Scrutiny Ability](https://arxiv.org/abs/2508.04017) | Yuan Wu, Yi Chang, Gengxu Li, Jinzhe Li, Haiqi Yang | - This paper introduces ISEval, a novel framework for evaluating the input scrutiny ability of Large Multimodal Models (LMMs). - ISEval uses seven categories of flawed premises and three evaluation metrics to assess LMMs' ability to proactively identify and report errors in inputs. - The evaluation of ten advanced LMMs reveals that most struggle to autonomously detect flawed inputs, relying heavily on explicit prompts. - Error type significantly impacts performance, with models excelling at logical fallacies but struggling with surface-level linguistic errors. - Cross-modal inconsistencies reveal modality preferences, with most models favoring visual input when conflicts arise. | ['Multimodal'] | [Link](https://github.com/MLGroupJLU/LMM_ISEval) | N/A |
| [InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs
  to Enhance Reasoning Capabilities](https://arxiv.org/abs/2508.05496) | Zhijie Sang, Kejing Yang, Qi Zhou, Su Lu, Shuo Cai | - InfiAlign is a novel post-training framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) in a scalable and sample-efficient manner. - It integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) and a robust data selection pipeline that automatically curates high-quality alignment data using multi-dimensional quality metrics. - When evaluated on the Qwen-2.5-Math-7B-Base model, InfiAlign achieves performance on par with DeepSeek-R1-Distill-Qwen-7B while using only approximately 12% of the training data. - Further improvements are achieved through DPO, resulting in an average improvement of 3.89% on AIME 24/25 benchmarks. - The results highlight the effectiveness of combining principled data selection with a multi-stage post-training approach for aligning LLMs to enhance reasoning capabilities. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT) |
| [Evaluating, Synthesizing, and Enhancing for Customer Support
  Conversation](https://arxiv.org/abs/2508.04423) | Feng Chen, Lifan Guo, Junhui Li, Huaixia Dou, Jie Zhu | This paper introduces the task of Customer Support Conversation (CSC) and proposes a structured framework grounded in COPC guidelines, defining five conversational stages and twelve strategies.  A new evaluation dataset, CSConv (1,855 real-world conversations), and a training dataset, RoleCS (synthetic data using LLMs), are constructed. Experiments demonstrate that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses. Human evaluations further confirm gains in problem resolution.  The contribution of this work lies in the novel framework and datasets for a previously underexplored task in NLP. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/aliyun/qwen-dianjin) | N/A |
| [Don't Overthink It: A Survey of Efficient R1-style Large Reasoning
  Models](https://arxiv.org/abs/2508.02120) | Fangzhou Yao, Weibo Gao, Yizhi Wang, Yichao Du, Linan Yue |  - This paper surveys efficient reasoning methods for R1-style Large Reasoning Models (LRMs), categorizing existing works into single-model optimization and model collaboration.  -  The authors propose a novel taxonomy to organize existing efficient reasoning methods for R1-style LRMs. -  The survey covers various efficient reasoning techniques including early exit, CoT compression, adaptive reasoning, representation engineering, and model collaboration strategies.  -  The paper also discusses future research directions such as multimodal reasoning, tool-integrated reasoning, multi-agent systems and truthful and efficient reasoning. - A public GitHub repository is maintained to track the latest progress in efficient reasoning methods. | ['Natural Language Processing'] | [Link](https://github.com/yuelinan/Awesome-Efficient-R1-style-LRMS) | N/A |
| [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923) | Taiwei Shi, Jieyu Zhang, Viraj Prabhu, Yutong Dai, Linxin Song | - CoAct-1 is a novel multi-agent system that uses coding as an enhanced action for computer-using agents, combining GUI-based control with direct programmatic execution. - It features an Orchestrator that dynamically delegates subtasks to a GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. - This hybrid approach significantly outperforms prior methods on the OSWorld benchmark, achieving a new state-of-the-art success rate of 60.76% and reducing the average number of steps required to complete a task to 10.15. - The system's efficiency is attributed to its ability to strategically bypass inefficient GUI action sequences by using code for tasks like file management and data processing. - The results demonstrate the effectiveness and scalability of integrating coding as a core action for generalized computer automation. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/salesforce/CoAct-1) | N/A |
| [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038) | Qingjuan Li, Haoqin Sun, Xuanfan Ni, Chenyang Lyu, Fengping Tian | - This paper introduces Marco-Voice, a novel multifunctional speech synthesis system that integrates voice cloning and emotion control within a unified framework. - The model architecture employs a speaker-emotion disentanglement mechanism with in-batch contrastive learning and a rotational emotion embedding integration method for smooth emotion control. - Marco-Voice achieves substantial improvements in both objective and subjective metrics compared to existing systems, showcasing competitive performance in terms of speech clarity and emotional richness. - The authors contribute CSEMOTIONS, a high-quality Mandarin emotional speech dataset with 10 hours of speech from six professional speakers across seven emotional categories, supporting comprehensive training and evaluation. - The code and dataset are publicly available, furthering research and development in the field of expressive neural speech synthesis. | ['Text-to-Speech'] | [Link](https://github.com/AIDC-AI/Marco-Voice) | [Link](https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS) |
| [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during
  Multi-Hop Analysis](https://arxiv.org/abs/2508.04699) | Reshmi Ghosh, Yashwanth Babu, Srujana Pillarichety, Isha Nalawade, Anushka Yadav | - This paper introduces a novel diagnostic framework for analyzing reasoning failures in multi-hop question answering models. - The framework categorizes errors across three dimensions: hops, coverage, and overthinking, providing a nuanced understanding of model limitations. - The authors manually annotated model traces from six language models across three datasets, revealing common reasoning issues such as incomplete reasoning and unnecessary steps. - An LLM-as-a-Judge framework was developed to scale the analysis, demonstrating strong agreement with human annotations. - Findings highlight the prevalence of overthinking and its negative impact on answer accuracy, especially in complex datasets. | ['Question Answering'] | N/A | N/A |
| [PRvL: Quantifying the Capabilities and Risks of Large Language Models
  for PII Redaction](https://arxiv.org/abs/2508.05545) | Prajit Das, Lavanya Elluri, Aritran Piplai, Anantaa Kotal, Leon Garza | - This paper introduces PRvL, an open-source suite of fine-tuned models and evaluation tools for general-purpose PII redaction, built on open-source LLMs and supporting multiple inference settings. - PRvL addresses the limitations of rule-based and NER-based methods by leveraging the contextual understanding capabilities of LLMs for accurate and privacy-aware PII redaction across domains and languages. - The research evaluates a range of LLM architectures and training strategies, including fine-tuning, instruction-tuning, and retrieval-augmented generation, measuring redaction performance, semantic preservation, and PII leakage. - The empirical results provide practical guidance for configuring LLM-based redactors and demonstrate the effectiveness of instruction-tuning for high-accuracy, generalizable PII redaction. - The open-source nature of PRvL, along with its flexibility and compliance features, enables data owners to perform redactions within their own secure environments without relying on third-party services. | ['Natural Language Processing', 'Token Classification', 'Text2Text Generation'] | [Link](https://anonymous.4open.science/r/PRvL-C1BF) | [Link](https://huggingface.co/datasets/ai4privacy/pii-masking-300k) |
| [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating
  Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939) | Chirag Shah, Aman Chadha, Tanya Roosta, Julia Kharchenko |  - This paper introduces a new benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths, which are subtle linguistic markers that can reveal demographic attributes.  - The benchmark uses 100 validated question-response pairs in simulated interview scenarios to measure how LLMs systematically penalize certain linguistic patterns.  -  The paper demonstrates that hedged responses receive significantly lower ratings than confident responses despite equivalent content quality.  - The benchmark's effectiveness is validated by identifying model-specific biases in various LLMs.  - The proposed framework provides a foundation for detecting and measuring linguistic discrimination in AI systems. | ['Natural Language Processing'] | N/A | N/A |
| [I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal
  Entity Linking](https://arxiv.org/abs/2508.02243) | Chao Wang, Tong Ruan, Kaiwen Li, Junwen Li, Ziyan Liu | - This paper introduces I2CR, a novel framework for multimodal entity linking that prioritizes text information and uses visual clues iteratively when necessary. - The I2CR framework employs a multi-round iterative strategy, integrating key visual features from various aspects of the image to support reasoning and enhance accuracy. - Experiments on three widely used public datasets demonstrate that I2CR outperforms existing state-of-the-art methods, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. - The framework addresses the challenges of unnecessary image data incorporation and reliance on one-time visual feature extraction present in previous methods. - The core of the framework is a fine-tuned Large Language Model that makes initial entity selections, and then employs intra- and inter-modal consistency checks and visual feedback loops before returning a final result. | ['Multimodal'] | [Link](https://github.com/ziyan-xiaoyu/I2CR/) | N/A |


## Papers for 2025-08-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191) | Zhen Tan, Bohan, wjldw, ympc08, chengshuaizhao | - This paper introduces DATAALCHEMY, a controlled environment for training LLMs from scratch to investigate the effects of distributional shifts on chain-of-thought (CoT) reasoning. - The authors propose a data distribution lens for analyzing CoT reasoning, arguing that its effectiveness is fundamentally limited by the discrepancy between training and test data distributions. - Their findings reveal that CoT reasoning is a fragile phenomenon, easily breaking down under moderate distributional shifts. - The study examines CoT reasoning across three dimensions: task, length, and format, showing that its performance significantly degrades even with modest changes in these dimensions. - The work underscores the need to develop models with genuine and generalizable reasoning capabilities, moving beyond surface-level pattern matching. | ['Natural Language Processing'] | [Link](https://github.com/ChengshuaiZhao0/DataAlchemy) | [Link](None) |
| [LaTCoder: Converting Webpage Design to Code with Layout-as-Thought](https://arxiv.org/abs/2508.03560) | Tianpeng Lv, Guohao Wang, Zhongyi Zhang, Zhen Li, starmage520 | - The paper introduces LaTCoder, a novel approach for converting webpage designs into code that leverages Layout-as-Thought (LaT) to improve layout preservation during code generation. - LaTCoder divides the webpage design into image blocks, uses a chain-of-thought prompt to generate code for each block using an MLLM, and then assembles the code using either absolute positioning or MLLM-based assembly, dynamically choosing the better method. - LaTCoder outperforms existing design-to-code methods on both a public dataset (Design2Code-Hard) and a newly introduced dataset (CC-HARD) based on human preference evaluation and metrics. - The CC-HARD dataset is introduced as a more challenging dataset for evaluating design-to-code models, containing webpages with more complex layouts than existing datasets. - The study evaluates the effectiveness of LaTCoder using multiple backbone models (DeepSeek-VL2, Gemini, and GPT-4) on both datasets, demonstrating consistent improvement across all models. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/CGCL-codes/naturalcc/tree/main/examples/latcoder) | [Link](https://huggingface.co/datasets/xcodemind/CC-HARD) |
| [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web
  Agents](https://arxiv.org/abs/2508.01858) | Xinyu Yang, Hongliang He, Aiwen Sun, Cong Guo, Gnonymous | - This paper introduces Web-CogReasoner, a novel multimodal large-scale model for web agents that leverages a knowledge-driven Chain-of-Thought (CoT) reasoning framework. - The model's architecture incorporates three knowledge domains: Factual, Conceptual, and Procedural, which are systematically instilled using the Web-CogDataset and Web-CogBench. - Extensive experiments demonstrate that Web-CogReasoner significantly outperforms existing state-of-the-art models on various benchmarks, particularly in generalization to unseen tasks. - The Web-CogDataset comprises 12 fine-grained tasks meticulously designed to incrementally build the agent's knowledge, cognition, and reasoning abilities. - The Web-CogBench is a novel evaluation suite that comprehensively assesses agent performance across delineated knowledge domains and cognitive capabilities. | ['Multimodal'] | [Link](https://github.com/Gnonymous/Web-CogReasoner) | N/A |
| [LeanK: Learnable K Cache Channel Pruning for Efficient Decoding](https://arxiv.org/abs/2508.02215) | Yuqing Yang, Chengruidong Zhang, Huiqiang Jiang, hzy46, zhangyik21 | - LeanK is a novel learning-based method for pruning unimportant key (K) cache channels in large language models (LLMs) to improve decoding efficiency.  - It employs a two-stage training process to learn a channel-wise static mask that satisfies specific sparsity ratios and hardware alignment requirements.  - Experiments show that LeanK achieves up to 70% K cache and 16%-18% V cache memory reduction, resulting in a 1.3x speedup for attention computation.  - The method maintains accuracy while reducing memory usage and enhancing decoding speed.  - LeanK is compatible with existing KV cache optimization techniques and can be combined for further acceleration. | ['Natural Language Processing'] | [Link](https://aka.ms/LeanK) | [Link](None) |
| [Sculptor: Empowering LLMs with Cognitive Agency via Active Context
  Management](https://arxiv.org/abs/2508.04664) | Yunxin Liu, Ting Cao, Qitai Tan, L. H. Xu, Mor-Li | - This paper introduces Sculptor, a novel framework that enhances LLMs by enabling them to actively manage their internal working memory. - Sculptor equips LLMs with tools for context fragmentation, summarization, hiding/restoring information, and intelligent search, allowing them to selectively focus on relevant information and filter out distractions. - Experimental evaluations on PI-LLM and NeedleBench Multi-Needle Reasoning benchmarks demonstrate that Sculptor significantly improves LLM performance on long-context tasks, even without specific training. - The key advantage of Sculptor lies in its ability to mitigate proactive interference, which is a significant challenge in processing long contexts, by enabling LLMs to selectively manage the context instead of simply increasing the context window. - Sculptor's active context management strategy provides a more cognitive approach to handling long contexts than simply enlarging the context window, leading to greater reliability and robustness at scale. | ['Natural Language Processing'] | N/A | N/A |
| [DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a
  Stage-Wise Diffusion Transformer Framework](https://arxiv.org/abs/2508.02807) | Chao Liang, Ente Lin, Shuliang Ning, Zaiyu Huang, Tongchun Zuo | - DreamVVT is a novel two-stage framework for high-fidelity video virtual try-on, leveraging diffusion transformers and addressing limitations of existing methods by utilizing unpaired data and pretrained models.  - The first stage generates high-fidelity try-on images for keyframes, using a multi-frame try-on model integrated with a vision-language model for semantic consistency.  - The second stage synthesizes a coherent try-on video using a pretrained video generation model enhanced with LoRA adapters, guided by keyframe try-on images, motion information, and textual descriptions.  - Experimental results on the ViViD and Wild-TryOn datasets demonstrate that DreamVVT surpasses existing methods in terms of garment detail preservation, temporal consistency, and generalization to unseen scenarios.  - Ablation studies confirm the effectiveness of the two-stage design and the use of LoRA adapters for efficient fine-tuning. | ['Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Enhancing Vision-Language Model Training with Reinforcement Learning in
  Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280) | Ruslan Rakhimov, Viacheslav Sinii, Stanislav Dereka, kefirski, GeorgeBredis | - This paper introduces Vision-Language Decoupled Actor-Critic (VL-DAC), a novel reinforcement learning algorithm for training vision-language models (VLMs). - VL-DAC decouples policy and value updates, resulting in faster, more stable training compared to previous methods like RL4VLM and LOOP. - The algorithm is evaluated across multiple lightweight simulators and is shown to transfer learned skills to real-world benchmarks with measurable improvements in agentic control, spatial planning, and web navigation. - Experiments demonstrate that VL-DAC achieves +50% relative improvement on BALROG, +5% on VSI-Bench, and +2% on VisualWebBench. - The authors contribute a simple, hyperparameter-free RL algorithm that enables effective transfer learning from synthetic environments to complex real-world tasks. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/corl-team/VL-DAC) | N/A |
| [A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding](https://arxiv.org/abs/2508.01197) | Jianke Zhu, Junbo Chen, Zhan Shi, songw-zju | - This paper introduces Talk2Occ, a novel benchmark dataset for 3D occupancy grounding in autonomous driving, and GroundingOcc, a new end-to-end model for this task. - GroundingOcc combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine, incorporating a multimodal encoder, an occupancy head, and a grounding head. - The model architecture includes a 2D grounding module and a depth estimation module to enhance geometric understanding, improving model performance. - Extensive experiments on Talk2Occ demonstrate that GroundingOcc outperforms existing baselines on 3D occupancy grounding, achieving a significant improvement in localization accuracy. - The dataset and code are publicly available on GitHub, enabling further research and development in this area. | ['Multimodal', 'Text-to-3D', 'Image-to-3D', 'Object Detection', 'Depth Estimation', 'Computer Vision'] | [Link](https://github.com/RONINGOD/GroundingOcc) | N/A |
| [Reasoning Language Models for Root Cause Analysis in 5G Wireless
  Networks](https://arxiv.org/abs/2507.21974) | Haozhe Zhang, Yibin Kang, Antonio De Domenico, Mohamed Sana, nicopi | - This paper introduces a novel lightweight framework for Root Cause Analysis (RCA) in 5G wireless networks using Large Language Models (LLMs). - A new curated dataset, TeleLogs, is introduced to benchmark RCA capabilities and evaluate the performance of LLMs in this task. - The authors propose a two-stage training methodology that leverages supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs for RCA. - The proposed approach significantly outperforms existing open-source reasoning LLMs on the TeleLogs dataset, achieving accuracy gains of up to 7x in some cases. - Extensive experiments demonstrate the effectiveness of the proposed method, highlighting the potential of domain-adapted reasoning-enhanced LLMs for practical and explainable RCA in network operation and management. | ['Natural Language Processing', 'Reinforcement Learning', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/netop/TeleLogs) |
| [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with
  Verifiable Rewards](https://arxiv.org/abs/2508.04632) | Ling-I Wu, Xiaogui Yang, Tong Jian, Tianyi Liang, Xu Guo |  - This paper introduces IFDecorator, a framework that enhances Reinforcement Learning with Verifiable Rewards (RLVR) for instruction following in large language models (LLMs).  - IFDecorator addresses two key limitations of RLVR4IF: training inefficiency due to inadequate difficulty assessment and over-optimization (reward hacking).  - It consists of three components: a cooperative-adversarial data flywheel, IntentCheck (a bypass module), and trip wires (a diagnostic mechanism).  - IFDecorator significantly improves instruction-following performance on multiple benchmarks, outperforming larger proprietary models while preserving general capabilities.  - The trip wires effectively reduce reward hacking rates, demonstrating the robustness of the framework. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/guox18/IFDecorator) | [Link](https://huggingface.co/datasets/guox18/IFDecorator) |
| [SonicMaster: Towards Controllable All-in-One Music Restoration and
  Mastering](https://arxiv.org/abs/2508.03448) | Ambuj Mehrish, Jan Melechovsky, dorienh | - This paper introduces SonicMaster, a unified generative model for music restoration and mastering that addresses various audio artifacts with text-based control. - The model architecture uses a flow-matching generative training paradigm and combines a VAE codec with multimodal and dual-stream DiT blocks to process audio and text inputs and produce high-fidelity enhanced outputs. - SonicMaster is trained on a large dataset of paired degraded and high-quality music tracks, created by simulating common degradation types with nineteen degradation functions. - Objective and subjective evaluations demonstrate that SonicMaster significantly improves sound quality across all artifact categories compared to baselines and that listeners prefer SonicMaster's outputs over original degraded audio. - The model, code, and dataset are available through the provided GitHub link. | ['Audio-to-Audio'] | [Link](https://amaai-lab.github.io/SonicMaster/) | N/A |
| [MiDashengLM: Efficient Audio Understanding with General Audio Captions](https://arxiv.org/abs/2508.03983) | Yadong Niu, Jian Luan, Jizhong Liu, Gang Li, Heinrich Dinkel | - MiDashengLM is a novel open audio-language model designed for efficient and comprehensive audio understanding using general audio captions. - The model integrates Dasheng, an open-source audio encoder, to process diverse auditory information effectively, unlike previous works which primarily focus on ASR-based audio-text alignment. - MiDashengLM is trained using publicly available datasets, ensuring full transparency and reproducibility. - It achieves a 4x speedup in time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. - Experiments show MiDashengLM outperforms baseline models on various benchmarks including audio captioning, question answering, and audio classification tasks. | ['Audio'] | [Link](https://github.com/xiaomi-research/dasheng-lm) | N/A |
| [Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and
  Self-Checking for Complex Instruction Following](https://arxiv.org/abs/2508.03178) | Liang Xu, Xiangzheng Zhang, Shousheng Jia, Liang Wen, Chenyang Wang | - The paper introduces Light-IF, a novel framework that enhances LLMs' generalizable reasoning abilities for complex instruction following through preview and self-checking mechanisms. - Light-IF addresses the issue of "lazy reasoning" in LLMs by employing an entropy-preserving supervised fine-tuning strategy coupled with a token-wise entropy-adaptive reinforcement learning approach. - The framework involves generating a dataset of complex instructions and uses rejection sampling to curate a high-quality subset for training. - Experimental results on various instruction-following benchmarks demonstrate that Light-IF significantly outperforms existing LLMs, achieving state-of-the-art performance across various model scales. - Light-IF-32B surpasses both large open-source models like DeepSeek-R1 and closed-source models like Doubao-1.6. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/qihoo360/Light-IF-32B) |


## Papers for 2025-08-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed
  Inference](https://arxiv.org/abs/2508.02193) | Fan Xia, Pengyang Gao, Cheng Luo, Zheng Zhang, Yuxuan Song | - The paper introduces Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion that offers remarkably fast inference speed. - The model achieves an inference speed of 2,146 tokens/s on H20 GPUs while maintaining competitive performance across various code evaluation benchmarks. - Seed Diffusion Preview's speed is significantly faster than contemporary models like Mercury and Gemini, establishing a new state-of-the-art in speed-quality tradeoff. - The model employs a two-stage curriculum for robust diffusion training, including mask-based and edit-based forward processes. - An on-policy learning paradigm is introduced to unlock parallel processing during inference, further enhancing efficiency. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding
  and Generation](https://arxiv.org/abs/2508.03320) | Tianyidan Xie, Liang Hu, Yimeng Gan, Yi Peng, Peiyu Wang | - Skywork UniPic is a 1.5-billion parameter unified autoregressive model that combines image understanding, text-to-image generation, and image editing into a single architecture. - The model uses a decoupled encoding strategy with a masked autoregressive encoder for generation and a SigLIP2 encoder for understanding, both feeding into a shared autoregressive decoder. - Skywork UniPic outperforms most existing unified models, achieving a GenEval score of 0.86, a DPG-Bench complex-generation record of 85.5, and high scores on image editing benchmarks. - The model is trained using a progressive, resolution-aware schedule and meticulously curated datasets, achieving high fidelity while maintaining efficiency (under 15 GB GPU memory). - Code and weights are publicly available. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image'] | [Link](https://github.com/SkyworkAI/UniPic) | [Link](https://huggingface.co/Skywork/Skywork-UniPic-1.5B) |
| [CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and
  Outcome Reward](https://arxiv.org/abs/2508.03686) | Songyang Gao, Linchen Xiao, Junnan Liu, Hongwei Liu, Shudong Liu | CompassVerifier is a lightweight and robust verifier model designed for evaluating and providing outcome rewards for large language models.  It addresses limitations in existing methods by offering comprehensive benchmarks and handling complex edge cases. CompassVerifier demonstrates multi-domain competency across math, knowledge, and reasoning tasks and can process various answer types. The model is enhanced by the VerifierBench benchmark, a dataset of model outputs augmented with manual analysis of error patterns.  Experiments show CompassVerifier outperforms existing models on the VerifierBench benchmark, establishing a new state-of-the-art. | ['Question Answering'] | [Link](https://github.com/open-compass/CompassVerifier) | N/A |
| [LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?](https://arxiv.org/abs/2508.01780) | Yaojie Lu, Xuanang Chen, Jiawei Chen, Wenliang Zhong, Guozhao Mo | - The paper introduces LiveMCPBench, a comprehensive benchmark for evaluating large language model (LLM) agents' tool-use capabilities within the Model Context Protocol (MCP) ecosystem. - LiveMCPBench features 95 real-world tasks across diverse domains and a curated toolset (LiveMCPTool) encompassing 70 servers and 527 tools. - The benchmark incorporates LiveMCPEval, an LLM-as-a-Judge framework for automated and adaptive evaluation in dynamic task environments. - Evaluation on 10 leading models reveals significant performance variance, with the best-performing model achieving 78.95% success rate, highlighting challenges in meta-tool learning. - The paper also introduces MCP Copilot Agent, a multi-step agent showcasing dynamic planning and API interaction across the entire tool suite. | ['Natural Language Processing'] | [Link](https://github.com/icip-cas/LiveMCPBench) | N/A |
| [AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided
  Decomposition and Riemannian-Geodesic Collision Regularization](https://arxiv.org/abs/2508.02079) | Aman Chadha, Vinija Jain, Abhilekh Borah, Amitava Das |  - This paper introduces ALIGNGUARD-LORA, a novel framework for preserving the alignment of large language models (LLMs) during low-rank fine-tuning, mitigating alignment drift. - ALIGNGUARD-LORA decomposes LoRA updates into alignment-critical and task-specific components, using the Fisher Information Matrix to identify and regularize alignment-sensitive directions. - The framework incorporates collision-aware regularization, which blends Riemannian overlap and geodesic separation penalties to ensure structural disentanglement between updates. - Empirical evaluations demonstrate that ALIGNGUARD-LORA mitigates alignment drift by up to 50% on safety-critical benchmarks without sacrificing downstream task performance. - The authors also propose DRIFTCHECK, a targeted diagnostic benchmark for quantifying alignment drift and safety degradation, and validate a scaling law for catastrophic forgetting. | ['Natural Language Processing'] | N/A | N/A |
| [TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to
  Training-Time Belief Sources in LLMs](https://arxiv.org/abs/2508.02063) | Aman Chadha, Vinija Jain, Amitava Das | *- TRACEALIGN is a novel framework that traces the root causes of unsafe large language model (LLM) outputs back to their training data. - It leverages a suffix-array based index (TRACEINDEX) and a belief conflict index (BCI) to pinpoint and quantify problematic belief fragments. - TRACEALIGN proposes three defense mechanisms: TRACESHIELD (an inference-time safety filter), Contrastive Belief Deconfliction Loss (a contrastive fine-tuning objective), and Prov-Decode (a provenance-aware decoding strategy). - Experiments on a curated Alignment Drift Benchmark (ADB) demonstrate that these defenses reduce alignment failures by up to 85% while maintaining utility. - The study advances alignment research by shifting the focus from surface behavior to the underlying beliefs within the model, enabling the construction of more robust and accountable LLMs. | ['Natural Language Processing'] | [Link](https://anonymous.4open.science/r/tracealign-2DA7) | N/A |


## Papers for 2025-08-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324) | Kaiyuan Gao, Junyang Lin, Jingren Zhou, Jiahao Li, Chenfei Wu | The paper introduces Qwen-Image, a multimodal image generation foundation model that achieves significant improvements in complex text rendering and precise image editing.  The model utilizes a dual-encoding mechanism incorporating Qwen2.5-VL and a VAE encoder to balance semantic consistency and visual fidelity during editing.  Evaluated on multiple benchmarks, Qwen-Image demonstrates state-of-the-art performance, particularly excelling in text rendering tasks, especially in Chinese.  The model architecture employs an improved multi-task training paradigm and a curriculum learning strategy for enhanced capabilities. | ['Text-to-Image', 'Image-to-Image', 'Image-to-Text', 'Multimodal'] | [Link](https://github.com/QwenLM/Qwen-Image) | [Link](https://huggingface.co/Qwen/Qwen-Image) |
| [SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic
  Association and Long Story Comprehension](https://arxiv.org/abs/2508.01959) | Liyan Xu, Lemao Liu, Yuqing Li, Jiangnan Li, Junjie Wu | - This paper introduces SitEmb, a novel situated embedding model for improved context-aware dense retrieval, addressing limitations of existing methods in handling long documents and complex semantic associations. - The model incorporates a broader context window into short chunk embeddings, enhancing retrieval performance without straining model capacity. - SitEmb significantly outperforms state-of-the-art embedding models on a curated book-plot retrieval dataset, showcasing superior performance with fewer parameters. - The model demonstrates strong results across diverse downstream applications, including long story comprehension tasks and semantic association tasks. - A residual learning framework is used to enhance situated context usage, enabling the model to focus on additional contextual information beyond shallow heuristics. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/SituatedEmbedding) |
| [Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report](https://arxiv.org/abs/2508.01059) | Anu Vellore, Baturay Saglam, Blaine Nelson, Paul Kassianik, Sajana Weerawardhena | - This paper introduces Foundation-Sec-8B-Instruct, a cybersecurity-specialized large language model (LLM) built upon Llama 3.1. - The model undergoes instruction tuning and post-training on cybersecurity-focused datasets to enhance its cybersecurity knowledge and instruction-following capabilities. - Evaluation on various benchmarks demonstrates that Foundation-Sec-8B-Instruct achieves state-of-the-art performance on cybersecurity-specific tasks and competitive results on general post-training benchmarks. - The study analyzes the model's cybersecurity knowledge distribution across various topics and identifies potential biases and limitations, suggesting areas for further improvement. - A safety analysis is performed to evaluate the model's vulnerability to malicious prompts and toxic outputs, proposing safety mechanisms to mitigate risks. | ['Natural Language Processing', 'Text Classification', 'Text Generation'] | N/A | [Link](https://huggingface.co/Foundation-models/Sec-8B-Instruct) |
| [InstructVLA: Vision-Language-Action Instruction Tuning from
  Understanding to Manipulation](https://arxiv.org/abs/2507.17520) | Yang Tian, Bin Wang, Yilun Chen, Hao Li, Shuai Yang |  - InstructVLA is a novel end-to-end vision-language-action (VLA) model that integrates multimodal reasoning with precise action generation, bridging the gap between understanding and manipulation.  - It introduces Vision-Language-Action Instruction Tuning (VLA-IT), a new training paradigm that jointly optimizes textual reasoning and action generation using a Mixture-of-Experts adaptation strategy.  - On SimplerEnv tasks, InstructVLA outperforms existing methods such as SpatialVLA by 30.5% and achieves a 92% improvement over OpenVLA on SimplerEnv-Instruct.  - InstructVLA also demonstrates superior performance on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning.  - The model shows strong potential for bridging intuitive and steerable human-robot interaction with efficient policy learning. | ['Robotics', 'Multimodal', 'Reinforcement Learning'] | N/A | N/A |


## Papers for 2025-08-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language
  Models](https://arxiv.org/abs/2508.00819) | Jiaqi Wang, Yuhang Cao, Yuhang Zang, Xiaoyi Dong, Jinsong Li | - This paper introduces DAEDAL, a novel training-free denoising strategy for Diffusion Large Language Models (DLLMs) that addresses the limitation of statically predefined generation lengths. - DAEDAL operates in two phases: 1) Initial Length Adjustment, where it iteratively expands the generation length guided by a sequence completion metric, and 2) Iterative Mask Insertion, where it dynamically expands insufficient generation regions. - Experiments on various benchmarks demonstrate that DAEDAL achieves performance comparable to, and in some cases superior to, meticulously tuned fixed-length baselines, while simultaneously improving computational efficiency. - By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging the gap with autoregressive counterparts. - The method dynamically adjusts the length based on the task's complexity, resulting in improved performance and computational efficiency compared to fixed-length baselines. | ['Text Generation'] | [Link](https://github.com/Li-Jinsong/DAEDAL) | N/A |
| [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265) | Zuxuan Wu, Chang Liu, Shuting He, Song Tang, Henghui Ding | This survey paper provides a comprehensive overview of multimodal referring segmentation across various visual scenes (images, videos, and 3D scenes) and modalities (text, audio, and multi-modal cues).  The authors introduce a unified meta-architecture for referring segmentation, followed by a detailed review of representative methods for each scene and modality.  The survey also covers more recent trends such as Generalized Referring Expression (GREx) methods and their applications. Finally, the authors analyze existing benchmarks and their limitations. | ['Image Segmentation', 'Video Classification', 'Multimodal'] | [Link](https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation) | N/A |
| [3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding](https://arxiv.org/abs/2507.23478) | Hao Tang, Zeyu Zhang, Ting Huang |  - 3D-R1 is a foundation model that enhances the reasoning capabilities of 3D Vision-Language Models (VLMs) by leveraging reinforcement learning and a high-quality synthetic dataset, Scene-30K.  - The model architecture employs a multi-modal approach, integrating text, multi-view images, 3D point clouds, and depth maps to perform comprehensive 3D tasks as autoregressive sequence prediction. - 3D-R1 incorporates three reward functions within a GRPO-based RLHF policy to enhance reasoning capabilities, including perception, semantic similarity, and format rewards. - Experiments demonstrate that 3D-R1 significantly outperforms existing methods on various 3D scene understanding benchmarks, achieving an average improvement of 10%. - A dynamic view selection strategy is introduced to improve model efficiency and reduce reliance on pre-defined viewpoints. | ['Multimodal', 'Text-to-3D', 'Image-to-3D', 'Visual Question Answering', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/AIGeeksGroup/3D-R1) | N/A |
| [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348) | Heng Lian, Xiaodong Gu, Shaoxin Lin, Yuling Shi, Han Li | - This paper introduces SWE-Debate, a novel competitive multi-agent debate framework for resolving software issues. - SWE-Debate leverages fault propagation traces generated from code dependency graphs and structured three-round debates among specialized agents to achieve more consolidated issue localization. - Experimental results on the SWE-bench benchmark demonstrate that SWE-Debate outperforms existing baselines by a large margin, achieving state-of-the-art results in open-source agent frameworks. - Ablation studies highlight the significant contributions of multiple chain generation, multi-agent debate, and the MCTS-based code modification agent to the overall performance. - The proposed method addresses the limitations of existing agent-based approaches by incorporating diverse reasoning paths and structured competitive analysis. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering', 'Graph Machine Learning'] | [Link](https://github.com/YerbaPage/SWE-Debate) | N/A |
| [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454) | Chengfei Lv, Zhiwen Chen, Yunfeng Wang, Kehua Feng, Yuqi Tang | - This paper introduces MTDEval, a novel multi-turn dialogue evaluator that efficiently aggregates preference knowledge from multiple Large Language Model (LLM) judges into a single model. - The model architecture consists of a text-embedding model with specialized scoring heads, trained using a learning-to-rank strategy. - MTDEval outperforms existing baselines across various multi-turn dialogue evaluation benchmarks, demonstrating its effectiveness and robustness in single rating, pairwise comparison, and multi-dimensional comparison tasks. - The efficiency of MTDEval is highlighted by its significantly reduced computational cost during inference compared to traditional multi-judge methods. - A large-scale pairwise preference dataset (P2-MTD) and a high-quality human-annotated evaluation dataset (Daily-MTD) are constructed and released to facilitate future research. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/James-TYQ/MTDEval) | N/A |
| [Investigating Hallucination in Conversations for Low Resource Languages](https://arxiv.org/abs/2507.22720) | Fatemeh Jamshidi, Zheng Zhang, Souvika Sarkar, Md. Najib Hasan, Amit Das | - This paper investigates hallucination in conversational data across three low-resource languages (Hindi, Farsi, and Mandarin) using six different LLMs. - The main contribution is a comprehensive analysis of hallucination tendencies in these languages, considering both factual and linguistic errors. - The findings show significantly higher hallucination rates in Hindi and Farsi compared to Mandarin, highlighting the impact of data availability on LLM performance. - The study also analyzes the performance of different LLMs across these languages, showing that some models are more robust to hallucination than others. - The authors suggest several mitigation techniques for addressing hallucination, such as incorporating retrieval-augmented generation and using models specifically pretrained for the target languages. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/AmitDasRup123/LLM-Hallucination-Low-Resource-Languages/) | N/A |
| [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware
  Video Generation](https://arxiv.org/abs/2508.00782) | Long Chen, Qifeng Chen, Yazhou Xing, Yingqing He, Kien T. Pham | - SpA2V is a novel framework that leverages spatial auditory cues from audio to generate realistic videos with accurate semantic and spatial alignment. - The model uses a two-stage approach: Audio-guided Video Planning and Layout-grounded Video Generation. - Audio-guided Video Planning uses a Multimodal Large Language Model (MLLM) to construct Video Scene Layouts (VSLs) that capture spatial and semantic information from the audio. - Layout-grounded Video Generation uses a pre-trained diffusion model to generate videos based on the VSLs, achieving training-free video generation. - Experiments on a new benchmark, AVLBench, demonstrate that SpA2V outperforms state-of-the-art methods in generating videos with high semantic and spatial correspondence to input audios. | ['Audio-to-Audio', 'Text-to-Video', 'Audio Classification', 'Video Classification', 'Multimodal'] | [Link](https://github.com/tkpham3105/SpA2V) | N/A |


## Papers for 2025-08-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Phi-Ground Tech Report: Advancing Perception in GUI Grounding](https://arxiv.org/abs/2507.23779) | Kai Qiu, Qi Dai, Jialiang Zhu, Ziqiang Xu, Miaosen Zhang | [- The paper introduces Phi-Ground, a family of models achieving state-of-the-art performance across five GUI grounding benchmarks for models under 10B parameters in agent settings. - The models adopt a two-stage implementation: a large language model generates detailed reference expressions, and a smaller model outputs coordinates. - The study explores various data augmentation techniques, coordinate representations, and loss functions, highlighting the impact of input order and computational cost. - Phi-Ground demonstrates strong generalization capabilities across diverse benchmarks and improved in-domain performance through post-training. - The authors also discuss the challenges and ethical considerations surrounding the use of Computer Use Agents (CUAs).] | ['Multimodal'] | [Link](https://zhangmiaosen2000.github.io/Phi-Ground/) | N/A |
| [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring
  Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968) | Yiwen Guo, Wei Tao, Chengqian Ma | - This paper introduces C3, a new bilingual benchmark dataset for evaluating spoken dialogue models (SDMs) focusing on complex conversational phenomena like ambiguity and context-dependency. - The dataset contains 1079 instances in English and Chinese, encompassing five key challenges: phonological ambiguity, semantic ambiguity, omission, coreference, and multi-turn interactions. - C3 employs an LLM-based evaluation method aligning well with human judgment, facilitating a comprehensive analysis of SDM performance. - The empirical study highlights the distinct difficulties posed by the five phenomena, along with language-specific challenges. - The findings reveal that SDMs exhibit significant performance variations across different tasks and languages, emphasizing the need for improved robustness and cross-lingual capabilities in SDM development. | ['Audio'] | N/A | N/A |
| [RecGPT Technical Report](https://arxiv.org/abs/2507.22879) | Jian Wu, Jiakai Tang, Gaoyang Guo, Dian Chen, Chao Yi |  - RecGPT is a novel framework for recommender systems that utilizes large language models (LLMs) to model user intent and generate personalized explanations. - RecGPT is deployed on Taobao's homepage, demonstrating consistent performance improvements for users, merchants, and the platform. Online experiments show significant gains across various metrics like Click Through Rate (CTR), Daily Click Active Users (DCAU), and user Dwell Time (DT). - The framework addresses the limitations of existing log-fitting approaches by placing user intent at the center of the recommendation pipeline. This involves integrating LLMs into stages like user interest mining, item retrieval, and explanation generation. - RecGPT employs a multi-stage training paradigm that integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. This ensures the effective alignment of general-purpose LLMs to domain-specific recommendation tasks. - The study validates the effectiveness of RecGPT through online A/B tests, human evaluation experiments, and case studies, demonstrating its ability to foster a more sustainable and mutually beneficial recommendation ecosystem. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Text2Text Generation', 'Other'] | N/A | N/A |
| [Persona Vectors: Monitoring and Controlling Character Traits in Language
  Models](https://arxiv.org/abs/2507.21509) | Jack Lindsey, Owain Evans, Henry Sleight, Andy Arditi, Runjin Chen | This paper introduces persona vectors, which are directions in a language model's activation space that correspond to specific personality traits.  These vectors can monitor personality changes during deployment and training.  A new automated pipeline extracts persona vectors from natural language descriptions.  Experiments show a strong correlation between persona vector shifts and personality changes, both intended and unintended.  A preventative steering method mitigates unwanted persona drift during finetuning. | ['Natural Language Processing'] | [Link](https://github.com/safety-research/persona_vectors) | N/A |
| [On the Expressiveness of Softmax Attention: A Recurrent Neural Network
  Perspective](https://arxiv.org/abs/2507.23632) | Eric C. Larson, Gabriel Mongaras | - This paper introduces a recurrent neural network (RNN) formulation of softmax attention, which helps explain its superior performance compared to linear attention. - The RNN formulation reveals that softmax attention is a structured process with interpretable, sequential dynamics, not merely a heuristic construction. - Linear attention is derived as a first-order approximation of softmax attention in the RNN framework. - Ablation studies demonstrate that higher-order terms in the Taylor expansion contribute to the expressiveness of softmax attention. - The paper shows that replacing the softmax denominator with a vector norm maintains performance while using a gate leads to slight performance degradation. | ['Natural Language Processing'] | [Link](https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective) | N/A |
| [TARS: MinMax Token-Adaptive Preference Strategy for Hallucination
  Reduction in MLLMs](https://arxiv.org/abs/2507.21584) | Jiasheng Tang, Chang Liu, Zhiming Luo, Keda Tao, Kejia Zhang | - This paper introduces TARS, a novel token-adaptive preference strategy to mitigate hallucinations in large multimodal language models (MLLMs). - TARS reformulates direct preference optimization (DPO) as a min-max optimization problem, maximizing token-level distributional shifts under semantic constraints and minimizing expected preference loss. - The proposed method outperforms existing DPO baselines and matches GPT-40 in hallucination reduction, achieving a 13.2% hallucination rate on the AMBER benchmark. - Experiments show that TARS effectively reduces hallucinations without sacrificing factual grounding and improves visual grounding. - The method is data-efficient, requiring only 4.8k preference samples without expert feedback. | ['Multimodal'] | [Link](https://kejiazhang-robust.github.io/tars_web) | N/A |
| [Enhanced Arabic Text Retrieval with Attentive Relevance Scoring](https://arxiv.org/abs/2507.23404) | Abdenour Hadid, Fadi Dornaika, Yazid Bounab, Azeddine Benlamoudi, Salah Eddine Bekhouche | - The paper introduces APR, a novel Arabic Dense Passage Retrieval (DPR) framework that incorporates Attentive Relevance Scoring (ARS) for enhanced semantic matching. - APR uses a dual-encoder architecture with a lightweight Arabic-specific encoder (MiniBERT) and integrates ARS to improve retrieval performance. - The ARS module replaces standard interaction mechanisms with an adaptive scoring function, which more effectively models semantic relevance between questions and passages. - Experimental results on the ArabicaQA dataset show that APR outperforms existing state-of-the-art methods, achieving absolute gains of +0.91% in Top-1, +4.77% in Top-10, and +1.53% in Top-100 accuracy. - The code is made publicly available on GitHub. | ['Question Answering'] | [Link](https://github.com/Bekhouche/APR) | N/A |


## Papers for 2025-07-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ScreenCoder: Advancing Visual-to-Code Generation for Front-End
  Automation via Modular Multimodal Agents](https://arxiv.org/abs/2507.22827) | Qunzhong Wang, Yuxuan Wan, Yaozhi Zheng, Yilei Jiang, csuhan | - This paper introduces ScreenCoder, a modular multi-agent framework for UI-to-code generation that addresses limitations of existing end-to-end methods by decomposing the task into grounding, planning, and generation stages. - The framework consists of three agents: a grounding agent (detects and labels UI components), a planning agent (constructs a hierarchical layout), and a generation agent (produces HTML/CSS code via adaptive prompt-based synthesis). - ScreenCoder achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness, outperforming existing methods on various evaluation metrics. - The framework also functions as a scalable data engine that automatically generates large-scale image-code pairs for training and improving vision-language models. - Extensive experiments demonstrate that ScreenCoder achieves significant improvements in both inference-time performance and the quality of generated code. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Text Generation'] | [Link](https://github.com/leigest519/ScreenCoder) | N/A |
| [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency
  and Performance](https://arxiv.org/abs/2507.22448) | Maksim Velikanov, Iheb-Chaabane, ifarhat1993, ybelkada, JingweiZuo | The paper introduces Falcon-H1, a new series of large language models featuring a parallel hybrid architecture combining Transformer attention with State Space Models (SSMs).  The models are released in various sizes (0.5B, 1.5B, 3B, 7B, and 34B parameters) and are optimized for high performance and efficiency.  Falcon-H1-34B-Instruct outperforms leading models up to 70B parameters on various benchmarks despite being approximately half the size and trained on a fraction of the data.  All Falcon-H1 models are open-source, supporting up to 256K tokens and 18 languages. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/tiiuae/falcon-h1) | [Link](https://huggingface.co/tiiuae) |
| [BANG: Dividing 3D Assets via Generative Exploded Dynamics](https://arxiv.org/abs/2507.21493) | Wei Yang, Yinuo Bai, Haoran Jiang, Qixuan Zhang, ZarkLngeW |  - This paper introduces BANG, a novel generative framework for dividing 3D assets into interpretable parts using Generative Exploded Dynamics.  The model uses a pre-trained latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter and temporal attention module, enabling smooth transitions and consistency.  - BANG incorporates spatial prompts (bounding boxes and surface regions) for precise control over the decomposition process.  Furthermore, it leverages multimodal models (like GPT-4) for intuitive 2D-to-3D manipulations.  - The framework enhances control over object decomposition, enabling users to specify which parts to decompose and how.  - Experimental results demonstrate the effectiveness of BANG in generating high-quality exploded views with smooth transitions, which outperforms existing methods by preserving semantic and geometric coherence.  - The authors show the application of BANG for 3D printing, part-level editing, interactive dialogues for part-level 3D analysis and creation, and 3D asset generation workflows. | ['Text-to-3D', 'Image-to-3D', 'Image Segmentation', 'Multimodal'] | N/A | N/A |
| [VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced
  Multimodal Reasoning](https://arxiv.org/abs/2507.22607) | Sicong Leng, Chenghao Xiao, Ruifeng Yuan, 26hzhang, kenchan0226 | - This paper introduces VL-Cogito, a multimodal large language model (MLLM) trained using a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. - PCuRL incorporates an online difficulty soft weighting mechanism and a dynamic length reward mechanism to improve the model's reasoning abilities and efficiency. - VL-Cogito outperforms existing reasoning-oriented models on various multimodal benchmarks, achieving state-of-the-art or competitive performance across mathematics, science, logic, and general understanding domains. - Ablation studies confirm the effectiveness of PCuRL's components, demonstrating that the progressive curriculum strategy and the dynamic reward mechanism contribute significantly to VL-Cogito's superior performance. - The paper provides extensive experimental results and detailed visualizations of the training process, validating the effectiveness and efficiency of the proposed approach. | ['Multimodal', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/alibaba-damo-academy/VL-Cogito) | N/A |
| [Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual
  Segmentation](https://arxiv.org/abs/2507.22886) | Yu-Gang Jiang, Guanquan Jie, Henghui Ding, Kaining Ying | - This paper introduces OmniAVS, a new benchmark dataset for omnimodal referring audio-visual segmentation, containing 2,104 videos and 61,095 multimodal referring expressions. - OmniAVS supports diverse multimodal referring expressions combining text, speech, sound, and images, emphasizing audio content understanding and complex reasoning. - The paper proposes Omnimodal Instructed Segmentation Assistant (OISA), a baseline model using a Multimodal Large Language Model (MLLM) for omnimodal referring audio-visual segmentation. - OISA incorporates Audio-Visual Interleaving for temporal alignment and query propagation for efficient segmentation, outperforming existing methods on OmniAVS and achieving competitive results on related tasks. - The experimental results demonstrate OISA's effectiveness on OmniAVS and other datasets, highlighting its capabilities in handling complex multimodal expressions and reasoning. | ['Image Segmentation', 'Multimodal', 'Audio Classification', 'Video Classification'] | N/A | N/A |
| [Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement
  Learning](https://arxiv.org/abs/2507.22565) | Gilbert Fridgen, Ramin Bahmani, Igor Tchappi, Amir Sartipi, akhadangi |  - The paper introduces RLDP, a novel framework for differentially private fine-tuning of LLMs using reinforcement learning.  - RLDP dynamically learns per-adapter gradient clipping thresholds and noise levels, outperforming seven strong baselines in terms of perplexity reduction (1.3-30.5%, mean 5.4%) and downstream utility gain (5.6%). - The framework achieves this by casting the DP optimization as a closed-loop control problem and using a soft actor-critic (SAC) hyper-policy. - RLDP also significantly reduces training steps (71% fewer on average) while maintaining the same privacy guarantees. - The approach has been validated on four model families (GPT2-small, Llama-3.2-1B/3B, and Mistral-7B) and is shown to be resistant to membership inference and canary extraction attacks. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/) | [Link](https://huggingface.co/) |


## Papers for 2025-07-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D
  Worlds from Words or Pixels](https://arxiv.org/abs/2507.21809) | Junta Wu, Zhenwei Wang, HunyuanWorld Team, nightkiller, LeoLau | - This paper introduces HunyuanWorld 1.0, a novel framework for generating immersive, explorable, and interactive 3D worlds from text or image inputs. - The framework uses a semantically layered 3D mesh representation with panoramic images as world proxies, enabling coherent 360° views and seamless integration with existing graphics pipelines. - HunyuanWorld 1.0 features disentangled object representations for enhanced interactivity, supporting applications in virtual reality, physical simulation, and game development. - Extensive experiments demonstrate state-of-the-art performance compared to existing methods in terms of generating coherent, explorable, and interactive 3D worlds. - The model supports versatile applications in virtual reality, physical simulation, game development, and interactive content creation. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0) | N/A |


## Papers for 2025-07-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World
  Shorts](https://arxiv.org/abs/2507.20939) | Junfu Pu, Teng Wang, Chen Li, Yixiao Ge, Yuying Ge | - This paper introduces ARC-Hunyuan-Video-7B, a 7B-parameter multimodal model for structured video comprehension. - The model architecture incorporates an audio encoder with fine-grained visual-audio synchronization and timestamp overlay mechanism for temporal awareness. - The model undergoes a comprehensive training regimen including pre-training, instruction fine-tuning, cold start, reinforcement learning, and final instruction fine-tuning. - ARC-Hunyuan-Video-7B outperforms existing baselines on ShortVid-Bench, a new benchmark for real-world short video comprehension, and excels in temporal video grounding. - The model's real-world deployment shows improvements in user engagement and satisfaction, and stress tests indicate an inference time of just 10 seconds for a one-minute video on H20 GPU. | ['Video Classification', 'Video-Text-to-Text', 'Multimodal', 'Summarization', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/TencentARC/ARC-Hunyuan-Video-7B) | N/A |
| [SmallThinker: A Family of Efficient Large Language Models Natively
  Trained for Local Deployment](https://arxiv.org/abs/2507.20984) | Dongliang Wei, Zhenliang Xue, qsstcl, Sorrymaker2024, yixinsong | - This paper introduces SmallThinker, a family of LLMs designed for local deployment, addressing limitations of existing cloud-based models. - SmallThinker utilizes a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, and a pre-attention router to optimize performance on devices with limited memory and slow storage. - The models achieve state-of-the-art performance, exceeding 20 tokens/s on ordinary CPUs with Q4_0 quantization, while consuming only 1GB and 8GB of memory, respectively. - SmallThinker-4B-A0.6B and SmallThinker-21B-A3B outperform comparable models in terms of both speed and accuracy, demonstrating the efficiency of its design. - The research challenges the traditional paradigm of adapting cloud-based LLMs for local deployment by designing them natively for resource-constrained environments. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct), [Link](hf.co/PowerInfer/SmallThinker-21BA3B-Instruct) |
| [Region-based Cluster Discrimination for Visual Representation Learning](https://arxiv.org/abs/2507.20025) | Yongle Zhao, Yin Xie, Athinklo, xiangan, Kaichengalex | - This paper introduces RICE, a novel method for visual representation learning that enhances region-level visual and OCR capabilities. - RICE uses a region transformer layer to extract rich regional semantics from a billion-scale candidate region dataset. - A unified region cluster discrimination loss jointly supports object and OCR learning, enabling efficient and scalable distributed training. - Extensive experiments show that RICE consistently outperforms previous methods on segmentation, dense detection, and visual perception for MLLMs. - The pre-trained models have been released at https://github.com/deepglint/MVT. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Object Detection', 'Image Segmentation'] | [Link](https://github.com/deepglint/MVT) | N/A |
| [Music Arena: Live Evaluation for Text-to-Music](https://arxiv.org/abs/2507.20900) | Wei-Lin Chiang, Anastasios N. Angelopoulos, Wayne Chi, Yonghyun Kim, chrisdonahue | - The paper introduces Music Arena, an open platform for human preference evaluation of text-to-music (TTM) models.  - Music Arena addresses challenges in TTM evaluation by providing a scalable and standardized approach for collecting human preferences.  - The platform features an LLM-based system for prompt moderation and routing, detailed preference collection, and a rolling data release policy.  - Music Arena's modular architecture allows for easy integration of new TTM systems while maintaining a unified evaluation protocol.  - The authors demonstrate how Music Arena addresses key challenges in the TTM ecosystem and showcases the potential of live evaluation for AI domains with unique characteristics. | ['Audio', 'Text-to-Audio'] | [Link](https://github.com/gclef-cmu/music-arena) | N/A |
| [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](https://arxiv.org/abs/2507.16806) | Leshem Choshen, Idan Shenfeld, Stewart Slocum, Isha Puri, Mehul Damani | This paper introduces RLCR, a novel reinforcement learning approach for training language models to reason about their uncertainty.  RLCR augments the standard binary correctness reward with a Brier score, incentivizing calibrated confidence estimates alongside accurate predictions.  Theoretical analysis proves that RLCR optimizes for both accuracy and calibration. Empirical results across diverse datasets demonstrate that RLCR substantially improves calibration without sacrificing accuracy, outperforming both standard RL training and post-hoc calibration methods.  Finally,  the study shows that verbalized confidence scores can be leveraged at test time to further improve model performance. | ['Reinforcement Learning', 'Question Answering'] | N/A | N/A |
| [GenoMAS: A Multi-Agent Framework for Scientific Discovery via
  Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035) | Haohan Wang, Yijiang Li, Liu-Hy | GenoMAS is a novel multi-agent framework designed for scientific discovery through code-driven gene expression analysis. It leverages six specialized large language models (LLMs) to automate complex workflows and surpasses existing methods by 10.61% and 16.85% on the GenoTEX benchmark in data preprocessing and gene identification, respectively.  The framework integrates autonomous agents and structured workflows for enhanced flexibility and reliability, addressing challenges in handling noisy data and adapting to edge cases.  GenoMAS surfaces biologically plausible gene-phenotype associations, corroborated by the literature, while adjusting for latent confounders.  The framework introduces a novel guided-planning mechanism and uses a typed message-passing protocol for efficient agent collaboration. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Other'] | [Link](https://github.com/Liu-Hy/GenoMAS) | N/A |


## Papers for 2025-07-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane
  Algorithm](https://arxiv.org/abs/2507.18553) | Dan Alistarh, Torsten Hoefler, softmax | - This paper presents a novel geometric interpretation of the GPTQ algorithm for post-training quantization of large language models. - It establishes a mathematical equivalence between GPTQ and Babai's nearest plane algorithm for the closest vector problem (CVP) in lattice theory. - This equivalence provides a geometric interpretation of GPTQ's error propagation step and allows for the inheritance of Babai's algorithm's error bound under specific conditions. - The findings are used to develop a new quantization order heuristic, the "min-pivot" order, which aims to improve the accuracy of the algorithm. - The paper's theoretical contributions provide a firmer foundation for the understanding and improvement of post-training quantization methods for large language models. | ['Natural Language Processing'] | N/A | N/A |
| [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075) | Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yanfei Chen, Rujun Han | - This paper introduces Test-Time Diffusion Deep Researcher (TTD-DR), a novel framework that conceptualizes research report generation as a diffusion process, inspired by the iterative nature of human research. - TTD-DR enhances report quality through two mechanisms: (1) Report-Level Refinement via Denoising with Retrieval, and (2) Component-wise Optimization via Self-Evolution. - The framework significantly outperforms existing deep research agents on various benchmarks requiring intensive search and multi-hop reasoning, as evidenced by the superior results achieved in Table 1 (Win Rate, Correctness). - The model's performance is further enhanced by a Self-Evolutionary Algorithm, which is implemented in parallel, with several stages including Initialization, Feedback, Revision, and Crossover. - Despite its notable achievements, the current work primarily focuses on search tools and may not be applicable to other scenarios due to the lack of integration with browsing or coding tools.  | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [Specification Self-Correction: Mitigating In-Context Reward Hacking
  Through Test-Time Refinement](https://arxiv.org/abs/2507.18742) | vicgalle | - This paper introduces Specification Self-Correction (SSC), a novel test-time framework that allows language models to identify and correct flaws in their own guiding specifications. - SSC employs a multi-step inference process: initial response generation, self-critique, specification revision, and final response generation. - Experiments across creative writing and agentic coding tasks demonstrate that SSC reduces the vulnerability to reward hacking by over 90%. - The method requires no weight modification and operates at inference time, leading to more robustly aligned model behavior. - The authors empirically demonstrate the effectiveness of SSC across diverse domains and language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/vicgalle/specification-self-correction) | N/A |


## Papers for 2025-07-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958) | Jian Zhang, Yifei Li, Rongman Xu, Fangzhi Xu, Hang Yan |  - This paper introduces MUR, a novel method for improving the reasoning efficiency of Large Language Models (LLMs) without additional training. - MUR dynamically allocates computational resources to critical reasoning steps by tracking and aggregating step-wise uncertainty using a momentum-based approach. - The method introduces a y-control mechanism to flexibly control the reasoning budget and performance via a single hyperparameter. - Comprehensive evaluations across four challenging benchmarks demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%. - Theoretical analysis supports the superiority of MUR in terms of stability and convergence. | ['Natural Language Processing'] | [Link](https://github.com/yayayacc/MUR) | N/A |
| [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844) | Xingyu Wu, Linjuan Wu, tricktreat, yanyc, paradox122 | - This paper introduces Hierarchical Budget Policy Optimization (HBPO), a novel reinforcement learning framework that enhances the efficiency of large reasoning models without sacrificing accuracy. - HBPO addresses the challenge of exploration space collapse by partitioning rollout samples into multiple subgroups with distinct token budgets, enabling efficient resource allocation. - The framework introduces differentiated reward mechanisms that incentivize budget-aware behavior, allowing models to automatically adjust reasoning depth based on problem complexity. - Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. - Unlike existing methods that rely on external constraints or discrete mode selection, HBPO exhibits emergent adaptive behavior, allowing models to automatically adjust reasoning depth based on problem complexity. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/zju-real/hbpo) | N/A |
| [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013) | Yu Zhao, Chao Wang, Yitong Yao, Xinzhang Liu, Zihan Wang | This paper introduces three new large language models: TeleChat2, TeleChat2.5, and T1.  These models utilize enhanced training strategies, including a 10-trillion token pre-training phase and techniques like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).  TeleChat2.5 prioritizes speed, while T1 excels in complex reasoning tasks.  Benchmark results indicate that T1-115B outperforms existing models like OpenAI's 01-mini and GPT-40. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Tele-AI/TeleChat2), [Link](https://github.com/Tele-AI/TeleChat2.5), [Link](https://github.com/Tele-AI/T1) | [Link](https://modelscope.cn/models/TeleAI/TeleChat2-35B), [Link](https://modelscope.cn/models/TeleAI/TeleChat2-115B), [Link](https://modelscope.cn/models/TeleAI/TeleChat2.5-35B), [Link](https://modelscope.cn/models/TeleAI/TeleChat2.5-115B), [Link](https://modelscope.cn/models/TeleAI/T1-35B), [Link](https://modelscope.cn/models/TeleAI/T1-115B) |
| [A New Pair of GloVes](https://arxiv.org/abs/2507.18103) | Christopher D. Manning, John Bauer, Riley Carlson | - This paper introduces updated 2024 English GloVe word embedding models, addressing limitations of the 2014 models by incorporating newer data and improved vocabulary selection techniques. - The new models were trained using Wikipedia, Gigaword, and a subset of the Dolma corpus, demonstrating improved coverage of contemporary language. - Evaluations on word analogy and similarity tasks show comparable performance to the 2014 models, while NER evaluations highlight improved performance on recent, temporally dependent datasets. - The updated lexicon includes numerous new words and expressions reflecting current linguistic and cultural trends, enriching the semantic expressiveness of the embeddings. - Overall, these improved embeddings are valuable for low-resource settings, computationally efficient models, and interpretability-focused applications. | ['Natural Language Processing'] | N/A | N/A |
| [DMOSpeech 2: Reinforcement Learning for Duration Prediction in
  Metric-Optimized Speech Synthesis](https://arxiv.org/abs/2507.14988) | Kaifeng Xu, Cheng Niu, Fei Tao, Xilin Jiang, Yinghao Aaron Li | - DMOSpeech 2 is a novel model that extends metric optimization to the duration predictor in text-to-speech synthesis using a reinforcement learning approach. - It uses a novel duration policy framework with group relative preference optimization (GRPO), speaker similarity, and word error rate as reward signals. - The model introduces teacher-guided sampling, a hybrid approach that leverages a teacher model for initial denoising steps, improving output diversity and efficiency. - DMOSpeech 2 demonstrates superior performance across all metrics compared to previous systems, reducing sampling steps by half without quality degradation. - The audio samples, code, and pre-trained models are publicly available. | ['Text-to-Speech'] | [Link](https://dmospeech2.github.io/) | N/A |
| [GLiNER2: An Efficient Multi-Task Information Extraction System with
  Schema-Driven Interface](https://arxiv.org/abs/2507.18546) | Ash Lewis, George Hurn-Maloney, Oliver Boyd, Gil Pasternak, Urchade Zaratiana | - This paper introduces GLiNER2, a unified multi-task information extraction framework that improves upon the original GLINER architecture. - GLiNER2 is designed for CPU efficiency and supports named entity recognition, text classification, and hierarchical structured data extraction within a single model. - The model utilizes a schema-driven interface for intuitive task composition and is implemented as an open-source Python library. - Experimental results demonstrate competitive performance across various benchmarks, with substantial improvements in accessibility compared to LLM-based alternatives. - The authors highlight GLiNER2's enhanced efficiency and CPU performance as key advantages for practical applications, particularly in resource-constrained environments. | ['Natural Language Processing', 'Text Classification', 'Token Classification', 'Zero-Shot Classification', 'Feature Extraction'] | [Link](https://github.com/fastino-ai/GLiNER2) | [Link](https://huggingface.co/knowledgator/GLiClass) |
| [Agentar-Fin-R1: Enhancing Financial Intelligence through Domain
  Expertise, Training Efficiency, and Advanced Reasoning](https://arxiv.org/abs/2507.16802) | Zhaowen Zhou, Xiaoke Zhao, Longfei Liao, Xiyang Du, Yanjun Zheng | - Agentar-Fin-R1, a family of 8B and 32B parameter financial LLMs, is introduced, enhancing financial intelligence through domain expertise, training efficiency, and advanced reasoning. - The models are based on the Qwen3 foundation model and utilize a high-quality, systematic financial task label system with a multi-layered trustworthiness assurance framework. -  Agentar-Fin-R1 achieves state-of-the-art performance on mainstream financial benchmarks (Fineva, FinEval, and FinanceIQ) and general reasoning datasets (MATH-500 and GPQA-diamond). - A novel evaluation benchmark, Finova, is proposed to assess real-world deployment capabilities, focusing on agent-level financial reasoning and compliance verification. - Experimental results demonstrate that Agentar-Fin-R1 exhibits exceptional general reasoning capabilities and is a trustworthy solution for high-stakes financial applications. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/antgroup/Finova) | N/A |


## Papers for 2025-07-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863) | Xinhao Li, Jingyi Tang, Lin Xu, Zihao Huang, Hongcheng Gao | - This paper introduces the Turing Eye Test (TET), a new benchmark designed to evaluate the perceptual capabilities of Multimodal Large Language Models (MLLMs). - TET consists of four challenging tasks that involve intuitive human perception, focusing on synthetic images. - The results show that state-of-the-art MLLMs fail catastrophically on TET, indicating a significant gap between machine and human perception. - Fine-tuning the vision tower improves performance, suggesting that the problem lies in visual generalization and not in the knowledge and reasoning capabilities of the language backbone. - The authors plan to release the full set of TET tasks with more diverse challenges and will further explore methods to improve visual generalization in future work. | ['Multimodal'] | [Link](https://TuringEyeTest.github.io) | N/A |
| [Re:Form -- Reducing Human Priors in Scalable Formal Software
  Verification with RL in LLMs: A Preliminary Study on Dafny](https://arxiv.org/abs/2507.16331) | Xin Li, Xu Xu, Xuhan Huang, Fengdi Che, Chuanhao Yan | This paper introduces Re:Form, a novel pipeline that leverages reinforcement learning (RL) within large language models (LLMs) to reduce reliance on human priors in scalable formal software verification.  The pipeline is built around the Dafny formal language verifier and a newly designed benchmark, DafnyComp. Re:Form surpasses prior methods by achieving stronger generalization to out-of-domain tasks and outperforming strong baselines on DafnyComp.  The RL designs incorporates feedback from the formal language verifier, and automatically generates formal specifications using proprietary frontier LLMs to seed the training data.  The use of smaller LLMs (0.5B to 14B parameters) highlights efficiency gains.  | ['Reinforcement Learning', 'Text2Text Generation', 'Natural Language Processing'] | [Link](https://github.com/Veri-Code/ReForm) | [Link](https://huggingface.co/Veri-Code) |
| [RAVine: Reality-Aligned Evaluation for Agentic Search](https://arxiv.org/abs/2507.16725) | Jinhua Gao, Zhi Zheng, Xiang Long, Yilong Xu |  - This paper introduces RAVine, a novel evaluation framework designed to address misalignments in existing evaluation methods for agentic search.  - RAVine targets multi-point queries and long-form answers, reflecting real-world user scenarios more accurately.  - It employs an attributable ground truth construction strategy and examines the iterative process inherent to agentic search.  - The framework also accounts for efficiency factors, providing a more comprehensive assessment of model performance.  - RAVine benchmarks a series of models, providing insights into the strengths and limitations of current agentic search systems. | ['Natural Language Processing'] | [Link](https://github.com/SwordFaith/RAVine) | N/A |


## Papers for 2025-07-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning](https://arxiv.org/abs/2507.16784) | Tina Li, Nathaniel Morgan, Hongyin Luo, thejackobrien, drkylj | - This paper introduces TIM, a novel family of large language models (LLMs) designed for recursive and decompositional problem-solving, and TIMRUN, an inference runtime that enables long-horizon reasoning. - TIM models the reasoning process as a tree structure measured by both length and depth, overcoming output limits and positional-embedding constraints of traditional LLMs that model language as linear sequences. - TIMRUN maintains a working memory that retains only key/value states of relevant context tokens, which enables reuse of positional embeddings and GPU memory pages throughout the reasoning process. - Experimental results show that TIMRUN sustains high inference throughput and delivers accurate reasoning on mathematical and information retrieval tasks that require long-horizon reasoning. - This approach allows for virtually unlimited working memory and multi-hop tool calls within a single language model inference, significantly improving efficiency and accuracy compared to traditional LLMs. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/subconscious-systems/TIMRUN) | N/A |
| [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632) | Chao Yan, Boyong Wu, Insects, SmailAA, petronny | - Step-Audio 2 is a new end-to-end multi-modal large language model designed for high-quality audio understanding and speech conversation.  It integrates a latent audio encoder and reasoning-centric reinforcement learning. - The model incorporates the generation of discrete audio tokens into language modeling, improving responsiveness to paralinguistic information (speaking style and emotion). - Step-Audio 2 utilizes retrieval-augmented generation (RAG) and external tools (web search and audio search) to mitigate hallucinations and improve audio search. - Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. - The model was trained on 680 billion tokens of text data and 8 million hours of real and synthesized audio data. | ['Audio', 'Automatic Speech Recognition', 'Text-to-Speech', 'Audio-to-Audio', 'Multimodal'] | [Link](https://github.com/stepfun-ai/Step-Audio2) | N/A |
| [MegaScience: Pushing the Frontiers of Post-Training Datasets for Science
  Reasoning](https://arxiv.org/abs/2507.16812) | Pengfei Liu, SinclairWang, Vfrz | - This paper introduces MegaScience, a post-training dataset designed to improve the scientific reasoning capabilities of large language models (LLMs). - MegaScience comprises 25 million high-quality question-answer pairs covering diverse scientific disciplines, exhibiting superior quality and scale compared to existing datasets. - The dataset's curation process involves meticulous steps such as textbook sourcing, question-answer pair extraction, refinement, and difficulty level assessment, along with comprehensive data decontamination to enhance reliability. - Empirical evaluations demonstrate significant performance gains on various scientific reasoning benchmarks when LLMs are fine-tuned using MegaScience, surpassing the performance of models trained on other existing datasets. - The authors publicly release MegaScience to foster future research in scientific reasoning and advance the development of more capable LLMs. | ['Question Answering'] | [Link](https://github.com/GAIR-NLP/MegaScience) | N/A |
| [Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking
  Reasoning](https://arxiv.org/abs/2507.16814) | Songyang Gao, Harold-lkk, vanilla1116, haitengzhao, shenjunhao | - This paper introduces SOPHIA, a novel semi-off-policy reinforcement learning framework designed to enhance vision-language slow-thinking reasoning. - SOPHIA combines an off-policy visual understanding module with a policy model for reasoning, effectively leveraging both visual and textual information. - The framework is evaluated on multiple benchmarks, consistently outperforming existing baselines and achieving state-of-the-art results on several challenging datasets. - Ablation studies demonstrate the effectiveness of various components within SOPHIA, including the semi-off-policy learning strategy and the reward design. - SOPHIA's superior performance highlights the potential of reinforcement learning techniques in improving the capabilities of large language models for complex visual reasoning tasks. | ['Reinforcement Learning', 'Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent
  Planning](https://arxiv.org/abs/2507.16815) | Fu-En Yang, Yu-Chiang Frank Wang, Yueh-Hua Wu, cmhungsteve, jasper0314-huang | - ThinkAct is a novel dual-system framework for vision-language-action reasoning that uses a multimodal large language model (LLM) for high-level reasoning and a downstream action model for low-level action execution. - The model architecture involves a multimodal LLM that generates embodied reasoning plans, which are compressed into a visual plan latent that conditions a downstream action model. - ThinkAct utilizes reinforcement learning with action-aligned visual rewards to guide the reasoning process, enabling few-shot adaptation, long-horizon planning, and self-correction. - Experimental results on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct outperforms existing methods in terms of success rate and generalization ability. - The model demonstrates few-shot adaptation and self-correction capabilities through structured reasoning and visual feedback. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://jasper0314-huang.github.io/thinkact-vla/) | N/A |
| [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.16746) | Zikui Cai, Kaiyu Yue, deqing, charleslwang, leonli66 | - This paper introduces ZEBRA-COT, a new large-scale dataset (182,384 samples) designed for interleaved vision-language reasoning. - The dataset focuses on four categories of tasks: scientific questions, 2D visual reasoning, 3D visual reasoning, and visual logic and strategic games. - Fine-tuning the Anole-7B model on ZEBRA-COT improves test-set accuracy by 12% and yields up to a 13% performance gain on standard VLM benchmark evaluations. - Fine-tuning Bagel-7B on ZEBRA-COT produces a model capable of generating high-quality interleaved visual reasoning chains. - The dataset and models are open-sourced to facilitate further research and evaluation of visual chain-of-thought reasoning. | ['Multimodal'] | [Link](https://github.com/multimodal-reasoning-lab/Bagel-Zebra-CoT) | N/A |
| [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement
  Feedback](https://arxiv.org/abs/2507.15024) | Hongyu Lin, Bowen Yu, Le Yu, Hao Xiang, Qiaoyu Tang | - RefCritic, a novel long chain-of-thought critic model, is proposed to enhance the critique abilities of LLMs. - RefCritic uses reinforcement learning with dual rule-based rewards focusing on instance-level correctness and refinement accuracies. - The model consistently outperforms existing methods across multiple benchmarks, achieving gains of up to 7.2% on AIME25 and 9.9% on Olympiad. - Under majority voting, policy models filtered by RefCritic demonstrate superior scaling with increased voting numbers. - RefCritic achieves strong performance on ProcessBench, surpassing step-level supervised approaches. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | N/A | N/A |


## Papers for 2025-07-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via
  Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683) | Yao Xiao, LidongBing, ZonglinY, binwang, veggiebird | - This paper introduces MiroMind-M1, a series of fully open-source reasoning language models (RLMs) that match or exceed the performance of existing open-source RLMs. - MiroMind-M1 models are trained using a two-stage process: supervised fine-tuning (SFT) on a curated corpus of 719K math-reasoning problems and reinforcement learning with verifiable reward (RLVR) on 62K challenging problems. - The RLVR process employs a novel Context-Aware Multi-Stage Policy Optimization (CAMPO) algorithm to enhance robustness and efficiency. -  MiroMind-M1 achieves state-of-the-art or competitive performance among Qwen-2.5-based open-source models on the AIME24, AIME25, and MATH benchmarks. - The complete stack, including models, datasets, and training configurations, is released to foster reproducibility and further research. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/MiroMindAsia/MiroMind-M1) | [Link](https://huggingface.co/miromind-ai/MiroMind-M1-RL-7B), [Link](https://huggingface.co/datasets/miromind-ai/MiroMind-M1-RL-62K) |
| [WebShaper: Agentically Data Synthesizing via Information-Seeking
  Formalization](https://arxiv.org/abs/2507.15061) | Baixuan Li, Junkai Zhang, Wenbiao Yin, Jialong Wu, Zhengwei Tao | - This paper introduces WebShaper, a novel framework for synthesizing high-quality training data for information-seeking agents. - WebShaper employs a formalization-driven approach, systematically formalizing information-seeking tasks using set-theoretic constructs, unlike existing information-driven methods. - Central to WebShaper is the concept of Knowledge Projections (KP), enabling precise control over reasoning structures. - The framework includes an agentic Expander module, which autonomously expands formal questions with retrieval and validation tools. - Experimental results demonstrate that WebShaper achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks. | ['Question Answering'] | [Link](https://github.com/Alibaba-NLP/WebAgent) | [Link](https://huggingface.co/datasets/Alibaba-NLP/WebShaper) |
| [SeC: Advancing Complex Video Object Segmentation via Progressive Concept
  Construction](https://arxiv.org/abs/2507.15852) | Jianfan Lin, Songxin He, Xiaoyi Dong, Shuangrui Ding, rookiexiong | - The paper introduces Segment Concept (SeC), a novel concept-driven framework for video object segmentation that leverages Large Vision-Language Models (LVLMs) to construct and utilize high-level, object-centric representations. - SeC shifts from conventional feature matching to the progressive construction and utilization of robust conceptual priors, leading to improved robustness against drastic visual variations, occlusions, and complex scene changes. - A new benchmark, Semantic Complex Scenarios Video Object Segmentation (SeCVOS), is introduced to rigorously evaluate VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding; SeCVOS comprises 160 manually annotated multi-scenario videos. - Empirical evaluations on SeCVOS and standard VOS benchmarks demonstrate that SeC substantially outperforms state-of-the-art approaches, achieving an 11.8-point improvement over SAM 2.1 on SeCVOS. - SeC employs a scene-adaptive activation strategy, dynamically adjusting computational efforts based on scene complexity, making it computationally efficient. | ['Video Classification', 'Image Segmentation', 'Multimodal'] | [Link](https://github.com/OpenIXCLab/SeC) | N/A |
| [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417) | Jacob Goldman-Wetzler, Andy Arditi, Runjin Chen, Alexander Hägele, Aryo Pradipta Gema | This paper reveals inverse scaling in large reasoning models (LRMs), where increasing reasoning length reduces accuracy.  They identify five distinct failure modes: distraction by irrelevant information, overfitting to problem framings, reliance on spurious correlations, difficulty maintaining focus on complex tasks, and amplification of concerning behaviors.  Evaluation spans simple counting, regression, and deduction tasks.  Findings highlight the importance of evaluating models across diverse reasoning lengths to mitigate these failure modes. | ['Natural Language Processing'] | [Link](https://github.com/safety-research/inverse-scaling-ttc) | [Link](https://huggingface.co/datasets/inverse-scaling-ttc/inverse-scaling-ttc-main) |
| [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for
  Spoken Language Models](https://arxiv.org/abs/2507.15375) | Kevin Lin, Chung-Ching Lin, Linjie Li, xiaofei-wang, dcml0714 | - This paper introduces STITCH, a novel method for spoken language model generation that alternates between generating unspoken reasoning chunks and spoken response chunks.  - STITCH addresses the issue of latency in spoken language models by using the remaining time between spoken response chunks to generate reasoning tokens, effectively enabling simultaneous thinking and talking. - Experimental results demonstrate that STITCH matches the latency of baseline models while outperforming them by 15% on math reasoning datasets and performing equally well on non-reasoning datasets. - The authors introduce two variants of STITCH: STITCH-R (reasoning first) and STITCH-S (speaking first), both of which exhibit improved performance over baselines. - The method is validated using several benchmark datasets, including math reasoning datasets and non-reasoning datasets, showing consistent improvements over baseline models. | ['Audio', 'Text-to-Speech'] | [Link](https://d223302.github.io/STITCH) | N/A |


## Papers for 2025-07-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges
  in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563) | Mikhail Gorodnichev, Maxim Maslov, Vasiliy Kudryavtsev, Nikita Vasiliev, Kirill Borodin | - The paper introduces Balalaika, a new Russian speech dataset containing over 2000 hours of high-quality speech with comprehensive annotations, including punctuation and stress markings. -  The dataset significantly improves the performance of speech synthesis and enhancement models compared to existing datasets, as demonstrated by experimental results. - The authors detail the dataset construction pipeline, including data collection, audio cutting, separation, transcription, punctuation, and stress placement, along with the various models used in these steps. -  The paper also presents a comparative analysis of various speech restoration and synthesis models trained on Balalaika and other datasets using different metrics, highlighting Balalaika's superior performance. - The results show improvements in NISQA, MOS, and CER metrics indicating higher quality speech synthesis when using the Balalaika dataset.  This is attributed to improved annotation quality and inclusion of prosodic features. | ['Text-to-Speech'] | [Link](https://github.com/mtuciru/balalaika) | [Link](https://huggingface.co/RUPunct), [Link](https://huggingface.co/salute-developers/GigaAM) |
| [The Devil behind the mask: An emergent safety vulnerability of Diffusion
  LLMs](https://arxiv.org/abs/2507.11097) | Ruixi Wu, Zhiyuan Liu, Dongrui Liu, Zichen Wen, Joshua999 | - This paper introduces DIJA, a novel jailbreak attack framework that exploits the unique safety vulnerabilities of diffusion-based large language models (dLLMs). - DIJA constructs adversarial interleaved mask-text prompts that leverage the bidirectional context modeling and parallel decoding mechanisms of dLLMs to generate harmful outputs. - Through comprehensive experiments, DIJA significantly outperforms existing jailbreak methods across multiple dLLMs and benchmarks, achieving up to 100% keyword-based attack success rate. - The findings underscore the urgent need for rethinking safety alignment in dLLMs and highlight the critical gaps in current alignment strategies. - DIJA's effectiveness is demonstrated on several publicly available dLLMs across multiple benchmarks, showing its robustness against existing defense mechanisms. | ['Natural Language Processing', 'Text Generation', 'Fill-Mask'] | [Link](https://github.com/ZichenWen1/DIJA) | N/A |
| [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal
  Large Language Models](https://arxiv.org/abs/2507.12566) | Xue Yang, Wenhao Li, Wenhan Dou, Gen Luo, wzk1015 | This paper introduces Mono-InternVL-1.5, a monolithic multimodal large language model (MLLM) that integrates visual encoding and language decoding into a single model.  The model architecture uses a multimodal mixture-of-experts to incorporate visual experts and addresses the challenge of catastrophic forgetting through delta tuning.  Mono-InternVL-1.5 significantly improves training and inference efficiency compared to its predecessor, Mono-InternVL, and outperforms existing monolithic MLLMs on 12 out of 15 benchmarks.  The authors also introduce an improved Endogenous Visual Pre-training (EViP++) method and a fused CUDA kernel for faster inference. | ['Multimodal'] | [Link](https://github.com/OpenGVLab/Mono-InternVL) | N/A |
| [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking
  Services](https://arxiv.org/abs/2507.10605) | Ziyan Liu, Zheyong Xie, Yue Wang, Chonggang Lu, Hiiamein | - RedOne is a domain-specific large language model (LLM) for social networking services (SNS), trained using a three-stage approach (continued pre-training, supervised fine-tuning, and preference optimization). - It achieves an average improvement of up to 14.02% across eight major SNS tasks and 7.56% in an SNS bilingual evaluation benchmark compared to baseline models. - RedOne demonstrates strong generalization capabilities across various tasks and shows promising applicability in real-world scenarios, such as reducing harmful content exposure and improving click-through rates. - The model's performance is evaluated through extensive experiments on various benchmarks, both general and SNS-specific. - Ablation studies confirm the effectiveness of each training stage, highlighting the importance of a comprehensive approach for optimal performance. | ['Natural Language Processing'] | N/A | N/A |
| [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455) | Zhuotao Tian, Li Jiang, Senqiao Yang, Shangpin Peng | This paper introduces SENTINEL, a novel framework to reduce object hallucinations in multimodal large language models (MLLMs).  SENTINEL leverages an in-domain preference learning approach, mitigating hallucinations at early generation stages without using external models.  Experimental results show that SENTINEL reduces object hallucinations by over 90% compared to previous state-of-the-art methods and consistently improves generalization performance across various benchmark tasks.  The framework is model-agnostic and computationally efficient, addressing limitations of existing methods. | ['Multimodal'] | [Link](https://github.com/pspdada/SENTINEL) | N/A |
| [The Generative Energy Arena (GEA): Incorporating Energy Awareness in
  Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302) | Pedro Reviriego, Javier Conde, Eneko Sendin, Gonzalo Martínez, Carlos Arriaga | - The paper introduces the Generative Energy Arena (GEA), a platform for evaluating large language models (LLMs) that incorporates information on energy consumption into the human evaluation process. - GEA presents energy consumption information to users after they have made their initial model choice based on response quality, mitigating bias toward smaller models. - Preliminary results demonstrate that users tend to favor smaller, more energy-efficient LLMs when energy consumption is considered, suggesting that energy awareness can influence decision-making in LLM evaluation. -  The study employs a two-step evaluation method to collect user preferences with and without energy information. - Further investigation is needed to examine energy consumption's effect across diverse LLM models, question types, and languages. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/) |


## Papers for 2025-07-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334) | ShowerMaker, LImax72, YuyaoGe, Theodyy, Chevalier | This survey introduces Context Engineering as a formal discipline for optimizing information payloads for LLMs, going beyond simple prompt design.  It presents a comprehensive taxonomy that decomposes Context Engineering into foundational components (Context Retrieval and Generation, Context Processing, Context Management) and system implementations (RAG, Memory Systems, Tool-Integrated Reasoning, Multi-Agent Systems).  The survey analyzes over 1400 research papers, revealing a critical research gap: current models, while proficient in understanding complex contexts, struggle to generate equally sophisticated outputs.  Finally, the survey provides a unified framework for researchers and engineers in the field. | ['Natural Language Processing'] | [Link](https://github.com/Meirtz/Awesome-Context-Engineering) | [Link](null) |
| [VisionThink: Smart and Efficient Vision Language Model via Reinforcement
  Learning](https://arxiv.org/abs/2507.13348) | Hengshuang Zhao, Bei Yu, Xin Lai, Junyi Li, Senqiao Yang |  - VisionThink is a novel efficient vision language model (VLM) that uses reinforcement learning to dynamically adjust image resolution based on task complexity.  - The model starts with a downsampled image and requests higher resolution only when needed, saving computational resources without sacrificing accuracy.  - VisionThink is shown to outperform existing methods on various benchmarks, particularly those with strong OCR-related tasks, as demonstrated by achieving 102% performance on an OCR-related benchmark (compared to baseline 100%).  - A key innovation is the LLM-as-Judge strategy for reinforcement learning, which uses an external LLM to assess the accuracy of VLM responses, enabling training on diverse general VQA tasks.  - Extensive experiments demonstrate superior efficiency and effectiveness compared to other efficient VLMs and state-of-the-art models. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/dvlab-research/VisionThink) | [Link](null) |
| [The Imitation Game: Turing Machine Imitator is Length Generalizable
  Reasoner](https://arxiv.org/abs/2507.13332) | Songyang Gao, Chengqi Lyu, Wenwei Zhang, vanilla1116, ZhouqiHUA | - This paper introduces TAIL (Turing Machine Imitation Learning), a novel method to enhance the length generalization ability of large language models (LLMs). - TAIL synthesizes chain-of-thought (CoT) data that mimics the execution process of a Turing Machine, addressing the challenge of length generalization in LLMs. - The proposed method uses three key structures in the synthesized CoT data: Linear Transition, Atomic State, and Memory Fetcher, which emulate the core properties of Turing Machine execution. - Experiments on a challenging synthetic dataset with 8 algorithm classes and 18 tasks show that TAIL significantly improves length generalization ability compared to previous methods and DeepSeek-R1. - Ablation studies demonstrate the necessity of each core module of TAIL for effective length generalization. | ['Natural Language Processing'] | N/A | N/A |
| [AnyCap Project: A Unified Framework, Dataset, and Benchmark for
  Controllable Omni-modal Captioning](https://arxiv.org/abs/2507.12841) | Gao Meng, Yu Li, Zhiqiang Lin, Yiming Ren, Ruihang |  - This paper introduces the AnyCap Project, a unified framework, dataset, and benchmark for controllable omni-modal captioning.  - The AnyCapModel (ACM) is a lightweight, plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning.  - The AnyCapDataset (ACD) is a large-scale omni-modal dataset covering 3 modalities and 28 types of user instructions, with 300k high-quality data entries. - The AnyCapEval is a novel benchmark that offers more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. - Experiments show ACM significantly improves caption quality across diverse base models and outperforms state-of-the-art methods on widely-used benchmarks. | ['Multimodal'] | [Link](https://github.com/qishisuren123/AnyCap) | N/A |
| [MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://arxiv.org/abs/2507.12508) | Reuben Tan, Siyuan Zhou, Zheyuan Zhang, Jiageng Liu, yyuncong | - MindJourney is a test-time scaling framework that enhances Vision-Language Models (VLMs) for spatial reasoning by integrating a controllable world model based on video diffusion. - The VLM iteratively generates camera trajectories, while the world model synthesizes corresponding views, enabling reasoning over multi-view evidence without fine-tuning. - On the SAT benchmark, MindJourney achieves an average 8% performance improvement, surpassing the performance of test-time inference VLMs trained with reinforcement learning. - The proposed method is model-agnostic, enhancing multiple VLMs with different world models. - MindJourney offers a simple, plug-and-play route for robust 3D reasoning by leveraging the strengths of both VLMs and world models. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://umass-embodied-agi.github.io/MindJourney/) | N/A |
| [AbGen: Evaluating Large Language Models in Ablation Study Design and
  Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300) | Yixin Liu, Manasi Patwardhan, Zhijian Xu, Weiyuan Chen, Yilun Zhao | - ABGEN, a benchmark for evaluating LLMs in designing ablation studies for scientific research, is introduced. It comprises 1,500 expert-annotated examples derived from 807 NLP papers. - LLMs are tasked with generating detailed ablation study designs based on given research contexts, and leading LLMs are evaluated on their performance. - A significant performance gap is found between LLMs and human experts regarding the importance, faithfulness, and soundness of generated designs. - ABGEN-EVAL, a meta-evaluation benchmark, is developed to assess the reliability of commonly used automated evaluation systems. - The study investigates various LLM-as-Judge systems on ABGEN-EVAL, providing insights for future research on developing reliable LLM-based evaluation systems for scientific tasks. | ['Natural Language Processing'] | [Link](https://github.com/yale-nlp/AbGen) | [Link](https://huggingface.co/yale-nlp/AbGen) |
| [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720) | Sachin Kumar, Orevaoghene Ahia, Abraham Toluase Owodunni | - This paper introduces FLEXITOKENS, a novel training objective for byte-level language models that enables flexible and adaptive tokenization. - The model architecture incorporates a submodule that learns to predict boundaries between byte sequences, dynamically adapting to new data distributions during finetuning. - FLEXITOKENS consistently outperforms existing subword and gradient-based tokenization methods across diverse multilingual benchmarks and morphologically rich tasks, showing up to 10% improvement in downstream task performance. - The proposed method reduces token over-fragmentation, resulting in improved efficiency and better generalization to unseen languages and domains. - The code and data for the experiments are publicly available at https://github.com/owos/flexitokens | ['Natural Language Processing'] | [Link](https://github.com/owos/flexitokens) | N/A |
| [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255) | Nay Oo, Tri Cao, Ziwen Xu, Mengru Wang, Lyucheng Wu | - AutoSteer, a novel inference-time intervention technique, is introduced to mitigate safety risks in multimodal large language models (MLLMs) without requiring model retraining. - The method incorporates three core components: a Safety Awareness Score (SAS) to identify safety-relevant layers, an adaptive safety prober to estimate toxicity, and a lightweight Refusal Head to intervene when necessary. - Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the attack success rate (ASR) for various threats while preserving general abilities. - AutoSteer is shown to be practical, interpretable, and effective, offering a modular and model-agnostic framework for safer MLLM deployment. - The findings highlight AutoSteer's potential as a practical solution for improving the safety of multimodal AI systems in real-world applications. | ['Multimodal'] | N/A | N/A |
| [Voxtral](https://arxiv.org/abs/2507.13264) | Corentin Barreau, Clément Denoix, Andy Lo, Andy Ehrenberg, Alexander H. Liu | - This paper introduces Voxtral Mini and Voxtral Small, two open-source multimodal audio chat models that achieve state-of-the-art performance on various audio benchmarks while maintaining strong text capabilities. - Voxtral models utilize a Transformer architecture with an audio encoder (Whisper large-v3), an adapter layer for downsampling, and a language decoder (Mistral). - The models are trained in three phases: pretraining (with audio-text repetition and cross-modal continuation patterns), supervised finetuning on speech understanding tasks, and preference alignment using Direct Preference Optimization. - Voxtral Small outperforms several closed-source models, demonstrating its effectiveness in speech transcription, translation, and question answering. - The authors also introduce three new benchmarks for evaluating speech understanding models on knowledge and trivia, contributing to the field's evaluation ecosystem. | ['Audio', 'Automatic Speech Recognition', 'Multimodal', 'Question Answering'] | N/A | [Link](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507), [Link](https://huggingface.co/mistralai/Voxtral-Small-24B-2507), [Link](https://huggingface.co/collections/mistralai/speech-evals-6875e9b26c78be4a081050f4) |


## Papers for 2025-07-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning
  Systems in LLMs](https://arxiv.org/abs/2507.09477) | Wei-Chieh Huang, Yuyao Yang, Yangning Li, TreeForest, WZDavid |  - This paper surveys Retrieval-Augmented Generation (RAG) systems that incorporate deep reasoning in large language models (LLMs). - The authors categorize RAG-Reasoning methods into Reasoning-Enhanced RAG, RAG-Enhanced Reasoning, and Synergized RAG-Reasoning, based on how retrieval and reasoning interact. - The paper further categorizes methods within each category by their approach to retrieval optimization, integration enhancement, and generation enhancement. -  A taxonomy of recent advances in RAG-reasoning systems is presented in the paper along with a list of benchmarks and datasets used to evaluate them. - The paper concludes by discussing open challenges and research avenues for improving the effectiveness, adaptability, and trustworthiness of RAG-Reasoning systems. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/DavidZWZ/Awesome-RAG-Reasoning) | N/A |
| [MOSPA: Human Motion Generation Driven by Spatial Audio](https://arxiv.org/abs/2507.11949) | Leo Ho, Liang Pan, Mingyi Shi, frankzydou, JimSYXu | - This paper introduces MOSPA, a novel diffusion-based generative model for human motion generation driven by spatial audio, and a new Spatial Audio-Driven Human Motion (SAM) dataset. - The MOSPA model effectively captures the relationship between body motion and spatial audio through an effective fusion mechanism, integrating spatial features such as Mel-Frequency Cepstral Coefficients (MFCCs), Tempograms, and root mean square (RMS) energy. - The SAM dataset contains diverse and high-quality spatial audio and motion data, including diverse spatial audio signals and high-quality 3D human motion pairs. - Extensive experiments demonstrate that MOSPA achieves state-of-the-art performance, outperforming existing baselines in generating realistic and diverse motion responses to spatial audio. - The model and dataset are planned to be open-sourced upon acceptance. | ['Multimodal', 'Audio', 'Text-to-3D'] | N/A | N/A |
| [MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior
  Understanding](https://arxiv.org/abs/2507.12463) | Mingyang Wu, Renjie Li, vztu, waynefan, jerryye0110 | - This paper introduces MMHU, a large-scale multimodal benchmark dataset for human behavior understanding in autonomous driving scenarios. - The dataset contains 57k human instances with diverse behaviors and 1.73M frames from various sources, including Waymo, YouTube, and self-collected videos. - MMHU provides rich annotations such as motion and trajectory, text descriptions, and critical behavior labels relevant to driving safety. - Multiple tasks are benchmarked including motion prediction, motion generation, and human behavior question answering, offering a comprehensive evaluation suite. - The dataset improves performance on several baseline models and demonstrates its effectiveness in various human-centric tasks. | ['Multimodal', 'Visual Question Answering', 'Video Classification', 'Keypoint Detection', 'Text Generation', 'Text2Text Generation'] | [Link](https://MMHU-Benchmark.github.io) | N/A |
| [SWE-Perf: Can Language Models Optimize Code Performance on Real-World
  Repositories?](https://arxiv.org/abs/2507.12415) | Zhijie Fan, Lin Yan, Xinyi He, Elfsong, SivilTaram | - SWE-Perf is introduced as the first benchmark designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. - It comprises 140 instances derived from performance-improving pull requests from popular GitHub repositories. - Each instance includes the codebase, target functions, performance-related tests, expert-authored patches, and executable environments. - The benchmark reveals a substantial capability gap between existing LLMs and expert-level optimization performance. - This highlights critical research opportunities in this emerging field. | ['Natural Language Processing'] | [Link](https://swe-perf.github.io) | [Link](null) |
| [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025) | Franck-Dernoncourt, Nikosapa, TrungBui1111, jasubram, haniehds | - This paper introduces Lizard, a novel linearization framework that transforms pre-trained transformer-based large language models (LLMs) into flexible subquadratic architectures for infinite-context generation. - Lizard addresses the quadratic complexity of softmax attention and the growing key-value cache by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving output quality. - Unlike previous linearization methods, Lizard incorporates a gating module, enabling adaptive memory control, constant-memory inference, and strong length generalization. - Experimental results demonstrate that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, significantly outperforming previous linearization methods by 8-18 points on the 5-shot MMLU benchmark. - The authors also introduce a hardware-aware algorithm that accelerates the training speed of Lizard models by up to 24%. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2025-07-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and
  Reasoning Modes](https://arxiv.org/abs/2507.11407) | Stanley Jungkyu Choi, Kibong Choi, Eunbi Choi, Kyunghoon Bae, LG AI Research |  - This technical report introduces EXAONE 4.0, a unified large language model that integrates both non-reasoning and reasoning modes, improving upon the capabilities of its predecessors, EXAONE 3.5 and EXAONE Deep. - The model architecture employs a hybrid attention mechanism combining local and global attention, enhancing long-context processing and reducing computational costs.  It also features a re-positioning of layer normalization for improved performance. - EXAONE 4.0 supports three languages: English, Korean, and Spanish, with multilingual capabilities expanded.  The model series includes two sizes: a 32B parameter model optimized for performance, and a 1.2B parameter model for on-device applications. - Compared to other models in its class, EXAONE 4.0 demonstrates superior performance, maintaining competitiveness even against frontier-class models and excelling in areas such as world knowledge, mathematical and coding reasoning tasks. - The models are publicly available for research purposes and are designed to pave the way for the agentic AI era by incorporating agentic tool use. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/LGAI-EXAONE) |
| [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404) | Enrico Fini, David Grangier, Dan Busbridge, Louis Bethune, Mustafa Shukor | - This paper introduces a novel systematic method for determining the optimal data mixture for large foundation models, using scaling laws.- The approach accurately predicts model loss as a function of model size, training tokens, and domain weights, validated across LLMs, NMMs, and LVMs.- Scaling laws are shown to extrapolate well to new data mixtures and scales, allowing for the estimation of optimal domain weights using only a few small-scale training runs.- The proposed method offers a principled alternative to the costly trial-and-error approach typically used for data mixture selection in large-scale model training.- Results demonstrate that the scaling laws accurately predict performance at larger scales and new domain weights, and that the derived optimal domain weights lead to improved performance compared to other methods. | ['Multimodal'] | N/A | N/A |
| [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via
  Self-Critique](https://arxiv.org/abs/2507.09075) | Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek, Wasi Uddin Ahmad, smajumdar94 | - This paper introduces OPENCODEREASONING-II, a new dataset with 2.5M question-solution-critique triples, nearly double the size of existing datasets. -  It presents a two-stage supervised fine-tuning strategy for code generation and critique, achieving performance exceeding or equal to prior open-weight distilled models. - The integration of code generation and critique models significantly improves competitive coding performance. - An extension of the LiveCodeBench benchmark is introduced to specifically support the C++ programming language. - The paper evaluates a simple test-time scaling approach using self-critique for selecting the best code solutions from multiple generations. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/nvidia/OpenCodeReasoning-2), [Link](https://huggingface.co/datasets/nvidia/LiveCodeBench-CPP) |
| [Planted in Pretraining, Swayed by Finetuning: A Case Study on the
  Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186) | Gabriel Stanovsky, Yonatan Belinkov, itay1itzhak | - This paper introduces a two-step causal experimental approach to investigate the origins of cognitive biases in large language models (LLMs). - The first step assesses the impact of training randomness by repeatedly finetuning the same pretrained models with different random seeds. - The second step uses a novel "cross-tuning" approach that swaps instruction datasets between models with different bias patterns to isolate bias sources. - Results reveal that pretraining is the main factor shaping cognitive biases in LLMs, while training randomness and finetuning data have smaller effects. - The findings highlight the importance of considering pretraining origins when evaluating and mitigating biases in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/itaylitzhak/planted-in-pretraining) | N/A |


## Papers for 2025-07-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual
  Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862) | Deyu Zhou, Jiahe Zhang, Duomin Wang, Zhaoyang Li, Youliang Zhang | - SpeakerVid-5M, a large-scale, high-quality dataset for audio-visual dyadic interactive human generation, is introduced.  The dataset contains over 8,743 hours of data and 5.2 million video clips, offering rich annotations such as structured text, skeletal sequences, and blur scores. - The dataset is structured along two key dimensions: interaction type (dialogue, single, listening, multi-turn) and data quality (pre-training and SFT subsets). - An autoregressive method for audio-visual dyadic human generation is proposed, using Qwen2.5-Omni for multimodal understanding and a next-chunk prediction model for joint audio and video generation. - The proposed method achieves state-of-the-art performance on the VidChatBench benchmark, showcasing improvements across metrics such as FID, FVD, PSNR, SSIM, and ArcFace. - The dataset and code are publicly released to facilitate research in audio-visual dyadic interactive human generation. | ['Multimodal', 'Video-Text-to-Text', 'Text-to-Video', 'Image-to-Video', 'Audio'] | [Link](https://dorniwang.github.io/SpeakerVid-5M/) | N/A |
| [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548) | Kui Wu, Chengjie Jiang, Yitang Li, Wei Huang, Mingxian Lin | - EmbRACE-3K, a novel dataset of over 3,000 language-guided embodied tasks in diverse photorealistic environments, is introduced to address limitations of existing vision-language models (VLMs) in embodied settings. - The dataset contains approximately 26,000 decision steps, each annotated with multimodal context and step-wise reasoning, enabling fine-grained evaluation of embodied reasoning capabilities. - Experiments with state-of-the-art VLMs such as GPT-40, Gemini 2.5 Pro, and Qwen2.5-VL-7B reveal significant performance limitations in zero-shot settings, underscoring the challenges posed by embodied tasks. - Fine-tuning Qwen2.5-VL-7B using a two-stage approach (supervised fine-tuning followed by reinforcement learning) on EmbRACE-3K yields substantial improvements across three key dimensions: exploration, dynamic spatial-semantic reasoning, and multi-stage goal execution. - The results demonstrate the effectiveness of EmbRACE-3K in facilitating the development of embodied reasoning capabilities in VLMs. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://mxllc.github.io/EmbRACE-3K/) | N/A |
| [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems
  at Once](https://arxiv.org/abs/2507.10541) | Zinan Tang, Qiyao Sun, Yu Li, Qizhi Pei, Zhuoshi Pan | - This paper introduces REST, a novel evaluation framework designed to rigorously assess the reasoning capabilities of Large Language Models (LLMs) by presenting them with multiple reasoning problems simultaneously. - REST significantly enhances the discriminative power of existing benchmarks, revealing weaknesses in LLMs that are not apparent in traditional single-question evaluations. - The experimental results demonstrate that under REST stress testing, the performance of various LLMs, even state-of-the-art models, degrades substantially. - A detailed error analysis pinpoints common issues such as question omission and reasoning errors which can inform the future development of more robust and capable LLMs. - The study identifies key factors affecting LLM performance under stress, including overthinking, output length limitations, and question ordering bias. | ['Question Answering'] | N/A | N/A |
| [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive
  Token-Level Computation](https://arxiv.org/abs/2507.10524) | Jiyoun Ha, Sungnyun Kim, Reza Bayat, Yujin Kim, Sangmin Bae | - Mixture-of-Recursions (MoR) is a novel Transformer architecture that uses dynamic recursive depths for adaptive token-level computation, achieving unified parameter efficiency and memory efficiency. - MoR introduces two key mechanisms: expert-choice routing and recursive key-value caching, which dynamically adjust the recursion depth and manage memory usage efficiently. - Experimental results demonstrate that MoR significantly improves upon existing Transformer models, achieving better performance with lower computational and memory costs. - Ablation studies confirm the importance of both expert-choice routing and recursive key-value caching, showing their substantial contributions to MoR's performance. - The paper provides comprehensive analysis of MoR's adaptive computation capabilities, showing its scalability and robustness across various model sizes and sequence lengths. | ['Natural Language Processing'] | [Link](https://github.com/raymin0223/mixture_of_recursions) | [Link](string) |
| [LayerCake: Token-Aware Contrastive Decoding within Large Language Model
  Layers](https://arxiv.org/abs/2507.04404) | Yanqiang Zheng, Jiawang Cao, Wenbo Zhu, Yongliang Wu, Jingze Zhu | - This paper introduces LayerCake, a novel decoding-time method that improves the factuality of large language models (LLMs) without requiring additional training or model modifications. - LayerCake leverages both layer-wise dynamics and token-specific information to guide decoding, addressing the limitations of existing methods that treat these signals in isolation. - The method involves suppressing attention to specific token types at their most influential layers to induce controlled factual degradation and derive contrastive signals to guide final factual decoding. - Experiments demonstrate that LayerCake consistently improves factuality across multiple LLMs and various benchmarks, outperforming existing state-of-the-art contrastive decoding methods. - The code for LayerCake is available on GitHub. | ['Question Answering'] | [Link](https://github.com/Styxiian/LayerCake) | N/A |
| [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104) | Kai Chen, Songyang Zhang, Alexander Lam, Maosong Cao, Taolin Zhang | - CompassJudger-2 is a novel generalist judge model that uses a task-driven, multi-domain data curation strategy and verifiable rewards to improve the robustness and generalization of LLM judgment. - The model employs rejection sampling to select high-quality training examples and uses a margin policy gradient loss to enhance performance. - CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, outperforming larger models like DeepSeek-V3 and Qwen3-235B-A22B. - The paper introduces JudgerBenchV2, a comprehensive benchmark that evaluates cross-domain judgment accuracy and rank consistency. - Overall, CompassJudger-2 advances robust, scalable LLM judgment and sets new performance and evaluation standards. | ['Natural Language Processing'] | [Link](https://github.com/open-compass/CompassJudger) | N/A |
| [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for
  LLM Evaluation](https://arxiv.org/abs/2507.08924) | Yeonjung Hong, Soyeon Kim, Guijin Son, Sunkyoung Kim, Seokhee Hong | - This paper introduces KMMLU-Redux and KMMLU-Pro, two benchmarks designed to evaluate large language models (LLMs) on Korean professional knowledge.- KMMLU-Redux is a revised version of the KMMLU benchmark, with improved data quality and a focus on eliminating challenging problems.- KMMLU-Pro expands on KMMLU-Redux by incorporating questions from actual Korean professional license exams, making it a more rigorous and realistic evaluation.- The authors conduct experiments on various LLMs and show that their performance varies widely across the two benchmarks, highlighting the importance of evaluating models' real-world professional capabilities.- The results demonstrate the need for further research and development of LLMs tailored to the specific requirements of the Korean professional domain. | ['Question Answering'] | N/A | N/A |
| [DreamPoster: A Unified Framework for Image-Conditioned Generative Poster
  Design](https://arxiv.org/abs/2507.04218) | Dexiang Hong, Hui Zhang, Zhongqi Qi, Haokun Chen, Xiwei Hu | - DreamPoster is a novel framework that generates high-quality posters from user-provided images and text prompts, addressing the limitations of existing methods by integrating multimodal information and supporting flexible resolution and layout. - The model is built upon a transformer-based diffusion architecture that processes text and image inputs jointly using positional embeddings, allowing for better alignment of textual and visual information in the generated posters. - DreamPoster employs a three-stage progressive training strategy, starting with single-task pretraining, progressing to multi-task mixed training, and finally fine-tuning for aesthetic alignment, enabling more sophisticated poster generation. - Quantitative evaluations demonstrate DreamPoster's superiority over existing methods, achieving a high usability rate of 88.55%, compared to GPT-40 (47.56%) and SeedEdit3.0 (25.96%), signifying its effectiveness in generating aesthetically pleasing and relevant posters. - The results suggest that DreamPoster’s progressive training strategy successfully improves poster design quality across several tasks and addresses several limitations of previously existing methods. | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | N/A | N/A |


## Papers for 2025-07-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951) | Jie Gao, Mengting Xing, Xiaorui Wang, Yuxin Wang, Zixiao Wang | - This paper introduces MetaStone-S1, a novel reflective generative model that achieves comparable performance to OpenAI's 03-mini series for various reasoning tasks. - MetaStone-S1 employs a new Reflective Generative Form, which features a unified interface for policy and process reward models, eliminating the reliance on process-level annotations. - The model architecture utilizes a shared backbone network for both reasoning trajectory prediction and scoring, reducing the number of extra parameters needed for trajectory scoring. - Experiments demonstrate that MetaStone-S1 achieves comparable performance to OpenAI 03-mini with only 32B parameters, outperforming several other open-source and closed-source models in various benchmarks. - The authors open-sourced MetaStone-S1 to support further research and development within the community. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/MetaStone-AI/MetaStone-S1) | N/A |
| [NeuralOS: Towards Simulating Operating Systems via Neural Generative
  Models](https://arxiv.org/abs/2507.08800) | Yuntian Deng, Wenhu Chen, Hongyu Guo, Sun Sun, Luke Rivard | - NeuralOS is a novel neural framework that simulates graphical user interfaces (GUIs) by directly predicting screen frames based on user inputs (mouse movements, clicks, keyboard events). - The model architecture consists of a recurrent neural network (RNN) that tracks the computer's state and a diffusion-based neural renderer that generates screen images. - NeuralOS was trained on a large-scale dataset of Ubuntu XFCE recordings, combining randomly generated and realistic interactions from AI agents. - Experimental results demonstrate that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions. - While precise keyboard interaction modeling is challenging, NeuralOS is a significant step toward creating fully adaptive and generative neural interfaces for human-computer interaction systems. | ['Multimodal', 'Image-to-Image', 'Image-to-Video', 'Computer Vision', 'Reinforcement Learning'] | [Link](https://github.com/google-research/google-research/tree/master/neural_os) | N/A |
| [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799) | Cees G. M. Snoek, M. Jehanzeb Mirza, Michael Dorkenwald, Dawid J. Kopiczko, Max Belitsky | - This paper introduces cache steering, a novel method for implicitly guiding language models' reasoning behavior by directly modifying the key-value cache. - The method uses GPT-4-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without requiring fine-tuning or prompt modifications. - Experimental evaluations on several reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. - Compared to activation steering, cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration. - The proposed method is lightweight and compatible with standard inference APIs, making it a more practical and robust solution for controlled generation. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/MaxBelitsky/cache-steering) | N/A |
| [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397) | Zilong Ye, Wangbo Zhao, Xiaopeng Peng, Jie Xia, Pengfei Zhou | - LoongX, a novel hands-free image editing approach driven by multimodal neurophysiological signals (EEG, fNIRS, PPG, and head motion) is proposed. - LoongX integrates two key modules: a cross-scale state space (CS3) module and a dynamic gated fusion (DGF) module to effectively address the heterogeneity of multimodal signals. - LoongX achieves performance comparable to text-driven methods and outperforms them when neural signals are combined with speech, highlighting the promise of neural-driven generative models. - The model is trained on a comprehensive dataset of 23,928 image editing pairs with synchronized multimodal neural signals. - Datasets and code will be released to support future work and foster progress in this emerging area. | ['Image-to-Image', 'Multimodal'] | [Link](https://loongx1.github.io) | N/A |
| [Lumos-1: On Autoregressive Video Generation from a Unified Model
  Perspective](https://arxiv.org/abs/2507.08801) | Jingyun Liang, Hu Yu, Jun Cen, Weihua Chen, Hangjie Yuan | - Lumos-1 is a novel autoregressive video generation model that leverages the LLM architecture with minimal modifications, addressing limitations of existing methods. - It incorporates a modified rotary position embedding (MM-ROPE) scheme to effectively capture spatiotemporal correlations in video data, improving upon previous 3D RoPE techniques by addressing frequency spectrum imbalances. - To mitigate frame-wise loss imbalance caused by spatial information redundancy, Lumos-1 introduces Autoregressive Discrete Diffusion Forcing (AR-DF), a training and inference masking strategy that maintains temporal causality. - The model achieves performance comparable to state-of-the-art methods on multiple benchmarks (GenEval, COSMOS-Video2World, OpenSoraPlan), despite being pre-trained on a relatively smaller scale (48 GPUs). - The authors introduce several memory-efficient training techniques, demonstrating the potential for effective autoregressive video generation within a unified model framework. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/alibaba-damo-academy/Lumos) | N/A |
| [Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for
  Visual Reasoning](https://arxiv.org/abs/2507.05255) | Jisheng Yin, Kangheng Lin, Jianjian Sun, Liang Zhao, Yana Wei |  - This paper introduces Open-Vision-Reasoner (OVR), a novel multimodal reasoning model that leverages a two-stage training paradigm: linguistic cold-start fine-tuning followed by multimodal reinforcement learning. - OVR achieves state-of-the-art performance on various reasoning benchmarks, including MATH500 (95.3%), MathVision (51.8%), and MathVerse (54.6%), surpassing previous open-source efforts. - The model's superior performance is attributed to the transfer of linguistic cognitive behaviors to visual reasoning, a phenomenon observed early in the training process. -  OVR exhibits high utility behaviors such as visual reflection, effectively leveraging visual cognitive behaviors in complex visual tasks. - The authors contribute a comprehensive analysis of the model's training dynamics and visual cognitive behavior, releasing the model, data, and training dynamics to foster further research. | ['Multimodal'] | N/A | N/A |
| [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,
  Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261) | Noveen Sachdeva, Ice Pasupat, Mike Schaekermann, Eric Bieber, Gheorghe Comanici |  - The paper introduces the Gemini 2.X family of models, including Gemini 2.5 Pro and Gemini 2.5 Flash, which are multimodal models with advanced reasoning and tool-use capabilities.  - Gemini 2.5 Pro is a sparse mixture-of-experts (MoE) transformer that achieves state-of-the-art performance on various benchmarks, including coding, reasoning, and multimodal understanding.  - The models support long context inputs of over 1 million tokens and have native tool use support, enabling them to comprehend vast datasets and handle complex problems from various sources.  - Different models in the series are designed to span the Pareto frontier of model capability versus cost, offering users the ability to explore complex agentic problem-solving scenarios.  - The paper showcases several example applications of Gemini 2.5 Pro, including Gemini Plays Pokémon, where the model successfully completed the game autonomously. | ['Multimodal', 'Video-Text-to-Text', 'Reinforcement Learning', 'Robotics'] | N/A | N/A |
| [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with
  Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771) | Yingfa Chen, Chaojun Xiao, Xu Han, Weilin Zhao, Chenyang Song | - This paper introduces BlockFFN, a novel Mixture-of-Experts (MoE) architecture designed for efficient large language model (LLM) inference on resource-constrained devices. - BlockFFN integrates a ReLU-based differentiable and flexible routing mechanism, which is more efficient than existing MoE routers, and uses CLS-aware training objectives to improve chunk-level sparsity, making it friendlier to acceleration. - The proposed model achieves over 80% token-level sparsity and 70% 8-token chunk-level sparsity. - BlockFFN is evaluated on various tasks and demonstrates significant improvements in terms of perplexity and speed compared to existing MoE models. - Efficient acceleration kernels are implemented that combine activation sparsity and speculative decoding, resulting in up to 3.67x speedup on end-side devices compared to dense models. | ['Natural Language Processing'] | [Link](https://github.com/thunlp/BlockFFN) | N/A |


## Papers for 2025-07-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966) | Hanrong Ye, Qinghao Hu, Baifeng Shi, Wei Huang, Yukang Chen | - The paper introduces LongVILA-R1, a framework that scales reasoning in vision-language models to long videos, using reinforcement learning. - LongVILA-R1 incorporates a large-scale dataset (Long Video-Reason), a two-stage training pipeline (CoT-SFT and RL), and a novel training infrastructure (MR-SP) for efficient long video RL training. - Experiments show LongVILA-R1 achieves strong performance on long video QA benchmarks, outperforming existing methods and matching the performance of Gemini-1.5-Pro on temporal, goal, spatial, and plot reasoning tasks. - The MR-SP system achieves up to 2.1x speedup on long video RL training, enabling training on hour-long videos on a single A100 node. - The authors release their training system, supporting RL training on various modalities (video, text, and audio), various models, and even image and video generation models. | ['Video-Text-to-Text', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/NVlabs/Long-RL) | N/A |
| [Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and
  Methodology](https://arxiv.org/abs/2507.07999) | Zilong Huang, garlicisnotmyfavor, stormthunder, LXT, HaochenWang |  - This paper introduces TreeBench, a new benchmark for evaluating visual grounded reasoning in large multimodal models.  TreeBench emphasizes three key aspects: focused visual perception, traceable evidence, and vision-centric second-order reasoning. - TreeBench contains 405 challenging visual question-answering pairs with accurate bounding boxes of target instances.  Existing models achieve less than 60% accuracy, highlighting its difficulty. - The authors also propose TreeVGR, a training paradigm using reinforcement learning with traceable evidence. TreeVGR enhances the accuracy of localization and the explainability of reasoning pathways. - Experiments on TreeBench and other benchmarks show that TreeVGR improves performance compared to existing methods, demonstrating the effectiveness of its approach. - TreeBench and TreeVGR are publicly available and are expected to advance the field of multimodal reasoning assessment. | ['Visual Question Answering', 'Multimodal'] | [Link](https://github.com/Haochen-Wang409/TreeVGR) | N/A |
| [OST-Bench: Evaluating the Capabilities of MLLMs in Online
  Spatio-temporal Scene Understanding](https://arxiv.org/abs/2507.07984) | Xihui Liu, Xiaohan Mao, Runsen Xu, Chenming Zhu, JingLi Lin |  - OST-Bench is a new benchmark for evaluating online spatio-temporal scene understanding in multi-modal large language models (MLLMs).  - It consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes, focusing on online and spatio-temporal understanding.  - OST-Bench evaluates several leading MLLMs and finds that they fall short on tasks requiring complex spatio-temporal reasoning, with accuracy declining as the exploration horizon extends.  - Common error patterns across models are identified, highlighting the challenges of complex clue-based spatial reasoning and long-term memory retrieval.  - The benchmark is publicly available to foster further research and development in the field. | ['Visual Question Answering', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/rbler1234/OST-Bench) | N/A |
| [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998) | Qilong Wu, Ming Li, Shaoheng Lin, haoquan03, stzhao | - PyVision is a novel multi-turn framework that enables large language models (LLMs) to autonomously generate, execute, and refine Python-based tools for visual reasoning tasks. - It introduces dynamic tooling, allowing LLMs to create custom Python code snippets tailored to the specific problem, going beyond using pre-defined toolsets. - PyVision achieves consistent performance gains across various benchmarks, including a +7.8% improvement on V* with GPT-4.1 and a +31.1% improvement on VLMsAreBlind-mini with Claude-4.0-Sonnet. - The framework uses a multi-turn interaction loop between the LLM and a Python interpreter, allowing for iterative refinement of code and reasoning based on visual feedback. - PyVision's dynamic tooling enables adaptive, grounded, verifiable visual reasoning by creating highly task-specific tools, demonstrating its potential in more agentic visual reasoning scenarios. | ['Multimodal'] | N/A | N/A |
| [Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs](https://arxiv.org/abs/2507.07996) | Yang Li, Ziyue Li, zhoutianyi | - This paper introduces Chain-of-Layers (CoLa), a novel technique that adapts the architecture of pretrained LLMs at test time without any further training. - CoLa dynamically skips, repeats, or reorders layers from the pretrained model to create a customized architecture (CoLa) for each input, allowing for flexible, dynamic architectures. - The authors develop a Monte Carlo Tree Search (MCTS) algorithm to efficiently explore the CoLa search space and identify the optimal architecture for each sample. - Extensive experiments on math and commonsense reasoning benchmarks demonstrate that CoLa consistently improves prediction accuracy and efficiency compared to using the original LLM's fixed architecture. - CoLa's ability to dynamically adapt depth and architecture suggests a significant potential for improving the efficiency and generalization of LLMs on diverse downstream tasks. | ['Natural Language Processing'] | N/A | N/A |
| [Machine Bullshit: Characterizing the Emergent Disregard for Truth in
  Large Language Models](https://arxiv.org/abs/2507.07484) | Thomas L. Griffiths, Dawn Song, Xuandong Zhao, Haimin Hu, Kaiqu Liang |  - This paper introduces a novel metric, the Bullshit Index, to quantify large language models' (LLMs) indifference to truth.   - It proposes a taxonomy classifying four qualitative forms of machine bullshit: empty rhetoric, paltering, weasel words, and unverified claims.   - Empirical evaluations on several benchmarks demonstrate that reinforcement learning from human feedback (RLHF) significantly exacerbates LLM bullshit.   - The study also reveals that chain-of-thought prompting amplifies specific forms of bullshit.   - Findings highlight systematic challenges in AI alignment and offer new insights toward enhancing truthfulness in LLMs. | ['Natural Language Processing'] | [Link](https://machine-bullshit.github.io) | N/A |
| [Beyond the Linear Separability Ceiling](https://arxiv.org/abs/2507.07574) | Mohit Vaishnav, Tanel Tammet, envomp | This paper introduces a novel diagnostic framework to analyze the limitations of Visual-Language Models (VLMs) on abstract reasoning tasks. The framework identifies a "linear reasoning bottleneck" where VLM performance is limited by the linear separability of their visual embeddings.  The authors demonstrate that this bottleneck is not due to poor visual perception, but rather a failure in the reasoning pathways.  The study proposes two pathways to improve VLM performance: (1) refining embeddings for improved linear separability, (2) enabling non-linear reasoning through targeted alignment.  Finally, the framework helps identify intervention points for efficient fine-tuning to enhance VLM performance.  | ['Multimodal'] | N/A | N/A |


## Papers for 2025-07-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448) | Hongru Wang, Sofia Stoica, Xuehang Guo, Zhenhailong Wang, xhyandwyy | - The paper introduces PAPO, a novel reinforcement learning algorithm that improves multimodal reasoning by incorporating an Implicit Perception Loss. - PAPO encourages models to learn to perceive visual inputs while learning to reason, using internal supervision signals without relying on external reward models or additional data. - Experiments demonstrate that PAPO significantly improves performance on diverse multimodal reasoning benchmarks, with gains of up to 8% on tasks with high vision dependency. - A comprehensive error analysis reveals that PAPO substantially reduces perception errors, indicating improved perceptual capabilities. - The authors rigorously analyze and mitigate a unique loss hacking issue, proposing a Double Entropy Loss to prevent model collapse. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://mikewangwzhl.github.io/PAPO) | [Link](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) |
| [Rethinking Verification for LLM Code Generation: From Generation to
  Testing](https://arxiv.org/abs/2507.06920) | Minnan Luo, Wenwei Zhang, Maosong Cao, Taolin Zhang, MichaelErchi |  - This paper introduces SAGA, a novel human-LLM collaborative framework for Test Case Generation (TCG), which significantly improves the quality and diversity of generated test cases compared to existing methods.  - The effectiveness of SAGA is demonstrated through experiments on TCGBench, achieving a 90.62% detection rate and 32.58% verifier accuracy.  - SAGA's superior performance is further validated by its application to enhance the LiveCodeBench-v6 benchmark, demonstrating a 10.78% increase in verifier accuracy.  - The proposed multi-dimensional evaluation metrics rigorously quantify test suite thoroughness and provide a valuable framework for assessing the quality of code verifiers.  - Finally, the work includes the development of TCGBench, a benchmark dataset for facilitating future research into TCG. | ['Text Generation'] | [Link](https://github.com/open-compass/SAGA) | N/A |
| [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457) | Taylor Kergan, Yong Shan, Steven Abreu, Dustin Wang, ridger | - This paper presents a systematic analysis of hybrid linear attention models for long sequences, addressing the quadratic complexity limitations of traditional Transformers. - It comprehensively evaluates various linear attention models across generations, both standalone and hybridized, training and open-sourcing 72 models with varying parameters and training data. - The study reveals that superior standalone linear models do not necessarily translate to superior hybrid performance, and recall significantly improves with increased full attention layers. - It highlights the importance of selective gating, hierarchical recurrence, and controlled forgetting in achieving efficient hybrid models, recommending architectures like HGRN-2 or GatedDeltaNet with a specific linear-to-full attention ratio. - The paper offers practical guidelines for building memory-efficient long-context language models based on these findings. | ['Natural Language Processing'] | N/A | N/A |
| [Towards Solving More Challenging IMO Problems via Decoupled Reasoning
  and Proving](https://arxiv.org/abs/2507.06804) | Feng Zhang, Tao Yang, Yang Li, Linfeng Song, Zhenwen Liang | This paper introduces a novel framework for automated theorem proving that decouples high-level reasoning from low-level proof generation.  The framework uses two specialized models: a general-purpose Reasoner (LLM) to generate strategic lemmas and an efficient Prover to verify them.  This approach successfully solves 5 challenging post-2000 IMO problems, significantly outperforming existing methods.  The authors release a dataset of generated and verified lemmas to facilitate further research. The framework's decoupled design allows the Reasoner and Prover to leverage their respective strengths, enhancing overall problem-solving capabilities. | ['Natural Language Processing'] | [Link](https://tencent-imo.github.io/) | [Link](https://tencent-imo.github.io/) |
| [ModelCitizens: Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455) | Karolina Naranjo, notaphonologist, hamidpalangi, christinachance, Ashima | - This paper introduces MODELCITIZENS, a new dataset comprising 6.8K social media posts and 40K toxicity annotations across diverse identity groups, addressing the limitations of existing datasets which often collapse diverse annotator perspectives into a single ground truth. - To capture the conversational context typical of social media posts, the dataset includes LLM-generated conversational scenarios. - State-of-the-art toxicity detection tools underperform on MODELCITIZENS, highlighting the importance of community-informed annotation and modeling. - Two new models, LLAMACITIZEN-8B and GEMMACITIZEN-12B, are presented, which outperform existing models by 5.5% on in-distribution evaluations. - The findings underscore the significance of community-informed annotation and modeling for inclusive content moderation. | ['Text Classification'] | [Link](https://github.com/asuvarna31/modelcitizens) | N/A |
| [Evaluating the Critical Risks of Amazon's Nova Premier under the
  Frontier Model Safety Framework](https://arxiv.org/abs/2507.06260) | Vincent Ponzo, Matteo Memelli, Abhinav Mohanty, Ninareh Mehrabi, Satyapriya Krishna | - This paper presents a comprehensive evaluation of Amazon's Nova Premier, a multimodal foundation model, using the Frontier Model Safety Framework. - The evaluation focuses on three high-risk domains: Chemical, Biological, Radiological & Nuclear (CBRN) weapons proliferation, Offensive Cyber Operations, and Automated AI R&D. - The methodology combines automated benchmarks, expert red-teaming, and uplift studies to assess whether the model exceeds release thresholds. - The findings indicate that Nova Premier is safe for public release, according to the commitments made at the 2025 Paris AI Safety Summit. - This work provides a template for future cross-organisational safety audits of frontier models. | ['Multimodal'] | N/A | N/A |
| [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large
  Language Models on Harmfulness](https://arxiv.org/abs/2507.01702) | Zhen Ye, Ziyang Luo, Kaixin Li, Hongzhan Lin, Zixin Chen | - This paper introduces AdamMeme, a novel evaluation framework for assessing the reasoning capabilities of multimodal large language models (mLLMs) on meme harmfulness. - AdamMeme uses a multi-agent system with a harmfulness mining agent, model scoring agent, and iterative refinement agent to dynamically assess the mLLMs. - The framework addresses the limitations of existing static benchmarks by iteratively generating challenging meme samples, revealing model-specific weaknesses. - Extensive experiments demonstrated that AdamMeme effectively reveals varying performance and model-specific vulnerabilities of different mLLMs on harmfulness analysis. - The proposed framework is adaptable to the evolving nature of memes, promoting more comprehensive and diverse evaluation. | ['Multimodal'] | [Link](https://github.com/Lbotirx/AdamMeme) | N/A |


## Papers for 2025-07-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203) | Tianhao Peng, jeshragh, chujiezheng, Jinfa, ridger | - This survey paper provides a comprehensive overview of latent reasoning in large language models (LLMs), focusing on how these models can perform complex reasoning tasks without explicitly generating intermediate reasoning steps. - It introduces a novel taxonomy for categorizing different latent reasoning approaches, including vertical and horizontal methods, and discusses the trade-offs between these approaches. - The authors analyze existing latent reasoning models and highlight key architectural design choices and training strategies. - They also explore the mechanistic interpretability of latent reasoning, investigating the role of different layers in the model's reasoning process. - Finally, the paper looks ahead to the future of latent reasoning, discussing the potential for infinite-depth reasoning and the development of more efficient and powerful reasoning systems. | ['Natural Language Processing'] | [Link](https://github.com/multimodal-art-projection/LatentCoT-Horizon/) | N/A |
| [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566) | Ron Kimmel, Daniel Bensaïd, David Bensaïd, royve, noamrot | - SingLoRA is a novel parameter-efficient fine-tuning method that reformulates low-rank adaptation using a single matrix instead of two, addressing the scale disparity issues that cause unstable training in existing methods like LoRA. - Unlike LoRA, SingLoRA inherently removes inter-matrix scale conflicts, ensuring stable optimization and roughly halving the parameter count. - Experiments on common sense reasoning (MNLI) show SingLoRA outperforming LoRA and LoRA+ while using only 60% of their parameter budget, and in image generation (DreamBooth), SingLoRA significantly improves image fidelity. - Theoretical analysis within the infinite-width neural network framework demonstrates that SingLoRA guarantees stable feature learning by construction. - The method is extended to non-square matrices and validated through comprehensive experiments across multiple modalities. | ['Natural Language Processing', 'Image-to-Image'] | N/A | N/A |
| [CriticLean: Critic-Guided Reinforcement Learning for Mathematical
  Formalization](https://arxiv.org/abs/2507.06181) | Yifan Yao, Zhongyuan Peng, zhangysk, zhouliang, yifAI | CriticLean is a novel critic-guided reinforcement learning framework for translating natural language mathematical statements into formal, executable code.  It introduces CriticLeanGPT, a model trained to assess the semantic fidelity of Lean 4 formalizations, and CriticLeanBench, a benchmark to measure models' ability to distinguish semantically correct from incorrect formalizations.  CriticLean significantly outperforms existing baselines on CriticLeanBench.  Furthermore, it constructs FineLeanCorpus, a dataset of over 285K problems exhibiting rich domain diversity and difficulty coverage.  The results highlight that optimizing the critic phase is essential for reliable formalization. | ['Reinforcement Learning', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/multimodal-art-projection/CriticLean) | N/A |
| [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138) | Songyang Zhang, Maosong Cao, Taolin Zhang, jnanliu, MichaelErchi | - This paper introduces the "Code Triangle" framework, a novel approach to evaluating Large Language Models (LLMs) in code generation. - The framework assesses LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation, revealing inconsistencies in LLM cognition. - Experiments on competitive programming benchmarks show that while LLMs can form self-consistent systems, their solutions lack diversity and robustness compared to human programmers. - The study identifies a significant distribution shift between model cognition and human expertise, highlighting the impact of training data biases. - Incorporating human-generated data and leveraging model mixtures are shown to significantly enhance both performance and robustness. | ['Natural Language Processing'] | N/A | N/A |
| [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791) | Yuhao Yang, Yutong Dai, Dongxu Li, Yan Yang, Ziyang | - This paper introduces GTA1, a novel GUI Test-time Scaling Agent that addresses the challenges of ambiguity in task planning and accurate grounding of actions in complex interfaces. - GTA1 employs a test-time scaling method where multiple candidate action proposals are sampled at each step, and a judge model selects the most appropriate one. - The agent also uses a grounding model that predicts interaction coordinates directly, without relying on explicit reasoning. - Experiments on diverse benchmarks show that GTA1 achieves state-of-the-art performance in GUI grounding and task completion, outperforming existing methods across various metrics. - The code and models are open-sourced, allowing for further research and development in the field of GUI agents. | ['Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts](https://arxiv.org/abs/2507.04569) | Mohamed Anwar, Amr Mohamed, Ahmad Chamma, Hadi Abdine, guokan-shang | - This paper introduces Nile-Chat, a family of large language models (LLMs) for Egyptian Arabic that natively supports both Arabic and Latin scripts. - Nile-Chat-3x4B-A6B, a Mixture-of-Experts (MoE) model, leverages a novel language adaptation approach using the Branch-Train-MiX strategy to merge script-specialized experts. - The models significantly outperform existing multilingual and Arabic LLMs on new Egyptian evaluation benchmarks, achieving a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. - All models, data, and evaluation code are publicly available, making it a valuable resource for research on LLMs for underrepresented and dual-script languages. - The work introduces a comprehensive methodology for adapting LLMs to dual-script languages. | ['Translation', 'Text Generation', 'Natural Language Processing'] | [Link](https://github.com/MBZUAI-Paris/lm-evaluation-harness-nile-chat) | [Link](https://hf.co/MBZUAI-Paris/Nile-Chat-12B), [Link](https://hf.co/datasets/MBZUAI-Paris/Egyptian-SFT-Mixture), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianBench), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianMMLU), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianHellaSwag), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianPIQA), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianWinoGrande), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianOpenBookQA), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianRACE), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianAlpacaEval) |
| [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223) | Yi Fang, Ting-ruen Wei, Zhiyuan Peng, yilunzhao, songtingyu | - This paper introduces new metrics, RPP and QPP, to evaluate the efficiency-effectiveness trade-off of LLM-based rerankers, addressing the limitations of existing proxy metrics. - It proposes a closed-form, interpretable formula for estimating the FLOPs of LLM-based rerankers and provides an open-source calculator. - The paper conducts a large-scale study comparing various LLM-based rerankers across different architectures and tasks using the proposed metrics. - Experimental results reveal that pointwise methods generally outperform pairwise and listwise methods in terms of both ranking metrics and FLOP efficiency. - The study highlights the importance of considering efficiency-effectiveness trade-offs when deploying LLM-based rerankers in practice. | ['Natural Language Processing'] | [Link](https://github.com/zhiyuanpeng/EER-FLOPs) | N/A |
| [LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation
  framework](https://arxiv.org/abs/2507.04723) | Ruoxi Sun, Baibei Ji, Haitian Wang, Zecheng Tang, QQTang1223 | - This paper introduces LOOM-Scope, a comprehensive and efficient framework for evaluating long-context models. - LOOM-Scope standardizes evaluation settings, supports efficient inference acceleration methods, and provides a holistic benchmark suite. - The framework addresses inconsistencies in existing benchmarks by standardizing evaluation settings and minimizing confounding factors. - LOOM-Scope supports 22 long-context benchmarks and over 140 tasks, covering diverse capabilities and context lengths. - Experiments demonstrate LOOM-Scope's efficiency, enabling comprehensive evaluations with significantly reduced computational costs compared to existing benchmarks. | ['Natural Language Processing'] | [Link](https://github.com/LCM-Lab/LOOM-Scope) | N/A |
| [Differential Mamba](https://arxiv.org/abs/2507.06204) | Eliya Nachmani, Itamar Zimerman, Nadav Schneider | - This paper introduces Diff-Mamba, a novel modification of the Mamba architecture that incorporates a differential mechanism to mitigate the issue of over-allocating attention to irrelevant context in sequence models. - Diff-Mamba achieves improved retrieval capabilities and superior performance compared to the vanilla Mamba architecture on language modeling benchmarks. - The proposed differential mechanism is empirically validated through extensive ablation studies and empirical analyses demonstrating improved performance on various language tasks. - The authors address the limitations of a naive adaptation of differential design to Mamba by introducing architectural modifications to effectively mitigate the over-allocation problem. - The code for Diff-Mamba is publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NadavSc/Diff-Mamba) | N/A |
| [any4: Learned 4-bit Numeric Representation for LLMs](https://arxiv.org/abs/2507.04610) | Jeff Johnson, melhoushi | - This paper introduces any4, a novel 4-bit weight quantization technique for Large Language Models (LLMs). Unlike other methods, any4 does not require preprocessing of weights or activations.  - Any4 achieves higher accuracy than existing 4-bit numeric representation types (int4, fp4, nf4) across various model sizes and families (Llama 2, Llama 3, Mistral, Mixtral).  - It demonstrates competitiveness with orthogonal techniques (AWQ, GPTQ) that do require preprocessing.  - The method uses a single, curated diverse sample for calibration rather than hundreds, improving efficiency.  - Along with any4, the authors release tinygemm, a latency-optimized GPU matrix multiplication library. | ['Natural Language Processing'] | [Link](https://github.com/facebookresearch/any4) | N/A |
| [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based
  Reinforcement Learning](https://arxiv.org/abs/2507.05920) | Rui Feng, Bo Li, Weiwei Tian, Yuhao Dong, Xinyu Huang | - This paper introduces Multi-turn Grounding-based Policy Optimization (MGPO), a novel reinforcement learning framework that enables large multimodal models (LMMs) to iteratively focus on key visual regions by automatically cropping sub-images.  - MGPO effectively elicits stronger grounding capabilities compared to GRPO, leading to a 5.4% improvement on in-distribution MME-Realworld and a 5.2% improvement on the challenging out-of-distribution (OOD) V* Bench.  - MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses OpenAI's o1 and GPT-40 models on the OOD V* Bench.  - The proposed method overcomes the maximum pixel constraints of LMMs and does not require additional grounding annotations.  - MGPO achieves top-down and interpretable visual reasoning by providing outputs that indicate which image regions are attended to throughout the reasoning process. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/EvolvingLMMs-Lab/MGPO) | N/A |
| [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and
  Mitigation](https://arxiv.org/abs/2507.05578) | Dawn Song, Aneesh Pappu, Xuandong Zhao, Alexander Xiong | - This paper investigates the landscape of memorization in large language models (LLMs), examining its mechanisms, measurement, and mitigation. - It explores key drivers of memorization, including data duplication, training dynamics, and fine-tuning procedures. - The paper examines methodologies for detecting and measuring memorized content, such as prefix-based extraction and membership inference. - It discusses mitigation strategies, including data cleaning, differential privacy, and post-training unlearning. - Finally, the paper identifies critical directions for future research on LLM memorization, highlighting open challenges in balancing the minimization of harmful memorization with utility. | ['Natural Language Processing'] | N/A | N/A |
| [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411) | Hanzhi Zhou, John Peebles, Chang Lan, Tom Gunter, Mark Lee | - AXLearn is a production-ready deep learning system designed for scalable and high-performance training of large deep learning models. - It prioritizes modularity and supports heterogeneous hardware infrastructure, allowing for rapid model development and experimentation across various platforms. - AXLearn uses a novel method to quantify modularity via Lines-of-Code (LoC)-complexity, demonstrating its constant complexity even as components are added, unlike other systems with linear or quadratic complexity. - It achieves equivalent performance compared to state-of-the-art systems. - The paper provides details on the development and operation experiences of AXLearn. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/apple/axlearn) | N/A |


## Papers for 2025-07-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|


## Papers for 2025-07-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Eka-Eval : A Comprehensive Evaluation Framework for Large Language
  Models in Indian Languages](https://arxiv.org/abs/2507.01853) | Mayank Singh, Abhishek Upperwal, Samridhi Raj Sinha, RajveeSheth | - This paper introduces EKA-EVAL, a comprehensive and production-ready evaluation framework for large language models (LLMs) in Indian languages. - EKA-EVAL integrates over 35 benchmarks, including 10 Indic-specific datasets, covering various categories like reasoning, mathematics, tool use, and long-context understanding. - Compared to existing tools, EKA-EVAL offers broader benchmark coverage and built-in support for distributed inference, quantization, and multi-GPU usage. - The framework is open-source and publicly available, significantly lowering the barrier to multilingual benchmarking. - EKA-EVAL is designed to be modular, easy to configure, and compatible with HuggingFace and proprietary models. | ['Natural Language Processing'] | [Link](https://github.com/lingo-iitgn/eka-eval) | N/A |
| [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation
  Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955) | Oğuzhan Fatih Kar, Andrei Atanov, Roman Bachmann, Ali Garjani, Rahul Ramachandran | This paper introduces a novel benchmark for evaluating multimodal foundation models' vision capabilities.  The benchmark focuses on established computer vision tasks such as object detection, segmentation, and depth estimation. The authors address the challenge of evaluating models that primarily output text by devising prompt-chaining techniques.  They find that while the models are not state-of-the-art on individual tasks, they perform respectably as generalists and exhibit better performance on semantic tasks compared to geometric tasks.  Finally, GPT-40 demonstrates the best overall performance among the models evaluated. | ['Multimodal', 'Computer Vision', 'Depth Estimation', 'Image Classification', 'Object Detection', 'Image Segmentation', 'Image-to-Text'] | [Link](https://github.com/EPFL-VILAB/fm-vision-evals) | N/A |


## Papers for 2025-07-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with
  TriMap Video Diffusion](https://arxiv.org/abs/2507.02813) | Minghui Yang, Jiawei Chi, Hao Li, Fangfu Liu, hanyang-21 | - LangScene-X is a novel generative framework that reconstructs generalizable 3D language-embedded scenes from sparse views (as few as two images) using a TriMap video diffusion model and a Language Quantized Compressor (LQC). - The TriMap video diffusion model generates 3D-consistent RGB images, normal maps, and semantic maps, while the LQC efficiently encodes language embeddings for cross-scene generalization. - LangScene-X unifies and generates 3D consistent multi-modality information for reconstruction and understanding, supporting open-ended language queries. - Experimental results demonstrate that LangScene-X outperforms state-of-the-art methods in terms of quality and generalizability on real-world datasets. - The model uses a progressive multi-task training strategy that integrates knowledge from diverse domains to ensure 3D consistency and high-quality generation. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://liuff19.github.io/LangScene-X/) | N/A |
| [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592) | Liwen Zhang, Huifeng Yin, Zhongwang Zhang, Kuan Li, xxwu | - This paper introduces WebSailor, a post-training methodology designed to enhance the reasoning capabilities of large language models (LLMs) for complex web-based information-seeking tasks. - WebSailor significantly outperforms all open-source agents and matches the performance of proprietary agents on complex benchmarks like BrowseComp, by systematically reducing uncertainty when navigating vast information landscapes. - The approach involves generating high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm called DUPO. - WebSailor's performance improvements stem from its training data synthesis, which generates complex, emergent structures that compel the model to develop advanced reasoning patterns. - The experimental results demonstrate that WebSailor significantly outperforms existing open-source models and is on par with commercial, closed-source models. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Alibaba-NLP/WebAgent) | N/A |
| [Thinking with Images for Multimodal Reasoning: Foundations, Methods, and
  Future Frontiers](https://arxiv.org/abs/2506.23918) | Zhenhua Liu, Hangyu Guo, Peng Xia, Zhaochen Su, Xiaoye08 |  - This paper introduces a novel three-stage framework for multimodal reasoning which transcends the limitations of existing text-centric approaches by enabling models to "think with images". - The framework categorizes the evolution of image utilization in AI models into three stages: tool-driven visual exploration, programmatic visual manipulation, and intrinsic visual imagination. - Each stage is characterized by specific methodologies (prompt-based, SFT-based, RL-based) and unique challenges (computational cost, information density, architectural divide, cross-task generalization). - The paper provides a comprehensive review of core methods and evaluation benchmarks for each stage and identifies significant challenges and promising future directions for research. -  The study aims to establish foundational principles for "Thinking with Images", offering a clear roadmap for future research towards more powerful and human-aligned multimodal AI. | ['Multimodal'] | [Link](https://github.com/zhaochen0110/Awesome_Think_With_Images) | N/A |
| [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for
  Deep Search](https://arxiv.org/abs/2507.02652) | Yutao Zhu, Yuyao Zhang, Guanting Dong, Xiaoxi Li, Jiajie Jin | - This paper introduces HiRA, a hierarchical reasoning framework for deep search that decouples strategic planning from specialized execution to improve efficiency and scalability. - HiRA decomposes complex search tasks into focused subtasks, assigns them to domain-specific agents with external tools, and coordinates results through a structured mechanism. - Experiments on four complex, cross-modal deep search benchmarks show that HiRA significantly outperforms state-of-the-art RAG and agent-based systems in both answer quality and system efficiency. - The separation of planning and execution in HiRA prevents execution details from disrupting high-level reasoning, enabling the system to leverage specialized expertise for different types of information processing. - HiRA's modular design allows for easy integration of new tools and capabilities, improving extensibility and adaptability to diverse search scenarios. | ['Question Answering'] | [Link](https://github.com/ignorejjj/HiRA) | N/A |
| [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754) | Jiecao Yu, Sijia Chen, Sai Surya Duvvuri, Timothy Chou, Aurko Roy | - This paper introduces the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions using an efficient Triton kernel. - The 2-simplicial Transformer achieves better token efficiency than standard Transformers; for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. - The authors demonstrate that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention. - The paper includes a detailed description of the 2-simplicial attention mechanism, including its implementation in Triton, and an analysis of its computational complexity. - Experimental results show that the 2-simplicial Transformer achieves significant improvements in downstream performance on reasoning-heavy tasks. | ['Natural Language Processing'] | N/A | N/A |
| [Can LLMs Identify Critical Limitations within Scientific Research? A
  Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694) | Arman Cohan, Lovekesh Vig, Manasi Patwardhan, Yilun Zhao, Zhijian Xu | This paper introduces LIMITGEN, a novel benchmark dataset designed to evaluate Large Language Models' (LLMs) ability to identify critical limitations within scientific research papers. The dataset is categorized into two subsets, one synthetic and one human-generated. The model uses a retrieval augmented generation (RAG) technique to ground LLM limitation generation and improve feedback quality. The study found that LLMs struggle to identify limitations, but RAG consistently improves performance. The researchers propose a taxonomy of limitation types, focusing on AI research, to guide evaluations. | ['Natural Language Processing'] | [Link](https://github.com/yale-nlp/LimitGen) | [Link](https://huggingface.co/datasets/yale-nlp/LimitGen) |
| [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092) | Peixuan Han, Md Mofijul Islam, Ganesh Nanduru, Alexi Gladstone, amanchadha | This paper introduces Energy-Based Transformers (EBTs), a new class of energy-based models that leverage a dynamic allocation of computation to improve model performance.  The EBT model architecture uses Transformers to enable efficient EBM training, outperforming the Transformer++ approach in scaling and demonstrating improvements with System 2 Thinking, or improved reasoning capabilities through additional computation.  Experiments across discrete and continuous modalities show EBTs scaling faster and achieving higher performance than Transformer++ and Diffusion Transformers on a variety of downstream tasks. EBTs also exhibit improved generalization to out-of-distribution data. | ['Natural Language Processing', 'Text Generation', 'Computer Vision', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/alexiglad/EBT) | N/A |
| [Selecting and Merging: Towards Adaptable and Scalable Named Entity
  Recognition with Large Language Models](https://arxiv.org/abs/2506.22813) | Wei Wei, Zhuojun Ding, Facico | - This paper introduces the SaM framework, a novel model merging strategy for Named Entity Recognition (NER) that dynamically selects and merges expert models at inference time. - SaM improves generalization across various domains without extra training by dynamically merging beneficial expert models, enhancing adaptability and scalability. - Extensive experiments demonstrate SaM's effectiveness, outperforming unified models by an average of 10% across multiple benchmarks and up to 20% in specific domains. - The framework's scalability is highlighted by the ability to conveniently add or remove experts, adapting to evolving needs. - SaM addresses the limitations of existing methods which struggle with adaptation and scalability due to the cost of data annotation and training domain-specific models. | ['Natural Language Processing'] | [Link](https://github.com/Ding-ZJ/SaM) | [Link](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) |
| [Self-Correction Bench: Revealing and Addressing the Self-Correction
  Blind Spot in LLMs](https://arxiv.org/abs/2507.02778) | Ken Tsui |  - This paper introduces Self-Correction Bench, a novel framework designed to systematically evaluate the self-correction capabilities of large language models (LLMs). - The framework injects controlled errors into LLM outputs, and it measures their ability to identify and correct these errors across three complexity levels. - Through experiments involving 14 models, it is observed that LLMs exhibit a significant "self-correction blind spot," failing to correct identical errors in their own outputs while succeeding on similar errors in user inputs. - This blind spot is linked to the composition of training data, where error-free responses are predominantly presented compared to error-correction sequences. -  A simple intervention, appending "Wait," remarkably improves self-correction performance, suggesting that the capability is present but underutilized. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear
  Attention](https://arxiv.org/abs/2507.01004) | Tianjian Li, Xinyi Wan, Ruijie Zhu, Zehao Liu, Yuhong Chou | - ZeCO is a novel sequence parallelism method for linear attention models that reformulates sequence parallelism by leveraging an All-Scan collective communication primitive, achieving the theoretically minimum communication volume.  - ZeCO's integrated approach enables efficient overlap of communication and computation, resulting in minimal extra computational and I/O overhead.  - Theoretical optimality of ZeCO is proven by showing that its time cost is minimal compared to other methods.  - Empirical results show that ZeCO achieves up to a 3.9x communication speedup and a 9.3x overall speedup compared to SOTA methods, demonstrating near-linear scalability from 8 to 256 devices.  - The method establishes a clear path towards efficiently training next-generation LLMs on previously intractable sequence lengths. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-07-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949) | huxiao09, yw95, TinaGao, hjy, caojiangxia |  - This paper introduces Kwai Keye-VL, an 8-billion parameter multimodal foundation model designed for short-video understanding and general-purpose vision-language tasks. - The model architecture uses a Qwen3-8B language model with a vision encoder initialized from SigLIP, supporting native dynamic resolution and 3D ROPE for unified processing of text, image, and video information. - Keye-VL was trained using a four-stage pre-training process and a two-phase post-training process, achieving state-of-the-art results on public video benchmarks and remaining highly competitive on general image-based tasks. - The authors also introduce KC-MMBench, a new benchmark for real-world short-video scenarios, where Keye-VL shows a significant advantage. - The training methodology, data construction, and evaluation results are detailed in the paper, providing insights for building the next generation of MLLMs for the video era. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/Kwai-Keye/Keye) | [Link](https://huggingface.co/Kwai-Keye) |
| [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/abs/2506.23552) | Youngjung Uh, Jaesik Park, Jaeseok Jung, Mingi Kwon, alex4727 | - JAM-Flow is a novel unified framework for simultaneously synthesizing and conditioning on both facial motion and speech, addressing talking head generation and text-to-speech synthesis as a single task. - The model leverages flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) architecture with specialized Motion-DiT and Audio-DiT modules coupled via selective joint attention layers. - It incorporates key architectural choices such as temporally aligned positional embeddings and localized joint attention masking for effective cross-modal interaction while preserving modality-specific strengths. - JAM-Flow supports various conditioning inputs, including text, reference audio, and reference motion, facilitating tasks like synchronized talking head generation from text and audio-driven animation. - Experimental results on HDTF and CelebV-Dub datasets demonstrate that JAM-Flow significantly outperforms existing methods on talking head generation and automated video dubbing tasks. | ['Multimodal', 'Text-to-Speech', 'Text-to-Video', 'Image-to-Video', 'Audio-to-Audio'] | N/A | N/A |


## Papers for 2025-07-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable
  Reinforcement Learning](https://arxiv.org/abs/2507.01006) | tanghme0www, bigganbing, xgeric, iyuge2, wenyi | The paper introduces GLM-4.1V-Thinking, a vision-language model (VLM) designed for versatile multimodal reasoning.  The model architecture consists of a ViT encoder, an MLP projector, and a large language model decoder.  It utilizes Reinforcement Learning with Curriculum Sampling (RLCS) for training, achieving state-of-the-art results on 28 public benchmarks, outperforming even significantly larger models in many tasks. The model and training code are open-sourced to facilitate further research.  The authors also highlight various strategies employed for efficiency and stability during training. | ['Multimodal'] | [Link](https://github.com/THUDM/GLM-4.1V-Thinking) | N/A |
| [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional
  Multimodal Embeddings](https://arxiv.org/abs/2506.23115) | Nan Yang, Liang Wang, roosephu, hongliu9903, Haon-Chen | - This paper introduces MoCa, a two-stage framework designed to enhance bidirectional multimodal embeddings by leveraging pre-trained Vision Language Models (VLMs). - The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, improving bidirectional context-aware reasoning. - The second stage, Heterogeneous Contrastive Fine-tuning, utilizes diverse multimodal data (beyond image-caption pairs) to improve generalization and alignment. - Experiments on MMEB and ViDoRe-v2 benchmarks demonstrate that MoCa consistently outperforms existing state-of-the-art methods, achieving new state-of-the-art results on MMEB. - MoCa exhibits strong scalability with both model size and training data, showcasing its effectiveness in various downstream applications. | ['Multimodal'] | [Link](https://haon-chen.github.io/MoCa/) | N/A |
| [SciArena: An Open Evaluation Platform for Foundation Models in
  Scientific Literature Tasks](https://arxiv.org/abs/2507.01001) | Ronan Le Bras, Sihong Wu, HughieHu, maxzky, yilunzhao | SciArena is an open-source platform designed to evaluate foundation models' performance on tasks related to scientific literature.  It uses a community voting system to compare models, allowing for open-ended questions and long-form responses.  The platform currently supports 23 models and has collected over 13,000 votes.  SciArena also provides a meta-evaluation benchmark, SciArena-Eval, to assess the accuracy of automated evaluation systems.  The results show that even the best-performing model achieves only 65.1% accuracy compared with human preference. | ['Question Answering'] | [Link](https://github.com/yale-nlp/SciArena) | [Link](https://huggingface.co/datasets/yale-nlp/SciArena) |
| [Does Math Reasoning Improve General LLM Capabilities? Understanding
  Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432) | Seungone Kim, Xiaoyu Xu, Yuetai Li, Maggie Huan, aaabiao |  - This paper introduces a novel metric, the Transferability Index, to evaluate the generalization capabilities of large language models (LLMs) trained on mathematical reasoning tasks. - The study reveals that reinforcement learning (RL)-tuned models generalize better across various reasoning and non-reasoning tasks than supervised fine-tuning (SFT)-tuned models. - Latent-space representation and token-space distribution shift analyses are employed to show that SFT induces significant representation and output drift, whereas RL preserves the general-domain structure. - Controlled experiments on Qwen3-14B models using math-only data and different tuning methods confirm that RL-tuned models generalize well across domains, while SFT-tuned models often forget general capabilities. - The findings suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models. | ['Natural Language Processing'] | [Link](https://github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning) | [Link](huggingface.co/ReasoningTransferability) |
| [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code
  Generation](https://arxiv.org/abs/2506.20639) | Navdeep Jaitly, Jiatao Gu, Huangjie Zheng, Ruixiang Zhang, Shansan Gong | This paper introduces DiffuCoder, a 7B parameter diffusion language model (LLM) specifically designed for code generation.  The model architecture is based on masked diffusion models and is trained on a large-scale corpus of 130B code tokens.  The authors investigate the decoding behavior of DiffuCoders, revealing differences from autoregressive (AR) models, such as the ability to control the causality of generation. A novel RL training framework, coupled-GRPO, significantly improves DiffuCoder's performance, surpassing existing methods by +4.4% on EvalPlus benchmarks and demonstrating the effectiveness of RL aligned with diffusion principles.  Further experiments showcase the impact of temperature on the model's autoregressive nature, with higher temperatures leading to increased generation diversity. | ['Text Generation'] | [Link](https://github.com/apple/ml-diffucoder) | N/A |
| [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277) | Weixuan Chen, Shimin Yao, BBBBCHAN, fushh7, PhilipC | - HumanOmniV2 is a novel multimodal large language model designed to enhance omni-modal reasoning capabilities by focusing on global context understanding and preventing shortcut problems. - The model incorporates context, format, accuracy, and logical rewards, using an LLM to assess context and logical reasoning, ensuring a thorough understanding of multimodal information. - A new reasoning training dataset is introduced, incorporating context information across various tasks (images, videos, and audio), and a new benchmark, IntentBench, evaluates models' ability to understand complex human intentions. - HumanOmniV2 outperforms existing open-source omni-modal models across multiple benchmarks, demonstrating superior performance in understanding complex human intentions and emotions. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/HumanMLLM/HumanOmniV2) | N/A |
| [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive
  Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951) | Abbas Shah, Ranjan Sapkota, Rizwan Qureshi, amanchadha, shainaraza | This paper reviews the current state of artificial general intelligence (AGI) research, emphasizing the limitations of token-level prediction models and highlighting the need for architectures grounded in cognitive neuroscience and embodied understanding.  It proposes a unified framework that integrates insights from artificial intelligence, cognitive neuroscience, and psychology, with a focus on agentic reasoning and modular architectures.  The paper discusses emergent AGI-enabling methods and challenges, including generalization strategies and alignment issues. Finally, it advocates for systems that are not only intelligent but also transparent, value-aligned, and socially grounded.  | ['Multimodal'] | N/A | N/A |
| [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545) | Chong Li, Wenshan Wu, Xin Zhang, Yangyu Huang, Yalun Dai | - This paper introduces a novel paradigm called DELT for improving the data efficacy of language model training by optimizing the organization of training data, rather than just focusing on data efficiency. - DELT consists of three components: Data Scoring, Data Selection, and Data Ordering, which are used to assign scores, select subsets, and organize the training data, respectively. - The paper proposes a new Data Scoring method called Learnability-Quality Scoring (LQS) and a new Data Ordering method called Folding Ordering (FO), which are shown to enhance LM performance. - Experiments demonstrate that DELT instances enhance Language Model performance without increasing the data scale and model size, and that the combination of LQS and FO achieves the most significant improvement. - The authors conclude that data efficacy is a promising foundational area in LM training and complements data efficiency. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-07-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044) | Pengxin Zhan, Liangfu Cao, Xinjie Zhang, Shanshan Zhao, Flourish | - The paper introduces Ovis-U1, a 3-billion parameter unified multimodal model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. - Ovis-U1 uses a diffusion-based visual decoder and a bidirectional token refiner, enabling image generation and editing tasks comparable to leading models such as GPT-4. - The model achieves state-of-the-art results on several benchmarks, including a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models. - Ovis-U1's unified training approach, starting from a language model, yields better performance than training solely on understanding or generation tasks. - The model pushes the boundaries of multimodal understanding, generation, and editing, demonstrating the enhancement achieved by integrating these two tasks. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image', 'Image-to-Video', 'Visual Question Answering'] | [Link](https://github.com/AIDC-AI/Ovis-U1) | [Link](https://huggingface.co/AIDC-AI/Ovis-U1-3B) |
| [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832) | Anton Gusarov, Andrey Galichin, Li Pengyi, barracuda049, alexgambashidze | - This paper introduces a novel listener-augmented reinforcement learning framework for training vision-language models (VLMs) to reason about human visual preferences. - The key contribution is a listener-shaped reward mechanism that leverages an independent, frozen VLM to evaluate the reasoner's chain-of-thought, providing a dense and calibrated confidence score to shape the RL reward signal.  - Experimental results demonstrate that the proposed method achieves state-of-the-art accuracy on the ImageReward benchmark (67.4%) and significantly improves out-of-distribution performance on a large-scale human preference dataset. - The method also reduces reasoning contradictions compared to strong GRPO and SFT baselines, showcasing its effectiveness in aligning VLMs with nuanced human preferences. - The authors will release their reasoning model on Hugging Face. | ['Reinforcement Learning', 'Text-to-Image', 'Multimodal'] | N/A | [Link](https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner) |
| [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930) | LidongBing, Zhiqiang007, Jianyu |  - This paper introduces a novel prompt design paradigm for large language models (LLMs) that involves pruning random demonstrations to improve performance. - It challenges the conventional wisdom of using well-crafted instructions and demonstrations for in-context learning. - The proposed method, PROMPTQUINE, is an evolutionary search framework that automatically discovers effective pruning strategies. - Experiments show that PROMPTQUINE consistently outperforms state-of-the-art automatic prompt optimization techniques across various tasks and LLMs.  - The findings provide insights into the mechanisms of in-context learning and call for more open-ended search algorithms for improved LLM prompting. | ['Natural Language Processing'] | [Link](https://github.com/jianyu-cs/PromptQuine/) | N/A |
| [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in
  Inference-time Scaling?](https://arxiv.org/abs/2506.17417) | Kaizhuo Yan, Jize Jiang, Jingcheng Yang, Meitang Li, Mingyuan1997 | - This paper investigates whether inference-time scaling techniques, successful in LLMs, extend to VLMs, focusing on self-verification capabilities. - The study reveals that generation-based methods outperform verification-based methods in VLMs, indicating a deficiency in self-verification. - Experiments show that RL-trained VLMs do not significantly benefit from self-correction behaviors like "aha moments". - The authors found that the self-verification performance of VLMs is counterintuitively better without visual input, highlighting the models' limited use of visual information for verification. - This research emphasizes the need for improved self-verification capabilities in VLMs to fully leverage the potential of inference-time scaling. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500) | Ligeng Zhu, Junxian Guo, Xiuyu Li, zhijianliu, Skhaki | - This paper introduces SparseLoRA, a novel method that accelerates Large Language Model (LLM) fine-tuning through contextual sparsity. - SparseLoRA employs a lightweight, training-free Singular Value Decomposition (SVD) sparsity estimator to dynamically select a sparse subset of weights for computation. - Experimental results demonstrate that SparseLoRA reduces computational cost by up to 2.2x and achieves a speedup of up to 1.6x while maintaining accuracy across various downstream tasks. - The method is shown to outperform existing parameter-efficient fine-tuning methods such as LoRA and DoRA in terms of both speed and computational efficiency. - SparseLoRA incorporates techniques such as layer sensitivity analysis, token sensitivity analysis, and progressive sparse fine-tuning to further enhance its performance and efficiency. | ['Natural Language Processing'] | N/A | N/A |
| [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992) | Maria Brbić, Yekun Chai, mdmoor, yljblues | - This paper introduces MARBLE, a challenging multimodal spatial reasoning benchmark designed to evaluate the step-by-step reasoning abilities of Multimodal Language Models (MLLMs). - MARBLE consists of two tasks: M-PORTAL, which involves solving complex spatial reasoning and planning problems inspired by the game Portal 2, and M-CUBE, which requires assembling 3D cubes from six jigsaw pieces. - Current MLLMs perform poorly on MARBLE, achieving near-random performance on M-PORTAL and 0% accuracy on M-CUBE, highlighting the challenges of complex multimodal reasoning. - The benchmark emphasizes reasoning trajectories and plans, providing gold-standard rationales and mechanisms for evaluating intermediate step fidelity, unlike prior datasets that overemphasize final answer accuracy. - MARBLE aims to spur the development of the next generation of models with stronger capabilities in multi-step multimodal reasoning and planning. | ['Multimodal'] | [Link](https://marble-benchmark.github.io) | N/A |
| [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394) | s-emanuilov | - This paper introduces TUCAN, a novel methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. - The approach involves continued training of the BgGPT model series on a bilingual dataset of 10,035 function-calling examples. - TUCAN achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding. - The models, evaluation framework, and dataset are released open-source to enable replication for other languages. - This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems. | ['Natural Language Processing'] | [Link](https://github.com/llm-bg/Tucan-Eval), [Link](https://github.com/insait-institute/lm-evaluation-harness-bg) | [Link](https://huggingface.co/datasets/llm-bg/Tucan-BG-v1.0), [Link](https://huggingface.co/datasets/llm-bg/Tucan-BG-Eval-v1.0) |
| [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence
  with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219) | Yong Li, Yanxin Xi, Tianhui Liu, Shengyuan Wang, JJ-TMT |  - UrbanLLaVA is a novel multi-modal large language model designed for urban intelligence that leverages multiple modalities (e.g. images, geospatial data, text) to process various tasks.  - It introduces a new urban instruction dataset (UData) spanning different views (location, trajectory, global) of the urban environment and a multi-stage training framework to enhance spatial reasoning and domain knowledge.  - UrbanLLaVA outperforms existing open-source and proprietary models in single and cross-modal urban tasks across various cities, demonstrating its robustness and adaptability.  - The model achieves superior performance on an enhanced benchmark (UBench) composed of 12 tasks related to various urban data types, showcasing its capacity for complex urban understanding.  - The research contributes a systematic multi-view dataset, a novel training methodology, and enhanced evaluation benchmarks, thus advancing urban intelligence research. | ['Multimodal'] | [Link](https://github.com/tsinghua-fib-lab/UrbanLLaVA) | N/A |
| [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694) | Yifan Zao, Junyoung Park, Mukul Gagrani, Sudhanshu Agrawal, Raghavv Goel | - This paper introduces VOCABTRIM, a training-free technique to enhance the efficiency of drafter-based speculative decoding in large language models (LLMs). - VOCABTRIM improves speed by reducing the drafter's vocabulary size to only the most frequently used tokens, thus decreasing the inference overhead. - Experiments on Llama-3 models using Spec-Bench show a 16% memory-bound speed-up for Llama-3.2-3B-Instruct and a significant speed increase on other tasks with minor drops in block efficiency. - The method is compatible with existing SpD techniques, requiring minimal modifications. - VOCABTRIM offers a novel approach to optimizing the LM head of the drafter, a previously under-explored component in SpD research. | ['Text Generation'] | N/A | N/A |
| [ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language
  Models for Audio Generation and Editing](https://arxiv.org/abs/2506.21448) | Qian Chen, Wen Wang, Kaicheng Luo, Jialei Wang, Huadai Liu | - ThinkSound is a novel framework for audio generation and editing that leverages chain-of-thought (CoT) reasoning in multimodal large language models (MLLMs). - The model uses a three-stage process: foundational foley generation, interactive object-centric refinement, and targeted audio editing, each guided by CoT reasoning. - ThinkSound incorporates a unified audio foundation model based on flow matching, enabling high-fidelity audio synthesis from various input modalities. - The paper introduces AudioCoT, a new dataset with structured reasoning annotations to support the training of the model. - Experimental results demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation, outperforming existing baselines on both audio and CoT metrics. | ['Audio', 'Text-to-Audio', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Tower+: Bridging Generality and Translation Specialization in
  Multilingual LLMs](https://arxiv.org/abs/2506.17080) | Pedro Teixeirinha, João Alves, José Pombal, Nuno M. Guerreiro, RicardoRei |  - TOWER+, a new suite of multilingual LLMs, is introduced to address the trade-off between translation specialization and general-purpose capabilities. - The model achieves a Pareto frontier between translation specialization and multilingual general-purpose capabilities by using a novel training recipe comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. - TOWER+ models outperform larger general-purpose open-weight and proprietary LLMs (e.g., LLAMA 3.3 70B, GPT-40) on various benchmarks including translation and instruction following. - The findings highlight the possibility of rivaling frontier models in general capabilities while optimizing for specific business domains such as translation and localization. - IF-MT, a new benchmark evaluating both translation and instruction-following is introduced and the models are made available on Huggingface. | ['Translation', 'Text Generation'] | N/A | [Link](https://huggingface.co/infly/INF-ORM-Llama3.1-70B), [Link](Huggingface) |


## Papers for 2025-06-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing](https://arxiv.org/abs/2506.17450) | Sanghyun Woo, Saining Xie, Xuhui Jia, Ramin Mehran, cccjc | - BlenderFusion is a novel generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background using a layering-editing-compositing pipeline. - It extends a pre-trained diffusion model to process both original and edited scenes in parallel, fine-tuned on video frames with two key training strategies (source masking and simulated object jittering). - BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks, as shown through quantitative and qualitative evaluations on three video datasets. - The model integrates 3D-grounded editing capabilities of Blender with the strong synthesis abilities of diffusion models, enabling flexible, disentangled, and 3D-aware manipulation of objects, camera, and background. - The proposed framework leverages a layering-editing-compositing process that includes object-centric layering, Blender-guided editing, and generative compositing. | ['Image-to-3D', 'Image-to-Image', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://blenderfusion.github.io/) | N/A |
| [ShotBench: Expert-Level Cinematic Understanding in Vision-Language
  Models](https://arxiv.org/abs/2506.21356) | Yuhao Dong, Dian Zheng, Yi Jin, Jingwen He, Hongbo Liu | This paper introduces ShotBench, a benchmark dataset for evaluating vision-language models' (VLMs) understanding of cinematic language.  ShotBench comprises over 3.5k expert-annotated QA pairs from images and video clips across 200 films, spanning eight key cinematography dimensions.  Evaluation of 24 leading VLMs shows substantial limitations, with even the top-performing model achieving less than 60% accuracy.  To address this, a large-scale multimodal dataset called ShotQA (70k cinematic QA pairs) and ShotVL, a novel VLM significantly outperforming existing models on ShotBench are introduced. The authors make their models, data, and code open-source. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Computer Vision', 'Video Classification'] | [Link](https://vchitect.github.io/ShotBench-project/) | N/A |
| [MiCo: Multi-image Contrast for Reinforcement Visual Reasoning](https://arxiv.org/abs/2506.22434) | Xiaogang Xu, Xiaoyang Wu, Shaoteng Liu, Mingkang Zhu, Xi Chen | - This paper introduces MiCo, a novel method for multi-image visual reasoning that leverages inherent image constraints as supervision, reducing reliance on manual annotations. - MiCo constructs image triplets comprising two augmented views of the same image and a different but similar image, prompting the model to compare them and generate a reasoning process. - The model is trained using rule-based reinforcement learning, optimizing its ability to attend to subtle visual changes and perform logical reasoning. - Experiments demonstrate MiCo's effectiveness on multi-image reasoning benchmarks, achieving significant improvements without relying on human-annotated question-answer pairs. - The method also shows strong performance on general vision tasks, indicating the learned reasoning ability generalizes effectively to a wide range of questions. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656) | Xiaofeng Zhang, Xu Cao, Jingyuan Zhu, Yuanzhe Liu, Yifan Shen |  - This paper introduces SpatialReasoner-R1, a novel vision-language model designed for spatial reasoning that uses Long Chain-of-Thought (LongCoT) reasoning.  - The model employs a novel fine-grained Direct Preference Optimization (fDPO) method which applies differentiated learning updates tailored to two semantically distinct components: descriptive grounding and logical reasoning.  - A Multi-Model Monte Carlo Tree Search (M3CTS) method is proposed to generate high-quality supervision for spatial reasoning.  - SpatialReasoner-R1 achieves state-of-the-art performance on the SPATIALRGPT-BENCH benchmark, outperforming the strongest baseline by 9.8% in average accuracy.  - The fDPO method achieves an average improvement of 4.1% over standard DPO across spatial quality tasks and a 9.0% gain in spatial quantity tasks. | ['Multimodal', 'Visual Question Answering', 'Text2Text Generation'] | [Link](https://plan-lab.github.io/spatialreasoner/) | N/A |
| [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411) | Wei Guo, Xiaosong Li, MightyCrane, Fangcheng2, tangyehui | *- The paper introduces a novel Mixture of Grouped Experts (MoGE) model architecture designed to address expert load imbalance in large language models (LLMs). - MoGE groups experts and constrains tokens to activate an equal number of experts within each predefined group, improving the workload balance compared to conventional MoE. - The paper presents Pangu Pro MoE, a 72-billion parameter LLM based on MoGE, optimized for Ascend NPUs, achieving 1148 tokens/s per card inference speed. - Experiments show that Pangu Pro MoE outperforms comparable dense models (32B and 72B) and other open-source models (GLM-Z1-32B and Qwen3-32B) on various benchmarks. - The model's efficiency stems from a hierarchical parallelism strategy and optimized inference techniques for Ascend NPUs, including fused operators and a hierarchical hybrid parallelism strategy. | ['Natural Language Processing', 'Text Generation'] | [Link](https://gitcode.com/ascend-tribe/pangu-pro-moe) | N/A |
| [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with
  Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594) | Amr Fawzy, Mostafa Samy, Ahmed M. Adly | - Gazal-R1, a 32-billion parameter language model, achieves state-of-the-art performance in medical reasoning by utilizing a two-stage training pipeline. - The first stage involves supervised fine-tuning on a dataset of 107,033 synthetic medical reasoning examples, enhanced by parameter-efficient techniques like DoRA and rsLORA. - The second stage employs reinforcement learning with GRPO, refining accuracy and reasoning quality through a multi-component reward system. - Gazal-R1 surpasses larger models (up to 12 times larger) on medical benchmarks like MedQA (87.1%), MMLU Pro (Medical) (81.6%), and PubMedQA (79.6%). - The study provides insights into the challenges of training reasoning models in specialized domains, such as reward hacking and training instability. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/TachyHealth/Gazal-R1-32B-GRPO-preview), [Link](https://huggingface.co/datasets/TachyHealth/structured_medical), [Link](https://huggingface.co/datasets/TachyHealth/medical_grpo) |
| [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for
  Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330) | Yitao Duan, Jiachen Wang, Qiao Cheng, Na Cai, nomadlx | - This paper introduces Confucius3-Math, a 14B parameter open-source large language model designed for Chinese K-12 mathematics learning. - The model is lightweight, achieving state-of-the-art performance on various mathematical reasoning tasks while running efficiently on a single consumer-grade GPU. - Three novel techniques are introduced to improve the model's performance and training stability: Targeted Entropy Regularization, Recent Sample Recovery, and Policy-Specific Hardness Weighting. - The model outperforms many larger models on various benchmarks, demonstrating the feasibility of building high-performance reasoning models at low cost. - The model's code and weights are made publicly available on GitHub. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/netease-youdao/Confucius3-Math) | N/A |
| [RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation
  Models](https://arxiv.org/abs/2506.22149) | Hrvoje Bogunović, Ursula Schmidt-Erfurth, José Morano, Ronald Fecso | - RetFiner is a novel vision-language refinement scheme designed to enhance the representational capabilities of existing retinal foundation models (FMs). - The model architecture employs a Vision Transformer (ViT) vision encoder and a Transformer text encoder, utilizing cross-attention layers for multimodal encoding and generation. - RetFiner incorporates multiple training objectives (ITC, ITM, MLM, GM) to effectively leverage textual data for improving visual representations, leading to significant performance gains. - Experimental results on seven diverse OCT classification tasks demonstrate that RetFiner substantially improves the performance of three state-of-the-art retinal FMs (RETFound, UrFound, and VisionFM), achieving average gains of up to 5.8 percentage points in balanced accuracy. - The method is efficient, requiring less than ten epochs to refine a model and exhibits strong adaptability to specific populations, making it a practical solution for adapting retinal FMs to various applications and datasets. | ['Multimodal', 'Image Classification', 'Image Feature Extraction'] | [Link](https://github.com/ronnief1/RetFiner) | N/A |


## Papers for 2025-06-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670) | Bo You, Yiding Liu, Wei Li, Zihao Deng, kimingng |  - This paper introduces MMSearch-R1, a novel reinforcement learning framework that enables large multimodal models (LMMs) to perform on-demand, multi-turn search in real-world internet environments. - MMSearch-R1 integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. - The model outperforms RAG-based baselines of the same model size, and matches the performance of larger RAG-based models while reducing search calls by over 30% on various VQA tasks. -  A multimodal search VQA dataset was created with a semi-automated pipeline to support training. - The framework was shown to improve models' ability to recognize the boundaries of their knowledge and perform on-demand search, which enhanced their ability to utilize internal knowledge. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1) | N/A |
| [Where to find Grokking in LLM Pretraining? Monitor
  Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551) | Ziyue Li, zhoutianyi, Fcr09 | - This paper introduces two novel metrics for monitoring generalization during LLM pretraining without relying on test sets or finetuning.  - The study verifies that grokking, the phenomenon where test performance improves long after training loss converges, also occurs during the pretraining of large-scale LLMs.  - It is shown that, unlike previous findings, grokking in LLMs is local and asynchronous across different data domains.  - The proposed metrics quantify the complexity and similarity of routing pathways within a Mixture-of-Experts (MoE) LLM, providing mechanistic insights into the memorization-to-generalization transition.  - The findings are grounded in a theoretical analysis showing that more structured pathways reduce model complexity and improve the generalization bound. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SAM4D: Segment Anything in Camera and LiDAR Streams](https://arxiv.org/abs/2506.21547) | Sheng Yang, Chunyong Hu, Ziqian Ni, Jianyun Xu, songw-zju | - SAM4D is a novel multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. - It utilizes Unified Multi-modal Positional Encoding (UMPE) to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. - Motion-aware Cross-modal Memory Attention (MCMA) enhances temporal consistency and long-horizon feature retrieval. - SAM4D is trained on the constructed Waymo-4DSeg dataset, which contains over 300k camera-LiDAR associated masklets. - Extensive experiments demonstrate SAM4D's superior performance and generalizability in promptable multi-modal segmentation. | ['Image Segmentation', 'Multimodal'] | [Link](https://github.com/open-mmlab/mmsegmentation) | N/A |
| [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552) | Trevor Darrell, Yann LeCun, Amir Bar, dans123, Emma02 | - This paper introduces PEVA, a novel model for predicting egocentric videos conditioned on detailed 3D human motion. - The model architecture is based on an autoregressive conditional diffusion transformer, which leverages a structured action representation that captures both global body dynamics and fine-grained joint articulations. - PEVA is trained on the Nymeria dataset, a large-scale dataset of real-world egocentric video and body pose capture, and includes random timeskips to handle delayed visual consequences of actions. - The authors conduct a comprehensive evaluation demonstrating that PEVA outperforms baseline methods in terms of video quality, semantic consistency, and action control abilities. - The results showcase PEVA's ability to predict long-term visual consequences and even perform planning tasks by simulating action candidates and selecting the one with highest perceptual similarity to the goal. | ['Image-to-Video', 'Video Classification', 'Multimodal'] | N/A | N/A |
| [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655) | Adil Hafeez, Co Tran, nehcgs, parachas | - This paper introduces Arch-Router, a 1.5B parameter language model for preference-aligned LLM routing. - Arch-Router maps queries to user-defined domains or action types to guide model selection, aligning with subjective human preferences. - The framework allows for adding new models without retraining or architectural changes, enhancing flexibility and adaptability. - Experiments show that Arch-Router outperforms existing top proprietary LLMs by 7.71% on average in matching queries with human preferences. - The approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/katanemo/Arch-Router-1.5B) |
| [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430) | Pengcheng Qiu, Xiaoman Zhang, Yanjie Fan, Chaoyi Wu, Weike Zhao | DeepRare is an agentic system for rare disease diagnosis that uses a large language model (LLM) to process heterogeneous clinical inputs, including free text, structured data, and genomic data.  It generates ranked diagnostic hypotheses with traceable reasoning. DeepRare outperforms 15 other methods in HPO-based evaluations, achieving a Recall@1 score of 57.18%.  In multi-modal input scenarios, it achieves 70.6% Recall@1 compared to Exomiser's 53.2%. Clinical experts verified the system's reasoning chains with 95.4% agreement, indicating high reliability.  The system has been implemented as a user-friendly web application. | ['Natural Language Processing', 'Multimodal', 'Question Answering', 'Zero-Shot Classification'] | [Link](None) | [Link](https://huggingface.co/datasets/Angelakeke/DeepRare) |
| [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103) | Laurence Aitchison, tim-lawson | - This paper introduces a novel Transformer architecture that dynamically skips a variable number of middle layers based on a learned gating mechanism.  - The architecture aims to improve efficiency by reducing computation for simpler tokens and potentially fostering a multi-level representational hierarchy. - A gated attention mechanism prevents tokens from attending to skipped positions, and adaptive regularization is used to control gate sparsity. - The proposed approach does not demonstrate improvements in validation cross-entropy and estimated FLOPs compared to dense baselines at the scales investigated. - Future research directions include scaling the model to potentially realize the benefits of the architecture. | ['Natural Language Processing'] | [Link](https://github.com/tim-lawson/skip-middle) | N/A |
| [MuseControlLite: Multifunctional Music Generation with Lightweight
  Conditioners](https://arxiv.org/abs/2506.18729) | Bo-Rui Chen, Sheng-Ping Yang, Weijaw Lee, Shih-Lun Wu, fundwotsai2001 | - MuseControlLite is a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. - It utilizes positional embeddings in the decoupled cross-attention layers, which increases control accuracy while requiring fewer trainable parameters compared to existing methods. - The model demonstrates improved controllability over MusicGen-Large and Stable Audio Open ControlNet in musical attribute control, audio inpainting, and audio outpainting tasks. - Experiments show that MuseControlLite achieves superior performance in melody control, with a 4.5% improvement in melody accuracy compared to existing ControlNet-based approaches. - The model is efficient, using only 85M trainable parameters and demonstrating that positional embeddings are critical for time-varying conditions. | ['Audio-to-Audio', 'Text-to-Audio'] | [Link](https://MuseControlLite.github.io/web/) | N/A |


## Papers for 2025-06-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image
  Generation](https://arxiv.org/abs/2506.18095) | Ke Ji, Shunian Chen, Zhenyang Cai, Junying Chen, cppppppc | - This paper introduces ShareGPT-40-Image, the first publicly available dataset containing 45K text-to-image and 46K image-to-image pairs generated using GPT-40's image generation capabilities. - Leveraging this dataset, a new multimodal large language model called Janus-40 is developed, demonstrating significant improvements in text-to-image and text-and-image-to-image generation. - Janus-40 achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8xA800 GPU machine. - The experiments show that Janus-40 outperforms other open-source models on several benchmarks, including improvements in image quality and instruction following. - This work contributes to the democratization of advanced image generation techniques by providing a large-scale, high-quality dataset and a powerful open-source model. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/FreedomIntelligence/ShareGPT-40-Image) | N/A |
| [Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large
  Language Models](https://arxiv.org/abs/2506.19697) | Jaewoo Kang, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, affjljoo3581 |  - This paper introduces Outlier-Safe Pre-training (OSP), a novel method for preventing outlier formation during the pre-training of large language models (LLMs).  - OSP combines three key innovations: the Muon optimizer, Single-Scale RMSNorm, and a learnable embedding projection.   -  The proposed method achieves a 35.7 average score across 10 benchmarks under aggressive 4-bit quantization, significantly outperforming an Adam-trained model (26.5).   -  OSP models exhibit near-zero excess kurtosis (0.04), compared to extreme values (1818.56) in standard models, demonstrating a fundamental alteration of LLM quantization behavior.   - The study trained a 1.4B-parameter model on 1 trillion tokens without outliers, showcasing the scalability and effectiveness of the proposed method. | ['Natural Language Processing'] | [Link](https://github.com/dmis-lab/Outlier-Safe-Pre-Training) | N/A |
| [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315) | Jing Shao, Zhe Zhang, Lehan He, lsheng2024, zx55 | - The paper introduces Property-Generated Solver (PGS), a novel framework that leverages Property-Based Testing (PBT) to validate LLM-generated code. - PGS uses two LLM-based agents: a Generator for code generation and refinement, and a Tester for managing the PBT lifecycle and providing feedback. - Experimental results on multiple code generation benchmarks demonstrate that PGS achieves substantial improvements (23.1% to 37.3% relative gains) over established TDD methods. - The framework addresses the limitations of traditional TDD approaches by focusing on high-level program properties instead of relying on specific input-output examples. - PGS provides a robust mechanism for steering LLMs towards more correct and generalizable code by effectively decoupling code generation from its validation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Is There a Case for Conversation Optimized Tokenizers in Large Language
  Models?](https://arxiv.org/abs/2506.18674) | Pedro Reviriego, Gonzalo Martínez, Javier Conde, Raquel Ferrando | - This paper investigates the potential benefits of optimizing tokenizers specifically for conversational applications in large language models (LLMs). - The authors retrain several popular tokenizers using a chatbot conversation dataset and compare their performance to the original tokenizers on both the conversation dataset and the original LLM training dataset. - Results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, leading to energy savings in the range of 5% to 10%, with minimal or positive impact on tokenization efficiency for the training corpus. - Although further research is needed to confirm the results and evaluate the impact on training costs, the findings suggest that customizing tokenizers for conversation can improve LLM efficiency. - The study highlights the importance of considering tokenization efficiency, often overlooked, in optimizing LLMs for real-world applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/RaquelFerrando/conversational_tokenizers.git) | N/A |
| [When Life Gives You Samples: The Benefits of Scaling up Inference
  Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544) | Sara Hooker, Julia Kreutzer, Ye Shen, Daniel D'souza, ammar-cohere | This paper introduces novel sampling and selection strategies for improving the inference-time compute of multilingual LLMs. The proposed methods significantly improve performance across multiple languages and tasks, particularly in underrepresented languages. Compared to existing methods, the proposed approach yields notable gains in win-rates on various benchmarks, even against strong commercial models. These strategies involve adapting sampling and selection to account for diversity in domains and languages, leading to an average +6.8 jump in win-rates for 8B models and +9.0 for 111B models.  The researchers also provide a detailed experimental setup and extensive analysis of existing methods, highlighting the need for language- and task-aware approaches. Overall, the findings underscore the potential for democratizing performance improvements across various languages and tasks through efficient inference-time compute strategies. | ['Natural Language Processing'] | N/A | N/A |
| [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403) | Carlos C. N. Kuhn, adnaan525 | - This paper introduces the Debugging Decay Index (DDI), a novel metric to quantify the effectiveness of debugging in large language models (LLMs) for code generation. - The DDI is based on an exponential decay model that captures the diminishing returns of successive debugging attempts. - The authors propose a "fresh start" strategy to mitigate debugging decay by strategically restarting the code generation process when the effectiveness drops below a certain threshold. - Empirical results across multiple models demonstrate that this fresh start strategy can improve overall code generation accuracy compared to continuing debugging. - The DDI framework offers a multi-dimensional evaluation of debugging effectiveness, providing insights into initial performance, decay rate, optimal intervention points, and model fit quality. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining
  and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331) | Eric de la Clergerie, Nathan Godey, rntc | - This paper introduces Biomed-Enriched, a biomedical dataset created through a two-stage annotation process using LLMs. - The dataset contains 2 million clinical case paragraphs, including 450K high-quality ones from articles with commercial-use licenses, addressing the scarcity of publicly available clinical text. - Experiments show that strategically combining quality filtering and domain upsampling significantly improves data efficiency and targeted model performance. - The curated subsets in Biomed-Enriched enabled targeted improvements in continual pretraining experiments, with clinical upsampling boosting performance by 5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by 1%. - The results demonstrate the potential for more efficient and effective biomedical pretraining strategies. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility
  Applications](https://arxiv.org/abs/2506.19502) | Paul Laban, Matt Laing, AleksandrAlgazinov | - The paper introduces MATE, a multimodal accessibility MAS that performs modality conversions based on user needs, outperforming other LLMs and statistical models in experiments. - MATE supports multiple model types, from LLM API calling to custom ML classifiers, and runs locally to ensure privacy and security. - It includes ModCon-Task-Identifier, a model that extracts modality conversion tasks from user input, showing consistent improvement over other LLMs and models. - The system is adaptable to various needs and compatible with a wide range of hardware, designed to assist individuals with disabilities in interacting with digital environments. - The code and data for MATE are publicly available, promoting further research and development in the field of AI for accessibility. | ['Multimodal', 'Any-to-Any', 'Text-to-Speech', 'Text-to-Image', 'Image-to-Text', 'Image-to-Video', 'Text-to-Audio', 'Image-to-3D', 'Text-to-Video', 'Audio-to-Audio', 'Automatic Speech Recognition', 'Natural Language Processing', 'Text Classification'] | [Link](https://github.com/AlgazinovAleksandr/Multi-Agent-MATE) | N/A |


## Papers for 2025-06-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion
  Models](https://arxiv.org/abs/2506.19851) | lsheng2024, pookiefoof, Yang-Tian, fenghora, huanngzh | - AnimaX is a novel feed-forward 3D animation framework that leverages video diffusion models and skeleton-based animation for generating high-quality animations from articulated 3D meshes and textual descriptions. - The model architecture employs a joint video-pose diffusion model, which processes multi-view 2D pose maps and video frames simultaneously to effectively transfer motion knowledge from the video domain to the 3D animation domain. - It introduces shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between the video and pose streams. - AnimaX achieves state-of-the-art results on VBench, outperforming existing methods in generalization, motion fidelity, and efficiency, as evidenced by quantitative comparisons and user studies. - The framework supports diverse articulated meshes with arbitrary skeletons, demonstrating significant improvements in generating high-quality animations compared to prior work that primarily focuses on fixed skeletal topologies or relies on costly optimization. | ['Text-to-3D', 'Image-to-3D', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://anima-x.github.io/) | N/A |
| [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal
  Reasoning](https://arxiv.org/abs/2506.16141) | Junhao Cheng, Yixiao Ge, Rui Wang, Yuying Ge, Yi Chen | - This paper introduces GRPO-CARE, a novel consistency-aware reinforcement learning framework that enhances the reasoning capabilities of multimodal large language models (MLLMs). - GRPO-CARE addresses the limitation of standard outcome-supervised methods by jointly optimizing for both answer correctness and reasoning coherence, improving interpretability and generalization. - The proposed method introduces a two-tiered reward mechanism: a base reward for accuracy and a consistency bonus based on the likelihood that a reference model reproduces the same answer given the reasoning trace. - Experiments on SEED-Bench-R1, a new benchmark specifically designed to evaluate post-training methods for MLLMs, demonstrate that GRPO-CARE consistently outperforms standard GRPO, achieving a 6.7% performance gain on the most challenging evaluation level and a 24.5% improvement in consistency rates. - The effectiveness of GRPO-CARE is further validated by its strong transferability across diverse video understanding benchmarks. | ['Video-Text-to-Text', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/TencentARC/GRPO-CARE) | N/A |
| [Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in
  LLMs](https://arxiv.org/abs/2506.19290) | Changshi Li, Yuzhen Xiao, chrisliu298, lycfight, zengliangcs | - This paper introduces Skywork-SWE, a new dataset for software engineering tasks containing 10,169 real-world Python instances from 2,531 distinct GitHub repositories. - The dataset includes natural language descriptions, code, and validated unit tests, addressing limitations of existing datasets. - They fine-tune a 32B parameter LLM, achieving state-of-the-art performance on the SWE-bench Verified benchmark (38% pass@1 accuracy without verifiers). - The authors empirically demonstrate a data scaling law for software engineering in LLMs, showing consistent performance improvements with increased training data size. - They release their model checkpoint and dataset to facilitate future research. | ['Natural Language Processing', 'Text2Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/SkyworkAI/Skywork-SWE) | [Link](https://huggingface.co/Skywork/Skywork-SWE-32B) |
| [SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in
  Real-World Applications](https://arxiv.org/abs/2506.18951) | Per Jacobsson, Ge Qu, Jinyang Li, Tebmer, xia01ongLi | - This paper introduces BIRD-CRITIC, a new benchmark for SQL issue debugging, comprising 530 PostgreSQL tasks and 570 multi-dialect tasks distilled from real user issues. - It presents SIX-GYM, a training environment leveraging the SQL-Rewind strategy to automatically generate executable issue-solution datasets. - The paper proposes f-Plan Boosting, a method to extract high-level debugging plans from SQL solutions to improve training. - BIRD-FIXER, an open-source agent based on Qwen-2.5-Coder-14B, achieves a success rate of 38.11% on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-MULTI, surpassing many leading proprietary models. - The results demonstrate the potential of open-source models for SQL issue debugging and highlight the challenges of the task. | ['Natural Language Processing'] | [Link](https://bird-critic.github.io/) | N/A |
| [JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo
  Retouching Agent](https://arxiv.org/abs/2506.17612) | Panwang Pan, Jinbin Bai, Kunjie Lin, Zixu Lin, LYL1015 |  - This paper introduces JarvisArt, a novel intelligent photo retouching agent that leverages a multi-modal large language model (MLLM) to understand user intent and coordinate over 200 retouching tools within Adobe Lightroom. - JarvisArt undergoes a two-stage training process: Chain-of-Thought supervised fine-tuning and Group Relative Policy Optimization for Retouching (GRPO-R). - The proposed Agent-to-Lightroom Protocol facilitates seamless integration between JarvisArt and Lightroom. - Experiments on the MMArt-Bench, a novel benchmark, show that JarvisArt outperforms GPT-4 by 60% on average pixel-level metrics while maintaining comparable instruction-following capabilities. - User preference studies demonstrate that JarvisArt achieves superior performance in ease of use, consistency, efficiency, and overall satisfaction compared to existing methods. | ['Image-to-Image', 'Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/abs/2506.14012) | Michalis Vazirgiannis, Yang Zhang, guokan-shang, amr-mohamed | - This paper introduces a novel evaluation framework for assessing the comprehension capabilities of large language models (LLMs) when processing code-switched (CSW) text. - The framework uses a multi-step pipeline to generate linguistically grounded CSW variants of established benchmarks. - Experiments reveal that code-switching has a nuanced effect on LLM comprehension, with degradation evident when foreign tokens disrupt English text. - Fine-tuning LLMs on code-switched data provides a more stable path to mitigating comprehension degradation compared to prompt-based methods. - The findings highlight the importance of developing more robust and adaptable LLMs to effectively process the increasingly prevalent mixed-language data found online. | ['Natural Language Processing'] | [Link](https://github.com/amr-mohamedd/Lost-in-the-Mix.git) | N/A |
| [Can Large Language Models Capture Human Annotator Disagreements?](https://arxiv.org/abs/2506.19467) | Alexander Hoyle, Donya Rooein, Vilém Zouhar, Yu Fan, JingweiNi | This research paper's main contribution is a comprehensive evaluation of LLMs' capacity to predict human annotator disagreements without access to repeated human labels.  The findings reveal that LLMs struggle with this task, a limitation often overlooked by majority label-based evaluations.  Interestingly, RLVR-style reasoning, which generally improves LLM performance, degrades its performance in disagreement prediction.  The authors highlight the need for improved methods for evaluating and enhancing LLMs' capabilities in this area.  The study uses various datasets and LLM architectures, examining both verbalized and sampling-based distribution expression methods. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/EdisonNi-hku/Disagreement_Prediction) | N/A |


## Papers for 2025-06-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871) | yzwang, sienna223, Shitao, Ruiran, wcyno23 | - OmniGen2 is a versatile, open-source multimodal generative model designed for diverse generation tasks including text-to-image, image editing, and in-context generation. - It features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer, enabling it to leverage existing multimodal understanding models without needing to re-adapt VAE inputs. - OmniGen2 introduces a reflection mechanism for image generation, and a new benchmark named OmniContext to evaluate in-context generation, achieving state-of-the-art performance among open-source models in terms of consistency. - The model is trained using a comprehensive dataset created through data construction pipelines encompassing image editing and in-context generation data. - The authors release the model, training code, datasets, and data construction pipeline to support future research. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Text'] | [Link](https://github.com/VectorSpaceLab/OmniGen2) | [Link](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), [Link](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) |
| [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement
  Learning](https://arxiv.org/abs/2506.18841) | Juanzi Li, Roy Ka-Wei Lee, Yushi Bai, Yuhao Wu, Zhiqiang007 | - LongWriter-Zero is a novel approach for ultra-long text generation in LLMs that leverages reinforcement learning (RL) without relying on synthetic data. - It employs specialized reward models to guide the LLM towards improved length control, writing quality, and structural formatting. - LongWriter-Zero, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods and achieves state-of-the-art results on WritingBench and Arena-Write. - The model surpasses even 100B+ models like DeepSeek R1 and Qwen3-235B. - The data and model checkpoints are open-sourced. | ['Reinforcement Learning', 'Text Generation'] | N/A | [Link](https://huggingface.co/THU-KEG/) |
| [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought
  Reasoning in LLMs](https://arxiv.org/abs/2506.18896) | Ke Shen, Jiahao Qiu, Jingwen Gu, Ling Yang, Jiaru Zou |  - ReasonFlux-PRM is a novel trajectory-aware Process Reward Model (PRM) designed to enhance long chain-of-thought reasoning in LLMs by incorporating both step-level and trajectory-level supervision.  - It addresses the limitations of existing PRMs, which primarily focus on final responses, by explicitly evaluating intermediate reasoning steps.  - ReasonFlux-PRM demonstrates consistent performance improvements across various downstream benchmarks (AIME, MATH500, GPQA-Diamond), surpassing existing PRMs and human-curated baselines in supervised fine-tuning, reinforcement learning, and test-time scaling.  - The model's architecture involves a joint training objective that balances step-level and trajectory-level rewards, thereby enabling effective supervision of trajectory-response data.  - The paper also explores offline data selection and online reward modeling applications of ReasonFlux-PRM, showcasing its versatility in diverse reasoning scenarios. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/Gen-Verse/ReasonFlux) | N/A |
| [Vision as a Dialect: Unifying Visual Understanding and Generation via
  Text-Aligned Representations](https://arxiv.org/abs/2506.18898) | Qi Zhao, Yang Zhao, Hao Chen, hywang66, csuhan | - This paper introduces Tar, a multimodal large language model (MLLM) that unifies visual understanding and generation using a shared discrete semantic representation. - The core of Tar is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from an LLM vocabulary. - Tar utilizes two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model, to handle diverse decoding needs and achieve high-fidelity visual outputs. - Experiments demonstrate that Tar matches or surpasses existing multimodal LLM methods in terms of speed and training efficiency across benchmarks for both visual understanding and generation tasks. - The authors also investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation capabilities. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image'] | [Link](https://tar.csuhan.com) | N/A |
| [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741) | Yeyi Guan, Heyuan Huang, He Zhu, kangz, tianyue818 | - This paper introduces OAgents, a modular open-source framework for building effective language agents. - OAgents achieves state-of-the-art performance on the GAIA benchmark, outperforming existing open-source and closed-source methods. - The authors conduct a systematic empirical study on popular design choices in agent components, revealing which designs are crucial for effective agents. - OAgents includes a modular design for various components, enabling future research in agentic AI. - The framework offers a more robust evaluation protocol to stabilize comparisons and improve reproducibility. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/OPPO-PersonalAI/OAgents) | N/A |
| [LettinGo: Explore User Profile Generation for Recommendation System](https://arxiv.org/abs/2506.18309) | Jianfeng Liu, Pu Zhao, Fangkai Yang, Di Zhang, Lu Wang | - This paper introduces LETTINGO, a novel framework for generating diverse and adaptive user profiles for recommendation systems, which significantly enhances recommendation accuracy, flexibility, and contextual awareness. - LETTINGO leverages the expressive power of LLMs and incorporates direct feedback from downstream recommendation tasks, avoiding the rigid constraints of supervised fine-tuning. - It operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. - Experimental results demonstrate that LETTINGO significantly outperforms traditional embedding-based profiles and other state-of-the-art methods. - The framework's flexibility allows it to capture the full diversity of user behaviors and adapt to various downstream tasks and recommendation needs. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Reinforcement Learning', 'Summarization', 'Feature Extraction', 'Other'] | N/A | N/A |
| [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123) | Potsawee Manakul, Panop Pitchayarthorn, Warit Sirichotedumrong, pittawat, natnitaract | - FinCoT is a novel prompting framework that integrates expert financial workflows into structured chain-of-thought prompting to improve the accuracy of large language models on financial reasoning tasks. - FinCoT significantly improves performance over standard prompting methods and even outperforms a domain-specific fine-tuned model (Fin-R1) on various financial reasoning benchmarks. - The framework incorporates Mermaid blueprints that encode expert financial workflows and enhances interpretability of LLM reasoning. - FinCoT achieves accuracy improvements of up to +17.3 percentage points and reduces the number of generated tokens by 8 times compared to other prompting techniques. - The study presents a comprehensive investigation of prompting styles in FinNLP and releases nine blueprint templates for various financial domains. | ['Question Answering', 'Zero-Shot Classification'] | N/A | [Link](https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1) |
| [Auto-Regressively Generating Multi-View Consistent Images](https://arxiv.org/abs/2506.18527) | Chen Zhao, Jinbo Wu, Jialun Liu, Yuxiao Yang, JiaKui Hu | - The paper introduces a novel Multi-View Auto-Regressive (MV-AR) model for generating multi-view consistent images from various prompts (text, images, shapes). - The model utilizes an autoregressive approach, progressively generating each view conditioned on previously generated views, enhancing consistency across different viewpoints. - A unified model architecture is designed to handle multiple input modalities simultaneously. - To mitigate the issue of limited high-quality training data, a "Shuffle View" data augmentation technique is proposed. - Experimental results demonstrate that MV-AR achieves comparable or better performance compared to leading diffusion-based methods in terms of image quality and consistency. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://github.com/MILab-PKU/MVAR) | N/A |
| [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming
  and Distillation](https://arxiv.org/abs/2506.18349) | Young Jin Kim, Ilgee Hong, Zixuan Zhang, Chen Liang, Pearush | - This paper introduces SlimMoE, a novel multi-stage compression framework designed to efficiently compress large Mixture-of-Experts (MoE) language models. - SlimMoE systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, mitigating the performance degradation often associated with one-shot pruning. - The framework compresses Phi-3.5-MoE (41.9B total / 6.6B activated parameters) into two smaller models: Phi-mini-MoE (7.6B total / 2.4B activated) and Phi-tiny-MoE (3.8B total / 1.1B activated) using less than 10% of the original training data. - The compressed models demonstrate strong performance, exceeding or matching the capabilities of models with similar sizes and remaining competitive with larger models. - The authors release their models on HuggingFace for broader adoption of MoE architectures across computational environments. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/microsoft/Phi-mini-MoE-instruct), [Link](https://huggingface.co/microsoft/Phi-tiny-MoE-instruct) |
| [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962) | Wenjie Li, Yujie Zhang, Wenjie Lou, Yankai Jiang, manglu3935 |  - This paper introduces Mentor-Intern Collaborative Search (MICS), a novel method for generating high-quality chain-of-thought (CoT) data for medical visual question answering.  - MICS uses mentor models to initialize reasoning paths and intern models to evaluate and refine them, selecting optimal paths based on an MICS-score.  - The paper also introduces MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-01, a new medical MLLM trained using MICS and curriculum learning.  - Experiments show that Chiron-01 achieves state-of-the-art performance on various medical visual question answering and reasoning benchmarks.  - The code for the model will be available at the provided GitHub repository. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/manglu097/Chiron-o1) | N/A |
| [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879) | Tianle Cai, Talha Chafekar, Muhammad Yusuf Hassan, Yang Zhang, Junyan Li | - CommVQ, a novel method for compressing Key-Value (KV) caches in large language models (LLMs), is proposed to address the memory bottleneck caused by long context lengths. - CommVQ employs additive vector quantization with a lightweight encoder and codebook that is commutative with Rotary Position Embedding (RoPE), allowing efficient integration into the self-attention mechanism. - Experiments on LongBench, InfiniteBench, and GSM8K demonstrate that CommVQ reduces FP16 KV cache size by 87.5% with 2-bit quantization and enables 1-bit quantization with minimal accuracy loss. - The proposed method outperforms state-of-the-art KV cache quantization methods and allows a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. - CommVQ achieves superior trade-offs between memory savings and accuracy by combining additive quantization and RoPE-commutative codebook. | ['Natural Language Processing'] | [Link](https://github.com/UMass-Embodied-AGI/CommVQ) | N/A |
| [FaithfulSAE: Towards Capturing Faithful Features with Sparse
  Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673) | Andrew Bermingham, Luis Eduardo Rodrigues Vieira, Donghyun Lee, Harryn Oh, seonglae | - This paper introduces FaithfulSAE, a novel method for training Sparse Autoencoders (SAEs) that avoids the instability issues associated with using external datasets. - The proposed method trains SAEs on a synthetic dataset generated by the model itself, eliminating the reliance on external datasets which can introduce out-of-distribution (OOD) data. - Experimental results show that FaithfulSAEs outperform SAEs trained on web-based datasets in terms of stability across different initialization seeds and exhibit a lower Fake Feature Ratio. - The findings support the hypothesis that using OOD data in SAE training is the primary cause of their instability and that training on model-internal data leads to more stable and faithful feature representations. - This method improves the faithfulness and stability of SAEs, advancing the interpretability of large language models. | ['Natural Language Processing', 'Feature Extraction'] | [Link](https://github.com/seonglae/FaithfulSAE) | [Link](https://huggingface.co/collections/seonglae/faithful-saes-67f3b25ff21a185017879b33), [Link](https://huggingface.co/collections/seonglae/faithful-dataset-67f3b21ff8fca56b87e5370f) |
| [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code
  Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323) | Bertalan Borsos, Nils Gruschka, Richard A. Dubniczky, Tamas Bisztray, Neo111x | - This paper introduces CodeT5-Authorship, a novel model for LLM authorship attribution of C programs, which uses only the encoder layers from the original CodeT5 model. - The model achieves high accuracy (97.56% in binary classification and 95.40% in multi-class attribution) in distinguishing between C programs generated by different LLMs. - A new benchmark dataset, LLM-AuthorBench, containing 32,000 compilable C programs generated by eight state-of-the-art LLMs, is introduced to evaluate the proposed model and other methods. - The CodeT5-Authorship model outperforms several traditional ML classifiers and other transformer models in the authorship attribution task, demonstrating the effectiveness of the proposed approach. - The CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant Google Colab scripts are publicly available on GitHub. | ['Text Classification'] | [Link](https://github.com/LLMauthorbench/) | N/A |
| [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597) | Daoyuan Wu, Zongjie Li, Wenxuan Wang, Zhenlan Ji, Xunguang Wang | This paper presents the first comprehensive taxonomy for categorizing jailbreak guardrails for LLMs along six key dimensions.  A novel Security-Efficiency-Utility evaluation framework is introduced to assess guardrail effectiveness.  Extensive analysis and experiments identify the strengths and limitations of existing guardrails and provide insights for optimizing defense combinations.  A leaderboard is presented, ranking guardrails based on their performance across various metrics.  The findings guide the principled advancement and deployment of robust LLM guardrails. | ['Natural Language Processing'] | [Link](https://github.com/xunguangwang/SoK4JailbreakGuardrails) | N/A |


## Papers for 2025-06-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406) | Xuanlei Zhao, Yuhao Zhou, Dongwen Tang, Zhiyuan Liang, VictorKai1996NUS | - This paper introduces Drag-and-Drop LLMs (DnD), a novel prompt-conditioned parameter generator that eliminates the need for per-task training in large language models (LLMs). - DnD maps unlabeled task prompts directly to LoRA weight updates, achieving task-specific parameter generation in seconds, significantly reducing adaptation overhead compared to traditional methods. - The model architecture consists of a lightweight text encoder that distills prompt batches into embeddings, and a cascaded hyper-convolutional decoder that transforms these embeddings into LoRA matrices. - Experiments demonstrate that DnD achieves up to 12,000× lower overhead and average performance gains of up to 30% over the strongest training LoRAs on unseen datasets across various benchmarks. - DnD shows robust cross-domain generalization, highlighting the potential of prompt-conditioned parameter generation as a viable alternative to gradient-based adaptation for rapidly specializing LLMs. | ['Natural Language Processing', 'Text Generation', 'Zero-Shot Classification'] | [Link](https://jerryliang24.github.io/DnD) | N/A |
| [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal
  Document Understanding](https://arxiv.org/abs/2506.16035) | Biddwan Ahmed, Indraneel Das, Tanmay Odapally, udayallu, vishesh-t27 | - This paper introduces a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents, enhancing Retrieval Augmented Generation (RAG) systems. - The approach addresses limitations of traditional text-based methods by preserving semantic coherence and structural integrity across page boundaries, even when handling complex layouts, tables, figures, and visual elements. - It processes documents in configurable page batches with cross-batch context preservation, improving the accuracy of downstream RAG performance and achieving better quantitative results compared to traditional RAG systems. - The method's effectiveness is demonstrated on an internal benchmark dataset of diverse PDF documents, showing better preservation of document structure and semantic coherence. - The paper also contributes a new benchmark dataset and a detailed analysis of chunk quality, addressing limitations of traditional chunking approaches. | ['Document Question Answering', 'Multimodal'] | N/A | N/A |
| [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement
  Learning](https://arxiv.org/abs/2506.09049) | Jie Yang, Yiran Qin, Heng Zhou, Xiufeng Song, FACEONG | - This paper introduces VIKI-Bench, a hierarchical benchmark designed for evaluating visual reasoning in embodied multi-agent cooperation, and VIKI-R, a two-stage framework that leverages vision-language models for this task.  - VIKI-Bench incorporates three levels of visual reasoning tasks: agent activation, task planning, and trajectory perception, using diverse robot embodiments and multi-view observations.  - VIKI-R utilizes a two-stage approach: first, supervised fine-tuning with Chain-of-Thought annotations; then, reinforcement learning with hierarchical reward signals.  - Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels, showing the effectiveness of the proposed approach.  - The work highlights the emergence of compositional cooperation patterns among heterogeneous agents, which is facilitated by reinforcement learning. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://faceong.github.io/VIKI-R/) | N/A |
| [UniFork: Exploring Modality Alignment for Unified Multimodal
  Understanding and Generation](https://arxiv.org/abs/2506.17202) | Xizhou Zhu, Hao Li, Lirui Zhao, Quanfeng Lu, Teng Li | - UniFork is a novel Y-shaped architecture for unified image understanding and generation that addresses the limitations of fully shared Transformer backbones. - The model consists of shared shallow layers for cross-task representation learning and task-specific branches in deeper layers to avoid interference. - UniFork consistently outperforms conventional fully shared Transformer architectures and achieves performance on par with or better than task-specific models on various benchmarks. - Extensive ablation studies demonstrate the effectiveness of the proposed architecture, highlighting the importance of balancing shared learning and task specialization. - The analysis of modality alignment patterns reveals that understanding tasks benefit from progressively increasing alignment across network depth, while generation tasks require a rise-then-fall pattern. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://github.com/tliby/UniFork) | N/A |
| [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925) | Kathleen McKeown, Nicholas Deas, narutatsuri |  - This paper introduces a novel reranking-based generation method for creating unbiased perspective summaries.  - The method is evaluated using a new test set designed for evaluating perspective summary quality and newly defined metrics, showing that reranking significantly outperforms zero-shot inference and prompting-based techniques.  - Human evaluations and automatic evaluations using LLM-based metrics (LLM-Coverage and ALIGNSCORE) further support the superiority of the reranking method.  - Preference tuning with synthetic data improves both coverage and faithfulness of the summaries.  - The findings contribute to the reliable development and evaluation of perspective summarization methods. | ['Summarization'] | [Link](https://github.com/narutatsuri/Unbiased-Perspective-Summarization) | N/A |


## Papers for 2025-06-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain
  Perspective](https://arxiv.org/abs/2506.14965) | Yutao Xie, Fan Zhou, Tianyang Liu, Shibo Hao, Zhoujun Cheng | This paper introduces GURU, a large-scale, curated reinforcement learning dataset for LLM reasoning, spanning six diverse domains.  The authors systematically investigate the effectiveness of reinforcement learning across these domains, revealing a nuanced relationship between pretraining exposure and RL performance gains.  Two new models, GURU-7B and GURU-32B, achieve state-of-the-art performance on a unified evaluation suite among open models. The dataset and models are publicly available, and the code is open source. | ['Reinforcement Learning', 'Question Answering', 'Table Question Answering'] | [Link](https://github.com/LLM360/Reasoning360) | N/A |
| [SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](https://arxiv.org/abs/2506.15154) | Dorien Herremans, Abhinaba Roy, Anuradha Chopra | - The paper introduces SonicVerse, a novel multi-task music captioning model that integrates caption generation with auxiliary music feature detection tasks. - SonicVerse uses a projection-based architecture that transforms audio input into language tokens while simultaneously detecting music features through dedicated auxiliary heads, improving caption quality and detail. - The model is trained on an extended MusicBench dataset annotated with music features using MIRFLEX, achieving state-of-the-art performance on several NLP metrics such as BLEU, ROUGE, and BERT. - An LLM-chaining mechanism is used to generate temporally-aware captions for longer music pieces by concatenating short segment captions. - The model and its weights are open-sourced to promote reproducibility and future research. | ['Audio Classification', 'Text Generation', 'Multimodal'] | [Link](https://github.com/AMAAI-Lab/sonicverse) | [Link](https://huggingface.co/m-a-p/MERT-VO) |
| [Improved Iterative Refinement for Chart-to-Code Generation via
  Structured Instruction](https://arxiv.org/abs/2506.14837) | Weiran Huang, Lichao Sun, Yuyang Wang, Chengzhi Xu, WaltonFuture | - This paper introduces ChartIR, a training-free iterative refinement method for chart-to-code generation that uses structured instructions. - ChartIR distinguishes between visual understanding and code translation, using description and difference instructions to transform visual features into language representations. - The method decomposes chart generation into two stages: initial code generation and iterative refinement, enabling progressive enhancement. - Experimental results on Plot2Code and ChartMimic benchmarks show that ChartIR outperforms existing methods (METAL) and direct generation on both open-source (Qwen2-VL) and closed-source (GPT-40) models. - Ablation studies confirm the importance of both structured description and iterative refinement for superior performance. | ['Image-to-Text', 'Text-to-Image', 'Multimodal'] | N/A | N/A |


## Papers for 2025-06-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GenRecal: Generation after Recalibration from Large to Small
  Vision-Language Models](https://arxiv.org/abs/2506.15681) | Yueh-Hua Wu, Yu-Chiang Frank Wang, Yong Man Ro, rhachiuma, BK-Lee | This paper introduces GenRecal, a novel general-purpose distillation framework for Vision-Language Models (VLMs).  GenRecal incorporates a recalibrator module that aligns feature representations between heterogeneous VLMs, enabling effective knowledge transfer regardless of token type differences.  Extensive experiments demonstrate that GenRecal significantly improves baseline performance, surpassing both large-scale open- and closed-source VLMs.  The recalibrator effectively bridges the gap between large and small VLMs, making knowledge transfer possible even with diverse architectures. GenRecal is shown to work for various model sizes and across many challenging benchmarks. | ['Multimodal'] | N/A | N/A |
| [ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning
  in LLMs](https://arxiv.org/abs/2506.15211) | Yunqi Qiu, Tingting Ma, Xinnian Liang, Zijun Chen, Feng He | - This paper introduces ProtoReasoning, a novel framework that enhances the reasoning capabilities of Large Language Models (LLMs) by leveraging scalable and verifiable prototypical representations. - The framework uses Prolog and PDDL as core representations for logical reasoning and planning, respectively, to transform problems into corresponding prototype representations. - ProtoReasoning features an automated pipeline for prototype construction and a comprehensive verification system using Prolog/PDDL interpreters, ensuring correctness and scalability. - Experiments demonstrate that ProtoReasoning achieves significant improvements (4.7% on Enigmata-Eval, 6.3% on planning, and 4.0% on general reasoning) over baseline models, showcasing enhanced generalization. - Ablation studies confirm that learning in prototype space improves generalization compared to training solely on natural language representations, supporting the hypothesis that reasoning prototypes are crucial for generalizable reasoning in LLMs. | ['Natural Language Processing'] | N/A | N/A |
| [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated
  Agent Intelligence](https://arxiv.org/abs/2506.15677) | Maxine Wu, Xingcheng Yao, Bingxuan Li, Rui Sun, Yining Hong | This paper introduces EMBODIED WEB AGENTS, a novel paradigm for AI agents that integrates web-scale reasoning with physical embodiment.  A new benchmark is presented with diverse tasks requiring coordinated reasoning across physical and digital domains (cooking, navigation, shopping, tourism, geolocation).  State-of-the-art LLMs are evaluated on this benchmark, revealing significant performance gaps compared to human capabilities.  The platform incorporates realistic 3D indoor and outdoor environments with functional web interfaces.  The results highlight challenges and opportunities in cross-domain intelligence. | ['Multimodal', 'Reinforcement Learning', 'Robotics'] | [Link](https://embodied-web-agent.github.io/) | N/A |
| [Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form
  Generation](https://arxiv.org/abs/2506.15068) | Zichao Liang, Xiyang Wu, Yuhang Zhou, Yapei Chang, Zongxia Li | - This paper introduces PrefBERT, a lightweight scoring model for evaluating open-ended long-form text generation. - PrefBERT is trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality and uses ModernBERT architecture. -  The model offers better semantic reward feedback for guiding the training of RL models than traditional metrics such as ROUGE-L and BERTScore. - Experiments using LLM-as-a-judge, human ratings, and qualitative analysis demonstrate that PrefBERT consistently aligns with human preferences. -  Training policy models with PrefBERT yields responses better aligned with human preferences than those trained with traditional metrics. | ['Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/zli12321/long_form_rl) | N/A |
| [SciVer: Evaluating Foundation Models for Multimodal Scientific Claim
  Verification](https://arxiv.org/abs/2506.15569) | Arman Cohan, Zexi Kuang, Yifei Shen, Chengye Wang, yilunzhao | - We introduce SCIVER, the first benchmark for evaluating multimodal scientific claim verification. - SCIVER consists of 3,000 expert-annotated examples across 1,113 scientific papers, covering four reasoning types. - We evaluate 21 state-of-the-art multimodal foundation models, revealing a substantial performance gap between models and human experts. - Through in-depth analysis of RAG and human-conducted error evaluations, we identify critical model limitations. - Our findings offer key insights for advancing models' comprehension and reasoning in multimodal scientific literature. | ['Multimodal', 'Question Answering'] | [Link](https://github.com/chengyewang/SciVer) | [Link](https://huggingface.co/QDRhhhh/SciVer) |
| [CoMemo: LVLMs Need Image Context with Image Memory](https://arxiv.org/abs/2506.06279) | Jifeng Dai, Wenhai Wang, Xizhou Zhu, jackroos, CLLBJ16 | - CoMemo, a novel dual-path framework for Large Vision-Language Models (LVLMs), is proposed to address the limitations of existing architectures in handling dynamic high-resolution images and long sequences. - The model employs a context path for autoregressive processing and a memory path for cross-attention, effectively alleviating visual information neglect. - A novel positional encoding mechanism, ROPE-DHR, is introduced to maintain 2D spatial awareness while mitigating remote decay issues. - Evaluations across seven benchmarks demonstrate CoMemo's superior performance compared to conventional LVLM architectures, especially in long-context comprehension and multi-image reasoning tasks. - The three-stage training strategy effectively balances the contributions of the two visual pathways, preventing over-reliance on either path. | ['Multimodal'] | [Link](https://lalbj.github.io/projects/CoMemo/) | N/A |
| [SwarmAgentic: Towards Fully Automated Agentic System Generation via
  Swarm Intelligence](https://arxiv.org/abs/2506.15672) | Shijie Zhou, Haokun Chen, Shijie Tang, Chenyang Lin, Yao Zhang |  - SwarmAgentic is a novel framework that generates fully automated agentic systems from scratch, optimizing both agent functionality and collaboration through language-driven exploration. - It uses a population-based optimization scheme inspired by Particle Swarm Optimization (PSO) to efficiently search over system-level structures. - The framework outperforms all baselines on six real-world tasks involving high-level planning, system-level coordination, and creative reasoning, achieving a +261.8% improvement over ADAS on the TravelPlanner benchmark. - SwarmAgentic introduces three core capabilities: From-Scratch Agent Generation, Self-Optimizing Agent Functionality, and Self-Optimizing Agent Collaboration. - The code for SwarmAgentic is publicly available on GitHub. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/SwarmAgentic) | N/A |
| [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal
  Models](https://arxiv.org/abs/2506.14435) | Yitao Zhai, Yan Feng, Ruiping Wang, Jiayu Xu, Hongyu Wang | - This paper introduces MoTE, a novel Mixture-of-Ternary-Experts (MoTE) model for memory-efficient large multimodal models. - The MoTE architecture uses a pre-trained full-precision feed-forward network (FFN) as a shared expert and trains ternary routed experts with parameters in {-1, 0, 1} during up-cycling. - Experiments demonstrate that MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering a lower memory footprint. - MoTE's performance further improves when combined with post-training quantization methods, outperforming MoE-LLaVA by 4.3% average accuracy on end tasks with the same expert memory footprint. - The results highlight MoTE's effectiveness and potential for memory-constrained devices. | ['Multimodal'] | N/A | N/A |
| [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866) | Zico Kolter, Francesco Croce, Hao Zhao, Agatha Duzan, Thomas Kuntz | - This paper introduces OS-HARM, a new benchmark for evaluating the safety of computer use agents. - OS-HARM tests agents across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. - The benchmark includes 150 tasks that require agents to interact with various OS applications. - An automated LLM judge is proposed to evaluate both accuracy and safety, achieving high agreement with human annotations. - The results show that current frontier models tend to directly comply with many deliberate misuse queries and are vulnerable to prompt injection attacks. | ['Multimodal', 'Any-to-Any'] | [Link](https://github.com/tml-epfl/os-harm) | N/A |


## Papers for 2025-06-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Scaling Test-time Compute for LLM Agents](https://arxiv.org/abs/2506.12928) | Dehua Ma, Tianshun Xing, Siwei Wu, Hanhao Li, King Zhu | - This paper introduces a novel method for scaling test-time compute for large language model (LLM) agents, improving their reasoning abilities. - The approach systematically explores various test-time scaling strategies, including parallel sampling, sequential revision, verification and merging, and rollout diversification. - Experimental results show significant performance improvements compared to baselines, demonstrating the effectiveness of test-time scaling for LLM agents. - Among the tested strategies, list-wise verification and merging methods and increasing diversified rollouts yield the best results. - The findings align with observations from previous test-time scaling research on LLMs, highlighting the potential for further advancements in this area. | ['Natural Language Processing'] | [Link](https://github.com/OPPO-PersonalAI/OAgents) | N/A |
| [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429) | Ziwei He, Qipeng Guo, Zengfeng Huang, Zhigeng Liu, Xiaoran Liu | - This paper introduces LongLLaDA, a training-free method for extending the context window of diffusion Large Language Models (LLMs). - LongLLaDA integrates LLaDA with NTK-based RoPE extrapolation to achieve this context extension, empirically validating the effectiveness of established extrapolation scaling laws for diffusion LLMs. - The study presents a systematic comparison of the long-context performance of diffusion LLMs and traditional autoregressive LLMs, identifying unique characteristics of diffusion LLMs such as stable perplexity during context extrapolation and a "local perception" phenomenon enabling successful retrieval from recent context segments. - LongLLaDA successfully extends context windows, showing that it maintains effective extrapolation up to a 6x extension in context length, without additional training. - The paper provides empirical benchmarks and theoretical insights into the long-context behavior of diffusion LLMs, crucial for future research in this area. | ['Natural Language Processing'] | N/A | N/A |
| [Stream-Omni: Simultaneous Multimodal Interactions with Large
  Language-Vision-Speech Model](https://arxiv.org/abs/2506.13642) | Yang Feng, Yan Zhou, Qingkai Fang, Shoutao Guo, Shaolei Zhang | - This paper introduces Stream-Omni, a novel large language-vision-speech model designed for simultaneous multimodal interactions. - The model architecture employs an LLM backbone, aligning vision and speech modalities to text through sequence-dimension concatenation and CTC-based layer-dimension mapping respectively. - Stream-Omni demonstrates strong performance on various benchmarks, including visual understanding, speech interaction, and vision-grounded speech interaction tasks, often outperforming existing methods. - It achieves these results using only 23,000 hours of speech data, highlighting its efficient modality alignment. - The model simultaneously generates intermediate text outputs during speech interaction, providing a comprehensive multimodal user experience. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Visual Question Answering'] | [Link](https://github.com/ictnlp/Stream-Omni) | [Link](https://huggingface.co/ICTNLP/stream-omni-8b) |
| [Efficient Medical VIE via Reinforcement Learning](https://arxiv.org/abs/2506.13363) | Chong Li, Chenglin Zhu, Lijun Liu, zhaocheng, lryyyy | - This paper introduces a novel method for efficient medical visual information extraction (VIE) using reinforcement learning with verifiable rewards (RLVR). - The proposed RLVR framework addresses the challenges of domain-specific schemas and high annotation costs in medical VIE by using only 100 annotated samples. - The method incorporates a balanced precision-recall reward mechanism and innovative sampling strategies to enhance reasoning capabilities and reduce hallucinations. - The authors achieve state-of-the-art performance on medical VIE tasks by fine-tuning Qwen2.5-VL-7B with their RLVR method, significantly improving F1 score, precision, and recall. - Case studies demonstrate the effectiveness of reasoning during training and inference for VIE, highlighting the need for domain-specific optimization. | ['Reinforcement Learning', 'Image-to-Text', 'Multimodal'] | N/A | N/A |
| [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just
  Like an Olympiad Team](https://arxiv.org/abs/2506.14234) | Md Rizwan Parvez, Md Kishor Morol, Salman Rahman, Md Tanzib Hosain | This paper introduces Xolver, a training-free multi-agent reasoning framework that enhances large language models (LLMs) by incorporating a persistent memory of holistic experiences.  Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative agent interactions, agent-driven evaluation, and iterative reasoning refinement.  Empirical results demonstrate that Xolver consistently outperforms specialized reasoning agents and achieves state-of-the-art results on several benchmarks, including GSM8K, AIME'24, and LiveCodeBench.  The authors attribute Xolver's success to its holistic experience learning approach, which allows it to leverage past experiences to inform future reasoning. Xolver is open-sourced. | ['Natural Language Processing'] | [Link](https://kagnlp.github.io/xolver.github.io/) | [Link](null) |
| [QFFT, Question-Free Fine-Tuning for Adaptive Reasoning](https://arxiv.org/abs/2506.12860) | Ke Ji, Yukang Lin, Fei Yu, Junxiao Xu, lwl-uestc | - This paper introduces Question-Free Fine-Tuning (QFFT), a novel fine-tuning approach that removes the input question during training to enable adaptive reasoning in language models. - QFFT learns exclusively from Long Chain-of-Thought (CoT) responses, allowing the model to adaptively switch between Short CoT and Long CoT patterns based on the complexity of the question. - Experiments on mathematical datasets demonstrate that QFFT reduces the average response length by over 50% while maintaining performance comparable to Supervised Fine-Tuning (SFT). - QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios. - The QFFT method is publicly available on GitHub. | ['Question Answering'] | [Link](https://github.com/LWL-cpu/Question-Free-Fine-Tuning) | N/A |
| [Can LLMs Generate High-Quality Test Cases for Algorithm Problems?
  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure](https://arxiv.org/abs/2506.12278) | Xue Xia, Zexi Kuang, Zheyuan Yang, yilunzhao | - This paper introduces TestCase-Eval, a new benchmark for evaluating LLMs' ability to generate high-quality test cases for algorithm problems. - The benchmark comprises 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform, focusing on two tasks: Fault Coverage and Fault Exposure. - A comprehensive evaluation of 19 state-of-the-art LLMs on TestCase-Eval reveals that even top-performing models significantly underperform compared to human experts in the Fault Exposure task (43.8% vs. 93.3%). - The study provides valuable insights into LLMs' strengths and limitations in generating effective test cases, highlighting the challenges in achieving human-level performance. - The findings underscore the inherent difficulty of generating diverse, impactful test cases that can effectively expose subtle bugs in algorithm implementations. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/FlowRays/TestCase-Eval) | N/A |
| [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language
  Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977) | Junjie Ye, Siyu Yuan, Zehui Chen, Shiting Huang, CostaliyA | This paper introduces CRITICTOOL, a new benchmark designed to evaluate the self-critique capabilities of Large Language Models (LLMs) in tool-calling scenarios.  CRITICTOOL includes diverse error patterns and evaluates models from multiple perspectives, addressing limitations in existing benchmarks. The proposed evolutionary strategy for dataset construction improves the diversity and realism of the benchmark, better reflecting real-world scenarios.  Extensive experiments on CRITICTOOL reveal insights into LLMs' self-critique abilities and highlight performance differences across various models.  The code for the benchmark is publicly available. | ['Natural Language Processing'] | [Link](https://github.com/Shellorley0513/CriticTool) | N/A |
| [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse
  Autoencoders](https://arxiv.org/abs/2506.14002) | Zhuoran Yang, Tianhao Wang, Xuyuan Xiong, Heejune Sheen, Siyu Chen |  - A novel statistical framework is proposed to address the feature recovery problem for Large Language Models (LLMs), which models polysemantic features as sparse mixtures of underlying monosemantic features.  - A new SAE training algorithm called Group Bias Adaptation (GBA) is introduced, which uses bias adaptation to directly control neuron activation sparsity.  - GBA achieves superior empirical performance on LLMs with up to 1.5 billion parameters compared to existing methods, achieving the sparsity-loss frontier.  - The proposed method provides the first provable recovery guarantee for SAE training algorithms.  - The theoretical findings are validated through experiments on synthetic and real-world data. | ['Feature Extraction', 'Natural Language Processing'] | N/A | N/A |
| [VideoMolmo: Spatio-Temporal Grounding Meets Pointing](https://arxiv.org/abs/2506.05336) | Zhiqiang Shen, Abdelrahman Shaker, Hanan Gani, Ghazi Shazan Ahmad, ahmedheakl | - This paper introduces VIDEOMOLMO, a large multimodal model for spatio-temporal grounding that improves upon existing methods by decomposing the task into two steps: precise point localization and sequential mask fusion. - The model architecture incorporates a temporal module using an attention mechanism for temporal consistency and a novel temporal mask fusion pipeline using SAM2 for bidirectional point propagation. - VIDEOMOLMO demonstrates improved spatio-temporal reasoning in visual grounding, producing more accurate and coherent segmentation masks compared to previous approaches, as shown in Figure 1. - The model was trained and evaluated on a new dataset curated by the authors, including a comprehensive dataset of 72k video-caption pairs annotated with 100k object points and a challenging out-of-distribution benchmark called VPoS-Bench. - Results on various benchmarks (VPoS-Bench, Refer-VOS, Reason-VOS) show that VIDEOMOLMO substantially improves spatio-temporal pointing accuracy and reasoning capability compared to existing methods. | ['Video-Text-to-Text', 'Multimodal', 'Mask Generation', 'Keypoint Detection'] | [Link](https://github.com/mbzuai-oryx/VideoMolmo) | N/A |
| [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time
  Markers](https://arxiv.org/abs/2506.14702) | Sara Hooker, Ahmet Üstün, Adrien Morisot, Julia Kreutzer, Daniel D'souza | This paper introduces a novel approach to improve the performance of large language models (LLMs) on low-frequency tasks.  The method involves introducing training-time markers that capture various data characteristics. These markers allow for flexible control over generation attributes and improved performance, especially on long-tail data. The authors demonstrate substantial improvements in win rates on open-ended generation tasks, achieving an average lift of 5.7% and gains exceeding 9.1% in underrepresented domains.  Their method shows effectiveness on various tasks such as code generation and length instruction following, with relative improvements up to 14.1%. The introduced framework is flexible and optional at inference time, since the markers can be inferred accurately. | ['Natural Language Processing'] | [Link](None) | [Link](None) |
| [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic
  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise
  Pooled Representations](https://arxiv.org/abs/2506.13901) | Utkarsh Bhatt, Danush Khanna, Chhavi Sharma, Abhilekh Borah, amanchadha |  - The paper introduces a novel intrinsic metric called Alignment Quality Index (AQI) to diagnose large language model (LLM) alignment.   - AQI assesses alignment by analyzing the latent geometry of safe and unsafe activations, unlike existing methods that rely on behavioral proxies.  - The method leverages layerwise pooled representations and cluster quality indices such as Davies-Bouldin and Calinski-Harabasz to detect hidden misalignments and jailbreak risks, even when outputs appear compliant.  - The experimental evaluation on LITMUS dataset demonstrates that AQI correlates strongly with external human judgments and is robust to decoding variation and prompt paraphrasing.  - Unlike other metrics, AQI provides insights into latent model behaviour. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility
  Simulation](https://arxiv.org/abs/2506.13599) | Yong Li, Jian Yuan, Yuwei Du, JJ-TMT | - This paper introduces CAMS, a novel agentic framework for simulating human mobility in urban environments using CityGPT, an urban-foundation large language model. - CAMS comprises three core modules: MobExtractor, GeoGenerator, and TrajEnhancer, which work synergistically to generate realistic and plausible trajectories. - The model integrates urban spatial knowledge into the reasoning process of LLMs, enabling more accurate and generalizable simulations of human mobility. - Experiments on real-world datasets demonstrate that CAMS outperforms existing methods in terms of generating more realistic trajectories while not relying on external geospatial information. - CAMS establishes a new paradigm that integrates agentic frameworks with urban-knowledgeable LLMs for human mobility simulation. | ['Natural Language Processing'] | N/A | N/A |
| [TR2M: Transferring Monocular Relative Depth to Metric Depth with
  Language Descriptions and Scale-Oriented Contrast](https://arxiv.org/abs/2506.13387) | Hongliang Ren, Long Bai, Yiming Huang, Beilei Cui | - This paper introduces TR2M, a novel framework that transfers monocular relative depth to metric depth using image and text descriptions. - The model architecture consists of separate frozen image and text encoders, a cross-modality attention module, and two lightweight decoder heads to predict scale and shift maps for pixel-wise rescaling. - A scale-oriented contrastive learning strategy is used to enforce feature consistency based on depth distribution, improving scale perception. - TR2M outperforms other language-based methods on several datasets and exhibits superior zero-shot capabilities on unseen datasets. - The authors demonstrate the potential of pixel-wise relative-to-metric depth transfer with language assistance. | ['Depth Estimation', 'Multimodal'] | [Link](https://github.com/BeileiCui/TR2M) | N/A |
| [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880) | Mahmood Sharif, Mor Geva, MatanBT | - This paper introduces a novel technique to enhance and mitigate suffix-based jailbreaks against large language models (LLMs). - The core contribution is the identification of a shallow, critical mechanism in GCG attacks, focusing on information flow from the adversarial suffix to the chat template tokens. - The authors quantify the dominance of the adversarial suffix in the contextualization process and link this to the universality of the attacks. - Practical implications are demonstrated by enhancing GCG universality (up to 5x in some cases) and surgically mitigating attacks (at least halving attack success). - Code and data are released for reproducibility. | ['Natural Language Processing', 'Text Generation'] | [Link](http://github.com/matanbt/interp-jailbreak) | N/A |


## Papers for 2025-06-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning
  Attention](https://arxiv.org/abs/2506.13585) | ManTle, windlx, LINMUJIE-judy, enochzhang, sheep33333 | - MiniMax-M1 is introduced, a novel open-weight, large-scale hybrid-attention reasoning model featuring a hybrid Mixture-of-Experts (MoE) architecture and a lightning attention mechanism.  - The model boasts 456 billion parameters and supports a context length of 1 million tokens, outperforming existing models in terms of context size. -  MiniMax-M1 demonstrates superior test-time compute efficiency compared to DeepSeek R1, consuming only 25% of the FLOPs at a generation length of 100K tokens. - A novel reinforcement learning algorithm, CISPO, is proposed to further enhance training efficiency, enabling full RL training within three weeks on 512 H800 GPUs.  - Experiments show MiniMax-M1 achieves competitive or superior results to other leading models across various benchmarks, particularly excelling in complex software engineering, tool utilization, and long-context tasks. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/MiniMax-AI/MiniMax-M1) | N/A |
| [Scientists' First Exam: Probing Cognitive Abilities of MLLM via
  Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521) | Ruoyao Xiao, Xuming He, Yiheng Wang, Yuhao Zhou, WilsonHwang |  - This paper introduces a new benchmark, Scientists' First Exam (SFE), designed to evaluate the scientific cognitive abilities of Multimodal Large Language Models (MLLMs).  - The benchmark comprises 830 expert-verified VQA pairs across three cognitive levels (perception, understanding, and reasoning), covering 66 multimodal tasks across five high-value disciplines.  - Experiments show that state-of-the-art models like GPT-03 and InternVL-3 achieve low scores (34.08% and 26.52% respectively), highlighting significant room for improvement.  - SFE categorizes scientific tasks by cognitive capacity, introducing a three-level taxonomy, and releases bilingual (English & Chinese) tasks constructed from native scientific data formats.  - The benchmark reveals performance gaps across disciplines and model types, indicating potential shifts in MLLM capabilities from knowledge understanding to high-order reasoning. | ['Visual Question Answering', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/PrismaX/SFE) |
| [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves
  Reasoning Efficiency](https://arxiv.org/abs/2506.08343) | Ranjay Krishna, Zhaoyang Chu, Dongping Chen, Yuanning Feng, Chenlong Wang | - This paper introduces NOWAIT, a training-free method that improves the efficiency of large reasoning models (LRMs) by suppressing self-reflection tokens like "Wait" and "Hmm" during inference. - NOWAIT reduces the chain-of-thought (CoT) trajectory length by 27%-51% across ten benchmarks spanning textual, visual, and video reasoning tasks without compromising model utility. - The method demonstrates consistent improvements across five R1-style model series and various reasoning tasks, showcasing its generalizability. - Experimental results show that NOWAIT outperforms existing training-free approaches such as Token-Budget while achieving comparable or even better accuracy than training-based methods like 01-Pruner. - The authors conclude that explicit self-reflection is not essential for advanced reasoning and that NOWAIT provides a plug-and-play solution for efficient and utility-preserving multimodal reasoning. | ['Natural Language Processing', 'Question Answering', 'Multimodal'] | N/A | N/A |
| [Discrete Diffusion in Large Language and Multimodal Models: A Survey](https://arxiv.org/abs/2506.13759) | Xinchao Wang, Qi Li, Runpeng Yu | This survey paper provides a comprehensive overview of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).  It systematically analyzes the mathematical foundations, model architectures, training strategies, and inference techniques used in this emerging field.  The authors categorize representative models, highlighting their capabilities and limitations compared to autoregressive approaches.  Finally, the paper concludes by discussing future research directions and potential applications across various domains. | ['Natural Language Processing', 'Text Generation', 'Multimodal'] | [Link](https://github.com/LiQiiiii/DLLM-Survey) | N/A |
| [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055) | Weizhen Li, Weichen Sun, Qianben Chen, Jingyi Cao, Dingfeng Shi | - This paper introduces TaskCraft, an automated workflow for generating agentic tasks (tasks requiring multi-step problem-solving with autonomy, tool use, and adaptive reasoning) with execution trajectories. - TaskCraft addresses the limitations of existing instruction data by automatically generating difficulty-scalable, multi-tool, and verifiable tasks. - The workflow uses depth-based and width-based extensions to create structurally and hierarchically complex tasks, improving prompt optimization and supervised fine-tuning of agentic foundation models. - Experiments demonstrate that the generated tasks improve performance on various benchmarks, including HotpotQA, Musique, and Bamboogle. - TaskCraft provides a large-scale synthetic dataset (approximately 36,000 tasks) with varying difficulty to support future research. | ['Natural Language Processing', 'Text Generation', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/OPPO-PersonalAI/TaskCraft) | N/A |
| [VGR: Visual Grounded Reasoning](https://arxiv.org/abs/2506.11991) | Haiyong Jiang, Haochen Wang, Zijiang Kang, bongbohong, stormthunder | - This paper introduces VGR, a novel multimodal large language model (MLLM) designed for visual grounded reasoning. - VGR enhances fine-grained visual perception by first detecting relevant image regions and then incorporating them into the reasoning process using a selective visual replay mechanism. - The model was trained on a new large-scale dataset called VGR-SFT containing reasoning data with mixed vision grounding and language deduction. - Experimental results on various multimodal benchmarks demonstrate that VGR outperforms the baseline LLaVA-NeXT-7B by significant margins while using only 30% of the image tokens. - VGR's self-driven selective visual replay method improves both accuracy and interpretability of multi-modal reasoning. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/BytedanceDouyinContent/VGR) |
| [PersonaFeedback: A Large-scale Human-annotated Benchmark For
  Personalization](https://arxiv.org/abs/2506.12915) | Yuchen Eleanor Jiang, Tiannan Wang, Dongyi Ding, Chenghao Zhu, Meiling Tao | - This paper introduces PERSONAFEEDBACK, a new large-scale human-annotated benchmark dataset for evaluating the personalization capabilities of large language models (LLMs). - The dataset consists of 8298 human-annotated test cases categorized into easy, medium, and hard tiers based on the complexity of user personas and the difficulty of distinguishing between personalized responses. - PERSONAFEEDBACK decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas, unlike existing benchmarks. - Empirical results show that even state-of-the-art LLMs struggle with the harder tasks in PERSONAFEEDBACK, highlighting the challenges in LLM personalization. - All benchmark data, annotation protocols, and evaluation pipeline are publicly available to facilitate future research on LLM personalization. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/PersonalAILab/PersonaFeedback) |
| [From Real to Synthetic: Synthesizing Millions of Diversified and
  Complicated User Instructions with Attributed Grounding](https://arxiv.org/abs/2506.03968) | Zhendong Mao, Xiaorui Wang, Benfeng Xu, IgnoraZ | - This paper introduces a novel framework for synthesizing a large-scale dataset of diverse and complex user instructions with attributed grounding. - The framework utilizes a top-down attribution process, grounding real instructions to users and motivations, and a bottom-up synthesis process, leveraging web documents to generate instructions. - A dataset of 1 million instructions, called SYNTHQUESTIONS, is constructed using this framework. - Models trained on SYNTHQUESTIONS achieve state-of-the-art performance on several common benchmarks, outperforming models trained on datasets 10 times larger. - The improvements continually scale with the addition of more web corpora. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Ignoramus0817/SynthQuestions) | N/A |
| [Language Surgery in Multilingual Large Language Models](https://arxiv.org/abs/2506.12450) | Muhammad Ilham Ghozali, samuel-cahyawijaya, tackhwa, muhammadravi251001, joanitolopo | This paper introduces Inference-Time Language Control (ITLC), a novel method for enhancing cross-lingual performance in large language models (LLMs). ITLC leverages the naturally emerging representation alignment in the middle layers of LLMs to disentangle language-specific and language-agnostic information, enabling precise language control without semantic degradation. Experiments demonstrate ITLC's effectiveness in zero-shot cross-lingual language generation and mitigating language confusion. The approach outperforms existing methods in both tasks, showcasing its potential for improving cross-lingual capabilities of LLMs. ITLC is a practical solution for enhancing cross-lingual performance in LLMs and provides a better understanding of representation alignment. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/SEACrowd/itlc) | N/A |
| [AI Agent Behavioral Science](https://arxiv.org/abs/2506.06366) | Honglin Zhang, Haoye Chai, Yunke Zhang, Lin Chen, JJ-TMT |  - This paper introduces a novel research paradigm: AI Agent Behavioral Science, which studies how AI agents act, adapt, and interact.  - It proposes a framework for understanding individual AI agent behavior, emphasizing intrinsic attributes, environmental constraints, and behavioral feedback. - The paper examines emergent behaviors in multi-agent interactions, categorizing them into cooperative, competitive, and open-ended dynamics. - It discusses the behavioral roles of AI agents in human-agent interaction, such as companion, catalyst, and contender in cooperative settings and manipulator in competitive settings. - Finally, it addresses the importance of AI Agent Behavioral Science for responsible AI and proposes six promising research directions to further this new field. | ['Reinforcement Learning', 'Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [Supernova Event Dataset: Interpreting Large Language Model's Personality
  through Critical Event Analysis](https://arxiv.org/abs/2506.12189) | Ioana Ciucă, pranavAL2109 |  - This paper introduces the Supernova Event Dataset, a new dataset for evaluating LLMs' ability to extract and rank critical events from various text types.  - The dataset consists of Wikipedia articles on biographies, news events, historical events, and scientific discoveries.  - A novel framework is proposed where one LLM acts as a judge, evaluating other LLMs' personality based on their event selection and ranking.  - The authors find distinct personality traits across different LLMs, showing how models prioritize events based on their underlying reasoning styles.  - This work helps improve LLM interpretability and provides a valuable tool for further research on LLM personality and alignment. | ['Natural Language Processing'] | N/A | N/A |
| [MS4UI: A Dataset for Multi-modal Summarization of User Interface
  Instructional Videos](https://arxiv.org/abs/2506.12623) | Jiuxiang Gu, Seunghyun Yoon, Hao Tan, Yuan Zang, Franck-Dernoncourt | - This paper introduces MS4UI, a new dataset for multi-modal summarization of user interface (UI) instructional videos. - The dataset contains 2,413 videos totaling 167 hours, manually annotated for video segmentation, text summarization, and video summarization. - Three core tasks are proposed: video segmentation, text summarization, and video summarization, focusing on generating concise and executable step-by-step instructions and illustrations. - Experiments show that existing multi-modal summarization methods struggle with UI video summarization, highlighting the need for new methods tailored to this domain. - The dataset includes key frame annotations to illustrate actions and ensure executability of the generated summaries. | ['Video-Text-to-Text', 'Summarization', 'Multimodal'] | N/A | N/A |
| [Profiling News Media for Factuality and Bias Using LLMs and the
  Fact-Checking Methodology of Human Experts](https://arxiv.org/abs/2506.12552) | Preslav Nakov, Maha Tufail Agro, Dilshod Azizov, Zain Muhammad Mujahid | - This paper introduces a novel methodology for assessing the factuality and political bias of news media outlets using Large Language Models (LLMs). - The methodology leverages various prompts designed to emulate the criteria used by human fact-checkers, eliciting responses from LLMs and aggregating them to make predictions. - Experiments demonstrate significant improvements over strong baselines, showcasing the effectiveness of the proposed approach in predicting both factuality and political bias. - An in-depth error analysis reveals the impact of media popularity and region on model performance, highlighting biases towards popular and U.S.-based outlets. - The study also includes an ablation study to identify the crucial components of the dataset that contribute to the improved performance. | ['Text Classification', 'Zero-Shot Classification'] | [Link](https://github.com/mbzuai-nlp/llm-media-profiling) | N/A |
| [Incorporating Domain Knowledge into Materials Tokenization](https://arxiv.org/abs/2506.11115) | SangKeun Lee, SungHo Kim, Junho Kim, Jun-Hyung Park, yerim0210 | - This paper introduces MATTER, a novel tokenization framework that integrates domain knowledge into the tokenization process for materials science. - MATTER uses MatDetector, a material concept identifier trained on a corpus of material knowledge, to score material concepts and prioritize them during token merging. - Experimental results demonstrate that MATTER outperforms existing methods, achieving an average performance gain of 4% in generation tasks and 2% in classification tasks. - The key contributions include a novel domain-specific tokenization framework, a novel scheme for materials tokenization based on MatDetector, and a demonstration of MATTER's superior performance. - The results highlight the importance of incorporating domain knowledge into tokenization strategies for scientific text processing, particularly in specialized domains like materials science. | ['Natural Language Processing', 'Token Classification', 'Text Generation'] | [Link](https://github.com/yerimoh/MATTER) | N/A |
| [Steering LLM Thinking with Budget Guidance](https://arxiv.org/abs/2506.13752) | Chuang Gan, Yang Zhang, Wenshuo Zhao, Junyan Li | - This paper introduces Budget Guidance, a novel method for controlling the reasoning length of LLMs without fine-tuning. - Budget Guidance employs a lightweight predictor that models a Gamma distribution over the remaining thinking length during token generation, guiding the LLM towards the target budget. - The method demonstrates significant improvements in token efficiency and accuracy on challenging math benchmarks compared to baseline methods, achieving up to a 26% accuracy gain under tight budgets. - Budget Guidance exhibits emergent capabilities such as estimating question difficulty and generalizes well to broader tasks. - The source code is available on GitHub. | ['Natural Language Processing'] | [Link](https://github.com/UMass-Embodied-AGI/BudgetGuidance) | N/A |
| [Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging
  Unsubstantiated Claims and Ambiguous Pronouns](https://arxiv.org/abs/2506.13172) | PChemGuy | - This paper introduces a novel methodology for using Large Language Models (LLMs) to perform high-level semantic and linguistic analysis of scholarly manuscripts. - The method employs structured workflow prompts to guide LLMs in identifying unsubstantiated claims and ambiguous pronoun references in abstracts and conclusions. - The proposed approach is evaluated on two state-of-the-art LLMs, Gemini Pro 2.5 Pro and ChatGPT Plus 03, under varied context conditions. - Results show significant divergence in model performance depending on the task and context, highlighting the need for rigorous, model-specific testing. - The findings suggest that structured prompting is a viable methodology for complex textual analysis, but model performance is highly dependent on the interplay between the model, task, and context. | ['Natural Language Processing'] | N/A | N/A |
| [QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299) | Yunho Maeng, Soo Yong Kim, Hyoungseo Cho, Jeonghwa Yoo, Taegyeong Lee | - QGuard is a novel zero-shot safety guard method for multi-modal LLMs that uses question prompting to effectively block harmful prompts. - The method defends against both text-based and multi-modal harmful prompts without requiring any fine-tuning, making it robust against the latest harmful prompts. - Experimental results show that QGuard performs competitively on both text-only and multi-modal harmful datasets, outperforming various baselines. - QGuard employs a white-box analysis of user inputs by analyzing the logits of question prompting, providing valuable insights into the decision-making process. - The proposed method is simple yet effective, making it suitable for real-world applications in mitigating security risks associated with harmful prompts. | ['Multimodal', 'Zero-Shot Classification'] | N/A | N/A |
| [Hatevolution: What Static Benchmarks Don't Tell Us](https://arxiv.org/abs/2506.12148) | Albert Meroño-Peñuela, Yulan He, Barbara McGillivray, Chiara Di Bonaventura | - This paper investigates the robustness of language models in the context of evolving hate speech. - The authors empirically evaluate 20 language models on two hate speech experiments that simulate the temporal dynamics of hate speech. - The findings reveal a significant temporal misalignment between static and time-sensitive evaluations, emphasizing the limitations of using static benchmarks for assessing language model safety. - The study advocates for incorporating time-sensitive linguistic benchmarks in the evaluation of hate speech models to ensure accurate and reliable assessments. - The paper's main contribution lies in demonstrating the limitations of static hate speech benchmarks and advocating for the development of more dynamic and time-sensitive evaluation methods. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/ChiaraDiBonaventura/hatevolution/tree/main) | N/A |


## Papers for 2025-06-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600) | Guy Uziel, Matan Vetzler, Koren Lazar, George Kour, Itay Nakash | - This paper introduces CRAFT, a novel multi-agent red-teaming system designed to evaluate the robustness of policy-adherent Large Language Model (LLM)-based agents against adversarial users. - CRAFT leverages policy-aware persuasive strategies and outperforms conventional methods such as DAN prompts in bypassing safety policies. - The authors introduce T-break, a complementary benchmark built upon T-bench to rigorously assess agent resilience against manipulation. - Several straightforward defense strategies are evaluated, highlighting the need for stronger research-driven safeguards. - The findings reveal critical vulnerabilities in policy-adherent agents and underscore the importance of developing robust defenses against adversarial attacks. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [The Diffusion Duality](https://arxiv.org/abs/2506.10892) | Justin Chiu, Guanghan Wang, Aaron Gokaslan, Justin Deschenaux, Subham Sekhar Sahoo | This paper introduces Duo, a novel framework that leverages the duality between Gaussian and Uniform-state discrete diffusion models to improve text generation.  Duo employs curriculum learning guided by the underlying Gaussian process to accelerate training and surpass autoregressive models in zero-shot perplexity on several benchmarks.  A new distillation technique, Discrete Consistency Distillation, is introduced to enhance sampling speed, achieving a two-order magnitude improvement over previous methods.  The framework transfers advanced techniques from Gaussian diffusion to improve both training and sampling efficiency in USDMs.  The code and model checkpoints are available on the project page. | ['Text Generation'] | [Link](https://github.com/s-sahoo/duo) | N/A |
| [LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive
  Programming?](https://arxiv.org/abs/2506.11928) | Kaiyuan Liu, Shang Zhou, Zeyu Shen, Zerui Cheng, Zihan Zheng | This paper introduces LiveCodeBench Pro, a benchmark for evaluating LLMs in competitive programming. It uses problems from Codeforces, ICPC, and IOI, annotated by Olympiad medalists.  The benchmark reveals that frontier models have significant limitations in algorithmic reasoning and complex case analysis, achieving only 53% pass@1 on medium-difficulty problems and 0% on hard problems. High performance is largely driven by implementation precision, not superior reasoning.  LiveCodeBench Pro highlights the gap between LLMs and human grandmasters, providing diagnostics to improve code-centric LLM reasoning. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [A High-Quality Dataset and Reliable Evaluation for Interleaved
  Image-Text Generation](https://arxiv.org/abs/2506.09427) | kpzhang, ZhangShenglin, fanrui00, cyrilli, finyorko |  - This paper introduces InterSyn, a large-scale multimodal dataset for instruction-following, multi-turn image-text generation, and SynJudge, an automatic evaluation model assessing multimodal outputs along four dimensions. - The Self-Evaluation with Iterative Refinement (SEIR) method used to build InterSyn yielded substantially higher dataset quality compared to an identical process without refinement, as shown via experimental studies. - SynJudge, the automatic evaluation model, aligns well with human judgment and provides interpretable quantitative feedback, facilitating more effective model training. - LMMs trained on InterSyn demonstrated uniform performance gains across all evaluation metrics, confirming its utility for advancing multimodal systems. - InterSyn contains approximately 1.8 million single-turn samples and 50k multi-turn dialogues, offering a robust foundation for training unified multimodal models. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Visual Question Answering', 'Text Generation'] | N/A | N/A |
| [Detecting Harmful Memes with Decoupled Understanding and Guided CoT
  Reasoning](https://arxiv.org/abs/2506.08477) | Anh Tuan Luu, Fengjun Pan, bobxwu | This paper introduces U-CoT+, a novel framework for harmful meme detection that addresses limitations of existing methods in resource efficiency, flexibility, and explainability.  The framework decouples meme interpretation from classification using a meme-to-text pipeline, enabling resource-efficient detection with LLMs.  Targeted, human-crafted guidelines are incorporated to guide CoT prompting, allowing easy adaptation to different criteria.  Experiments on seven benchmark datasets demonstrate U-CoT+'s effectiveness, highlighting its potential for low-resource settings and achieving comparable performance to state-of-the-art, fully supervised methods. | ['Zero-Shot Classification', 'Text Classification', 'Multimodal'] | [Link](https://anonymous.4open.science/r/HMC-AF2B/README.md) | N/A |
| [Beyond Homogeneous Attention: Memory-Efficient LLMs via
  Fourier-Approximated KV Cache](https://arxiv.org/abs/2506.11886) | Yuerong Song, Ruixiao Li, Qiqi Wang, Siyang He, Xiaoran Liu | - This paper introduces FourierAttention, a novel training-free framework for memory-efficient LLMs that exploits the heterogeneous roles of transformer head dimensions. - FourierAttention prioritizes local context using lower dimensions and captures long-range dependencies using upper dimensions projected onto orthogonal Fourier bases, approximating their temporal evolution with fixed-length spectral coefficients. - Evaluation on LLaMA models demonstrates that FourierAttention achieves state-of-the-art long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). - The proposed method is implemented using a custom Triton kernel, FlashFourierAttention, for efficient memory management and deployment without performance compromise. - Experimental results show that FourierAttention outperforms existing training-free KV cache compression methods on LongBench and NIAH, achieving the best long-context accuracy while maintaining lower memory consumption. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Configurable Preference Tuning with Rubric-Guided Synthetic Data](https://arxiv.org/abs/2506.11702) | vicgalle | - This paper introduces Configurable Preference Tuning (CPT), a novel framework that allows language models to dynamically adjust their behavior based on human-interpretable directives, without retraining. - CPT leverages synthetically generated preference data conditioned on system prompts derived from structured, fine-grained rubrics, which define desired attributes such as writing style. - The model is fine-tuned using a DPO-style objective with rubric-guided preference pairs, enabling it to modulate its outputs at inference time in response to system prompts. - Experiments demonstrate that CPT significantly improves the models' ability to adhere to system-prompted configurations, achieving higher accuracy and stronger rank correlations compared to baseline models. - CPT is shown to enhance other techniques, such as Best-of-N sampling, by improving generation efficiency and quality. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/vicgalle/configurable-preference-tuning) | [Link](https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences) |
| [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual
  Perception in VLMs](https://arxiv.org/abs/2506.10128) | Yuhang Zhou, Yongyuan Liang, Chao Feng, Zhengyuan Yang, Xiyao Wang | - ViCrit is a novel reinforcement learning proxy task designed to enhance visual perception in vision-language models (VLMs) by training them to identify subtle, synthetic visual hallucinations in image captions. - The task involves injecting minor visual errors into detailed image captions and training the VLM to pinpoint the erroneous span using a binary reward system, which is computationally efficient and unambiguous. - ViCrit training consistently improves performance on various vision-language benchmarks, including those involving abstract image reasoning and visual mathematics. - ViCrit-Bench, a new benchmark dataset with diverse image domains and fine-grained hallucination categories, is also introduced to evaluate and diagnose VLMs' visual perception abilities.  The results on ViCrit-Bench strongly correlate with overall VLM performance on general tasks. - The experiments demonstrate that ViCrit effectively enhances the fine-grained visual perception of VLMs, leading to improved performance across various downstream tasks. | ['Reinforcement Learning', 'Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/si0wang/ViCrit) | [Link](https://huggingface.co/collections/russwang/ViCrit), [Link](https://huggingface.co/datasets/zyang39/ViCrit-Train), [Link](https://huggingface.co/datasets/russwang/ViCrit-Bench) |
| [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity
  Dilemma of Embeddings](https://arxiv.org/abs/2506.08592) | Fandong Meng, Jiangnan Li, Mo Yu, Zhenlin Su, lxucs | - This paper introduces CapRetrieval, a new Chinese evaluation dataset for dense retrieval, focusing on the challenge of fine-grained semantic matching in image captions. - The dataset consists of image captions as passages and short phrases as queries, requiring fine-grained semantic understanding for accurate retrieval. - Zero-shot evaluation on CapRetrieval reveals limitations of existing text encoders in handling fine-grained details, regardless of model size or training data. - The authors propose data generation strategies using LLMs to enhance encoder training and address the identified granularity dilemma. - Finetuning with the proposed strategies improves performance on CapRetrieval, surpassing even large language models in zero-shot settings. | ['Question Answering'] | [Link](https://github.com/lxucs/CapRetrieval) | N/A |


## Papers for 2025-06-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical
  Reasoning](https://arxiv.org/abs/2506.09513) | Weiwen Xu, Xingyu Qian, Swrooy, 26hzhang, YuSun-AI | - ReasonMed, a new 370K medical reasoning dataset, is introduced.  It was generated using a multi-agent system and refined through a multi-stage process. - The dataset includes diverse medical insights and combines detailed chain-of-thought reasoning with concise answer summaries. - A new benchmark, ReasonMed-7B, is presented, outperforming the previous state-of-the-art by 4.17% and exceeding LLaMA3.1-70B on PubMedQA by 4.60%. - The study systematically investigates best practices for training medical reasoning models and finds that combining detailed Chain-of-Thought reasoning with concise answer summaries is most effective. - ReasonMed is publicly released to facilitate future research in medical reasoning. | ['Question Answering'] | N/A | N/A |
| [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993) | Jihye Park, Jaeeun Lee, paulcho98, jinlovespho, Min-Jaewon |  - This paper introduces a novel task called Text-Aware Image Restoration (TAIR), which aims to simultaneously recover visual content and textual fidelity in degraded images.  - A large-scale benchmark dataset called SA-Text (100K high-quality images with dense text annotations) is created to facilitate research on TAIR.  - A multi-task diffusion model named TeReDiff is proposed. TeReDiff integrates internal features from diffusion models into a text-spotting module, allowing both components to benefit from joint training.  - TeReDiff outperforms existing state-of-the-art restoration methods on SA-Text, achieving significant improvements in text recognition accuracy.  - Extensive experiments demonstrate that the proposed model consistently achieves higher performance in both perceptual quality and character legibility compared to existing methods. | ['Computer Vision', 'Image-to-Image', 'Multimodal'] | [Link](https://cvlab-kaist.github.io/TAIR/) | N/A |
| [Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture
  without Training](https://arxiv.org/abs/2506.10952) | Xipeng Qiu, Lu Wang, Howe77, mzzhang |  - This paper introduces DOMAIN2VEC, a novel method that vectorizes datasets into linear combinations of meta-domains to identify optimal data mixtures for large language models (LLMs). - DOMAIN2VEC uses a classifier to decompose datasets into domain vectors representing distributions over meta-domains, enabling training-free identification of optimal mixtures under the Distribution Alignment Assumption (DA2). - The method is shown to enhance downstream task performance with minimal computational overhead; achieving the same validation loss as the original mixture using only 51.5% of the computational cost, and a 2.83% average performance improvement under equal compute budget. - DOMAIN2VEC seamlessly integrates with previous methods, improving efficiency and scalability by modeling the relationship between domain vectors and LLM performance. - Extensive experiments demonstrate that DOMAIN2VEC effectively finds data mixtures that enhance downstream task performance. | ['Natural Language Processing'] | N/A | N/A |
| [Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable
  Task Experts](https://arxiv.org/abs/2506.10357) | Weili Guan, Gongwei Chen, Rui Shao, Yuquan Xie, Zaijing Li | - This paper introduces Optimus-3, a generalist multimodal agent for Minecraft that integrates perception, planning, action, grounding, and reflection capabilities. - Optimus-3 utilizes a Mixture-of-Experts (MoE) architecture with task-level routing to mitigate interference among heterogeneous tasks and improve scalability. - The model employs a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance reasoning capabilities and adapt to the visual diversity of Minecraft. - Experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a range of Minecraft tasks. - The authors address the challenges of insufficient domain-specific data, task interference, and visual diversity in open-world environments through three key contributions: a knowledge-enhanced data generation pipeline, a task-level routing MoE architecture, and a multimodal reasoning-augmented reinforcement learning approach. | ['Reinforcement Learning', 'Multimodal', 'Robotics'] | [Link](https://cybertronagent.github.io/Optimus-3.github.io/) | N/A |
| [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/abs/2506.10960) | Bozhong Tian, Siyuan Cheng, Kangwei Liu, Jasonchen123, Ningyu | - This paper introduces ChineseHarm-Bench, a new benchmark dataset for Chinese harmful content detection, addressing the scarcity of such resources. - The dataset comprises six categories of harmful content (gambling, pornography, abuse, fraud, illicit ads, and non-violation), with each category containing approximately 15,000 real-world examples and a total of 52,000 non-violation samples. -  A knowledge rule base is created during the annotation process, providing explicit expert knowledge to assist LLMs in detection and improve resource efficiency for smaller models. - A knowledge-augmented baseline is proposed to enhance the performance of smaller LLMs by integrating both annotated knowledge rules and implicit knowledge from large language models, achieving performance comparable to state-of-the-art LLMs. -  Extensive experiments demonstrate the effectiveness of the proposed approach, particularly in improving the performance of smaller models and addressing the challenges of Chinese harmful content detection. | ['Text Classification'] | [Link](https://github.com/zjunlp/ChineseHarm-bench) | N/A |
| [Magistral](https://arxiv.org/abs/2506.10910) | Gabrielle Berrada, Andy Lo, Albert Q. Jiang, Abhinav Rastogi, Mistral-AI | - Introduced Magistral, Mistral's first reasoning model, trained using a novel reinforcement learning pipeline. - The pipeline does not rely on existing implementations or RL traces, instead relying solely on Mistral's own models and infrastructure. - Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, achieved a nearly 50% boost in AIME-24 (pass@1) over the initial Mistral Medium 3 checkpoint, demonstrating the effectiveness of the approach. - The model maintains or improves multimodal understanding, instruction following, and function calling. - Magistral Small (Apache 2.0) was open-sourced, further including cold-start data from Magistral Medium. | ['Reinforcement Learning', 'Multimodal', 'Question Answering'] | N/A | [Link](https://huggingface.co/mistralai/Magistral-Small-2506) |
| [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967) | Ömer Faruk Akgül, Julian Asilis, willieneis, deqing, upup-ashton-wang | This paper introduces Resa, a family of 1.5B reasoning language models trained via a novel Sparse Autoencoder Tuning (SAE-Tuning) procedure.  SAE-Tuning first trains a sparse autoencoder (SAE) to capture reasoning abilities from a source model and then uses the trained SAE to guide standard supervised fine-tuning to elicit these abilities in a target model. Resa models achieve comparable reasoning performance to reinforcement learning (RL)-trained counterparts while reducing training costs by over 2000x and training time by over 450x.  The extracted reasoning abilities demonstrate generalizability and modularity, meaning they transfer across different datasets and models without retraining.  The SAE-Tuning method offers increased transparency into the model's reasoning process. | ['Natural Language Processing', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/shangshang-wang/Resa) | [Link](https://huggingface.co/Resa-Yi) |
| [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344) | Chunluan Zhou, Chuanyang Zheng, Cheng Zou, Biao Gong, Inclusion AI |  - Ming-Omni is a unified multimodal model that processes images, text, audio, and video, exhibiting strong performance in speech and image generation. - It uses a Mixture-of-Experts (MoE) architecture with modality-specific routers for efficient multimodal input processing and fusion. - The model supports audio and image generation via an advanced audio decoder and Ming-Lite-Uni, enabling versatile tasks like context-aware chatting and image editing. - Experimental results demonstrate Ming-Omni's superior performance in various tasks, matching GPT-4's modality support. - All code and model weights are open-sourced to encourage further research and development. | ['Multimodal'] | [Link](https://github.com/inclusionAI/Ming/tree/main) | N/A |
| [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time
  Techniques](https://arxiv.org/abs/2506.08060) | codelion | - This paper presents a novel method for approximating the capabilities of fine-tuned transformer models using inference-time techniques, without altering the model parameters. - The approach leverages in-context learning (ICL), where the model is prompted with a subset of the fine-tuning dataset to elicit desired behaviors. - The authors provide theoretical proofs demonstrating that this approach can accurately approximate fine-tuned behavior under idealized conditions, with minimal data requirements. - These results are extended to more realistic scenarios with finite context lengths and partial dataset access, with dataset size bounds provided for text generation and linear classification tasks. - The study establishes a theoretical foundation for resource-efficient deployment of large language models, bridging the gap between theory and practice through practical techniques such as retrieval-augmented generation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Compound AI Systems Optimization: A Survey of Methods, Challenges, and
  Future Directions](https://arxiv.org/abs/2506.08234) | Guan-Bo Yang, Jui-Chao Lu, Mei-Yi Liu, Guan-Ting Yi, Yu-Ang Lee | - This paper surveys recent advances in optimizing compound AI systems, which are systems integrating multiple components like LLMs, simulators, and code interpreters. - It proposes a 2x2 taxonomy of existing optimization methods based on structural flexibility and learning signals (numerical or language-based). - The paper identifies four key dimensions for analyzing compound AI system optimization methods: structural flexibility, learning signals, component options, and system representations. - It highlights open research challenges, such as manual hyperparameter configuration, excessive computation burden, and limited experimental scope. - The authors suggest future directions, including developing automated optimization algorithms, reducing computational overhead, expanding experimental scope, and providing more theoretical guarantees for existing methods. | ['Natural Language Processing'] | [Link](https://github.com/MiuLab/AISysOpt-Survey) | N/A |
| [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795) | Shu Wu, Mengqi Zhang, Acruxos | - This paper introduces Rank-One Concept Redirection (ROCR), a novel training-free method for LLM unlearning that addresses the issue of form-dependent bias. - ROCR modifies model parameters to redirect the activation of dangerous concepts to harmless ones, improving unlearning effectiveness and generalization. - The method is shown to significantly outperform existing unlearning methods in terms of both unlearning effectiveness and knowledge preservation across various downstream tasks. - Extensive experiments demonstrate ROCR's superior performance and its ability to generate highly natural outputs, showcasing its potential as a practical and robust solution for LLM unlearning. - A new benchmark, ORT, is introduced to systematically evaluate the robustness of unlearning methods against variations in knowledge expression, highlighting the prevalence of form-dependent bias. | ['Natural Language Processing'] | N/A | N/A |
| [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950) | Nancy F. Chen, Kenji Kawaguchi, Ngoc-Hai Nguyen, Duy Dinh, Do Xuan Long | This paper introduces a novel property-and-human-centric framework for evaluating the quality of natural language prompts, identifying 21 key properties across six dimensions.  A meta-analysis of 150+ prompting-related papers reveals significant research gaps and imbalances across models and tasks.  Correlations among properties in high-quality prompts are analyzed to derive practical prompting recommendations.  Empirical exploration on reasoning tasks demonstrates that single-property enhancements often yield greater impact than multi-property enhancements, and instruction-tuning on property-enhanced prompts leads to superior reasoning models.  Finally, it introduces various open questions regarding model-specific impacts and task-specific versus universal properties. | ['Natural Language Processing'] | N/A | N/A |
| [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373) | Hyung Il Koo, Minjae Lee, Wonjun Kang, Ethan Ewer, Kevin Galim |  - This paper introduces a novel framework for approximate inference in Large Language Models (LLMs) that leverages smaller "draft" models to predict token and key-value pair importance more accurately than existing methods. - Two instantiations of this framework are presented: SpecKV, for effective KV cache dropping, and SpecPC, for prompt compression, both showing strong correlations between the draft and target models. - SpecKV and SpecPC consistently outperform existing baselines on long-context benchmarks, achieving higher accuracy while maintaining improvements in memory usage, latency, and throughput. - Theoretical analyses support the methods' effectiveness, showing a strong correlation between the attention patterns of draft and target models. - The proposed methods are evaluated extensively on multiple benchmarks, demonstrating consistent improvements in accuracy over existing methods. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/furiosa-ai/draft-based-approx-llm) | N/A |
| [MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness
  Against VLM-based Attacks](https://arxiv.org/abs/2506.05982) | Yiren Song, Xin Wei, Yule Xue, Zonglin Wu | - The paper introduces MCA-Bench, a comprehensive multimodal benchmark for evaluating CAPTCHA robustness against vision-language model (VLM)-based attacks. - MCA-Bench integrates various CAPTCHA types into a single evaluation protocol, leveraging a shared VLM backbone to fine-tune specialized cracking agents for each CAPTCHA category. - Extensive experiments demonstrate that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs and provides quantitative analysis of challenge complexity, interaction depth, and model solvability. - Based on the experimental findings, the paper proposes three actionable design principles for next-generation CAPTCHAs to enhance security against VLM-based attacks. - Datasets and code are publicly available for further research and community collaboration. | ['Multimodal'] | [Link](https://github.com/noheadwuzonglin/MCA-Bench) | [Link](https://www.kaggle.com/datasets/luffy798/mca-benchmultimodal-captchas) |
| [Discovering Hierarchical Latent Capabilities of Language Models via
  Causal Representation Learning](https://arxiv.org/abs/2506.10378) | Hanlin Zhang, Sham Kakade, Vasilis Syrgkanis, Jikai Jin |  - This paper introduces a novel causal representation learning framework for evaluating language model capabilities.  - The framework models observed benchmark performance as a linear transformation of latent capability factors, identified as causally interrelated after controlling for base model variations.  - Applying this approach to a large dataset of over 1500 models, the authors identify a three-node linear causal structure that reliably explains performance variations across six benchmarks from the Open LLM Leaderboard.  - The causal structure reveals a clear causal direction from general problem-solving capabilities through instruction-following proficiency to mathematical reasoning ability.  - This work underscores the importance of controlling for base model variations during evaluation to accurately uncover causal relationships between latent model capabilities. | ['Natural Language Processing'] | [Link](https://github.com/hlzhang109/causal-eval) | [Link](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) |


## Papers for 2025-06-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Autoregressive Adversarial Post-Training for Real-Time Interactive Video
  Generation](https://arxiv.org/abs/2506.09350) | Yuxi Ren, Jianwen Jiang, Hao He, Ceyuan Yang, Shanchuan Lin | - This paper introduces Autoregressive Adversarial Post-Training (AAPT), a novel method that transforms pre-trained latent video diffusion models into real-time interactive video generators. - AAPT employs a single neural function evaluation (1NFE) to autoregressively generate latent frames, enabling real-time streaming and interactive control. - The model architecture utilizes a causal transformer with block causal attention and a KV cache for efficient one-step generation. - Experimental results demonstrate that the proposed 8B parameter model achieves real-time 24fps video generation at 736x416 resolution on a single H100 GPU, outperforming existing state-of-the-art methods. - The effectiveness of the adversarial training paradigm and long-video training is showcased through experiments on pose-conditioned virtual human generation and camera-controlled world exploration. | ['Image-to-Video', 'Text-to-Video', 'Multimodal'] | N/A | N/A |
| [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790) | Weihua Luo, Longyue Wang, Xue Yang, Yiyu Wang, Zhenran Xu | - ComfyUI-R1 is a large reasoning model for automated workflow generation in ComfyUI, achieving a 97% format validity rate and surpassing previous state-of-the-art methods. - The model uses a two-stage training framework: (1) cold-start CoT fine-tuning and (2) reinforcement learning with a rule-metric hybrid reward function. - ComfyUI-R1 outperforms existing methods such as GPT-40 and Claude series in terms of format validity rate, node-level and graph-level F1 scores and pass rate on ComfyBench. - The model leverages a code-based workflow representation for superior performance compared to a JSON-based representation. - Qualitative comparison demonstrates ComfyUI-R1's capability in handling complex workflows and ensuring alignment with user instructions. | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | [Link](https://github.com/AIDC-AI/ComfyUI-Copilot) | N/A |
| [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889) | Yu Cheng, Yuqing Xia, Shijie Cao, Shuming Guo, Yizhao Gao | - SeerAttention-R is a novel sparse attention framework designed to enhance the efficiency of long decoding in reasoning models. - It extends SeerAttention by removing query pooling and introducing modifications to support autoregressive decoding, improving efficiency. - The model uses a lightweight plug-in gating mechanism that can be easily integrated into existing pretrained models without modifying original parameters. - Experiments demonstrate that SeerAttention-R achieves near-lossless accuracy with large sparse attention blocks (64/128), outperforming existing methods. - An optimized sparse decoding kernel achieves near-theoretical speedups, demonstrating the efficiency and scalability of the proposed framework. | ['Natural Language Processing'] | [Link](https://github.com/microsoft/SeerAttention) | [Link](string) |
| [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003) | Mouxiang Chen, Jian Yang, Min Yang, Jiaxi Yang, Lei Zhang | SWE-Flow is a novel data synthesis framework that leverages Test-Driven Development (TDD) to automatically generate software engineering data.  The core of SWE-Flow is the construction of a Runtime Dependency Graph (RDG) which captures function interactions, enabling the generation of a structured development schedule. At each step, SWE-Flow produces a partial codebase, corresponding unit tests, and code modifications, resulting in fully verifiable TDD tasks.  SWE-Flow generated 16,061 training and 2,020 test instances from real-world projects, creating the SWE-Flow-Bench benchmark.  Experiments showed fine-tuning on this dataset significantly improved performance in TDD-based coding tasks. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Hambaobao/SWE-Flow) | N/A |
| [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio
  Conditions](https://arxiv.org/abs/2506.09984) | Gaojie Lin, Chao Liang, Jianwen Jiang, Jiaqi Yang, Zhenzhi Wang | - InterActHuman is a novel framework for multi-concept human animation that uses layout-aligned audio conditions. - It addresses the limitations of existing methods by introducing an attention module to explicitly predict spatial locations of concepts and bind audio conditions to corresponding regions. - The model is trained using a large dataset of human-centric videos with over two million video-entity pairs. - InterActHuman outperforms existing methods in terms of lip synchronization, motion diversity, and overall video quality, as demonstrated in both qualitative and quantitative evaluations. - The framework introduces a novel iterative mask prediction strategy to achieve accurate spatial alignment of multi-modal conditions during inference. | ['Text-to-Video', 'Multimodal', 'Audio'] | N/A | N/A |
| [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001) | Bernhard Schölkopf, Maximilian Dax, Tim Z. Xiao, Simon Buchholz, Zeju Qiu |  - This paper introduces POET, a novel reparameterized training algorithm for LLMs that uses orthogonal equivalence transformation to optimize neurons.  - POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix, which preserves the spectral properties of weight matrices.  - The proposed method demonstrates improved generalization and stability compared to standard AdamW optimization, achieving state-of-the-art validation perplexity results in several large-scale LLaMA model experiments.  - POET introduces two levels of approximations for efficient training: stochastic primitive optimization and approximate orthogonality via Cayley-Neumann parameterization.  - Extensive experiments demonstrate POET's effectiveness and scalability in training LLMs. | ['Natural Language Processing'] | [Link](https://github.com/jiaweizzhao/GaLore) | [Link](https://huggingface.co/) |
| [MIRAGE: Multimodal foundation model and benchmark for comprehensive
  retinal OCT image analysis](https://arxiv.org/abs/2506.08900) | Taha Emre, Ronald Fecso, Emese Sükei, Botond Fazekas, José Morano |  - The paper introduces MIRAGE, a novel multimodal foundation model for retinal OCT/SLO image analysis, trained using a paired multimodal Masked Autoencoder (MAE) approach.  - MIRAGE significantly outperforms state-of-the-art foundation models on both classification and segmentation tasks, showcasing its robustness and generalization capabilities.  - A new comprehensive evaluation benchmark is also proposed for validating foundation models in retinal OCT/SLO analysis, including classification and segmentation tasks across multiple datasets.  - The MIRAGE model is based on a Vision Transformer encoder with modality-specific linear projection layers and Transformer decoders, effectively utilizing complementary information from OCT and SLO images.  - The superior performance of MIRAGE across multiple datasets highlights its suitability for developing robust AI systems for retinal image analysis, making it a valuable tool for clinicians. | ['Image Segmentation', 'Image Classification', 'Multimodal'] | [Link](https://github.com/j-morano/MIRAGE) | N/A |


## Papers for 2025-06-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Geopolitical biases in LLMs: what are the "good" and the "bad" countries
  according to contemporary language models](https://arxiv.org/abs/2506.06751) | Dmitrii Korzh, tlenusik, apanc, IvanLazichny, msalnikov | - This paper introduces a novel dataset containing neutral event descriptions and contrasting viewpoints from different countries to evaluate geopolitical biases in LLMs. - The findings reveal significant geopolitical biases in LLMs, with models exhibiting preferences for specific national narratives. - Simple debiasing prompts show limited effectiveness in mitigating these biases. - Experiments manipulating participant labels demonstrate models' sensitivity to attribution, sometimes amplifying biases or highlighting inconsistencies. - The study offers a framework and dataset for future research into geopolitical bias in LLMs. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/AIRI-Institute/geopolitical_llm_bias) | N/A |
| [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic
  Sampling](https://arxiv.org/abs/2506.08672) | Jiaqi Li, Yang Liu, zlzheng |  - This paper introduces RuleReasoner, a novel method for rule-based reasoning that uses reinforcement learning and domain-aware dynamic sampling.  - RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards, which improves both training efficiency and task performance.  - Experiments show that RuleReasoner outperforms existing large reasoning models (LRMs) by a significant margin on both in-distribution (ID) and out-of-distribution (OOD) benchmarks.  - The model achieves an average improvement of 4.1% on eight ID tasks and 10.4% on three OOD tasks over OpenAI-01.  - RuleReasoner demonstrates its effectiveness and efficiency by reducing training steps while achieving comparable performance to other state-of-the-art methods. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/bigai-nlco/RuleReasoner) | [Link](https://huggingface.co/RuleReasoner) |
| [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927) | Alex Gu, Tony Xia, Jikai Jin, Luna Lyu, Jiayi Sheng | This paper introduces INEQMATH, a new dataset of Olympiad-level inequality problems, designed to evaluate large language models' (LLMs) ability to prove inequalities.  The authors propose a novel LLM-as-judge framework for evaluation, measuring both final-answer accuracy and step-wise soundness. The experiments show a significant gap between final-answer accuracy and overall proof correctness in leading LLMs. The findings highlight promising research directions, such as theorem-guided reasoning and self-refinement. The paper also presents an informal yet verifiable task formulation for inequality proving, decomposing it into two automatically checkable subtasks.  The dataset and evaluation framework are made publicly available. | ['Natural Language Processing'] | [Link](https://ineqmath.github.io/) | [Link](https://huggingface.co/spaces/AI4Math/IneqMath-Leaderboard) |
| [Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error
  Diagnosis in GUI Automation](https://arxiv.org/abs/2506.04614) | Junyang Wang, Haowei Liu, Haiyang Xu, Xi Zhang, Yuyang Wanyan | - The paper introduces GUI-Critic-R1, a pre-operative critic model for GUI automation that uses a Suggestion-aware Group Relative Policy Optimization (S-GRPO) strategy to enhance the reliability of its feedback. - GUI-Critic-R1 incorporates a novel suggestion reward to improve the quality of the model's feedback, helping to prevent errors before they occur. - The model was evaluated on both mobile and web domains, outperforming existing MLLMs on a GUI automation benchmark. - A reasoning-bootstrapping based data collection pipeline was developed to create the GUI-Critic-Train and GUI-Critic-Test datasets, addressing the lack of publicly available GUI critic data. - Experiments demonstrated that GUI-Critic-R1 offers significant advantages in critic accuracy and operational efficiency compared to current MLLMs. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/X-PLUG/MobileAgent/tree/main/GUI-Critic-R1) | N/A |
| [Aligning Text, Images, and 3D Structure Token-by-Token](https://arxiv.org/abs/2506.08002) | Georgia Gkioxari, Vansh Tibrewal, Aadarsh Sahoo |  - Kyvo is a novel unified multimodal large language model that aligns text, images, and structured 3D scenes in a token-by-token manner. - The model architecture is a decoder-only transformer that uses a structured 3D modality representing scenes as lists of objects with attributes such as shape, type, position, pose, and size. - Kyvo achieves state-of-the-art performance on four core 3D tasks including rendering, recognition, instruction following, and question answering. - The model generalizes well to complex object shapes and real-world scenarios demonstrating effectiveness on real-world datasets. - Kyvo also uses a vector-quantized 3D shape representation and significantly reduces sequence length compared to text-only tokenizers. | ['Multimodal', 'Image-to-3D', 'Text-to-3D', 'Image-Text-to-Text', 'Visual Question Answering', 'Question Answering', 'Text-to-Image', 'Image-to-Image', 'Image-Text-to-Text', 'Text Generation', 'Image-Text-to-Text', 'Object Detection', 'Image Classification'] | N/A | N/A |
| [MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient
  Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.05928) | Wenqiao Zhang, Rolan Yan, Hongyang He, Tianwei Lin, cajie | - This paper introduces a novel heterogeneous Mixture-of-Adapters (MoA) method for parameter-efficient fine-tuning of large language models (LLMs). - The MoA architecture dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to enhance knowledge transfer to downstream tasks. -  MoA supports two variants: Soft MoA (weighted fusion of all expert outputs) and Sparse MoA (sparsely activates experts based on contribution). - Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LORA methods in both performance and parameter efficiency. - The project's code is available on GitHub. | ['Natural Language Processing'] | [Link](https://github.com/DCDmllm/MoA) | N/A |
| [Institutional Books 1.0: A 242B token dataset from Harvard Library's
  collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300) | Kristi Mukk, Jack Cushman, John Hess, Catherine Brobston, Matteo Cargnelutti | This research paper introduces Institutional Books 1.0, a 242B token dataset comprising public domain books from Harvard Library.  The dataset undergoes several processing steps, including OCR extraction, text analysis, and rights determination, and is released with comprehensive metadata.  The authors created a topic classification model (achieving 97.8% accuracy during benchmarking) to categorize the volumes into 20 high-level topics.  They also offer post-processed OCR text alongside the original.  The goal is to create a publicly available, high-quality dataset to promote further research and development in LLMs. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/instdin/institutional-books-1-pipeline), [Link](https://github.com/instdin/institutional-books-1-0) | [Link](https://huggingface.co/datasets/instdin/institutional-books-1.0), [Link](https://huggingface.co/instdin/institutional-books-topic-classifier-bert) |
| [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047) | Roozbeh Yousefzadeh, Pengyi Zhai, Zijin Feng, Yu Xuejun, Jianyuan1 | - This paper introduces Mathesis, a novel end-to-end theorem proving pipeline that processes informal problem statements in natural language. - Mathesis-Autoformalizer, a reinforcement learning-based autoformalizer, enhances the formalization ability of natural language problems and outperforms existing methods by 22% in pass rate on the Gaokao-Formal benchmark. - A novel LeanScorer framework provides nuanced formalization quality assessment. - Mathesis-Prover generates formal proofs from formalized statements, achieving state-of-the-art accuracy on MiniF2F (64%) and Gaokao-Formal (18%). - The Gaokao-Formal benchmark, comprising 488 complex problems from China's national college entrance exam, is introduced to evaluate the real-world applicability of the system. | ['Natural Language Processing'] | N/A | N/A |
| [RKEFino1: A Regulation Knowledge-Enhanced Large Language Model](https://arxiv.org/abs/2506.05700) | Jeff Zhao, Ruoyu Xiang, Yueru He, YanAdjeNole | - This paper introduces RKEFino1, a regulation knowledge-enhanced large language model built upon Fino1 and fine-tuned with domain knowledge from XBRL, CDM, and MOF. - The model is evaluated on three tasks: knowledge-based QA, mathematical reasoning QA, and a novel Numerical NER task covering financial entities in sentences and tables. - RKEFino1 significantly outperforms Fino1 across all three tasks, demonstrating the effectiveness of incorporating regulatory knowledge. - The improvement is particularly noticeable in tasks requiring precise answers or detailed explanations of financial regulations. - The authors have released their model on Hugging Face. | ['Question Answering'] | N/A | [Link](https://huggingface.co/YanAdjeNole/RKEFino1-14B) |
| [QQSUM: A Novel Task and Model of Quantitative Query-Focused
  Summarization for Review-based Product Question Answering](https://arxiv.org/abs/2506.04020) | Zhuang Li, Minh Ngoc Dinh, Xiuzhen Zhang, An Quang Tang | - This paper introduces a novel task, Quantitative Query-Focused Summarization (QQSUM), aiming to generate comprehensive answers to product questions by summarizing diverse customer opinions and quantifying their prevalence. - The proposed model, QQSUM-RAG, extends the Retrieval-Augmented Generation (RAG) framework by integrating KP-oriented retrieval and summarization, ensuring the generation of diverse and representative summaries. - QQSUM-RAG jointly trains a KP-oriented retriever and a KP summary generator using a co-training strategy, achieving superior performance in both textual quality and quantification accuracy compared to state-of-the-art RAG baselines. - The model leverages few-shot learning, addressing the challenge of limited training data for this specialized task, and utilizes a carefully curated dataset of queries with KPs and their prevalence quantification for few-shot learning. - Experimental results demonstrate that QQSUM-RAG significantly outperforms existing RAG baselines, showcasing improvement in textual similarity with ground-truth KPs and quantification performance over state-of-the-art systems. | ['Question Answering', 'Summarization'] | [Link](https://github.com/antangrocket1312/QQSUMM) | N/A |


## Papers for 2025-06-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007) | Tianzhu Ye, Qingxiu Dong, frontierai, YaoTang23, unilm | - This paper introduces Reinforcement Pre-Training (RPT), a novel scaling paradigm that reframes next-token prediction as a reasoning task trained using reinforcement learning with verifiable rewards. - RPT offers a scalable and general-purpose method to leverage vast amounts of text data for general-purpose reinforcement learning, unlike previous approaches that relied on domain-specific annotated data. - Experimental results demonstrate that RPT significantly improves the accuracy of next-token prediction and provides a stronger pre-trained foundation for subsequent reinforcement fine-tuning, achieving state-of-the-art results on various downstream tasks. - The scaling curves show that RPT's performance improves consistently with increased training compute across different data difficulty levels. - This work positions RPT as a promising scaling paradigm to advance language model pre-training. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical
  Understanding and Reasoning](https://arxiv.org/abs/2506.07044) | 26hzhang, gowitheflow, Jianyu, kenchan0226, xww033 | This paper introduces LINGSHU, a new multimodal large language model (MLLM) specialized for medical applications.  LINGSHU's architecture is based on the Qwen2.5-VL model and undergoes multi-stage training, incorporating medical image-text pairs, medical texts, and general-domain data.  The model is evaluated on various benchmarks, demonstrating superior performance over existing open-source models in most cases.  Further, the authors introduce MEDEVALKIT, a unified evaluation framework for medical multimodal models. The use of reinforcement learning with verifiable rewards is also explored to improve the model's reasoning capabilities. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Text Generation', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/lingshu-medical-mllm) |
| [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900) | Yuxuan Li, MiniCPM Team, BigDong, guojunshaoyao, xcjthu |  - This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed for end-side devices.  - MiniCPM4 achieves efficiency through innovations in model architecture (InfLLM v2, a trainable sparse attention mechanism), training data (UltraClean data filtering and UltraChat v2 fine-tuning dataset), training algorithms (ModelTunnel v2 and chunk-wise rollout for RL), and inference systems (CPM.cu). - MiniCPM4 outperforms similar-sized open-source models on multiple benchmarks, demonstrating significant speed improvements over Qwen3-8B for long sequences. - The model is available in 0.5B and 8B parameter versions, suitable for diverse on-device applications. - MiniCPM4 successfully powers diverse applications, showcasing its broad usability through its application in generating surveys and tool use with model context protocols. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Summarization', 'Question Answering'] | [Link](https://github.com/openbmb/minicpm) | [Link](https://huggingface.co/openbmb/MiniCPM4-8B), [Link](https://huggingface.co/openbmb/MiniCPM4-0.5B) |
| [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety
  Assurance](https://arxiv.org/abs/2506.06444) | Hanghang Tong, Jingrui He, Tianxin Wei, Gaotang Li, Ruizhong Qiu | - This paper introduces SAFFRON, a novel inference scaling paradigm for enhancing the safety of LLMs. - SAFFRON employs a multifurcation reward model (MRM) to significantly reduce the number of reward model calls during inference, addressing the exploration-efficiency dilemma. - The MRM is trained using a partial supervision objective and incorporates a conservative exploration constraint to prevent out-of-distribution explorations. - Experiments demonstrate that SAFFRON outperforms existing advanced inference scaling methods on challenging jailbreaking attacks, achieving considerably lower attack success rates with lower computational costs. - The authors release the trained MRM (SAFFRON-1) and a token-level safety reward dataset (Safety4M) to promote future research in LLM safety. | ['Natural Language Processing'] | [Link](https://github.com/q-rz/saffron) | N/A |
| [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491) | Rui Tang, Chuan Fang, Junhao Zhong, bertjiazheng, ysmao | - This paper introduces SPATIALLM, a large language model designed for processing 3D point cloud data and generating structured 3D scene understanding outputs, including architectural elements and object bounding boxes. - The model uses a standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs, unlike previous methods that rely on task-specific network designs. - SPATIALLM is trained on a large-scale, high-quality synthetic dataset of indoor scenes with 3D annotations. - The paper shows that SPATIALLM achieves state-of-the-art performance in layout estimation and competitive results in 3D object detection on public benchmarks. - The results demonstrate a feasible path for enhancing the spatial understanding capabilities of modern LLMs for augmented reality and embodied robotics. | ['Text-to-3D', 'Object Detection', 'Image-to-3D', 'Multimodal'] | [Link](https://manycore-research.github.io/SpatialLM) | N/A |
| [Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal
  Learning](https://arxiv.org/abs/2506.06205) | Yansheng Wang, Ziyang Liu, Jiaxin Hu, Peiyu He, sc-bd | - This paper introduces Astra, a novel dual-model architecture for mobile robot navigation that uses a multimodal large language model (MLLM) for high-level tasks and a multitask network for low-level tasks. - Astra-Global, the MLLM, handles goal and self-localization using a hybrid topological-semantic graph as a global map and outperforms traditional visual place recognition methods. - Astra-Local, the multitask network, manages local path planning and odometry estimation using a 4D spatial-temporal encoder and a novel masked ESDF loss to minimize collisions. - When deployed on real robots, Astra achieves a high end-to-end mission success rate across diverse indoor environments. - The experiments demonstrate that Astra outperforms existing methods in terms of goal and self-localization accuracy and path planning efficiency. | ['Robotics', 'Multimodal', 'Reinforcement Learning'] | [Link](https://astra-mobility.github.io/) | N/A |
| [Through the Valley: Path to Effective Long CoT Training for Small
  Language Models](https://arxiv.org/abs/2506.07712) | Wei Lu, Jiaxi Li, Albus-Chen, RogerLos | - This paper introduces the concept of "Long CoT Degradation", a phenomenon where small language models (SLMs) trained on extensive long chain-of-thought (CoT) data experience significant performance drops. - The authors demonstrate that this degradation is prevalent across various SLMs and is attributed to error accumulation in longer reasoning traces. - They propose two hypotheses to explain this phenomenon: early adoption of surface-level reasoning patterns leading to verbose outputs and error accumulation in longer responses. - Their empirical findings show that sufficiently scaled supervised fine-tuning (SFT) can alleviate Long CoT Degradation and improve the efficiency of subsequent reinforcement learning (RL). - The research provides practical guidance for building more effective small-scale reasoning models by carefully considering the scale of long CoT data during training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298) | Jennifer J. Sun, Yahya Satter, Zhaolin Gao, sarahdean, DaiYijia |  - This paper demonstrates that pre-trained large language models (LLMs) can effectively model data generated by hidden Markov models (HMMs) through in-context learning.  - Using a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum.   - Novel scaling trends are identified, influenced by HMM properties, and theoretical conjectures are proposed to explain these trends.   - Practical guidelines are offered for using in-context learning as a diagnostic tool, with real-world applications in animal decision-making achieving performance comparable to models built by human experts.  - This is the first demonstration that in-context learning in LLMs can learn and predict HMM-generated sequences. | ['Natural Language Processing'] | [Link](https://github.com/DaiYijia02/icl-hmm) | N/A |
| [The Illusion of Thinking: Understanding the Strengths and Limitations of
  Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941) | Samy Bengio, Maxwell Horton, Keivan Alizadeh, Iman Mirzadeh, parshinsh | - This paper investigates the strengths and limitations of Large Reasoning Models (LRMs) using controlled puzzle environments. - The study reveals three performance regimes: standard LLMs outperform LRMs at low complexity, LRMs excel at moderate complexity, and both collapse at high complexity. - The findings show that LRMs exhibit counterintuitive scaling limits: their reasoning effort increases up to a point then declines despite having an adequate token budget. - The authors analyze reasoning traces in detail, finding that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. - The research highlights both strengths and limitations of existing LRMs, raising crucial questions about their true reasoning capabilities. | ['Natural Language Processing'] | N/A | N/A |
| [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large
  Language Models](https://arxiv.org/abs/2506.07463) | Yang Yu, Jijie Li, Yonghua, ldwang, ZacLiu | - This paper introduces CCI4.0, a large-scale bilingual (Chinese and English) pre-training dataset designed to enhance reasoning in large language models. - CCI4.0 comprises two sub-datasets: CCI4.0-M2-Base (a 35TB corpus combining various web corpora, mathematical, wiki, and code data) and CCI4.0-M2-CoT (4.5 billion chain-of-thought templates). - A novel data processing pipeline was developed, including deduplication, quality scoring, and domain-aware fluency filtering, to ensure high data quality. - Experiments show CCI4.0 consistently outperforms existing datasets across various downstream tasks, especially in mathematics and code-related tasks. - The results highlight the importance of high-quality, diverse, and reasoning-focused data in improving LLM performance. | ['Natural Language Processing'] | N/A | N/A |
| [Well Begun is Half Done: Low-resource Preference Alignment by
  Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434) | Tianyu Liu, Yuxuan Fan, Wen Luo, SylvainWei, songff | - The paper introduces a novel framework called Weak-to-Strong Decoding (WSD) for low-resource preference alignment in large language models (LLMs). - WSD uses a small, pre-trained model to generate well-aligned response beginnings, which are then extended by a larger base LLM. - A new dataset, GenerAlign, was created to fine-tune the small draft model, enhancing its alignment capabilities. - Experiments demonstrated that WSD outperforms several baseline methods on various benchmarks without degrading performance on downstream tasks. - The effectiveness of WSD is further analyzed through ablation studies focusing on the impact of hyperparameters such as window size, threshold, and maximum draft length. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection
  Behavior](https://arxiv.org/abs/2506.08012) | Lewei Lu, Jiaheng Yu, Bo Wang, Shengnan Ma, Penghao Wu | This paper introduces GUI-Reflection, a novel framework that enhances multimodal GUI models by integrating self-reflection and error correction capabilities.  GUI-Reflection leverages three dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning, and online reflection tuning.  The framework incorporates a GUI-Reflection Task Suite to explicitly train reflection-oriented abilities and automatically generates reflection data from successful trajectories.  Experiments show that GUI-Reflection significantly improves the model's ability to recover from errors and adapt to challenging tasks, achieving a 34.72% success rate on level-2 tasks in online evaluation. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://penghao-wu.github.io/GUI_Reflection/) | N/A |
| [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309) | Alicia Sun, Vera Yan, Kai Sun, Yifan Ethan Xu, MaggieHuang | - This paper introduces ConfQA, a novel fine-tuning strategy for Large Language Models (LLMs) designed to reduce hallucination rates. - ConfQA achieves this by training the LLM to respond with the answer only when it is highly confident; otherwise, it admits uncertainty. - The method incorporates a dampening prompt ("Answer only if you are confident") and uses simple factual statements from knowledge graphs for training, leading to robust generalization. - Experiments demonstrate a significant reduction in hallucination rate (from 20-40% to under 5%) across multiple benchmarks. - ConfQA is further integrated into a Dual Neural Knowledge framework that combines internal and external knowledge sources to improve accuracy while reducing latency. | ['Question Answering'] | N/A | N/A |
| [Dreamland: Controllable World Creation with Simulator and Generative
  Models](https://arxiv.org/abs/2506.08006) | Honglin He, Weizhen Wang, Leon Liu, Ziyang Leng, Sicheng Mo | - Dreamland is a novel hybrid world generation framework that combines a physics-based simulator with large-scale pre-trained generative models to create realistic and controllable visual worlds. - It employs a layered world abstraction (LWA) to align the simulator and generative model, enabling fine-grained control and minimizing adaptation costs. - Dreamland outperforms existing baselines by 50.8% in image quality and 17.9% in controllability, demonstrating improved image quality and stronger controllability. - A new dataset, D3Sim, is introduced to facilitate the training and evaluation of hybrid generation pipelines. - Dreamland shows great potential in enhancing embodied agent training, as demonstrated by improvements in downstream tasks like visual question answering. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Video', 'Video Classification', 'Computer Vision', 'Image Segmentation', 'Depth Estimation', 'Reinforcement Learning'] | [Link](https://metadriverse.github.io/dreamland/) | N/A |
| [Bootstrapping World Models from Dynamics Models in Multimodal Foundation
  Models](https://arxiv.org/abs/2506.06006) | Shay B. Cohen, Anna Korhonen, Yftah Ziser, ducdauge, yfqiu-nlp | - This paper introduces a novel method for bootstrapping world models from dynamics models in multimodal foundation models. - The proposed method uses two main strategies: 1) weakly supervised learning from synthetic data generated by the dynamics model and 2) inference-time verification using the dynamics model to score world model candidates. - The authors evaluate their approach on the AURORA-BENCH dataset, demonstrating that their best model achieves performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets. - The findings suggest that acquiring a dynamics model through supervision is significantly easier than acquiring a world model, and dynamics models can effectively bootstrap world models. - This work has implications for improving the capabilities of multimodal foundation models and creating more realistic and robust AI agents. | ['Image-to-Image', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/yfqiu-nlp/vlm-world-model) | N/A |
| [Learning What Reinforcement Learning Can't: Interleaved Online
  Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527) | Xiaochen Ma, Lexiang Tang, Meiyi Qiang, Hao Liang, RoadQAQ | - This paper introduces ReLIFT, a novel training approach that interleaves reinforcement learning (RL) with online fine-tuning (SFT) to improve the reasoning abilities of large language models (LLMs). - ReLIFT addresses the limitations of RL by incorporating SFT to learn what RL cannot, enabling the acquisition of new information and reasoning patterns. - The model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning. - ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. - The experiments demonstrate that ReLIFT outperforms both RL and SFT while using only 13% of the detailed demonstration data, highlighting its scalability and efficiency. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/TheRoadQaQ/ReLIFT) | [Link](https://github.com/huggingface/Math-Verify) |
| [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path
  Lengths in LLMs](https://arxiv.org/abs/2506.07240) | Lior Wolf, Itamar Zimerman, royeis | - This paper introduces a novel method to monitor and control the reasoning process of large language models (LLMs) by manipulating internal progress representations. - The authors introduce an interactive progress bar visualization to make the reasoning process more transparent and easier for users to understand. - They demonstrate that manipulating these internal representations can effectively mitigate overthinking and improve answer accuracy and efficiency. - Their empirical results show that the proposed "overclocking" method outperforms baseline methods in terms of both accuracy and efficiency on several mathematical reasoning benchmarks. - The code for this method is publicly available on GitHub. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/royeisen/reasoning_loading_bar) | [Link](None) |
| [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive
  Policy Optimization](https://arxiv.org/abs/2506.07160) | Qipeng Guo, Zimian Peng, Dianyi Wang, Yibin Wang, LibraTree | - This paper introduces GeometryZero, a family of geometric reasoning models that utilizes Group Contrastive Policy Optimization (GCPO) to improve the performance of LLMs in solving geometry problems. - GCPO addresses the limitations of existing reinforcement learning methods by incorporating two key innovations: Group Contrastive Masking and Length Reward, which help to judiciously determine when to use auxiliary construction. - GeometryZero models consistently outperform baselines on popular geometric benchmarks (Geometry3K, MathVista), achieving an average improvement of 4.29% across all benchmarks. - The paper provides an in-depth ablation study demonstrating the effectiveness of each component in GCPO and detailed analysis of training dynamics. - The results highlight the benefits of using a conditional reward mechanism for auxiliary construction, showing that a flexible approach is crucial for effective geometric reasoning. | ['Reinforcement Learning', 'Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/abs/2506.03690) | Xingyu Lu, Zhibo Zhu, Jiancan Wu, Junkang Wu, Sunshine279 | - This paper introduces γ-PO, a novel dynamic target margin preference optimization algorithm that enhances the robustness of Direct Preference Optimization (DPO). - γ-PO dynamically adjusts reward margins at the pair-wise level, prioritizing high-confidence pairs while suppressing noise from ambiguous pairs. - The proposed method is compatible with existing DPO variants and achieves an average 4.4% improvement over other baselines on AlpacaEval2 and Arena-Hard. - γ-PO requires minimal code changes and has a negligible impact on training efficiency. - Experimental results demonstrate the effectiveness of γ-PO in enhancing LLM alignment and robustness. | ['Natural Language Processing'] | [Link](https://github.com/sunjie279/gammaPO) | N/A |
| [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011) | Junfei Xiao, Alan Yuille, Shiyi Lan, Yinsong Ma, Yunfei Xie | This paper introduces Visual Game Learning (ViGaL), a novel post-training paradigm that enhances multimodal large language models (MLLMs) reasoning abilities by training them to play arcade-like games using reinforcement learning. ViGaL achieves out-of-domain generalization on downstream multimodal reasoning tasks without using in-domain data during training, outperforming models trained on multimodal reasoning data. The authors demonstrate that gameplay post-training enables the capture of transferable reasoning skills, unlocking generalizable multimodal reasoning abilities in MLLMs. ViGaL's effectiveness stems from the use of simple arcade games as controllable and scalable pre-text tasks. Finally, this method preserves the base model's performance on general visual benchmarks. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/yunfeixie233/ViGaL) | N/A |
| [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833) | Dacheng Tao, Jiaxing Huang, Xikun Zhang, michaelchenkj | - This paper introduces Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that improves large language models (LLMs) by enabling the learning of sequences spanning multiple tokens. - CAFT addresses the limitation of existing next-token prediction paradigms which hinder the formation of coherent, high-level concepts. - The method is evaluated on diverse tasks, including text summarization, code generation, and molecular design, demonstrating significant improvements over conventional next-token fine-tuning methods. - CAFT's effectiveness suggests that models do not sufficiently learn and plan beyond the next immediate token, and that an explicit multi-token objective is more effective. - The authors provide open-source code and data to facilitate broader adoption and further research in this area. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/michaelchen-lab/caft-llm) | [Link](null) |
| [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645) | Karolina Seweryn, llmAttack, mchraba | - This paper introduces a novel framework for evaluating the robustness of Large Language Models (LLMs) in less-resourced languages. - The framework utilizes proxy models and attribution methods to identify and perturb the most important words in a sentence, generating human-understandable perturbed examples. - The proposed methodology is validated on Polish, a low-resource language, demonstrating the potential vulnerabilities of LLMs to character and word-level attacks. - The authors find that surprisingly strong attacks can be cheaply created by altering just a few characters and words, drastically altering the predictions of different LLMs. - The created datasets and code are publicly released to facilitate further research in this area. | ['Natural Language Processing'] | N/A | N/A |
| [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and
  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473) | Chong Teng, Fei Li, Xin Zhang, Xiaofeng Mao, Xiaorui Wu | - This paper introduces EVOREFUSE, a novel prompt optimization algorithm that leverages evolutionary search to generate diverse pseudo-malicious instructions that consistently elicit confident refusals across various LLMs. - EVOREFUSE outperforms existing methods by achieving a 140.41% higher average refusal triggering rate, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores. - Two novel datasets are created using EVOREFUSE: EVOREFUSE-TEST (582 pseudo-malicious instructions) and EVOREFUSE-ALIGN (3,000 instructions with responses for supervised and preference-based alignment training). - Fine-tuning LLAMA3.1-8B-INSTRUCT on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset without compromising safety. - The analysis reveals that models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-06-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Will It Still Be True Tomorrow? Multilingual Evergreen Question
  Classification to Improve Trustworthy QA](https://arxiv.org/abs/2505.21115) | VityaVitalich, nakrayko, VirVen, zlatamaria, memyprokotow | - This paper introduces EverGreenQA, the first multilingual question answering dataset with labels indicating whether questions are evergreen (answers remain stable over time) or mutable (answers change). - The dataset supports both evaluation and training of models for evergreen question classification. - They benchmark 12 modern LLMs on EverGreenQA to assess their ability to encode question temporality and train EG-E5, a lightweight multilingual classifier that achieves state-of-the-art performance on this task. - They demonstrate the practical utility of evergreen question classification in three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4's retrieval behavior. - The dataset and trained model are publicly released to facilitate further research. | ['Question Answering'] | N/A | [Link](https://huggingface.co/collections/s-nlp/evergreen-683465909575cb89d6b904fe) |
| [FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal
  Contextual Fusion](https://arxiv.org/abs/2506.01111) | Owen Lee, Liyan Zhao, Zheshu Chen, Shunian Chen, SatsukiVie | - This paper introduces FusionAudio-1.2M, a novel large-scale dataset comprising 1.2 million detailed audio captions and 6 million QA pairs. - A two-stage automated pipeline for fine-grained audio caption generation is proposed. This pipeline first uses pretrained models to extract contextual cues (speech, music, general sounds, and visual information) and then synthesizes these cues using an LLM to generate detailed captions. - The proposed method demonstrates enhanced accuracy and detail by leveraging visual and comprehensive auditory cues, outperforming baselines in terms of caption detail and accuracy according to a manual evaluation. - FusionAudio-1.2M improves audio-text alignment and instruction following, achieving better performance in audio-text retrieval tasks compared to existing datasets. - Ablation studies confirm the contributions of diverse modalities and the effectiveness of multimodal contextual fusion. | ['Audio'] | [Link](https://github.com/satsuki2486441738/FusionAudio) | N/A |
| [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872) | Yu Su, Muhao Chen, Kai Zhang, DarthZhu | - This paper investigates the effects of modality extension, a common technique for training multimodal models, on achieving true omni-modality. - The researchers analyze the trade-offs between extending modalities and preserving core language abilities, exploring model merging as an alternative approach. - Three key research questions are addressed: whether modality extension compromises core language abilities, whether model merging effectively integrates independently fine-tuned modality-specific models, and whether omni-modality extension leads to better generalization. - Experiments reveal that modality extension can enhance certain capabilities but may compromise others; weighted model merging outperforms standard average merging by preserving more crucial attributes. - The findings suggest that omni-modality fine-tuning is less efficient than modality-specific fine-tuning for specific tasks. | ['Multimodal', 'Any-to-Any', 'Natural Language Processing'] | [Link](https://github.com/DarthZhu/lm-extend) | N/A |
| [Audio-Aware Large Language Models as Judges for Speaking Styles](https://arxiv.org/abs/2506.05984) | Linjie Li, Kevin Lin, Chung-Ching Lin, xiaofei-wang, dcml0714 | - This paper explores using audio-aware large language models (ALLMs) as automatic judges for evaluating the speaking styles of speeches generated by spoken language models (SLMs). - Two tasks, voice style instruction following and role-playing, were designed to evaluate the SLMs' ability to control speaking styles, including emotion, volume, pace, and emphasis. - Human evaluations and ALLM evaluations were compared, showing that the Gemini ALLM's agreement with human judges was comparable to the agreement between human evaluators. - The results demonstrated that ALLMs can serve as effective automatic judges for speaking styles, while also highlighting areas where current SLMs need improvement in speaking style control. - The paper contributes two new evaluation tasks for SLMs and demonstrates the feasibility and effectiveness of using ALLMs as automatic judges for speaking styles. | ['Audio', 'Text-to-Speech'] | N/A | N/A |
| [Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs](https://arxiv.org/abs/2506.05629) | sambaran, abhi1nandy2, ananthmuppidi | - This paper introduces a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) for parameter-efficient fine-tuning of large language models. - ID-SPAM generates soft prompts based on input tokens, attending to different tokens with varying importance, and keeping the number of trainable parameters small. - The proposed approach is compared to state-of-the-art techniques on various tasks, demonstrating improved zero-shot domain transfer capability. - ID-SPAM outperforms several baselines on multiple benchmarks, including GLUE and SuperGLUE, showcasing better performance with fewer parameters. - Ablation studies highlight the importance of the self-attention mechanism in ID-SPAM, indicating improved efficiency and effectiveness. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence
  with Egocentric-Exocentric Vision](https://arxiv.org/abs/2506.06253) | Baoqi Pei, Lidong Lu, Yifei Huang, Yuping He, cg1177 | This survey paper does not introduce any new models or datasets.  - It provides a comprehensive overview of video understanding techniques from both egocentric and exocentric viewpoints. - It systematically analyzes recent advancements in three main research directions: leveraging egocentric data to enhance exocentric understanding; utilizing exocentric data to improve egocentric analysis; and joint learning frameworks that unify both perspectives. - The paper also discusses benchmark datasets and proposes promising future research directions.  - A key contribution is the identification and organization of key research challenges and opportunities for future development in cross-view collaborative intelligence. | ['Computer Vision', 'Video Classification', 'Multimodal'] | [Link](https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision) | N/A |
| [HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource
  Utilization](https://arxiv.org/abs/2506.04255) | Harshil Patel, helloparthshah, guineapig | - HASHIRU is a novel multi-agent system framework designed to enhance flexibility, resource efficiency, and adaptability in handling complex tasks. - It features a hierarchical structure with a "CEO" agent dynamically managing specialized "employee" agents, leveraging a hybrid intelligence model that prioritizes smaller, cost-effective local LLMs while flexibly integrating external APIs and larger models when necessary. - The system incorporates an economic model with hiring/firing costs, promoting efficient resource allocation and team stability, and includes autonomous API tool creation. - Evaluations demonstrate HASHIRU's capabilities in academic paper review, safety assessments, and complex reasoning tasks, often outperforming existing models. - Through case studies, it showcases self-improvement via autonomous cost model generation, tool integration, and budget management. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/HASHIRU-AI/HASHIRU) | N/A |
| [Truth in the Few: High-Value Data Selection for Efficient Multi-Modal
  Reasoning](https://arxiv.org/abs/2506.04755) | Chong Peng, Hao Yang, Lei Wang, Kaiyuan Deng, Shenshen Li | - This paper introduces a novel data selection paradigm, Reasoning Activation Potential (RAP), to improve the efficiency of multi-modal reasoning in large language models (LLMs). - RAP identifies high-value cognitive samples by using two estimators: Causal Discrepancy Estimator (CDE) and Attention Confidence Estimator (ACE), which remove samples that overly rely on language priors and those with attention on irrelevant tokens, respectively. - The Difficulty-aware Replacement Module (DRM) replaces easy samples with more challenging ones to ensure complexity in the training process. - Experimental results show RAP achieves superior performance using only 9.3% of the training data and reduces computational costs by over 43%, outperforming existing methods on six datasets. - The effectiveness of RAP is demonstrated across different model architectures and RL algorithms, showcasing its robustness and broad applicability. | ['Multimodal'] | [Link](https://github.com/Leo-ssl/RAP) | N/A |
| [GuideX: Guided Synthetic Data Generation for Zero-Shot Information
  Extraction](https://arxiv.org/abs/2506.00649) | Eneko Agirre, Iker García-Ferrero, OSainz, neildlf | - This paper introduces GUIDEX, a novel method for automatically defining domain-specific schemas, inferring guidelines, and generating synthetically labeled instances for improved zero-shot information extraction. - GUIDEX achieves state-of-the-art results across seven zero-shot Named Entity Recognition benchmarks, surpassing previous methods by up to 7 F1 points without human-labeled data and nearly 2 F1 points higher when combined with it. - The method involves four steps: document summarization, structured representation, guideline generation, and instance extraction, all leveraging LLMs. - GUIDEX demonstrates enhanced comprehension of complex, domain-specific annotation schemas and produces high-quality synthetic data by using executable Python code for validation and ensuring consistency. - The generated synthetic dataset exhibits diverse labels across domains like medicine, economics, history, music, and education, showcasing its versatility and broad applicability. | ['Natural Language Processing', 'Zero-Shot Classification', 'Feature Extraction', 'Text Generation'] | [Link](https://neilus03.github.io/guidex.com) | N/A |


## Papers for 2025-06-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow
  Development](https://arxiv.org/abs/2506.05010) | Zijiao Wu, Qingli Hu, Yiyu Wang, Xue Yang, imryanxu | - ComfyUI-Copilot is a large language model-powered plugin for ComfyUI, an open-source platform for AI-driven art creation, designed to enhance usability and efficiency. - It addresses challenges such as limited documentation, model misconfigurations, and workflow complexity by offering intelligent node and model recommendations, and automated workflow construction. - The system uses a hierarchical multi-agent framework with a central assistant agent and specialized worker agents, supported by curated ComfyUI knowledge bases. - Evaluations show that ComfyUI-Copilot accurately recommends nodes and accelerates workflow development, lowering barriers for beginners and enhancing efficiency for experienced users. -  The system has been shown to achieve high recall rates for workflows and nodes (both exceeding 88.5%), demonstrating practical efficacy. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/AIDC-AI/ComfyUI-Copilot) | N/A |
| [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865) | Emilien Biré, Breno Baldas Skuk, Mathieu Andreux, tonywu71, hamza-hcompany | - This paper introduces Surfer-H, a cost-efficient web agent that leverages Vision-Language Models (VLMs) to perform user-defined tasks on the web.  The agent integrates three modules: a policy, a localizer, and a validator. -  The key innovation is Holol, a new open-weight collection of VLMs trained on curated data sources, including open-access web content, synthetic examples, and self-produced data. Holol outperforms existing methods in UI benchmarks and surpasses state-of-the-art performance on WebVoyager. - Surfer-H achieves a 92.2% success rate on the WebVoyager benchmark, demonstrating Pareto optimality by achieving this high accuracy while being cost-efficient. - The researchers also introduce WebClick, a new benchmark specifically designed for web-based UI localization, to aid in evaluating the performance of similar web agents. - To accelerate research, the WebClick evaluation dataset and the Holol model weights are open-sourced. | ['Reinforcement Learning', 'Multimodal'] | N/A | [Link](https://huggingface.co/collections/Hcompany/holo1-683dd1eece7eb077b96d0cbd), [Link](https://huggingface.co/datasets/Hcompany/WebClick) |
| [Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers
  for Long Contexts](https://arxiv.org/abs/2506.05229) | Ivan Oseledets, Yuri Kuratov, Gleb Kuzmin, Ivan Rodkin, Danil Sivtsov | - This paper introduces Diagonal Batching, a novel scheduling scheme that enhances the parallelism of Recurrent Memory Transformers (RMTs) for long-context inference. - Diagonal Batching reorders computations, enabling efficient GPU inference even with single long-context inputs, eliminating the need for complex batching and pipelining. - When applied to a LLaMA-1B ARMT model, Diagonal Batching achieves a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation. - The method preserves the exact recurrence of RMTs and requires no model retraining, making it adaptable to existing models. - Experimental results demonstrate reduced inference costs and latency, which makes RMTs a practical solution for real-world applications with long contexts. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/svtdanny/diagonal-batching) | N/A |
| [Qwen3 Embedding: Advancing Text Embedding and Reranking Through
  Foundation Models](https://arxiv.org/abs/2506.05176) | Huan Lin, Mingxin Li, Yanzhao Zhang, izhx, thenlper | - This paper introduces the Qwen3 Embedding series, a significant advancement in text embedding and reranking capabilities built upon the Qwen3 foundation models. - The model architecture utilizes LLMs with causal attention for embeddings and employs LLMs for point-wise reranking. - A multi-stage training pipeline is used, combining large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. - The Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks, excelling in multilingual evaluation and various retrieval tasks. - The models are publicly available under the Apache 2.0 license to facilitate reproducibility and community-driven research. | ['Natural Language Processing', 'Sentence Similarity', 'Text Generation', 'Feature Extraction'] | [Link](https://github.com/QwenLM/Qwen3-Embedding) | [Link](https://huggingface.co/Qwen) |
| [Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual
  Simulations](https://arxiv.org/abs/2506.04633) | Yinuo Yang, Zixian Ma, Mahtab Bigverdi, Linjie Li, kuvvi | This paper introduces STARE, a benchmark dataset for evaluating multimodal large language models on spatial reasoning tasks.  STARE contains approximately 4,000 tasks spanning foundational geometric transformations, integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning).  Evaluation reveals that current models struggle with complex visual simulations, performing near random chance on 3D tasks whereas humans achieve near-perfect accuracy.  The authors highlight the importance of visual simulation capabilities for robust spatial reasoning and provide extensive analysis of model performance across various task complexities and evaluation settings.  STARE aims to advance research on multimodal spatial reasoning and inspire further development of AI models capable of human-level visual simulation. | ['Multimodal'] | [Link](https://github.com/STARE-bench/STARE) | N/A |
| [SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs](https://arxiv.org/abs/2506.05344) | Jiwen Lu, Yongming Rao, Jiahui Wang, Zuyan | - This paper introduces SparseMM, a novel KV-Cache optimization strategy that leverages the sparsity of visual heads in Multimodal Large Language Models (MLLMs) to accelerate inference. - SparseMM identifies visual heads through a training-free framework that quantifies head-level visual relevance using OCR. - The method allocates asymmetric computation budgets to heads based on their visual scores, prioritizing visual semantics during decoding. - Extensive evaluations demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs compared to existing KV-Cache acceleration methods, delivering 1.38x real-time acceleration and 52% memory reduction. - SparseMM is shown to generalize across diverse LLM architectures and vision-language tasks. | ['Multimodal'] | [Link](https://github.com/CR400AF-A/SparseMM) | N/A |
| [AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual
  Counting for MLLMs](https://arxiv.org/abs/2506.05328) | Tong Lu, Yicheng Liu, Zhiqi Li, cg1177, lulidong | - This paper introduces CG-AV-Counting, a new benchmark dataset for evaluating clue-grounded audio-visual counting in long videos, addressing limitations of existing benchmarks. - It proposes AV-Reasoner, a model trained with GRPO and curriculum learning, which outperforms existing models on multiple benchmarks, demonstrating the effectiveness of reinforcement learning. - AV-Reasoner achieves state-of-the-art results across multiple benchmarks in Audio-Visual Question Answering, and other related tasks. - Experiments reveal that reasoning in the language space does not improve performance on out-of-domain benchmarks. - The dataset includes 1,027 multimodal questions and 5,845 annotated clues over 497 long videos, supporting both black-box and white-box evaluation. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://av-reasoner.github.io) | [Link](https://huggingface.co/datasets/Video-R1/DVD-counting) |
| [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence
  Training of LLMs](https://arxiv.org/abs/2506.03077) | Xiao Li, Lei Zhao, Qijun Luo, Kullpar | - StreamBP is a novel memory-efficient exact backpropagation method for training large language models (LLMs) on long sequences. - It linearly decomposes the chain rule along the sequence dimension, significantly reducing the memory cost of activation values and logits. - StreamBP is applicable to common training objectives such as supervised fine-tuning (SFT), group relative policy optimization (GRPO), and direct preference optimization (DPO). - Experimental results demonstrate that StreamBP scales up the maximum sequence length by 2.8-5.5 times compared to gradient checkpointing, while using comparable or even less BP time. - A communication-efficient distributed StreamBP is also developed to support multi-GPU training. | ['Natural Language Processing'] | [Link](https://github.com/Ledzy/StreamBP) | N/A |
| [MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical
  Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.05331) | Shilin Yan, Aojun Zhou, Renrui Zhang, CaraJ, xy06 | - The paper introduces MINT-CoT, a novel method that incorporates visual tokens into the chain-of-thought (CoT) reasoning process for solving mathematical problems.- MINT-CoT utilizes an Interleave Token to dynamically select relevant visual regions within mathematical figures, addressing limitations of existing methods that rely on coarse-grained regions or external visual modifications.- The proposed method is evaluated on three benchmark datasets (MathVista, GeoQA, and MMStar), demonstrating significant improvements (+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar) over baseline models.- A new dataset, MINT-CoT, is constructed containing 54K mathematical problems with token-level alignment between reasoning steps and visual regions, facilitating the training of the MINT-CoT model.-  The model is trained using a three-stage strategy combining text-only CoT, interleaved CoT supervised fine-tuning, and interleaved CoT reinforcement learning, enhancing the model's ability to effectively reason with interleaved visual information. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/xinyan-cxy/MINT-CoT) | N/A |
| [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345) | Edoardo M. Ponti, Piotr Nawrot, Konrad Staniszewski, Adrian Łańcucki | This paper introduces Dynamic Memory Sparsification (DMS), a novel method for compressing key-value caches in Transformer LLMs during inference to improve efficiency.  DMS achieves 8x compression with only 1K training steps, outperforming training-free sparse attention methods. The method is shown to boost accuracy for comparable inference runtime and memory load across multiple families of LLMs on reasoning tasks.  Experiments show that DMS consistently outperforms baselines, particularly at higher compression ratios. The work also validates the effectiveness of efficient attention, unlocked by DMS, for inference-time scaling, improving reasoning capabilities. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an
  Egocentric World?](https://arxiv.org/abs/2506.05287) | Dian Jiao, Wentong Li, Long Li, Ronghao Dang, CircleRadon |  - The paper introduces EOC-Bench, a novel benchmark for evaluating multimodal large language models' (MLLMs) object-centric embodied cognition in dynamic egocentric scenarios.  - EOC-Bench features 3,277 meticulously annotated QA pairs categorized into past, present, and future temporal categories, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types.  - The benchmark is designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios, addressing the gap in existing benchmarks which primarily focus on static scene exploration.  - A mixed-format human-in-the-loop annotation framework is used for comprehensive evaluation, incorporating four question types and a novel multi-scale temporal accuracy metric.  - The comprehensive evaluation on EOC-Bench reveals clear deficiencies in object-level temporal perception for mainstream MLLMs, highlighting the challenges and importance of advancing these capabilities. | ['Video Classification', 'Visual Question Answering', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Language-Image Alignment with Fixed Text Encoders](https://arxiv.org/abs/2506.04209) | Yi Ma, Yue Zhao, robinwuzy, JingfengY | - This paper introduces LIFT, a novel framework for language-image alignment that uses a fixed, pre-trained large language model (LLM) as the text encoder and trains only the image encoder. - LIFT outperforms CLIP in most scenarios involving compositional understanding and long captions, demonstrating significant improvements in computational efficiency. - The authors conduct comprehensive benchmarking and ablation studies to evaluate LIFT's performance against CLIP, showcasing superior accuracy in several compositional understanding tasks and LLaVA downstream tasks. - They also investigate several design choices for LLM-based text encoders, finding that contrastive fine-tuning is necessary for optimal performance. - LIFT simplifies the design choices in mainstream contrastive language-image alignment approaches, enabling the use of a simpler cosine similarity loss while achieving comparable performance. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification', 'Image Feature Extraction'] | [Link](https://github.com/Jingfeng0705/LIFT) | N/A |
| [FlexPainter: Flexible and Multi-View Consistent Texture Generation](https://arxiv.org/abs/2506.02620) | Luozhou Wang, Jiantao Lin, Leyi Wu, yingcongchen, StarYDY | - FlexPainter is a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. - It leverages a shared conditional embedding space to perform flexible aggregation between different input modalities, achieving reference image-based stylization via an image-based CFG method. - FlexPainter generates multi-view images simultaneously using a grid representation to enhance global understanding and uses a view synchronization and adaptive weighting module during diffusion sampling to ensure local consistency. - A 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. - The experimental results demonstrate that FlexPainter significantly outperforms state-of-the-art methods in both flexibility and generation quality. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | N/A | N/A |
| [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly
  Licensed Text](https://arxiv.org/abs/2506.05209) | Stella Biderman, Colin Raffel, Brian Lester, Nikhil Kandpal, storytracer |  - The paper introduces Common Pile v0.1, an 8TB dataset of openly licensed text for large language model (LLM) pre-training.  - It addresses the limitations of prior datasets which were too small or low quality to produce performant LLMs.  - The dataset comprises content from 30 diverse sources, spanning research papers, code, books, encyclopedias, and more.  - The authors validate the dataset by training two 7-billion parameter LLMs, achieving competitive performance to those trained on unlicensed text with similar computational budgets.  -  The Common Pile v0.1, associated code, training mixture, and checkpoints for the trained LLMs are publicly released. | ['Natural Language Processing'] | [Link](https://github.com/commonpile/commonpile) | [Link](https://huggingface.co/datasets/commonpile/commonpile) |
| [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at
  Scale](https://arxiv.org/abs/2506.04405) | Yue Yu, Yishan Zhong, Yuchen Zhuang, Ran Xu, wshi83 | MedAgentGym is introduced as the first publicly available training environment designed to improve the coding-based medical reasoning capabilities of large language models (LLMs).  It contains 72,413 task instances across 129 categories from 12 real-world biomedical scenarios.  Benchmarking over 25 LLMs reveals a significant performance gap between commercial API-based models and open-source LLMs. Med-Copilot-7B achieves substantial performance gains via fine-tuning and reinforcement learning, providing a cost-effective and privacy-preserving alternative.  MedAgentGym offers a unified platform for developing LLM-based coding assistants for biomedical research and practice. | ['Reinforcement Learning', 'Natural Language Processing', 'Tabular', 'Question Answering'] | [Link](https://github.com/wshi83/MedAgentGym) | [Link](https://huggingface.co/MedAgentGym) |
| [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning
  Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734) | Xiaoqi Jian, Yongfu Zhu, Jinzhu Wu, Weihong Lin, lincharliesun | - This paper reveals that the benchmark evaluation results of reasoning models, specifically the Deepseek-R1-Distill series, are susceptible to significant fluctuations due to subtle variations in evaluation conditions. - The study highlights the impact of various factors, including evaluation dataset versions, instruction positions, option bias, and tensor parallelism settings, on the model's performance. - The authors advocate for establishing a more rigorous paradigm for model performance evaluation, emphasizing transparency and stability. - They propose a methodology to estimate the required number of independent inferences for stable performance evaluation. - This research emphasizes the importance of standardized and well-documented evaluation procedures to prevent misrepresentation of model capabilities and ensure reproducibility. | ['Question Answering'] | N/A | N/A |
| [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245) | Janardhan Kulkarni, Huseyin A. Inan, wulu, sahar-abdelnabi, Eric-Lan | - This paper introduces a novel reinforcement learning (RL) framework to enhance the contextual integrity (CI) of large language models (LLMs). - The framework leverages chain-of-thought (CoT) prompting to guide LLMs to reason explicitly about CI norms before generating responses. - A synthetic dataset of approximately 700 automatically generated examples with diverse contexts and information disclosure norms was created to train and evaluate the model. - The proposed method significantly reduces inappropriate information disclosure while maintaining task performance across multiple LLMs. - Improvements transfer from the synthetic dataset to established CI benchmarks, demonstrating the effectiveness and generalizability of the framework. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Micro-Act: Mitigate Knowledge Conflict in Question Answering via
  Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278) | Xiaolong Li, Ge Qu, Bowen Qin, Jinyang Li, NanHUO | MICRO-ACT is a novel framework that dynamically decomposes each knowledge source in a retrieval-augmented generation (RAG) system into a sequence of fine-grained comparisons to mitigate knowledge conflicts.  It achieves significant improvements in question-answering (QA) accuracy across five benchmark datasets, outperforming state-of-the-art baselines, especially in temporal and semantic conflict scenarios.  The hierarchical action space in MICRO-ACT allows for reasoning beyond superficial context.  The framework also exhibits robust performance on non-conflict questions.  The code is available on GitHub. | ['Question Answering'] | [Link](https://github.com/Nan-Huo/Micro-Act) | N/A |
| [Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric
  Approach](https://arxiv.org/abs/2506.03238) | Weidi Xie, Yanfeng Wang, Ya Zhang, Lisong Dai, zzh99 | This paper introduces OminiAbnorm-CT, a novel system for abnormality-centric whole-body CT image interpretation.  It includes a hierarchical classification system with 404 abnormal findings, a new dataset (OminiAbnorm-CT-14K) with over 14.5K CT images and annotations for 19K abnormalities, and a new model (OminiAbnorm-CT) that outperforms existing methods in three clinically relevant evaluation tasks. OminiAbnorm-CT's architecture leverages a multi-modal language model and a segmentation module for grounded abnormality description, enabling flexible interaction through both text and visual prompts. The model shows significant improvement over existing baselines in all three tasks. | ['Image-to-Text', 'Multimodal'] | [Link](https://github.com/zhaoziheng/OminiAbnorm-CT) | N/A |


## Papers for 2025-06-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569) | Prestonprom, dwzhu, tobiaslee, gsh33, ShuhuaiRen |  - This paper introduces MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models.  - MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks and achieves a score of 59.4 on OlympiadBench, surpassing models with up to 78B parameters. - The models are trained using a four-stage pre-training phase (2.4 trillion tokens) combined with Mixed On-policy Reinforcement Learning (MORL).  - The training incorporates high-quality reasoning data with long Chain-of-Thought and addresses challenges in simultaneous multi-domain optimization.  - Model checkpoints and a comprehensive evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL. | ['Multimodal'] | [Link](https://github.com/XiaomiMiMo/MiMo-VL) | N/A |
| [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged
  Reinforcement Learning](https://arxiv.org/abs/2506.04207) | Yafu Li, Yue Guo, Shuang Chen, JC-Chen, Warrieryes | - This paper introduces ReVisual-R1, a novel 7B parameter open-source multimodal large language model (MLLM) that achieves state-of-the-art performance on various challenging multimodal reasoning benchmarks. - ReVisual-R1 employs a three-stage training curriculum: a text-only cold start phase, a multimodal reinforcement learning phase using a novel Prioritized Advantage Distillation (PAD) algorithm, and a final text-only reinforcement learning phase. - The PAD algorithm addresses the gradient stagnation problem in standard Group Relative Policy Optimization (GRPO) by prioritizing informative training samples. - Experiments demonstrate that ReVisual-R1 significantly outperforms existing open-source models and even surpasses some commercial models on several key benchmarks. - The findings highlight the importance of a well-designed training curriculum that balances perceptual grounding and cognitive reasoning development in MLLMs. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/CSfufu/Revisual-R1) | N/A |
| [SuperWriter: Reflection-Driven Long-Form Generation with Large Language
  Models](https://arxiv.org/abs/2506.04180) | Roy Ka-Wei Lee, Juanzi Li, Yushi Bai, Yuhao Wu, Zhiqiang007 | This paper introduces SuperWriter-Agent, a novel agent-based framework designed to improve the quality and consistency of long-form text generation.  It incorporates explicit structured thinking through planning and refinement stages, guiding the model using a more deliberate process. The framework is used to create a supervised fine-tuning dataset to train a 7B SuperWriter-LM. SuperWriter-LM outperforms larger-scale baseline models in both automatic and human evaluation.  Hierarchical Direct Preference Optimization (DPO) further enhances performance by optimizing each generation step using Monte Carlo Tree Search (MCTS). The code and models are publicly available. | ['Text Generation'] | [Link](https://github.com/mozhu621/SuperWriter) | N/A |
| [Voyager: Long-Range and World-Consistent Video Diffusion for Explorable
  3D Scene Generation](https://arxiv.org/abs/2506.04225) | Zhenwei Wang, Yuhao Liu, Tengfei Wang, Wangguandong Zheng, tyhuang | - Voyager is a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera paths. - It jointly generates aligned depth and RGB videos, ensuring global coherence and eliminating the need for 3D reconstruction pipelines. - The model employs a world cache mechanism and auto-regressive inference with smooth video sampling for efficient long-range scene generation. - Voyager outperforms existing methods in visual quality and geometric accuracy, as demonstrated by quantitative evaluations on the RealEstate 10K dataset. - The framework also incorporates a scalable data engine for efficient data curation without manual 3D annotations, further enhancing its versatility. | ['Image-to-Video', 'Text-to-Video', 'Image-to-3D', 'Video Classification', 'Depth Estimation', 'Multimodal'] | N/A | N/A |
| [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/abs/2506.03139) | Xingyu Wu, Xinyu Dong, yanyc, zjuxhl, xiaoooobai |  - The paper introduces SVGenius, a comprehensive benchmark for evaluating LLMs' capabilities in SVG processing.  - SVGenius evaluates models across three progressive dimensions: understanding, editing, and generation, using real-world data from 24 application domains.  - The benchmark features a novel complexity stratification framework based on quantitative metrics, allowing for a systematic assessment of model performance across varying complexities.  - Experiments on 22 mainstream models reveal that proprietary models significantly outperform open-source counterparts, but all models exhibit systematic performance degradation with increasing complexity.  - Reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations. | ['Multimodal'] | [Link](https://github.com/ZJU-REAL/SVGenius-Bench) | N/A |
| [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique
  Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295) | Wenhu Chen, Lijun Wu, Kai Zou, Ping Nie, Yubo Wang | - This paper introduces Critique Fine-Tuning (CFT), a novel technique for enhancing the reasoning capabilities of pre-trained large language models (LLMs). - CFT constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques, then fine-tunes the model on this data. - Experiments show that CFT, even when applied to only one problem, significantly improves the performance of LLMs on various reasoning tasks, outperforming reinforcement learning methods while requiring considerably less compute. - The effectiveness of one-shot CFT is demonstrated across different model sizes and problem types, showcasing its robustness and generalizability. - Ablation studies highlight the impact of model scale and the diversity of candidate solutions on the performance of CFT. | ['Natural Language Processing'] | N/A | N/A |
| [Rectified Sparse Attention](https://arxiv.org/abs/2506.04108) | Jian Chen, Yuqing Xia, Li Dong, Tianzhu Ye, Yutao Sun | - This paper introduces Rectified Sparse Attention (ReSA), a novel method for efficient long-sequence generation in large language models. - ReSA combines block-sparse attention with periodic dense rectification to mitigate the accumulation of approximation errors in the key-value cache, which is a common problem in sparse decoding methods. - Experimental results across various tasks demonstrate that ReSA significantly improves efficiency while maintaining near-lossless generation quality compared to dense attention methods. - ReSA achieves up to a 2.42x end-to-end speedup under decoding at 256K sequence length, making it practical for real-world deployment. - The code for ReSA is publicly available. | ['Text Generation'] | [Link](https://aka.ms/ReSA-LM) | N/A |
| [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592) | Yankai Lin, Enrui Hu, Xinyu Zhang, Hao Wang, JaxChen | - This paper introduces the DBG score, a novel metric for measuring self-preference bias in LLMs, which addresses the confounding effect of response quality.- The DBG score uses gold judgments as proxies for actual response quality, mitigating the confounding influence of response quality on bias measurement.- Comprehensive experiments using the DBG score reveal that self-preference bias exists across LLMs with varying versions, sizes, and reasoning abilities; larger models tend to exhibit less bias.- It investigates the impact of response text style and post-training data on self-preference bias, showing that aligning response styles and training on the same data can help alleviate the bias.- An attention-based perspective explores the potential underlying mechanisms of self-preference bias, showing that models naturally tend to assign higher attention scores to their own responses. | ['Natural Language Processing'] | [Link](https://github.com/zhiyuanc2001/self-preference) | N/A |
| [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142) | Juanzi Li, Lei Hou, Zhuoran Jin, Shangqing Tu, Kejian Zhu | - This paper introduces a novel method for establishing trustworthy Large Language Model (LLM) evaluation by analyzing shortcut neurons. - The method identifies shortcut neurons through comparative and causal analysis, focusing on neurons exhibiting significant activation differences between contaminated and uncontaminated models. - A shortcut neuron patching technique is proposed to mitigate the impact of contamination by suppressing shortcut neuron activation, leading to more reliable evaluation results. - Experiments demonstrate the effectiveness of the method, showing a strong correlation with existing trustworthy benchmarks (Spearman coefficient exceeding 0.95). - The method's generalizability is validated across various benchmarks and hyperparameter settings. | ['Natural Language Processing'] | [Link](https://github.com/GaryStack/Trustworthy-Evaluation) | N/A |
| [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/abs/2506.03355) | Matthias Hein, Yongtao Wu, Naman Deep Singh, Elias Abad Rocamora, chs20 | This paper introduces LEAF, an adversarial finetuning method for improving the robustness of CLIP text encoders. LEAF is efficient and scalable to large CLIP models, significantly improving zero-shot adversarial accuracy in the text domain while maintaining vision performance.  When combined with text-to-image diffusion models, LEAF improves generation quality under adversarial noise and enhances recall in multimodal retrieval tasks.  Finally, LEAF facilitates better text reconstruction from embeddings.  Experiments demonstrate improved robustness across various tasks compared to standard CLIP models and other methods. | ['Multimodal', 'Text-to-Image', 'Zero-Shot Image Classification', 'Text Classification'] | [Link](https://github.com/LIONS-EPFL/LEAF) | [Link](https://huggingface.co/LEAF-CLIP) |
| [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945) | Pranchal Agarwal, Tushar Parmanand Budhwani, Jeevana Kruthi Karnuthala, Aishwarya Sahoo, Franck-Dernoncourt | - This paper introduces quantitative LLM judges, a novel framework that enhances existing LLM judges by using regression models to align their evaluation scores with human scores. - The framework decouples qualitative reasoning from quantitative assessment, improving accuracy and efficiency. - Four quantitative judges are proposed for different types of feedback, demonstrating the framework's versatility. - Experiments on four datasets show that quantitative judges effectively improve the predictive power of existing judges, outperforming both base judges and fine-tuned models in most cases. - The framework is more computationally efficient than supervised fine-tuning, making it particularly useful when human feedback is limited. | ['Natural Language Processing'] | N/A | N/A |
| [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM
  Evaluation](https://arxiv.org/abs/2506.00482) | Hitesh Patel, Guijin Son, Haneul Yoo, aliceoh, EunsuKim |  - This paper introduces BenchHub, a unified benchmark suite designed for holistic and customizable LLM evaluation.  - BenchHub aggregates and automatically classifies benchmark datasets from various domains, integrating 303K questions across 38 benchmarks.  - It supports continuous updates, scalable data management, and flexible and customizable evaluation.  - Experiments demonstrate that model performance varies significantly across domains, emphasizing the importance of domain-aware benchmarking.  - BenchHub provides a critical infrastructure for advancing LLM evaluation research. | ['Question Answering'] | [Link](https://github.com/rladmstn1714/BenchHub) | [Link](https://huggingface.co/BenchHub) |
| [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807) | Yingting Li, Yingying Zhang, Jiale Han, Bo Cheng, yulichen |  - This paper introduces a novel dynamic layerwise pruning method called Dynamic Layerwise Pruning (DLP) for Large Language Models (LLMs).  - DLP adaptively determines the relative importance of each layer by integrating model weights with input activation information and assigns pruning rates accordingly, overcoming the limitations of existing methods that rely on predefined values.  - Experimental results on multiple LLMs demonstrate that DLP effectively preserves model performance at high sparsity levels, outperforming state-of-the-art methods.  Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves average accuracy by 2.7%.  - The method is compatible with various LLM compression techniques and seamlessly integrates into Parameter-Efficient Fine-Tuning (PEFT).  - The code for DLP is publicly available on Github. | ['Natural Language Processing'] | [Link](https://github.com/ironartisan/DLP) | N/A |
| [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management
  in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/abs/2506.04133) | Christos Emmanouilidis, Manoj Karkee, Ranjan Sapkota, shainar | This paper reviews Trust, Risk, and Security Management (TRISM) in Large Language Model (LLM)-based Agentic Multi-Agent Systems (AMAS).  It introduces a conceptual TRISM framework tailored to agentic AI, detailing four pillars: governance, explainability, ModelOps, and privacy/security.  Unique threat vectors for AMAS are identified and a comprehensive risk taxonomy is presented.  The paper surveys state-of-the-art explainability strategies and trust-building mechanisms, along with security and privacy measures. Finally, it provides a roadmap for future research directions for responsible agentic AI. | ['Natural Language Processing'] | N/A | N/A |
| [Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.04034) | Lei Zhang, Junzhi Yu, Zhaoyang Zeng, Xingyu Chen, Qing Jiang | Rex-Thinker is a novel multimodal large language model that performs object referring via Chain-of-Thought (CoT) reasoning.  The model first extracts candidate object boxes corresponding to a given referring expression and then performs step-by-step reasoning over each candidate box.  A large-scale CoT-style referring dataset named HumanRef-CoT was created to support this paradigm. Experiments demonstrate that the CoT-based approach outperforms existing methods in terms of both precision and interpretability, while also showing strong generalization ability.  The model uses a two-stage training process: cold-start supervised fine-tuning followed by reinforcement learning. | ['Multimodal', 'Object Detection', 'Visual Question Answering'] | [Link](https://github.com/IDEA-Research/Rex-Thinker) | N/A |
| [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614) | Chaochao Lu, Chao Yang, Lingjie Chen, Zhanhui Zhou | - This paper introduces the concept of visual stitching in Vision-Language Models (VLMs), where the model integrates visual information from scattered training patches to generate coherent responses. - The authors demonstrate that visual stitching enables adversarial attacks by splitting harmful images into benign-looking patches, bypassing data moderation, and causing the VLM to incorrectly label harmful content as safe. - Three synthetic datasets (food, animal, and landmark) are used to evaluate the emergent capabilities of visual stitching in various open-source VLMs. - Experiments show that most open-source VLMs exhibit strong image-based visual stitching, even when trained on extremely small patches, while reference-based visual stitching shows less reliability. - The paper highlights the safety implications of visual stitching, demonstrating a potential vulnerability in data moderation techniques and urging further research on mitigating this risk. | ['Multimodal'] | [Link](https://github.com/ZHZisZZ/visual-stitching) | N/A |
| [Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic
  Agents](https://arxiv.org/abs/2506.01344) | Ryan A. Rossi, Nedim Lipka, Manan Suri, Franck-Dernoncourt, puneetm | - This paper introduces the task of Fine-grained Flowchart Attribution, aiming to identify the optimal path within a flowchart that supports the model's response. - A novel benchmark, FlowExplainBench, is presented for evaluating flowchart attribution across diverse styles, domains, and question types. - The paper proposes FlowPathAgent, a neurosymbolic agent that performs fine-grained post-hoc attribution through graph-based reasoning, outperforming strong baselines by 10-14% on FlowExplainBench. - FlowPathAgent leverages graph tools to precisely attribute the model's reasoning steps to specific decision points within the flowchart, enhancing the interpretability and reliability of automated decision-making. - The study also addresses challenges in handling diverse flowchart styles and explores potential future directions for dynamic flowchart processing and more robust handling of complex structures. | ['Visual Question Answering', 'Graph Machine Learning', 'Multimodal'] | N/A | N/A |
| [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial
  Reasoning](https://arxiv.org/abs/2506.02515) | Rushil Thareja, Georgi Georgiev, Debopriyo Banerjee, Dhruv Sahnan, Zhuohan Xie | - This paper introduces FINCHAIN, a new symbolic benchmark dataset designed for verifiable chain-of-thought financial reasoning. - The dataset includes 54 topics across 12 financial domains, each with five parameterized templates varying in reasoning complexity. - Each dataset instance has an executable Python trace, enabling automatic generation of training data and easy adaptation to other domains. - The paper introduces CHAINEVAL, a new metric for automatic evaluation of both final answers and intermediate reasoning steps. - Experimental results show that even state-of-the-art LLMs have significant room for improvement in multi-step financial reasoning tasks. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/mbzuai-nlp/finchain) | N/A |


## Papers for 2025-06-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [UniWorld: High-Resolution Semantic Encoders for Unified Visual
  Understanding and Generation](https://arxiv.org/abs/2506.03147) | Yuwei Niu, Xinhua Cheng, Zongjian Li, BestWishYsh, LanguageBind | - UniWorld is a new unified generative framework for image perception and manipulation tasks, which uses high-resolution contrastive semantic encoders instead of VAEs. - The model architecture consists of pre-trained multi-modal large models for auto-regressive understanding and high-resolution contrastive semantic encoders for visual feature extraction. - UniWorld outperforms BAGEL on image editing benchmarks using only 1% of BAGEL's training data and achieves competitive performance on image understanding and generation tasks. - The paper also includes empirical observations on GPT-40-Image, inferring that it uses semantic encoders rather than VAEs for visual feature extraction. - UniWorld's code, model weights, training and evaluation scripts, and datasets are fully open-sourced. | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | [Link](https://github.com/PKU-YuanGroup/UniWorld-V1) | [Link](https://huggingface.co/LanguageBind/UniWorld-V1), [Link](https://huggingface.co/datasets/LanguageBind/UniWorld-V1) |
| [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in
  Multi-Agent Environments](https://arxiv.org/abs/2506.02387) | Xinlei Chen, Xiangmin Yi, Zhexuan Xu, HuiningYuan, zelaix | The paper introduces VS-BENCH, a new multimodal benchmark for evaluating Vision-Language Models (VLMs) in multi-agent environments.  VS-BENCH comprises eight vision-grounded environments covering cooperative, competitive, and mixed-motive interactions.  The benchmark uses two complementary evaluation dimensions: offline evaluation of strategic reasoning via next-action prediction accuracy and online evaluation of decision-making via normalized episode return. Experiments on fourteen leading VLMs reveal a significant gap between current models and optimal performance, highlighting areas for future research.  The code and data are publicly available. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://vs-bench.github.io) | [Link](https://huggingface.co/datasets/zelaix/VS-Bench) |
| [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for
  Vision Language Models](https://arxiv.org/abs/2506.03135) | Xinqiang Yu, Wenyao Zhang, Shaochen Zhang, Mengdi Jia, qizekun | This paper introduces OmniSpatial, a comprehensive benchmark for evaluating spatial reasoning capabilities in vision-language models.  It comprises over 1.5K question-answer pairs covering four categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking. The benchmark demonstrates significant limitations of current models in comprehensive spatial understanding, especially in complex logic and perspective-taking tasks. The paper also proposes enhancing spatial reasoning by incorporating auxiliary models such as point-graph and spatial chain-of-thought. The results reveal that while large language models show promising results, there is still a significant gap compared to human-level performance. | ['Visual Question Answering', 'Multimodal', 'Computer Vision', 'Robotics'] | [Link](https://github.com/thu-ml/omnispatial) | N/A |
| [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096) | Hang Yan, Zichen Liu, Xiangyan Liu, Jinjie Ni, Jakumetsu | - SynthRL is a novel method for scaling visual reasoning in reinforcement learning by synthesizing additional training data. - SynthRL comprises three key stages: seed question selection, targeted synthesis, and verification. - Experiments on the MMK12 dataset show that SynthRL synthesizes over 3.3K additional verifiable questions, leading to consistent gains across five out-of-domain visual math reasoning benchmarks. - Detailed analysis reveals that SynthRL is particularly effective in eliciting deeper reasoning on challenging evaluation samples. - The proposed approach is scalable and guarantees near-perfect correctness, making it suitable for large-scale data augmentation in visual reasoning tasks. | ['Reinforcement Learning', 'Visual Question Answering', 'Multimodal'] | [Link](github.com/NUS-TRAIL/SynthRL) | [Link](hf.co/collections/Jakumetsu/SynthRL) |
| [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143) | Jianwei Yang, vyokky, Ray2333, cckevinn, qianhuiwu | - GUI-Actor is a novel coordinate-free visual grounding method for visual grounding in GUI agents that uses an attention-based action head and an <ACTOR> token to identify relevant visual regions for action execution. - The model outperforms previous state-of-the-art methods on multiple GUI action grounding benchmarks, demonstrating improved generalization to unseen screen resolutions and layouts. - GUI-Actor-7B achieves scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL, outperforming UI-TARS-72B (38.1) on ScreenSpot-Pro. - A grounding verifier is designed to evaluate and select the most plausible action region from candidates proposed for action execution, further enhancing model performance. - The authors demonstrate that fine-tuning only the newly introduced action head while keeping the VLM backbone frozen achieves comparable performance to previous state-of-the-art models. | ['Multimodal', 'Image-to-Text', 'Reinforcement Learning'] | N/A | N/A |
| [Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in
  Robotics](https://arxiv.org/abs/2506.00070) | Jaehyung Kim, Jinwoo Shin, Huiwon Jang, Sumin Park, Dongyoung Kim | - This paper introduces ROBOT-R1, a novel framework that uses reinforcement learning to enhance embodied reasoning in robotics, addressing limitations of supervised fine-tuning (SFT). - ROBOT-R1 predicts the next keypoint state for task completion, conditioned on the scene image and environment metadata from expert demonstrations. - The model is trained using a multiple-choice question-answering (MCQA) approach, which converts the continuous action space into a discrete one. - Experiments show that ROBOT-R1 outperforms SFT methods and even surpasses GPT-40 on embodied reasoning tasks, particularly in low-level action control. - The paper also introduces a new benchmark, ROBOT-R1 Bench, to rigorously evaluate the diverse embodied reasoning capabilities required for robotic tasks. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136) | Mengdi Wang, Ke Shen, Ye Tian, Ling Yang, Yinjie Wang | The paper introduces CURE, a novel reinforcement learning framework that co-evolves LLM coder and unit tester capabilities without ground-truth code supervision.  CURE uses a dedicated reward design based on interaction outcomes, enabling flexible and scalable training. The ReasonFlux-Coder models (7B and 14B) derived from this framework improve code generation accuracy and Best-of-N accuracy, outperforming existing models of similar size.  Furthermore, the framework extends to downstream tasks, showing improvements in test-time scaling and agentic coding. Finally, the trained unit tester serves as an effective reward model for reinforcement learning on base models. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Gen-Verse/CURE) | [Link](https://huggingface.co/Gen-Verse/ReasonFlux-Coder) |
| [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061) | Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Debangshu Banerjee, Tarun Suresh | This paper introduces DINGO, a novel constrained decoding algorithm designed for diffusion language models.  DINGO uses dynamic programming to ensure that generated outputs adhere to user-specified regular expressions while preserving the output distribution. It achieves up to a 68% improvement over unconstrained inference on benchmark tasks like symbolic math and JSON generation. The method provably guarantees the correctness and optimality of the generated output. It addresses the challenges of applying constrained decoding to the parallel nature of diffusion LLMs, outperforming previous methods.  DINGO can handle any user-specified regular expression. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation](https://arxiv.org/abs/2505.24714) | Jinsheng Huang, Xiao Luo, chunfenri, alan1027, luojunyu | - This paper introduces FINMME, a new benchmark dataset for evaluating multimodal large language models (MLLMs) in the financial domain.  - FINMME contains over 11,000 high-quality samples covering 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes.  - A novel evaluation system, FinScore, is proposed to address challenges in financial data, such as hallucination and varying domain complexities.  - Experiments show that state-of-the-art models like GPT-40 underperform on FINMME, highlighting its challenging nature and the need for further research in financial MLLMs.  - The dataset and evaluation protocol are publicly available. | ['Multimodal'] | [Link](https://github.com/luo-junyu/FinMME) | [Link](https://huggingface.co/datasets/luojunyu/FinMME) |
| [PCoreSet: Effective Active Learning through Knowledge Distillation from
  Vision-Language Models](https://arxiv.org/abs/2506.00910) | Sung Ju Hwang, Dongseop Kim, Hyungjoon Jang, Dong Bok Lee, Seongjae Kang |  - This paper introduces ActiveKD, a novel framework that integrates active learning (AL) with knowledge distillation (KD) using vision-language models (VLMs).  - ActiveKD leverages the zero- and few-shot capabilities of VLMs to overcome the limitations of traditional KD in data-scarce scenarios.  - The framework proposes a new sample selection strategy, Probabilistic CoreSet (PCoreSet), which maximizes coverage in the probability space rather than the feature space.  - Evaluations on 11 datasets demonstrate that ActiveKD with PCoreSet consistently outperforms existing active learning methods, achieving significant improvements in final-round accuracy.  - This work advances the intersection of AL and KD by effectively transferring knowledge from VLMs to compact, task-specific student models. | ['Image Classification', 'Zero-Shot Image Classification', 'Multimodal'] | N/A | N/A |
| [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for
  Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397) | Changwang Zhang, Jiawei Chen, Junjie Wu, jwanglux, Cynthia-1628 | - OThink-R1 is a novel method that dynamically switches between fast and slow thinking modes to mitigate over-reasoning in large reasoning models (LRMs). - The model classifies reasoning trajectories as either redundant or essential, pruning redundant steps while preserving logical validity. - OThink-R1 reduces reasoning redundancy by approximately 23% without sacrificing accuracy, as demonstrated by experiments on mathematical and question-answering tasks. - A dual-reference KL-divergence loss function is used to fine-tune the LRM, further enhancing its ability to switch between fast and slow thinking modes. - The approach is inspired by human cognitive processes and provides practical guidelines for creating efficient and accurate reasoning models. | ['Question Answering'] | [Link](https://github.com/AgenticIR-Lab/OThink-R1) | N/A |
| [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and
  Accountability](https://arxiv.org/abs/2506.01789) | David Anugraha, Genta Indra Winata, cryptexcode, seungone, patrickamadeus | This paper introduces DATARUBRICS, a structured framework for assessing the quality of datasets.  It addresses the lack of standardized, measurable metrics for evaluating data quality in existing tools like datasheets.  DATARUBRICS uses rubric-based evaluation metrics and explores cost-effective methods for synthetic data generation, including LLMs. It offers a reproducible solution for dataset quality assessment, beneficial for both authors and reviewers. The framework covers 10 dimensions of data quality, including data sources, annotations, novelty, and utility. | ['Natural Language Processing'] | [Link](https://github.com/datarubrics/datarubrics) | N/A |
| [Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.24726) | Kiran Kamble, Christopher Bryant, Umar Jamil, Shelly Bensal, melisa | - This paper introduces a novel methodology for improving large language models (LLMs) by training them to generate better self-reflections when they make mistakes. - The framework operates in two stages: first, upon failing a task, the model generates a self-reflective commentary; second, the model retries the task with the self-reflection in context.  If successful, the self-reflection tokens are rewarded using Group Relative Policy Optimization (GRPO). - The method only requires binary success/failure feedback and is effective across various model architectures, showing substantial performance gains (up to 34.7% improvement in math equation writing and 18.1% in function calling). - Smaller fine-tuned models (1.5 billion to 7 billion parameters) outperformed larger models (10 times larger), demonstrating efficiency gains. - The approach effectively reduces the need for extensive external feedback data, while also addressing the issue of catastrophic forgetting. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/abs/2506.03096) | Matthias Hein, Nicolas Flammarion, Francesco Croce, chs20 | - FuseLIP is a novel multimodal embedding model that uses early fusion of discrete image and text tokens processed by a single transformer encoder. - It outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks. - The model leverages recent progress in discrete image tokenizers, enabling interaction between modalities at each depth of encoding. - FuseLIP is trained with a contrastive loss and a masked multimodal modeling (MMM) loss, consistently enhancing performance across various zero-shot tasks. - Novel datasets for multimodal pre-training and evaluation were collected, including challenging tasks designed to assess modality interactions. | ['Multimodal'] | [Link](https://github.com/chs20/fuselip) | N/A |
| [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports
  From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454) | Xingyu Liu, Yiyao Wang, Han Wang, Bo Pan, Zhaorui Yang | This paper introduces Multimodal DeepResearcher, a novel agentic framework for generating text-chart interleaved reports from scratch.  It leverages a structured textual representation of charts (FDV) to enable LLMs to generate diverse, high-quality visualizations.  Multimodal DeepResearcher outperforms baseline methods, achieving an 82% win rate over DataNarrative using the same Claude 3.7 Sonnet model across various evaluation metrics. The framework decomposes the task into researching, exemplar report textualization, planning, and multimodal report generation.  A new benchmark, MultimodalReportBench, was developed to evaluate the generated reports. | ['Multimodal'] | [Link](https://rickyang1114.github.io/multimodal-deepresearcher/) | N/A |
| [One Missing Piece for Open-Source Reasoning Models: A Dataset to
  Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338) | Sunghyun Park, Beong-woo Kwak, Jihyuk Kim, Dongjin Kang, hyungjoochae | - This paper introduces the Long CoT Collection, a new dataset designed to mitigate the cold-start problem in reinforcement learning for short chain-of-thought (CoT) large language models (LLMs). - The dataset contains 100,000 long CoT rationales generated using existing short CoT LLMs, guided by a smaller seed dataset of 1,000 examples from a state-of-the-art LLM (R1). - The authors demonstrate that training LLMs on this dataset significantly improves their reasoning capabilities, leading to 2-3x larger gains in reinforcement learning performance compared to models trained without it. - Experiments show the dataset achieves comparable or slightly lower quality compared to R1, demonstrating its effectiveness as a strong foundation for reinforcement learning. - The Long CoT Collection is made publicly available, promoting further research and development in open-source reasoning models. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413) | Aditya Grover, Guy Van den Broeck, danielmisrael | - This paper introduces Adaptive Parallel Decoding (APD), a novel decoding method for diffusion large language models (dLLMs) that dynamically adjusts the number of tokens sampled in parallel to improve generation speed without sacrificing quality. - APD works by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model, enabling flexible trade-offs between throughput and quality. - The method is optimized by enabling KV caching and limiting the size of the masked input, resulting in three tunable parameters for balancing speed and quality. - Experiments show that APD achieves significantly higher throughput than autoregressive models and other dLLM decoding methods with minimal quality degradation. - The authors demonstrate that APD outperforms existing methods on several benchmark tasks, achieving a Pareto-optimal performance in terms of both speed and quality. | ['Text Generation'] | [Link](None) | [Link](None) |
| [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition
  Query](https://arxiv.org/abs/2506.03144) | Qi Xu, Xian Wang, Linfeng Li, Yuan Gao, WeiChow | This paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval.  It identifies limitations of existing models: neglecting conditional elements while focusing on global semantics, failing to extract attributes, and misinterpreting visual content.  A novel fine-tuning framework, CORAL, adapts pre-trained MLLMs using embedding reconstruction for conditional elements and contrastive learning for global semantics.  Experiments demonstrate CORAL achieves a 45.9% performance improvement over conventional methods on MERIT and strong generalization across eight benchmarks. | ['Multimodal'] | [Link](MERIT-2025.github.io) | N/A |
| [M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial
  Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510) | Lifan Guo, Xiandong Li, Yalong Wen, Junhui Li, amazingj | This research introduces M³FinMeeting, a novel multilingual, multi-sector, and multi-task dataset designed for evaluating financial meeting understanding in large language models (LLMs).  It addresses limitations of existing datasets by focusing on real-world financial meeting transcriptions across various industry sectors (defined by GICS) and three tasks: summarization, question-answer pair extraction, and question answering.  Experiments on seven popular LLMs show that even advanced models struggle with the tasks, demonstrating the effectiveness of M³FinMeeting. The dataset includes English, Chinese, and Japanese and has over 600 financial meetings. | ['Document Question Answering', 'Question Answering', 'Summarization'] | [Link](https://github.com/aliyun/qwen-dianjin) | N/A |
| [Knowing Before Saying: LLM Representations Encode Information About
  Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362) | Florian Matthes, yziser, galchechik, anumafzal94 | - This paper introduces a novel method for predicting the success of Chain-of-Thought (CoT) prompting in large language models (LLMs) before the generation process is complete. - The method uses a probing classifier trained on LLM internal representations to predict CoT success, outperforming a strong BERT-based baseline that relies only on generated tokens. - Experiments show that the classifier achieves high accuracy even before any tokens are generated, suggesting that crucial information about the reasoning process is encoded early in the LLM's internal representations. - Early stopping experiments demonstrate that truncating CoT reasoning can still improve performance compared to not using CoT, but there is a remaining gap compared to full reasoning. - The findings suggest that optimizing CoT's efficiency may be possible by leveraging the classifier's guidance to identify when early stopping is most effective. | ['Natural Language Processing', 'Text Classification', 'Question Answering', 'Zero-Shot Classification'] | [Link](https://github.com/anum94/CoTpred) | N/A |
| [Revisiting LRP: Positional Attribution as the Missing Ingredient for
  Transformer Explainability](https://arxiv.org/abs/2506.02138) | Lior Wolf, Hila Chefer, Itamar Zimerman, Yarden Bakish | - This paper introduces Positional-Aware Layer-wise Relevance Propagation (PA-LRP), a novel technique for improving transformer explainability by incorporating positional encoding (PE) information into the attribution process. - PA-LRP significantly outperforms existing LRP-based methods for transformer explainability on both NLP and vision tasks, as demonstrated by extensive experiments with fine-tuned classifiers and zero-shot foundation models. - The method reformulates the input space for transformer explainability to include position-token pairs, enabling the propagation of attributions across various positional encoding schemes. - PA-LRP introduces novel, theoretically grounded LRP rules designed to handle PE layers, including learnable, rotary, and absolute PEs. - The authors provide both quantitative and qualitative results that highlight the effectiveness of PA-LRP in achieving more faithful and comprehensive explanations. | ['Natural Language Processing', 'Computer Vision', 'Zero-Shot Classification'] | [Link](https://github.com/YardenBakish/PE-AWARE-LRP) | N/A |
| [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural
  Understanding and Transcreation](https://arxiv.org/abs/2506.01565) | Wenyan Li, Shaohuan Cheng, Dongchu Xie, Lutong Yu, Li Zhou | This paper introduces Hanfu-Bench, a novel multimodal benchmark dataset focusing on cross-temporal cultural understanding and transcreation using Hanfu (traditional Chinese clothing).  The benchmark includes two core tasks: cultural visual understanding (CVU) using multiple-choice visual question answering and cultural image transcreation (CIT) which involves transforming traditional Hanfu images into modern designs.  Evaluation on both tasks reveals significant challenges for existing vision-language models, highlighting the need for further advancements in temporal cultural understanding.  The dataset and evaluation tools are publicly available on HuggingFace. | ['Multimodal', 'Visual Question Answering', 'Image-to-Image', 'Text-to-Image'] | [Link](https://github.com/lizhou21/Hanfu-Bench) | [Link](https://huggingface.co/lizhou21/Hanfu-Bench), [Link](https://huggingface.co/lizhou21/TemporalCulture) |


## Papers for 2025-06-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Taming LLMs by Scaling Learning Rates with Gradient Grouping](https://arxiv.org/abs/2506.01049) | danxu, MarcusB3n, ZedongWangAI, Juanxi, Lupin1998 | This paper introduces Scaling with Gradient Grouping (SGG), a novel optimizer wrapper designed to improve adaptive learning rate estimation in Large Language Models (LLMs).  SGG dynamically groups gradient statistics within each layer into clusters and applies cluster-specific scaling to calibrate learning rates. Experiments across various model sizes and benchmarks demonstrate that SGG consistently improves performance and convergence speed compared to baselines.  The method seamlessly integrates with existing optimizers and PEFT techniques, offering a robust and efficient optimization solution. SGG's efficacy is validated across diverse tasks including pre-training, supervised fine-tuning, and parameter-efficient fine-tuning, highlighting its broad applicability. | ['Natural Language Processing'] | [Link](https://github.com/ScalingOpt/SGG) | N/A |
| [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with
  Jigsaw Puzzles](https://arxiv.org/abs/2505.23590) | Feiyu Xiong, Zhiyu Li, Bo Tang, RyanZhu, wangzifu | - This paper introduces Jigsaw-R1, a novel rule-based reinforcement learning framework for training multimodal large language models (MLLMs) on jigsaw puzzles. - The framework uses jigsaw puzzles as a structured environment to study rule-based visual reinforcement learning, revealing that MLLMs can generalize to complex, unseen configurations and other visual tasks through fine-tuning. - The study demonstrates that RL outperforms supervised fine-tuning (SFT) in generalization, and an initial SFT phase can hinder subsequent RL optimization. - Experiments show that complex reasoning patterns appear to be pre-existing rather than emergent, and their frequency increases with training and task difficulty. - The findings contribute to a better understanding of rule-based visual RL and its potential in multimodal learning. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | [Link](https://github.com/zifuwanggg/Jigsaw-R1) | N/A |
| [ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and
  Understanding](https://arxiv.org/abs/2506.01853) | Jun Zhu, Shenghao Xie, Zhengyi Wang, Junliang Ye, zzzrw | - This paper introduces ShapeLLM-Omni, a novel multimodal large language model (MLLM) capable of understanding and generating 3D content from text or images. - The model architecture uses a 3D vector-quantized variational autoencoder (VQVAE) to map 3D objects into a discrete latent space, enabling efficient and accurate shape representation and reconstruction. - ShapeLLM-Omni is trained on a large-scale dataset called 3D-Alpaca, encompassing generation, comprehension, and editing tasks, which provides rich resources for future research and training. -  The experimental results demonstrate that ShapeLLM-Omni outperforms other baselines on multiple benchmarks, including text-to-3D and image-to-3D generation and 3D captioning tasks. - Overall, ShapeLLM-Omni provides a comprehensive solution for multimodal 3D generation and understanding, exhibiting promising results across various tasks. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://github.com/JAMESYJL/ShapeLLM-Omni/) | N/A |
| [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware
  Reinforcement Learning](https://arxiv.org/abs/2506.01713) | Dongfei Cui, Yu Zhang, Che Liu, Zhihao Dou, Zhongwei Wan | - This paper introduces SRPO, a novel two-stage reflection-aware reinforcement learning framework designed to enhance multimodal LLM reasoning. - SRPO uses a high-quality reflection-focused dataset, constructed using an advanced MLLM, to help a policy model learn reasoning and self-reflection. - The framework introduces a novel reward mechanism that encourages meaningful reflection while avoiding redundancy. - Extensive experiments show that SRPO significantly outperforms state-of-the-art models across multiple multimodal reasoning benchmarks, such as MathVista, MathVerse, and MMMU-Pro. - SRPO achieves notable improvements in both reasoning accuracy and reflection quality. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation
  with Large Multimodal Models](https://arxiv.org/abs/2506.01667) | Luc Van Gool, Danda Pani Paudel, Zhitong Xiong, Bin Ren, Yan Shu | - This paper introduces EarthMind, a novel vision-language framework for multi-granular and multi-sensor Earth Observation (EO) data understanding. - EarthMind features two core components: Spatial Attention Prompting (SAP) and Cross-modal Fusion. - The model achieves state-of-the-art performance on EarthMind-Bench, a comprehensive benchmark with over 2,000 human-annotated multi-sensor image-question pairs. - EarthMind outperforms existing methods on multiple public EO benchmarks, showcasing its potential to handle both multi-granular and multi-sensor challenges in a unified framework. - The effectiveness of EarthMind is demonstrated through extensive experiments on multiple public benchmarks and a newly proposed benchmark called EarthMind-Bench. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image Segmentation', 'Mask Generation', 'Multimodal', 'Image-Text-to-Text'] | [Link](https://github.com/shuyansy/EarthMind) | N/A |
| [Incentivizing Reasoning for Advanced Instruction-Following of Large
  Language Models](https://arxiv.org/abs/2506.01413) | Yuchen Shi, Zihan Xu, Zongyi Li, Gang Li, yolay | This paper introduces a novel method to enhance the instruction-following capabilities of Large Language Models (LLMs) by incentivizing reasoning.  The method addresses the limitations of existing chain-of-thought prompting by employing reinforcement learning with rule-centric reward signals.  Experimental results across multiple benchmarks show significant performance improvements, with a 1.5B LLM achieving comparable results to an 8B LLM in some cases. The approach also incorporates self-evolving instruction generation and behavior cloning techniques to improve robustness. The proposed method boosts the ability of LLMs to handle complex instructions effectively. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/yuleiqin/RAIF) | N/A |
| [DyePack: Provably Flagging Test Set Contamination in LLMs Using
  Backdoors](https://arxiv.org/abs/2505.23001) | Soheil Feizi, mmoayeri, wangwenxiao, yizecheng | - DyePack is a novel framework that uses backdoor attacks to detect test set contamination in large language models (LLMs) without needing access to internal model details. - It incorporates multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation, and provably prevents false accusations. - DyePack is evaluated on five models across three datasets (MMLU-Pro, Big-Bench-Hard, and Alpaca), demonstrating its effectiveness in detecting contamination with guaranteed low FPRs. - The framework generalizes well to both multiple-choice and open-ended generation tasks, successfully identifying all contaminated models. - The approach enhances the reliability and trustworthiness of open benchmarks by providing a principled method for detecting test set contamination. | ['Natural Language Processing'] | N/A | N/A |
| [Reasoning Like an Economist: Post-Training on Economic Problems Induces
  Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577) | Yifang Chen, Xiangqi Jin, Xingyu Dong, Steven-Shaobo, MasterZhou | - This paper introduces Recon, a 7B parameter open-source Large Language Model (LLM) post-trained on a curated dataset of 2100 high-quality economic reasoning problems. - Recon employs Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) to enhance its reasoning capabilities. - Evaluation on economic reasoning benchmarks and multi-agent games demonstrates improvements in structured reasoning and strategic decision-making. - The results highlight the potential of domain-aligned post-training for improving reasoning and agent alignment in LLMs. - The code for Recon is publicly available on GitHub. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Question Answering'] | [Link](https://github.com/MasterZhou1/Recon) | N/A |
| [VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL](https://arxiv.org/abs/2505.23977) | Bhaskar Ramasubramanian, Yuetai Li, Fengqing Jiang, zhangchenxu, EthanSta | - This paper introduces VISUALSPHINX, a large-scale synthetic dataset containing over 660K visual logic puzzles designed to enhance the logical reasoning capabilities of vision-language models (VLMs) through reinforcement learning. - The dataset is generated using a four-stage pipeline that leverages rule abstraction, rule-level genetic algorithms, program-based image synthesis, and strategic puzzle assembly. - Experiments demonstrate that a vision language model fine-tuned using reinforcement learning on VISUALSPHINX outperforms existing models on various benchmarks. - The dataset exhibits strong generalizability and robustness, improving the model's accuracy in solving various visual logic puzzles and other reasoning tasks. - VISUALSPHINX is cost-effective, generated at a cost of less than \$1000, making it scalable and accessible. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://visualsphinx.github.io) | [Link](https://hf.co/VisualSphinx) |
| [From Token to Action: State Machine Reasoning to Mitigate Overthinking
  in Information Retrieval](https://arxiv.org/abs/2505.23059) | Seung-won Hwang, yeonseokjeong, waylight3 | - This paper introduces State Machine Reasoning (SMR), a novel framework for mitigating overthinking in information retrieval by using large language models. - SMR formulates reasoning as transitions between structured states, enabling fine-grained control and early stopping, which avoids generating redundant or misaligned reasoning steps. - The framework consists of discrete actions (REFINE, RERANK, STOP) guided by an LLM, enabling efficient token usage and improved retrieval performance. - Experiments on benchmark datasets (BEIR and BRIGHT) demonstrate that SMR improves retrieval performance (nDCG@10) while significantly reducing token usage. - This method generalizes across LLMs and retrievers without requiring task-specific tuning. | ['Natural Language Processing'] | [Link](https://github.com/ldilab/SMR) | N/A |
| [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent
  Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881) | Kyrie Zhixuan Zhou, Yuanli Wang, Jindan Huang, simonycl, FreaxRuby | This paper introduces STORM, a novel framework for modeling the evolution of user intent in task-oriented dialogues.  STORM uses two LLMs, one simulating the user's internal state and another observing only dialogue history, reflecting real-world information asymmetry.  Experimental results across four language models indicate that moderate uncertainty can outperform complete transparency in certain scenarios, showing model-specific patterns.  The framework contributes to understanding asymmetric reasoning dynamics and informs uncertainty-calibrated dialogue system design.  STORM's contributions include formalizing asymmetric information processing, modeling intent formation tracking, and introducing new evaluation metrics.  These findings suggest that excessive profile information might lead to presumptive reasoning, and moderate uncertainty encourages more exploratory interaction strategies that better support user’s understanding of their own needs. | ['Natural Language Processing'] | N/A | N/A |
| [Stepsize anything: A unified learning rate schedule for
  budgeted-iteration training](https://arxiv.org/abs/2505.24452) | Zhouchen Lin, zhou Xun, Yiming Dong, Anda Tang, Taoer |  - This paper introduces the Unified Budget-Aware (UBA) learning rate schedule for budgeted-iteration training.  - UBA is theoretically grounded, explicitly addressing robustness to landscape curvature variations.  - It outperforms existing schedules across diverse vision and language tasks using various network architectures under different training budgets.  - UBA is controlled by a single hyperparameter that balances flexibility and simplicity, removing the need for per-network numerical optimization.  - Extensive experiments demonstrate UBA's consistent superiority across various tasks, scales, and architectures. | ['Natural Language Processing', 'Computer Vision'] | [Link](https://github.com/Ttt-answer/UBA.git) | N/A |
| [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.24183) | Chongxiao Li, Xiaoyun Zhang, Hanqi Lyu, dihuang, zhuyaoyu |  - CodeV-R1 is a novel reinforcement learning framework designed for training large language models (LLMs) to generate Verilog code from natural language descriptions.  - It addresses the challenges of automated Verilog verification, high-quality data scarcity, and high computational cost of reinforcement learning by introducing a rule-based testbench generator, a round-trip data synthesis method, and an adaptive DAPO algorithm.  - CodeV-R1-7B, the model trained using this framework, surpasses previous state-of-the-art methods by 12-20% on VerilogEval v2 and RTLLM v1.1 benchmarks.  - The model, training pipeline, and dataset are publicly released to facilitate further research in electronic design automation (EDA) and LLM communities.  - CodeV-R1 employs a two-stage training pipeline that consists of a supervised fine-tuning (distillation) phase followed by reinforcement learning.  | ['Reinforcement Learning', 'Text Generation', 'Text2Text Generation'] | [Link](https://iprc-dip.github.io/CodeV-R1) | N/A |
| [Stress-testing Machine Generated Text Detection: Shifting Language
  Models Writing Style to Fool Detectors](https://arxiv.org/abs/2505.24523) | Giovanni Puccetti, Alessio Miaschi, Cristiano Ciaccio, Michele Papucci, andreapdr | - This paper introduces a novel pipeline to generate synthetic texts that are harder for machine-generated text (MGT) detectors to identify by fine-tuning LLMs with Direct Preference Optimization (DPO) to align their writing style with human-written text (HWT). - The pipeline is evaluated on existing state-of-the-art MGT detectors using two datasets: XSUM and arXiv Abstracts, demonstrating a significant drop in their accuracy after the alignment process. - The work highlights the importance of improving detection methods to make them more robust to unseen in-domain texts. - The authors further conduct a human evaluation to assess the effectiveness of their method, comparing the ability of human raters to identify MGT before and after the DPO runs. - Finally, this work provides valuable insights into linguistic characteristics of both HWT and MGT and explores the relationship between detection performance and linguistic features. | ['Natural Language Processing', 'Text Classification', 'Text Generation'] | [Link](https://github.com/gpucce/control_mgt) | N/A |
| [VAU-R1: Advancing Video Anomaly Understanding via Reinforcement
  Fine-Tuning](https://arxiv.org/abs/2505.23504) | Xiaodong Cun, Xi Shen, Qixiang Chen, Liyun Zhu | - This paper introduces VAU-R1, a novel data-efficient framework that leverages reinforcement fine-tuning to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs) for video anomaly understanding. - The framework is built upon Group Relative Policy Optimization (GRPO) and decomposes the video anomaly understanding task into four sub-tasks: multiple-choice QA, temporal anomaly grounding, anomaly reasoning, and anomaly classification. - VAU-R1 outperforms supervised fine-tuning (SFT) methods on reasoning-intensive tasks, demonstrating its effectiveness in enhancing anomaly reasoning and generalization. - The paper also introduces VAU-Bench, a new Chain-of-Thought benchmark for video anomaly reasoning that includes a diverse set of video clips and rich annotations. -  Empirical results on multiple datasets (MSAD, UCF-Crime, and ECVA) demonstrate that VAU-R1 improves accuracy, temporal grounding, and reasoning coherence across diverse contexts. | ['Video Classification', 'Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/GVCLab/VAU-R1) | N/A |
| [LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech
  Detoxification](https://arxiv.org/abs/2506.01484) | Helmut Schmid, Ashish Yashwanth Kangen, Lukas Kouba, Ercong Nie, shuzyuan | - This paper introduces PARADEHATE, a new large-scale parallel dataset for hate speech detoxification, containing over 8K hate/non-hate text pairs. - The dataset was created using a novel LLM-in-the-loop pipeline, which leverages GPT-40-mini to automatically generate detoxified versions of hate speech. - Experimental results demonstrate that models fine-tuned on PARADEHATE achieve better performance in style accuracy, content preservation, and fluency compared to baselines. - The study replicates the ParaDetox pipeline using LLMs instead of human annotators, showing comparable performance. - PARADEHATE is released as a benchmark for hate speech detoxification, offering a scalable alternative to human annotation. | ['Natural Language Processing', 'Text2Text Generation', 'Text Classification'] | N/A | N/A |
| [zip2zip: Inference-Time Adaptive Vocabularies for Language Models via
  Token Compression](https://arxiv.org/abs/2506.01084) | Chris Wendler, Maxime Peyrard, Yunzhen yao, Saibo Geng, nathanrchn | This paper introduces zip2zip, a framework that dynamically adjusts a language model's vocabulary at inference time using Lempel-Ziv-Welch (LZW) compression.  The method incrementally merges co-occurring tokens into reusable hypertokens, reducing input and output sequence lengths by 20-60%.  Zip2zip consists of three components: an LZW-based tokenizer, an embedding layer for new hypertokens, and a causal language modeling variant.  Experiments show significant improvements in inference latency with minimal performance degradation on downstream tasks.  The code is publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/epfl-dlab/zip2zip) | N/A |
| [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice
  Questions](https://arxiv.org/abs/2506.00643) | Stephanie Eckman, Chi Xue, Xi Fang, Shixian Cui, xwjzds | This paper introduces SATA-BENCH, the first benchmark specifically designed to evaluate Large Language Models (LLMs) on Select All That Apply (SATA) questions.  The benchmark includes 1604 human-validated SATA questions across diverse domains.  Evaluation of 27 LLMs reveals a significant performance gap, with even the strongest model achieving only 41.8% exact match.  To address this, the authors propose Choice Funnel, a decoding strategy that achieves up to 29% higher exact match accuracy than competitive baselines while reducing inference cost by over 64%. The SATA-BENCH dataset and Choice Funnel decoding algorithm are publicly released to encourage further LLM development. | ['Question Answering'] | [Link](https://github.com/sata-bench/sata-bench) | [Link](https://huggingface.co/datasets/sata-bench/sata-bench) |
| [Cascading Adversarial Bias from Injection to Distillation in Language
  Models](https://arxiv.org/abs/2505.24842) | Milad Nasr, Ilia Shumailov, Matthew Jagielski, Jamie Hayes, Harsh Chaudhari | This paper introduces BIASED-Roots, a novel data poisoning attack against language models, where an adversary injects adversarial biases into a teacher model during training. The attack demonstrates two distinct modes of bias propagation: Untargeted Propagation and Targeted Propagation. In the Untargeted Propagation scenario, the adversarial bias affects multiple tasks, while in the Targeted Propagation scenario, the bias focuses on a specific task. This bias gets amplified when transferred to the student model via distillation. The paper validates these findings across multiple bias types, distillation methods, and data modalities, revealing that current defense mechanisms are insufficient to mitigate this issue.  The paper proposes practical design principles to build more effective adversarial bias mitigation strategies in the future. | ['Natural Language Processing', 'Text Generation', 'Text Classification'] | N/A | N/A |
| [MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity
  Reconstruction and Generation](https://arxiv.org/abs/2506.00385) | Ziyang Ma, Chenpeng Du, Jiawei Chen, Yakun Song, xiaobinzhuang | - The paper introduces MagiCodec, a novel single-layer, streaming Transformer-based audio codec designed for both high-fidelity reconstruction and generation. - MagiCodec utilizes a multi-stage training pipeline incorporating Gaussian noise injection and latent regularization to improve the semantic expressiveness of generated codes. - Experimental results demonstrate that MagiCodec outperforms state-of-the-art codecs in reconstruction quality and downstream tasks such as text-to-speech and automatic speech recognition. - The tokens produced by MagiCodec exhibit Zipf-like distributions, enhancing compatibility with language-model-based generative architectures. - Theoretical analysis of noise injection in the frequency domain shows its efficacy in attenuating high-frequency components. | ['Audio', 'Audio-to-Audio', 'Automatic Speech Recognition', 'Text-to-Speech'] | [Link](https://github.com/Ereboas/MagiCodec) | N/A |
| [OmniResponse: Online Multimodal Conversational Response Generation in
  Dyadic Interactions](https://arxiv.org/abs/2505.21724) | Bernard Ghanem, Siyang Song, Bing Li, Jianghui Wang, Cheng Luo | - This paper introduces Online Multimodal Conversational Response Generation (OMCRG), a novel task focusing on generating synchronized verbal and non-verbal listener feedback based on speaker's multimodal input. - The proposed model, OmniResponse, is a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multimodal listener responses by leveraging a pretrained LLM enhanced with Chrono-Text and TempoVoice components. - Chrono-Text temporally anchors generated text tokens, while TempoVoice is a controllable online TTS module that synchronizes speech with facial reactions. - A new dataset, ResponseNet, containing 696 high-quality dyadic interactions, is introduced to support further OMCRG research. - Comprehensive evaluations on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. | ['Multimodal', 'Text-to-Speech', 'Text-to-Audio', 'Video-Text-to-Text', 'Any-to-Any'] | [Link](https://omniresponse.github.io/) | N/A |
| [Think Again! The Effect of Test-Time Compute on Preferences, Opinions,
  and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621) | Michal Shmueli-Scheuer, Ateret Anaby-Tavor, Itay Nakash, George Kour | - This paper introduces the Preference, Opinion, and Belief Survey (POBS) benchmark, designed to evaluate the subjective tendencies of Large Language Models (LLMs) across various domains. - The benchmark is applied to several leading LLMs, assessing their reliability, neutrality, and consistency in expressing opinions on various topics. - The study examines the impact of increasing test-time compute (through reasoning and self-reflection mechanisms) on these metrics, revealing limited gains. - Interestingly, newer LLM versions exhibited increased bias and reduced consistency compared to older versions. - The findings highlight the concerning trend of LLMs becoming more biased and less consistent, underscoring the need for ongoing evaluation and improved methods to mitigate these issues. | ['Natural Language Processing', 'Question Answering'] | [Link](https://ibm.github.io/POBS) | N/A |
| [LIFT the Veil for the Truth: Principal Weights Emerge after Rank
  Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772) | Tianjin Huang, Chaoqun Yang, Oleg Balabanov, Tianyu Pang, Zihang Liu |  - This paper introduces LIFT, a novel low-rank informed sparse fine-tuning method for LLMs that focuses on updating only the most important weights for reasoning tasks.  - LIFT outperforms Full FT and other state-of-the-art parameter-efficient methods on various reasoning benchmarks.  - LIFT consistently achieves better performance than Full FT while maintaining comparable memory efficiency to popular methods like LoRA.  - The method identifies principal weights by applying low-rank approximation and selecting weights with the largest magnitudes.  - LIFT balances learning and forgetting, retaining pre-training knowledge and adapting to new downstream tasks effectively. | ['Natural Language Processing'] | [Link](https://github.com/zihanghliu/LIFT) | N/A |
| [CityLens: Benchmarking Large Language-Vision Models for Urban
  Socioeconomic Sensing](https://arxiv.org/abs/2506.00530) | Tianjian Ouyang, Xin Zhang, Hetian Pang, Jie Feng, Tianhui Liu | - CityLens, a comprehensive benchmark, is introduced to evaluate large language-vision models (LLVMs) for predicting socioeconomic indicators from visual data. - The benchmark comprises a multi-modal dataset covering 17 cities globally, encompassing 6 key domains (economy, education, crime, transport, health, and environment) and 11 prediction tasks. - Three evaluation paradigms are utilized: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression, benchmarking 17 state-of-the-art LLVMs. - While LLVMs show promise, limitations remain in accurately predicting urban socioeconomic indicators, particularly for nuanced domains such as health and education. - CityLens offers a unified framework for diagnosing these limitations and guiding future research in using LLVMs for urban socioeconomic sensing. | ['Multimodal'] | [Link](https://github.com/tsinghua-fib-lab/CityLens) | N/A |
| [Massively Multilingual Adaptation of Large Language Models Using
  Bilingual Translation Data](https://arxiv.org/abs/2506.00469) | Hengyu Luo, Indraneil Paul, Jaakko Paavola, Zihao Li, jisx | This paper introduces the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs, and four massively multilingual models continually pre-trained on Llama 3.  The EMMA-500 models were trained with monolingual and bilingual data mixes. Experiments on seven tasks and twelve benchmarks demonstrate that bilingual data generally enhances language transfer and performance, especially for low-resource languages. The EMMA-500 models outperform baselines on multiple benchmarks, particularly in machine translation, showcasing superior multilingual abilities. The MaLA corpus, EMMA-500 Llama 3 models, and code are open-sourced. | ['Natural Language Processing', 'Translation', 'Summarization', 'Text Classification', 'Question Answering'] | [Link](https://github.com/MaLA-LM/emma-500) | [Link](https://huggingface.co/collections/MaLA-LM), [Link](https://hugface.co/datasets/MaLA-LM/mala-bilingual-translation-corpus) |
| [From Guidelines to Practice: A New Paradigm for Arabic Language Model
  Evaluation](https://arxiv.org/abs/2506.01920) | Abdulrahman Al-Batati, Yasser Al-Habashi, Adel Ammar, Omer Nacar, Serry Sibaee | - This paper introduces the Arabic Depth Mini Dataset (ADMD), a new evaluation framework for Arabic language models that addresses the limitations of existing datasets. - ADMD consists of 490 challenging questions across ten major domains, requiring deep cultural understanding and specialized knowledge. - Five leading language models were evaluated using ADMD, revealing significant variations in performance across domains. - Claude 3.5 Sonnet demonstrated the highest overall accuracy, highlighting the importance of cultural competence in model evaluation. - The study provides theoretical guidelines and practical insights for improving Arabic language model evaluation, emphasizing the need for culturally aware and methodologically rigorous benchmarks. | ['Question Answering'] | [Link](https://github.com/serrysibaee/EAED) | [Link](https://huggingface.co/CohereForAI/c4ai-command-r), [Link](https://qwenlm.github.io/blog/qwen2.5-max/) |
| [MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech
  Paralinguistic and Affect Labeling](https://arxiv.org/abs/2505.15772) | Jiatong Shi, Ruoyi Zhang, Yifan Cheng | - MIKU-PAL, a novel multimodal framework, is introduced to automate emotion annotation in audio, visual, and text modalities. - MIKU-PAL achieves high consistency in emotion judgments (Fleiss's kappa of 0.93) with flexible emotion categories, expanding them to 26 categories validated by human annotators. - MIKU-PAL outperforms human annotators on IEMOCAP and MELD in terms of both accuracy and consistency, with significant cost and time reduction. - A new fine-grained emotional speech dataset, MIKU-EmoBench (131.2 hours), is released as a benchmark for emotional text-to-speech and visual voice cloning. - MIKU-EmoBench demonstrates better performance for fine-tuned emotional TTS models compared to existing datasets, including IEMOCAP, MELD and MSP-Podcast. | ['Audio', 'Text-to-Speech', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/WhaleDolphin/MIKU-EmoBench) |


## Papers for 2025-06-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/abs/2505.24863) | Haoran Geng, Xuying Ning, Han Wang, RunpeiDong, jyzhang1208 |  - ALPHAONE (a1) is a novel framework designed to control the reasoning process in large reasoning models (LRMs) at test time.   - The framework introduces an 'alpha moment' which dynamically schedules slow and fast thinking transitions, thereby improving efficiency and generalizing existing methods.  -  a1 models the insertion of reasoning transition tokens as a Bernoulli stochastic process before the alpha moment, transitioning deterministically to fast reasoning afterwards.  - Extensive experiments across various benchmarks (mathematical, coding, scientific) demonstrate a1's superior reasoning capability and efficiency compared to baseline methods, achieving significant improvements in accuracy and efficiency.  - The paper analyzes various scheduling strategies for the activation of slow thinking, finding that a "slow thinking first, then fast thinking" approach yields better results. | ['Question Answering'] | [Link](https://alphaone-project.github.io/) | [Link](string) |
| [Don't Look Only Once: Towards Multimodal Interactive Reasoning with
  Selective Visual Revisitation](https://arxiv.org/abs/2505.18842) | Min Soo Kim, Jaeyoung Lee, Jiwan Chung, siyeolkim, kjunh | - This paper introduces v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that allows for selective visual revisitation during inference. - v1 incorporates a point-and-copy mechanism that enables the model to dynamically retrieve relevant image regions throughout the reasoning process, augmenting existing architectures with minimal modifications. - The authors create v1g, a dataset of 300K multimodal reasoning traces with visual grounding annotations, to train this capability. - Experiments on three multimodal mathematical reasoning benchmarks demonstrate consistent performance improvements over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. - The results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. | ['Multimodal'] | N/A | N/A |
| [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and
  Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878) | Xiaohan Zhao, Jiacheng Liu, Zhaoyi Li, Yaxin Luo, jiachengcui888 | - The paper introduces OpenCaptchaWorld, a web-based benchmark designed to evaluate the visual reasoning and interaction capabilities of multimodal large language models (MLLMs) through diverse CAPTCHA puzzles. - The benchmark comprises 20 modern CAPTCHA types, totaling 225 CAPTCHAs, and is annotated with a new metric, CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. - Experimental results reveal that humans consistently achieve near-perfect scores, while state-of-the-art MLLM agents struggle significantly, with success rates far below human-level performance. - OpenCaptchaWorld serves as a vital benchmark for diagnosing the limitations of current multimodal agents and guiding the development of more robust multimodal reasoning systems. - The platform is designed to test generalization and reasoning depth, not memorization from massive data. | ['Multimodal'] | [Link](https://github.com/MetaAgentX/OpenCaptchaWorld) | [Link](https://huggingface.co/spaces/OpenCaptchaWorld/platform) |
| [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/abs/2505.24196) | Ziqiang Liu, Lu Wang, Huiming Wang, Renke Shan, Longze Chen | - This paper introduces CLaSp, a novel in-context layer-skipping strategy for self-speculative decoding that accelerates the decoding process of large language models. - Unlike previous methods, CLaSp does not require additional modules or training, employing a plug-and-play mechanism by skipping intermediate layers of the verify model. - CLaSp uses a dynamic programming algorithm to optimize the layer-skipping process, dynamically adjusting its strategy after each verification stage without pre-optimized sets of skipped layers. - Experimental results demonstrate that CLaSp achieves a speedup of 1.3x~1.7x on LLaMA3 series models without altering the original distribution of generated text. - The method's efficiency stems from its dynamic adjustment to layer skipping based on the current context, eliminating the need for pre-optimization or retraining. | ['Text Generation'] | N/A | N/A |
| [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858) | Tim G. J. Rudner, Idan Szpektor, Avi Caciularu, Gal Yona, Gabrielle Kaili-May Liu | The paper introduces MetaFaith, a novel prompt-based calibration approach for Large Language Models (LLMs) that improves the alignment between a model's intrinsic uncertainty and its linguistically expressed uncertainty.  MetaFaith leverages metacognitive prompting strategies to elicit more faithful expressions of uncertainty.  Experiments across various LLMs, datasets, and prompting strategies demonstrate that MetaFaith significantly outperforms existing methods, achieving up to a 61% improvement in faithfulness.  Human evaluations further confirm the effectiveness of MetaFaith, showing an 83% win rate over baseline models. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/yale-nlp/MetaFaith) | N/A |
| [Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual
  Large Language Models](https://arxiv.org/abs/2505.20873) | Joon Son Chung, Jongmin Choi, Youngjoon Jang, Chae0 | - This paper introduces Fork-Merge Decoding (FMD), a novel inference-time strategy designed to enhance multimodal understanding in audio-visual large language models (AV-LLMs) without requiring additional training or architectural modifications. - FMD involves a two-stage decoding process: a fork phase, where audio and video inputs are processed separately; and a merge phase, where the resulting representations are combined for joint reasoning. - Experiments on two AV-LLMs, VideoLLaMA2 and video-SALMONN, across three benchmark datasets (AVQA, MUSIC-AVQA, and AVHBench) demonstrate consistent performance improvements compared to existing methods. - The attention-guided fusion mechanism in FMD promotes balanced modality contributions, effectively mitigating modality bias and enhancing robust multimodal understanding. - The proposed FMD is computationally efficient and compatible with both token-wise and channel-wise fusion strategies, improving inference speed while achieving higher accuracy. | ['Multimodal', 'Video Classification', 'Visual Question Answering', 'Audio Classification'] | N/A | N/A |
| [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293) | jamesgolden1 | - This paper demonstrates that the inference operations of several large language models (LLMs) can be mapped to an exactly equivalent linear system for a given input sequence. - The authors achieve this by strategically altering the gradient computation with respect to the input sequence, without modifying model weights or altering output predictions. - This approach reveals that LLMs operate in extremely low-dimensional subspaces, even with their expressive power and global nonlinearity. - The authors demonstrate this across multiple LLMs and show that many of the largest singular vectors decode to concepts related to the most-likely output token, providing insights into internal representations. - This technique enables examination of each layer's operation as nearly-exact linear systems and reveals interpretable semantic structures in next-token prediction. | ['Text Generation'] | [Link](https://github.com/jamesgoldenl/llms-are-llms) | N/A |
| [Harnessing Large Language Models for Scientific Novelty Detection](https://arxiv.org/abs/2505.24615) | Erik Cambria, Thanh-Son Nguyen, Soujanya Poria, Yan Liu, ZonglinY | - This paper proposes a novel method for scientific novelty detection using large language models (LLMs). - Two new benchmark datasets in marketing and NLP are introduced to evaluate the method. - The method leverages LLMs to construct datasets by extracting closure sets of papers and summarizing their main ideas. - A lightweight retriever is trained using knowledge distillation from LLMs to align ideas with similar conceptions. - Experiments demonstrate that the proposed method consistently outperforms existing methods on the benchmark datasets. | ['Natural Language Processing'] | [Link](https://anonymous.4open.science/r/NoveltyDetection-10FB/) | N/A |
| [un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via
  Inverting unCLIP](https://arxiv.org/abs/2505.24517) | Shiguang Shan, Ruibing Hou, Hong Chang, Jiahe Zhao, yinqi | - This paper introduces un²CLIP, a novel method to improve CLIP's visual detail capturing ability by inverting the unCLIP model. - un²CLIP finetunes the CLIP image encoder using a pretrained unCLIP image generator, transferring the generator's rich visual knowledge into the encoder while preserving its alignment with the original text encoder. - The method is evaluated on various tasks, including the MMVP-VLM benchmark, dense prediction, and multimodal large language model tasks, showing significant improvements over the original CLIP and previous methods. - un²CLIP addresses the limitation of CLIP in capturing visual details without modifying the network architecture or requiring additional training data. - The proposed method achieves state-of-the-art results on several benchmark datasets, demonstrating its effectiveness in improving CLIP for various visual tasks. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/LiYinqi/un2CLIP) | N/A |
| [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,
  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009) | Alex Smola, Mu Li, Xingjian Shi, Yuzhi Tang, ruskinmanku |  - The paper introduces EmergentTTS-Eval, a comprehensive benchmark for evaluating TTS models on complex prosodic, expressiveness, and linguistic challenges.  - It uses a model-as-a-judge approach, employing a Large Audio Language Model (LALM) to automate evaluation across multiple dimensions.  - The benchmark includes 1,645 diverse test cases generated iteratively using LLMs, covering six challenging scenarios.  - The model-as-a-judge approach demonstrates high correlation with human preferences and provides robust assessment of TTS systems.  - EmergentTTS-Eval is open-sourced, allowing for easy extensibility and reproducibility. | ['Text-to-Speech'] | [Link](https://github.com/boson-ai/EmergentTTS-Eval-public) | [Link](https://huggingface.co/datasets/bosonai/EmergentTTS-Eval) |
| [Enabling Flexible Multi-LLM Integration for Scalable Knowledge
  Aggregation](https://arxiv.org/abs/2505.23844) | Xin Meng, Yifan Gong, Shiyue Hou, Zheng Zhan, Zhenglun Kong | - This paper introduces a novel framework for integrating multiple large language models (LLMs) to enhance knowledge aggregation. - The framework incorporates an adaptive selection network to dynamically select the most relevant LLMs for a given task, along with a dynamic weighted fusion strategy and a feedback-driven loss function to reduce knowledge interference. - Experimental results demonstrate that the proposed method significantly improves performance compared to existing approaches while reducing knowledge interference by up to 50%. - The adaptive selection network efficiently evaluates diverse LLMs and chooses the subset that best improves the fused model's performance, mitigating the interference issues. - The method achieves stability and scalability without increasing the target model's parameter size or computation costs. | ['Question Answering'] | [Link](https://github.com/ZLKong/LLM_Integration) | N/A |
| [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157) | Yvan Peter, Julian Alvarez, Walter Nuninger, yelboudouri | - This paper introduces Role-Playing Eval (RPEval), a novel benchmark designed to assess Large Language Model (LLM) role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. - RPEval uses single-turn interactions to ensure cost efficiency and reproducibility, focusing on dimensions easily verifiable with automated methods. - The benchmark was constructed using a character profile generator and OpenAI's GPT-40 to create a diverse set of characters and scenarios, which were annotated through crowdsourcing. - Evaluation results on GPT-40, Gemini-1.5-Pro, and Llama 3.2 1B showed that Gemini-1.5-Pro achieved the highest average score, demonstrating balanced performance across dimensions. - RPEval's design choices, such as focusing on single-turn interactions, offer efficiency and reproducibility but limit the assessment of more nuanced, long-term role-playing attributes. | ['Natural Language Processing'] | [Link](https://github.com/yelboudouri/RPEval) | N/A |
| [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements
  Generation](https://arxiv.org/abs/2505.23832) | Wonseok Hwang, Jinu Lee, Chaeeun Kim | - This paper introduces LEGAR BENCH, a large-scale Korean Legal Case Retrieval (LCR) benchmark with 1.2M cases and 411 diverse crime types in queries. - It proposes LEGAL SEARCHLM, a novel retrieval model that performs legal element reasoning and directly generates content grounded in target cases using constrained decoding. - LEGAL SEARCHLM outperforms baselines by 6-20% on LEGAR BENCH, demonstrating state-of-the-art performance and strong generalization to out-of-domain cases. - The model employs a first-token-aware generation strategy and self-supervised fine-tuning (SSFT), which contribute to its improved performance and generalization ability. - Experimental results highlight that LEGAL SEARCHLM significantly outperforms naive generative models by 15%, showcasing its robustness and superior performance in complex retrieval scenarios. | ['Natural Language Processing'] | N/A | N/A |
| [More Thinking, Less Seeing? Assessing Amplified Hallucination in
  Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523) | James Zou, Juncheng Wu, Qingyue Wei, Zhongxing Xu, Chengzhi Liu | - This paper introduces RH-AUC, a new metric to evaluate the balance between reasoning ability and hallucination in multimodal reasoning models, and RH-Bench, a diagnostic benchmark dataset. - The authors find that longer reasoning chains can lead to increased hallucination, as models shift focus away from visual inputs and rely more on language priors. - Larger models generally exhibit a better balance between reasoning and perception than smaller models. - The performance of the model is dependent on the types and domains of the training data rather than the volume of training data. - The authors investigate the impact of reasoning length on the hallucination-reasoning balance and propose methods to control reasoning length. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |


## Papers for 2025-05-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in
  Learning to Reason](https://arxiv.org/abs/2505.22653) | Rui Yan, Zhanhui Kang, Xingwu Sun, Ang Lv, Ruobing-Xie | - This paper investigates the impact of noisy rewards in reinforcement learning for large language models (LLMs) focusing on reasoning tasks. - The authors found that LLMs demonstrate strong robustness to substantial reward noise, even when a significant portion of the rewards are flipped randomly. - They propose a novel reasoning pattern reward (RPR) method, which rewards the appearance of key reasoning phrases in the model's output, regardless of the correctness of the final answer. - Experiments show that RPR, combined with noisy reward models, improves LLM performance on open-ended tasks and mitigates the impact of false negatives. - The findings highlight the importance of improving LLMs' foundational abilities during pre-training and provide insights for advancing post-training techniques. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason) | N/A |
| [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial
  Intelligence](https://arxiv.org/abs/2505.23747) | Yueqi Duan, Yi-Hsin Hung, Fangfu Liu, Diankun Wu | - This paper introduces Spatial-MLLM, a novel framework for visual-based spatial reasoning from 2D observations, enhancing the capabilities of existing video Multimodal Large Language Models (MLLMs). - The model architecture comprises a dual-encoder design: a pretrained 2D visual encoder to extract semantic features and a spatial encoder (initialized from a visual geometry model) to extract 3D structure features, integrated via a connector. - A space-aware frame sampling strategy is proposed, focusing on spatially informative frames for enhanced spatial understanding, even with limited token lengths. - Spatial-MLLM achieves state-of-the-art performance on various real-world datasets in a wide range of visual-based spatial understanding and reasoning tasks. - The model is trained on the Spatial-MLLM-120k dataset using supervised fine-tuning and Group Relative Policy Optimization (GRPO). | ['Video-Text-to-Text', 'Visual Question Answering', 'Multimodal'] | [Link](https://diankun-wu.github.io/Spatial-MLLM/) | N/A |
| [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762) | Yue Yu, Xuan Dong, Shi Liu, Shiqian Su, cyyang822 |  - ZeroGUI is a novel online learning framework for training GUI agents that eliminates the need for manual data annotation.   - It uses a VLM for automatic task generation and reward estimation, enabling continuous learning from the GUI environment.   - Experiments on two advanced GUI agents (UI-TARS and Aguvis) in OSWorld and AndroidLab show significant performance boosts compared to offline learning methods.  - ZeroGUI incorporates a two-stage reinforcement learning strategy: a first stage that trains on automatically generated tasks and a second stage for test-time adaptation.  - The proposed framework achieves zero human cost and scalability to diverse GUI environments. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/OpenGVLab/ZeroGUI) | N/A |
| [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software
  Engineering](https://arxiv.org/abs/2505.23604) | Subhro Das, Zhenting Qi, Delin Chen, Guangtao Zeng, maohaos2 | This paper introduces EvoScale, a novel sample-efficient test-time scaling method for improving the performance of small language models on software engineering tasks.  EvoScale leverages reinforcement learning to enable self-evolution, eliminating the need for external reward models during inference.  Experimental results show that Satori-SWE-32B, a 32B parameter model using EvoScale, achieves performance comparable to models exceeding 100B parameters on the SWE-Bench-Verified dataset.  EvoScale reduces the number of samples required by iteratively refining outputs through selection and mutation. The model learns to self-improve, making it more efficient and reducing sampling costs. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/satori-reasoning/Satori-SWE) | [Link](None) |
| [VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video
  Reasoning?](https://arxiv.org/abs/2505.23359) | Lin Sui, Yi Liu, Haoning Wu, Yuanxin Liu, RUBBISHLIKE | - This paper introduces VIDEOREASONBENCH, a new benchmark designed to evaluate vision-centric complex video reasoning capabilities of large language models (LLMs). - VIDEOREASONBENCH features videos depicting sequences of fine-grained operations on a latent state, requiring models to recall visual information, infer latent states, and predict future information. - Evaluation of 18 state-of-the-art MLLMs reveals that most perform poorly on complex video reasoning, with GPT-4 achieving only 6.9% accuracy. - Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy, highlighting the importance of extended thinking chains for complex video reasoning. - The benchmark shows that the accuracy of models drastically degrades when visual information is reduced or the 'thinking mode' is disabled, highlighting strong vision reliance. | ['Video Classification', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/llyx97/video_reason_bench) | [Link](huggingface.co/datasets/lyx97/reasoning_videos) |
| [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646) | Junfeng Fang, Jianhui Chen, Yanxu Chen, Yantao Liu, Zijun Yao | - This paper investigates whether reasoning models are more prone to hallucination than non-reasoning models. - The authors conduct a holistic evaluation of hallucination in large reasoning models (LRMs) across various post-training pipelines, including supervised fine-tuning (SFT) and reinforcement learning (RL). - They identify two key cognitive behaviors that affect the factuality of LRMs: Flaw Repetition and Think-Answer Mismatch. - The findings suggest that LRMs developed with both SFT and RL are generally less prone to hallucination, while RL-only and SFT-only LRMs show higher rates of hallucination. - Finally, the authors analyze the mechanism behind hallucination from the perspective of model uncertainty, showing that increased hallucination is often associated with misalignment between model uncertainty and factual accuracy. | ['Question Answering'] | [Link](https://github.com/THU-KEG/LRM-FactEval) | N/A |
| [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement
  Learning](https://arxiv.org/abs/2505.22914) | Ilya Zisman, Alexander Nikulin, Denis Tarasov, Maksim Kolodiazhnyi, zhemchuzhnikov | - The paper introduces cadrille, a novel multi-modal CAD reconstruction model that leverages vision-language models (VLMs) and processes point cloud, image, and text modalities simultaneously. - The model employs a two-stage training pipeline: supervised fine-tuning (SFT) on procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback. - cadrille outperforms existing single-modal approaches on the DeepCAD benchmark across all three input modalities. - The study shows that online RL algorithms, such as Group Relative Preference Optimization (GRPO), are superior to offline methods for CAD reconstruction. - After RL fine-tuning, cadrille establishes new state-of-the-art results on three challenging datasets, including a real-world dataset. | ['Text-to-3D', 'Image-to-3D', 'Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088) | Roi Reichart, Liat Ein-Dor, Nitay Calderon | This paper introduces a novel, fully automated method for generating local and global concept-based explanations of preferences across multiple domains.  A hierarchical multi-domain regression (HMDR) model is proposed to capture both domain-general and domain-specific effects on preferences.  The method achieves strong preference prediction performance, outperforming baselines.  Furthermore, the paper demonstrates two application-driven settings where the generated explanations successfully improve the performance of LLM-as-judges. | ['Natural Language Processing', 'Text Classification', 'Text Generation'] | [Link](https://github.com/nitaytech/PrefExplain) | N/A |
| [UniRL: Self-Improving Unified Multimodal Models via Supervised and
  Reinforcement Learning](https://arxiv.org/abs/2505.23380) | Mike Zheng Shou, Zhenheng Yang, Weijia Mao | - This paper introduces UniRL, a self-improving post-training approach for unified multimodal models that requires no external image data. - UniRL enables the model to generate images from prompts and use them as training data in each iteration, allowing the model to improve both generation and understanding tasks simultaneously. - The model is optimized using supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), which enables the two tasks to enhance each other. - UniRL achieves a GenEval score of 0.77 for Show-o and 0.65 for Janus, outperforming several existing baselines and demonstrating its effectiveness in improving both generation and understanding performance. - The method also reduces the imbalance between generation and understanding in unified multimodal models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Reinforcement Learning'] | [Link](https://github.com/showlab/UniRL) | N/A |
| [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419) | Bowen Li, Yu Kang, Chaoyun Zhang, Shilin He, Linghao Zhang | - This paper introduces SWE-bench-Live, a continuously updated benchmark for evaluating large language models (LLMs) on real-world issue-resolution tasks. - The benchmark addresses limitations of existing benchmarks, such as staleness, limited repository coverage, and heavy reliance on manual effort. - SWE-bench-Live uses REPOLAUNCH, an automated curation pipeline, to streamline the entire process from instance creation to environment setup. - Evaluations reveal a substantial performance gap compared to static benchmarks, highlighting the importance of dynamic, contamination-resistant evaluation. - The benchmark provides a diverse set of tasks derived from real GitHub issues, facilitating rigorous evaluation of LLMs in dynamic, real-world settings. | ['Natural Language Processing'] | [Link](https://github.com/SWE-bench-Live) | [Link](https://huggingface.co/SWE-bench-Live) |
| [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255) | Nikita Balagansky, Daniil Gavrilov, Daniil Laptev, Yaroslav Aksenov, Vadim Kurochkin | - This paper introduces KronSAE, a novel sparse autoencoder (SAE) architecture that improves the efficiency of training SAEs by factorizing the latent representation using the Kronecker product and employing a differentiable AND-like activation function. - KronSAE significantly reduces the computational and memory overhead compared to traditional SAEs, especially when dealing with large dictionary sizes, making it suitable for scaling to large language models. - Experimental results on several language models demonstrate that KronSAE achieves comparable or better reconstruction fidelity with significantly fewer parameters than existing SAE models, particularly under tight compute constraints. - The authors also show that KronSAE improves the interpretability of the latent features by reducing feature absorption, a common problem in sparse autoencoders. - The improved efficiency and interpretability of KronSAE suggest that it could be a valuable tool for interpreting and probing the internals of large language models. | ['Natural Language Processing'] | N/A | N/A |
| [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV
  Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618) | Shizhe Diao, Hao Zhang, Chengyue Wu, zhijianliu, Cauthyyy | - This paper introduces Fast-dLLM, a novel method to accelerate inference in diffusion-based large language models (LLMs) without additional training. - Fast-dLLM incorporates a block-wise approximate key-value (KV) cache mechanism and a confidence-aware parallel decoding strategy. - The KV cache reuse mechanism is designed for bidirectional diffusion models, minimizing the performance drop. - The confidence-aware parallel decoding addresses the quality degradation in parallel decoding by selectively decoding tokens exceeding a confidence threshold. - Experiments on LLaDA and Dream models across multiple benchmarks demonstrate a speed improvement of up to 27.6x with minimal accuracy loss. | ['Text Generation'] | N/A | N/A |
| [Muddit: Liberating Generation Beyond Text-to-Image with a Unified
  Discrete Diffusion Model](https://arxiv.org/abs/2505.23606) | Kaidong Yu, Wenhao Chai, Zhuoran Zhao, BryanW, QingyuShi | - Muddit is a novel unified discrete diffusion transformer model that performs fast and parallel generation across text and image modalities. - The model integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation. - Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency on various benchmarks, including GenEval, CIDEr, VQAv2, MME, and GQA. - The model's architecture comprises a text encoder, image encoder, transformer generator, sampler, text decoder, and image decoder. The generator is initialized from a pre-trained text-to-image backbone to leverage strong image priors. - The work demonstrates that discrete diffusion, when equipped with strong visual priors, is a promising approach for building scalable and efficient unified generation models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/M-E-AGI-Lab/Muddit) | N/A |
| [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735) | Yuan Deng, Majid Daliri, Praneeth Kacham, Zeman Li, Ali Behrouz |  - The paper introduces ATLAS, a novel long-term memory module designed to enhance the context memorization capabilities of recurrent neural networks.  - ATLAS addresses limitations of existing methods by using a sliding window update rule and optimizing memory based on past and current tokens, overcoming the online nature of previous models.  - The model architecture incorporates higher-order feature mappings to increase memory capacity and utilizes the Muon optimizer for efficient parallel training.  - Experiments on language modeling, common-sense reasoning, and long-context understanding tasks demonstrate that ATLAS surpasses the performance of Transformers and recent linear recurrent models, achieving significant improvements in long-context performance (+80% accuracy at 10M context length on the BABILong benchmark). - Theoretical justifications are provided to support the enhancements in memory capacity and the effectiveness of the proposed learning rules and optimization methods. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416) | Sangdoo Yun, Jae W. Lee, Sangwoo Kwon, jusjinuk, Jang-Hyun | - This paper introduces KVzip, a novel query-agnostic key-value (KV) cache eviction method for large language models (LLMs). - KVzip quantifies the importance of KV pairs by reconstructing the original contexts from compressed caches, subsequently evicting pairs with lower importance. - Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4x and FlashAttention decoding latency by approximately 2x, with negligible performance loss. - KVzip outperforms existing query-aware KV eviction methods, which suffer from performance degradation in multi-query scenarios. - The proposed method is evaluated on various models and datasets, showing consistent improvement across diverse benchmarks. | ['Question Answering'] | [Link](https://github.com/snu-mllab/KVzip) | N/A |
| [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559) | Ziheng Qi, Jiaxun Zhang, HakHan, m-serious, Leozkl | - This paper introduces SafeScientist, a novel AI scientist framework designed to enhance safety and ethical responsibility in AI-driven scientific exploration. - SafeScientist proactively refuses ethically inappropriate or high-risk tasks and integrates multiple defensive mechanisms, including prompt monitoring and an ethical reviewer component. - The framework is evaluated using SciSafetyBench, a new benchmark comprising 240 high-risk scientific tasks across six domains and 30 specially designed scientific tools. - Experiments demonstrate that SafeScientist improves safety performance by 35% compared to traditional AI scientist frameworks without compromising scientific output quality. - The robustness of SafeScientist is further validated against diverse adversarial attack methods. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/ulab-uiuc/SafeScientist) | N/A |
| [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961) | Jiaxuan You, m-serious, HakHan | - This paper introduces ToMAP, a novel framework for training opponent-aware large language model (LLM) persuaders by incorporating two theory-of-mind (ToM) modules. - ToMAP utilizes a counterclaim predictor to anticipate potential objections and an opponent attitude predictor to estimate the persuadee's stance on these counterclaims. - The model, with only 3 billion parameters, outperforms larger baselines like GPT-40 by a relative gain of 39.4% across multiple datasets. - Experiments demonstrate that ToMAP generates more diverse and effective arguments, reducing repetition and exhibiting complex reasoning chains. - Ablation studies highlight the importance of both ToM modules and reinforcement learning for achieving effective persuasion. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ulab-uiuc/ToMAP) | N/A |
| [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient
  Interactions](https://arxiv.org/abs/2505.17818) | Jae Ho Sohn, Jiho Kim, Seongsu Bae, Hyunseung Chung, Daeun Kyung | This paper introduces PatientSim, a novel patient simulator designed to generate realistic and diverse patient personas for training and evaluating doctor LLMs in realistic multi-turn doctor-patient interaction settings.  PatientSim leverages real-world clinical data from MIMIC-ED and MIMIC-IV datasets, generating 37 unique combinations of persona traits.  The authors evaluate eight different LLMs, finding Llama 3.3 performs best at generating realistic responses based on diverse patient personas.  Clinical validation confirms the simulator's robustness, demonstrating its usefulness as a scalable and privacy-compliant tool for research and education. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural
  Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754) | Qiuzhi Liu, Tian Liang, Zhiwei He, Jiahao Xu, Ziyin Zhang |  - DeepTheorem is a novel framework that uses natural language to enhance LLMs' mathematical reasoning abilities for theorem proving.  - It introduces a large-scale benchmark dataset of 121K high-quality informal mathematical theorems and proofs, annotated for correctness, difficulty, and topic categories.  - A novel reinforcement learning strategy, RL-Zero, is proposed to improve LLM performance in informal theorem proving.  - Comprehensive evaluation metrics assess proof correctness and reasoning quality.  - DeepTheorem significantly outperforms existing methods and datasets, showcasing its potential to advance automated informal theorem proving. | ['Natural Language Processing'] | [Link](https://github.com/Jiahao004/DeepTheorem) | [Link](https://huggingface.co/datasets/Jiahao004/DeepTheorem) |
| [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic
  Reasoning in Chest X-rays](https://arxiv.org/abs/2505.18087) | Hyuk Gi Hong, Hangyul Yoon, Jung-Oh Lee, Geon Choi, ttumyche |  - This paper introduces CheXStruct and CXReasonBench, a structured pipeline and benchmark for evaluating structured diagnostic reasoning in chest X-rays. - CheXStruct automatically extracts clinically relevant reasoning steps from chest X-rays, including segmentation, measurements, and indices. - CXReasonBench leverages CheXStruct to evaluate model reasoning through multiple intermediate steps, supporting multi-path, multi-stage evaluation. - The benchmark includes 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, with up to 4 visual inputs per pair. - Even strong LVLMs struggle with structured reasoning and generalization, highlighting the challenge of linking abstract knowledge with anatomical visual interpretation. | ['Multimodal', 'Visual Question Answering', 'Mask Generation'] | [Link](https://github.com/ttumyche/CXReasonBench) | N/A |
| [Differential Information: An Information-Theoretic Perspective on
  Preference Optimization](https://arxiv.org/abs/2505.23761) | Minjoon Seo, Hyeonbin Hwang, Hyunji Lee, yunjae-won | This paper introduces the Differential Information Distribution (DID) to analyze Direct Preference Optimization (DPO).  The DID captures information gained during policy updates in DPO, revealing that the log-ratio reward is uniquely optimal under specific conditions. The paper explores the relationship between DID entropy and policy dynamics, offering insights into log-likelihood displacement. Experiments on synthetic data validate the theoretical findings, demonstrating that DPO using the log-ratio reward optimally recovers the target policy under conditions where preferences encode DID.  Real-world instruction-following datasets are analyzed, showcasing that high-entropy DID is crucial for general instruction-following tasks, while low-entropy DID benefits knowledge-intensive tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | N/A | N/A |
| [ZeroSep: Separate Anything in Audio with Zero Training](https://arxiv.org/abs/2505.23625) | Yunlong Tang, Susan Liang, Junxuan Huang, Yuesheng Ma, Chao Huang | - ZeroSep is a novel zero-shot audio source separation framework that leverages pre-trained text-guided audio diffusion models. - It achieves this by inverting mixed audio into the diffusion model's latent space and using text conditioning to guide the denoising process to recover individual sources. - ZeroSep outperforms existing supervised and unsupervised methods on multiple separation benchmarks, demonstrating its effectiveness and efficiency. - It inherently supports open-set scenarios due to its reliance on rich textual priors from the pre-trained model. - The method is compatible with various pre-trained audio diffusion models, showcasing its versatility and adaptability. | ['Audio-to-Audio'] | [Link](https://wikichao.github.io/ZeroSep/) | N/A |
| [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765) | Yossi Adi, gallilmaimon, iyosha | - This paper introduces StressTest, a benchmark designed to evaluate speech language models' ability to understand sentence stress. - It introduces a novel synthetic data generation pipeline and creates Stress-17k, a training set that simulates changes in meaning implied by stress variation. - The paper empirically shows that fine-tuning models with Stress-17k improves performance on sentence stress reasoning and detection tasks. - A model called StresSLM significantly outperforms existing models on both sentence stress reasoning and detection tasks. - The results demonstrate the importance of sentence stress for understanding spoken language and highlight the limitations of existing models in this area. | ['Audio', 'Automatic Speech Recognition', 'Natural Language Processing'] | N/A | N/A |
| [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282) | Bryan Dai, Joey Zhou, Lynx Chen, zgao3186 | - This paper introduces one-shot entropy minimization (EM), a novel post-training method for large language models (LLMs) that requires only a single unlabeled data point and 10 optimization steps.  - The proposed method surpasses the performance of existing reinforcement learning (RL) methods which use thousands of data points and carefully designed rewards, as demonstrated by the experiments on various mathematical reasoning benchmarks.  - EM's effectiveness stems from its ability to reduce the model's uncertainty by minimizing token-level entropy, which results in a rightward shift of the logits distribution.  - The study reveals that EM is a distribution-shaping tool rather than a learning method, as evidenced by the inconsistency between the loss-reasoning curve and the logit shift effect.  - Further research is warranted to investigate the full potential of EM, including its applicability to other domains and its combination with other post-training techniques. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/zitian-gao/one-shot-em) | N/A |
| [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360) | Ryan A. Rossi, Nedim Lipka, Manan Suri, Franck-Dernoncourt, puneetm | - This paper introduces ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques and set-of-marks prompting with multimodal LLMs for fine-grained visual attribution. - ChartLens improves fine-grained attributions by 26-66% compared to baselines. - The authors introduce ChartVA-Eval, a new benchmark with synthetic and real-world charts from diverse domains, featuring fine-grained attribution annotations. - ChartLens leverages Segment Anything Model (SAM) for instance segmentation and Lineformer for line segmentation to extract visual features for attribution. -  The proposed method employs a set-of-marks prompting technique with multimodal LLMs to facilitate accurate attribution. | ['Visual Question Answering', 'Multimodal', 'Image Segmentation', 'Mask Generation', 'Question Answering'] | N/A | N/A |
| [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of
  Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943) | Gunhee Kim, Dayoon Ko, Heeseung Yun, ahnpersie | This paper introduces Multimodal Adversarial Compositionality (MAC), a benchmark to evaluate the compositional vulnerabilities of pre-trained multimodal representations like CLIP.  MAC leverages LLMs to generate deceptive text samples that exploit these vulnerabilities, evaluating both sample-wise attack success rate and group-wise entropy-based diversity.  A self-training approach with diversity-promoting filtering is proposed to improve zero-shot methods.  Experiments show the approach's superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.  The proposed method outperforms existing approaches across multiple modalities. | ['Multimodal'] | N/A | N/A |
| [When Models Reason in Your Language: Controlling Thinking Trace Language
  Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888) | Danielle S. Bitterman, Raquel Fernández, Zidi Xiong, Shan Chen, Jirui Qi | - This paper introduces XReasoning, a novel multilingual reasoning benchmark to evaluate the capabilities of Large Reasoning Models (LRMs) in various languages. - The study reveals a significant trade-off between the accuracy of LRM answers and their ability to generate thinking traces in the user's specified language. - The researchers demonstrate that prompt hacking can improve the thinking trace language matching rate but at the cost of reduced answer accuracy. - Post-training with a small number of instances can mitigate the language mismatch problem but also leads to a decrease in accuracy. - This work highlights the need for future research to improve the multilingual reasoning capabilities and user-friendliness of LRMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Betswish/mCoT-XReasoning) | [Link](https://huggingface.co/collections/shanchen/xreasoning-models-68377e15a2e86143dc4b0383), [Link](https://huggingface.co/collections/shanchen/xreasoning-681e7625c7a9ec4111a634b6) |
| [CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian
  Splatting](https://arxiv.org/abs/2505.22854) | Marcin Mazur, Tadeusz Dziarmaga, Piotr Borycki, Joanna Waczyńska, Kornel Howil | - CLIPGaussian is a novel universal style transfer model that operates directly on Gaussian primitives, supporting various data modalities such as images, videos, 3D objects, and 4D dynamic scenes. - The model architecture leverages a two-stage training process: first training a Gaussian Splatting model tailored to a specific data modality, then using a composite loss function (content preservation, background preservation, local style transfer, and global style transfer) to leverage training images, randomly sampled patches, and conditioning inputs (image or text) in the feature spaces of VGG-19 and CLIP models. - CLIPGaussian demonstrates superior style fidelity and consistency compared to existing baselines (I-GS2GS, DGE, StyleGaussian, and G-Style) across various tasks and modalities, as shown through quantitative comparisons (CLIP-S, CLIP-SIM, CLIP-F, CLIP-CONS) and user studies. - It integrates as a plug-in module into existing Gaussian Splatting pipelines without requiring retraining or large generative models, enhancing efficiency and versatility. -The method achieves temporal coherence in videos, handles multiple data modalities through a unified architecture, and performs end-to-end optimization of Gaussian parameters, enabling joint optimization of both color and geometry. | ['Multimodal', 'Image-to-Image', 'Image-to-Video', 'Text-to-Image', 'Text-to-Video', 'Image-to-3D', 'Text-to-3D'] | [Link](https://github.com/kornelhowil/CLIPGaussian) | N/A |
| [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199) | Ruichuan An, Renrui Zhang, Joey Tsai, Shilin Yan, Pengxiang Li | - This paper introduces Adaptive Classifier-Free Guidance (A-CFG), a novel method to enhance the controllability of iterative masked language models during text generation. - A-CFG dynamically adjusts the unconditional input in Classifier-Free Guidance (CFG) based on the model's predictive confidence, focusing guidance on uncertain areas. - Experiments on various benchmarks demonstrate that A-CFG significantly improves the performance of LLaDA, outperforming standard CFG and achieving substantial gains in accuracy. - A-CFG's effectiveness is demonstrated across different tasks including general language understanding, mathematical reasoning, and planning, highlighting its adaptability. - The method improves the results on multiple benchmarks, for instance, a 3.9 point gain on GPQA and an 8.0 point improvement on the Sudoku task, showcasing A-CFG's efficacy. | ['Text Generation'] | [Link](https://github.com/pixeli99/A-CFG) | N/A |
| [Evaluating Text Creativity across Diverse Domains: A Dataset and Large
  Language Model Evaluator](https://arxiv.org/abs/2505.19236) | Fang Luo, Yahui Liu, Yuzhuo Yuan, Xiting Wang, Aman | This paper introduces CreataSet, a large-scale dataset with over 100K human-level and 1M+ synthetic creative instruction-response pairs across diverse domains.  A novel pairwise-comparison framework is proposed for evaluating text creativity, improving evaluation consistency.  An LLM-based evaluator, CrEval, is developed and trained on CreataSet, demonstrating superior performance over existing methods in alignment with human judgments.  CrEval shows significant improvements in F1-score, Kappa score, and agreement rate compared to strong baselines,  and exhibits strong generalization capabilities across various domains.  All data, code, and models are publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://creval-creative-evaluation.github.io/) | N/A |


## Papers for 2025-05-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large
  Model Token Routing](https://arxiv.org/abs/2505.21600) | Zhihang Yuan, Enshu Liu, Yi Ge, youyc22, fuvty | - This paper introduces Roads to Rome (R2R), a novel token routing method that improves the efficiency of large language model (LLM) inference by selectively using LLMs only for tokens that cause divergence in reasoning paths compared to small language models (SLMs). - R2R uses a lightweight neural router to identify these critical tokens, automatically generated through a developed data pipeline that labels divergent tokens. - When evaluated on challenging math, coding, and QA benchmarks, R2R outperforms the average accuracy of R1-7B by 1.6x and even surpasses R1-14B, while delivering a 2.8x wall-clock speedup compared to R1-32B. - This method achieves a 4.6x accuracy improvement over the R1-1.5B SLM with only 12.9% LLM usage. - The R2R model combines R1-1.5B and R1-32B models, achieving an average activated parameter size of 5.6B. | ['Question Answering'] | [Link](https://github.com/thu-nics/R2R) | N/A |
| [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651) | Ruqi Zhang, Tuwhy |  - Sherlock is a novel self-correcting and self-improving training framework for reasoning Vision-Language Models (VLMs) that addresses the challenges of reasoning errors, high data demands, and limited generalization capabilities. - The framework introduces a trajectory-level self-correction objective, a preference data construction method using visual perturbation, and a dynamic beta for preference tuning. - Sherlock achieves state-of-the-art results on eight benchmarks with less than 20% of the annotated data used by existing methods. - The model's self-correction capabilities allow for continued improvement without external supervision after initial training. - An ablation study shows that the trajectory-level objective and dynamic beta are crucial for effective self-correction. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://dripnowhy.github.io/Sherlock/) | N/A |
| [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453) | Chen Wang, Yuting Li, weiranhuang, weiranhuang, WaltonFuture | - This paper introduces MM-UPT, a novel framework for unsupervised post-training of Multi-modal Large Language Models (MLLMs). - MM-UPT uses GRPO, a stable and scalable online reinforcement learning algorithm, and replaces traditional reward signals with a self-rewarding mechanism based on majority voting. - Experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B on multiple benchmarks, even outperforming previous unsupervised baselines and approaching the results of supervised GRPO. - The use of synthetic questions generated by the MLLM itself can further boost performance, highlighting a promising approach for scalable self-improvement. - MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/waltonfuture/MM-UPT) | N/A |
| [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136) | Pengle Zhang, Haofeng Huang, Jia Wei, Xiaoming Xu, jt-zhang | - This paper introduces SageAttention2++, an improved implementation of SageAttention2, focusing on accelerating the second matrix multiplication in the attention mechanism. - SageAttention2++ utilizes the faster FP8 Matmul instruction accumulated in FP16, resulting in a 3.9x speedup over FlashAttention. - The improved model maintains the accuracy of SageAttention2, demonstrating effectiveness across various models for text, image, and video generation, with negligible end-to-end metric loss. - The enhanced efficiency is achieved by narrowing the quantization range of P and V to ensure that accumulated values remain within the representable range of FP16. - Comprehensive evaluations on various state-of-the-art models show consistent performance improvements, supporting its plug-and-play acceleration capabilities. | ['Text2Text Generation', 'Text-to-Image', 'Text-to-Video'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [Advancing Multimodal Reasoning via Reinforcement Learning with Cold
  Start](https://arxiv.org/abs/2505.22334) | Kaipeng Zheng, Yuting Li, weiranhuang, weiranhuang, WaltonFuture | - This paper introduces a two-stage approach for enhancing multimodal reasoning in large language models (LLMs): first, supervised fine-tuning (SFT) with Chain-of-Thought (CoT) patterns to create a strong foundation and then reinforcement learning (RL) with GRPO to further refine the model's capabilities. - The study demonstrates that the presence of "aha moment" patterns in MLLMs before RL training does not necessarily correlate with improved reasoning performance, challenging existing assumptions. - The proposed approach achieves state-of-the-art performance among open-source MLLMs at both 3B and 7B scales on multiple challenging multimodal reasoning benchmarks, consistently outperforming both SFT-only and RL-only methods. - Ablation studies explore the impact of different SFT strategies and data qualities on subsequent RL performance, revealing that high-quality supervision during SFT is crucial for maximizing gains. - The findings highlight the importance of a well-structured reasoning format, regardless of whether reflective patterns are present, demonstrating that cold-start SFT provides a strong foundation for subsequent RL scaling. | ['Multimodal', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/waltonfuture/RL-with-Cold-Start) | N/A |
| [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for
  Frozen LLMs](https://arxiv.org/abs/2505.19075) | Jong Chul Ye, Choonghan Kim, Hyunmin Hwang, Hangeol Chang, kjm981995 | - This paper introduces UniR, a lightweight, composable, and plug-and-play reasoning module that enhances the reasoning capabilities of frozen LLMs without retraining. - UniR decomposes rewards into a standalone reasoning module, trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. - UniR outperforms existing baseline fine-tuning methods on mathematical reasoning and machine translation tasks, demonstrating strong weak-to-strong generalization across different LLM sizes. - The additive structure of UniR enables modular composition, allowing multiple UniR modules trained for different tasks to be jointly applied. - UniR is computationally efficient, adaptable, and robust, offering a cost-effective solution for enhancing reasoning in LLMs without compromising their core capabilities. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/hangeol/UniR) | N/A |
| [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648) | Liwen Zhang, Wenbiao Yin, Runnan Fang, Baixuan Li, callanwu | - WebDancer is a novel end-to-end agentic information-seeking agent built upon the ReAct framework. - The model utilizes a data-centric approach, constructing diverse and challenging deep information-seeking QA pairs through two methods: CRAWLQA and E2HQA. - A two-stage training paradigm is employed: rejection sampling fine-tuning (RFT) with subsequent on-policy RL using the DAPO algorithm. - WebDancer demonstrates strong performance on the challenging information seeking benchmarks, GAIA and WebWalkerQA, outperforming existing methods. - Further analysis reveals valuable insights into developing more capable agentic models. | ['Question Answering'] | [Link](https://github.com/Alibaba-NLP/WebAgent) | N/A |
| [Judging Quality Across Languages: A Multilingual Approach to Pretraining
  Data Filtering with Language Models](https://arxiv.org/abs/2505.22232) | Abbas Goher Khan, Elias Wendt, Max Lübbering, Mehdi Ali, mbrack | This paper introduces JQL, a novel multilingual data filtering approach that uses pretrained multilingual embeddings and lightweight annotators to efficiently curate high-quality training data.  JQL significantly outperforms existing heuristic methods, demonstrating robust performance across 35 languages, including low-resource languages.  The proposed pipeline includes four stages: human annotation, LLM-as-a-judge annotation, lightweight annotator training, and data filtering.  JQL achieves high data retention rates and enhances downstream model training quality. | ['Natural Language Processing'] | [Link](https://github.com/JQL-AI/JQL-Annotation-Pipeline/) | [Link](https://huggingface.co/spaces/Jackal-AI), [Link](https://huggingface.co/datasets/Jackal-AI/jql_human_edu_annotations), [Link](https://huggingface.co/datasets/Jackal-AI/jql_llms_edu_annotations), [Link](https://huggingface.co/Jackal-AI/JQL-Edu-Heads), [Link](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) |
| [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time
  Scaling](https://arxiv.org/abs/2505.19187) | Kaishuai Xu, Chunpu Xu, Ruifeng Yuan, Jiashuo Wang, YangXiao-nlp | This paper introduces PIR (Perplexity-based Importance Refinement), a novel framework that refines large language model (LLM) reasoning chains by systematically pruning low-importance functional steps while preserving progressive reasoning.  The framework identifies and removes low-importance functional steps using perplexity scores, improving the computational efficiency of reasoning-capable LLMs.  The approach demonstrates significant improvements in accuracy (+0.9% to +6.6%) and reduced token usage (-3% to -41%) across various benchmarks. The method is shown to generalize across different model sizes and data sources, offering a practical solution for deploying reasoning-capable LLMs with efficient test-time scaling.  The code and dataset are publicly available. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal
  Evolution of Human States](https://arxiv.org/abs/2505.17663) | Chunpu Xu, Changhe Song, Qiancheng Xu, Jiashuo Wang, YangXiao-nlp | - This paper introduces DYNTOM, a novel benchmark designed to evaluate LLMs' ability to track and understand the temporal evolution of mental states in social interactions. - DYNTOM comprises 1,100 social contexts, 5,500 scenarios, and 78,100 multiple-choice questions, each validated for realism and quality. - Evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7%, with performance significantly degrading when tracking mental state shifts. - The performance gap highlights the limitations of current LLMs in modeling the dynamic nature of human mental states. - This work provides a comprehensive framework for evaluating LLMs' understanding of temporal evolution in mental states and a benchmark with extensive empirical evaluation results. | ['Natural Language Processing'] | N/A | N/A |
| [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich
  Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019) | Zehui Chen, Ruixue Ding, Lin-Chen, YuZeng260, autumncc | - This paper introduces VRAG-RL, a novel reinforcement learning framework for training Vision-Language Models (VLMs) to reason and understand visually rich information. - VRAG-RL incorporates a visual perception action space, allowing VLMs to extract information from a coarse-to-fine perspective and enhance reasoning capabilities. - A comprehensive reward structure, integrating retrieval performance and model-based outcome rewards, bridges the gap between user inquiries and retriever outputs, aligning with real-world applications. - Extensive experiments on various benchmarks demonstrate that VRAG-RL outperforms existing methods, achieving over 20% improvement. - The code is available at https://github.com/Alibaba-NLP/VRAG. | ['Multimodal', 'Document Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Alibaba-NLP/VRAG) | N/A |
| [RICO: Improving Accuracy and Completeness in Image Recaptioning via
  Visual Reconstruction](https://arxiv.org/abs/2505.22613) | Linli Yao, Sihan Yang, Shuhuai Ren, Yishuo Cai, Yuchi Wang | - This paper introduces RICO, a novel framework for image recaptioning that refines captions through visual reconstruction. - RICO leverages a text-to-image model to reconstruct a caption into a reference image, and then uses an MLLM to identify discrepancies between the original and reconstructed images to refine the caption iteratively. - RICO-Flash, a more efficient end-to-end variant of RICO, is also introduced; it uses Direct Preference Optimization (DPO) to learn to generate captions like RICO. - Extensive experiments demonstrate that RICO significantly improves caption accuracy and completeness, outperforming most baselines by approximately 10% on both CapsBench and CompreCap. - The code for RICO is released on GitHub. | ['Image-to-Text', 'Text-to-Image', 'Multimodal'] | [Link](https://github.com/wangyuchi369/RICO) | N/A |
| [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202) | Hoyeon Chang, Jiyeon Kim, Seungone Kim, Byeongguk Jeon, Hyeonbin Hwang | - This paper introduces a novel framework that allows pretrained Language Models (LMs) to reason over higher-level abstractions like sentences rather than raw token sequences. - The framework adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of the next sentences. - Two embedding paradigms are explored: semantic embeddings (learned via autoencoding) and contextual embeddings (trained via next-sentence prediction). - The proposed approach, using contextual embeddings under a continuous inference regime, achieves competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs by half. - The effectiveness of this approach was demonstrated across four domains: mathematics, logic, commonsense reasoning, and planning. | ['Question Answering'] | N/A | N/A |
| [Thinking with Generated Images](https://arxiv.org/abs/2505.22525) | Jiadi Su, Siqi Kou, Steffi Chern, Zhulin Hu, ethanchern | - This paper introduces "Thinking with Generated Images", a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities. - The core contribution is a new method called "native long-multimodal thought process", which enables unified LMMs to seamlessly generate intermediate visual thoughts, establish visual subgoals, and iteratively critique their visual hypotheses. - Two complementary mechanisms are proposed: vision generation with intermediate visual subgoals and vision generation with self-critique, both showing substantial performance improvements over baseline approaches on vision generation benchmarks. - The proposed approach achieves up to a 50% relative improvement in handling complex multi-object scenarios, demonstrating the effectiveness of the native long-multimodal thought process. - The authors release an open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images. | ['Multimodal', 'Text-to-Image', 'Image-to-Image'] | [Link](https://github.com/GAIR-NLP/thinking-with-generated-images) | N/A |
| [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338) | Si Qin, Tianjun Mao, Chaoyun Zhang, Lu Wang, Hanyang Wang | - This paper introduces TEXT2GRAD, a novel reinforcement learning paradigm that converts free-form natural language feedback into span-level gradients for precise policy optimization. - TEXT2GRAD surpasses traditional RLHF and prompt-only baselines in summarization, code generation, and question answering tasks. - The method comprises three components: a feedback-annotation pipeline, a fine-grained reward model, and a span-level policy optimizer. - Unlike prior work, TEXT2GRAD directly incorporates textual feedback into gradient updates, leading to more targeted and interpretable learning. - Experimental results show that TEXT2GRAD consistently outperforms existing methods, with higher task metrics and improved interpretability. | ['Reinforcement Learning', 'Text Generation', 'Natural Language Processing'] | [Link](https://github.com/microsoft/Text2Grad) | N/A |
| [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video
  Guidance](https://arxiv.org/abs/2505.21876) | Han Lin, Jialu Li, Jaemin Cho, Zun Wang, jaehong31 | - The paper introduces EPiC, a novel framework that learns precise camera control using a lightweight conditioning module called Anchor-ControlNet and precisely-aligned anchor videos generated by masking source videos based on first-frame visibility. - Anchor-ControlNet uses less than 1% of backbone parameters and integrates anchor video guidance in visible regions to pretrained video diffusion models. - Unlike previous methods, EPiC doesn't require camera trajectory annotations or modifications to the diffusion model backbone to mitigate misalignments. - EPiC achieves state-of-the-art performance on RealEstate10K and MiraData for I2V camera control tasks and generalizes to V2V scenarios. - The proposed method demonstrates efficiency with fewer parameters, training steps, and data compared to existing methods. | ['Image-to-Video', 'Text-to-Video', 'Video Classification', 'Multimodal'] | [Link](https://zunwang1.github.io/Epic) | N/A |
| [GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language
  Models and Enhanced Reasoning Chains](https://arxiv.org/abs/2505.18700) | Yiren Song, Haofan Wang, Zihao Pan, Xiaoran Pan, Chun Wang | - The paper introduces GRE Suite, a novel framework for geo-localization inference that augments VLMs with structured reasoning chains. - GRE Suite is composed of three key components: a high-quality geo-localization dataset (GRE30K), a multi-stage reasoning model (GRE), and a comprehensive evaluation benchmark (GREval-Bench). - The GRE model uses a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features to enhance precision in location inference. - Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, highlighting the effectiveness of reasoning-augmented VLMs. - The GRE30K dataset and the GREval-Bench are systematically developed to facilitate fine-grained visual and contextual analysis for improved accuracy and interpretability. | ['Multimodal'] | [Link](https://github.com/Thorin215/GRE) | N/A |
| [Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat
  Falsehoods](https://arxiv.org/abs/2505.17870) | Deval Pandya, Marcelo Lotif, Rizwan Qureshi, amanchadha, Shainarazavi | - The paper introduces a novel training framework called "Model Immunization" to combat the generation of false information by large language models (LLMs). - Model Immunization involves fine-tuning the model on a small, curated set of explicitly labeled falsehoods, analogous to biological immunization, to enhance the model's ability to identify and reject misleading claims. - The authors demonstrate through a case study that immunized models generate substantially less misinformation than baseline models, improving truthfulness while maintaining overall accuracy. - The proposed framework incorporates ethical safeguards and governance controls to ensure the responsible use of false data in model training. - Model Immunization offers a proactive approach to aligning AI systems with factuality, by immunizing models to falsehoods before they propagate, rather than merely reacting after misinformation is generated. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Unveiling Instruction-Specific Neurons & Experts: An Analytical
  Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191) | Zhaorui Hou, Jungang Li, Yibo Yan, Yubo Gao, Junyan Zhang | - This paper introduces HEXAINST, a meticulously curated and balanced instructional dataset, and SPARCOM, a novel analytical framework for examining how fine-tuning modifies LLMs' instruction-following capabilities. - SPARCOM comprises three key components: a method for identifying instruction-specific sparse components (neurons and experts), an evaluation of their functional generality and uniqueness, and a systematic comparison of their alterations. - Through experiments on LLaMA, Mistral, and Qwen-MoE model families, the study demonstrates the critical role of these sparse components in instruction execution, showing functional generality and uniqueness. - The findings elucidate the relationship between fine-tuning-induced adaptations and sparse computational substrates, offering insights into how LLMs internalize instruction-following behavior. - The study also proposes a three-stage framework for understanding the internal mechanism of LLMs' instruction-following capabilities. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf), [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf), [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf), [Link](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1), [Link](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B), [Link](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat) |
| [Characterizing Bias: Benchmarking Large Language Models in Simplified
  versus Traditional Chinese](https://arxiv.org/abs/2505.22645) | Allison Koenecke, Jian Kang, Jiebo Luo, Hanjia Lyu | This paper introduces SC-TC-BENCH, a benchmark dataset designed to evaluate large language models' (LLMs) performance in both Simplified and Traditional Chinese.  The study focuses on identifying biases in LLMs' responses to prompts presented in these two Chinese variants by evaluating the LLMs on two tasks, one focusing on regional term selection and the other on regional name selection.  The results reveal that LLMs exhibit biases based on both the task and the prompting language.  Finally, the authors provide an open-sourced benchmark dataset to facilitate reproducible evaluations of future LLMs. | ['Natural Language Processing'] | [Link](https://github.com/brucelyu17/SC-TC-Bench) | N/A |
| [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal
  Manga Understanding](https://arxiv.org/abs/2505.20298) | Yuki Imajuku, Atsuyuki Miyai, Shota Onohara, Kazuki Egashira, Jeonghun Baek | - This paper introduces two benchmarks for multimodal manga understanding: MangaOCR (for in-page text recognition) and MangaVQA (a novel benchmark for contextual understanding through visual question answering). - MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. - They develop MangaLMM, a manga-specialized model fine-tuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks (MangaOCR and MangaVQA). - Through extensive experiments, including comparisons with proprietary models such as GPT-40 and Gemini 2.5, they assess how well LMMs understand manga.  - The benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga. | ['Visual Question Answering', 'Multimodal'] | [Link](https://github.com/manga109/MangaLMM/) | N/A |
| [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051) | Vahab Mirrokni, Dan Alistarh, Vincent Cohen-Addad, Mahdi Nikdan |  - Influence Distillation, a novel framework for data selection, is introduced, using second-order information to optimally weigh training samples.  - The method assigns model-specific weights for selecting training data, improving LLM performance in the target domain.  - Optimal weights are derived for Gradient Descent and Adam optimizers, with a landmark-based approximation for scalability.  - Experiments on instruction tuning using the Tulu V2 dataset show that Influence Distillation matches or outperforms state-of-the-art methods, while being up to 3.5 times faster.  - The method is validated across several LLMs from the Llama and Qwen families, targeting tasks such as GSM8k, SQUAD, and MMLU. | ['Natural Language Processing'] | N/A | N/A |
| [AITEE -- Agentic Tutor for Electrical Engineering](https://arxiv.org/abs/2505.21582) | Christian Bernhardt, Alexander Bernhardt, CKnievel | - This paper introduces AITEE, an intelligent tutoring system for electrical engineering that combines large language models with agent-based tutoring to provide personalized learning experiences. - AITEE uses a novel graph-based similarity measure to identify relevant context from lecture materials and employs a Socratic dialogue approach to guide students toward solutions. - The system supports both hand-drawn and digital circuits through an adapted circuit reconstruction process and employs SPICE simulation for enhanced accuracy. - Experimental evaluations demonstrate that AITEE significantly outperforms baseline approaches in domain-specific knowledge application, particularly for complex circuits. - The results suggest that agentic tutors have the potential to deliver scalable and effective personalized learning environments for electrical engineering education. | ['Multimodal', 'Question Answering', 'Object Detection', 'Graph Machine Learning', 'Text2Text Generation'] | [Link](https://github.com/CKnievel/aitee-dataset) | [Link](string) |
| [First Finish Search: Efficient Test-Time Scaling in Large Language
  Models](https://arxiv.org/abs/2505.18149) | Tanmoy Chakraborty, Ayan Sengupta, aradhye |  - First Finish Search (FFS) is a novel training-free, parallel decoding strategy that improves reasoning in large language models (LLMs) at test time.   - FFS launches multiple independent samples and returns the first completed sample, significantly reducing compute cost and latency.   - Empirical evidence shows shorter traces are more likely to be correct in reasoning tasks, supporting FFS's early stopping mechanism.  - Experiments on four reasoning models and four datasets demonstrate that FFS consistently outperforms existing test-time scaling methods in accuracy, efficiency, and scalability.   - Theoretical analysis explains why shorter traces tend to be correct and how FFS's compute efficiency increases with more samples. | ['Question Answering'] | [Link](https://github.com/Aradhye2002/reasoning_exps) | [Link](https://huggingface.co) |


## Papers for 2025-05-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.21327) | Renrui Zhang, Yiting Lu, Yilei Jiang, Tianshuo Peng, Jiakang Yuan |  - MME-Reasoning is a comprehensive benchmark for evaluating the logical reasoning capabilities of multimodal large language models (MLLMs).  - It addresses the limitations of existing benchmarks by explicitly categorizing logical reasoning types (inductive, deductive, and abductive) and carefully curating data to focus on reasoning ability.  - Evaluation reveals substantial limitations of current state-of-the-art MLLMs in holistic logical reasoning, highlighting performance imbalances across reasoning types.  - An in-depth analysis investigates the impact of 'thinking mode' and rule-based reinforcement learning on reasoning performance.  - The findings provide valuable insights for understanding and evaluating reasoning capabilities in MLLMs. | ['Multimodal'] | [Link](https://github.com/Alpha-Innovator/MME-Reasoning) | [Link](https://huggingface.co/datasets/U4R/MME-Reasoning) |
| [Paper2Poster: Towards Multimodal Poster Automation from Scientific
  Papers](https://arxiv.org/abs/2505.21497) | Philip Torr, Xi He, HideOnBush, KevinQHLin, weipang142857 |  - This paper introduces Paper2Poster, the first benchmark and metric suite for academic poster generation.   - Paper2Poster pairs recent conference papers with corresponding author-designed posters, evaluating outputs on visual quality, textual coherence, holistic assessment, and PaperQuiz (poster's ability to convey core content).   - A multi-agent pipeline called PosterAgent is proposed, which parses papers into structured assets, plans text-visual alignments, and refines panel layouts using VLM feedback.   - Evaluations show that GPT-40, while visually appealing, produces noisy text and low PaperQuiz scores, highlighting reader engagement as a primary bottleneck.   - Open-source variants of PosterAgent significantly outperform GPT-40-driven systems across most metrics, using fewer tokens. | ['Multimodal'] | [Link](https://github.com/Paper2Poster/Paper2Poster) | N/A |
| [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189) | Ivan Oseledets, glebzok | - This paper demonstrates that frozen LLMs can generate hundreds of accurate tokens in a single forward pass without iterative decoding, using only two learned embeddings. - The study reveals a surprising capability of LLMs, namely multi-token generation without iterative decoding. - The authors investigated the behavior of the embeddings, providing insight into the information they encode, and empirically showed that these representations form connected and local regions in embedding space. -  The authors also examined the reconstruction capability variations with model size and target sequence characteristics (e.g., natural vs. synthetic text). - This research challenges the assumption that autoregressive generation is essential for reconstructing accurate multi-token sequences from compressed representations. | ['Text Generation'] | [Link](https://github.com/Glebzok/OneStepLLMGeneration) | [Link](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) |
| [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM
  Reasoning](https://arxiv.org/abs/2505.17813) | Roy Schwartz, Yossi Adi, Gabriel Synnaeve, Michael Hassid | - This paper introduces short-m@k, a novel LLM inference method that prioritizes shorter reasoning chains to improve efficiency and accuracy. - The method executes k independent generations in parallel and stops once m (m ≤ k) thinking processes are complete, using majority voting to determine the final answer. - Experiments on three reasoning LLMs and three benchmarks show that short-m@k, particularly short-1@k and short-3@k, significantly outperforms standard majority voting in terms of accuracy and speed, often using fewer thinking tokens. - It demonstrates that shorter reasoning chains are more likely to yield correct answers, challenging the conventional wisdom that longer chains lead to better reasoning. - Further, finetuning an LLM using short reasoning chains improves performance, reinforcing the paper's main argument that prioritizing brevity in reasoning can yield significant improvements. | ['Question Answering'] | N/A | N/A |
| [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based
  Mobile GUI Agents](https://arxiv.org/abs/2505.21496) | Zimu Lu, Yuxiang Chai, Guozhi Wang, AJZhou, HanXiao1999 |  - The paper introduces UI-Genie, a self-improving framework for training MLLM-based mobile GUI agents that addresses the challenges of outcome verification and scalability of high-quality training data.   - UI-Genie uses a reward model, UI-Genie-RM, with an image-text interleaved architecture to process historical context and unify action and task-level rewards.  - UI-Genie includes data generation strategies like rule-based verification, controlled trajectory corruption, and hard negative mining to support UI-Genie-RM training.   - The self-improvement pipeline in UI-Genie iteratively enhances both the agent and reward models through reward-guided exploration and outcome verification, expanding solvable GUI tasks.  - Experiments show that UI-Genie achieves state-of-the-art performance on multiple GUI agent benchmarks across three generations of self-improvement. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/Euphoria16/UI-Genie) | N/A |
| [MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks](https://arxiv.org/abs/2505.16459) | Chaoran Hu, Ruihang Zhang, Tianhe Gu, Guiyao Tie, zhouxueyang | This paper introduces MMMR, a new benchmark for evaluating multi-modal reasoning in large language models.  MMMR includes a challenging dataset of 1,083 questions spanning six diverse reasoning types and a modular evaluation pipeline assessing reasoning quality beyond accuracy. Results show that models incorporating explicit thinking traces perform better but still exhibit reasoning pathologies.  MMMR provides a scalable and comprehensive evaluation for multi-modal reasoning systems. The benchmark is designed to rigorously evaluate multi-modal reasoning with explicit thinking traces. | ['Multimodal'] | [Link](https://mmmr-benchmark.github.io/) | N/A |
| [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for
  Subject-to-Video Generation](https://arxiv.org/abs/2505.20292) | chongyangma, Jinfa, dyf, pkuhexianyi, BestWishYsh |  - This paper introduces OpenS2V-Nexus, a comprehensive benchmark and million-scale dataset for Subject-to-Video (S2V) generation.  - OpenS2V-Eval, a fine-grained benchmark, is proposed to evaluate S2V models on seven categories, using three automatic metrics focusing on subject consistency, naturalness, and text relevance. - OpenS2V-5M, a large-scale dataset with 5.4 million high-quality 720P subject-text-video triples, incorporates subject diversity through cross-video associations and GPT-Image prompting.  - Evaluations on 16 S2V models highlight the strengths and weaknesses of existing methods, showing that closed-source models generally outperform open-source models.  - The dataset and benchmark aim to accelerate future S2V generation research by addressing the three major challenges in this area: poor generalization, copy-paste issues, and inadequate human fidelity. | ['Text-to-Video', 'Image-to-Video', 'Multimodal'] | [Link](https://github.com/PKU-YuanGroup/OpenS2V-Nexus) | [Link](https://huggingface.co/collections/BestWishYsh) |
| [GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient
  Fine-Tuning](https://arxiv.org/abs/2505.20355) | Eunhyeok Park, Taesu Kim, Hyungjun Kim, Daehyun Ahn, yeonjoon-jung | - This paper introduces GraLoRA, a novel method for parameter-efficient fine-tuning of large language models that addresses the limitations of existing low-rank adaptation methods like LoRA. - GraLoRA partitions weight matrices into sub-blocks, each with its own low-rank adapter, overcoming LoRA's overfitting issues at higher ranks. - Experimental results on code generation and commonsense reasoning benchmarks demonstrate that GraLoRA consistently outperforms LoRA and other baselines, achieving significant gains in accuracy. - The improvement is consistent across different model sizes and rank settings, showcasing the scalability and robustness of GraLoRA. - GraLoRA's enhanced expressivity and robustness to input outliers make it a promising solution for parameter-efficient fine-tuning. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SqueezeBits/GraLoRA.git) | N/A |
| [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale
  Verified Dataset](https://arxiv.org/abs/2505.21297) | Xudong Zhou, Bingcheng Dong, Yi Zhu, Li Lyna Zhang, YF-L | - The paper introduces rStar-Coder, a large-scale (418K problems, 580K solutions) verified dataset for improving code reasoning in large language models. - rStar-Coder significantly improves LLM code reasoning capabilities by providing high-difficulty competition-level code problems and solutions with rich test cases. - The dataset construction involves three core contributions: curating competitive programming problems, building a reliable input-output test case synthesis pipeline, and augmenting problems with high-quality, test-case-verified long-reasoning solutions. - Experiments on Qwen models demonstrate rStar-Coder's effectiveness, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. - rStar-Coder consistently improves the performance of LLMs on various code reasoning benchmarks, surpassing existing state-of-the-art models in several cases. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/microsoft/rStar) | N/A |
| [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent
  Systems](https://arxiv.org/abs/2505.18943) | Yixuan Li, Yuxuan Chen, samuelyeh, XUANMINGZHANG | MetaMind is a novel multi-agent framework designed for emulating human-like social reasoning.  It decomposes social understanding into three collaborative stages: a Theory-of-Mind Agent, a Domain Agent, and a Response Agent.  MetaMind achieves state-of-the-art performance across multiple benchmarks, including a 35.7% improvement in real-world social scenarios and surpasses human-level performance on key ToM tasks for the first time.  Ablation studies demonstrate the importance of each component in balancing contextual plausibility, social appropriateness, and user adaptation. The framework advances AI towards human-like social intelligence. | ['Natural Language Processing'] | [Link](https://github.com/XMZhangAI/MetaMind) | N/A |
| [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language
  Neurons Perspective](https://arxiv.org/abs/2505.21505) | Xiao Liu, Shuaijie She, Xiang Liu, DreamW1ngs, Shimao-Zhang | - This paper proposes a novel finer-grained neuron identification algorithm to enhance LLMs' multilingual capabilities by detecting language neurons (including language-specific and language-related neurons) and language-agnostic neurons. - The proposed algorithm improves upon existing methods by more precisely categorizing neurons, leading to a more comprehensive understanding of LLMs' multilingual mechanisms. - The paper divides the LLMs' internal process for multilingual inference into four parts: multilingual understanding, shared semantic space reasoning, multilingual output space transformation, and vocabulary space outputting, providing a detailed analysis of each stage. - The study systematically analyzes models before and after alignment, focusing on different neuron types, and reveals how multilingual alignment significantly enhances the activation of relevant neuron types. - Empirical results demonstrate the effectiveness of the proposed algorithm and provide valuable insights into multilingual alignment and the multilingual capabilities of LLMs, including the phenomenon of "Spontaneous Multilingual Alignment". | ['Natural Language Processing', 'Translation'] | [Link](https://github.com/NJUNLP/Language-Neurons-Alignment) | [Link](https://huggingface.co/kevinpro/MistralMathOctopus-7B), [Link](https://huggingface.co/kevinpro/MetaMathOctopus-7B), [Link](https://huggingface.co/facebook/nllb-200-distilled-600M) |
| [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with
  Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952) | Yong Dai, Zhongwei Wan, Jiazhen Pan, Haozhe Wang, Che Liu | - AlphaMed is a novel medical LLM that achieves state-of-the-art results on six medical QA benchmarks by using reinforcement learning with minimalist rule-based rewards, without relying on supervised fine-tuning or distilled chain-of-thought data. - It surpasses larger and closed-source models, demonstrating the effectiveness of its minimalist approach. - The study shows that dataset informativeness is a key factor influencing reasoning performance. - Analysis reveals that minimalist rule-based RL can effectively incentivize reasoning in LLMs, even with small datasets. - The authors underscore the limitations of current medical QA benchmarks and call for more challenging, reasoning-oriented benchmarks to fully evaluate medical LLMs. | ['Question Answering'] | [Link](https://cheliu-computation.github.io/AlphaMed/) | N/A |
| [Active-O3: Empowering Multimodal Large Language Models with Active
  Perception via GRPO](https://arxiv.org/abs/2505.21457) | Zongze Du, Hao Zhong, MingyuLiu, Canyu, Z-MU-Z | - ACTIVE-03 is a novel reinforcement learning-based training framework that equips Multimodal Large Language Models (MLLMs) with active perception capabilities using Group Relative Policy Optimization (GRPO). - The model significantly enhances active perception capabilities compared to existing methods, such as Qwen-VL2.5-CoT, across various tasks including open-world object grounding and domain-specific scenarios like remote sensing and autonomous driving. - ACTIVE-03 employs a two-stage policy separating region proposal and task execution, incorporating structured instruction prompts and a dual-form reward design for effective training. - A comprehensive benchmark suite is established to evaluate ACTIVE-03, covering diverse tasks and scenarios. The code and evaluation protocols are publicly released. - Experiments demonstrate that ACTIVE-03 consistently improves search efficiency and accuracy under fixed computational budgets, showing remarkable zero-shot generalization on challenging tasks. | ['Multimodal', 'Reinforcement Learning', 'Robotics', 'Computer Vision', 'Object Detection', 'Image Segmentation', 'Zero-Shot Object Detection'] | [Link](https://github.com/aim-uofa/Active-03) | N/A |
| [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering
  Target Atoms](https://arxiv.org/abs/2505.20322) | Shumin Deng, Shengyu Mao, Ziwen Xu, Mengru Wang, Ningyu | - This paper introduces Steering Target Atoms (STA), a novel method for precise behavior control in large language models (LLMs). - STA manipulates disentangled knowledge components to enhance safety and reliability, surpassing conventional steering techniques. - Comprehensive experiments demonstrate STA's effectiveness, particularly in adversarial scenarios, exhibiting superior robustness and flexibility. - The method is applied to a large reasoning model, confirming its effectiveness in precise reasoning control. - An analysis comparing STA to prompting techniques reveals that STA provides finer-grained control and robustness. | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/steer-target-atoms) | N/A |
| [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in
  Brain MRI](https://arxiv.org/abs/2505.14064) | Lena Schmitzer, Evamaria O. Riedel, Philipp Raffler, Jun Li, ci-ber | - This paper introduces NOVA, a new benchmark dataset for anomaly localization and clinical reasoning in brain MRI. - NOVA contains approximately 900 brain MRI scans spanning 281 rare pathologies and diverse acquisition protocols, along with detailed clinical narratives and expert annotations. - The benchmark evaluates models on three tasks: anomaly localization, image captioning, and diagnostic reasoning. - Experiments on state-of-the-art vision-language models reveal substantial performance drops across all tasks, highlighting the challenge of generalizing to truly unknown anomalies in medical imaging. - NOVA serves as a rigorous testbed for advancing models that can detect, localize, and reason about unseen anomalies in real-world clinical settings. | ['Multimodal'] | N/A | [Link](https://huggingface.co/datasets/Ano-2090/Nova) |
| [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in
  Vision-Language Models](https://arxiv.org/abs/2505.21500) | Hang Zhang, Yuchen Yan, Zixuan Wang, Hongxing Li, Dingming Li | - ViewSpatial-Bench, a comprehensive benchmark for evaluating multi-perspective spatial localization in vision-language models (VLMs), is introduced.  The benchmark contains over 5,700 question-answer pairs across five distinct task types, systematically assessing VLMs' spatial reasoning from both camera and human perspectives. - An automated 3D spatial annotation pipeline efficiently generates large-scale, precisely annotated multi-view datasets. This pipeline provides rich spatial relationship data for VLM training. - A Multi-View Spatial Model (MVSM) is developed and trained on a large-scale multi-viewpoint VQA dataset.  The model achieves a 46.24% overall performance improvement over baselines, demonstrating the effectiveness of the proposed methodology. - The study reveals a significant performance disparity between models' performance on camera-perspective and human-perspective tasks, highlighting a critical limitation in current VLMs' ability to generalize to allocentric viewpoints. - The MVSM is evaluated on VSI-Bench and a custom VSI-App dataset, showing that it generalizes well to tasks requiring perspective transformation in both indoor and outdoor scenarios. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics
  Reasoning](https://arxiv.org/abs/2505.19099) | Zirong Liu, Terry Jingchen Zhang, Kun Xiang, yinyahuang, HengLi29 | This paper introduces SEEPHYS, a large-scale multimodal benchmark for evaluating large language models' (LLMs) physics reasoning capabilities.  SEEPHYS comprises 2000 rigorously validated questions across seven core physics domains and 21 diagram types, spanning from middle school to PhD levels. The benchmark features a significant proportion of vision-essential problems (75%) where visual information is crucial for solving, revealing significant challenges for even the most advanced models, which achieve below 60% accuracy.   The analysis highlights challenges in coupling visual interpretation with physics reasoning and the models' over-reliance on textual cues.  The dataset and experimental results are available on GitHub and Hugging Face. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/SeePhys/seephys-project) | [Link](https://huggingface.co/datasets/SeePhys/SeePhys) |
| [Adversarial Attacks against Closed-Source MLLMs via Feature Optimal
  Alignment](https://arxiv.org/abs/2505.21494) | Chao Du, Tianyu Pang, Simeng Qin, Sensen Gao, jiaxiaojunQAQ |  - This paper introduces FOA-Attack, a novel targeted transferable adversarial attack method for multimodal large language models (MLLMs). - FOA-Attack achieves superior transferability compared to existing methods, especially against closed-source MLLMs, by aligning both global and fine-grained features. - The method uses a cosine similarity-based global feature loss and a local clustering optimal transport loss to refine feature alignment. - A dynamic ensemble model weighting strategy is employed to further enhance transferability. - Extensive experiments demonstrate that FOA-Attack outperforms state-of-the-art methods across various models. | ['Multimodal'] | [Link](https://github.com/jiaxiaojunQAQ/FOA-Attack) | N/A |
| [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.21178) | Mao Zheng, Nickyang | - This paper introduces ConciseR, a novel two-stage reinforcement learning framework for achieving concise reasoning in LLMs. - ConciseR addresses the overthinking phenomenon in LLMs by optimizing response length only after all rollouts are correct, following a "walk before you run" principle. - The framework uses Group Relative Policy Optimization with clip-higher and dynamic sampling in the first stage to improve reasoning capabilities and Length-aware Group Relative Policy Optimization in the second stage to enforce conciseness. - Experimental results demonstrate that ConciseR outperforms state-of-the-art reasoning models across various benchmarks, achieving significant improvements in accuracy and efficiency. - The code, training dataset, and model checkpoints will be publicly released. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/nick7nlp/ConciseR) | [Link](https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset), [Link](https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k), [Link](https://huggingface.co/datasets/EleutherAI/hendrycks_math) |
| [VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual
  Tool Selection](https://arxiv.org/abs/2505.20289) | Wen Xiao, Zefan Cai, Yuyang Ji, AniSundar18, ZeyiHuang1010 | - This paper introduces VisualToolAgent (VisTA), a novel reinforcement learning framework designed for visual tool selection in complex visual reasoning tasks. - VisTA dynamically explores, selects, and combines tools from a diverse library based on empirical performance, unlike previous methods that rely on training-free prompting or large-scale fine-tuning. - The framework uses Group Relative Policy Optimization (GRPO) to enable agents to autonomously discover effective tool-selection pathways without explicit reasoning supervision. - Experimental results on ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. - VisTA's ability to enhance generalization, adaptively utilize diverse tools, and improve efficiency highlights its potential for flexible, experience-driven visual reasoning systems. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://oodbag.github.io/vista_web/) | N/A |
| [Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic
  Capabilities in LLM Compression](https://arxiv.org/abs/2505.19433) | Xiaowen Chu, Lujun Li, Xiang Liu, Zhenheng Tang, Peijie Dong |  - This paper introduces Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression affects LLMs' agentic capabilities.  - ACBench evaluates 12 tasks across four capabilities: workflow generation, tool use, long-context understanding, and real-world application.  - The benchmark includes quantization and pruning methods (GPTQ, AWQ, Wanda, SparseGPT), and uses 15 models of different sizes and reasoning abilities.  - Experiments reveal compression trade-offs: 4-bit quantization preserves workflow generation and tool use but degrades real-world application accuracy.  -  Three novel metrics (ERank, Top-k Ranking Correlation, Energy) are introduced for systematic analysis of compression impacts. | ['Natural Language Processing'] | [Link](https://github.com/pprp/ACBench) | [Link](null) |
| [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs
  via Reinforcement Learning](https://arxiv.org/abs/2505.17005) | Zhipeng Chen, Wenqing Tian, Jinhao Jiang, Huatong Song, EliverQ | - R1-Searcher++, a novel framework, is introduced to enhance LLMs' ability to dynamically leverage both internal and external knowledge sources. - It employs a two-stage training strategy: an initial Supervised Fine-Tuning (SFT) phase for format learning followed by Reinforcement Learning (RL) for dynamic knowledge acquisition. - The RL stage incorporates outcome supervision to incentivize exploration, a reward mechanism for internal knowledge utilization, and a memorization mechanism to assimilate retrieved information. - Experimental results demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods, achieving efficient retrieval while reducing the number of retrievals. - The code is publicly available on GitHub. | ['Question Answering'] | [Link](https://github.com/RUCAIBox/R1-Searcher-plus) | N/A |
| [DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in
  Digital Forensics and Incident Response](https://arxiv.org/abs/2505.19973) | Saeed Alshehhi, Aaesha Aldahmani, Richard A. Dubniczky, Tamas Bisztray, Bilel Cherif | - This paper introduces DFIR-Metric, a novel benchmark dataset designed for evaluating the performance of Large Language Models (LLMs) in Digital Forensics and Incident Response (DFIR). - DFIR-Metric is composed of three modules: Knowledge Assessment (multiple-choice questions), Realistic Forensic Challenges (CTF-style tasks), and Practical Analysis (disk and memory forensics cases). - The benchmark rigorously evaluates LLMs across various DFIR tasks, focusing on technical accuracy and procedural rigor.  - A new metric, the Task Understanding Score (TUS), is introduced to enhance the evaluation by considering partial successes in complex multi-step tasks.  - All scripts, artifacts, and results are publicly available to ensure reproducibility and facilitate further research and development in the field. | ['Question Answering'] | [Link](https://github.com/DFIR-Metric) | N/A |
| [SoloSpeech: Enhancing Intelligibility and Quality in Target Speech
  Extraction through a Cascaded Generative Pipeline](https://arxiv.org/abs/2505.19314) | Kai Li, Chen Chen, Dongchao Yang, Jiarui Hai, Helin Wang | - This paper introduces SoloSpeech, a novel cascaded generative pipeline for target speech extraction (TSE). - SoloSpeech consists of three main components: a generative audio compressor, a generative target extractor, and a generative corrector. - The target extractor in SoloSpeech is speaker-embedding-free, utilizing conditional information from the cue audio's latent space. - Experimental results on the Libri2Mix dataset show that SoloSpeech achieves state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks. - SoloSpeech demonstrates strong generalization capabilities on out-of-domain data and real-world scenarios. | ['Audio-to-Audio'] | [Link](https://github.com/WangHelin1997/SoloSpeech) | [Link](https://wanghelin1997.github.io/SoloSpeech-Demo/) |
| [MLLMs are Deeply Affected by Modality Bias](https://arxiv.org/abs/2505.18657) | Yuanhuiyi Lyu, Kaiyu Lei, Yuqian Fu, Xu Zheng, Chenfei-Liao | - This paper investigates modality bias in Multimodal Large Language Models (MLLMs), demonstrating that MLLMs heavily rely on language while underutilizing other modalities. - The authors propose a research roadmap to mitigate modality bias, highlighting three key directions: measuring bias through benchmarks, avoiding bias through dataset construction, and reducing bias through specific methods. - Five key factors contributing to modality bias are identified: data characteristics, imbalanced backbone capabilities, training objectives, asymmetric modal backbone capabilities, and modal interactions and integrations. - Experiments demonstrate the influence of each factor, highlighting the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. - The authors call for interdisciplinary efforts to address these challenges, offering actionable suggestions for future research to advance progress towards Artificial General Intelligence. | ['Multimodal'] | N/A | N/A |
| [ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and
  Reactive Feedback](https://arxiv.org/abs/2505.17908) | Jinsong Zhou, Jiantao Lin, Luozhou Wang, Xinli Xu, Litao Guo | - ComfyMind is a general-purpose generative framework that uses tree-based planning and reactive feedback to improve the robustness and scalability of complex generation workflows. - It utilizes a semantic workflow interface (SWI) to abstract low-level node graphs into callable functional modules, enabling high-level composition and reducing structural errors. - ComfyMind's search tree planning mechanism models generation as a hierarchical decision process and allows for adaptive corrections at each stage, improving stability and flexibility. - Evaluations on three public benchmarks (ComfyBench, GenEval, and Reason-Edit) demonstrate that ComfyMind consistently outperforms existing open-source baselines. - The framework achieves performance comparable to GPT-Image-1, paving a path for open-source general-purpose generative AI systems. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Video', 'Image-to-Text', 'Any-to-Any'] | [Link](https://github.com/LitaoGuo/ComfyMind) | N/A |
| [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large
  Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673) | Yibo Wang, Min Yang, Jingyi Zhang, Qixiang Yin, Huanjin Yao | - Share-GRPO, a novel reinforcement learning approach, is proposed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs). - The method addresses the sparse reward and advantage vanishing issues in reinforcement learning by expanding the question space and sharing diverse reasoning trajectories. - Share-GRPO employs semantically consistent transformations to expand the question space, encouraging the exploration of diverse reasoning paths. - Reward information is shared during advantage computation, estimating advantages hierarchically, improving accuracy and stability. - Extensive evaluations on six reasoning benchmarks demonstrate Share-GRPO's superior performance compared to existing state-of-the-art methods. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | [Link](https://github.com/HJYao00/R1-ShareVL) | N/A |
| [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning
  of LLMs](https://arxiv.org/abs/2505.11277) | Junfeng Fang, Zhiyuan Liu, Chang Wu, Shihan Li, yrshi | - AutoRefine, a novel reinforcement learning post-training framework, is proposed to improve the reasoning capabilities of LLMs by incorporating a "search-and-refine-during-think" paradigm. - The model iteratively refines retrieved knowledge through explicit refinement steps between successive search calls, enhancing the accuracy of reasoning. - AutoRefine utilizes both outcome-based and retrieval-specific rewards, optimizing the model's ability to effectively retrieve and utilize relevant information. - Experiments on various QA benchmarks demonstrate that AutoRefine significantly outperforms existing methods, especially in complex multi-hop reasoning scenarios. - Detailed analysis shows that AutoRefine issues more frequent, higher-quality searches and synthesizes evidence effectively. | ['Question Answering'] | [Link](https://github.com/syr-cn/AutoRefine) | N/A |
| [Modality Curation: Building Universal Embeddings for Advanced Multimodal
  Information Retrieval](https://arxiv.org/abs/2505.19650) | Shi Feng, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, friedrichor |  - UNITE, a universal multimodal framework, addresses multimodal information retrieval challenges by introducing data curation and modality-aware training.   - It achieves state-of-the-art results on multiple benchmarks, surpassing existing methods by significant margins.    - The model architecture uses a large language model, a vision encoder, and a vision projector, handling various combinations of modalities.  -  A Modal-Aware Masked Contrastive Learning (MAMCL) strategy is used to improve the performance of training.  -  Extensive experiments demonstrate the effectiveness of UNITE across various retrieval scenarios. | ['Multimodal'] | [Link](https://friedrichor.github.io/projects/UNITE) | N/A |
| [Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion
  Enhances Protein Representations](https://arxiv.org/abs/2505.20052) | Ahmed Elnaggar, Mohamed Elkerdawy, Mohamed Elshaffei, Hazem Alsamkary | - This paper introduces Ankh3, a multi-task protein language model (PLM) that enhances protein representations by incorporating sequence denoising and completion tasks during pre-training. - Ankh3 leverages a T5 transformer architecture with an encoder-decoder structure and is trained on two tasks: masked language modeling with varying masking probabilities and protein sequence completion. - The model's performance was evaluated on various downstream tasks such as secondary structure prediction, fluorescence prediction, GB1 fitness, and contact prediction, demonstrating improved accuracy compared to previous Ankh models and competitive results against other state-of-the-art PLMs like ESM2 and ESM3. - The experiments show that Ankh3 benefits from the multi-task learning approach which leads to improved robustness and generalization across downstream tasks. - The Ankh3 models and pre-training data are made publicly available on HuggingFace for reproducibility and further research. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/agemagician/uniref50), [Link](https://huggingface.co/ElnaggarLab/ankh3-large), [Link](https://huggingface.co/ElnaggarLab/ankh3-xl) |
| [Beyond Simple Concatenation: Fairly Assessing PLM Architectures for
  Multi-Chain Protein-Protein Interactions Prediction](https://arxiv.org/abs/2505.20036) | Abdallah Amr, Sara Ossman, Mohamed Soudy, Mohamed Elshaffei, Hazem Alsamkary | - This paper introduces a meticulously curated version of the PPB-Affinity dataset containing 8,207 unique protein-protein interaction entries, addressing annotation inconsistencies and duplicates. - Four novel architectures for adapting PLMs to PPI binding affinity prediction are proposed and evaluated: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). - The HP and PAD architectures consistently outperform EC and SC methods, achieving up to a 12% increase in Spearman correlation, highlighting the importance of sophisticated architectural designs. - The study uses two training methods: full fine-tuning and a lightweight approach using ConvBERT heads. - Experiments were conducted across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3), demonstrating the superior performance of HP and PAD. | ['Natural Language Processing'] | [Link](https://github.com/Proteinea/ppiseq) | [Link](https://huggingface.co/datasets/proteinea/ppb_affinity) |
| [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561) | Fabrizio Silvestri, Yoelle Maarek, Guy Horowitz, Simone Filice, florin-hf | - This paper investigates the impact of positional bias in Retrieval Augmented Generation (RAG) systems, focusing on how the LLM's weighting of information based on its position in the prompt affects both relevant and distracting passages. - Through extensive experiments on three benchmarks, the authors demonstrate that state-of-the-art retrieval pipelines tend to retrieve highly distracting passages alongside relevant ones. - The results reveal that the impact of positional bias is marginal in real-world RAG scenarios since both relevant and distracting passages are penalized by the LLM's positional preferences. - Sophisticated strategies aimed at rearranging passages based on LLM positional preferences do not outperform random shuffling, suggesting that focusing on retrieval quality and LLM robustness is more crucial than optimizing passage ordering. - The study's findings highlight the need for future research to improve retrieval quality and reduce the LLM's susceptibility to distraction rather than focusing on mitigating positional bias. | ['Question Answering'] | N/A | N/A |


## Papers for 2025-05-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147) | Pppeach33, coderchen01, Steven-Shaobo, zichenwen, xuyang-liu16 |  - This paper advocates for a paradigm shift in AI efficiency research, moving from model-centric to data-centric compression, focusing on token compression. - The authors introduce a unified mathematical framework that encompasses existing model efficiency strategies, and argue that the quadratic cost of self-attention in LLMs is now a primary bottleneck.  - Token compression methods aim to improve AI efficiency by reducing the number of tokens during model training or inference. - The authors analyze current challenges and outline promising future directions for token compression research.  -  The study systematically reviews the research landscape of token compression, and presents compelling advantages and challenges. | ['Natural Language Processing', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression) | N/A |
| [BizFinBench: A Business-Driven Real-World Financial Benchmark for
  Evaluating LLMs](https://arxiv.org/abs/2505.19457) | Ji Liu, Qlisp, Tinker250, xuntao, guilong |  - BizFinBench, a new benchmark for evaluating LLMs in real-world financial applications, is introduced. It comprises 6,781 well-annotated queries in Chinese, covering five dimensions and nine fine-grained categories.  - IteraJudge, a novel LLM evaluation method, is presented to mitigate bias when LLMs serve as evaluators, enhancing the robustness and reliability of the benchmark.  - Experiments on 25 LLMs, including both proprietary and open-source models, reveal that while current LLMs handle routine financial queries proficiently, complex scenarios demand significant improvement.  - The benchmark highlights the capabilities of different LLMs across various financial tasks, uncovering distinct capability patterns, such as the stronger performance of proprietary models in reasoning.  - BizFinBench offers a rigorous, business-aligned benchmark for future research; the dataset and code are publicly available. | ['Natural Language Processing', 'Question Answering', 'Text Classification'] | [Link](https://github.com/HiThink-Research/BizFinBench) | N/A |
| [PATS: Process-Level Adaptive Thinking Mode Switching](https://arxiv.org/abs/2505.19250) | Shujian Huang, Jiajun Chen, Shimao Zhang, master-lan, Yi53 | - This paper introduces PATS (Process-Level Adaptive Thinking Mode Switching), a novel reasoning paradigm that dynamically adjusts the reasoning strategy of LLMs based on the difficulty of each reasoning step. - PATS integrates Process Reward Models (PRMs) with Beam Search, incorporating progressive mode switching and bad-step penalty mechanisms to optimize the balance between accuracy and computational efficiency. - Experiments on various mathematical benchmarks demonstrate that PATS achieves high accuracy with moderate token usage, outperforming existing methods that use a fixed reasoning strategy. - The analysis of PATS highlights the significance of process-level, difficulty-aware reasoning strategy adaptation, offering valuable insights into efficient inference for LLMs. - Future work involves extending the experiments to larger-scale models and exploring alternative evaluation methods to further validate the proposed paradigm. | ['Question Answering'] | [Link](https://github.com/NJUNLP/PATS) | N/A |
| [ARM: Adaptive Reasoning Model](https://arxiv.org/abs/2505.20258) | Kai Zhang, Aili Chen, Arist12, hsaest, Siye01 | - This paper introduces the Adaptive Reasoning Model (ARM), a novel reasoning model that dynamically selects appropriate reasoning formats based on task difficulty. - ARM incorporates three efficient reasoning formats (Direct Answer, Short CoT, and Code) alongside a more elaborate format (Long CoT) to enhance efficiency. - The model is trained using Ada-GRPO, an improved version of Group Relative Policy Optimization (GRPO), which mitigates the format collapse issue and improves training speed. - Experimental results demonstrate that ARM achieves comparable performance to models solely using Long CoT while reducing the token count by an average of 30% and up to 70%. - In addition to the adaptive mode, ARM offers instruction-guided and consensus-guided modes for better control and performance prioritization. | ['Question Answering'] | [Link](https://team-arm.github.io/arm) | N/A |
| [B-score: Detecting biases in large language models using response
  history](https://arxiv.org/abs/2505.18545) | Daeyoung Kim, anhng8, taesiri, anvo25 | The paper introduces B-score, a novel metric for detecting biases in LLMs using response history.  The study finds that LLMs can reduce bias when allowed to observe previous answers in multi-turn conversations, particularly in random questions.  B-score is shown to significantly improve answer verification accuracy compared to using confidence scores or single-turn answers alone on benchmark datasets (MMLU, HLE, CSQA).  A 2-step verification framework using B-score is also proposed and tested.  The code and data are available at b-score.github.io. | ['Question Answering'] | [Link](https://github.com/b-score/b-score) | [Link](string) |
| [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815) | Linchen Xiao, Hongwei Liu, zsytony, Sudanl, jnanliu | This paper introduces a novel framework, RAML, which interprets LLM reasoning through the lens of meta-learning.  Reasoning trajectories are formalized as pseudo-gradient descent updates to the LLM's parameters, providing a novel way to understand and analyze LLM reasoning.  Empirical evaluations on mathematical reasoning tasks using QwQ-32B and other LLMs demonstrate the strong connection between LLM reasoning and meta-learning.  RAML enhances the understanding of LLM reasoning and provides insights for improving LLMs via established meta-learning techniques.  The paper also analyzes the effects of different training strategies (SFT and RL) on the effectiveness of LLM reasoning. | ['Natural Language Processing'] | [Link](https://github.com/open-compass/RaML) | [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-aime) |
| [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259) | Min Lin, Chao Du, Yifei Zhao, Zeyu Qin, Haoyu Wang | - This paper introduces a lifelong safety alignment framework for Language Models (LLMs) that enables continuous adaptation to new and evolving jailbreaking strategies. - The framework uses a competitive setup between a Meta-Attacker, trained to discover novel jailbreaking strategies, and a Defender, trained to resist them. - To warm-up the Meta-Attacker, the authors leverage the GPT-4 API to extract key insights from a large collection of jailbreak-related research papers. - Iterative training leads to the Defender progressively improving its robustness and reducing the Meta-Attacker's success rate to 7%. - The proposed framework demonstrates improved robustness against both seen and unseen attacks compared to existing methods. | ['Natural Language Processing', 'Text Classification', 'Reinforcement Learning'] | [Link](https://github.com/sail-sg/LifelongSafetyAlignment) | N/A |
| [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis
  Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209) | Wei Li, Yujie Liu, Ben Gao, Wanhao Liu, ZonglinY | - This paper introduces a novel task of fine-grained scientific hypothesis discovery, focusing on generating detailed, experimentally actionable hypotheses. - It proposes a hierarchical search method to address the combinatorial optimization problem inherent in this task, showing improvements over baseline methods. - The method leverages LLMs' internal heuristics to formulate hypotheses and evaluates the alignment between LLM-judged hypotheses and ground-truth hypotheses. - Experiments on a new chemistry benchmark demonstrate that the proposed method consistently outperforms strong baselines in LLM self-evaluation, expert evaluation, and recall. - The study also explores the impact of using diverse LLMs versus identical LLMs within an ensemble for improved hypothesis generation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual
  Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675) | Lingdong Kong, Shuyi Ouyang, Song Wang, Huan-WhoRegisteredMyName, FSCCS | - This paper introduces REASONMAP, a benchmark dataset designed to evaluate the fine-grained visual understanding and spatial reasoning capabilities of Multimodal Large Language Models (MLLMs). - REASONMAP contains high-resolution transit maps from 30 cities and includes 1008 question-answer pairs covering two question types and three templates. - The dataset is evaluated using a two-level framework assessing both answer correctness and quality, revealing a counterintuitive pattern where base models outperform reasoning models in open-source settings, but the opposite is true for closed-source models. - Experimental results highlight the importance of visual input for strong performance, even with additional textual information. - The study contributes to a deeper understanding of visual reasoning in MLLMs and investigates the gap between open-source and closed-source models. | ['Multimodal', 'Visual Question Answering'] | [Link](https://fscdc.github.io/Reason-Map) | N/A |
| [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal
  Large Language Models](https://arxiv.org/abs/2505.18536) | Yifei Zhao, Yifu Luo, Bo Xia, Jiaqi Wu, Haoyuan Sun | This paper explores the effectiveness of reinforcement fine-tuning (RFT) in enhancing the reasoning capabilities of multimodal large language models (MLLMs).  The authors argue that RFT empowers MLLMs with robust reasoning capabilities across diverse modalities, tasks, and domains.  The paper provides a comprehensive overview of existing RFT methods for MLLMs and proposes five promising directions for future research.  A key contribution is the meticulous summarization of advancements in five key areas: diverse modalities, tasks and domains, training algorithms, benchmarks, and engineering frameworks.  Finally, the authors offer recommendations for future research directions to further advance the state-of-the-art in MLLM reasoning. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs) | N/A |
| [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601) | Se-Young Yun, Sungwoo Cho, Jongwoo Ko, sungnyun |  - FLEX-Judge is a reasoning-guided multimodal judge model that generalizes across multiple modalities using minimal textual reasoning data. - The model leverages structured textual reasoning explanations to enable effective transfer to multimodal judgments, including images, videos, and audio. - Empirical results demonstrate that FLEX-Judge achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators, despite being trained on significantly fewer text data. - FLEX-Judge demonstrates broad impact in modalities like molecule, where evaluation benchmarks are scarce. - The framework highlights reasoning-based text supervision as a powerful cost-effective alternative to traditional annotation-intensive approaches. | ['Multimodal', 'Image-to-Text', 'Image-to-Image', 'Video-Text-to-Text', 'Text-to-Image', 'Audio', 'Text-to-Speech'] | [Link](https://github.com/jongwooko/flex-judge) | N/A |
| [Which Data Attributes Stimulate Math and Code Reasoning? An
  Investigation via Influence Functions](https://arxiv.org/abs/2505.19949) | Zhijie Deng, Zihao Zeng, Hanwen Xu, Qingyuan Tian, Siqi Kou | - This paper introduces Infra, a novel Influence-based Reasoning Attribution technique, to systematically analyze the impact of individual training examples, sequences, and tokens on Large Language Models' (LLMs) reasoning abilities in math and coding tasks. - Infra reveals that high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks mainly benefit code reasoning; this finding leads to a dataset reweighting strategy that improves AIME24 accuracy by 100% and LiveCodeBench accuracy by approximately 5%. - The study further explores the impact of sequence-level exploratory behaviors (verification, backtracking, etc.) and finds that such behaviors enhance reasoning performance in both domains. - Fine-grained token-level analysis reveals distinct influence patterns for math (natural language logic) and code (structural syntax) reasoning. - Overall, this research provides valuable insights into the factors that contribute to effective data for training LLMs and offers a novel technique for systematically investigating the effects of training data on LLM reasoning abilities. | ['Natural Language Processing'] | N/A | N/A |
| [Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System
  Collaboration](https://arxiv.org/abs/2505.20256) | Zheng Huang, Zongze Du, Muzhi Zhu, Hao Zhong, Canyu | - This paper introduces Omni-R1, a novel two-system architecture for omnimodal reasoning that uses reinforcement learning to address the trade-off between temporal coverage and spatial resolution. - The architecture consists of a Global Reasoning System that selects informative keyframes and a Detail Understanding System that performs pixel-level grounding on the selected snippets. - Omni-R1 is trained end-to-end using Group Relative Policy Optimization (GRPO), resulting in superior performance on two challenging benchmarks, RefAVS and REVOS, compared to existing state-of-the-art models. - The model shows significant improvements in out-of-domain generalization and reduces multimodal hallucination. - This work demonstrates the first successful application of RL to large-scale omnimodal reasoning. | ['Reinforcement Learning', 'Multimodal', 'Video Classification', 'Image Segmentation', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/aim-uofa/Omni-R1) | N/A |
| [Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured
  Multi-Turn Decomposition](https://arxiv.org/abs/2505.19788) | Zhijie Deng, Hao Zhang, Boxiu Li, Zihao Zeng, ElysiaTrue | - This paper introduces Multi-Turn Decomposition (MinD), a novel method to improve the efficiency of large reasoning models (LRMs). - MinD decomposes the conventional chain-of-thought (CoT) reasoning process into a sequence of explicit, structured, and turn-wise interactions. - The model is trained using a supervised fine-tuning (SFT) approach, followed by reinforcement learning (RL) with the GRPO algorithm to prioritize correct outputs with fewer turns. - Experimental results show that MinD achieves up to ~70% reduction in both output token usage and time to first token (TTFT) on various reasoning benchmarks, while maintaining competitive performance. - The method is shown to generalize well to out-of-distribution benchmarks. | ['Question Answering'] | N/A | N/A |
| [Hard Negative Contrastive Learning for Fine-Grained Geometric
  Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152) | Ji Qi, Jiajie Zhang, Zhen Yang, Yushi Bai, Kai Sun | - The paper introduces MMCLIP, a novel hard negative contrastive learning framework to improve geometric reasoning in Large Multimodal Models (LMMs). - MMCLIP uses image-based and text-based contrastive learning with hard negatives generated by perturbing diagram generation code and modifying geometric descriptions. - The proposed model, MMGeoLM, outperforms existing open-source models on three geometric reasoning benchmarks and rivals powerful closed-source models like GPT-40. - Experiments show that using authentic, exam-based images as hard negatives significantly boosts model performance, demonstrating the importance of high-quality negative samples. - The study analyzes the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMMs. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/THU-KEG/MMGeoLM) | [Link](null) |
| [The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT
  Distillation](https://arxiv.org/abs/2505.18759) | Song Wang, Zhen Tan, Rana Muhammad Shahroz Khan, Ruichen Zhang, wjldw | - This paper introduces DC-CoT, the first data-centric benchmark for evaluating chain-of-thought (CoT) distillation in large language models (LLMs). - DC-CoT investigates data manipulation from method, model, and data perspectives, using various teacher and student models. - The benchmark rigorously evaluates the impact of data augmentation, selection, and mixing strategies on student model performance across multiple reasoning datasets. - Findings provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. - The dataset and code are publicly available. | ['Question Answering'] | N/A | N/A |
| [AdaCtrl: Towards Adaptive and Controllable Reasoning via
  Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822) | Jiazhan Feng, Zhaochen Su, Wanjun Zhong, Hongru Wang, JoeYing | - AdaCtrl is a novel framework that supports both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. - AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty while also allowing users to manually control the budget. - This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase and a difficulty-aware reinforcement learning (RL) stage. - Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, yielding performance improvements and simultaneously reducing response length. - AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JoeYing1019/AdaCtrl) | N/A |
| [G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language
  Model via Reinforcement Learning](https://arxiv.org/abs/2505.13426) | Flood Sung, Zhiqi Huang, Tianyu Liu, Hongcheng Gao, Liang Chen | - This paper introduces VLM-Gym, a reinforcement learning environment designed for training vision-language models (VLMs) in visually rich games. - Two models, G0 and G1, are trained using VLM-Gym, demonstrating emergent perception and reasoning abilities. - G1, which incorporates a perception-enhanced cold start, outperforms leading proprietary models like Claude-3.7-Sonnet-Thinking across various games. - Systematic analysis reveals that perception and reasoning abilities mutually bootstrap each other during RL training. - The source code and training details are publicly released to encourage further research in the field. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/chenllliang/G1) | N/A |
| [The Coverage Principle: A Framework for Understanding Compositional
  Generalization](https://arxiv.org/abs/2505.20278) | Miyoung Ko, Sohee Yang, Hanseul Cho, Jinho Park, Hoyeon Chang |  - This paper introduces the Coverage Principle, a novel framework for understanding compositional generalization in large language models.  - The principle posits that models relying on pattern matching for compositional tasks cannot reliably generalize beyond substituting fragments that yield identical results in the same contexts.  - The authors empirically demonstrate this principle's predictive power using Transformer models on synthetic compositional tasks, confirming the quadratic growth of required training data with token set size and the context-dependent nature of Transformer representations in path-ambiguous tasks.  - They further propose a mechanism-based taxonomy for generalization, distinguishing between structure-based, property-based, and shared-operator generalization.  - This work provides a unified lens for understanding compositional reasoning and highlights the need for fundamental architectural or training innovations to achieve truly systematic compositionality. | ['Natural Language Processing'] | [Link](https://github.com/kaistAI/coverage-principle) | N/A |
| [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136) | Andreas Hotho, Fotis Jannidis, Jan Pfister, Julia Wunderle, Anton Ehrmanntraut | - This paper introduces ModernGBERT, a family of German-only encoder models trained from scratch, with sizes of 134M and 1B parameters. - ModernGBERT incorporates architectural innovations from ModernBERT, such as enhanced relative positional embeddings and efficient attention patterns. - The authors evaluate ModernGBERT on various natural language understanding tasks and demonstrate that the 1B parameter model outperforms existing state-of-the-art German encoders. - Additionally, they present LLäMmlein2Vec, a family of encoders derived from German decoder-only models using LLM2Vec, enabling a controlled comparison. - All models, training data, checkpoints, and code are publicly available to foster advancements in German NLP. | ['Natural Language Processing'] | N/A | N/A |
| [Interleaved Reasoning for Large Language Models via Reinforcement
  Learning](https://arxiv.org/abs/2505.19640) | Yanchao Sun, Dong Lin, Deepak Gopinath, David Qiu, Roy Xie | This paper introduces a novel reinforcement learning training paradigm called interleaved reasoning that enhances Large Language Models' (LLMs) reasoning capabilities by interleaving thinking and answering processes.  The approach improves time-to-first-token (TTFT) by over 80% and boosts Pass@1 accuracy by up to 19.3% across five datasets and three RL algorithms.  A simple rule-based reward incentivizes correct intermediate steps, guiding the model towards accurate reasoning paths. The method generalizes well to complex reasoning tasks, such as MATH, GPQA, and MMLU, demonstrating strong capabilities even without training data from those specific domains. | ['Reinforcement Learning', 'Question Answering'] | N/A | N/A |
| [WINA: Weight Informed Neuron Activation for Accelerating Large Language
  Model Inference](https://arxiv.org/abs/2505.19427) | Colby Banbury, Jongwoo Ko, Dan Zhao, Sihan Chen, tianyic | - This paper introduces WINA, a novel training-free sparse activation framework for accelerating large language model (LLM) inference. - WINA jointly considers hidden state magnitudes and weight matrices to determine neuron activation, unlike existing methods that rely solely on hidden state magnitudes. - Theoretical analysis demonstrates that WINA obtains tighter approximation error bounds compared to state-of-the-art methods. - Empirical results show that WINA outperforms existing methods (e.g., TEAL) by up to 2.94% on average performance across various LLMs and datasets. - The source code for WINA is publicly available on GitHub. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/microsoft/wina) | N/A |
| [LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language
  Diffusion Models](https://arxiv.org/abs/2505.19223) | zhenxuan00, jrwen, lyk423, surfingtomchen, xiaolu0714 |  - This paper introduces LLaDA 1.5, a large language diffusion model that significantly improves upon its predecessor, LLaDA, by incorporating Variance-Reduced Preference Optimization (VRPO). - LLaDA 1.5 demonstrates superior performance across various benchmarks, including mathematical reasoning, code generation, and alignment tasks. - The core of VRPO is a framework for formally analyzing the variance of ELBO estimators and developing unbiased variance reduction strategies. - These strategies include optimal Monte Carlo budget allocation and antithetic sampling, which are shown to significantly improve the performance of MDM alignment. - The results suggest that masked diffusion models (MDMs) are compatible with RL-based alignment algorithms. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://ml-gsai.github.io/LLaDA-1.5-Demo/) | N/A |
| [Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications
  of Agentic AI](https://arxiv.org/abs/2505.19443) | Manoj Karkee, Konstantinos I. Roumeliotis, RanjanSapkota |  - This paper introduces two novel AI-assisted software development paradigms: vibe coding and agentic coding.  Vibe coding emphasizes intuitive human-in-the-loop interaction, while agentic coding allows for more autonomous development through AI agents.  - The authors propose a detailed taxonomy encompassing conceptual foundations, execution models, and real-world tools for each paradigm.  - A comparative analysis is presented across various aspects, including workflow models, debugging techniques, and safety mechanisms.  - The paper presents 20 use cases to illustrate how each approach excels in specific scenarios (e.g., vibe coding for prototyping, agentic coding for automation).  - Finally, the authors articulate a future roadmap for agentic AI, outlining the necessary infrastructure and challenges for building trustworthy, explainable, and collaborative systems. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Reinforcement Learning', 'Robotics'] | N/A | N/A |
| [StructEval: Benchmarking LLMs' Capabilities to Generate Structural
  Outputs](https://arxiv.org/abs/2505.20139) | Yuxuan Zhang, Sherman Siu, Lipeng He, Dongfu Jiang, Jialin Yang | - StructEval, a comprehensive benchmark, is introduced to evaluate LLMs' ability to generate structured outputs in diverse formats, including text-only and renderable formats. - StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: generation and conversion tasks. - The benchmark encompasses 18 formats and 44 types of tasks, with novel metrics for format adherence and structural correctness, revealing significant performance gaps between state-of-the-art models. - Generation tasks are found to be more challenging than conversion tasks, and producing correct visual content is more difficult than generating text-only structures. - The results highlight the performance gap between commercial and open-source LLMs, underscoring the need for further research to improve structured output generation. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://tiger-ai-lab.github.io/StructEval/) | N/A |
| [InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer
  Interaction](https://arxiv.org/abs/2505.10887) | Xi Xie, Winson Chen, Zijian Zhang, Weitai Kang, Bin12345 | - INFANTAGENT-NEXT is a multimodal generalist agent that integrates tool-based and pure vision agents within a modular architecture, enabling different models to collaboratively solve decoupled tasks.- Unlike existing approaches, INFANTAGENT-NEXT achieves high accuracy and broad generality by modularizing agent workflows, tool selection, and tool execution.- The agent's performance is demonstrated on real-world benchmarks (OSWorld, GAIA, and SWE-Bench), showing superior accuracy compared to existing methods.- INFANTAGENT-NEXT uses a unified dialogue context to seamlessly merge outputs from different specialist models (reasoning, visual grounding, audio analysis).- The code and evaluation scripts for INFANTAGENT-NEXT are open-sourced. | ['Multimodal'] | [Link](https://github.com/bin123apple/InfantAgent) | N/A |
| [Error Typing for Smarter Rewards: Improving Process Reward Models with
  Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706) | Soujanya Poria, Chuan Li, Amir Zadeh, Panshul Sharma, Tej Deep Pala | - This paper introduces PathFinder-PRM, a novel hierarchical, error-aware discriminative Process Reward Model (PRM) that improves mathematical reasoning in large language models. - The model first classifies math and consistency errors at each step before combining these signals to estimate step correctness, using a two-forward-pass approach. - PathFinder-PRM achieves state-of-the-art performance on PRMBench, outperforming existing methods by a margin of 2.2 points (67.7 vs. 65.5), and demonstrates improved performance on ProcessBench as well. - The model uses a 400K sample dataset, created by combining human-annotated and RLHFlow Mistral traces, that leverages three-dimensional step-level labels. - Experiments show that decoupled error detection and reward estimation leads to more accurate end-to-end reward-guided mathematical reasoning. | ['Natural Language Processing'] | [Link](https://github.com/declare-lab/PathFinder-PRM) | N/A |
| [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning
  System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630) | Yixue Li, Lu Zhou, Yichun Feng, Jarvis1111 | - DoctorAgent-RL, a novel multi-agent collaborative reinforcement learning framework, is proposed for enhanced multi-turn clinical dialogues. - The model uses a reinforcement learning mechanism to dynamically adjust its information-gathering strategy in response to patient interactions. - DoctorAgent-RL outperforms existing models in both multi-turn reasoning and final diagnostic accuracy, as demonstrated through experimental results on the MTMedDialog dataset. - The MTMedDialog dataset is introduced, the first English multi-turn medical consultation dataset to simulate realistic patient interactions. - Future work focuses on extending the model's capabilities to integrate with multimodal data and ethical considerations. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JarvisUSTC/DoctorAgent-RL) | N/A |
| [Jodi: Unification of Visual Generation and Understanding via Joint
  Modeling](https://arxiv.org/abs/2505.19084) | Xilin Chen, Shiguang Shan, Meina Kan, Zhenliang He, xyfJASON | - Jodi is a novel diffusion model that unifies visual generation and understanding by jointly modeling image and multiple label domains. - The model architecture uses a linear diffusion transformer with a role switch mechanism, enabling it to perform joint generation, controllable generation, and image perception. - Jodi achieves state-of-the-art performance on various image generation and understanding tasks, outperforming existing methods on multiple benchmarks including depth estimation, normal estimation, and semantic segmentation. - The model's efficiency is demonstrated by its linear time and space complexity with respect to the number of domains, allowing it to handle up to 8 visual domains simultaneously. - A new dataset, Joint-1.6M, containing 200,000 high-quality images with automatic labels across 7 visual domains, is introduced and made publicly available. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Text', 'Image Segmentation', 'Depth Estimation'] | [Link](https://github.com/VIPL-GENUN/Jodi) | N/A |
| [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056) | George Turkiyyah, Bernard Ghanem, Hasan Abed Al Kader Hammoud, Harethah Abu Shairah | - This paper introduces extended-refusal fine-tuning, a novel defense against LLM abliteration attacks, which involves modifying how models express refusals to make them more robust against attacks that neutralize safety mechanisms. - The method involves creating an extended-refusal dataset with comprehensive responses containing a neutral topic overview, explicit refusal, and ethical rationale, and fine-tuning LLMs on this dataset. - Experiments demonstrate that extended-refusal models maintain high refusal rates (dropping at most by 10%) after abliteration, whereas baseline models' refusal rates drop by 70-80%. - The approach neutralizes the abliteration attack while preserving general performance, showcasing its effectiveness in enhancing the robustness of LLMs. - This work offers valuable insights into how safety alignment is represented within neural networks and how it can be effectively integrated with general capabilities. | ['Natural Language Processing'] | N/A | N/A |
| [Strong Membership Inference Attacks on Massive Datasets and (Moderately)
  Large Language Models](https://arxiv.org/abs/2505.18773) | Matthew Jagielski, Christopher A. Choquette-Choo, Ilia Shumailov, Jamie Hayes, pasta41 |  - This paper explores the scalability of strong membership inference attacks (MIAs) on large language models (LLMs).  - The authors scale LiRA, a strong MIA, to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset.  - Their findings show that strong MIAs can succeed on pre-trained LLMs but their effectiveness remains limited (AUC < 0.7) in practical settings.  - The study also reveals a non-straightforward relationship between MIA success and related privacy metrics.  - This research contributes to a better understanding of the potency and reliability of membership inference attacks on LLMs. | ['Natural Language Processing'] | N/A | N/A |
| [EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via
  Action Pruning](https://arxiv.org/abs/2505.16312) | Defu Lian, Quan Liu, Jianshu Zhang, Qisi Chen, Jiawei1222 | - EquivPruner, a novel technique to enhance the efficiency and quality of LLM-based search, is introduced in this paper.  It identifies and prunes semantically equivalent actions during the search process, thereby reducing redundant computations. - The method is evaluated on two benchmark datasets, MATH and GSM8K, showing improvements across various models. For instance, on Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by 48.1% while improving accuracy. - A new dataset, MathEquiv, for mathematical statement equivalence is presented, enabling the training of a lightweight equivalence detector. This detector is used to identify and remove redundant actions. - Experiments show that EquivPruner maintains or improves accuracy while significantly decreasing token consumption, illustrating its effectiveness in boosting efficiency and accuracy in LLM-based complex reasoning tasks.  - The approach is shown to be generalizable across various LLMs and tasks, indicating robustness and broad applicability. | ['Natural Language Processing'] | [Link](https://github.com/Lolo1222/EquivPruner) | [Link](https://huggingface.co/datasets/Jiawei1222/MathEquiv) |
| [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800) | Bernard Ghanem, Maged S. Al-Shaibani, Zaid | - This paper introduces MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers. - MOLE processes entire documents across multiple input formats (LaTeX, PDF) and incorporates robust validation mechanisms. - The framework extracts around 30 different metadata attributes per paper, significantly more than existing approaches. - A new benchmark is introduced to evaluate research progress on this task, encompassing datasets in multiple languages. - Systematic analysis demonstrates promising results with modern LLMs, while highlighting areas for future improvements. | ['Natural Language Processing'] | [Link](https://github.com/IVUL-KAUST/MOLE) | [Link](https://huggingface.co/datasets/IVUL-KAUST/MOLE) |
| [Architectural Backdoors for Within-Batch Data Stealing and Model
  Inference Manipulation](https://arxiv.org/abs/2505.18323) | Ilia Shumailov, Conrad Grobler, Ivan Petrov, Nicolas Küchler | - This paper introduces a novel class of architectural backdoors that exploit batched inference in machine learning models to steal data and manipulate model inferences for concurrent users. - The backdoors are designed to be injected into the model architecture and activated by a specific trigger in a user's input. - Once activated, they facilitate information leakage between concurrent users within the same batch, allowing attackers to steal data or manipulate the model's outputs for other users. - The authors propose a deterministic mitigation strategy, the Batch Isolation Checker, which uses static analysis of the model graph to formally guarantee non-interference between user inputs. - The Checker is evaluated on over 200 Hugging Face models to identify vulnerabilities and demonstrate its effectiveness. | ['Natural Language Processing'] | N/A | N/A |
| [Towards Holistic Evaluation of Large Audio-Language Models: A
  Comprehensive Survey](https://arxiv.org/abs/2505.15957) | Hung-yi Lee, Neo S. Ho, zenyn | This paper introduces a comprehensive taxonomy for evaluating Large Audio-Language Models (LALMs), categorizing existing benchmarks into four dimensions: General Auditory Awareness and Processing, Knowledge and Reasoning, Dialogue-oriented Ability, and Fairness, Safety, and Trustworthiness.  It offers a structured overview of the LALM evaluation landscape, highlighting challenges and future directions. The authors propose a novel structured taxonomy for LALM evaluations, providing clear guidelines for researchers and bridging the gap in fragmented existing benchmarks. This survey is the first to specifically focus on LALM evaluations, which is an important contribution to the field. | ['Audio'] | [Link](https://github.com/ckyang1124/LALM-Evaluation-Survey) | N/A |
| [TAGS: A Test-Time Generalist-Specialist Framework with
  Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283) | Haochen Xue, Ming Hu, Yulong Li, Feilong Tang, JianghaoWu | - This paper introduces TAGS, a test-time framework that combines a generalist and a specialist model for medical question answering. - TAGS utilizes a hierarchical retrieval mechanism to provide multi-scale exemplars for reasoning and a reliability scorer to evaluate reasoning consistency. - The framework achieves state-of-the-art performance on nine MedQA benchmarks, surpassing several fine-tuned medical LLMs without any parameter updates. -  The improvements are significant, boosting GPT-4 accuracy by 13.8% and DeepSeek-R1 by 16.8%. - The code for the framework will be available at https://github.com/JianghaoWu/TAGS. | ['Question Answering'] | [Link](https://github.com/JianghaoWu/TAGS) | N/A |


## Papers for 2025-05-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Reasoning Model is Stubborn: Diagnosing Instruction Overriding in
  Reasoning Models](https://arxiv.org/abs/2505.17225) | Eunho Yang, Hyun Ryu, Chanjae Park, yjyjyj98, jadohu | - This paper introduces ReasoningTrap, a new diagnostic dataset designed to identify and analyze reasoning rigidity in large language models (LLMs). - Reasoning rigidity is defined as the tendency of LLMs to override explicit instructions and default to familiar reasoning patterns, even when those patterns lead to incorrect conclusions. - The dataset contains modified versions of existing mathematical benchmarks and logic puzzles, designed to require deviations from typical reasoning strategies. - By analyzing model performance on ReasoningTrap, the authors identify three distinct modes of contamination caused by reasoning rigidity: Interpretation Overload, Input Distrust, and Partial Instruction Attention. - ReasoningTrap is publicly released to facilitate future research on mitigating reasoning rigidity in LLMs. | ['Natural Language Processing'] | N/A | N/A |
| [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129) | Pengfei Li, Shaoxiang Chen, Linge Du, Yan Ma, Ryan1122 | - This paper introduces V-Triune, a Visual Triple Unified Reinforcement Learning system that enables Vision-Language Models to jointly learn visual reasoning and perception tasks within a single training pipeline.  - V-Triune comprises three complementary components: Sample-Level Data Formatting, Verifier-Level Reward Computation, and Source-Level Metric Monitoring.  - A novel Dynamic IoU reward is introduced, providing adaptive and progressive feedback for perception tasks.  - The resulting model, Orsta, demonstrates consistent improvements across both reasoning and perception tasks, achieving substantial gains on MEGA-Bench Core (+2.1% to +14.1%).  - V-Triune and Orsta models are publicly available on Github. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering', 'Object Detection', 'Image Segmentation', 'Image-to-Text'] | [Link](https://github.com/MiniMax-AI) | N/A |
| [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669) | Jiale Chen, Oliver Sieberling, Soroush Tabesh, Andrei Panferov, Roberto L. Castro | - The paper introduces Quartet, a novel algorithm enabling accurate end-to-end FP4 training for large language models (LLMs). - Quartet achieves state-of-the-art accuracy for FP4 precision by maximizing both parameter and data efficiency. - The algorithm is implemented using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, demonstrating speedups of almost 2x relative to FP8. - Experiments on Llama-type models reveal a new low-precision scaling law that identifies Quartet as a near-optimal low-precision training technique. - The research demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. | ['Natural Language Processing'] | [Link](https://github.com/IST-DASLab/Quartet) | N/A |
| [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/abs/2505.17612) | Sung Ju Hwang, Jaewoong Cho, Seanie Lee, Jongwon Jeong, Minki Kang | This paper introduces Agent Distillation, a novel framework for transferring the problem-solving capabilities of large language model (LLM)-based agents to smaller language models (SLMs).  The method improves agent distillation by introducing a prompting method called first-thought prefix to enhance teacher-generated trajectories and a self-consistent action generation method to improve small agents' test-time robustness.  Evaluated on eight reasoning tasks, the results demonstrate that SLMs as small as 0.5B parameters achieve performance competitive with next-tier larger models fine-tuned using chain-of-thought distillation. The proposed agent distillation method consistently improves performance across all model sizes, particularly on out-of-domain tasks. | ['Question Answering'] | [Link](https://github.com/Nardien/agent-distillation) | N/A |
| [PhyX: Does Your Model Have the "Wits" for Physical Reasoning?](https://arxiv.org/abs/2505.15929) | Yunta Hsieh, Qi Han, Hui Shen, John-ai-bee, taki555 |  - This paper introduces PHYX, a large-scale benchmark for evaluating multimodal models' capacity for physics-grounded reasoning.  - PHYX includes 3000 meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains.  - State-of-the-art models achieve only around 45% accuracy on PHYX, significantly underperforming human experts, revealing critical limitations in current models.  - The evaluation protocol uses widely used toolkits such as VLMEvalKit, enabling one-click evaluation and ensures reproducibility.  - The authors provide fine-grained statistics, detailed case studies, and multiple evaluation paradigms for comprehensive analysis of physical reasoning capabilities. | ['Multimodal', 'Visual Question Answering'] | [Link](https://phyx-bench.github.io/) | N/A |
| [QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092) | Shaopeng Lai, Shengyi Liao, Chenliang Li, Weizhou Shen, Wanfq | - QWENLONG-CPRS is a novel context compression framework designed for optimizing long-context processing in large language models (LLMs). - It introduces a dynamic context optimization mechanism that compresses input contexts into query-specific content at varying granularities. - The framework comprises four key innovations: natural language-guided dynamic optimization, bidirectional reasoning layers, token critic mechanisms, and window-parallel inference. - Evaluations across five benchmarks demonstrate QWENLONG-CPRS's threefold effectiveness: consistent superiority over existing methods, architecture-agnostic integration with various LLMs, and establishment of new state-of-the-art performance. - QWENLONG-CPRS achieves significant context compression (up to 290.5 times) and performance gains (up to 19.15 points) while maintaining high efficiency. | ['Natural Language Processing'] | [Link](https://github.com/Tongyi-Zhiwen/QwenLong-CPRS) | [Link](https://huggingface.co/Tongyi-Zhiwen/QwenLong-CPRS-7B) |
| [VeriThinker: Learning to Verify Makes Reasoning Model Efficient](https://arxiv.org/abs/2505.17941) | Xinchao Wang, Ruonan Yu, Gongfan Fang, Xinyin Ma, Zigeng |  - VeriThinker is a novel approach for Chain-of-Thought (CoT) compression in Large Reasoning Models (LRMs) that avoids the need for synthetic data.   - It uses Supervised Verification Fine-Tuning (SVFT), where the model is trained on an auxiliary verification task to distinguish correct from incorrect CoT solutions.   - This approach significantly reduces the length of reasoning chains while maintaining or improving accuracy across multiple mathematical reasoning benchmarks.   - VeriThinker also generalizes to solution-wise speculative reasoning, further enhancing efficiency.   - Experimental results demonstrate that VeriThinker achieves effective CoT compression without relying on synthetic concise CoT data, outperforming existing SFT or RL-based methods. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/czg1225/VeriThinker) | N/A |
| [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated
  Experimental Feedback](https://arxiv.org/abs/2505.17873) | Lidong Bing, Jue Wang, di-zhang-fdu, ZonglinY, wanhaoliu | - This paper introduces the task of experiment-guided hypothesis ranking, aiming to prioritize hypotheses based on experimental feedback. - A novel simulator, CSX-Sim, is developed to address the challenge of limited access to real experimental data in chemistry, grounded on three foundational assumptions. - CSX-Rank, a pseudo experiment-guided ranking method, is proposed, which clusters hypotheses based on functional characteristics and utilizes simulated experimental feedback. - Experimental results demonstrate that CSX-Rank outperforms pre-experiment ranking baselines and strong ablation variants, with a reduction in trials required to identify optimal hypotheses by more than 50%. - The study contributes to automated scientific discovery, particularly in chemistry, by providing a more efficient hypothesis selection process. | ['Natural Language Processing'] | [Link](https://github.com/wanhaoliu/ChemsimX.git) | [Link](None) |
| [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large
  Language Models](https://arxiv.org/abs/2505.16211) | Jirui Han, Yile Liu, Can Shen, Kai Li, jiaxiaojunQAQ | The paper introduces AudioTrust, a comprehensive benchmark for evaluating the trustworthiness of Audio Large Language Models (ALLMs).  It assesses ALLMs across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication.  AudioTrust uses a meticulously constructed dataset of over 4,420 audio/text samples from real-world scenarios.  The benchmark employs 9 audio-specific evaluation metrics and uses a large-scale automated pipeline for objective and scalable scoring.  Results show trustworthiness boundaries and limitations of current ALLMs. | ['Audio'] | [Link](https://github.com/JusperLee/AudioTrust) | [Link](https://huggingface.co/datasets/JusperLee/AudioTrust) |
| [Scaling Image and Video Generation via Test-Time Evolutionary Search](https://arxiv.org/abs/2505.17618) | Di Zhang, Pengfei Wan, Xintao Wang, Jiajun Liang, haoranhe |  - This paper introduces EvoSearch, a novel test-time scaling framework that enhances the scalability of image and video generation models by allocating more compute at inference time. - EvoSearch reformulates test-time scaling as an evolutionary search problem, leveraging principles from biological evolution to iteratively generate higher-quality samples. - The method is applicable to both diffusion and flow models and does not require additional training or model expansion. - Extensive experiments demonstrate that EvoSearch significantly improves sample quality and diversity across various models and tasks, outperforming existing methods. - Notably, EvoSearch enables a 1.3B parameter model to surpass a 14B parameter model with 10x fewer parameters in video generation. | ['Text-to-Image', 'Text-to-Video', 'Multimodal'] | [Link](https://github.com/tinnerhrhe/evosearch) | N/A |
| [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering
  Workflow](https://arxiv.org/abs/2505.17399) | Yu Cheng, Linjie Li, Huichen Will Wang, Haoyu Sun, Kuvvi | - FullFront, a novel benchmark, evaluates Multimodal Large Language Models (MLLMs) across the entire front-end engineering workflow, encompassing conceptualization, comprehension, and implementation. - Unlike existing benchmarks that focus on isolated tasks, FullFront assesses three crucial stages: Webpage Design, Webpage Perception QA, and Webpage Code Generation. - The benchmark employs a two-stage process to transform real-world webpages into clean, standardized HTML, resolving copyright concerns and data inconsistencies. - Evaluation reveals significant MLLM limitations in page perception, code generation (especially handling images and layouts), and interaction implementation, highlighting performance discrepancies between open-source and proprietary models. - FullFront offers a comprehensive, multi-faceted evaluation pipeline and code, advancing research in MLLM capabilities for front-end engineering. | ['Multimodal'] | [Link](https://github.com/Mikivishy/FullFront) | N/A |
| [Teaching with Lies: Curriculum DPO on Synthetic Negatives for
  Hallucination Detection](https://arxiv.org/abs/2505.17558) | Ying Ding, Liu Leqi, ashwinnv, SP2001 | - This paper introduces HaluCheck, a novel hallucination detection model that leverages a curriculum learning strategy with Direct Preference Optimization (DPO). - HaluCheck uses high-quality hallucinated samples as negative examples in the DPO alignment process, improving performance over using standard negative samples. - The model demonstrates significant improvements of up to 24% on benchmarks like MedHallu and HaluEval, outperforming larger state-of-the-art models. - HaluCheck also shows robustness in zero-shot settings across multiple benchmarks and domains. - The authors introduce a curriculum learning strategy that gradually transitions the training from easier to harder samples, ensuring stable and incremental learning. | ['Natural Language Processing', 'Text Classification', 'Question Answering'] | [Link](https://teachingwithlies.github.io/) | N/A |
| [Teaching Large Language Models to Maintain Contextual Faithfulness via
  Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483) | Zhitong Wang, Yuzhuo Bai, Cheng Gao, Shuzheng Si, BleachNick | This paper introduces CANOE, a novel framework designed to enhance the contextual faithfulness of large language models (LLMs).  CANOE leverages synthetic short-form question-answering (QA) data and a rule-based reinforcement learning method called Dual-GRPO to achieve this goal.  The Dual-GRPO method employs three tailored rewards and simultaneously optimizes both short-form and long-form response generation, eliminating the need for manual data labeling. Experimental results on 11 downstream tasks indicate that CANOE significantly improves LLM faithfulness and outperforms existing state-of-the-art models in multiple cases. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Question Answering'] | [Link](https://github.com/S1s-Z/CANOE) | N/A |
| [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508) | Jiaxuan You, Haoru Li, Haofei Yu, Peixuan Han, m-serious |  - Time-R1 is a novel framework that enhances a 3B-parameter LLM with comprehensive temporal reasoning abilities, encompassing understanding, prediction, and creative generation.  - It employs a three-stage reinforcement learning curriculum with a dynamic reward system, progressively building foundational understanding, future prediction skills, and creative scenario generation capabilities.  - Time-R1 surpasses models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on challenging benchmarks involving future event prediction and creative scenario generation.  - The study provides strong evidence that smaller, efficient models can achieve superior temporal performance via thoughtfully engineered progressive reinforcement learning.  - Time-Bench, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, is released to facilitate further research. | ['Reinforcement Learning', 'Natural Language Processing', 'Time Series Forecasting'] | [Link](https://github.com/ulab-uiuc/Time-R1) | [Link](https://huggingface.co/collections/ulab-ai/time-r1-682626aea47cb2b876285a16) |
| [Speechless: Speech Instruction Training Without Speech for Low Resource
  Languages](https://arxiv.org/abs/2505.17417) | Shreyas Gopal, Tuan Le Duc Anh, Huy Hoang Ha, Dinh Bach Vu, alandao | - The paper introduces Speechless, a novel method for generating synthetic training data for early-fusion speech language models without using traditional text-to-speech (TTS) systems. - Speechless leverages a quantized Whisper encoder to generate semantic speech tokens, bypassing the need for waveform generation, making it particularly useful for low-resource languages. - The proposed method achieves competitive automatic speech recognition (ASR) performance on Vietnamese, a low-resource language, without speech-based fine-tuning, showcasing its effectiveness. - Speechless demonstrates comparable performance to Llama-Omni on VoiceBench, a benchmark for evaluating speech instruction tuning, but underperforms some more recent models. - A new pre-tokenized Vietnamese instruction dataset is released to facilitate further research and development in speech-language models for low-resource languages. | ['Automatic Speech Recognition', 'Text-to-Audio', 'Multimodal'] | [Link](https://github.com/menloresearch/ichigo/tree/legacy/main) | [Link](https://huggingface.co/Menlo/Speechless-llama3.2-v0.1), [Link](https://huggingface.co/datasets/Menlo/Ichigo-instruction-tokenized-v0.2), [Link](https://huggingface.co/Menlo/Ichigo-whisper-v0.1) |
| [RBench-V: A Primary Assessment for Visual Reasoning Models with
  Multi-modal Outputs](https://arxiv.org/abs/2505.16770) | Qianrui Yang, uyzhang, Mo-ZheHan, CXY07, MenghaoGuo | - This paper introduces RBench-V, a new benchmark designed to evaluate the visual reasoning capabilities of multimodal models, focusing specifically on their ability to generate multi-modal outputs (not just process them). - RBench-V contains 803 questions covering math, physics, counting, and games, requiring image manipulation and generation as part of the solution process. - Evaluation of numerous open- and closed-source models on RBench-V reveals a significant performance gap between models and human experts (25.8% accuracy vs. 82.3% human accuracy), highlighting the challenges of multimodal reasoning. - The benchmark highlights the fact that current models struggle to effectively leverage multi-modal reasoning, even the best performing models achieve low accuracy. - The authors suggest the need for new paradigms, such as incorporating multi-modal chain of thought (M-CoT) or agent-based reasoning frameworks, to improve the performance of multimodal reasoning models. | ['Multimodal', 'Visual Question Answering'] | [Link](https://evalmodels.github.io/rbenchv) | N/A |
| [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146) | Zifeng Wang, Jinfeng Xiao, Jiacheng Lin, Xueqiang Xu, Pengcheng Jiang | - This paper introduces s3, a lightweight, model-agnostic framework that trains a search agent using reinforcement learning to improve the accuracy of large language models (LLMs) in question answering. - s3 decouples the searcher from the generator, training only the searcher using a novel reward signal called Gain Beyond RAG (GBR), which measures the improvement in generation accuracy over naive RAG. - The results show that s3 outperforms other baselines on six general QA and five medical QA benchmarks, achieving stronger downstream performance with significantly less training data (2.4k samples compared to 70x more for other methods). - s3 is a modular framework, making it compatible with various frozen or proprietary LLMs and facilitating targeted optimization of retrieval quality. - The approach addresses the limitations of existing methods that either use search-only metrics or jointly optimize retrieval and generation, often limiting real search utility and entanglement with the LLM. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/pat-jj/s3) | N/A |
| [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270) | Ruizhong Qiu, Yunzhe Qi, Zihao Li, Yikun Ban, jiaruz2 |  - This paper introduces Transformer Copilot, a novel framework that enhances the inference performance of pre-trained LLMs by learning from their mistakes during fine-tuning. - The framework comprises three key components: a novel Copilot model design, a joint training paradigm, and a fused inference paradigm. - The Copilot model rectifies the Pilot model's logits using a Mistake Log that systematically tracks the model's learning behavior and recurring errors. - Experiments on 12 benchmarks demonstrate that Transformer Copilot improves performance by up to 34.5%, while introducing marginal computational overhead. - The method exhibits strong scalability and transferability, outperforming several strong baselines with fewer parameters. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/jiaruzouu/TransformerCopilot) | [Link](https://huggingface.co/docs/transformers/main/en/index) |
| [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark
  Study](https://arxiv.org/abs/2505.15389) | Hwanjo Yu, Jihae Jeong, Joonwon Jang, oneonlee |  - This paper introduces MEMESAFETYBENCH, a new benchmark dataset designed to evaluate the safety of vision-language models (VLMs) using real-world meme images. - The dataset contains 50,430 instances pairing meme images with harmful and benign instructions, enabling a more comprehensive evaluation of VLM safety in real-world contexts. - Results show that VLMs are more vulnerable to meme-based harmful prompts than to synthetic or typographic images, highlighting the need for stronger safety mechanisms. - The study also analyzes the mitigating effects of conversational context and the relationship between model scale and safety metrics. - Overall, the findings demonstrate that existing benchmarks that utilize artificial visual inputs fail to adequately capture real-world vulnerabilities. | ['Multimodal'] | N/A | N/A |
| [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091) | Mert Pilanci, Prateek Verma | - This paper introduces a novel approach that leverages pre-trained text LLMs to perform image and audio classification without using modality-specific encoders. - The method involves replacing the traditional Vision Transformer (ViT) or Audio Transformer with a text LLM, thereby enabling the model to implicitly learn to "see" and "hear" through textual training. - Experimental results demonstrate the effectiveness of this approach across multiple datasets (CIFAR-10, Fashion-MNIST, FSD-50K, GTZAN), showcasing competitive performance with architectures trained from scratch and even outperforming those with frozen weights. - The paper highlights the potential for transfer learning and efficient fine-tuning, particularly using Low-Rank Adaptation (LoRA), to adapt pretrained LLMs for various tasks and modalities. - The study contributes to understanding the emergent capabilities of large language models and their potential to solve tasks beyond their initial training scope. | ['Audio Classification', 'Image Classification', 'Multimodal'] | N/A | N/A |
| [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293) | Sai Rajeswar, Shiva Krishna Reddy Malay, Khyati Mahajan, Masoud Hashemi, rmahesh | This paper introduces NotesWriting, a method for augmenting Large Language Model (LLM) reasoning in complex question answering by generating concise and relevant notes from retrieved documents at each step.  NotesWriting addresses context overload and information noise in iterative Retrieval Augmented Generation (RAG).  Experiments across three iterative RAG methods, two LLMs, and four evaluation datasets show an average improvement of 15.6 percentage points, with minimal increase in output tokens.  The method is framework-agnostic and can be easily integrated with different iterative RAG methods, improving both efficiency and scalability.  | ['Question Answering'] | N/A | N/A |
| [NOVER: Incentive Training for Language Models via Verifier-Free
  Reinforcement Learning](https://arxiv.org/abs/2505.16022) | Yali Du, Chen Qian, Xinyu Wang, Siya Qi, Wei Liu | - This paper introduces NOVER, a novel verifier-free reinforcement learning framework for incentive training in language models. - NOVER computes rewards solely based on the model's reasoning process, enabling incentive training across various text-to-text tasks without external verifiers. - Experimental results demonstrate that NOVER outperforms models of the same size distilled from large reasoning models by 7.7% on several benchmark datasets. - The framework's flexibility allows for optimizing large language models through techniques like inverse incentive training. - NOVER addresses the limitations of existing incentive training methods that rely on external verifiers, which are often expensive and limited in applicability. | ['Reinforcement Learning', 'Text2Text Generation'] | [Link](https://github.com/thinkwee/NOVER) | N/A |
| [Keep Security! Benchmarking Security Policy Preservation in Large
  Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805) | Hwanhee Lee, Yonghyun Jun, Yumin Kim, HwanChang0106 | - This paper introduces CoPriva, a new large-scale benchmark dataset for evaluating the ability of Large Language Models (LLMs) to adhere to user-defined security policies in question answering, especially against indirect attacks. - CoPriva includes realistic contexts, explicit policies, and queries designed as direct and challenging indirect attacks seeking prohibited information. - The evaluation of 10 LLMs on CoPriva reveals a significant vulnerability: many models violate user-defined policies and leak sensitive information, particularly against indirect attacks. - The analysis reveals that while models can often identify the correct answer, they struggle to incorporate policy constraints during generation, highlighting a critical gap in current LLM safety alignment. - The findings underscore the urgent need for more robust methods to guarantee contextual security in sensitive applications. | ['Question Answering'] | N/A | N/A |
| [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in
  Real-World Scenarios](https://arxiv.org/abs/2505.12891) | Tianyi Zhuang, Wen Luo, Wei Li, Shaohang Wei, songff |  - This paper introduces TIME, a multi-level benchmark for evaluating the temporal reasoning capabilities of Large Language Models (LLMs) in real-world scenarios. - TIME consists of 38,522 question-answer pairs covering three levels with eleven fine-grained sub-tasks, encompassing three sub-datasets (TIME-WIKI, TIME-NEWS, and TIME-DIAL). - The benchmark incorporates challenges such as intensive temporal information, fast-changing event dynamics, and complex temporal dependencies in social interactions. - Extensive experiments are conducted on various reasoning and non-reasoning models, providing insights into the impact of test-time scaling on temporal reasoning performance. - A human-annotated subset, TIME-LITE, is also released to facilitate future research and standardized evaluation in temporal reasoning. | ['Question Answering'] | [Link](https://github.com/sylvain-wei/TIME) | [Link](https://huggingface.co/datasets/SylvainWei/TIME) |
| [Not All Models Suit Expert Offloading: On Local Routing Consistency of
  Mixture-of-Expert Models](https://arxiv.org/abs/2505.16056) | Duyu Tang, Yitong Li, Miren Tian, Siyuan Wang, ljcleo | This paper introduces two metrics, SRP and SCH, to measure the local routing consistency of Mixture-of-Expert (MoE) models.  The study analyzes 20 MoE LLMs, revealing that models applying MoE on every layer without shared experts exhibit the highest consistency. Domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones.  Finally, the research demonstrates that most models can effectively balance cache size and efficiency with cache sizes approximately twice the number of active experts.  The code for replicating experiments is publicly available. | ['Natural Language Processing'] | [Link](https://github.com/ljcleo/moe-lrc) | N/A |


## Papers for 2025-05-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Scaling Reasoning, Losing Control: Evaluating Instruction Following in
  Large Reasoning Models](https://arxiv.org/abs/2505.14810) | Yu Cheng, Xiaoye Qu, Jiawei Gu, yaful, TingchenFu | - This paper introduces MathIF, a new benchmark for evaluating instruction following in large language models (LLMs) specifically designed for mathematical reasoning tasks. - The benchmark includes 420 high-quality evaluation samples with varying difficulty and constraints, allowing for a comprehensive evaluation of instruction following capabilities. - Empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability in LLMs, with models that reason more effectively often struggling to adhere to user instructions. - The authors find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning degrade in instruction adherence, especially when generation length increases. - This work highlights a fundamental tension in current LLM training paradigms and motivates the need for more instruction-aware reasoning models. | ['Natural Language Processing'] | [Link](https://github.com/TingchenFu/MathIF) | N/A |
| [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement
  Learning](https://arxiv.org/abs/2505.16410) | Hongjin Qian, Jiajie Jin, Xiaoxi Li, Yifei Chen, Guanting Dong | This paper introduces Tool-Star, a reinforcement learning framework that enables LLMs to autonomously utilize multiple external tools during reasoning.  Tool-Star incorporates six types of tools and systematic designs in data synthesis and training to address the scarcity of tool-use data.  A two-stage training framework enhances multi-tool collaborative reasoning through cold-start fine-tuning and a multi-tool self-critic RL algorithm. Experimental results on over 10 challenging reasoning benchmarks demonstrate Tool-Star's effectiveness and efficiency.  Tool-Star outperforms other TIR baselines across various reasoning tasks, showcasing strong overall reasoning performance and reliability in tool usage. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/dongguanting/Tool-Star) | N/A |
| [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with
  Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966) | Fangzhen Lin, Weimin Ren, Haozhe Wang, Alex Su, wenhu | - This paper introduces Pixel Reasoner, a novel framework that introduces the concept of reasoning in pixel space to enhance the reasoning capabilities of Vision-Language Models (VLMs). - Pixel Reasoner is a 7B parameter model that improves VLM performance across diverse visual reasoning benchmarks, achieving state-of-the-art results on several datasets. - The model is trained using a two-phase approach: instruction tuning on synthesized reasoning traces, and reinforcement learning with a curiosity-driven reward scheme. - The proposed method addresses the challenges of imbalanced competence and reluctance to adopt pixel-space operations in VLMs through a two-stage training process. - This framework highlights the importance of pixel-space reasoning and the effectiveness of the proposed training approach for improving VLM performance on visually intensive tasks. | ['Multimodal', 'Visual Question Answering'] | [Link](https://tiger-ai-lab.github.io/Pixel-Reasoner/) | N/A |
| [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933) | Jun Zhou, Jun Hu, Xiaolu Zhang, Shen Nie, Zebin You | - LLaDA-V is a purely diffusion-based multimodal large language model (MLLM) that integrates visual instruction tuning with masked diffusion models. - The model architecture incorporates a vision encoder and an MLP connector to project visual features into the language embedding space, enabling effective multimodal alignment. - LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts. - When trained on the same instruction data, LLaDA-V is highly competitive with LLaMA3-V across multimodal tasks and achieves state-of-the-art performance in multimodal understanding. - The findings suggest that large language diffusion models show promise in multimodal contexts. | ['Multimodal'] | [Link](https://github.com/ml-gsai/LLaDA-V-demo/) | N/A |
| [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604) | Wenqi Zhang, Haolei Xu, Yongliang Shen, Yuchen Yan, Haoran Zhao | - This paper introduces Self-Braking Tuning (SBT), a novel framework that enables large language models (LLMs) to autonomously regulate their reasoning process and reduce overthinking. - SBT tackles overthinking by allowing the model to self-regulate, eliminating the need for external control mechanisms. - The proposed method uses a systematic approach to identify redundant reasoning steps, generating training signals to enhance the model's self-regulation capabilities. - Experiments on mathematical reasoning benchmarks show that SBT reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models. - The framework includes two data construction strategies: SBT-E (Exact) and SBT-D (Dynamic), each with unique strengths to help the model learn when to stop reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ZJU-REAL/Self-Braking-Tuning) | N/A |
| [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684) | Guiyang Hou, Wenqi Zhang, Yongliang Shen, Yuchen Yan, Haolei Xu | - This paper introduces a novel method called CoT-Bridge to improve the quality of chain-of-thought (CoT) reasoning in large language models (LLMs) by addressing the issue of "Thought Leaps", which are missing intermediate steps in CoT datasets. - CoT-Bridge automatically identifies and generates the missing steps, resulting in improved completeness and coherence in the reasoning chains. This is achieved by training a model on a specialized dataset called ScaleQM+, which is constructed from the ScaleQuestMath dataset by systematically removing intermediate steps. - Experiments demonstrate that fine-tuning LLMs on datasets with bridged CoTs consistently outperforms those trained on the original datasets with Thought Leaps. Improvements of up to +5.87% on NuminaMath and +3.36% on MetaMathQA were observed. - CoT-Bridge can be integrated with other techniques such as knowledge distillation and reinforcement learning, further enhancing model performance. - The effectiveness of the approach is validated across various benchmarks, including both mathematical and logical reasoning tasks, highlighting the broad applicability and generalizability of the method. | ['Natural Language Processing'] | [Link](https://zju-real.github.io/CoT-Bridge) | N/A |
| [Backdoor Cleaning without External Guidance in MLLM Fine-tuning](https://arxiv.org/abs/2505.16916) | Xun Xiao, Jinhe Bi, Jian Liang, Wenke Huang, Xuankun Rong | - The paper introduces Believe Your Eyes (BYE), a novel unsupervised data filtering framework that leverages attention entropy patterns to identify and filter backdoor samples in Multimodal Large Language Models (MLLMs) without external guidance. - BYE operates via a three-stage pipeline: extracting attention maps, computing entropy scores and profiling sensitive layers, and performing unsupervised clustering to remove suspicious samples. - Unlike prior defenses, BYE requires no clean supervision, auxiliary labels, or model modifications, making it robust and generalizable. - Extensive experiments across various datasets, models, and trigger types demonstrate BYE's effectiveness, achieving near-zero attack success rates while maintaining clean-task performance. - The code is publicly available at the provided GitHub URL. | ['Multimodal'] | [Link](https://github.com/XuankunRong/BYE) | N/A |
| [Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel
  Decoding](https://arxiv.org/abs/2505.16990) | Xinchao Wang, Xinyin Ma, Runpeng Yu | - Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM), is proposed, combining a vision encoder with a discrete diffusion language model. - A novel training paradigm combines an initial autoregressive phase with a subsequent diffusion phase, addressing training instability and length bias issues. - Dimple-7B surpasses LLaVA-NEXT by 3.9% in performance, demonstrating the effectiveness of the proposed approach. - Confident decoding dynamically adjusts the number of tokens generated per step, significantly improving inference efficiency. - Structure priors allow fine-grained control over the response structure, a capability difficult to achieve in autoregressive models. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/yu-rp/Dimple) | [Link](https://huggingface.co/rp-yu/Dimple-7B), [Link](https://huggingface.co/spaces/rp-yu/Dimple-7B) |
| [VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game
  Quality Assurance](https://arxiv.org/abs/2505.15952) | Nabajeet Barman, Saman Zadtootaghaj, Abhijay Ghildyal, corpaul, taesiri |  - This paper introduces VideoGameQA-Bench, a comprehensive benchmark dataset for evaluating vision-language models (VLMs) on video game quality assurance (QA) tasks.   - The benchmark includes a wide range of QA activities such as visual unit testing, visual regression testing, glitch detection, and bug report generation, covering both images and videos.   - The dataset comprises samples from over 800 games and synthetic game scenes, ensuring diversity and real-world applicability.   - Experiments show that while VLMs perform well on some tasks, they struggle with fine-grained details and certain types of glitches.   - This work highlights the need for standardized benchmarks to evaluate VLM performance in the video game QA domain. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text'] | [Link](https://asgaardlab.github.io/videogameqa-bench/) | N/A |
| [SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward](https://arxiv.org/abs/2505.17018) | Xiangyu Yue, Dongzhan Zhou, Haoming Lyu, Kaituo Feng, Kaixuan Fan | - SophiaVL-R1 is a novel multimodal large language model (MLLM) that integrates a thinking reward model to improve reasoning capabilities.  - The model enhances the quality of the thinking process by evaluating logical soundness, consistency, and redundancy.  - Trust-GRPO, a new training algorithm that weighs the thinking reward based on its reliability, reduces the risk of reward hacking.  - Experimental results demonstrate that SophiaVL-R1 outperforms several state-of-the-art MLLMs on various reasoning benchmarks, including MathVista and MMMU.  - Notably, SophiaVL-R1-7B surpasses LLaVA-OneVision-72B despite having 10 times fewer parameters. | ['Multimodal', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/kxfan2002/SophiaVL-R1) | N/A |
| [SpatialScore: Towards Unified Evaluation for Multimodal Spatial
  Understanding](https://arxiv.org/abs/2505.17012) | Yanfeng Wang, Ya Zhang, Yaohui Chen, Xiao Huang, Haoning Wu | This paper introduces SpatialScore, a comprehensive benchmark for evaluating multimodal large language models' (MLLMs) spatial understanding capabilities.  It integrates existing datasets and proposes VGBench, a specialized benchmark for visual geometry perception.  The paper also introduces SpatialAgent, a novel multi-agent system that leverages specialized tools for enhanced spatial understanding.  Extensive experiments show that SpatialAgent significantly improves the performance of various existing MLLMs on the SpatialScore benchmark.  The findings highlight that current MLLMs still face challenges in tasks involving visual geometry perception, suggesting future directions for MLLM development. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Depth Estimation', 'Image Classification', 'Object Detection', 'Image Segmentation', 'Image Feature Extraction', 'Keypoint Detection', 'Natural Language Processing', 'Question Answering', 'Robotics'] | [Link](https://haoningwu3639.github.io/SpatialScore) | N/A |
| [LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/abs/2505.16839) | Yusuke Kato, Akash Gokul, Hritik Bansal, Konstantinos Kallidromitis, Shufan Li | - LaViDa is a novel family of Vision-Language Models (VLMs) based on discrete diffusion models (DMs), which offer advantages such as parallel decoding for faster inference and bidirectional context for controllable generation. - The LaViDa architecture comprises a vision encoder (SigLIP-400M), a diffusion language model (LLaDA-8B or Dream-7B), and an MLP projection network, jointly fine-tuned for multimodal instruction following. - LaViDa incorporates novel techniques like complementary masking for efficient training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling, addressing challenges in adapting DMs to multimodal tasks. - Experiments on various benchmarks (MMMU, MathVista, ChartQA, ScienceQA) demonstrate that LaViDa achieves competitive or superior performance to existing AR VLMs, while providing unique advantages in terms of speed-quality trade-off, controllability, and bidirectional reasoning. - On COCO captioning, LaViDa surpasses Open-LLaVa-Next-Llama3-8B by +4.1 CIDEr with 1.92× speedup; on bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. | ['Multimodal'] | [Link](https://github.com/jacklishufan/LaViDa) | N/A |
| [Training-Free Reasoning and Reflection in MLLMs](https://arxiv.org/abs/2505.16151) | Zhenzhong Chen, Hongchen Wei | - This paper introduces FRANK, a training-free multimodal large language model (MLLM) that enhances existing MLLMs with reasoning and reflection capabilities without any gradient updates or extra supervision. - FRANK leverages two key insights: homologous model merging, which treats both vision and reasoning models as task-fine-tuned variants of the same base LLM, and layer-wise functional specialization, which recognizes that shallow layers in MLLMs focus on visual perception while deeper layers focus on text. - It uses a layer-wise Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow layers. - Experiments demonstrate FRANK's effectiveness on challenging multimodal reasoning benchmarks, outperforming the strongest baseline InternVL2.5-38B by +5.3 on the MMMU benchmark and even surpassing the proprietary GPT-40 model. - FRANK is a training-free method that addresses the challenges of retraining and data scarcity associated with reinforcement learning approaches for multimodal reasoning. | ['Multimodal'] | N/A | N/A |
| [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879) | Ching-Chen Kuo, Kaizhi Zheng, Diji Yang, Xuehai He, Yue Fan | - The paper introduces GRIT, a novel method that teaches large language models (LLMs) to perform grounded reasoning with images by generating reasoning chains that interleave natural language with explicit bounding box coordinates. - GRIT employs a reinforcement learning approach, GRPO-GR, with rewards focusing on answer accuracy and the format of grounded reasoning, eliminating the need for data with reasoning chain annotations. - The method achieves high data efficiency, requiring only 20 image-question-answer triplets for training. - Experiments demonstrate that GRIT effectively trains LLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities. - The results indicate that models trained with GRIT outperform baselines across various testing sets in terms of both answer accuracy and grounding quality. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://grounded-reasoning.github.io) | N/A |
| [AGENTIF: Benchmarking Instruction Following of Large Language Models in
  Agentic Scenarios](https://arxiv.org/abs/2505.16944) | Youfeng Liu, Amy Xin, Xiaozhi Wang, Hao Peng, Yunjia Qi | - This paper introduces AGENTIF, the first benchmark designed for evaluating the instruction-following capabilities of LLMs in realistic agentic scenarios. - AGENTIF is composed of 707 instructions derived from 50 real-world agentic applications, featuring an average length of 1723 words and 11.9 constraints per instruction. - The benchmark systematically evaluates existing advanced LLMs and demonstrates that current models perform poorly, particularly in handling complex constraint structures and tool specifications. - An error analysis reveals that challenges mainly stem from condition and tool constraints, highlighting the need for further research in these areas. - The code and data for AGENTIF are publicly available to facilitate future research. | ['Natural Language Processing'] | [Link](https://github.com/THU-KEG/AgentIF) | [Link](https://huggingface.co/datasets/THU-KEG/AgentIF) |
| [Think or Not? Selective Reasoning via Reinforcement Learning for
  Vision-Language Models](https://arxiv.org/abs/2505.16854) | Mike Zheng Shou, James Cheng, Kevin Qinghong Lin, Jiaqi Wang | - This paper introduces TON, a two-stage training framework that enhances reasoning in vision-language models by enabling selective reasoning. - The first stage uses a supervised fine-tuning method with a "thought dropout" operation, which randomly replaces reasoning traces with empty thoughts, creating a "think-or-not" format for selective reasoning. - The second stage utilizes Group Relative Policy Optimization (GRPO) to enable the model to freely learn when to engage in reasoning, maximizing task-aware outcome rewards. - Experimental results demonstrate that TON reduces completion length by up to 90% compared to vanilla GRPO without sacrificing performance, and in some cases, even improving it. - TON consistently demonstrates improved efficiency across diverse vision-language tasks, indicating the model progressively learns to bypass unnecessary reasoning steps. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/kokolerk/TON) | N/A |
| [VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced
  Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.16192) | Haiyang Xu, Han Yang, Wei Ye, Yongrui Heng, Chaoya Jiang | - This paper introduces VLM-R³, a novel multimodal large language model (MLLM) framework that incorporates region recognition, reasoning, and refinement for improved chain-of-thought reasoning. - The core of VLM-R³ is Region-Conditioned Reinforcement Policy Optimization (R-GRPO), a training paradigm that rewards the model for selecting informative regions, applying appropriate transformations, and integrating visual context into reasoning steps. - To bootstrap the R-GRPO policy, a new Visuo-Lingual Interleaved Rationale (VLIR) dataset was created providing step-level supervision on region selection and textual justification. - Experiments on various benchmarks such as MathVista, ScienceQA, and others demonstrate that VLM-R³ achieves state-of-the-art results in zero-shot and few-shot settings, especially on questions requiring subtle spatial reasoning or fine-grained visual cue extraction. - The largest gains are observed on tasks demanding subtle spatial reasoning or fine-grained visual cue extraction. | ['Multimodal'] | N/A | N/A |
| [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963) | Cheng Zeng, Jianxiang Wang, Zejun Li, Siyuan Wang, Shujun Liu | - This paper introduces OViP, an online vision-language preference learning framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. - OViP addresses the issue of Large Vision-Language Models (LVLMs) hallucinating by using a diffusion model to generate more relevant negative samples. - The framework incorporates both response-centric and image-centric preference learning to effectively reduce hallucinations while preserving core multimodal capabilities. - Experiments demonstrate that OViP effectively reduces hallucinations and outperforms other methods on several benchmark datasets. - The paper also introduces refined evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Let Androids Dream of Electric Sheep: A Human-like Image Implication
  Understanding and Reasoning Framework](https://arxiv.org/abs/2505.17019) | Yazhe Niu, Chenhao Zhang | - This paper introduces LAD, a novel framework for human-like image implication understanding and reasoning, addressing the challenge of contextual gaps in existing models. - LAD is a three-stage framework: Perception (converting visual information into textual representations), Search (iteratively searching and integrating cross-domain knowledge), and Reasoning (generating context-aligned image implications). - Using the lightweight GPT-4o-mini model, LAD achieves state-of-the-art performance on English image implication benchmarks and significant improvement on Chinese benchmarks. - The framework outperforms other models on Multiple-Choice Questions and Open-Style Questions, demonstrating the effectiveness of its contextual alignment approach. - LAD provides insights into how AI can effectively interpret image implications, advancing vision-language reasoning and human-AI interaction. | ['Multimodal'] | [Link](https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep) | N/A |
| [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186) | Aosong Feng, Jayanth Srinivasa, Gaowen Liu, Xuandong Zhao, Kaiwen Zhou | - This paper introduces SafeKey, a novel framework designed to enhance the safety of Large Reasoning Models (LRMs) by amplifying "aha-moment" insights during safety reasoning. - SafeKey incorporates two main objectives: a Dual-Path Safety Head to improve safety signals in the model's internal representations and a Query-Mask Modeling objective to focus the model's attention on query understanding. - Experiments across various safety benchmarks show that SafeKey significantly improves safety generalization, reducing the average harmfulness rate by 9.6% while maintaining general abilities. - The effectiveness of SafeKey is demonstrated through analysis of internal attention patterns and improved quality of hidden representations. - SafeKey addresses the limitation of supervised fine-tuned models that struggle to generalize to unseen malicious queries, a critical challenge in ensuring robust safety for LRMs. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://safekeylrm.github.io) | N/A |
| [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot
  Manipulation Datasets](https://arxiv.org/abs/2505.15517) | Ken Goldberg, Zehan Ma, Shuangyu Xie, keplerccc | - The paper introduces Robo2VLM, a novel framework for generating visual question answering (VQA) datasets from real-world robot manipulation trajectories.  - Robo2VLM leverages multiple sensory modalities (RGB images, stereo depth, end-effector pose, gripper state, force-torque) to segment trajectories into manipulation phases and generate VQA questions based on spatial, goal-conditioned, and interaction reasoning. - A large-scale VQA dataset, Robo2VLM-1, is created using 176k real robot trajectories from the Open X-Embodiment dataset, containing 684,710 questions covering 463 scenes and 3,396 robotic manipulation tasks.  - Experiments demonstrate that fine-tuning state-of-the-art VLMs on Robo2VLM-1 improves their performance on spatial and interaction reasoning tasks, with significant gains observed in some categories. - Comparison with human performance shows that the best performing models achieve near-human accuracy in object-centric categories but still show a considerable gap in complex reasoning tasks. | ['Visual Question Answering', 'Robotics', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/keplerccc/Robo2VLM-1) |
| [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal
  Large Language Models](https://arxiv.org/abs/2505.17015) | Xiaodong Wang, Xingyu Chen, Hao Tang, Weiyao Wang, Runsen Xu | - This paper introduces Multi-SpatialMLLM, a novel framework that enhances multi-modal large language models (MLLMs) with robust multi-frame spatial understanding. - The core of this framework is the MultiSPA dataset, a large-scale collection of over 27 million samples encompassing diverse 3D and 4D scenes, along with a comprehensive benchmark for evaluating spatial reasoning tasks. - Multi-SpatialMLLM significantly outperforms baseline and proprietary systems on the proposed MultiSPA benchmark, demonstrating its effectiveness in various spatial tasks. - The model showcases multi-task learning benefits and emergent capabilities in challenging scenarios, showcasing its potential applications in robotics. - Additionally, the research explores the use of Multi-SpatialMLLM as a multi-frame reward annotator for robotics. | ['Multimodal', 'Robotics', 'Visual Question Answering', 'Computer Vision', 'Depth Estimation'] | N/A | N/A |
| [When Do LLMs Admit Their Mistakes? Understanding the Role of Model
  Belief in Retraction](https://arxiv.org/abs/2505.16170) | Robin Jia, ayyyq | - This paper introduces the concept of retraction in LLMs, defining it as the acknowledgement of errors in previously generated answers. - The authors construct model-specific datasets to evaluate the frequency and causes of retraction in LLMs. - They demonstrate a strong correlation between a model's internal belief and its decision to retract, showing that models are less likely to retract answers they believe to be correct. - Through experiments, they establish a causal link between belief and retraction, showing that manipulating the model's belief influences its retraction behavior. - Finally, they demonstrate that supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ayyyq/llm-retraction) | N/A |
| [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal
  Reasoning](https://arxiv.org/abs/2505.16088) | Wei Zhao, Maxime Peyrard, Gagan Bhatia | - This paper introduces DATEAUGBENCH, a new benchmark dataset comprising 6,500 examples with 21 date formats, designed to evaluate the impact of date tokenization on temporal reasoning in large language models (LLMs). - They introduce a new metric, the date fragmentation ratio, which measures how faithfully a tokenizer preserves multi-digit date components. - Experiments reveal an emergent date-abstraction mechanism in LLMs, where models stitch together fragmented date components for temporal reasoning. - The study finds that excessive date fragmentation correlates with accuracy drops of up to 10 points on uncommon dates. - Analysis shows that larger models compensate for date fragmentation more effectively and quickly than smaller models. | ['Natural Language Processing'] | N/A | N/A |
| [How Do Large Vision-Language Models See Text in Image? Unveiling the
  Distinctive Role of OCR Heads](https://arxiv.org/abs/2505.15865) | Hwanhee Lee, Sunghyun Ryu, Hwan Chang, Ingeol Baek | - This paper introduces a novel method to identify Optical Character Recognition (OCR) heads within Large Vision-Language Models (LVLMs). - The proposed method leverages a scoring mechanism to distinguish OCR heads from other retrieval heads based on their activation patterns and ability to extract textual information from images. - The study reveals three key properties of OCR heads: reduced sparsity, qualitative distinctiveness, and static activation patterns. - Experiments on downstream tasks, including chain-of-thought prompting and attention masking, validate the specialized role of OCR heads in processing embedded textual information and improve performance. - The findings provide a deeper understanding of LVLMs' internal mechanisms for handling embedded textual information and offer insights for enhancing multimodal reasoning and reducing hallucination in OCR-centric applications. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation
  Capabilities in Any Language](https://arxiv.org/abs/2505.14395) | Jiho Jin, Eunsu Kim, Seogyeong Jeong, aliceoh, seyoungsong | MUG-Eval is a novel evaluation framework designed to assess the multilingual text generation capabilities of large language models (LLMs) across a wide range of languages, particularly those with limited resources.  It overcomes limitations of existing methods by transforming existing benchmarks into conversational tasks, thus avoiding the need for language-specific tools or LLM-as-judges.  The framework uses task success rate as a proxy for evaluating generation quality, demonstrating strong correlation with established benchmarks. MUG-Eval offers a robust and efficient solution, readily scalable to thousands of languages. | ['Text Generation'] | [Link](https://github.com/seyoungsong/mugeval) | N/A |


## Papers for 2025-05-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302) | Zeyue Xue, Yutao Zeng, Jing Liu, Chaoyi Zhang, ChenMnZ | - This paper introduces a unified scaling law for quantization-aware training (QAT) of large language models (LLMs), which models quantization error as a function of model size, training data volume, and quantization group size. - The proposed scaling law is validated through 268 QAT experiments, showing that quantization error decreases with increasing model size but increases with more training tokens and coarser quantization granularity.  - The paper decomposes quantization error into weight and activation components, finding that weight quantization error increases more rapidly with more training tokens, while activation quantization error in the FC2 layer is identified as a primary bottleneck. - A mixed-precision quantization approach is proposed to address the bottleneck, demonstrating that weight and activation quantization errors can converge to similar levels with sufficient training data.  - The findings provide key insights for improving QAT research and development, suggesting that reducing both weight and activation quantization error is important for efficient quantized LLMs. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement
  Learning](https://arxiv.org/abs/2505.14231) | Jing Tang, Yong Liu, Mingxing Li, Sule Bai, xiaochonglinghu | - UniVG-R1, a novel reasoning-guided multimodal large language model (MLLM), is proposed for universal visual grounding, enhancing reasoning capabilities through reinforcement learning and cold-start data. - A high-quality Chain-of-Thought (CoT) grounding dataset is constructed, annotated with detailed reasoning chains to guide the model towards correct reasoning paths. - Rule-based reinforcement learning is performed to encourage the model to identify correct reasoning chains, and a difficulty-aware weight adjustment strategy is introduced to address a difficulty bias in RL training. - UniVG-R1 achieves state-of-the-art performance on MIG-Bench, surpassing the previous method by 9.1%, and demonstrates strong generalizability with an average improvement of 23.4% in zero-shot performance across four benchmarks. - The effectiveness of UniVG-R1 is demonstrated through extensive experiments on various datasets, showcasing its superior performance compared to existing methods. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/abs/2505.15809) | Ke Shen, Bowen Li, Ling Yang, comin, tyfeld |  - MMADA is a novel class of multimodal diffusion foundation models that unifies textual reasoning, multimodal understanding, and text-to-image generation.  - It adopts a unified diffusion architecture with a shared probabilistic formulation and modality-agnostic design, eliminating the need for modality-specific components.  - A mixed long-chain-of-thought (CoT) fine-tuning strategy is implemented to curate a unified CoT format across modalities and enhance cold-start training.  - UniGRPO, a unified policy-gradient-based RL algorithm tailored for diffusion foundation models, is proposed to unify post-training.  - MMADA-8B surpasses existing models like LLaMA-3-7B and Qwen2-7B in textual reasoning, Show-o and SEED-X in multimodal understanding, and SDXL and Janus in text-to-image generation. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/Gen-Verse/MMaDA) | N/A |
| [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909) | Pengfei Liu, zizi-0123, henryhe0123 | - The paper introduces PC Agent-E, an efficient agent training framework that significantly reduces the reliance on large-scale human demonstrations for training computer use agents. - PC Agent-E leverages a small set of human-annotated trajectories, further improved by synthesizing diverse action decisions with Claude 3.7 Sonnet, to train a model that outperforms the strong Claude 3.7 Sonnet baseline on WindowsAgentArena-V2. - The model achieves a remarkable 141% relative improvement compared to the Qwen2.5-VL-72B baseline. - PC Agent-E demonstrates strong generalizability to different operating systems, showcasing its ability to adapt to various environments. - The authors open-source their code, data, and models to facilitate future research. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/GAIR-NLP/PC-Agent-E) | N/A |
| [Diffusion vs. Autoregressive Language Models: A Text Embedding
  Perspective](https://arxiv.org/abs/2505.15045) | Anh Tuan Luu, Arman Cohan, LYGeng, yilunzhao, siyue | This paper introduces DIFFEMBED, a novel approach for text embedding using diffusion language models.  DIFFEMBED utilizes a bidirectional attention architecture, which addresses limitations found in autoregressive models' unidirectional attention.  Experiments demonstrate DIFFEMBED's superior performance on various retrieval tasks such as long-document retrieval, reasoning-intensive retrieval, and instruction-following retrieval. The results show that bidirectional attention is crucial for encoding global context in long and complex text.  Finally, the authors introduce REASONAUG, a new dataset for training embedding models on reasoning-intensive tasks. | ['Natural Language Processing'] | [Link](https://github.com/anonymous) | N/A |
| [When to Continue Thinking: Adaptive Thinking Mode Switching for
  Efficient Reasoning](https://arxiv.org/abs/2505.15400) | Haodong Zhao, Yaawennn, Machine981, Amanda2023, DadaCloud01 | - This paper introduces Adaptive Self-Recovery Reasoning (ASRR), a novel framework that dynamically adjusts reasoning length in large reasoning models (LRMs) based on problem difficulty. - ASRR leverages an "Internal Self-Recovery Mechanism" where models implicitly supplement reasoning during answer generation, suppressing unnecessary reasoning for simple tasks. - The framework incorporates an accuracy-aware length reward regulation, allocating reasoning effort efficiently based on problem difficulty. - Experiments across multiple benchmarks and models show that ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss. - Results highlight ASRR's potential for efficient, adaptive, and safer reasoning in LRMs, improving performance and safety alignment on various benchmarks. | ['Natural Language Processing'] | N/A | N/A |
| [Deliberation on Priors: Trustworthy Reasoning of Large Language Models
  on Knowledge Graphs](https://arxiv.org/abs/2505.15210) | Jun Liu, Rui Xing, Zhitao Gao, Jie Ma, stillqu | - This paper introduces Deliberation over Priors (DP), a novel framework that enhances the trustworthiness of Large Language Models (LLMs) reasoning on Knowledge Graphs (KGs). - DP employs a progressive knowledge distillation strategy to enhance LLMs' structural pattern awareness of KGs and a reasoning-introspection strategy to verify the reasoning reliability. - Experiments on three benchmark datasets demonstrate that DP achieves state-of-the-art performance, particularly a 13% improvement on the Complex WebQuestions dataset, while generating highly trustworthy responses. - The framework’s flexibility is shown by integrating it with various LLMs, and its practicality is demonstrated via various analysis on its efficiency and interaction. - DP shows superior performance to existing methods, even in scenarios requiring high precision and where the correct answer must be in the top-ranked position. | ['Question Answering'] | [Link](https://github.com/reml-group/Deliberation-on-Priors) | N/A |
| [lmgame-Bench: How Good are LLMs at Playing Games?](https://arxiv.org/abs/2505.15146) | Eric P. Xing, Haoyang Yu, Mingjia Huo, Yuxuan13, Snyhlxde | This paper introduces Imgame-Bench, a new benchmark for evaluating large language models (LLMs) in playing video games.  It addresses challenges like brittle vision perception and prompt sensitivity through modular harnesses (perception, memory, reasoning modules) and data contamination mitigation.  Imgame-Bench is shown to effectively differentiate 13 leading LLMs across six diverse games, revealing unique capability blends.  Reinforcement learning on Imgame-Bench games is also demonstrated to transfer to unseen games and external planning tasks. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/lmgame-org/GamingAgent/lmgame-bench) | N/A |
| [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781) | Xinchao Wang, Gongfan Fang, Runpeng Yu, Xinyin Ma | - This paper introduces dKV-Cache, a novel KV-cache-like mechanism designed to accelerate the inference speed of Diffusion Language Models (DLMs). - The core idea is to address the incompatibility of DLMs with traditional KV-cache by employing a delayed and conditioned caching strategy for key and value states. - Two variants of dKV-Cache are proposed: dKV-Cache-Decode, which offers near lossless acceleration and even improves performance on long sequences, and dKV-Cache-Greedy, which achieves higher speedups with a slightly reduced performance. - Experiments on various benchmarks demonstrate that dKV-Cache achieves 2-10x speedup in inference, significantly narrowing the gap between autoregressive and diffusion language models. - The code for dKV-Cache is publicly available on GitHub. | ['Text Generation'] | [Link](https://github.com/horseee/dKV-Cache) | N/A |
| [How Should We Enhance the Safety of Large Reasoning Models: An Empirical
  Study](https://arxiv.org/abs/2505.15404) | Qi Zhu, Victor Shea-Jay Huang, Xian Qi Loye, Zhexin Zhang, yangjunxiao2021 | - This paper presents a comprehensive empirical study on enhancing the safety of Large Reasoning Models (LRMs). - It identifies three key failure patterns in directly distilling safe responses from LRMs and proposes methods to address these issues. - The study demonstrates that simpler reasoning processes (short or template-based) can achieve comparable safety performance to complex reasoning, which is more efficient for models to learn. - It explores the impact of incorporating benign reasoning data during safety fine-tuning and shows that it helps balance safety and over-refusal. - The findings offer insights into improving the safety of LRMs and highlight the trade-offs between reasoning, safety, and over-refusal. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/LRM-Safety-Study) | N/A |
| [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817) | Heng Huang, R. Thomas McCoy, Simeng Han, Lichang Chen, TongZheng1999 | This paper introduces Mixture-of-Thought (MoT), a novel framework that leverages multiple reasoning modalities (natural language, code, and truth tables) to enhance the logical reasoning capabilities of LLMs.  MoT employs a self-evolving training process which jointly learns across modalities, resulting in improved performance compared to single-modality baselines.  Experiments on the FOLIO and ProofWriter benchmarks show significant improvements in accuracy, achieving up to a +11.7pp average gain.  The improved performance is attributed to the synergistic combination of modalities during both training and inference.  The approach shows robustness on harder problems and addresses key bottlenecks in natural language inference. | ['Natural Language Processing'] | [Link](https://github.com/zhengkid/Truth_Table_Logical_Reasoning) | N/A |
| [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data
  Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656) | Hongning Wang, Shiyao Cui, Yuhao Sun, Zhexin Zhang, yangjunxiao2021 | - This paper reveals a novel security risk in fine-tuning open-source LLMs with proprietary data: the original creators of the LLM can extract the private fine-tuning data using a simple backdoor training technique. - The attack leverages a backdoor instruction to force the model to reproduce the training queries, requiring only black-box access to the downstream model. - Experiments across four popular open-source LLMs and two datasets demonstrate high extraction performance: up to 76.3% of the data can be extracted in realistic settings, increasing to 94.9% under ideal conditions. - A detection-based defense strategy is explored, but it is shown to be bypassable, highlighting the significance of the discovered vulnerability. - The authors emphasize the urgency of addressing this risk and release their code and data to foster further research in mitigating this security concern. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/Backdoor-Data-Extraction) | N/A |
| [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934) | Mingsheng Long, Ningya Feng, Shaofeng Yin, manchery | This paper introduces RLVR-World, a framework that uses reinforcement learning with verifiable rewards to train world models across diverse modalities.  It unifies world modeling into an autoregressive generation framework, treating states and actions as a sequence of tokens.  RLVR-World demonstrates significant performance gains on language and video-based world models in various domains. The model achieves +30.7% accuracy on text-based game state prediction and +9.2% relative improvement on LPIPS for robot manipulation trajectory prediction. Finally, it shows the utility of reinforced world models in downstream applications like policy evaluation and model-predictive control. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://thuml.github.io/RLVR-World) | N/A |
| [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous
  Concept Space](https://arxiv.org/abs/2505.15778) | Chenyang Zhao, Ao Shen, Weixiang Yan, Xuehai He, Zhen Zhang | - This paper introduces Soft Thinking, a training-free method that improves the reasoning capabilities of large language models (LLMs) by enabling reasoning in a continuous concept space. - Soft Thinking generates soft, abstract concept tokens as probability-weighted mixtures of token embeddings, allowing for smoother transitions and richer representations compared to traditional discrete token-based methods. - Empirical evaluations on mathematical and coding datasets show that Soft Thinking consistently improves both accuracy (up to 2.48% on pass@1 accuracy) and generation efficiency (up to 22.4% reduction in generation length). - The method's effectiveness is demonstrated across various LLMs, including QwQ-32B, DeepSeek-R1-Distill-Qwen-32B, and DeepSeek-R1-Distill-Llama-70B. - Qualitative analysis reveals that Soft Thinking generates highly interpretable and readable outputs, highlighting its potential to enhance LLM reasoning. | ['Natural Language Processing'] | [Link](https://github.com/eric-ai-lab/Soft-Thinking) | [Link](null) |
| [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search
  with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776) | Xipeng Qiu, Kai Song, Ruijun Feng, Siyin Wang, BeastyZ | - ConvSearch-R1 is a novel self-driven framework for conversational query reformulation that eliminates the need for external rewrite supervision by leveraging reinforcement learning. - It uses a two-stage approach: Self-Driven Policy Warm-Up (addresses the cold-start problem) and Retrieval-Guided Reinforcement Learning (addresses sparsity issues in conventional retrieval metrics). - ConvSearch-R1 significantly outperforms state-of-the-art methods on TopiOCQA and QReCC datasets, achieving over 10% improvement on TopiOCQA while using smaller (3B parameter) models. - The model employs a rank-incentive reward shaping mechanism to address sparsity in conventional retrieval metrics, enhancing stable and efficient exploration. - ConvSearch-R1 demonstrates strong generalization ability across models of various scales and datasets. | ['Reinforcement Learning', 'Question Answering', 'Natural Language Processing'] | [Link](https://github.com/BeastyZ/ConvSearch-R1) | N/A |
| [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529) | Chujie Zheng, Xiaoce Wang, Haoran Liu, Jinzhe Tu, yangjunxiao2021 | - This paper introduces BARREL, a novel framework that improves the factual reliability and reduces overconfidence in Large Reasoning Models (LRMs). - BARREL addresses two key pathological reasoning patterns: last-minute guessing and second-thought spiraling, which contribute to overconfident and incorrect answers. - The framework consists of three main components: Knowledge Labeling, Reasoning Trace Construction for SFT, and GRPO Stage, designed to improve concise and boundary-aware factual reasoning. - Experimental results demonstrate that BARREL training significantly enhances the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while maintaining accuracy comparable to models finetuned on reasoning data. - The study also highlights the importance of medium-level rewards in encouraging uncertainty-aware refusal, addressing the root cause of models' inability to admit ignorance. | ['Question Answering'] | [Link](https://github.com/thu-coai/BARREL) | [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1), [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B), [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B), [Link](https://huggingface.co/Qwen/QwQ-32B) |
| [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827) | Jianfeng Gao, Jingbo Shang, Chandan Singh, Liyuan Liu, Yufan Zhuang | - This paper introduces Mixture of Inputs (MOI), a training-free method that improves autoregressive text generation by incorporating the previously discarded token distribution into the generation process. - MOI uses a Bayesian estimation method to blend the sampled token with its distribution, resulting in a richer internal representation and improved text quality. - The method consistently improves performance across multiple LLMs (QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B) on mathematical reasoning, code generation, and PhD-level question answering tasks. - MOI is computationally efficient with negligible overhead and is immediately applicable to existing models without requiring additional training or architectural changes. - Experiments show that MOI outperforms standard autoregressive generation and an ablation study shows that the Bayesian approach is crucial for the performance gains. | ['Text Generation'] | [Link](https://github.com/EvanZhuang/mixinputs) | N/A |
| [RL Tango: Reinforcing Generator and Verifier Together for Language
  Reasoning](https://arxiv.org/abs/2505.15034) | Duane S. Boning, Zhang-Wei Hong, Maohao Shen, Zhengqi Gao, sunshinekevin | - TANGO, a novel framework, concurrently trains a large language model (LLM) generator and a generative LLM verifier using reinforcement learning (RL). - The verifier is trained solely on outcome-level verification correctness rewards without explicit process-level annotations, improving robustness and generalization. - TANGO achieves state-of-the-art results among 7B/8B-scale models on five competition-level math benchmarks and four challenging out-of-domain reasoning tasks. - Both components of TANGO exhibit substantial improvements on difficult mathematical reasoning problems. - The framework addresses limitations of previous approaches by utilizing a generative, process-level verifier that co-evolves with the generator. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/kaiwenzha/rl-tango) | N/A |
| [Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM](https://arxiv.org/abs/2505.15816) | Ziwei Liu, Lewei Lu, Penghao Wu | - This paper introduces ProxyV, a novel approach to reduce computational redundancy in large multimodal models (LMMs) by focusing on computation-level redundancy rather than token-level redundancy. - ProxyV uses proxy vision tokens to alleviate the computational burden on original vision tokens, enhancing efficiency without sacrificing performance and potentially improving performance. - The method is shown to be flexible and can be combined with token reduction methods to further boost efficiency. - Experimental results demonstrate that ProxyV consistently achieves no performance loss or even improvements on various benchmarks. - The code for ProxyV will be made publicly available. | ['Multimodal'] | [Link](https://github.com/penghao-wu/ProxyV) | N/A |
| [Evaluate Bias without Manual Test Sets: A Concept Representation
  Perspective for LLMs](https://arxiv.org/abs/2505.15524) | Zirui Song, Chenxi Wang, Wei Liu, Kaiyang Wan, Lang Gao | This paper introduces BIASLENS, a novel framework for evaluating bias in large language models (LLMs) without relying on manually curated test sets.  BIASLENS leverages concept activation vectors (CAVs) and sparse autoencoders (SAEs) to extract interpretable concept representations from the model's internal feature space.  Bias is quantified by measuring the asymmetry in representational similarity between a target concept and reference concepts.  The method demonstrates strong agreement with existing bias evaluation metrics and reveals new, subtle biases that are difficult to detect with current methodologies.  The framework is scalable and efficient, facilitating fully automatic bias evaluation. | ['Natural Language Processing'] | [Link](https://github.com/jbloomAus/SAELens) | N/A |
| [Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large
  Audio-Language Models](https://arxiv.org/abs/2505.15406) | Lang Gao, Mingzhe Li, Mingxuan Cui, Qian Jiang, Zirui Song | - This paper introduces AJailBench, the first benchmark for evaluating jailbreak vulnerabilities in Large Audio-Language Models (LAMs). - AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, is created by converting textual jailbreak attacks using realistic text-to-speech synthesis. - The Audio Perturbation Toolkit (APT) generates dynamic adversarial variants by applying targeted distortions across time, frequency, and amplitude domains, while enforcing semantic consistency. - Evaluation of state-of-the-art LAMs reveals that none exhibit consistent robustness across attacks, and even small, semantically preserved perturbations significantly reduce the safety performance. - AJailBench and APT are released to facilitate future research on LAM safety. | ['Audio', 'Audio Classification'] | [Link](https://github.com/mbzuai-nlp/AudioJailbreak) | N/A |
| [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818) | Haidong Wang, Jun Zheng, Leon Lin | - WebNovelBench, a novel benchmark for evaluating long-form Chinese web novel generation by LLMs, is introduced. - The benchmark uses a dataset of over 4,000 Chinese web novels and frames evaluation as a synopsis-to-story generation task. - A multifaceted evaluation framework encompassing eight narrative quality dimensions, automatically assessed via an LLM-as-Judge approach, is proposed. - The performance of 24 state-of-the-art LLMs is comprehensively analyzed, ranking their storytelling abilities and providing insights for future development. - WebNovelBench offers a scalable, replicable, and data-driven methodology for evaluating and advancing LLM-driven narrative generation. | ['Text Generation'] | [Link](https://github.com/OedonLestrange42/webnovelbench) | [Link](https://huggingface.co/datasets/0edon42/webnovelbench) |
| [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990) | Dilek Hakkani-Tür, Nimet Beyza Bozdag, Ishika Agarwal | - This paper introduces Language Specific Knowledge (LSK), a phenomenon where language models exhibit stronger performance or preference for certain languages when responding to specific topics. - The authors propose LSKEXTRACTOR, a two-stage framework that identifies expert languages for specific knowledge regions and leverages code-switching to improve inference. -  LSKEXTRACTOR maps LSK and their corresponding expert languages by conducting chain-of-thought reasoning in 13 languages on training queries from various datasets. - During test-time inference, LSKEXTRACTOR embeds an unseen query to identify its corresponding cluster and retrieves the optimal language for reasoning, showing an average relative improvement of 10% in accuracy. - The research contributes to the development of more inclusive language models aligned with cultural and linguistic contexts. | ['Natural Language Processing'] | N/A | N/A |
| [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation
  of LLM Hallucinations](https://arxiv.org/abs/2505.14101) | Johannes Bjerva, Katja Hose, Russa Biswas, ernlavr | - This paper introduces MultiHal, a novel multilingual, multi-hop benchmark for evaluating Large Language Model (LLM) hallucinations grounded in knowledge graphs. - MultiHal leverages 7 existing benchmarks, enriching them with 25,905 high-quality KG paths mined from Wikidata and translated into 5 languages. - A novel unified scalable framework systematically integrates entity linking methods, mapping question-answer pairs to KG paths, to curate factual information. - The baseline evaluation shows an absolute improvement in semantic similarity scores across various LLMs in KG-based retrieval augmented generation (RAG) compared to vanilla question answering. - MultiHal fosters research towards graph-based hallucination mitigation and fact-checking tasks. | ['Question Answering'] | [Link](https://github.com/ernlavr/multihal) | [Link](https://huggingface.co/datasets/ernlavr/multihal) |
| [HumaniBench: A Human-Centric Framework for Large Multimodal Models
  Evaluation](https://arxiv.org/abs/2505.11454) | Mukund S. Chettiar, Ashmal Vayani, Vahid Reza Khazaie, Aravind Narayanan, shainaraza | *- HumaniBench is introduced, a benchmark for evaluating large multimodal models (LMMs) on human-centered criteria including fairness, ethics, and empathy, addressing limitations in existing benchmarks. - The benchmark consists of 32K real-world image-question pairs, annotated using a GPT-4 assisted pipeline and verified by experts, probing seven HCAI principles through diverse tasks such as multilingual QA and visual grounding. - Evaluation of 15 state-of-the-art LMMs reveals that proprietary models generally perform better; however, gaps remain in robustness and visual grounding, with open-source models often lagging in human-aligned principles like ethics and inclusivity. - The dataset, annotation prompts, and evaluation code are publicly released to promote transparency and encourage future research. - HumaniBench is the first benchmark specifically designed to assess human-centered AI principles in LMMs, addressing the gap in evaluating their social responsibility and genuine alignment with human values. | ['Multimodal'] | [Link](https://vectorinstitute.github.io/HumaniBench/) | [Link](https://huggingface.co/vectorinstitute/HumaniBench) |


## Papers for 2025-05-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/abs/2505.14683) | Ziang, codecaution, whyu, gouc, Andy1621 | The paper introduces BAGEL, a unified, decoder-only multimodal foundation model pretrained on trillions of tokens from interleaved text, image, video, and web data.  BAGEL's architecture is a Mixture-of-Transformer-Experts (MoT), maximizing capacity without task-specific constraints.  It significantly outperforms existing open-source models on multimodal generation and understanding benchmarks.  Furthermore, BAGEL exhibits advanced multimodal reasoning capabilities, including free-form image manipulation and future frame prediction. | ['Multimodal'] | [Link](https://github.com/ByteDance/BAGEL) | N/A |
| [SageAttention3: Microscaling FP4 Attention for Inference and An
  Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594) | surfingtomchen, whx1003, haofeng666, Guyan, jt-zhang | - This paper introduces SageAttention3, a novel FP4 attention mechanism that achieves a 5x speedup over the fastest existing FlashAttention implementation on RTX5090 GPUs. - SageAttention3 leverages the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation, reaching 1038 TOPS. - The authors also explore 8-bit training for attention mechanisms, proposing SageBwd, which achieves lossless performance in fine-tuning tasks but exhibits slower convergence during pretraining. - Experimental results demonstrate that both SageAttention3 and SageBwd significantly accelerate inference and training across various models and tasks. - The code for SageAttention3 is publicly available on GitHub. | ['Text2Text Generation', 'Text-to-Image', 'Text-to-Video', 'Video Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14246) | sweetFruit, steins1096, zyshan, yuhangzang, ziyuliu | - This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT), a novel framework that enhances Large Vision-Language Models (LVLMs) with agentic reasoning and tool-use capabilities. - Visual-ARFT utilizes a reward-driven training strategy to enable LVLMs to perform complex multimodal reasoning tasks, such as browsing websites and writing code to manipulate images. - The proposed framework is evaluated on a new Multimodal Agentic Tool Bench (MAT) and existing benchmarks, demonstrating significant improvements over baseline methods. - Visual-ARFT outperforms GPT-4 on MAT-Coding and achieves considerable gains on multi-hop QA benchmarks like 2Wiki and HotpotQA. - The results suggest that Visual-ARFT presents a promising approach toward building robust and generalizable multimodal agents. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT) | N/A |
| [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388) | annariasdu, pabberpe, danihinjos, adriantormos, JordiBayarri-bsc | This paper introduces Aloe Beta, a new family of open-source large language models (LLMs) specialized for healthcare.  The models are created using a multi-stage training pipeline that involves supervised fine-tuning, model merging, and model alignment.  Evaluation results on various benchmarks show that Aloe Beta models achieve competitive performance compared to closed-source models and demonstrate improved safety against adversarial attacks.  The models and datasets used in this research are publicly available. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/axolotl-ai-cloud/axolotl), [Link](https://github.com/OpenRLHF/OpenRLHF), [Link](https://github.com/microsoft/DeepSpeed), [Link](https://github.com/HPAI-BSC/prompt_engine), [Link](https://github.com/HPAI-BSC/medical-specialities) | [Link](https://huggingface.co/collections/HPAI-BSC/healthcare-llms-aloe-family-6701b6a777f7e874a2123363), [Link](https://huggingface.co/collections/HPAI-BSC/aloe-beta-datasets-672374294ed56f43dc302499), [Link](https://huggingface.co/datasets/HPAI-BSC/medprompt_database_llama31), [Link](https://huggingface.co/datasets/aligner/aligner-20K), [Link](https://huggingface.co/datasets/BAAI/Infinity-Preference), [Link](https://huggingface.co/datasets/omi-health/medical-dialogue-to-soap-summary), [Link](https://huggingface.co/datasets/BI55/MedText), [Link](https://huggingface.co/datasets/ZahrizhalAli/mental_health_conversational_dataset), [Link](https://huggingface.co/datasets/gamino/wiki_medical_terms) |
| [Latent Flow Transformer](https://arxiv.org/abs/2505.14513) | Pei-Chen Ho, dsshiu, menghsichen, FengTing, yenchen | - The paper introduces the Latent Flow Transformer (LFT), a novel architecture that replaces a block of transformer layers with a single learned transport operator trained via flow matching, achieving significant compression while maintaining compatibility with the original architecture. - The LFT addresses the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm, which enhances the alignment of latent transport across distant transformer layers. - Experiments on the Pythia-410M model demonstrate that LFT, when trained with flow matching, compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of 0.407 vs. 0.529). - When trained with FW, LFT further distills 12 layers into one, reducing the KL divergence to 0.736, surpassing that from skipping 3 layers (0.932). - The results significantly narrow the gap between autoregressive and flow-based generation paradigms, showing the potential of LFT for efficient language modeling. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/mtkresearch/latent-flow-transformer) | [Link](null) |
| [Neurosymbolic Diffusion Models](https://arxiv.org/abs/2505.13138) | Antonio Vergari, ducdauge, pminervini, HEmile |  - This paper introduces Neurosymbolic Diffusion Models (NESYDMs), a novel class of neurosymbolic predictors that leverage discrete diffusion models to capture dependencies between symbolic concepts.  - NESYDMs address limitations of existing methods that assume conditional independence between symbols by reusing the independence assumption at each step of the diffusion process.  - The model architecture integrates masked diffusion models with symbolic programs, enabling scalable learning while capturing dependencies and uncertainty. - Experimental results on various benchmarks, including visual path planning and autonomous driving, demonstrate that NESYDMs achieve state-of-the-art accuracy and strong calibration among neurosymbolic predictors. - The authors demonstrate that their model improves on existing neurosymbolic methods in both accuracy and reliability. | ['Computer Vision', 'Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/HEmile/neurosymbolic-diffusion) | N/A |
| [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547) | Liangqiong-QU, limingcv, MENGTINGLIU, jcccy, gpx333 | - This paper introduces FedPrLLM, a federated pruning framework for Large Language Models (LLMs) that preserves data privacy. - FedPrLLM enables collaborative pruning of a global LLM model without sharing local calibration data. - Experiments demonstrate that one-shot pruning with layer comparison is optimal within the FedPrLLM framework. - The study explores various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and weight scaling. - Results show that weight scaling does not improve performance, and iterative pruning offers no significant benefits over one-shot pruning. | ['Natural Language Processing'] | [Link](https://github.com/Pengxin-Guo/FedPrLLM) | N/A |
| [Visionary-R1: Mitigating Shortcuts in Visual Reasoning with
  Reinforcement Learning](https://arxiv.org/abs/2505.14677) | Yixuan Li, Peng Gao, kaiyangzhou, yuhangzang, Jiaer-Xia | - This paper introduces Visionary-R1, a novel visual language model trained using reinforcement learning to mitigate shortcut learning in visual reasoning tasks. - Unlike existing methods that rely on chain-of-thought supervision, Visionary-R1 uses only question-answer pairs and a caption-reason-answer output format to encourage deeper image understanding. - The model outperforms strong multimodal models, including GPT-40, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks. - Visionary-R1 uses only reinforcement learning and addresses shortcut learning by requiring the model to generate a caption before reasoning, thereby improving generalization. - The results highlight the effectiveness of reinforcement learning with appropriate output formatting in mitigating shortcuts and improving the robustness of visual reasoning models. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/maifoundations/Visionary-R1) | N/A |
| [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652) | wenhu, zhangysk, DongfuJiang, SivilTaram, MrLight | - This paper introduces GENERAL-REASONER, a novel training paradigm designed to enhance Large Language Model (LLM) reasoning capabilities across diverse domains.  - It constructs a large-scale, high-quality dataset of questions with verifiable answers covering various disciplines and develops a generative model-based answer verifier. - GENERAL-REASONER outperforms existing baseline methods across 12 benchmarks, demonstrating robust and generalizable reasoning performance.  - The model-based verifier is shown to be superior to traditional rule-based methods, enabling effective reinforcement learning across diverse reasoning tasks.  - The research addresses the limitations of previous LLM reasoning works that primarily focus on mathematical and coding domains due to data abundance and ease of verification. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://tiger-ai-lab.github.io/General-Reasoner/) | N/A |
| [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489) | YongilKim, Sunkyoung, soheeyang, seungone, DKYoon | - This paper demonstrates that reasoning models, which utilize chain-of-thought (CoT) reasoning, exhibit superior performance in both problem-solving and accurately expressing confidence compared to their non-reasoning counterparts. - The study benchmarks six reasoning models across six datasets, revealing that reasoning models achieve better confidence calibration in 33 out of 36 settings. - Detailed analysis attributes this improved calibration to the slow thinking behaviors inherent in reasoning models, such as exploring alternatives and backtracking, allowing for dynamic confidence adjustments. - The authors find that reasoning models' calibration improves as their CoT unfolds, a trend not observed in non-reasoning models, further supporting the role of slow thinking. - This improved calibration is not exclusive to reasoning models; non-reasoning models also benefit when guided to perform slow thinking via in-context learning. | ['Question Answering'] | [Link](https://github.com/MattYoon/reasoning-models-confidence) | N/A |
| [Reasoning Path Compression: Compressing Generation Trajectories for
  Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866) | Jae-Joon Kim, YulhwaKim, dongwonjo, jiwonsong | - This paper introduces Reasoning Path Compression (RPC), a training-free method to accelerate inference in large language models (LLMs) that generate lengthy reasoning paths. - RPC leverages the semantic sparsity of reasoning paths by periodically compressing the KV cache, retaining only important entries based on an importance score computed using a selector window. - Experiments demonstrate that RPC improves the generation throughput of QwQ-32B by up to 1.60x compared to full KV cache inference, with minimal accuracy loss (1.2% drop on the AIME 2024 benchmark). - RPC offers a practical approach to deploying reasoning LLMs efficiently by mitigating memory usage and computational overhead associated with long reasoning sequences. - The method is training-free and easily integrated into existing LLM inference pipelines. | ['Natural Language Processing'] | [Link](https://github.com/jiwonsong-dev/ReasoningPathCompression) | N/A |
| [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680) | Wenjie Wang, chuats, jrwen, pl8787, KID-22 | - This paper introduces NExT-Search, a novel paradigm that aims to rebuild the user feedback ecosystem for generative AI search. - NExT-Search integrates two complementary modes: User Debug Mode, allowing engaged users to intervene at key stages of the search pipeline, and Shadow User Mode, using a personalized user agent to simulate user preferences. - The proposed framework leverages fine-grained feedback through online adaptation and offline updates, refining the search process in real-time and periodically fine-tuning models. - It introduces a feedback store that incentivizes user participation, further driving the continuous improvement of generative AI search systems. - The paper concludes by highlighting potential research directions in personalized user simulation, human-centric interface design, and efficient feedback integration. | ['Question Answering'] | N/A | N/A |
| [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the
  Limits of Large Language Models](https://arxiv.org/abs/2505.13559) | Eng Siong Chng, Lim Zhi Hao, Tanmay Surana, SkAndMl | - The paper introduces CS-Sum, a new benchmark dataset for evaluating code-switching dialogue summarization.  It contains 900-1300 human-annotated dialogues per language pair across three language pairs: Mandarin-English, Tamil-English, and Malay-English. - CS-Sum is the first benchmark for CS dialogue summarization across multiple languages and is designed to evaluate the comprehensibility of CS in LLMs. - Ten LLMs were evaluated, including open and closed-source models, using various approaches such as few-shot, translate-summarize, and fine-tuning. - The findings show that although LLMs achieve high scores on automatic metrics, they often make subtle mistakes affecting the overall meaning of the dialogues.  This highlights limitations of current LLMs in processing code-switched data. - Three main types of errors made by LLMs when summarizing CS dialogues were identified and analyzed: Code-Switching Loss, Meaning Shift from Poor Translation, and Speaker Misattribution. | ['Summarization'] | N/A | N/A |
| [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631) | Zewen Chi, Qingxiu Dong, Shaohan Huang, YUSHUIWX, lingjie23 | - This paper introduces Large Hybrid-Reasoning Models (LHRMs), a novel model architecture designed to adaptively determine whether to engage in extended reasoning processes based on the query's contextual information. - The LHRMs architecture utilizes a two-stage training pipeline: Hybrid Fine-Tuning (HFT) and Hybrid Group Policy Optimization (HGPO). - HFT provides a robust initialization, while HGPO implicitly learns to select the appropriate reasoning mode. - Experimental results across various benchmarks demonstrate that LHRMs significantly outperforms existing LLMs and LRMs in both reasoning and general capabilities while substantially improving efficiency. - The proposed Hybrid Accuracy (HAcc) metric provides a quantitative assessment of the model's ability to perform hybrid thinking. | ['Natural Language Processing'] | N/A | N/A |
| [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430) | Minxian Li, Jiayi Zhou, kaiyangzhou, chenyulin, sifengshang | - This paper introduces Quantized Zeroth-order Optimization (QZO), a novel technique for fine-tuning quantized neural networks.  - QZO minimizes memory usage by eliminating gradients and optimizer states using zeroth-order optimization and by employing model quantization. - Compared to full-parameter fine-tuning in bfloat16, QZO reduces the total memory cost by more than 18× for 4-bit LLMs.  - The effectiveness of QZO is demonstrated on various NLP benchmarks and on fine-tuning Stable Diffusion 3.5 Large using only 12.4GB of memory.  - QZO is orthogonal to existing post-training quantization methods and pushes the limits of memory-efficient training. | ['Natural Language Processing', 'Text Classification', 'Text Generation', 'Text-to-Image'] | [Link](https://github.com/maifoundations/QZO) | N/A |
| [SSR: Enhancing Depth Perception in Vision-Language Models via
  Rationale-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448) | Han Zhao, Pengxiang Ding, Xiaomin Yu, Ming Ma, yliu-cs |  - This paper introduces a novel framework, SSR, to improve spatial reasoning in Vision-Language Models (VLMs) by incorporating depth information.  - SSR translates raw depth data into structured textual rationales that serve as intermediate representations, enhancing spatial reasoning capabilities.   - Knowledge distillation is used to compress generated rationales into compact latent embeddings, enabling efficient integration into existing VLMs without retraining. - A new dataset, SSR-COT (a million-scale visual-language reasoning dataset with intermediate spatial reasoning annotations), and a benchmark, SSRBENCH, are introduced for comprehensive evaluation.  - Extensive experiments demonstrate that SSR substantially improves depth utilization and enhances spatial reasoning in VLMs. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464) | Sitong Zhao, Shuaiting Chen, Haotian Wang, Yunjie Ji, Emperorizzis | - This paper introduces three parallel datasets created by distilling reasoning data from three state-of-the-art teacher models: AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1. - The datasets comprise 1.89 million verified outputs on a shared corpus of queries. - Models trained on the AM-Thinking-v1-distilled dataset consistently outperform models trained on the other two datasets across multiple reasoning benchmarks (AIME2024, AIME2025, MATH500, and LiveCodeBench). - The AM-Thinking-v1-distilled model demonstrates adaptive output behavior, generating longer responses for more complex tasks and shorter responses for simpler ones. - The authors release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support further research. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://github.com/huggingface/Math-Verify), [Link](https://huggingface.co/datasets/hivaze/LOGIC-701) |
| [Towards eliciting latent knowledge from LLMs with mechanistic
  interpretability](https://arxiv.org/abs/2505.14352) | Emil Ryd, NeelNanda, srdm, bcywinski | - The paper introduces a novel "Taboo" model, a language model trained to describe a secret word without explicitly mentioning it, to investigate the ability of LLMs to conceal information. - It evaluates different methods for uncovering this hidden knowledge, including both black-box (non-interpretability) and white-box (mechanistic interpretability) approaches. - The findings suggest that interpretability-based techniques, such as Logit Lens and Sparse Autoencoders, are effective in eliciting the secret word, outperforming simpler black-box methods. - The study highlights the potential of mechanistic interpretability for addressing the crucial problem of detecting and extracting hidden knowledge in LLMs, a step towards safer and more reliable deployment. - Future work focuses on expanding to more complex scenarios and testing on more sophisticated models to confirm these findings. | ['Natural Language Processing'] | [Link](https://github.com/EmilRyd/eliciting-secrets) | [Link](https://huggingface.co/bcywinski) |
| [Hunyuan-Game: Industrial-grade Intelligent Game Creation Model](https://arxiv.org/abs/2505.14135) | vcvcvn, tangjs, YellowAddice, zhengsj, lslrh |  - This paper introduces Hunyuan-Game, a novel AI model for generating high-fidelity game assets, encompassing both images and videos.  - The model's architecture uses a combination of diffusion models and domain-specific knowledge to achieve state-of-the-art results in visual fidelity and motion naturalness.  - Extensive experiments show that Hunyuan-Game outperforms existing methods such as Midjourney and Kling in game scenarios.  - The model offers several key functionalities, including text-to-image generation, game visual effects generation, transparent image generation, and game character generation, as well as several video generation capabilities.   - The researchers aim to encourage community-driven innovation and foster collaborative development, paving the way for broader applications in the gaming industry. | ['Text-to-Image', 'Image-to-Video', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [Reward Reasoning Model](https://arxiv.org/abs/2505.14674) | Qingxiu Dong, Zewen Chi, Jiaxin Guo, YUSHUIWX, unilm | - This paper introduces Reward Reasoning Models (RRMs), a novel approach to reward modeling that incorporates a chain-of-thought reasoning process before generating final rewards. - RRMs leverage additional test-time compute to enhance performance, particularly on complex queries where appropriate rewards are not immediately apparent. - The authors implement a reinforcement learning framework to train RRMs, enabling them to self-evolve reward reasoning capabilities without requiring explicit reasoning traces as training data. - Experimental results show that RRMs outperform previous reward models across various benchmarks and model sizes, demonstrating their ability to adaptively exploit test-time compute to improve reward accuracy. - The pre-trained RRM models are available at https://huggingface.co/Reward-Reasoning. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/Reward-Reasoning) |
| [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534) | cchoquette, julsh, tux, iliashum, chongyangs | This paper presents a novel framework for evaluating and improving the robustness of large language models against indirect prompt injection attacks.  The researchers developed adaptive attack techniques that continuously evolve against past, current, and future versions of Gemini, Google's multimodal large language model.  They demonstrate the effectiveness of adversarial fine-tuning for enhancing security.  The findings highlight the importance of adaptive evaluation and defense-in-depth strategies for building resilient models.  They show how more capable models are not necessarily more secure. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [Warm Up Before You Train: Unlocking General Reasoning in
  Resource-Constrained Settings](https://arxiv.org/abs/2505.13718) | Keith Ross, xanubhav81, AadimNepal, guactastesgood, safal312 | - This paper introduces a two-stage training strategy for developing reasoning LLMs in resource-constrained settings.  - The first stage involves "warming up" the model by distilling Long CoTs from a toy domain (Knights & Knaves logic puzzles) to acquire general reasoning skills.  - The second stage applies Reinforcement Learning with Verifiable Rewards (RLVR) to the warmed-up model using a limited set of target-domain examples.  - Experiments demonstrate that this two-phase approach improves reasoning performance across various tasks, including MATH, HumanEval+, and MMLU-Pro, even when training data is scarce.  - The warmed-up model consistently outperforms the base model after RLVR training on the same small dataset and maintains cross-domain generalizability. | ['Question Answering', 'Reinforcement Learning', 'Natural Language Processing'] | [Link](https://anonymous.4open.science/r/warmup-before-you-train-0EEF/) | N/A |
| [Truth Neurons](https://arxiv.org/abs/2505.12182) | ZiningZhu, jordansuchow, ShirleyY, YupengCao, Acatsama | - This paper proposes a novel method to identify "truth neurons" in language models, which are neurons that encode truthfulness in a subject-agnostic manner. - The method uses integrated gradients to measure neuron attribution scores for truthful vs. untruthful responses, identifying neurons positively contributing to truthfulness and negatively correlated with untruthfulness. - Experiments across models of varying scales validate the existence of truth neurons, showing that the encoding of truthfulness at the neuron level is a property shared by many language models. - Suppressing the activations of truth neurons degrades performance on multiple benchmarks, indicating that the truthfulness mechanisms are not tied to a specific dataset. - The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness, suggesting that truthfulness mechanisms primarily appear in the middle to later stages of language models. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365) | Matteo Dora, inoki-giskard, bmalezieux, pierlj | This paper introduces Phare, a multilingual diagnostic framework designed to evaluate LLMs across three safety dimensions: hallucinations, social biases, and harmful content. Unlike traditional benchmarks, Phare focuses on exposing failure modes rather than ranking models.  The evaluation of 17 state-of-the-art LLMs reveals systematic vulnerabilities across all safety dimensions, including sycophancy and stereotype reproduction.  Phare provides actionable insights to build more robust and trustworthy language systems.  The framework includes three modules: Hallucination, Biases and Stereotypes, and Harmful Content. | ['Natural Language Processing', 'Text Classification', 'Text Generation'] | [Link](https://github.com/giskard-ai/phare) | [Link](https://huggingface.co/datasets/giskardai/phare) |
| [MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8](https://arxiv.org/abs/2505.09569) | Lin Chen, Qiang Zhou, omidvarb, sliuxl, linboliu | - Introduced MigrationBench, a novel benchmark dataset for repository-level code migration from Java 8 to Java 17 and 21. - The dataset comprises a comprehensive set of repositories, categorized into subsets based on complexity and the presence of test cases. - Proposed an evaluation framework designed to provide a rigorous and standardized assessment of LLMs in this complex task. - Demonstrated the efficacy of using LLMs with a proposed feedback mechanism to address repository-level code migration challenges. - The benchmark dataset and source code are publicly available on GitHub and HuggingFace. | ['Natural Language Processing'] | [Link](https://github.com/amazon-science/MigrationBench) | [Link](https://huggingface.co/collections/AmazonScience) |
| [Two Experts Are All You Need for Steering Thinking: Reinforcing
  Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681) | Jiahao Xu, Zhiwei He, Yue Wang, Xingyu Chen, Mengru Wang | - This paper introduces a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE) to improve reasoning performance in Mixture-of-Experts (MoE) reasoning models without additional training. - RICE leverages normalized Pointwise Mutual Information (nPMI) to identify specialized experts, termed cognitive experts, that orchestrate meta-level reasoning operations. - Empirical evaluations on DeepSeek-R1 and Qwen3-235B LRMs demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. - The RICE method substantially outperforms existing reasoning-steering techniques, such as prompt design and decoding constraints. - The results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models. | ['Natural Language Processing'] | N/A | N/A |
| [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic
  Reasoning Limits](https://arxiv.org/abs/2505.14178) | Yiwei Xu, Jiaqi Wei, Juntai Cao, Charlesyooo, Wyattz23 |  - This paper investigates the impact of tokenization on the symbolic and arithmetic reasoning capabilities of large language models (LLMs).  - The authors introduce the concept of "Token Awareness" to assess how well tokenization schemes align with the granularity of reasoning tasks.  - They demonstrate that poorly designed tokenization schemes hinder symbolic computation, even with techniques like Chain-of-Thought prompting.  - Through systematic evaluation on arithmetic and symbolic tasks, they showcase how atomically aligned tokenization improves reasoning performance.  - Their findings highlight that the success of symbolic reasoning in LLMs is not solely dependent on model architecture but also on token-level representations. | ['Natural Language Processing'] | [Link](None) | [Link](None) |
| [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via
  Competition](https://arxiv.org/abs/2505.13380) | Van Nguyen, Quang Pham, Huy Nguyen, nhatho, DavidNguyen |  - This paper introduces CompeteSMoE, a novel algorithm for training large language models using a sparse mixture-of-experts (MoE) approach.  - The core innovation is a competition mechanism for routing tokens to experts, which improves sample efficiency and convergence compared to traditional softmax routing.  - CompeteSMoE demonstrates superior zero-shot performance across multiple vision-language and language pre-training benchmarks compared to existing MoE algorithms.  - The competition mechanism is theoretically analyzed, showing its statistical guarantees.  - The algorithm includes a scheduled training approach for efficiency, including a novel diversity loss to encourage diverse representations and a distillation loss to guide the router. | ['Multimodal'] | [Link](https://github.com/Fsoft-AIC/CompeteSMoE) | N/A |
| [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010) | grohg, amosharafa, himel7 | - This paper introduces a novel sentence-level media bias detection model, Bias-Detector, which is a fine-tuned RoBERTa-based model. - Bias-Detector outperforms the existing state-of-the-art DA-ROBERTa model by achieving a significantly higher macro F1 score across multiple folds of cross-validation, as demonstrated through McNemar's Test and a 5x2 paired t-test. - The model's superior performance is attributed to its ability to attend more meaningfully to contextually relevant tokens, thereby avoiding oversensitivity to politically charged terms. - An attention-based analysis reveals that Bias-Detector successfully focuses on crucial contextual elements for accurate bias classification. - The work also integrates a bias type classifier to further enhance media bias analysis, paving the way for more comprehensive future research. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative
  Verifier](https://arxiv.org/abs/2505.11966) | Kezhi Li, Zhijian Xu, Zeju Li, XiangyuWen, Jianyuan1 | - This paper introduces FlexiVe, a novel generative verifier that dynamically balances computational resources between fast and slow thinking modes using a flexible allocation of verification budget strategy. - FlexiVe is integrated into a Solve-Detect-Verify pipeline, which proactively identifies solution completion points to trigger targeted verification and provide solver feedback, resulting in an efficient inference-time scaling framework.  - Experiments demonstrate that FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces and outperforms baselines in reasoning accuracy and inference efficiency on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO). - The Solve-Detect-Verify pipeline significantly outperforms baselines like self-consistency on mathematical reasoning benchmarks in terms of both accuracy and efficiency. - The study also includes ablation studies and comparisons with other methods, such as self-consistency, showing that the proposed method is superior in terms of accuracy and efficiency. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [Masking in Multi-hop QA: An Analysis of How Language Models Perform with
  Context Permutation](https://arxiv.org/abs/2505.11754) | Jeff Z. Pan, Mirella Lapata, pvougiou, hwy9855 | - This paper explores how Language Models (LMs) perform on multi-hop question answering (MHQA) tasks when the order of retrieved documents is permuted. - The authors find that encoder-decoder models generally outperform causal decoder-only models on MHQA, even when significantly smaller. - They observe that optimal performance is achieved when the order of documents aligns with the reasoning chain, and that bi-directional attention can improve the performance of causal decoder-only models. - Furthermore, the study reveals that attention weights tend to peak at higher values when the answer is correct, suggesting a potential heuristic for improving LM performance. - The code for this research is publicly available on Github. | ['Document Question Answering', 'Question Answering'] | [Link](https://github.com/hwy9855/MultiHopQA-Reasoning) | N/A |
| [Incorporating brain-inspired mechanisms for multimodal learning in
  artificial intelligence](https://arxiv.org/abs/2505.10176) | Xin Yang, Qingqun Kong, Yang Li, Dongcheng Zhao, Xiang He | - This paper introduces a novel multimodal fusion strategy called Inverse Effectiveness-driven Multimodal Fusion (IEMF) which dynamically adjusts the fusion module's weights according to the reliability of the unimodal signals. - IEMF is inspired by the biological inverse effectiveness mechanism observed in the brain's multimodal integration. - The experimental results across several audio-visual tasks (classification, continual learning, question answering) and different neural network architectures (ANNs, SNNs) demonstrate that IEMF consistently outperforms the baseline models by achieving higher accuracy and reducing computational costs. - Ablation studies validate the effectiveness of IEMF's key components, showing its ability to handle various levels of unimodal input quality, thus improving the robustness of multimodal fusion. - The proposed IEMF model exhibits good generalizability across different datasets, fusion methods, and network architectures | ['Multimodal', 'Audio-to-Audio', 'Audio Classification', 'Video Classification', 'Audio-to-Audio', 'Visual Question Answering'] | [Link](https://github.com/Brain-Cog-Lab/IEMF) | N/A |
| [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for
  Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306) | Shangbin Feng, Wenhao Yu, Yuwei Zhang, shangjingbo, KomeijiForce | - Introduced WIKIDYK, a novel, real-world, large-scale benchmark for knowledge injection that continuously evolves over time without human intervention. - Proposed WIKIDYK leverages recently-added and human-written facts from Wikipedia's "Did You Know..." entries, which are carefully selected by expert Wikipedia editors. - Extensive experiments using continued pre-training revealed that Bidirectional Language Models (BiLMs) exhibit significantly stronger knowledge memorization capabilities compared to Causal Language Models (CLMs). - Introduced a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to compensate for the smaller scales of current BiLMs and further improve the reliability accuracy. - Showcased that the framework further improves the reliability accuracy by up to 29.1%. | ['Question Answering'] | [Link](https://github.com/zhang-yu-wei/WikiDYK) | [Link](https://huggingface.co/datasets/YWZBrandon/wikidyk) |
| [Understanding Gen Alpha Digital Language: Evaluation of LLM Safety
  Systems for Content Moderation](https://arxiv.org/abs/2505.10588) | Fausto Giunchiglia, Manisha Mehta | - This research introduces a novel dataset comprising 100 contemporary Gen Alpha expressions, gathered from diverse online platforms. - The study systematically evaluates the capacity of four leading AI systems (GPT-4, Claude, Gemini, and Llama 3) to interpret and moderate Gen Alpha communication, focusing on masked harassment and manipulation. - A multi-perspective evaluation framework is developed, assessing comprehension levels of Gen Alpha users, their parents, and professional content moderators, along with the four AI systems. - The findings reveal significant gaps in AI systems' comprehension capabilities, particularly concerning context-dependent meanings and the rapid evolution of Gen Alpha slang. - The paper highlights the urgency for enhanced AI safety systems to effectively protect Gen Alpha users and addresses the broader ethical considerations of youth online safety. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-05-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820) | tricktreat, Chengruidong, iofu728, xutan, KaitaoSong |  * The paper introduces a novel learning paradigm called "Chain-of-Model" (CoM) which incorporates causal relationships into hidden states, improving scaling efficiency and inference flexibility.   * CoM formulates hidden states at each layer as a combination of sub-representations (chains) at the hidden dimension level, allowing progressive model scaling by increasing chains.   * The proposed Chain-of-Language-Model (CoLM) integrates CoM into each Transformer layer, offering multiple sub-models at varying sizes for elastic inference.   * CoLM-Air introduces a KV sharing mechanism, further enhancing extensibility with seamless LM switching and prefilling acceleration.  * Experimental results show that CoLM achieves comparable performance to standard Transformers while offering greater flexibility and efficiency. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/microsoft/CoLM) | N/A |
| [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417) | Ling Feng, Lei Hou, juanli, linny2002, NeoZ123 | - This paper introduces AdaptThink, a novel reinforcement learning algorithm that enables reasoning models to adaptively choose between "Thinking" (lengthy reasoning process) and "NoThinking" (direct answer generation) modes based on problem difficulty. - AdaptThink consists of a constrained optimization objective that encourages NoThinking while maintaining performance and an importance sampling strategy to balance both modes during training. - Experimental results on three math datasets (GSM8K, MATH500, and AIME2024) show that AdaptThink significantly reduces average response length (by 53%) while improving accuracy (by 2.4%) compared to the baseline model. - The method also demonstrates effectiveness on out-of-distribution datasets, further highlighting its adaptability and efficiency. - AdaptThink offers a promising approach to optimize the balance between reasoning quality and efficiency by leveraging adaptive thinking-mode selection. | ['Question Answering'] | [Link](https://github.com/THU-KEG/AdaptThink) | N/A |
| [AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via
  Reinforcement Learning](https://arxiv.org/abs/2505.11896) | Shuangzhi, qingping95, Swtheking, sunzewei2715, louchenwei | AdaCoT is a novel framework that allows LLMs to adaptively decide when to use Chain-of-Thought (CoT) prompting, addressing the inefficiency of indiscriminately using CoT for all queries.  It formulates this as a Pareto optimization problem, balancing model performance and CoT costs using reinforcement learning.  AdaCoT achieves significant reductions in CoT usage (as low as 3.18%) and tokens (69.06%), while maintaining high performance on complex tasks.  A key technical contribution is Selective Loss Masking (SLM), which prevents decision boundary collapse during training.  Experimental results show AdaCoT successfully navigates the Pareto frontier. | ['Question Answering'] | N/A | N/A |
| [Delta Attention: Fast and Accurate Sparse Attention Inference by Delta
  Correction](https://arxiv.org/abs/2505.11254) | Sung Ju Hwang, gmlwns5176, jeffwillette | - This paper introduces Delta Attention, a novel method to improve the accuracy of sparse attention inference in transformer models. - Delta Attention addresses the performance degradation caused by distributional shifts in sparse attention outputs by applying a simple correction procedure. - Experimental results demonstrate that Delta Attention achieves an average 36 percentage point increase in accuracy across various benchmarks, recovering 88% of full quadratic attention accuracy. - The method maintains approximately 98.5% sparsity, resulting in a 32 times speedup compared to Flash Attention 2 on a 1 million token prefill task. - Delta Attention can be applied to any existing sparse attention method with minimal overhead. | ['Natural Language Processing'] | N/A | N/A |
| [Scaling Computer-Use Grounding via User Interface Decomposition and
  Synthesis](https://arxiv.org/abs/2505.13227) | Mayome, RadioBlue, lixiaochuan2020, MillanK, tianbaoxiexxx |  - This paper introduces OSWORLD-G, a comprehensive benchmark with 564 finely annotated samples for evaluating GUI grounding models, addressing limitations of previous benchmarks.  - It also presents JEDI, a large-scale (4 million examples) computer use grounding dataset synthesized via multi-perspective task decoupling.  - Multi-scale models trained on JEDI outperform existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and OSWORLD-G.  - The authors demonstrate improved grounding with JEDI enhances agentic capabilities of foundation models, improving performance on complex computer tasks.  - Ablation studies identify key factors contributing to grounding performance and show combining specialized data for different interface elements enables compositional generalization. | ['Multimodal'] | [Link](https://osworld-grounding.github.io) | N/A |
| [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379) | wxcTest, horseee, Vinnnf | - This paper introduces Thinkless, a novel framework that enables LLMs to adaptively select between short-form and long-form reasoning based on task complexity and model capabilities. - Thinkless employs two control tokens, <short> and <think>, to direct the LLM's reasoning mode and is trained using a reinforcement learning paradigm with a decoupled optimization algorithm (DeGRPO). - DeGRPO decomposes the learning objective into two components: control token loss and response loss, enhancing training stability and preventing mode collapse observed in vanilla GRPO. - Experiments on several benchmarks show that Thinkless reduces long-chain reasoning usage by 50%-90%, significantly improving efficiency without sacrificing accuracy. - The code for Thinkless is publicly available on GitHub. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/VainF/Thinkless) | N/A |
| [Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient
  in Latent Space](https://arxiv.org/abs/2505.13308) | zlzheng, vickyandkekey, ColorfulAI, xuekai, henry12348 | The paper introduces LATENTSEEK, a novel framework that enhances Large Language Model (LLM) reasoning capabilities through Test-Time Instance-level Adaptation (TTIA) in the latent space.  LATENTSEEK leverages policy gradients to iteratively refine latent representations, guided by self-generated reward signals.  Experimental results on various reasoning benchmarks demonstrate that LATENTSEEK consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning based methods.  The method is highly efficient, converging within a few iterations for problems of average complexity and showing scalability with additional iterations.  The findings suggest LATENTSEEK is a lightweight, scalable, and effective solution for enhancing LLM reasoning. | ['Natural Language Processing'] | [Link](https://github.com/bigai-nlco/LatentSeek) | N/A |
| [MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable
  Step-Level Supervision](https://arxiv.org/abs/2505.13427) | wqshao126, Domingo12, SuperposedWave, FanqingM, Cierra0506 | - This paper introduces MM-PRM, a process reward model designed to enhance multimodal mathematical reasoning. - MM-PRM leverages a three-stage framework: policy model construction, process supervision data generation using Monte Carlo Tree Search (MCTS), and process reward model training. - The model achieves significant improvements across various benchmarks (MM-K12, OlympiadBench, MathVista, etc.), outperforming existing methods by a notable margin. - The effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance is demonstrated. - The researchers release their code and data at https://github.com/ModalMinds/MM-PRM. | ['Multimodal'] | [Link](https://github.com/ModalMinds/MM-PRM) | N/A |
| [FedSVD: Adaptive Orthogonalization for Private Federated Learning with
  LoRA](https://arxiv.org/abs/2505.12805) | Sangwoo Park, hbseong, dwgnr, dongboklee, Seanie-lee | - This paper introduces FedSVD, a novel method for private federated learning with LoRA that addresses noise amplification issues. - FedSVD uses singular value decomposition (SVD) to reparameterize the LoRA update, improving stability and performance under differential privacy (DP-SGD). - The proposed method avoids quadratic noise amplification by optimizing only one matrix (B) while the other matrix (A) is reinitialized using SVD. - FedSVD consistently outperforms baselines under both private and non-private settings across various benchmarks and privacy levels. - Theoretical analysis shows that the orthonormal structure of matrix A improves the condition number of the Hessian, resulting in faster convergence. | ['Natural Language Processing'] | N/A | N/A |
| [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992) | JunnanLi, doyensahoo, yuhuixu, hendrydong, baohao | - This paper introduces Fractured Sampling, a novel inference-time scaling technique that improves the reasoning capabilities of large language models (LLMs) by generating rich intermediate reasoning trajectories while reducing token costs. - Fractured Sampling operates along three orthogonal axes: the number of reasoning trajectories, the number of final solutions per trajectory, and the depth at which reasoning traces are truncated. - Extensive experiments on five reasoning benchmarks show that Fractured Sampling consistently achieves superior accuracy-cost trade-offs compared to traditional methods like full chain-of-thought (CoT) prompting and solution-only sampling. - The analysis reveals how to allocate computation across the three dimensions of Fractured Sampling to maximize performance, leading to more efficient and scalable LLM reasoning. - The authors demonstrate that truncated CoT, which stops reasoning before completion, often matches full CoT sampling in accuracy while using significantly fewer tokens. | ['Natural Language Processing'] | N/A | N/A |
| [VisionReasoner: Unified Visual Perception and Reasoning via
  Reinforcement Learning](https://arxiv.org/abs/2505.12081) | Shu Liu, BoHao0326, zszhong, TainU, Ricky06662 | - VisionReasoner is a novel unified framework for visual perception tasks that leverages reinforcement learning to enhance reasoning capabilities and solve diverse perception tasks within a shared model. - It outperforms existing models like Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting). - The model employs a multi-object cognitive learning strategy and reformulates tasks into three fundamental types: detection, segmentation, and counting. - It incorporates a reasoning module which processes the image and locates targeted objects, and a segmentation module that produces segmentation masks if needed. - The reward mechanism includes format rewards (thinking rewards, answer format rewards, non-repeat rewards) and accuracy rewards (multi-object IoU rewards, L1 rewards for precise localization) to strengthen multi-object cognition. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering', 'Object Detection', 'Image Segmentation', 'Zero-Shot Object Detection'] | [Link](https://github.com/dvlab-research/VisionReasoner) | N/A |
| [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932) | jrwen, wuyongkang, lixiaoxi45, douzc, KeriaZhang | - This paper introduces QCompiler, a neuro-symbolic framework designed to enhance Retrieval-Augmented Generation (RAG) systems' ability to handle complex queries. - QCompiler uses a Backus-Naur Form (BNF) grammar to formalize complex queries, minimizing redundancy and ensuring completeness. - The framework comprises a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs). - Experiments on multiple benchmarks demonstrate that QCompiler significantly improves the accuracy and efficiency of RAG systems in addressing complex queries compared to existing methods. - The atomicity of sub-queries in leaf nodes of the AST ensures precise document retrieval and response generation. | ['Question Answering'] | [Link](https://github.com/YuyaoZhangQAQ/Query_Compiler) | N/A |
| [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082) | Jing Liu, Chaoyi Zhang, Shen Yan, Yiyuan Ma, Yunshui Li | - This paper introduces Pre-trained Model Averaging (PMA), a novel technique for enhancing large language models (LLMs) during pre-training. - PMA merges checkpoints from the stable training phase, resulting in consistent performance improvements across various model sizes and architectures (dense and Mixture-of-Experts). - Experiments demonstrate that PMA achieves comparable or superior performance to traditional annealing methods, offering faster validation cycles and significant computational savings. - The optimal merging interval and number of checkpoints scale with model size, and incorporating more checkpoints generally improves performance. - PMA can also be used to stabilize training by initializing consecutive training (CT) and supervised fine-tuning (SFT) stages with PMA-applied checkpoints, leading to more reliable recovery from unstable trajectories. | ['Natural Language Processing'] | N/A | N/A |
| [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification
  of Scientific Research](https://arxiv.org/abs/2505.11855) | sngwon, Cartinoe5930, HazelNam, JW17, amphora | This paper introduces SPOT, a new benchmark dataset for evaluating the ability of Large Language Models (LLMs) to perform automated verification of scientific research. The dataset consists of 83 published papers paired with 91 manually-annotated errors, validated by authors and human annotators. The authors evaluate the performance of various state-of-the-art LLMs on SPOT, finding that none achieve satisfactory results, highlighting a significant gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.  The benchmark is multi-modal, containing both text and image data. The authors also present case studies and further analysis on the impact of context length and multi-modality on model performance. | ['Multimodal'] | [Link](https://github.com/guijinSON/ai4s_r2.git) | [Link](https://huggingface.co/datasets/amphora/SPOT-MetaData) |
| [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484) | Chunyan Miao, Xu Guo, Aver3, xuyige | - SoftCoT++, a novel framework, extends SoftCoT for test-time scaling in the continuous latent space of the chain-of-thought reasoning process. - It introduces multiple specialized initial tokens and contrastive learning to generate diverse soft thought representations. - Experiments across five reasoning benchmarks and two LLMs demonstrate that SoftCoT++ significantly outperforms SoftCoT and other baselines. - SoftCoT++'s performance is consistent across different architectures and tasks, highlighting its versatility. - The method's effectiveness is shown by its ability to amplify the scaling effect when combined with self-consistency, indicating the orthogonality of scaling the thinking and reasoning stages. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/xuyige/SoftCoT) | N/A |
| [Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset
  Generation & Smoke-Tests for Continuous LLM Evaluation](https://arxiv.org/abs/2505.12058) | vincentkoc | - This paper introduces Tiny QA Benchmark++ (TQB++), an ultra-lightweight evaluation suite designed for rapid LLM evaluation. - TQB++ consists of a small, hand-crafted English QA dataset and a Python-based synthetic data generation toolkit which creates multilingual datasets for various languages. - The toolkit uses LiteLLM to allow users to generate datasets using different LLMs. - The authors demonstrate how TQB++ helps in exposing critical failures in LLMs and detecting regressions in LLMOps workflows by using various models and datasets. - The dataset, generator script, and related tools are open-sourced and hosted on the Hugging Face Hub and GitHub. | ['Question Answering'] | [Link](https://github.com/vincentkoc/tiny_qa_benchmark_pp) | [Link](https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp) |
| [HelpSteer3-Preference: Open Human-Annotated Preference Data across
  Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475) | Felipe Soares, Hoo-Chang Shin, Olivier Delalleau, Jiaqi Zeng, Zhilin Wang | This paper introduces HelpSteer3-Preference, a high-quality human-annotated preference dataset with over 40,000 samples covering diverse real-world LLM applications.  It improves upon existing datasets by offering enhanced quality and diversity, encompassing STEM, coding, and multilingual scenarios. Reward models trained on this dataset achieve state-of-the-art performance on RM-Bench (82.4%) and JudgeBench (73.7%). The dataset is available under a permissive CC-BY-4.0 license.  The authors further demonstrate the dataset's applicability to training generative reward models and aligning policy models with RLHF. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/nvidia/HelpSteer3#preference) |
| [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973) | Hamid R. Rabiee, Zahra Dehghanian, Mahta Fetrat Qharabagh | This paper introduces HomoRich, a new large-scale Persian homograph dataset created using a semi-automated pipeline that leverages LLMs.  The authors propose two novel G2P tools: Homo-GE2PE, a fine-tuned neural model, and HomoFast eSpeak, an enhanced rule-based system.  Experiments show Homo-GE2PE improves homograph disambiguation accuracy by 29.72% and HomoFast eSpeak shows a 30.66% improvement.  The dataset and code are publicly available. | ['Text-to-Speech'] | [Link](https://github.com/MahtaFetrat/Homo-GE2PE-Persian), [Link](https://github.com/MahtaFetrat/HomoFast-eSpeak-Persian), [Link](https://github.com/MahtaFetrat/Persian-G2P-Tools-Benchmark) | [Link](https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian) |
| [LLM Context Conditioning and PWP Prompting for Multimodal Validation of
  Chemical Formulas](https://arxiv.org/abs/2505.12257) | PChemGuy | - This paper explores the use of Large Language Models (LLMs) for multimodal validation of chemical formulas in scientific documents. - It introduces a novel methodology that combines structured LLM context conditioning with Persistent Workflow Prompting (PWP) principles to improve the reliability of LLMs for such tasks. - The proposed approach is tested on a complex test paper with known textual and image-based errors, demonstrating improved accuracy in identifying errors compared to simpler prompting techniques. - Notably, the method enables the LLM to repeatedly identify a subtle image-based error overlooked during manual review. - The findings highlight the potential of PWP-informed context conditioning as a promising technique for developing more robust LLM-driven analytical workflows for scientific and technical documents. | ['Multimodal'] | N/A | N/A |
| [TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique
  Annotation in Cyber Threat Intelligence Text](https://arxiv.org/abs/2505.11988) | mparvez, TahaSencar, utsavshukla, lekssays | - This paper introduces TECHNIQUERAG, a novel retrieval-augmented generation (RAG) framework for annotating adversarial techniques in cyber threat intelligence text. - TECHNIQUERAG mitigates data scarcity by fine-tuning only the generation component on limited in-domain examples, unlike resource-intensive methods. - It enhances retrieval quality and domain specificity through zero-shot LLM re-ranking, which aligns retrieved candidates with adversarial techniques. - Experiments on multiple security benchmarks show that TECHNIQUERAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data. - Comprehensive analysis further reveals insights into the framework's strengths and limitations. | ['Text Classification', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/qcri/TechniqueRAG) | N/A |
| [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,
  Meta-Prompting, and Meta-Reasoning](https://arxiv.org/abs/2505.03332) | PChemGuy |  - This paper introduces Persistent Workflow Prompting (PWP), a novel prompt engineering methodology for guiding LLMs through complex analytical tasks such as scholarly peer review.  - PWP employs a hierarchical prompt structure to systematically codify expert workflows and mitigate LLM input bias.  - The authors demonstrate PWP's effectiveness through a proof-of-concept application to experimental chemistry manuscript reviews, showing that LLMs guided by PWP can reliably identify major methodological flaws.  - A key advantage of PWP is its ability to guide complex reasoning through standard LLM chat interfaces without requiring specialized APIs or coding.  - The paper further explores meta-prompting and meta-reasoning techniques for refining and formalizing complex prompts and workflows. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-05-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388) | huybery, BeichenZhang, Baosong, laf070810, yangapku |  - This paper introduces Qwen3, a series of large language models (LLMs) with parameter scales ranging from 0.6B to 235B, designed to improve performance, efficiency, and multilingual capabilities.   - Qwen3 integrates a thinking mode and a non-thinking mode within a unified framework, enabling dynamic mode switching based on user queries.   - The models achieve state-of-the-art results across diverse benchmarks, outperforming previous models and some leading open-source models. - Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility.  -  All Qwen3 models are publicly accessible under Apache 2.0 to promote research and development. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/QwenLM/Qwen3) | [Link](https://huggingface.co/Qwen) |
| [MMLongBench: Benchmarking Long-Context Vision-Language Models
  Effectively and Thoroughly](https://arxiv.org/abs/2505.10610) | Yu Zhao, Jipeng Zhang, Xiyu Ren, Wenhao Yu, Zhaowei Wang |  - This paper introduces MMLONGBENCH, a benchmark for evaluating long-context vision-language models (LCVLMs). - MMLONGBENCH includes 13,331 examples across five diverse downstream tasks and various image types to thoroughly evaluate LCVLMs.  - The benchmark uses a cross-modal tokenization scheme that combines vision patches and text tokens, delivering examples at five standardized input lengths (8K-128K tokens). - Through benchmarking 46 LCVLMs, the authors found that single-task performance is a weak proxy for overall LCVLM capability, closed-source and open-source models face challenges in long-context vision-language tasks, and models with strong reasoning ability tend to perform better. - MMLONGBENCH provides a foundation for diagnosing and improving LCVLMs. | ['Multimodal'] | [Link](https://github.com/EdinburghNLP/MMLongBench) | N/A |
| [GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049) | Tri Cao, Yulin Chen, Mingzhe Du, Shengfang Zhai, Yue Liu | - This paper introduces GuardReasoner-VL, a novel reasoning-based VLM guard model that uses online reinforcement learning (RL) to encourage deliberative reasoning before making moderation decisions. - GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, is constructed to enhance the model's reasoning capabilities. - The model is cold-started using supervised fine-tuning (SFT) and further improved through online RL, which incorporates rejection sampling, safety-aware data concatenation, a dynamic clipping parameter, and a length-aware safety reward. - Extensive experiments show that GuardReasoner-VL surpasses the runner-up by 19.27% F1 score on average across multiple multimodal guardrail benchmarks. - The data, code, and model weights (3B/7B) are publicly available. | ['Multimodal'] | [Link](https://github.com/yueliu1999/GuardReasoner-VL/) | N/A |
| [Simple Semi-supervised Knowledge Distillation from Vision-Language
  Models via texttt{D}ual-texttt{H}ead
  texttt{O}ptimization](https://arxiv.org/abs/2505.07675) | Sung Ju Hwang, Hyungjoon Jang, Seongjae Kang, dongboklee |  * This paper introduces Dual-Head Optimization (DHO), a novel semi-supervised knowledge distillation framework for vision-language models (VLMs).  * DHO uses two prediction heads which learn independently from labeled data and teacher predictions, mitigating gradient conflicts between supervised and distillation signals.  * Extensive experiments on ImageNet and eleven other datasets demonstrate that DHO consistently improves accuracy across multiple domains and fine-grained datasets.  * The findings show that DHO achieves state-of-the-art performance on ImageNet, improving accuracy by up to 3% with only 1% labeled data and 0.1% with 10% labeled data.  * The framework's simplicity and effectiveness make it a promising approach for deploying large-scale VLMs in resource-constrained environments. | ['Image Classification', 'Image Feature Extraction', 'Zero-Shot Image Classification', 'Multimodal'] | N/A | N/A |
| [Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token
  Level Granularity](https://arxiv.org/abs/2505.11107) | Yi-Chang Chen, Feng-Ting Liao, Jamie McGowan, Davide Buffelli, Splend1dchan | - The paper introduces GroupThink, a novel approach for collaborative reasoning in LLMs that enables multiple reasoning agents to collaborate concurrently at the token level.  - Unlike traditional turn-based methods, GroupThink allows agents to adapt dynamically to one another's progress mid-sentence, reducing redundancy and improving reasoning quality. - The proposed method is shown to significantly reduce latency compared to existing methods for various tasks (enumeration, divide-and-conquer, programming) without sacrificing accuracy. - GroupThink is implemented as a simple modification to any existing LLM, making it easily deployable on resource-constrained devices such as edge devices. - Experimental results demonstrate that GroupThink achieves significant latency improvements on multiple benchmarks, even when using LLMs not specifically trained for this collaborative paradigm. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Mergenetic: a Simple Evolutionary Model Merging Library](https://arxiv.org/abs/2505.11427) | erodola, crisostomi, teelinsan, tmencatt, adrianrob | - Mergenetic, an open-source library for evolutionary model merging, is introduced to facilitate flexible experimentation with evolutionary algorithms and merging methods in language models. - It offers comprehensive support for various merging methods and evolutionary algorithms, including lightweight fitness estimators to reduce evaluation costs. - Mergenetic demonstrates competitive results across tasks and languages using modest hardware, showcasing its efficiency and accessibility. - The library provides a Python API, CLI, and GUI for easy usage and customization, catering to both power users and those with limited coding experience. -  Future work will focus on extending Mergenetic's capabilities to handle extremely low-resource languages or domains where fine-tuned models may be unavailable. | ['Natural Language Processing'] | [Link](https://github.com/tommasomncttn/mergenetic) | N/A |
| [MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective
  Search and Data Curation](https://arxiv.org/abs/2505.10962) | Tao Yang, Yang Li, haitaominlp, freesunshine0316, invokerliang | - This paper introduces MPS-Prover, a novel stepwise automated theorem proving system that utilizes a multi-perspective search strategy and post-training data curation. - MPS-Prover incorporates a learned critic model with strategically designed heuristic rules to diversify tactic selection, enhancing search robustness and preventing unproductive states. - The post-training data curation strategy effectively eliminates redundant training data, improving model performance without sacrificing efficiency. - Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, outperforming existing 7B parameter models. - MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. | ['Natural Language Processing'] | N/A | N/A |
| [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518) | Nikos Komodakis, Spyros Gidaris, nasos10 | - MuToR, a novel multi-token prediction method, is introduced, which interleaves learnable register tokens into the input sequence to predict future tokens. - Compared to existing methods, MuToR introduces minimal additional parameters, requires no architectural changes, and is well-suited for supervised fine-tuning and parameter-efficient fine-tuning (PEFT). - The effectiveness and versatility of MuToR are demonstrated across a range of use cases, including supervised fine-tuning, PEFT, and pre-training, on challenging generative tasks in both language and vision domains. - MuToR consistently outperforms baselines (Next-Token and Multi-Token) in mathematical reasoning and summarization tasks, achieving superior results with negligible parameter overhead. - The 2D extension of MuToR adapts seamlessly to autoregressive image generation, showcasing its broader applicability and potential across diverse domains and training settings. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/nasosger/MuToR) | N/A |
| [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140) | rubis, bjerva, jjzha | - This paper introduces fs1, a new dataset containing 6K knowledge-graph enhanced reasoning traces designed to improve the factual accuracy of LLMs in open-domain question answering. - The authors fine-tune six variants of the Qwen2.5 architecture on both original and KG-enhanced reasoning traces and evaluate them across six benchmarks, encompassing over 22.6K questions. - Their findings indicate that smaller instruction-tuned models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts when using KG-enhanced reasoning traces. - Furthermore, test-time scaling consistently improves factual accuracy by 2-8%, demonstrating that longer reasoning chains enhance factual accuracy in open-domain QA. - All experimental artifacts, including reasoning traces and models, are publicly available for further research. | ['Question Answering'] | [Link](https://github.com/jjzha/fs1) | [Link](https://huggingface.co/jjzha/fs1) |
| [MatTools: Benchmarking Large Language Models for Materials Science Tools](https://arxiv.org/abs/2505.10852) | David J. Srolovitz, Bo Hu, Beilin Ye, Jiamin Xu, SiyuLiu | This paper introduces MatTools, a novel benchmark designed to evaluate the proficiency of Large Language Models (LLMs) in handling materials science tasks. MatTools comprises two key components: a QA benchmark and a real-world tool-usage benchmark. The QA benchmark assesses an LLM's understanding of materials science tools using 69,225 question-answer pairs from the pymatgen codebase.  The real-world benchmark challenges LLMs to generate functional Python code for materials property calculations, comprising 49 tasks with 138 subtasks. Experiments revealed that general-purpose LLMs significantly outperformed specialized models, with LLM-generated documentation proving superior as a retrieval source in Retrieval-Augmented Generation (RAG) systems.  A self-reflection LLM-doc RAG approach demonstrated state-of-the-art performance in real-world tool-usage tasks. | ['Document Question Answering', 'Question Answering'] | [Link](https://github.com/Grenzlinie/MatTools) | N/A |


## Papers for 2025-05-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large
  Reasoning Models](https://arxiv.org/abs/2505.10554) | cxiong, amritasaha87, yuhuixu, hendrydong, zhiyuanhucs | - This paper introduces a novel approach to enhance the reasoning capabilities of large reasoning models (LRMs) by explicitly aligning them with three meta-abilities: deduction, induction, and abduction. -  The proposed method uses automatically generated, self-verifiable tasks to train the models on each meta-ability individually, followed by merging the parameter spaces of the trained models and further fine-tuning with domain-specific reinforcement learning. - Experiments demonstrate that this three-stage pipeline (individual alignment, parameter-space merging, and domain-specific reinforcement learning) improves performance by over 10% compared to instruction-tuned baselines. - The study also shows that resuming domain-specific reinforcement learning from the aligned checkpoint yields an additional 2% average gain in performance across various benchmarks, highlighting the scalability and dependability of the proposed method. - The authors' code is publicly available on GitHub, making their approach reproducible and facilitating further research in the field. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/zhiyuanhubj/Meta-Ability-Alignment) | N/A |
| [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666) | Sung Ju Hwang, jinheon, YuminChoi | This paper introduces a novel bilevel system prompt optimization problem for Large Language Models (LLMs).  The proposed MetaSPO framework meta-learns a robust system prompt that generalizes effectively to unseen tasks and diverse user prompts. Experiments on fourteen datasets across five domains demonstrate MetaSPO's superior performance compared to baseline methods in both unseen generalization and test-time adaptation scenarios. The optimized system prompts enable rapid adaptation to unseen tasks, requiring fewer optimization steps while achieving improved performance.  MetaSPO's iterative optimization process ensures synergy between system and user prompts.  The framework is flexible and compatible with various prompt optimization techniques. | ['Natural Language Processing'] | [Link](https://github.com/Dozi01/MetaSPO) | N/A |
| [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a
  Reasoning Model will Think](https://arxiv.org/abs/2505.10185) | hbin0701, dreamgonfly, Minju2136, seungone, Seongyun | The CoT Encyclopedia is a novel bottom-up framework for analyzing and controlling long chain-of-thought (CoT) reasoning in large language models.  It automatically extracts diverse reasoning criteria from model-generated CoTs and clusters them into representative categories. Human evaluations demonstrate that the framework is more interpretable than existing methods and enables performance gains.  The study reveals that training data format has a greater impact on reasoning behavior than data domain. Finally, it provides insights into the controllability of reasoning strategies via model merging. | ['Natural Language Processing'] | N/A | N/A |
| [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562) | RobertLuo1, Paranioar, YufengCui, ryanzhangfan, gilnore | - This paper introduces ETT, an end-to-end vision tokenizer tuning approach that jointly optimizes vision tokenization and target autoregressive tasks.  Unlike previous methods that use only discrete indices from a frozen vision tokenizer, ETT leverages visual embeddings and optimizes the vision tokenizer end-to-end.  - ETT significantly improves performance on multimodal understanding and visual generation tasks (2-6% gains compared to baselines) while maintaining original reconstruction capabilities.  - The method is simple to implement and integrate without requiring adjustments to the original codebooks or architectures of large language models.  - Extensive experiments demonstrate the effectiveness of ETT across various benchmarks, showcasing consistent performance improvements over existing state-of-the-art approaches.  - This method empowers multimodal foundation models beyond image generation and understanding. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine
  Learning Engineering](https://arxiv.org/abs/2505.07782) | percyliang, Solute, yinghaoli-yh, yczhuang, Jerrycool | This paper introduces MLE-Dojo, a new benchmark and environment designed to evaluate and improve large language models (LLMs) for machine learning engineering (MLE) tasks.  The benchmark features 200+ real-world Kaggle challenges covering diverse MLE tasks.  MLE-Dojo provides an interactive environment, enabling agents to experiment and refine solutions iteratively.  Extensive evaluations across eight frontier LLMs demonstrate that current models show limitations, but iterative improvements are meaningful.  The framework's modular and extensible design promotes reproducibility and fosters community-driven innovation. | ['Reinforcement Learning', 'Tabular', 'Tabular Classification', 'Tabular Regression', 'Time Series Forecasting', 'Natural Language Processing', 'Text Classification', 'Computer Vision', 'Image Classification', 'Image Segmentation'] | [Link](https://github.com/MLE-Dojo/MLE-Dojo) | [Link](https://huggingface.co/spaces/MLE-Dojo/Leaderboard) |
| [Achieving Tokenizer Flexibility in Language Models through Heuristic
  Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738) | Vinayak Pahalwan, Shaurya Sharthak, adarshxs, adi-kmt | - This paper introduces TokenAdapt, a novel framework for achieving tokenizer flexibility in large language models (LLMs). - TokenAdapt employs a hybrid heuristic initialization strategy that combines local and global estimates to effectively transplant tokenizers. - The framework also introduces supertoken learning to improve compression and reduce fragmentation. - Empirical results demonstrate that TokenAdapt consistently outperforms existing methods such as ReTok and TransTokenizer, achieving up to a 2-fold improvement in perplexity. - The authors conclude that TokenAdapt offers a practical and computationally efficient method for adapting LLMs to new tokenization schemes. | ['Natural Language Processing'] | [Link](None) | [Link](None) |
| [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320) | Xian Li, Ping Yu, Tianlu Wang, Chenxi Whitehouse, swarna92 | - This paper introduces J1, a novel reinforcement learning approach for training LLMs to perform better judgment tasks. - J1 converts both verifiable and non-verifiable prompts into judgment tasks with verifiable rewards that incentivize stronger chain-of-thought reasoning. - The proposed method outperforms existing 8B and 70B LLMs, including models distilled from DeepSeek-R1, on various benchmarks. - J1's improved judgment ability stems from its learning to outline evaluation criteria, comparing against self-generated answers, and re-evaluating responses. - The authors provide comprehensive analysis and ablations on the model's performance, including comparisons between Pairwise-J1 and Pointwise-J1 models, offline vs. online training, reward strategies, and variations in thought content. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [PointArena: Probing Multimodal Grounding Through Language-Guided
  Pointing](https://arxiv.org/abs/2505.09990) | Boyang Li, Haoquan Fang, Yi Ru Wang, Jiafei Duan, Long Cheng | - PointArena is a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios, comprising Point-Bench (a curated dataset), Point-Battle (an interactive arena for model comparison), and Point-Act (a real-world robotic manipulation system). - PointArena's Point-Bench dataset contains approximately 1,000 pointing tasks across five reasoning categories, enabling a more thorough evaluation of multimodal pointing capabilities compared to existing benchmarks that focus primarily on object localization. - Extensive evaluations on PointArena demonstrate that Molmo-72B consistently outperforms other models, although proprietary models are increasingly demonstrating comparable performance, and that supervised training specifically targeting pointing tasks significantly enhances model performance. - PointArena's multi-stage evaluation pipeline reveals strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. - The platform facilitates blind, pairwise model comparisons through user voting in Point-Battle and real-world robotic manipulation evaluation in Point-Act. | ['Multimodal', 'Robotics'] | [Link](https://pointarena.github.io) | N/A |
| [OpenThinkIMG: Learning to Think with Images via Visual Tool
  Reinforcement Learning](https://arxiv.org/abs/2505.08617) | Zhengyuan Yang, Yunzhuo Hao, Mingyang Song, Linjie Li, Zhaochen Su | - This paper introduces OPENTHINKIMG, an open-source framework for tool-augmented large vision-language models (LVLMs). - It proposes a novel reinforcement learning framework, V-TOOLRL, to enable LVLMs to learn adaptive policies for invoking external vision tools. - V-TOOLRL significantly outperforms its supervised fine-tuning counterpart and surpasses established supervised tool-learning baselines on chart reasoning tasks. - The framework features standardized vision tool interfaces, scalable trajectory generation, and a flexible training environment to address the challenges of integrating diverse tools and training robust agents. - OPENTHINKIMG aims to advance dynamic, tool-augmented visual reasoning and provides a foundational framework for developing AI agents that can genuinely "think with images". | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/zhaochen0110/OpenThinkIMG) | N/A |
| [ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible
  Long-term Tracking](https://arxiv.org/abs/2505.08581) | Guanyi Qin, Ziyue Wang, Xuxiao Luo, Mingqi Gao, HeverLaw | - This paper introduces ReSurgSAM2, a two-stage framework for referring surgical segmentation in videos that uses Segment Anything Model 2 (SAM2). - ReSurgSAM2 first performs text-referred target detection using a Cross-Modal Spatial-Temporal Mamba and then performs tracking with a Diversity-Driven Long-term Memory. - The model achieves real-time performance at 61.2 FPS and outperforms existing methods on benchmark datasets. - The proposed method addresses the limitations of existing methods by improving efficiency and long-term tracking capabilities. - ReSurgSAM2 is evaluated on Ref-EndoVis17 and Ref-EndoVis18 datasets, demonstrating significant performance improvements over other state-of-the-art methods. | ['Video-Text-to-Text', 'Image Segmentation', 'Multimodal', 'Robotics'] | [Link](https://github.com/jinlab-imvr/ReSurgSAM2) | N/A |
| [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475) | Dayiheng Liu, Jiaxi Yang, Zeyu Cui, Binyuan Hui, Mouxiang Chen |  - This paper introduces a novel scaling paradigm for language models called Parallel Scaling (PARSCALE), which increases the model's parallel computation during training and inference.  - PARSCALE applies diverse and learnable transformations to the input, executes forward passes in parallel, and aggregates the outputs dynamically, improving inference efficiency without significantly increasing memory or latency.  - The authors propose a new scaling law showing that PARSCALE with P parallel streams is similar to scaling parameters by O(log P), offering superior inference efficiency compared to parameter scaling that achieves the same performance improvement.  - Experiments on large-scale pre-training and various downstream tasks validate the proposed scaling law and demonstrate PARSCALE's effectiveness.  - A two-stage training strategy is presented to reduce the training cost of PARSCALE and the method is demonstrated on an off-the-shelf pre-trained model, showing its versatility and applicability. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/QwenLM/ParScale) | [Link](https://huggingface.co/ParScale) |


## Papers for 2025-05-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,
  Training and Dataset](https://arxiv.org/abs/2505.09568) | Zhiyang Xu, Jiuhai Chen, xurantju, zhoutianyi, xcpan | - This paper introduces BLIP3-0, a family of fully open unified multimodal models that excel in both image understanding and generation tasks. - The model architecture employs a diffusion transformer to generate semantically rich CLIP image features, enhancing training efficiency and improving generation quality. - BLIP3-0 utilizes a sequential pretraining strategy, initially focusing on image understanding and subsequently on image generation, effectively preserving image understanding capabilities while developing robust image generation abilities. - A high-quality instruction-tuning dataset, BLIP3-0-60k, is introduced, which is carefully curated using GPT-4 to enhance model alignment with human preferences. - BLIP3-0 outperforms existing state-of-the-art unified multimodal models across various benchmark tasks, demonstrating its superior performance in both image understanding and generation. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://github.com/JiuhaiChen/BLIP30) | [Link](https://huggingface.co/BLIP30/BLIP30-Model), [Link](https://huggingface.co/datasets/BLIP30/BLIP30-Pretrain), [Link](https://huggingface.co/datasets/BLIP30/BLIP30-60k) |
| [Insights into DeepSeek-V3: Scaling Challenges and Reflections on
  Hardware for AI Architectures](https://arxiv.org/abs/2505.09343) | Huazuo Gao, Damai Dai, Chong Ruan, Chengqi Deng, Chenggang Zhao | - The paper presents DeepSeek-V3, a large language model (LLM) trained on 2048 NVIDIA H800 GPUs, showcasing cost-efficient training and inference at scale through hardware-aware model co-design. - Key architectural innovations include Multi-head Latent Attention (MLA) for memory efficiency, Mixture of Experts (MoE) for optimized computation-communication trade-offs, and FP8 mixed-precision training to leverage hardware capabilities. - DeepSeek-V3 achieves state-of-the-art performance while using significantly fewer computational resources than comparable dense models, demonstrating the efficacy of MoE architectures in cost-effective training. - The paper analyzes hardware bottlenecks encountered during DeepSeek-V3's development and proposes potential future hardware directions, such as precise low-precision computation units and innovations in low-latency communication fabrics. - This work offers a practical blueprint for innovation in next-generation AI systems by highlighting the critical role of hardware and model co-design in meeting the escalating demands of AI workloads. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/deepseek-ai/3FS), [Link](https://github.com/deepseek-ai/dualpipe), [Link](https://github.com/deepseek-ai/DeepGEMM), [Link](https://github.com/deepseek-ai/DeepEP), [Link](https://github.com/deepseek-ai/profile-data) | N/A |
| [SweRank: Software Issue Localization with Code Ranking](https://arxiv.org/abs/2505.07849) | Xuan Phi Nguyen, Ye Liu, JaeHyeok Doo, Tarun Suresh, Revanth Gangi Reddy | - This paper introduces SWERANK, a novel two-stage retrieve-and-rerank framework for software issue localization that significantly outperforms existing agent-based and code-ranking methods. - SWERANK consists of a bi-encoder embedding model (SWERANKEMBED) for retrieval and an instruction-tuned LLM (SWERANKLLM) for reranking. - The model achieves state-of-the-art performance on SWE-Bench-Lite and LocBench datasets, demonstrating its effectiveness in localizing code relevant to issue descriptions. - To train SWERANK, the authors created SWELOC, a new large-scale dataset curated from public GitHub repositories, which contains real-world issue descriptions paired with the corresponding code modifications. - SWERANK shows a considerably better performance-to-cost ratio compared to agent-based approaches that rely on closed-source LLMs. | ['Natural Language Processing'] | [Link](https://gangiswag.github.io/swerank) | N/A |
| [VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large
  Video Language Models](https://arxiv.org/abs/2505.08455) | Ali Etemad, pritamqu | - This paper introduces VCRBench, a novel benchmark designed to evaluate the long-form causal reasoning capabilities of Large Video Language Models (LVLMs). - VCRBench uses procedural videos of simple everyday activities, where the steps are deliberately shuffled, to test if LVLMs can correctly sequence events to achieve a specific goal. - The evaluation of state-of-the-art LVLMs on VCRBench shows that these models struggle with video-based long-form causal reasoning due to difficulty in modeling long-range dependencies. - To improve LVLMs' performance, the authors propose a Recognition-Reasoning Decomposition (RRD) approach that separates video recognition and causal reasoning, enhancing accuracy by up to 25.2%. - The thorough analysis of the results reveals interesting insights into the reasoning capabilities of LVLMs, indicating that they primarily rely on language knowledge when tackling complex video-based long-form causal reasoning tasks. | ['Video Classification', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](https://arxiv.org/abs/2505.09439) | Hilde Kuehne, Samuel Thomas, Edson Araujo, Saurabhchand Bhati, h9LtLSb | - The paper introduces Omni-R1, a new state-of-the-art model for audio question answering, which fine-tunes the Qwen2.5-Omni multi-modal LLM using the reinforcement learning method GRPO. - Omni-R1 achieves the highest accuracy on the MMAU benchmark across various categories, outperforming existing methods. - The authors propose a method for automatically generating audio question answering datasets, further improving the model's performance. - A key finding is that text-only fine-tuning can significantly improve audio-based performance, suggesting that enhanced text reasoning contributes substantially to the model's success. - The code, models, and datasets will be publicly released. | ['Audio Classification', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |


## Papers for 2025-05-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable
  Speaker Encoder](https://arxiv.org/abs/2505.07916) | Congchao Guo, Bowen Zhang, ymzhang0519, mqyang1s, JunjieYan | - This paper introduces MiniMax-Speech, a novel autoregressive Transformer-based Text-to-Speech (TTS) model featuring a learnable speaker encoder.  - The model architecture includes a tokenizer, autoregressive Transformer, and a latent flow matching model with Flow-VAE.  - MiniMax-Speech achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and secured top position on the public TTS Arena leaderboard.  - The model supports zero-shot and one-shot voice cloning and demonstrates strong performance across 32 languages.  - Ablation studies show the effectiveness of the learnable speaker encoder and Flow-VAE in improving audio quality and speaker similarity. | ['Text-to-Speech'] | [Link](https://minimax-ai.github.io/tts_tech_report) | [Link](https://huggingface.co/datasets/MiniMaxAI/TTS-Multilingual-Test-Set) |
| [A Multi-Dimensional Constraint Framework for Evaluating and Improving
  Instruction Following in Large Language Models](https://arxiv.org/abs/2505.07591) | xjhuang, sean-xl-y, wuyilong, avonfwj, Junjie-Ye | This paper introduces a novel multi-dimensional constraint framework for evaluating and enhancing instruction following in large language models.  The framework is utilized to construct a benchmark dataset of 1200 instruction-following test samples with code-verifiable constraints.  Experiments on 19 LLMs across seven model families reveal substantial performance variations across diverse constraint forms.  Finally, the authors demonstrate that reinforcement learning utilizing their generated dataset significantly improves instruction following, primarily by modifying attention module parameters. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Junjie-Ye/MulDimIF) | N/A |
| [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness
  Prediction via Human-AI Collaborative Annotation](https://arxiv.org/abs/2505.07416) | Kiet Van Nguyen, Dat Minh Nguyen, sonlam1102, trucnguyen28 | - This paper introduces ViMRHP, a new Vietnamese benchmark dataset for multimodal review helpfulness prediction (MRHP). - ViMRHP contains 46K reviews across four domains, with AI assistance significantly reducing annotation time and cost. - A human-AI collaborative annotation framework was used, resulting in a high-quality dataset. - Baseline models were evaluated on both AI-generated and human-verified annotations; human verification improved performance. - The dataset is publicly available and contributes to the advancement of MRHP research in low-resource languages. | ['Multimodal'] | [Link](https://github.com/trng28/ViMRHP) | N/A |


## Papers for 2025-05-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062) | kuma-zhao, yuanlp, 0nejiawei, chb1997, anyuzx | This paper introduces Seed1.5-VL, a vision-language multimodal model with a 532M parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) LLM.  Seed1.5-VL achieves state-of-the-art performance on 38 out of 60 public benchmarks. The model demonstrates strong reasoning abilities in various tasks like GUI control and visual puzzles, surpassing existing multimodal models.  Its development involved novel data synthesis strategies and hybrid parallelism training techniques.  The model is now accessible through Volcano Engine. | ['Multimodal'] | N/A | N/A |
| [MiMo: Unlocking the Reasoning Potential of Language Model -- From
  Pretraining to Posttraining](https://arxiv.org/abs/2505.07608) | whatseeker, Prestonprom, HugoZHL, dwzhu, xiabingquan | - MiMo-7B is a large language model designed for reasoning tasks, optimized across pre-training and post-training stages. - The model architecture uses a decoder-only Transformer with Grouped-Query Attention, SwiGLU activation, and Rotary Positional Embedding, incorporating Multi-Token Prediction for enhanced performance and speed. - Post-training involves reinforcement learning on a dataset of 130K verifiable math and programming problems, using a test-difficulty-driven code-reward scheme and data resampling. - MiMo-7B-RL outperforms larger 32B models on mathematics, code, and general reasoning benchmarks, surpassing OpenAI 01-mini. - Model checkpoints are available at the provided GitHub URL. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/xiaomimimo/MiMo) | [Link](null) |
| [Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured
  3D Assets](https://arxiv.org/abs/2505.07747) | PanJianxiong, flybirdtian, shian7, wchengad, xuanyangz | - This paper introduces Step1X-3D, an open-source framework for high-fidelity and controllable textured 3D asset generation. - Step1X-3D employs a two-stage architecture: a hybrid VAE-DiT geometry generator and a diffusion-based texture synthesis module.  The geometry generator uses a perceiver-based latent encoding with sharp edge sampling, while the texture module ensures cross-view consistency through geometric conditioning and latent-space synchronization. - The framework bridges 2D and 3D generation paradigms, allowing for the direct transfer of 2D control techniques (e.g., LoRA) to 3D synthesis. - Benchmark results show that Step1X-3D outperforms existing open-source methods and achieves competitive quality with proprietary solutions. - Step1X-3D's open-source release includes models, training code, and adaptation modules, promoting reproducibility and further research in 3D asset generation. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://github.com/stepfun-ai/Step1X-3D) | [Link](https://huggingface.co/docs/diffusers/index) |
| [Learning from Peers in Reasoning Models](https://arxiv.org/abs/2505.07787) | Benyou, tangzhy, Jiaxi0775, wydu, Zeno-Luo | This paper introduces LeaP, a novel approach to enhance the reasoning capabilities of Large Reasoning Models (LRMs). LeaP addresses the "Prefix Dominance Trap," where LRMs struggle to recover from poor initial reasoning paths, by enabling cross-path interaction during parallel inference.  Experimental results across several benchmarks demonstrate substantial improvements, with QwQ-32B with LeaP achieving nearly 5 absolute points higher than the baseline.  A fine-tuned version, LeaP-T, further enhances the performance of smaller models.  LeaP's robust error correction and strong handling of varied task difficulty showcase the effectiveness of collaboration during reasoning. | ['Question Answering'] | [Link](https://learning-from-peers.github.io/) | N/A |
| [REFINE-AF: A Task-Agnostic Framework to Align Language Models via
  Self-Generated Instructions using Reinforcement Learning from Automated
  Feedback](https://arxiv.org/abs/2505.06548) | Pawan Goyal, Somak Aditya, Aniruddha Roy, abhi1nandy2, Pretam | - This paper introduces REFINE-AF, a task-agnostic framework for aligning language models using self-generated instructions and reinforcement learning from automated feedback. - REFINE-AF utilizes small, open-source LLMs (LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B) to generate instructions, reducing the need for expensive and time-consuming human annotation. - The framework incorporates reinforcement learning to improve the quality of generated instruction-input-output triplets. - Experiments demonstrate that REFINE-AF achieves significant improvements (63-66%) across various tasks compared to previous approaches. - A large synthetic dataset of 45K instructions generated by REFINE-AF is released to facilitate further research. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/abs/2505.07818) | appleluo, ChenMnZ, ltzhu, wujie10, xzyhku | - DanceGRPO is a novel unified framework that adapts Group Relative Policy Optimization (GRPO) to visual generation tasks, achieving seamless integration across various generative paradigms and tasks. - The framework handles diffusion models and rectified flows, supporting text-to-image, text-to-video, and image-to-video generation tasks, demonstrating consistent and substantial improvements over existing methods. - DanceGRPO exhibits significant performance gains, outperforming baselines by up to 181% on benchmark datasets, showcasing effectiveness and scalability. - This unified approach addresses critical limitations of existing RL-based methods, including incompatibility with ODE-based sampling, instability in large-scale training, and lack of validation for video generation. - The code will be released, fostering further research and development in reinforcement learning for visual synthesis. | ['Reinforcement Learning', 'Text-to-Image', 'Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://dancegrpo.github.io/) | N/A |
| [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong
  Pretraining Data Selection](https://arxiv.org/abs/2505.07293) | Steven Wu, Kai Hua, shenke18, zhangysk |  - AttentionInfluence is proposed as a novel, training-free method for pretraining data selection that leverages the intrinsic attention head mechanisms of LLMs.   -  The approach identifies retrieval heads and computes loss differences when masking these heads, using a small pretrained language model as a data selector.   - The method demonstrates a weak-to-strong scaling property, meaning small models can significantly improve the performance of larger models.   - Experimental results on multiple benchmarks show improvements ranging from 1.4pp to 3.5pp, showcasing AttentionInfluence's effectiveness.   - The study highlights AttentionInfluence's ability to select high-quality, well-distributed data that enhances the reasoning capabilities of downstream models. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus) |
| [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional
  Websites from Scratch](https://arxiv.org/abs/2505.03733) | shiwk20, hht1113, Houxing, Yqy6, luzimu | This paper introduces WebGen-Bench, a new benchmark designed to evaluate Large Language Models' (LLMs) ability to generate functional websites from scratch.  WebGen-Bench includes diverse website generation instructions and corresponding test cases for thorough evaluation.  Experiments using multiple code-agent frameworks showed that the best-performing model achieves only 27.8% accuracy, highlighting the benchmark's difficulty.  A training set, WebGen-Instruct, was also created and used to fine-tune a model which reached 38.2% accuracy, surpassing existing proprietary models. The datasets and code are publicly available. | ['Text Generation', 'Text2Text Generation'] | [Link](https://github.com/mnluzimu/WebGen-Bench) | N/A |
| [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796) | Daniel Dajun Zeng, Lu Wang, Xingjin Wang, linjinglian, Howe77 | *- This paper introduces a novel CPT scaling law that accurately predicts loss at any training step in continual pre-training, considering both distribution shift and learning rate annealing. - The scaling law integrates various factors impacting CPT performance, including loss potential, peak learning rate, training steps, and replay ratio. - Extensive experiments demonstrate the law's accuracy across different datasets, training hyperparameters, and model sizes. - The scaling law helps optimize hyperparameters for diverse CPT goals, such as balancing general and domain-specific performance. - The study provides deeper insights into the learning dynamics of LLMs, facilitating better adaptation to downstream tasks. | ['Natural Language Processing'] | N/A | N/A |
| [Skywork-VL Reward: An Effective Reward Model for Multimodal
  Understanding and Reasoning](https://arxiv.org/abs/2505.07263) | Yi Peng, Wei Shen, Jiangbo Pei, OrlandoHugBot, shawn0wang | - This paper introduces Skywork-VL Reward, a novel multimodal reward model designed for both multimodal understanding and reasoning tasks. - The model architecture is based on Qwen2.5-VL-7B-Instruct, incorporating a reward head and utilizing a multi-stage fine-tuning process with pairwise ranking loss. - Experimental results demonstrate that Skywork-VL Reward achieves state-of-the-art performance on the VL-RewardBench benchmark and shows competitive results on RewardBench. - The preference data generated by Skywork-VL Reward is highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. - The model has been publicly released to ensure transparency and reproducibility. | ['Multimodal'] | N/A | [Link](https://huggingface.co/Skywork/Skywork-VL-Reward-7B) |
| [Reinforced Internal-External Knowledge Synergistic Reasoning for
  Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596) | Kang Liu, Jun Zhao, Yiming Ju, Xiaowei Yuan, hzy | - This paper introduces IKEA, a novel reinforcement learning-based adaptive search agent designed to synergistically integrate internal and external knowledge for efficient question answering. - IKEA utilizes a knowledge-boundary aware reward function and training dataset to incentivize the model to prioritize internal knowledge and only resort to external retrieval when necessary, thereby reducing retrieval redundancy and latency. - The proposed agent demonstrates robust generalization capabilities and significantly outperforms baseline methods across multiple knowledge-intensive reasoning tasks. - Evaluations show that IKEA effectively minimizes unnecessary retrievals while maintaining high accuracy compared to existing search agents. - The authors demonstrate the effectiveness of their approach through comprehensive evaluations on multiple datasets, showcasing improved performance and reduced retrieval frequency. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/hzy312/knowledge-r1) | N/A |
| [Overflow Prevention Enhances Long-Context Recurrent LLMs](https://arxiv.org/abs/2505.07793) | rgiryes, leokarlin, OmegaLittleBob, ItamarZ, assafbk | The paper introduces OPRM, a chunk-based inference strategy for enhancing long-context recurrent LLMs.  OPRM mitigates memory overflow issues by processing only the most relevant portion of the input, improving performance on several long-context tasks.  The method achieves state-of-the-art results on LongBench v2, surpassing comparable Transformer models.  Results are shown across multiple benchmarks and demonstrate consistent improvements in long-context tasks.  The work also challenges the assumption that recurrent models genuinely utilize long-range dependencies. | ['Natural Language Processing'] | [Link](https://github.com/assafbk/OPRM) | [Link](https://huggingface.co/tiiuae/falcon-mamba-7b-instruct), [Link](https://huggingface.co/tiiuae/Falcon3-Mamba-7B-Instruct), [Link](https://huggingface.co/google/recurrentgemma-9b-it), [Link](https://huggingface.co/RWKV/v6-Finch-7B-HF), [Link](https://huggingface.co/state-spaces/mamba-1.4b), [Link](https://huggingface.co/state-spaces/mamba-130m), [Link](https://huggingface.co/assafbk/decimamba-130m-niah), [Link](https://huggingface.co/assafbk/mamba-130m-niah) |
| [UMoE: Unifying Attention and FFN with Shared Experts](https://arxiv.org/abs/2505.07260) | Jing Li, Chaozheng Wang, ysngkil | - This paper introduces UMoE, a novel unified Mixture-of-Experts (MoE) architecture that combines both attention and feed-forward network (FFN) layers with shared experts. - UMoE achieves this unification by reformulating the attention mechanism to reveal an underlying FFN-like structure, enabling efficient parameter sharing between the two components. - The model demonstrates superior performance compared to existing attention-based and FFN-based MoE approaches, achieving this result in both language modeling pretraining and zero-shot evaluation tasks. - UMoE's effectiveness stems from its unified design and efficient parameter sharing, which allows for improved performance with the same number of parameters. - The authors conduct extensive experiments on large-scale language models across various datasets and tasks, showing consistent superiority in performance and efficiency. | ['Natural Language Processing'] | [Link](https://github.com/ysngki/UMoE) | N/A |
| [Document Attribution: Examining Citation Relationships using Large
  Language Models](https://arxiv.org/abs/2505.06324) | Nedim Lipka, Vipula Rawte, Franck-Dernoncourt, ryanrossi | - This paper introduces two novel techniques for addressing the challenge of attribution in large language models (LLMs), focusing on tracing generated outputs back to their source documents. - The first technique is a zero-shot approach that frames attribution as a textual entailment task, demonstrating improvements of 0.27% and 2.4% over baseline methods on the AttributionBench dataset for in-distribution and out-of-distribution sets respectively. - The second technique explores the role of the attention mechanism in improving attribution accuracy, showing that a smaller LLM outperforms the baseline across multiple layers. - Experiments were conducted using AttributionBench, evaluating performance using F1 score, false positives, and false negatives. - The findings highlight the effectiveness of both zero-shot textual entailment and attention mechanisms for enhancing LLM attribution, advancing the interpretability and reliability of these models. | ['Natural Language Processing', 'Question Answering', 'Zero-Shot Classification'] | [Link](null) | [Link](null) |


## Papers for 2025-05-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Bielik v3 Small: Technical Report](https://arxiv.org/abs/2505.02550) | Adrian Gwoździej, Łukasz Flis, djstrong, Remek, chrisociepa | This paper introduces Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B parameters) optimized for Polish language processing.  The models utilize a custom Polish tokenizer (APT4) and Adaptive Learning Rate for improved efficiency.  Bielik v3 models achieve performance comparable to much larger models on various benchmarks including the Open PL LLM Leaderboard and the Complex Polish Text Understanding Benchmark.  This demonstrates the effectiveness of the proposed techniques for resource-efficient language modeling in less-resourced languages. The models were trained on a large, high-quality Polish corpus of 292 billion tokens and achieve state-of-the-art results on several Polish NLP benchmarks. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/SpeakLeash) |
| [Bielik 11B v2 Technical Report](https://arxiv.org/abs/2505.02410) | Adrian Gwoździej, Łukasz Flis, Remek, djstrong, chrisociepa |  - This paper introduces Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing, built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling. - Two key technical innovations are introduced: Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, enhancing performance and efficiency. - Bielik 11B v2 outperforms many larger models across Polish language benchmarks while maintaining cross-lingual capabilities. - The model demonstrates parameter efficiency and offers various quantization options enabling deployment on diverse hardware configurations. - Extensive evaluation across multiple benchmarks establishes new benchmarks for resource-efficient language modeling in under-resourced languages. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/spaces/speakleash/open_pl_llm_leaderboard), [Link](https://huggingface.co/spaces/speakleash/cptu_bench), [Link](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard) |
| [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/abs/2505.05026) | Yejin Choi, Sumin Shim, Min Soo Kim, Jang Han Yoon, jeochris | - This paper introduces WISERUI-BENCH, a benchmark dataset for pairwise UI design persuasiveness assessment, comprising 300 real-world UI image pairs labeled with A/B test results and expert rationales. - It proposes G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by mitigating position bias and improving accuracy. - G-FOCUS surpasses existing inference strategies in terms of consistency and accuracy for pairwise UI evaluation, as demonstrated by experimental results. - The study uses various VLMs (e.g., GPT-40, Claude 3.5, LLaMA 3.2) for evaluation and analysis. - The contributions include a new benchmark dataset and a novel inference-time reasoning strategy for VLM-based UI design persuasiveness assessment. | ['Multimodal', 'Image-to-Text', 'Image-to-Image', 'Visual Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Sailing AI by the Stars: A Survey of Learning from Rewards in
  Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686) | Xiaobao Wu |  - This survey paper provides a comprehensive overview of the learning from rewards paradigm in post-training and test-time scaling of large language models (LLMs). - The authors categorize and analyze the learning from rewards strategies across different stages: training, inference, and post-inference, and discuss the applications and benchmarks for reward models. - The unified conceptual framework of learning from rewards for LLMs is introduced, abstracting the key components and interactions involved. - The paper explores the taxonomy of learning from rewards for LLMs, categorizing existing works with a unified conceptual framework regarding reward model design and learning strategies. - Finally, the authors highlight the challenges and promising future directions in this field. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Multimodal'] | [Link](https://github.com/bobxwu/learning-from-rewards-llm-papers) | N/A |


## Papers for 2025-05-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Perception, Reason, Think, and Plan: A Survey on Large Multimodal
  Reasoning Models](https://arxiv.org/abs/2505.04921) | imryanxu, xyidealist, TerenceL-TL, foggyforest, YunxinLi |  - This paper provides a comprehensive survey of large multimodal reasoning models (LMRMs).  - It introduces a three-stage roadmap for the development of LMRMs, from modular reasoning to multimodal chain-of-thought (MCoT), and finally to long-horizon reasoning.  - The survey also introduces and analyzes Native Large Multimodal Reasoning Models (N-LMRMs), a forward-looking paradigm where reasoning natively emerges from omnimodal perception and interaction.  - Existing datasets and benchmarks are reorganized to clarify their categories and evaluation dimensions.  - The paper identifies limitations of current LMRMs and outlines research directions to advance the field. | ['Multimodal'] | [Link](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models) | N/A |
| [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in
  Large Language Models](https://arxiv.org/abs/2505.02847) | Peisong Wang, Qingxuan Jiang, Bang Zhang, zptu, vvibt | - This paper introduces Sentient Agent as a Judge (SAGE), a novel automated evaluation framework for assessing higher-order social cognition in LLMs. - SAGE uses a Sentient Agent that simulates human-like emotional changes and inner thoughts to provide more realistic evaluation during multi-turn conversations. - Experiments on 100 supportive-dialogue scenarios demonstrate that the Sentient emotion score correlates strongly with established human-centric instruments, validating psychological fidelity. - A public Sentient Leaderboard is created, revealing substantial gaps between frontier systems and earlier baselines, which are not reflected in conventional leaderboards. - SAGE provides a principled, scalable, and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents. | ['Natural Language Processing'] | [Link](https://github.com/tencent/digitalhuman/SAGE) | N/A |
| [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315) | cxiong, JunnanLi, doyensahoo, hendrydong, yuhuixu | - The paper introduces Elastic Reasoning, a novel framework that enhances the scalability of chain-of-thought (CoT) reasoning in large language models (LLMs) by separating the reasoning process into two phases: thinking and solution, each with independent budget allocations. - The proposed framework addresses the challenge of uncontrolled output lengths in CoT reasoning, which often hinders real-world deployment due to constraints on inference-time budgets. - A lightweight budget-constrained rollout strategy is introduced to improve the model's robustness to truncated thinking, enabling it to generalize effectively to unseen budget constraints without additional training. - Empirical results on mathematical and programming benchmarks demonstrate that Elastic Reasoning significantly improves reliability under tight resource constraints and produces more concise and efficient reasoning than baseline methods. - The method outperforms existing approaches like Long2Short and length control methods in terms of both accuracy and efficiency, particularly under strict resource constraints. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](null) | [Link](null) |
| [ICon: In-Context Contribution for Automatic Data Selection](https://arxiv.org/abs/2505.05327) | Zhifang Sui, soliz1998, yaolily, Rsy24, yyxsghx | - This paper introduces ICON, a novel gradient-free method for automatic data selection in instruction tuning that leverages in-context learning (ICL). - ICON measures sample contribution without gradient computation or manual indicators, offering a computationally efficient alternative to existing methods. - Experiments on three LLMs across 12 benchmarks demonstrate ICON's effectiveness, with models trained on 15% of ICON-selected data outperforming full datasets. - ICON-selected high-contribution samples exhibit diverse tasks and appropriate difficulty levels. - The ICON-guided selection paradigm enables scalable inference through linear-complexity calls. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://annayang2020.github.io/ICon_Data_Selection/) | N/A |
| [X-Reasoner: Towards Generalizable Reasoning Across Modalities and
  Domains](https://arxiv.org/abs/2505.03981) | RustyArchimedes, sidkiblawi, hiaoxui, shengz, qianchu | - This paper introduces X-REASONER, a vision-language model that achieves strong generalizable reasoning capabilities across modalities and domains through general-domain text-based post-training. - The model is trained using a two-stage approach: supervised fine-tuning with distilled long chain-of-thoughts followed by reinforcement learning with verifiable rewards. - Experiments demonstrate that X-REASONER outperforms existing state-of-the-art models on various general and medical benchmarks, showcasing its ability to transfer reasoning capabilities to both multimodal and out-of-domain settings. - A medical-specialized variant, X-REASONER-MED, is also introduced, achieving new state-of-the-art results on numerous medical benchmarks. - The findings support the hypothesis that reasoning is generalizable across modalities and domains, and that general-domain text-based post-training can effectively enable strong generalizable reasoning. | ['Multimodal'] | [Link](https://github.com/microsoft/x-reasoner) | N/A |
| [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language
  Models in Chinese](https://arxiv.org/abs/2504.19314) | Bruce Leon, HawkFaust, yeeeqichen99, MindYing, PALIN2018 | - This paper introduces BrowseComp-ZH, a new benchmark designed to evaluate the web browsing capabilities of Large Language Models (LLMs) specifically in Chinese. - The benchmark consists of 289 multi-hop questions covering 11 diverse domains, each reverse-engineered from a short, verifiable answer. - The dataset underwent a two-stage quality control process to ensure question difficulty and answer uniqueness. - Evaluation of over 20 state-of-the-art LLMs and search systems revealed that most models struggle, with accuracy rates below 10% for many. - The best-performing system achieved only 42.9% accuracy, highlighting the challenge of web browsing for current LLMs. | ['Question Answering'] | [Link](https://github.com/PALIN2018/BrowseComp-ZH) | N/A |
| [Chain-of-Thought Tokens are Computer Program Variables](https://arxiv.org/abs/2505.04955) | Zhifang Sui, peiyiwang89, soliz1998 | - This paper explores the role of chain-of-thought (CoT) tokens in large language models (LLMs), proposing the hypothesis that they function like computer program variables. - The authors conduct empirical studies on multi-digit multiplication and dynamic programming tasks, demonstrating that CoT tokens primarily store intermediate results. - Their experiments reveal that models can maintain comparable performance even when non-result tokens are removed or intermediate results are represented in latent forms. - Randomly intervening values in CoT tokens causes corresponding changes in subsequent tokens and the final answer, further supporting the variable-like behavior of CoT tokens. - The findings suggest a potential limit to computation complexity between CoT tokens, influencing model performance on complex tasks. | ['Natural Language Processing'] | [Link](https://github.com/solitaryzero/CoTs_are_Variables) | N/A |


## Papers for 2025-05-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Unified Multimodal Understanding and Generation Models: Advances,
  Challenges, and Opportunities](https://arxiv.org/abs/2505.02567) | Minghao Fu, Jintao Guo, Xinjie Zhang, Flourish, Suikong |  - This paper surveys recent advancements in unified multimodal understanding and generation models, addressing the independent evolution of autoregressive and diffusion-based approaches.  - It categorizes existing unified models into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches.  - The survey analyzes the structural designs and innovations of each category, compiling datasets and benchmarks tailored for unified models.  - Key challenges in this field are discussed, including tokenization strategies, cross-modal attention, and data limitations.  - The paper aims to inspire further research and provide a valuable resource for the community by regularly updating the survey. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image'] | [Link](https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models) | N/A |
| [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588) | Yingyan Hou, Xuanbo Fan, Zile Qiao, Hao Sun, SpaceProduct | - This paper introduces ZEROSEARCH, a novel reinforcement learning framework that enhances the search capabilities of LLMs without needing to interact with real search engines. - ZEROSEARCH uses a lightweight supervised fine-tuning process to transform an LLM into a retrieval module, capable of producing both relevant and irrelevant documents to answer a query. - The framework employs a curriculum-based strategy that gradually decreases the quality of documents during the RL training, which helps to elicit the model's reasoning ability through increasingly challenging scenarios. - Experiments show that ZEROSEARCH significantly improves the search capabilities of LLMs, with a 7B retrieval module achieving performance comparable to a real search engine, and a 14B module even surpassing it. - The approach is shown to generalize effectively across various model sizes and RL algorithms, making it a scalable and versatile solution. | ['Reinforcement Learning', 'Question Answering'] | N/A | N/A |
| [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video
  Generation](https://arxiv.org/abs/2505.04512) | Yuan Zhou, Sen Liang, Zhengguang Zhou, Zhentao Yu, Teng Hu | - The paper introduces HunyuanCustom, a novel multi-modal customized video generation framework that prioritizes subject consistency. - The model architecture leverages LLaVA for enhanced multi-modal understanding, an image ID enhancement module for reinforcing identity features, and modality-specific condition injection mechanisms for audio and video. - HunyuanCustom outperforms state-of-the-art methods in ID consistency, realism, and text-video alignment across various downstream tasks, including audio and video-driven customized video generation. - The model's effectiveness is validated through extensive experiments on single- and multi-subject scenarios, demonstrating its robustness across diverse input modalities. - The code and models are publicly available, facilitating further research and development in controllable video generation. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://hunyuancustom.github.io) | N/A |
| [Beyond Recognition: Evaluating Visual Perspective Taking in Vision
  Language Models](https://arxiv.org/abs/2505.03821) | Maciej Wołczyk, Michał Nauman, Piotr Miłoś, Alicja Ziarko, Gracjan | - This paper introduces a novel benchmark for evaluating visual perspective-taking capabilities in vision language models (VLMs). - The benchmark consists of 144 unique visual tasks, each paired with seven diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. - The experimental results reveal that while state-of-the-art VLMs excel in scene understanding, their performance significantly declines on spatial reasoning and further deteriorates on perspective-taking tasks. - This indicates a substantial gap between surface-level object recognition and deeper spatial and perspective reasoning required for complex visual tasks. - The findings highlight the need for incorporating explicit geometric representations and tailored training protocols in future VLM development to improve their visual perspective-taking abilities. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal
  Problem-Solving](https://arxiv.org/abs/2505.04528) | Qinxiang Cao, Xingzhi Qi, Renqiu Xia, Xinhao Zheng, purewhite42 | This paper introduces a novel framework for formal problem-solving that leverages formal theorem proving environments. The framework, FPS (Formal Problem-Solving), is based on a deterministic Markov decision process and is proven to be sound and complete.  A deductive version, D-FPS, is also presented, improving human alignment by decoupling solving and verification.  Three benchmarks are constructed to evaluate the framework, and results show that FPS and D-FPS outperform existing methods.  A new symbolic approach, RPE (Restricted Propositional Equivalence), is proposed to ensure that answers are faithfully and interpretably aligned with human intuition. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Purewhite2019/formal_problem_solving_main) | N/A |
| [OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue
  Resolution](https://arxiv.org/abs/2505.04606) | Jiachi Chen, Yanlin Wang, Runhan Jiang, Lianghong Guo, itaowe | OmniGIRL is a novel multilingual and multimodal benchmark designed for evaluating the effectiveness of Large Language Models (LLMs) in resolving GitHub issues.  It addresses the limitations of existing benchmarks by incorporating issues from four programming languages, spanning eight domains, and including multimodal information (text, images, and web links).  The benchmark consists of 959 instances and demonstrates the challenges LLMs face in this complex task, with the best-performing model achieving a resolution rate of only 8.6%.  Furthermore, the analysis reveals that current LLMs struggle with issues that involve understanding images and resolving issues requiring multi-file modifications. | ['Multimodal'] | [Link](https://github.com/DeepSoftwareAnalytics/OmniGIRL) | N/A |
| [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570) | Sinéad Ryan, Arturo Márquez Flores, Patrick Barker, Daniel Jeffries, mariya-davydova | - This paper introduces OSUniverse, a benchmark designed for evaluating the performance of multimodal GUI-navigation AI agents. - OSUniverse is comprised of complex tasks that increase in difficulty, ranging from basic clicking to multi-step actions across multiple applications, requiring advanced reasoning and dexterity. - The benchmark's design prioritizes ease of use, extensibility, thorough test case coverage, and automated validation, using Gemini models for automatic scoring with an average error rate below 2%. - The benchmark is designed to assess agent capabilities across different levels of complexity, ensuring that state-of-the-art agents achieve scores below 50% while average office workers demonstrate perfect accuracy. - The authors provide a detailed architecture of OSUniverse, including its test cases, checks, runners, and validators, highlighting its open-ended nature in terms of supported agents and models. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/agentsea/osuniverse) | N/A |
| [Knowledge Augmented Complex Problem Solving with Large Language Models:
  A Survey](https://arxiv.org/abs/2505.03418) | Yuqi Zhu, Yuchen Tian, Junwei Su, Lun Du, Da Zheng | This paper surveys the use of large language models (LLMs) for complex problem-solving, focusing on the capabilities and limitations of LLMs in various domains.  The paper explores techniques such as chain-of-thought reasoning and knowledge augmentation.  Three key components of complex problem-solving are identified: multi-step reasoning, domain knowledge, and result verification. The fundamental limitations of current LLM solutions are analyzed, and future research directions are discussed.  The study covers various domains including software engineering, mathematics, data science, and scientific research, highlighting their specific challenges.  Finally, the work suggests approaches for enhancing LLM capabilities in addressing complex real-world problems. | ['Natural Language Processing'] | N/A | N/A |
| [R&B: Domain Regrouping and Data Mixture Balancing for Efficient
  Foundation Model Training](https://arxiv.org/abs/2505.00358) | Ziyi Chu, Avi Trost, John Cooper, Tzu-Heng Huang, Albert Ge | - This paper introduces R&B, a novel framework for efficient foundation model training that addresses the limitations of existing data mixing strategies by re-partitioning training data based on semantic similarity and dynamically optimizing data composition using domain gradients. - R&B achieves significant computational efficiency compared to existing methods, reducing compute overhead by orders of magnitude while matching or exceeding performance on diverse datasets. - The effectiveness of R&B is validated through theoretical analysis under standard regularity conditions and empirical evaluations on five datasets, encompassing natural language, instruction-following, reasoning, and multimodal tasks. - The proposed framework's ability to dynamically optimize data mixtures throughout training is shown to be crucial for achieving high performance and efficiency. - R&B's success is attributed to its ability to capture fine-grained semantic nuances and efficiently leverage information already computed during standard training, thereby avoiding the need for additional compute-intensive evaluations. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-05-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement
  Fine-Tuning](https://arxiv.org/abs/2505.03318) | Qinglin Lu, Chunyu Wang, Zhimin Li, Yibin Wang, yuhangzang | - This paper introduces UNIFIEDREWARD-THINK, a novel unified multimodal chain-of-thought (CoT) reward model that enhances the accuracy and reliability of reward signals for both visual understanding and generation tasks. - The model employs an exploration-driven reinforcement fine-tuning approach involving three key stages: cold start, rejection sampling, and Group Relative Policy Optimization (GRPO) to elicit and incentivize the model's latent reasoning capabilities. - UNIFIEDREWARD-THINK demonstrates significant performance improvements over existing baselines across multiple vision reward tasks, outperforming existing methods in various image and video generation and understanding benchmarks. - The model exhibits strong implicit reasoning capabilities, surpassing baselines even without explicit reasoning traces, indicating a robust internalization of CoT reasoning. - The proposed three-stage training pipeline effectively enhances the model's ability to perform multi-dimensional, step-by-step long-chain reasoning, improving both the accuracy and robustness of reward signals. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at
  Scale](https://arxiv.org/abs/2505.03005) | Eugene Cheah, Janna Lu, Eric Alcaide, SmerkyG | - The paper introduces RADLADS, a novel method for efficiently converting softmax attention transformers into linear attention decoder models. - RADLADS employs a three-step distillation process involving attention weight transfer, attention hidden state alignment, and knowledge distillation, significantly reducing the computational cost of training large language models. - Two new RWKV-variant architectures, RAD-RWKV6 and RAD-RWKV7, are proposed to improve conversion efficiency and inference speed. - The method achieves state-of-the-art performance on various benchmark tasks for linear attention models of similar size. - All models are made publicly available on HuggingFace under the Apache 2.0 license (except for 72B models, which are also governed by the Qwen License Agreement). | ['Natural Language Processing'] | [Link](https://github.com/recursal/RADLADS-paper) | [Link](https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102) |
| [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM
  Inference](https://arxiv.org/abs/2505.02922) | Chengruidong Zhang, Jinkai Zhang, Yaoqi Chen, qianxizhang, baotonglu | - RETROINFER, a novel system, is presented that improves long-context LLM inference by using a vector storage system instead of a key-value cache. - The core of RETROINFER is the wave index, an Attention-Aware Vector index which uses techniques like tripartite attention approximation and accuracy-bounded attention estimation to efficiently and accurately retrieve important tokens. - RETROINFER also uses a wave buffer to manage KV cache placement and computation across GPU and CPU, improving throughput. - Experiments on long-context benchmarks show RETROINFER achieves up to 4.5x speedup over full attention within GPU memory limits and up to 10.5x over sparse attention baselines when KV cache is extended to CPU memory. - The system maintains full-attention-level accuracy. | ['Natural Language Processing'] | [Link](https://github.com/microsoft/RetrievalAttention) | N/A |
| [Decoding Open-Ended Information Seeking Goals from Eye Movements in
  Reading](https://arxiv.org/abs/2505.02872) | Yevgeni Berzak, Yoav Meiri, Omer Shubi, Cfir Avraham Hadar | - This paper introduces a novel task of decoding open-ended information-seeking goals from eye movements during reading, addressing a previously understudied aspect of reading behavior. - Two formulations of this task are proposed: question classification and question reconstruction. The former involves identifying which of several candidate questions a reader had in mind, and the latter aims to generate the exact question. - The study utilizes a large-scale eye-tracking dataset with hundreds of text-specific information-seeking tasks to evaluate the performance of several discriminative and generative multimodal LLMs.  - Experimental results demonstrate considerable success in both question classification and reconstruction, suggesting the feasibility of extracting valuable insights into readers' goals from eye movement data. - The findings suggest that LLMs can effectively combine eye movements and text to automatically decode readers' text-specific goals, opening new avenues for applications in education, content personalization, and text accessibility. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [An Empirical Study of Qwen3 Quantization](https://arxiv.org/abs/2505.02214) | Xudong Ma, Yue Feng, Yuye Li, HaoranChu, Xingyu-Zheng | - This paper presents a systematic evaluation of the effectiveness of five classic post-training quantization techniques applied to the Qwen3 large language model. - The study assesses the impact of quantization on Qwen3's performance across various bit-widths (1-8 bits) and multiple datasets encompassing diverse language tasks. - Findings reveal that Qwen3 maintains competitive performance at moderate bit-widths (4-8 bits) but experiences notable degradation in linguistic tasks under ultra-low precision (1-3 bits). - The results underscore the need for further research to enhance quantization methods for LLMs, particularly focusing on mitigating performance loss in extreme quantization scenarios. - This work contributes actionable insights for advancing quantization techniques tailored to Qwen3 and future LLMs, aiming to improve their practicality without compromising accuracy. | ['Natural Language Processing'] | [Link](https://github.com/QwenLM/Qwen3) | N/A |
| [Multi-Agent System for Comprehensive Soccer Understanding](https://arxiv.org/abs/2505.03735) | Yanfeng Wang, Ya Zhang, Zifeng Li, haoningwu, Homie0609 | This paper introduces SoccerAgent, a multi-agent system for comprehensive soccer understanding.  SoccerAgent leverages a modular architecture and collaborative reasoning to answer diverse and challenging questions using a large-scale multimodal knowledge base (SoccerWiki) and benchmark (SoccerBench).  The system achieves state-of-the-art performance on SoccerBench across various tasks.  Furthermore, the SoccerWiki knowledge base and the SoccerBench benchmark are publicly available. The proposed framework addresses fragmented and specialized approaches for soccer understanding and offers an effective solution for holistic soccer analysis. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text', 'Question Answering'] | [Link](https://jyrao.github.io/SoccerAgent/) | N/A |
| [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2505.03368) | Kevin Roitero, Stefano Mizzaro, sdesabbata | - This paper introduces a novel framework for studying the geospatial mechanistic interpretability of large language models (LLMs). - It uses spatial analysis to reverse-engineer how LLMs process geographical information, focusing on the internal representations generated by these models. - The study employs probing techniques, including spatial autocorrelation, to analyze the internal representations of LLMs and reveal spatial patterns. - Sparse autoencoders are utilized to disentangle polysemantic internal representations into more interpretable, monosemantic features, enhancing the understanding of how LLMs encode geographical information. - The findings suggest that LLMs encode geographical information in a complex, polysemantic manner but that sparse autoencoders can improve the interpretability of these representations. | ['Natural Language Processing', 'Feature Extraction'] | [Link](https://github.com/sdesabbata/geospatial-mechanistic-interpretability) | [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0) |
| [InfoVids: Reimagining the Viewer Experience with Alternative
  Visualization-Presenter Relationships](https://arxiv.org/abs/2505.03164) | Kevin Hsu, Ivy Chen, Tongyu Zhou, Ji Won Chung, Franck-Dernoncourt | - This paper introduces InfoVids, a novel visualization-presenter paradigm that integrates visualizations and presenters within a shared 3D space, aiming to create a more human-centric viewing experience. - InfoVids are designed as infographic-inspired videos and are crafted to redefine the relationships between the presenter and visualizations. - The study uses mixed methods, comparing InfoVids against their 2D counterparts, which reveals that InfoVids reduced viewer attention splitting, shifted focus to the presenter, and fostered more engaging full-body performances. - Four custom-constructed InfoVids were evaluated with 30 participants, with attention being given to the viewer experience and the resulting design implications. - The findings indicate that designing InfoVids entails considering social expectations among the presenter, visualization, and viewer, leading to a re-imagining of traditional visualization presentation dynamics. | ['Multimodal'] | N/A | N/A |
| [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient
  Large Speech-Language Model](https://arxiv.org/abs/2505.03739) | Lijiang Li, Heting Gao, Chaoyou Fu, Yunhang Shen, Zuwei Long | - VITA-Audio is a novel end-to-end large speech model that uses a lightweight Multiple Cross-modal Token Prediction (MCTP) module to generate audio and text tokens efficiently.  - The model architecture utilizes separate efficient modules to generate multiple audio tokens in one forward pass, leading to minimal latency in real-time conversational capabilities.  - It incorporates a four-stage progressive training strategy to accelerate model training with minimal loss in speech quality.  - Experimental results show that VITA-Audio achieves an inference speedup of 3-5x at the 7B parameter scale while outperforming other open-source models on multiple benchmarks for ASR, TTS, and SQA.  - The model is fully reproducible and trained using only open-source data. | ['Audio', 'Text-to-Speech', 'Automatic Speech Recognition'] | [Link](https://github.com/VITA-MLLM/VITA-Audio) | N/A |
| [Invoke Interfaces Only When Needed: Adaptive Invocation for Large
  Language Models in Question Answering](https://arxiv.org/abs/2505.02311) | Biao Qin, Chunlai Zhou, Robot2050 | - This paper introduces AttenHScore, a novel metric for evaluating the invocation of large language models (LLMs) in question answering. - AttenHScore calculates the accumulation and propagation of hallucinations during small language model (SLM) generation, dynamically adjusting the detection threshold for more accurate LLM invocation. -  Uncertainty-aware knowledge reorganization assists SLMs in capturing critical information from different text chunks, improving performance even with complex queries. - Extensive experiments across multiple QA datasets show that AttenHScore outperforms baselines in real-time hallucination detection, eliminating the need for additional model training. - The method displays flexibility in adapting to various transformer-based LMs and improves overall efficiency by leveraging both LLMs and SLMs. | ['Question Answering'] | [Link](https://github.com/Robot2050/AttenHScore) | N/A |
| [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in
  Smart Personal Assistant](https://arxiv.org/abs/2504.18373) | Xiaoyu Shen, lorashen | - Auto-SLURP, a benchmark dataset for evaluating multi-agent frameworks in smart personal assistants, is introduced.  - The dataset extends the original SLURP dataset by relabeling data and integrating simulated servers and external services for comprehensive end-to-end evaluation. - Experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting the need for further advancements. - The dataset and related code are publicly available. - The paper analyzes the performance of several multi-agent frameworks on Auto-SLURP, showing variations in accuracy and efficiency. | ['Natural Language Processing'] | [Link](https://github.com/lorashen/Auto-SLURP/) | N/A |


## Papers for 2025-05-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Voila: Voice-Language Foundation Models for Real-Time Autonomous
  Interaction and Voice Role-Play](https://arxiv.org/abs/2505.02707) | Yu Shu, Yemin Shi, zhitinghu, Jaward, guangyil | - Voila, a family of large voice-language foundation models, is introduced for real-time, natural, and flexible voice interaction.  - Voila uses a hierarchical Transformer architecture with streaming audio encoding and tokenization, a multi-scale Transformer with an LLM backbone and audio generator, and is trained end-to-end with audio-text data. - Voila-e2e focuses on low-latency, nuanced voice conversations, while Voila-autonomous allows continuous listening, reasoning, and responses in a full-duplex manner. - On the Voila benchmark, Voila outperforms SpeechGPT and Moshi, achieving 30.56% accuracy.  - Voila also performs competitively on ASR and TTS tasks, with a WER of 2.7% and 2.8% respectively when trained with LibriSpeech data. | ['Multimodal', 'Audio', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition'] | [Link](github.com/maitrix-org/Voila) | [Link](hf.co/spaces/maitrix-org/Voila-demo), [Link](hf.co/maitrix-org/Voila-base), [Link](hf.co/maitrix-org/Voila-chat), [Link](hf.co/maitrix-org/Voila-autonomous-preview), [Link](hf.co/maitrix-org/Voila-Tokenizer), [Link](hf.co/datasets/maitrix-org/Voila-Benchmark), [Link](hf.co/datasets/maitrix-org/Voila-million-voice) |
| [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387) | Ziqi Wang, zhangdenghui123, Merlin-Hongru, gaotang, XtremSup | - This paper introduces Reasoning Reward Models (REASRMS), a new class of generative reward models that frame reward modeling as a reasoning task, enhancing interpretability and performance. - The proposed model, RM-R1, employs a two-stage training process: distillation of high-quality reasoning chains and reinforcement learning with verifiable rewards. - RM-R1 leverages a novel Chain-of-Rubrics (CoR) prompting framework to elicit structured reasoning, classifying tasks as reasoning or chat and generating tailored evaluations. - Empirical results demonstrate state-of-the-art or near state-of-the-art performance on multiple reward model benchmarks, outperforming larger open-weight and proprietary models by up to 13.8%. -  The enhanced performance is validated across various domains, including chat, safety, and reasoning, highlighting the effectiveness of the reasoning-based training approach. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/RM-R1-UIUC/RM-R1) | N/A |
| [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop
  Reasoning with Transformers](https://arxiv.org/abs/2504.20752) | Gjergji Kasneci, Roman Abramov, fsteinbauer | - This paper explores grokking, a phenomenon where neural networks transition from memorization to generalization, in the context of real-world multi-hop reasoning using transformers. - It addresses the challenge of sparse real-world data by augmenting existing knowledge graphs with synthetic data to achieve the threshold ratio of inferred to atomic facts required for grokking. - Surprisingly, even factually incorrect synthetic data can improve reasoning circuits, possibly by encouraging reliance on relational structure over memorization. - The approach achieves up to 95-100% accuracy on 2WikiMultiHopQA, surpassing baselines and matching or exceeding state-of-the-art results. - The analysis suggests that data augmentation based on grokking can unlock implicit multi-hop reasoning capabilities in large language models, leading to more robust and interpretable factual reasoning. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language
  Models](https://arxiv.org/abs/2505.02735) | ZhengYuan, yifanzhang114, Liam-Liu, prt66, zhouliang | - FormalMATH, a large-scale Lean4 benchmark with 5,560 formally verified problems ranging from high-school Olympiad challenges to undergraduate-level theorems is introduced to address limitations of existing benchmarks. - A human-in-the-loop autoformalization pipeline integrating LLMs for statement autoformalization, multi-LLM semantic verification, and negation-based disproof filtering is proposed to mitigate manual formalization costs, retaining 72.09% of statements while maintaining accuracy. - Evaluation of state-of-the-art LLM-based theorem provers revealed significant limitations, with the best model achieving only a 16.46% success rate under practical sampling budgets. - Existing provers exhibit domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics; counterintuitively, natural-language solution guidance is found to decrease proof success in chain-of-thought reasoning. - FormalMATH provides a robust benchmark for evaluating and improving formal mathematical reasoning capabilities in LLMs. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link]([data]) |
| [ReplaceMe: Network Simplification via Layer Pruning and Linear
  Transformations](https://arxiv.org/abs/2505.02819) | szagoruyko121, stamatisl, madrugado, ammarali32, dimitriish | - ReplaceMe is a training-free structured pruning method for LLMs that replaces transformer blocks with linear transformations, thereby reducing computational overhead without significant performance loss. - Unlike other pruning methods, ReplaceMe doesn't require any retraining or fine-tuning, making it computationally efficient. - It estimates linear transformations using a small calibration dataset and merges them into existing model weights without introducing additional parameters. - Experimental results show that ReplaceMe outperforms other training-free approaches and remains competitive with state-of-the-art pruning methods that involve retraining, achieving up to 25% pruning while retaining ~90% of the original model's performance. - The method also generalizes to other transformer architectures like ViT, showing its effectiveness beyond text generation tasks. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Practical Efficiency of Muon for Pretraining](https://arxiv.org/abs/2505.02222) | cadarsh-essential, monk-essential, karlstratos, ampolloreno, ishaan-essential | - This paper introduces Muon, a novel second-order optimizer for pretraining large language models, and demonstrates its superior performance over AdamW. - Muon achieves greater data efficiency at large batch sizes, expanding the Pareto frontier on the compute-time tradeoff. - The authors propose a simple telescoping algorithm that efficiently addresses hyperparameter tuning challenges across different model sizes. - Extensive experiments with models up to four billion parameters validate Muon's improved performance and the effectiveness of the telescoping algorithm. - The work demonstrates that Muon is a practical and efficient alternative to AdamW for large-scale pretraining. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/EssentialAI) |
| [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement
  Learning](https://arxiv.org/abs/2505.02835) | KevinTowne, KaiyuValley, bhsc24, XingyuLu, yifanzhang114 | - This paper introduces R1-Reward, a novel multimodal reward model trained using a novel reinforcement learning algorithm called StableReinforce. - StableReinforce addresses instability issues in existing RL algorithms by refining the training loss, advantage estimation, and reward design. - R1-Reward significantly outperforms existing state-of-the-art models on three benchmark datasets: VL Reward-Bench, Multimodal Reward Bench, and MM-RLHF Reward Bench. - The model's performance further improves with increased inference compute, highlighting the potential of RL algorithms in optimizing MRMs. - The authors collected 200K preference data to facilitate MRM training, demonstrating the model's efficiency in utilizing data. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/yfzhang114/r1_reward) | N/A |
| [A Survey on Inference Engines for Large Language Models: Perspectives on
  Optimization and Efficiency](https://arxiv.org/abs/2505.01658) | Sungryeol Jeon, leejaymin, Devcow, oos2, inputsh | - This paper presents a comprehensive survey of 25 Large Language Model (LLM) inference engines, both open-source and commercial, focusing on optimization and efficiency. - The paper analyzes each engine based on ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation, categorizing optimization techniques such as batching, parallelism, compression, fine-tuning, caching, attention optimization, sampling optimization, and structured outputs. - It further explores the design goals and supported optimization techniques of each engine, examining ecosystem maturity for open-source solutions and performance/cost policies for commercial offerings. - The study also assesses recent trends like reasoning-centric test-time scaling and LLM-based AI agents which require multiple inference calls, therefore requiring higher inference efficiency. - Finally, it outlines future research directions including support for complex LLM-based services, diverse hardware support, and enhanced security. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/sihyeong/Awesome-LLM-Inference-Engine) | N/A |
| [Think on your Feet: Adaptive Thinking via Reinforcement Learning for
  Social Agents](https://arxiv.org/abs/2505.02156) | Xinghua Zhang, Haobo Wang, bingliwu, Yongbin-Li, iiiiwis | This paper introduces Adaptive Mode Learning (AML), a novel framework that dynamically adjusts reasoning depth in social agents using reinforcement learning.  AML strategically selects from four thinking modes (intuitive reaction to deep contemplation) based on real-time context. The core of AML is the Adaptive Mode Policy Optimization (AMPO) algorithm, which improves upon existing methods by incorporating multi-granular thinking modes and context-aware switching.  Experiments show that AML outperforms state-of-the-art methods by 15.6% on social intelligence tasks, achieving a 7% performance gain and 32.8% reduction in token usage compared to GRPO.  The results demonstrate AML's ability to enable more human-like adaptive reasoning. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/MozerWang/AMPO) | N/A |
| [Low-Precision Training of Large Language Models: Methods, Challenges,
  and Opportunities](https://arxiv.org/abs/2505.01043) | Li Shen, Guoxia, csdvT, GGJY, Zhiwei840 | This paper surveys existing low-precision training methods for large language models (LLMs), categorizing them by numerical format (fixed-point, floating-point, and custom).  It systematically organizes the approaches and discusses quantization-aware training.  The study highlights several promising research directions such as advanced quantization methods, ultra low-precision training, and fine-grained scaling strategies.  It also emphasizes the need for unified training frameworks, standardized benchmarks, and integration with other efficient training paradigms.  A collection of papers is provided in Awesome-Low-Precision-Training. | ['Natural Language Processing'] | [Link](https://github.com/Hao840/Awesome-Low-Precision-Training) | N/A |
| [Ming-Lite-Uni: Advancements in Unified Architecture for Natural
  Multimodal Interaction](https://arxiv.org/abs/2505.02471) | bear-xxy, jianxinsun, chenjingdong, zhengdd0422, BiaoGong | - Ming-Lite-Uni is an open-source multimodal framework with a unified visual generator and a native multimodal autoregressive model, integrating MetaQueries and M2-omni framework with multi-scale learnable tokens and a multi-scale representation alignment strategy. - It leverages a fixed MLLM and a learnable diffusion model, enabling both text-to-image generation and instruction-based image editing, outperforming closed-source models like GPT-40 and Gemini-1.5-Pro in multimodal understanding benchmarks. - The framework enhances visual generation through a FlowMatching loss in the diffusion model, enabling concurrent optimization with a frozen MLLM, leading to improvements in generation quality. - Ming-Lite-Uni uses a multi-scale representation alignment strategy which enhances high-res reconstruction quality and boosts GenEval by 1.5%. - A multimodal dataset was curated for Ming-Lite-Uni exhibiting robust control fluency and contextual understanding, addressing fine-grained image editing and text-to-image QA tasks through natural language. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/inclusionAI/Ming/tree/main/Ming-unify) | N/A |
| [TEMPURA: Temporal Event Masked Prediction and Understanding for
  Reasoning in Action](https://arxiv.org/abs/2505.01583) | vibhav-vineet, yilche, wchai, hsiangwei0903, andaba | - TEMPURA, a two-stage training framework, enhances video temporal understanding in large multimodal models (LMMs) by combining dense event segmentation with masked event prediction. - The model architecture involves a vision encoder and a large language model (LLM), trained first on masked event prediction for reasoning and then on video segmentation and dense captioning for fine-grained temporal grounding. - TEMPURA outperforms the baseline Qwen2.5-VL and other state-of-the-art models on Charades-STA (for temporal grounding) and QVHighlights (for highlight detection) benchmarks without requiring target-task fine-tuning. - The model achieves a 39.2 mIoU on Charades-STA, surpassing the baseline by 6.3 points, and a 51.7 HIT@1 score on QVHighlights, outperforming the baseline by 6.9 points. - A new large-scale dataset, VER, comprising 500K untrimmed videos with dense event captions and structured reasoning steps, is introduced to facilitate the training process. | ['Video-Text-to-Text', 'Multimodal', 'Computer Vision', 'Video Classification'] | N/A | N/A |
| [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive
  Streaming Speech Synthesis](https://arxiv.org/abs/2505.02625) | Yang Feng, Yan Zhou, zhangshaolei, guoshoutao, poeroz | - This paper introduces LLaMA-Omni 2, a series of speech language models (SpeechLMs) with sizes ranging from 0.5B to 14B parameters, designed for real-time spoken chatbot applications. - The model architecture consists of a Whisper encoder for speech understanding, a Qwen2.5 series LLM as the core, and an autoregressive streaming speech decoder with a text-to-speech language model and a flow matching model for speech generation. - Trained on 200K synthesized multi-turn speech dialogues, LLaMA-Omni 2 demonstrates superior performance in spoken question answering and speech instruction following tasks, outperforming existing SpeechLMs like GLM-4-Voice and LLaMA-Omni, even with significantly less training data (millions of hours vs. 200K samples). - The model achieves low latency (around 600ms) for real-time interaction and maintains high consistency between generated speech and text. - Ablation studies highlight the efficacy of various components, including the gate fusion module and the TTS pretraining strategy. | ['Multimodal', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition', 'Audio-to-Audio', 'Text2Text Generation'] | [Link](https://github.com/ictnlp/LLaMA-Omni2) | [Link](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), [Link](https://huggingface.co/fishaudio/), [Link](https://huggingface.co/datasets/Stanford/web_questions), [Link](https://huggingface.co/datasets/google-research-datasets/LLaMA-Test-Set), [Link](https://huggingface.co/tatsu-lab/alpaca_eval) |
| [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and
  Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456) | Jie Peng, Peter Hase, mohitbansal, a2889184, vaidehi99 | - This paper introduces UNLOK-VQA, a novel benchmark designed for evaluating the targeted unlearning of sensitive information from Multimodal Large Language Models (MLLMs). - It proposes an "attack-and-defense" framework to assess the robustness of unlearning methods against various attack strategies, including white-box attacks leveraging model internals and black-box attacks employing input variations. - The dataset creation involves an automatic pipeline generating rephrase and neighborhood samples for nuanced evaluation of generalization and specificity, followed by manual filtering for quality control. - Experimental results on LLaVA-v1.5 demonstrate the effectiveness of multimodal extraction attacks and show that larger models exhibit greater resilience to attacks after unlearning. - The best defense mechanism, removing answer information from internal hidden states, significantly reduces attack success rates, establishing UNLOK-VQA as a valuable benchmark for future research in multimodal unlearning. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/Vaidehi99/UnLOK-VQA) | N/A |


## Papers for 2025-05-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|


## Papers for 2025-05-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|


## Papers for 2025-05-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [WebThinker: Empowering Large Reasoning Models with Deep Research
  Capability](https://arxiv.org/abs/2504.21776) | Yutao Zhu, Hongjin Qian, Guanting Dong, Jiajie Jin, Xiaoxi Li | - WebThinker is a novel deep research agent that empowers Large Reasoning Models (LRMs) to conduct autonomous web searches, navigate web pages, and generate research reports within their reasoning process. - It integrates a Deep Web Explorer module for dynamic information gathering and an Autonomous Think-Search-and-Draft strategy for real-time report writing. - RL-based training with online Direct Preference Optimization (DPO) is used to refine tool utilization. - Experimental results on complex reasoning (GPQA, GAIA, WebWalkerQA, HLE) and report generation (Glaive) benchmarks show WebThinker significantly outperforms current methods, including proprietary systems. - This approach enhances LRM reliability and opens possibilities for more capable deep research systems. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/RUC-NLPIR/WebThinker) | N/A |
| [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318) | Harkirat Behl, Vidhisha Balachandran, Ahmed Awadallah, Sahaj Agarwal, Marah Abdin | - This technical report introduces Phi-4-reasoning and Phi-4-reasoning-plus, two 14-billion parameter reasoning models fine-tuned from the Phi-4 LLM and enhanced for complex reasoning tasks. - Phi-4-reasoning is trained via supervised fine-tuning on curated "teachable" prompts with reasoning demonstrations generated using 03-mini, while Phi-4-reasoning-plus incorporates a subsequent phase of outcome-based reinforcement learning on math problems. - Both models demonstrate significant performance improvements over the base Phi-4 model and other larger open-weight models across a variety of reasoning benchmarks, including math, science, coding, and algorithmic problem-solving, approaching the accuracy of the larger DeepSeek-R1 model. - Evaluation highlights the importance of data curation and the potential of combining supervised fine-tuning with reinforcement learning for developing efficient, smaller reasoning models.  - The report also underscores the need for more robust evaluation practices in reasoning benchmarks, emphasizing the impact of non-determinism and small dataset sizes on single-score accuracy reporting. | ['Question Answering', 'Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/lchen001/AIME1983_2024), [Link](https://huggingface.co/datasets/lchen001/AIME2025) |
| [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720) | Tong Liu, Zhenlin Yang, Yixin Ji, Juntao Li, zenRRan | - This paper surveys methods for efficient Large Language Model (LLM) inference serving, categorizing them into instance-level optimizations, cluster-level strategies, emerging scenarios, and miscellaneous areas. - Instance-level methods include model placement, request scheduling, decoding length prediction, and KV cache optimization. - Cluster-level strategies involve GPU cluster configurations, service-oriented scheduling, and load balancing techniques to handle diverse workloads. - Emerging scenarios cover efficient serving strategies for long context, Retrieval-Augmented Generation (RAG), Mixture of Experts (MoE), Low-Rank Adaptation (LoRA), speculative decoding, augmented LLMs, and test-time reasoning. - Miscellaneous areas address critical aspects like hardware considerations, privacy concerns, the use of simulators, ensuring fairness, and managing energy consumption. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zenrran4nlp/Awesome-LLM-Inference-Serving) | N/A |
| [UniBiomed: A Universal Foundation Model for Grounded Biomedical Image
  Interpretation](https://arxiv.org/abs/2504.21336) | Hao Chen, Jiaxin Zhuang, Sunan He, Yuxiang Nie, Linshan Wu | - UniBiomed, a novel universal foundation model, is introduced for grounded biomedical image interpretation, unifying the generation of clinical texts and segmentation of corresponding objects using a Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM). - This model tackles a wide array of tasks across ten biomedical imaging modalities using a newly curated dataset of over 27 million image-annotation-text triplets. - Extensive validation across 84 datasets demonstrates state-of-the-art performance in segmentation, exceeding existing models like BiomedParse by an average of 10.25% Dice score. - UniBiomed automates end-to-end grounded interpretation, removing the need for expert pre-diagnosis and manual prompt creation required by previous methods, optimizing clinical workflow. - This model also excels in grounded disease recognition, region-aware diagnosis, visual question answering, and report generation across diverse imaging modalities. | ['Image Segmentation', 'Text Generation', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/Luffy03/UniBiomed) | N/A |


## Papers for 2025-04-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with
  Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734) | Sung Ju Hwang, Soyeong Jeong, jinheon, KangsanKim71, wgcyeo | - UniversalRAG, a novel Retrieval-Augmented Generation (RAG) framework, retrieves and integrates knowledge from diverse corpora spanning multiple modalities (text, image, and video) and granularities (paragraph, document, image, clip, and video). - It addresses the "modality gap" in unified representation spaces by employing a modality-aware routing mechanism that dynamically selects the most suitable modality-specific corpus for retrieval based on the query. - Within each modality, UniversalRAG further organizes corpora into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. - Experimental results on 8 multimodal benchmarks demonstrate that UniversalRAG consistently outperforms modality-specific and unified baselines, showing robust performance and efficient resource allocation. - Further analyses reveal the effectiveness of trained routers, the importance of granularity in retrieval, and the framework's scalability with larger LVLMs. | ['Multimodal', 'Question Answering', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595) | pangwei, sewon, Muennighoff, volpato30, rulins | • The paper introduces REASONIR-8B, a novel retriever model specifically trained for general reasoning tasks, addressing the limitations of existing retrievers on such tasks. • REASONIR-8B utilizes a synthetic data generation pipeline to create challenging and relevant queries with hard negatives for each document, enhancing its ability to handle complex reasoning. • The model achieves state-of-the-art performance on the BRIGHT benchmark, a widely used reasoning-intensive IR benchmark, without and with a reranker. • When applied to RAG tasks, REASONIR-8B demonstrates significant performance improvements on MMLU and GPQA compared to existing baselines. • The code, data, and model are open-sourced to facilitate future research and development in the area of reasoning-intensive information retrieval. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/facebookresearch/ReasonIR) | [Link](https://huggingface.co/reasonir/ReasonIR-8B) |
| [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998) | Yong Jae Lee, Trung Bui, Jing Shi, Krishna Kumar Singh, Thao Nguyen | - Yo'Chameleon, a novel approach for personalizing Large Multimodal Models (LMMs), enables tailored image and text generation for user-defined concepts using only 3-5 images. - The method addresses catastrophic forgetting by using "soft-positive" images, dynamically adjusting prompt length based on visual similarity, and introduces a self-prompting mechanism to optimize dual soft prompts for text and image generation tasks. - It leverages soft prompt tuning with a dynamic prompt length mechanism and a self-prompting optimization process to balance performance across both modalities. - Qualitative and quantitative results demonstrate Yo'Chameleon efficiently learns concepts with fewer tokens and outperforms prompting baselines in encoding visual attributes. - Evaluations on visual question answering, recognition accuracy, CLIP Image Similarity, and facial similarity reveal Yo'Chameleon's superior performance compared to existing baselines. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879) | Daniel D'Souza, Alex Wang, Yiyang Nan, Shivalika Singh, yuntian-deng | This work identifies systematic issues resulting in a distorted playing field in the Chatbot Arena, a benchmark for ranking AI systems.  The authors find that undisclosed private testing practices, selective score reporting, and biased model sampling rates lead to skewed results.  They establish that data access asymmetries exist, providing some providers unfair advantages in achieving higher rankings.  The paper concludes with actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking. | ['Natural Language Processing'] | [Link](https://github.com/lm-sys/FastChat/blob/0e6d3e4beaab66f4d3f93db72541a4abab8af28d/fastchat/serve/monitor/monitor_md.py#L7) | [Link](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) |
| [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046) | Daniel Khashabi, Benjamin Van Durme, Marc Marone, Jiacan Yu, jackzhang | - This paper introduces BLOOMSCRUB, a novel inference-time method for mitigating worst-case LLM copyright infringement by eliminating long verbatim quotes from copyrighted sources. - BLOOMSCRUB employs Bloom filters for efficient quote detection and dynamic rewriting techniques to transform potentially infringing segments, ensuring scalability and adaptability. - Experimental results demonstrate that BLOOMSCRUB significantly reduces infringement risk while preserving text quality and outperforms existing methods. - The method offers certified risk reduction through adaptive abstention when quotes cannot be rewritten, providing a reliable safeguard against legal liabilities. - BLOOMSCRUB's simple yet effective design makes it a practical and robust framework for certified copyright takedown in deployed LLMs. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting](https://arxiv.org/abs/2504.20630) | Tao Jin, Zhiyuan Zhu, Changhao Pan, Wenxiang Guo, AaronZ345 |  - This paper introduces ISDrama, a novel multimodal immersive spatial drama generation model that produces high-quality continuous multi-speaker binaural speech with dramatic prosody.  - The model architecture comprises a Multimodal Pose Encoder, which extracts unified pose information from diverse input modalities, and an Immersive Drama Transformer, a flow-based Mamba-Transformer model with Drama-MOE.  - ISDrama outperforms several baseline models on various objective and subjective metrics, demonstrating its ability to generate high-quality, immersive, spatial audio with dramatic prosody.  - The study also introduces MRSDrama, a new multimodal recorded spatial drama dataset used to train and evaluate the model, consisting of binaural audios, scripts, videos, geometric poses, and textual prompts.  - The proposed ISDrama model shows improved performance in various aspects like quality, speaker similarity, pose consistency and prosodic expressiveness due to utilizing various advanced techniques. | ['Audio', 'Text-to-Speech'] | N/A | N/A |
| [X-Fusion: Introducing New Modality to Frozen Large Language Models](https://arxiv.org/abs/2504.20996) | Yijun Li, Siddharth Srinivasan Iyer, Xun Huang, Thao Nguyen, Sicheng Mo | - X-Fusion is a novel framework that adapts pre-trained Large Language Models (LLMs) to multimodal tasks, preserving language capabilities while enabling image understanding and generation using a dual-tower architecture.  - This dual tower processes image data with trainable vision-specific weights and language data with frozen LLM weights, which enables cross-modal interaction between different modalities.  - X-Fusion outperforms other baseline architectures on image-to-text and text-to-image tasks, demonstrating the effectiveness of the dual-tower design.  - The study also reveals that using clean images for image understanding improves both generation and understanding performance, and there is an asymmetric relationship where understanding data benefits generation, but not vice-versa.  - Additionally, aligning vision features with pre-trained representations benefits smaller models but has less impact on larger ones. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://sichengmo.github.io/XFusion/) | N/A |
| [TreeHop: Generate and Filter Next Query Embeddings Efficiently for
  Multi-hop Question Answering](https://arxiv.org/abs/2504.20114) | Xuming Hu, Shuliang Liu, Jinghuai Ou, Zhonghao Li, kpzhang1028 | - TreeHop is an embedding-level framework for multi-hop question answering (MHQA) that dynamically updates query embeddings by fusing information from prior queries and retrieved documents, eliminating the need for LLMs in query refinement. - TreeHop replaces the traditional "Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined "Retrieve-Embed-Retrieve" loop, reducing computational overhead. - A rule-based stop criterion is introduced to prune redundant retrievals, balancing efficiency and recall. - Experimental results show TreeHop rivals advanced RAG methods on three open-domain MHQA datasets with only 5%-0.4% of the parameter size and 99% latency reduction. - TreeHop achieves this by using a gated cross-attention mechanism to extract salient information from retrieved chunks and is trained with contrastive learning. | ['Question Answering'] | [Link](https://github.com/allen-li1231/TreeHop-RAG) | N/A |


## Papers for 2025-04-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLM-Powered GUI Agents in Phone Automation: Surveying Progress and
  Prospects](https://arxiv.org/abs/2504.19838) | Yaxuan Guo, Guangyi Liu, Yuxiang007, melpancake, Pengxiangzhao | - This paper surveys the progress and prospects of Large Language Model (LLM)-powered GUI agents for phone automation, focusing on their evolution from script-based systems to intelligent and adaptive ones. - The authors propose a taxonomy that organizes the field into agent frameworks (single-agent, multi-agent, and plan-then-act), modeling methods (prompt engineering and training-based), and datasets/benchmarks. - The paper details task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that connect user intent with GUI operations. - This survey analyzes how LLMs improve phone automation by providing advanced language understanding, multimodal perception, and robust decision-making capabilities. - Finally, the authors identify open research challenges like dataset diversity, on-device efficiency, user adaptation, and security concerns in LLM-driven phone GUI agents. | ['Multimodal', 'Natural Language Processing'] | [Link](https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents) | N/A |
| [CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through
  Cryptography Challenges](https://arxiv.org/abs/2504.19093) | Chenlin Ming, Honglin Lin, Qizhi Pei, blue01223, yu0226 | - CipherBank, a benchmark designed to evaluate Large Language Models (LLMs) reasoning capabilities in cryptographic decryption tasks, is introduced. - The benchmark consists of 2358 problems covering 262 unique plaintexts across 5 domains and 14 subdomains, focusing on real-world, privacy-sensitive scenarios. - It uses 3 categories of encryption: Substitution, Transposition, and custom-designed ciphers, including 9 distinct algorithms and 5 difficulty levels. - State-of-the-art LLMs were evaluated, revealing significant performance gaps and highlighting the need for improvement in cryptographic reasoning. - Detailed analysis and error investigations provide insights into model limitations and potential areas for future research in LLM cryptographic reasoning capabilities. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919) | Juan Ciro, Hannah Rose Kirk, Guy Parsons, Rebecca Payne, Andrew M. Bean | - This paper investigates whether Large Language Models (LLMs) can effectively assist the public with medical self-assessment, focusing on identifying conditions and recommending appropriate courses of action. - A randomized controlled trial with 1298 participants was conducted, comparing LLM assistance (GPT-4, Llama 3, and Command R+) to a control group using their preferred methods. - Although LLMs achieved high accuracy on medical licensing exams and simulated scenarios, participants using LLMs performed worse than the control group in identifying conditions and no better in choosing dispositions. - User interaction difficulties were identified as the primary challenge, with participants providing incomplete information, misinterpreting LLM prompts, and inconsistently following recommendations. - Standard benchmarks and simulated user interactions did not effectively predict these real-world failures, highlighting the need for human user testing in evaluating LLM safety and reliability for medical applications. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/am-bean/HELPMed) | N/A |
| [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy
  Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780) | Yuan Feng, Qi Liu, Renqiu Xia, Zijun Chen, Daocheng Fu | - TrustGeoGen, a novel data engine, is introduced for generating large-scale, multimodal geometric problem-solving datasets with formal verification. - The engine comprises four components: a Constructor for creating geometric scenes, a Reasoner for building reasoning graphs, a Sampler for extracting reasoning paths, and a Translator for converting formal language to natural language. - A bootstrapping mechanism iteratively increases the complexity of generated problems, while GeoExplore algorithms ensure diverse solutions and self-reflective traceback data are produced.  - Experiments on the GeoTrust-200K dataset and GeoTrust-test testset reveal that state-of-the-art models achieve only 49.17% accuracy, demonstrating the challenging nature of the generated data.  - Training with GeoTrust data improves out-of-distribution generalization on GeoQA and reduces logical inconsistencies compared to using pseudo-labels generated by OpenAI. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/Alpha-Innovator/TrustGeoGen) | N/A |
| [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162) | Xiaodan Liang, Peisong Wang, Ruotian Ma, Bang Zhang, judge | - This paper introduces Self-Play Critic (SPC), a novel approach for evaluating the reliability of step-by-step reasoning in Large Language Models (LLMs), particularly for tasks like Chain-of-Thought. - SPC uses an adversarial self-play framework where two models, a "sneaky generator" and a "critic," are fine-tuned to compete against each other. The generator creates deliberately erroneous reasoning steps, and the critic attempts to identify them. - Through reinforcement learning based on game outcomes, both models iteratively improve their performance, with the winner rewarded and the loser penalized. This approach eliminates the need for manual step-level annotations. - Experiments show that SPC enhances error detection capabilities, surpassing strong baselines and other state-of-the-art models on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench). - Applying SPC during the test-time search of diverse LLMs improves mathematical reasoning performance on MATH500 and AIME2024 datasets. | ['Natural Language Processing', 'Question Answering'] | [Link](https://chen-judge.github.io/SPC/) | N/A |
| [Benchmarking Multimodal Mathematical Reasoning with Explicit Visual
  Dependency](https://arxiv.org/abs/2504.18589) | Xin Li, Zhiqiang Hu, Wenqi Zhang, Jiashuo Sun, cloudcatcher2 | - Introduces VCBENCH, a benchmark designed to evaluate multimodal mathematical reasoning with a focus on elementary-level math problems (grades 1-6) requiring explicit visual dependencies across multiple images. - VCBENCH comprises 1,720 question-answer pairs and 6,697 images (averaging 3.9 images per question) across six cognitive domains (Time and Calendar, Spatial and Positional Awareness, Geometry and Shapes, Objects and Motion, Reasoning and Observation, and Organization and Patterns) and evaluates five competencies (temporal reasoning, geometric reasoning, logical reasoning, spatial reasoning, and pattern recognition). - Evaluation of 26 state-of-the-art LVLMs on VCBENCH reveals significant performance disparities, with the best-performing models achieving accuracy below 50%, highlighting challenges in visual-mathematical integration, especially in multi-image reasoning. - Unlike knowledge-centric benchmarks, VCBENCH emphasizes vision-centric assessment through perceptual reasoning tasks that prioritize understanding of mathematical images and concepts rather than specialized knowledge. - The benchmark's multi-image focus and diverse competency coverage provide a comprehensive evaluation of visual reasoning abilities crucial for advancing toward broader AGI capabilities. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [MMInference: Accelerating Pre-filling for Long-Context VLMs via
  Modality-Aware Permutation Sparse Attention](https://arxiv.org/abs/2504.16083) | Xufang Luo, Qianhui Wu, Chengruidong Zhang, Yucheng Li, iofu728 | - MMInference, a dynamic sparse attention method, accelerates the pre-filling stage for long-context multi-modal inputs by leveraging modality-aware permutation and optimized GPU kernels. - It addresses unique sparse patterns like the Grid pattern in video inputs and handles modality boundary issues through permutation-based methods, achieving up to 8.3x speedup with a 1M-length context. - The method integrates seamlessly into existing VLM pipelines without modification and maintains accuracy on benchmarks like Video QA, Captioning, and Vision-NIAH with state-of-the-art VLMs (LongVila, Llava-Video, VideoChat-Flash, Qwen2.5-VL). - Experiments show significant acceleration while preserving accuracy, outperforming FlashAttention-2 and MInference by up to 8.3x and 1.7x, respectively, for a 1M-length context. - MMInference addresses the challenge of quadratic attention complexity in the pre-filling stage, enabling efficient processing of long multi-modal inputs crucial for real-world VLM deployment. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/datasets/lmms-lab/VideoDetailCaption) |
| [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via
  Substitution Ciphers](https://arxiv.org/abs/2504.19395) | Daniel Khashabi, Anqi Liu, Muhan Gao, Aayush Mishra, FocusV857 | - This paper introduces ICL CIPHERS, a novel method to quantify "learning" in In-Context Learning (ICL) by reformulating tasks using substitution ciphers. - The approach involves substituting tokens in input demonstrations with other tokens based on bijective or non-bijective mappings, making the text less interpretable but preserving task solvability. - The key idea is that improved performance on bijective (reversible) ciphers over non-bijective ciphers indicates the model's ability to learn and decode the cipher, thus quantifying ICL's learning component. - Experiments on various LLMs and datasets show consistent, albeit small, performance gains for bijective ciphers, suggesting LLMs' capacity for inference-time learning. - Further analysis of internal representations provides evidence of LLMs' ability to decode the ciphered inputs, supporting the hypothesis. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-04-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376) | Jay Karhade, Daniel Jiang, Stephen624, syCen, zhiqiulin | - This paper introduces CameraBench, a large-scale dataset and benchmark designed to improve camera motion understanding consisting of ~3,000 diverse internet videos, annotated with motion primitives and natural language descriptions. - The authors created a taxonomy of camera motion primitives in collaboration with cinematographers, capturing motion across various reference frames and using precise terminology. - The study found that current Structure-from-Motion (SfM) models struggle with semantic primitives and Video-Language Models (VLMs) struggle with geometric primitives. -  They fine-tuned a generative VLM on CameraBench, showing improvements and enabling applications like motion-augmented captioning and video question answering. -  A human study showed that training and experience significantly improves annotation accuracy, with experts outperforming novices, and that a proper training program effectively bridges this gap. | ['Computer Vision', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://linzhiqiu.github.io/papers/camerabench) | N/A |
| [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/abs/2504.16656) | Xiaokun Wang, Yi Peng, Yichen Wei, Chris, xuchensong | - Skywork R1V2 is a multimodal reasoning model employing a hybrid reinforcement learning paradigm combining Mixed Preference Optimization (MPO) and Group Relative Policy Optimization (GRPO) to balance reasoning capabilities and generalization, addressing the trade-off between specialized reasoning and maintaining general-purpose performance. - R1V2 utilizes a novel Selective Sample Buffer (SSB) mechanism to counteract the vanishing gradients issue in reinforcement learning, thereby improving training efficiency. - R1V2 leverages a reward model, Skywork-VL, and rule-based constraints for preference optimization and integrates a format reward based on DeepSeek's R1 chat template for reinforcement fine-tuning. - Achieving state-of-the-art results on various text and multimodal reasoning benchmarks including a score of 62.6% on OlympiadBench, 78.9% on AIME2024, 63.6% on LiveCodeBench, 73.6% on MMMU, and exceeding larger models such as Qwen2.5-VL-72B. - The model architecture consists of a frozen vision encoder (InternViT-6B), a reasoning-capable language model (QwQ-32B), and a lightweight MLP adapter to connect the two, enabling efficient knowledge transfer and reasoning.  | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/Skywork/Skywork-R1V2-38B) |
| [BitNet v2: Native 4-bit Activations with Hadamard Transformation for
  1-bit LLMs](https://arxiv.org/abs/2504.18415) | Furu Wei, Shuming Ma, Hongyu Wang | - BitNet v2 introduces native 4-bit activation quantization for 1-bit Large Language Models (LLMs) to enhance efficiency in batched inference. - The core innovation, H-BitLinear, applies an online Hadamard transformation before activation quantization, smoothing outlier-prone distributions for better 4-bit representation. - Trained from scratch with 8-bit activations, BitNet v2 matches BitNet b1.58 performance, then fine-tuned to native 4-bit activations with minimal degradation. - The Hadamard transformation reduces outliers, especially in the intermediate states of attention and FFN layers, enhancing the suitability for INT4 quantization. - Experiments show BitNet v2 with 4-bit activations (BitNet v2 (a4)) achieves performance comparable to BitNet a4.8 while being more computationally efficient for batched inference, exceeding post-training quantization methods like SpinQuant and QuaRot. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,
  Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821) | Wenhan Luo, Baotian Hu, Haoyuan Shi, Yunxin Li, Xinyu Chen | - This paper introduces VideoVista-CulturalLingo, a novel video evaluation benchmark designed to address the cultural, linguistic, and domain limitations of existing video comprehension datasets. - The benchmark comprises 1,389 videos and 3,134 question-answer pairs, spanning diverse cultures (China, North America, Europe), languages (Chinese, English), and domains (everyday life and scientific topics). - The dataset creation employed a hybrid approach combining large language models (LLMs) and human annotation for efficiency and quality. - Evaluation of 24 state-of-the-art large multimodal models (LMMs) revealed that existing models exhibit biases towards Western-centric content and struggle with temporal understanding, particularly in event localization tasks. - Gemini-2.0-Flash achieved the highest overall accuracy (76.3%), while open-source models like Qwen2.5-VL-72B showed promising results (61.3%) but lagged behind in certain tasks like video location. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text'] | [Link](https://github.com/HITsz-TMG/VideoVista) | [Link](https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo) |
| [Can Large Language Models Help Multimodal Language Analysis? MMLA: A
  Comprehensive Benchmark](https://arxiv.org/abs/2504.16427) | Peiwu Wang, Hua Xu, Yeshuang Zhu, Zhuohang Li, HanleiZhang | - This paper introduces MMLA, a comprehensive benchmark for evaluating multimodal language analysis capabilities of large language models (LLMs) and multimodal LLMs (MLLMs), encompassing six semantic dimensions (intent, emotion, dialogue act, sentiment, speaking style, and communication behavior) across nine datasets. - The benchmark comprises over 61K multimodal utterances with text, video, and audio modalities, sourced from diverse scenarios and evaluated using zero-shot inference, supervised fine-tuning, and instruction tuning on various LLMs and MLLMs. - Experimental results indicate that MLLMs, particularly after fine-tuning, outperform LLMs, showcasing their potential to leverage non-verbal cues for comprehending complex semantics, with small-scale (7B) MLLMs showing comparable performance to larger ones (72B) after training. Instruction tuning enables foundation models to perform various tasks effectively, even exceeding human performance on specific tasks like intent recognition. - Despite improvements with fine-tuning, the best performing MLLM only reached 69.18% accuracy, highlighting the complexity of high-level semantic understanding in MMLA and setting a new benchmark for future research. - The datasets and code for MMLA are open-sourced to facilitate advancement in the field. | ['Multimodal', 'Natural Language Processing'] | [Link](https://github.com/thuiar/MMLA) | N/A |
| [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768) | Kelly Marchisio, Sebastian Ruder, Renjie Huang, Robert Li, Piotr Nawrot | - This paper explores training-free sparse attention methods to improve long-context capabilities in Transformer LLMs, conducting the largest-scale empirical analysis to date across various model sizes, sequence lengths, and sparsity levels. - The study finds that for very long sequences, larger, highly sparse models outperform smaller, dense models under the same computational budget and that higher sparsity is tolerable during decoding than prefilling, correlating with model size. - There's no single best-performing sparse attention strategy across all tasks; the ideal approach varies depending on the task type and inference phase. - Even moderate sparsity can significantly degrade performance on some tasks, emphasizing the need for careful trade-off evaluation and task-specific customization. - The research introduces novel scaling laws for sparse attention, demonstrating that the findings likely generalize beyond the experimental setup, indicating potential for sparse attention to be instrumental for efficient long-sequence processing in future LLMs. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/PiotrNawrot/sparse-frontier) | N/A |
| [DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large
  Language Models](https://arxiv.org/abs/2504.15716) | Lifan Guo, Junhui Li, Huaixia Dou, Qian Chen, amazingj | - DianJin-R1 is a new reasoning-enhanced framework for Large Language Models (LLMs) designed to improve financial reasoning through reasoning-augmented supervision and reinforcement learning. - The framework uses a new high-quality dataset, DianJin-R1-Data, constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), and fine-tunes Qwen2.5-7B/32B-Instruct using a structured output format that generates reasoning steps and final answers. - Group Relative Policy Optimization (GRPO) is employed with dual reward signals, one for encouraging structured outputs and another rewarding answer correctness. - DianJin-R1 models outperform non-reasoning counterparts on financial datasets (CFLUE, FinQA, CCC) and general reasoning benchmarks (MATH-500, GPQA-Diamond). - On the real-world CCC dataset, DianJin-R1 achieves performance comparable to multi-agent systems at a lower computational cost. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/aliyun/qwen-dianjin) | [Link](https://huggingface.co/DianJin) |


## Papers for 2025-04-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/abs/2504.17761) | Peng Xing, Yucheng Han, Shiyu Liu, skicy, wchengad | - Step1X-Edit is an open-source, general-purpose image editing model that leverages a Multimodal Large Language Model (MLLM) and a Diffusion in Transformer (DiT) architecture. - The MLLM processes the image and text instruction, generating editing tokens that guide the DiT to produce the edited image.  - A novel dataset generation pipeline and a new benchmark called GEdit-Bench support training and evaluation.  - Experimental results on GEdit-Bench show Step1X-Edit substantially outperforming existing open-source methods and achieving comparable performance to closed-source models like GPT-40 and Gemini. - Step1X-Edit combines the advantages of both MLLMs and DiTs, enabling general image edits based on complex instructions while maintaining image fidelity. | ['Image-to-Image', 'Multimodal'] | [Link](https://github.com/stepfun-ai/Step1X-Edit) | N/A |
| [Paper2Code: Automating Code Generation from Scientific Papers in Machine
  Learning](https://arxiv.org/abs/2504.17192) | Sung Ju Hwang, Seongyun Lee, jinheon, iaminju | - Paper2Code is a multi-agent LLM framework that transforms machine learning papers into functional code repositories, addressing the reproducibility challenge in ML research by automating code generation directly from research papers. - The framework operates in three stages: planning (creating a roadmap, system architecture, and configuration files), analysis (interpreting implementation details), and generation (producing modular code). - Each stage utilizes specialized LLM agents designed for effective collaboration, enabling the system to emulate the typical workflow of human developers. - Evaluation on a Paper2Code benchmark of papers from top-tier venues and the PaperBench benchmark shows PaperCoder significantly outperforms baselines in generating valid and faithful code. - Human evaluations by original paper authors show 77% rate PaperCoder as best, with 85% finding the generated repositories helpful, and analysis indicates high executability with minor modifications. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Breaking the Modality Barrier: Universal Embedding Learning with
  Multimodal LLMs](https://arxiv.org/abs/2504.17432) | Yanzhao Zhang, Xingjun Wang, Ziyong Feng, Tiancheng Gu, Kaichengalex | - UniME, a two-stage framework, leverages Multimodal Large Language Models (MLLMs) to learn universal representations for various vision-language tasks. - The first stage, Textual Discriminative Knowledge Distillation, uses a powerful LLM-based teacher model to enhance the MLLM's language component's embedding capabilities. - The second stage, Hard Negative Enhanced Instruction Tuning, improves discriminative representation learning by filtering false negatives and using hard negative sampling. - Evaluations on the MMEB benchmark and multiple retrieval tasks show UniME achieves state-of-the-art performance, demonstrating strong discriminative and compositional understanding. - UniME surpasses existing models like E5-V and VLM2Vec on tasks such as short & long caption retrieval and compositional retrieval, showcasing its robust representation learning capabilities. | ['Multimodal', 'Image Feature Extraction', 'Image-to-Text'] | N/A | N/A |
| [Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery
  Simulation](https://arxiv.org/abs/2504.17207) | Leonidas Guibas, Mikaela Angelina Uy, Chanho Park, Jihyeon Je, Phillip Y. Lee | - This paper introduces Abstract Perspective Change (APC), a framework for enhancing perspective-aware spatial reasoning in Vision-Language Models (VLMs). - APC simulates the mental imagery process by constructing a 3D scene abstraction from an input image and question, using vision foundation models for object detection, segmentation, and orientation estimation. - The framework then transforms the scene abstraction to align with the perspective of a designated reference object in the image, converting the allocentric reasoning task into an egocentric one. - This transformed scene is then presented to the VLM as either a textual prompt with 3D coordinates or a rendered visual prompt depicting the scene from the new perspective. - Experiments on synthetic and real-world datasets show APC significantly outperforms existing VLMs and spatial reasoning models, demonstrating its effectiveness in handling alternative viewpoints. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://apc-vlm.github.io/) |
| [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM
  Pretraining](https://arxiv.org/abs/2504.16511) | Yifan Zhang, Zhimiao Yu, Binbin Liu, Weidong Zhou, Fengze Liu | - QuaDMix, a novel data selection framework, jointly optimizes data quality and diversity for Large Language Model (LLM) pretraining, addressing the trade-off between these two crucial aspects. - The framework employs multiple quality scorers and domain classification to label the pretraining data, and then utilizes a parameterized sampling function to determine the sampling frequency of each data point based on its quality and domain. - QuaDMix uses simulated experiments with smaller models and a LightGBM regressor to efficiently search for optimal parameters within the framework, reducing the computational cost of large-scale training. - Experiments on diverse models and datasets show that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks, outperforming methods that focus solely on quality or diversity. - The results highlight QuaDMix's ability to effectively balance data quality and diversity for enhanced LLM pretraining. | ['Natural Language Processing'] | N/A | N/A |
| [Token-Shuffle: Towards High-Resolution Image Generation with
  Autoregressive Models](https://arxiv.org/abs/2504.17789) | Chih-Yao Ma, Hao Tang, Haoyu Ma, Peize Sun, Xu Ma | - Token-Shuffle, a novel method reduces the number of image tokens in Transformers, enabling efficient high-resolution image generation within Multimodal Large Language Models (MLLMs). - It leverages the dimensional redundancy of visual vocabularies in MLLMs by merging spatially local tokens along the channel dimension (token-shuffle) and restoring the spatial arrangement after processing (token-unshuffle). - This approach allows for the generation of images up to 2048x2048 resolution using a unified next-token prediction framework, maintaining efficient training and inference. - The 2.7B model achieves a 0.77 overall score on hard prompts in the GenAI-benchmark, outperforming other autoregressive and diffusion models. - Large-scale human evaluations further demonstrate its superior capabilities in text-alignment, visual flaw handling, and overall visual appearance. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040) | Heng Ji, Silvio Savarese, Caiming Xiong, Senthil Purushwalkam, Zhenhailong Wang | - DyMU is a training-free framework that dynamically reduces the computational burden of Vision-Language Models (VLMs) by decreasing the number of visual tokens based on image complexity. - Dynamic Token Merging (DToMe) component merges similar visual tokens based on image complexity, and Virtual Token Unmerging (VTU) reconstructs the attention dynamics of a full token sequence for the language model. - Experiments on image and video understanding tasks show DyMU reduces visual tokens by 32-85% while maintaining comparable performance to full-length models, across diverse VLM architectures. - This method is training-free and readily applicable to any VLM architecture. - The approach provides greater control over computational cost and allows combination of visual reasoning tools and DYMU to further improve efficiency while maintaining performance. | ['Multimodal', 'Image Feature Extraction', 'Visual Question Answering'] | [Link](https://mikewangwzhl.github.io/dymu) | N/A |
| [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921) | Areg Mikael Sarvazyan, Álvaro Romo Herrero, Ian Borrego Obrador, José Ángel González, mchinea | - IberBench, a comprehensive benchmark designed to evaluate Large Language Models (LLMs) on both fundamental and industry-relevant Natural Language Processing (NLP) tasks in Iberian languages. - The benchmark includes 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment analysis, toxicity detection, and summarization. - Addresses limitations in current evaluation practices, such as lack of linguistic diversity and static evaluation by enabling continuous updates and community submissions. - Evaluates 23 LLMs ranging from 100 million to 14 billion parameters, finding that LLMs perform better in fundamental tasks than in industry-relevant tasks and Galician and Basque present greater challenges. - Offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification', 'Summarization'] | [Link](https://github.com/IberBench/iberbench-evaluation) | [Link](https://huggingface.co/spaces/iberbench/leaderboard) |
| [ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](https://arxiv.org/abs/2504.15921) | Mariano Beguerisse-Diaz, Shaogang Gong, Dimitrios Korkinof, Jian Hu | - ViSMaP (Video Summarisation by Meta-Prompting) is introduced for unsupervised hour-long video summarization, leveraging a three-stage approach involving short-form video learning, pseudo-summary generation with LLMs, and hour-long video adaptation. - The model employs TimeSformer as a feature encoder, DistilBERT for visual-language alignment, and GPT2 as a text decoder, utilizing cross-entropy and contrastive loss during training and symmetric cross-entropy loss during adaptation. - ViSMaP utilizes a novel iterative meta-prompting process to generate and refine pseudo-summaries for long videos, utilizing GPT-3.5 as optimizer and generator, and Gemini as evaluator to select key information from short video descriptions. - Experimental results on Ego4D-HCap show performance comparable to state-of-the-art supervised methods, outperforming zero-shot models and unsupervised baselines. - Further evaluations on MSRVTT, MSVD, and YouCook2 demonstrate generalization capabilities across diverse video captioning datasets, achieving results comparable to or exceeding supervised models. | ['Video-Text-to-Text', 'Summarization', 'Computer Vision'] | N/A | N/A |
| [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming
  Videos](https://arxiv.org/abs/2504.17343) | Shuhuai Ren, Lei Li, Yuancheng Wei, Yicheng Li, Linli Yao | - This paper introduces TimeChat-Online, an online Video Large Language Model (VideoLLM) designed for real-time interaction with streaming video content. - The core component is the Differential Token Drop (DTD) module, inspired by the Change Blindness phenomenon, which selectively preserves significant temporal changes in video frames while discarding redundant static visual information, achieving 82.8% token reduction. - TimeChat-Online maintains over 98% performance compared to full-token models on StreamingBench and shows a 5.7-point accuracy improvement on the VideoMME long set (30-60 minute videos) when integrated with Qwen2.5VL-7B. - It incorporates proactive responding, triggered by scene transitions detected by analyzing the token drop ratio curve, allowing the model to anticipate and answer questions related to future video content. - A new instruction-tuning dataset, TimeChat-Online-139K, is introduced, featuring diverse question-answer pairs specifically designed for streaming video question-answering scenarios including backward-tracing, current-time perception, and future-responding. | ['Video-Text-to-Text', 'Multimodal'] | N/A | [Link](https://timechat-online.github.io) |
| [Process Reward Models That Think](https://arxiv.org/abs/2504.16828) | Hao Peng, Jaekyeom Kim, Lajanugen Logeswaran, Rishabh Agarwal, Muhammad Khalifa | - This paper introduces THINKPRM, a generative process reward model (PRM) for verifying step-by-step reasoning, trained with minimal supervision on synthetic data using chain-of-thought (CoT) reasoning. - THINKPRM leverages the inherent reasoning capabilities of large language models (LLMs) and is fine-tuned on significantly fewer process labels than discriminative PRMs. - Experimental results demonstrate that THINKPRM outperforms both discriminative PRMs and LLM-as-a-Judge baselines across various benchmarks, including ProcessBench, MATH-500, and AIME '24, using only 1% of the process labels. - THINKPRM also excels in out-of-domain tasks like GPQA-Diamond and LiveCodeBench, showcasing its robustness and generalization ability. - The work highlights the potential of generative long CoT PRMs for scaling test-time compute for verification while requiring minimal supervision for training. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/mukhal/thinkprm) | N/A |


## Papers for 2025-04-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal
  Large Language Models](https://arxiv.org/abs/2504.15279) | Einsiedler, luotto, Weiyun1025, GenuineWWD, wilye | - This research introduces VisuLogic, a new benchmark designed for evaluating visual reasoning abilities in multimodal large language models (MLLMs). - VisuLogic contains 1,000 human-verified visual reasoning problems across six categories, focusing on pure visual reasoning, unlike existing benchmarks that permit shortcuts through text descriptions.  - Evaluations revealed that SOTA MLLMs performed poorly, scoring below 30% accuracy, significantly lower than human performance (51.4%). - A supplementary training dataset and reinforcement learning baseline were developed and showed improved performance compared to open-source and closed-source MLLMs, showcasing RL's potential in enhancing visual reasoning.  - All code and data are publicly available to support further research. | ['Multimodal', 'Visual Question Answering', 'Computer Vision'] | [Link](https://visulogic-benchmark.github.io/VisuLogic) | N/A |
| [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431) | Suyeong An, hist0613, kyudolski, scottsuk0306, sungjunhan-trl | - Introduces Trillion-7B, a Korean-centric multilingual large language model (LLM) that uses a novel Cross-lingual Document Attention (XLDA) mechanism for efficient knowledge transfer from English to other languages. - The model architecture is based on a Transformer decoder with ROPE, SwiGLU, and RMSNorm, consisting of 32 layers with a hidden size of 4096 and a feedforward dimension of 11008. - Achieves competitive multilingual performance using only 10% of its 2 trillion training tokens for multilingual data, trained with a cost of $148K. - Evaluations across 27 benchmarks in four languages demonstrate the model's robust multilingual performance and cross-lingual consistency, particularly in Korean and instruction following. - Demonstrates zero-shot cross-lingual transfer to vision modalities with Trillion-LLaVA, which outperforms other vision-language models on Korean benchmarks despite being trained only on English vision-language data. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Translation'] | N/A | N/A |
| [Pre-DPO: Improving Data Utilization in Direct Preference Optimization
  Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843) | Yue Zhang, Qiji Zhou, Shulin Huang, Junshu Pan, Swtheking | - This paper introduces Pre-DPO, a novel training paradigm for Direct Preference Optimization (DPO) that leverages a "guiding reference model" to enhance data utilization and improve performance in aligning large language models (LLMs). - Pre-DPO initializes training with a standard preference optimization method like DPO or SimPO, then re-optimizes the policy using the initial optimized model as a guiding reference, leading to adaptive data reweighting. - The guiding reference model assigns higher weights to more suitable training samples and lower weights to less suitable or conflicting samples, effectively transforming the role of the reference model from a static constraint to a dynamic guide. - Experiments on the Llama3.2, Qwen2.5 models, and AlpacaEval 2 and Arena-Hard benchmarks show that Pre-DPO consistently improves performance of both DPO and SimPO without external models or additional data. - Pre-DPO addresses the limitations of conventional reference models by utilizing the optimized policy as the reference, providing foresight and improving data reweighting during training. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/DtYXs/Pre-DPO) | [Link](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k), [Link](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) |
| [Decoupled Global-Local Alignment for Improving Compositional
  Understanding](https://arxiv.org/abs/2504.16801) | Ziyong Feng, Jun Wang, haoranxu, Kaichengalex, xiaoxing2001 | - This paper introduces Decoupled Global-Local Alignment (DeGLA), a framework designed to enhance the compositional understanding of Contrastive Language-Image Pre-training (CLIP) models while mitigating the loss of general capabilities often observed in existing methods. - DeGLA incorporates a self-distillation mechanism during global alignment, using a frozen teacher model derived from an exponential moving average to guide the learning of a student model, thus preserving pre-trained knowledge. - For local alignment, the framework leverages in-context learning capabilities of Large Language Models (LLMs) to construct a dataset of approximately 2 million high-quality negative captions. - It then introduces Image-Grounded Contrast (IGC) and Text-Grounded Contrast (TGC) losses to further refine compositional understanding by attracting and repelling image and text embeddings based on their alignment. - Experimental results show that DeGLA achieves state-of-the-art performance on compositional reasoning benchmarks (VALSE, SugarCrepe, and ARO) while improving zero-shot classification accuracy by an average of 13% across 11 datasets compared to previous state-of-the-art methods. | ['Multimodal', 'Zero-Shot Classification', 'Zero-Shot Image Classification', 'Computer Vision'] | [Link](https://github.com/xiaoxing2001/DeGLA) | N/A |
| [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777) | Ollie Liu, Enes Burak Bilgin, Ömer Faruk Akgül, Julian Asilis, upup-ashton-wang | - Tina, a family of tiny reasoning models, demonstrates substantial reasoning performance can be achieved with minimal resources by applying parameter-efficient updates during reinforcement learning (RL) using low-rank adaptation (LoRA) to a small 1.5B parameter base model. - The best Tina model achieves a >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24 at only $9 USD post-training and evaluation cost, an estimated 260x cost reduction compared to existing SOTA models. - The authors hypothesize LoRA's effectiveness stems from rapidly adapting the model to the structural format of reasoning rewarded by RL, while preserving the base model's underlying knowledge. - This "rapid reasoning format adaptation" hypothesis is supported by observations of a training phase transition where format-related metrics peak or destabilize just before optimal performance is reached, while accuracy rewards show more gradual trends. - All code, training logs, model weights, and checkpoints are open-sourced to promote accessibility and reproducibility. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/shangshang-wang/Tina) | [Link](https://huggingface.co/Tina-Yi) |
| [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training
  and Deployment](https://arxiv.org/abs/2504.15585) | Guibin Zhang, Kun Wang, Ningyu, Atarogic, Fred456 | - This paper presents a comprehensive survey of Large Language Model (LLM) safety, introducing the concept of "full-stack" safety, encompassing the entire LLM lifecycle from data preparation to commercialization and usage. - The survey analyzes safety and security risks across different stages, including data poisoning, privacy leakage, misalignment, jailbreak attacks, and vulnerabilities in LLM-based agent systems. - The research is based on an extensive review of 800+ papers and offers insights into mitigation strategies, defense mechanisms, and evaluation metrics. - The paper identifies promising research directions like data generation safety, alignment techniques, and robust prompt engineering. - It also provides roadmaps and perspectives for each LLM lifecycle stage, aiming to offer a holistic understanding and guide future research in LLM safety. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/bingreeky/full-stack-llm-safety) | N/A |
| [RePOPE: Impact of Annotation Errors on the POPE Benchmark](https://arxiv.org/abs/2504.15707) | Matthias Hein, YanNeu | - This paper assesses the impact of label errors in the MSCOCO dataset on the POPE benchmark, a standard for evaluating object hallucinations in vision-language models (VLMs). - The authors re-annotate the POPE benchmark images and create RePOPE, a corrected version of the dataset, finding an imbalance in annotation errors across different subsets. - Evaluating multiple VLM models on RePOPE reveals notable shifts in model rankings compared to the original POPE, highlighting the significant impact of label quality. - The paper observes a much higher error rate in the original POPE labels for questions with "yes" answers (9.3%) compared to "no" answers (1.7%). - This biased error distribution significantly affects the F1 scores and rankings of models on the benchmark, calling into question the reliability of some POPE subsets for measuring object hallucination. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/YanNeu/REPOPE) | N/A |
| [Rethinking the Generation of High-Quality CoT Data from the Perspective
  of LLM-Adaptive Question Difficulty Grading](https://arxiv.org/abs/2504.11919) | Keyu Wu, Kunlinliu2, MeiManlin, zcs1234, USTCYu | - This paper introduces a method for generating high-quality Chain-of-Thought (CoT) data by adapting question difficulty to the capabilities of Large Language Models (LLMs). - The approach involves grading question difficulty based on LLM performance and constructing an adaptive question database. - Questions are sampled from this database based on a difficulty distribution and DeepSeek-R1 (671B) is used to generate corresponding CoT data. - This LLM-adaptive CoT data is then used for fine-tuning smaller LLMs, improving their reasoning abilities. - Experiments demonstrate the effectiveness of this method, with models trained on 2k LLM-adaptive CoT data outperforming larger baseline models on mathematical and code generation tasks. | ['Question Answering', 'Text2Text Generation', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/open-r1/codeforces), [Link](https://huggingface.co/AI-MO/NuminaMath-CoT) |
| [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254) | Ziteng Wang, Jia Pan, Robert Zhang, gregdurrett, anirudhkhatry | - This paper introduces CRUST-Bench, a new benchmark for evaluating C-to-safe-Rust transpilation, comprising 100 C repositories with manually-written Rust interfaces and test cases. - CRUST-Bench focuses on evaluating the ability of systems to transpile entire C repositories into safe and idiomatic Rust code, addressing the limitations of previous benchmarks that primarily focus on isolated functions. - The authors evaluate several state-of-the-art large language models (LLMs) on CRUST-Bench and find that safe and idiomatic Rust generation remains challenging, with the best model (OpenAI o1) solving only 15% of tasks in a single-shot setting. - Applying iterative self-repair techniques, such as incorporating compiler error messages and failing test cases, improves performance, achieving up to 37% success rate. - The benchmark and analysis highlight the need for further research in automated code migration and the importance of considering realistic, multi-file scenarios. | ['Text2Text Generation', 'Translation', 'Natural Language Processing'] | [Link](https://github.com/anirudhkhatry/CRUST-bench) | N/A |
| [Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large
  Language Models with CheckboxQA](https://arxiv.org/abs/2504.10419) | Borchmann, sf-mchilinski, mturski | - This paper introduces CheckboxQA, a new dataset designed to evaluate and improve large language models' (LLMs) ability to interpret checkboxes in visually rich documents. - CheckboxQA contains diverse documents with annotated question-answer pairs related to checkbox interpretation, addressing a gap in existing benchmarks that often overlook this critical element. - Current LLMs struggle with checkbox interpretation due to challenges such as the small size and visual subtlety of checkboxes, their context-dependent significance, and limited training data capturing checked vs. unchecked states. - Experiments demonstrate that models trained on data that includes checkbox annotations gain a distinct advantage in this task, highlighting the importance of dedicated resources like CheckboxQA.  - Although improvements are observed, all models still fall short of human performance, highlighting the need for further research in accurately identifying and interpreting checkmarks. | ['Document Question Answering', 'Multimodal'] | [Link](https://github.com/Snowflake-Labs/CheckboxQA) | N/A |
| [Progressive Language-guided Visual Learning for Multi-Task Visual
  Grounding](https://arxiv.org/abs/2504.16145) | Dingjiang Huang, Kunhua Ji, Wenlong Zhang, Hong Wang, jcwang0602 | - This paper introduces PLVL, a Progressive Language-guided Visual Learning framework for Multi-Task Visual Grounding (MTVG) which includes Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). - PLVL progressively injects language information into the visual backbone using a cross-attention mechanism within a local-global group architecture, eliminating the need for an extra cross-modal fusion module. - A collaborative multi-task head based on convolutional layers is used to jointly predict outputs for REC and RES, leveraging the shared central position of identified objects. - The model outperforms state-of-the-art methods on RefCOCO, RefCOCO+, and RefCOCOg benchmarks in both REC and RES tasks under traditional and pre-trained settings. - Experimental results demonstrate improvements, such as 8% reduced time overhead per image compared to EEVG. | ['Multimodal', 'Object Detection', 'Image Segmentation', 'Computer Vision'] | [Link](https://github.com/jcwang0602/PLVL) | N/A |


## Papers for 2025-04-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120) | Omar Hadid, Sara Chrouf, ZeinaD, Moatasem444, Hennara | - This paper introduces Kuwain 1.5B, a compact multilingual Arabic-English language model trained by injecting Arabic into the English-centric TinyLlama 1.1B model, adding 8 new layers and expanding the vocabulary with 26K Arabic tokens. - This method reduces training costs by 70% compared to training a new model from scratch and improves Arabic language performance by 8% while maintaining original English proficiency with only 20% of the original English training data and 1% improvement compared to the base TinyLlama model. - The approach involves freezing the original model's layers and training only the new layers, achieving stability by keeping the final encoder layer trainable.  - Evaluation on Arabic benchmarks shows competitive performance against larger models, demonstrating efficient language model expansion. - A fine-tuned version, Lahajawi, achieves impressive results in Arabic cross-dialect translation, showcasing the method's potential for diverse language tasks. | ['Natural Language Processing', 'Translation', 'Text Generation'] | [Link](https://github.com/misraj-ai/Kuwain-Arabic-cleaner) | [Link](https://huggingface.co/spaces/OALL/Open-Arabic-LLM-Leaderboard) |
| [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521) | Huifeng Yin, Sinuo Liu, Weixuan Wang, Minghao Wu, ChenyangLyu | - This paper examines over 2,000 non-English multilingual benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. - The findings reveal that English remains significantly overrepresented in these benchmarks, most benchmarks rely on original language content rather than translations, and the majority of content is sourced from high-resource countries. - A comparison of benchmark performance with human judgments highlights notable disparities, with STEM-related tasks exhibiting stronger correlations than traditional NLP tasks. - The study finds that translating English benchmarks into other languages is insufficient, as localized benchmarks show significantly higher alignment with local human judgments. - The paper identifies six key limitations in current multilingual evaluation practices, proposes guiding principles for effective multilingual benchmarking, and outlines five critical research directions, including addressing imbalance in Natural Language Generation tasks and improving representation for low-resource languages. | ['Natural Language Processing'] | N/A | N/A |
| [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072) | Yifan Ding, richardaecn, yala, Boyiliee, longlian | - This paper introduces the Describe Anything Model (DAM) for detailed localized image and video captioning.  - DAM uses a focal prompt for high-resolution encoding of target regions and a localized vision backbone to integrate precise localization with broader context.  - It also introduces a semi-supervised learning data pipeline (DLC-SDP) to address data scarcity and DLC-Bench, a benchmark to evaluate detailed localized captioning without reference captions.  - DAM achieves state-of-the-art performance on seven benchmarks, including keyword, phrase, and detailed multi-sentence localized captioning for images and videos.  - The model surpasses strong API-only baselines like GPT-40 and o1. | ['Image-to-Text', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/richardaecn/describe-anything) | [Link](describe-anything.github.io) |
| [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466) | Charlie Snell, Long Lian, Jiayi Pan, yala, xiuyul | - This paper introduces Adaptive Parallel Reasoning (APR), a novel framework that allows language models to dynamically switch between serial and parallel computation during inference, improving reasoning capabilities. - APR utilizes a parent-child threading mechanism where parent threads can spawn child threads to explore different reasoning paths concurrently, returning results via a join operation, and it is optimized using reinforcement learning. - Experiments on the Countdown reasoning task show APR achieves higher accuracy within the same context window (83.4% vs. 60.0% at 4k context), better scalability with increased compute (80.1% vs. 66.6% at 20k tokens), and improved accuracy at equivalent latency (75.2% vs. 57.3% at ~5000ms) compared to serial methods. - APR's reinforcement learning focuses on optimizing when and how to parallelize, improving performance by increasing both sequence length and the number of child threads rather than just deepening the search. - The efficiency improvements stem from APR's ability to distribute reasoning across multiple threads, reducing the pressure on context window limitations and enabling exploration of more complex solutions. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Parallel-Reasoning/APR) | N/A |
| [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning
  in Multimodal LLMs](https://arxiv.org/abs/2504.15415) | Yifan Yao, Jarvis Guo, Yuanxing Zhang, JinChengRen, mdh98 | - IV-Bench is introduced as the first comprehensive benchmark designed for evaluating image-grounded video perception and reasoning capabilities of Multimodal Large Language Models (MLLMs). - The benchmark comprises 967 videos and 2,585 image-text queries across 13 tasks, categorized into perception and reasoning, and spanning five representative video categories. - Evaluations conducted on 27 state-of-the-art MLLMs, including both open and closed-source models, reveal that existing models struggle with these tasks, achieving a maximum accuracy of only 28.9%. - Larger models demonstrate moderate performance gains compared to smaller models, and factors like the number of frames and video resolution are shown to impact model performance. - A simple data synthesis approach is also proposed, suggesting that data format is not the sole challenge of IV-Bench tasks. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/multimodal-art-projection/IV-Bench) | N/A |
| [BookWorld: From Novels to Interactive Agent Societies for Creative Story
  Generation](https://arxiv.org/abs/2504.14538) | Yanghua Xiao, Jiaqing Liang, Tian Qiu, Xintao Wang, Yiting Ran | - BookWorld is introduced, a novel system designed for constructing and simulating book-based multi-agent societies for creative story generation. - The system extracts character data and background knowledge from source books, constructs a multi-agent system where characters interact within a simulated world, and uses LLMs to weave the simulation's events into novel-style narratives. - BookWorld incorporates a specialized method for extracting worldview details from source books and a dynamic attribute updating mechanism enabling characters to evolve while maintaining fidelity to their original traits. - Experimental results demonstrate that BookWorld generates high-quality, creative stories while maintaining fidelity to the source material, outperforming direct generation and a previous story generation method (HoLLMwood) in 75.36% of cases. - The system offers diverse applications, including interactive story generation and social simulation within fictional worlds, opening new possibilities for creative exploration and enjoyment of beloved literary works. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992) | Jianqiao Lu, Sijun Zhang, Shen Yan, Taoer, bongbohong | - The paper introduces PHD-Transformer, a novel framework for efficient length scaling during pre-training of large language models (LLMs) while preserving inference efficiency. - PHD-Transformer employs a KV cache management strategy that differentiates between original and hidden decoding tokens, retaining only the KV cache of original tokens for long-range dependencies. - The paper introduces two optimized variants: PHD-SWA uses sliding window attention to maintain local dependencies, and PHD-CSWA implements chunk-wise sliding window attention to reduce pre-filling time. - Experiments demonstrate consistent improvements across benchmarks like ARC, HellaSwag, PIQA, Winogrande, MMLU, and CommonsenseQA. - PHD-CSWA-3-16-32 shows an average 2.0% accuracy improvement and 0.034 decrease in training loss compared to the baseline 1.2B model. | ['Natural Language Processing'] | N/A | N/A |
| [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030) | Zejun Ma, Wei Li, Yiqi Lin, Ziyun Zeng, Joya Chen | - This paper introduces LiveCC, a novel Video Large Language Model (Video LLM) trained on a large-scale dataset of ASR transcripts, focusing on real-time video commentary. - A streaming training approach densely interleaves ASR words and video frames according to their timestamps, enabling fine-grained temporal modeling. - LiveCC-7B-Instruct outperforms existing 72B parameter models in commentary quality and achieves SOTA on video QA benchmarks. - A new benchmark, LiveSports-3K, is introduced for evaluating real-time video commentary using LLM-as-a-judge. - The proposed model showcases low-latency commentary generation capabilities and broad generalizability in video understanding tasks. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](showlab.github.io/livecc) | N/A |
| [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681) | Fan Chen, Chia-Wen Kuo, Celong Liu, Vidi Team, daviddousa | - Vidi is a family of Large Multimodal Models (LMMs) designed for video understanding and editing tasks, specializing in temporal retrieval. - The model identifies time ranges in videos corresponding to given text queries, handling hour-long videos and multiple modalities (vision, audio, text). - Vidi utilizes a Decomposed Attention (D-Attn) architecture, enabling efficient processing of long videos and facilitating multimodal alignment. - It is trained in three stages: adapter training on image/audio captions, synthetic data training for temporal alignment, and real video training with dense captions and subtitles. - Evaluation on the VUE-TR benchmark demonstrates that Vidi surpasses leading proprietary models, showcasing its efficacy in temporal retrieval tasks. | ['Video-Text-to-Text', 'Multimodal', 'Video Classification'] | N/A | N/A |
| [LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making
  Abilities](https://arxiv.org/abs/2504.16078) | Razvan Pascanu, Markus Wulfmeier, Jordi Grau-Moya, Jörg Bornschein, Thomas Schmied | - This paper investigates why Large Language Models (LLMs) often exhibit suboptimal performance in decision-making scenarios, focusing on greediness, frequency bias, and the knowing-doing gap as key failure modes. - It proposes Reinforcement Learning Fine-Tuning (RLFT) on self-generated Chain-of-Thought (CoT) rationales to mitigate these shortcomings, demonstrating improved decision-making abilities in multi-armed bandits, contextual bandits, and Tic-tac-toe environments. - The study shows that RLFT enhances exploration and narrows the knowing-doing gap, but also explores classic and LLM-specific exploration mechanisms to further improve RLFT effectiveness. - Experiments with different model sizes (2B, 9B, 27B) reveal that RLFT consistently improves performance across various tasks and scales, indicating its potential for enhancing LLM agents. - Ablation studies highlight the importance of CoT reasoning, expert data, and providing sufficient "thinking" tokens for achieving optimal performance. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082) | Yu-Xiong Wang, Ziqi Pang | - MR. Video is a new framework for long video understanding based on the MapReduce principle: independently perceiving short video clips (Map) and jointly aggregating information (Reduce). - It addresses limitations of sequence-to-sequence Vision-Language Models (VLMs) and video agents, strategically performing detailed short video perception without context length constraints and enabling sequence-parallel perception with comprehensive context aggregation. - Employs two MapReduce stages: (A) Captioning: generates short video captions and standardizes repeated elements; (B) Analysis: analyzes information from individual videos for each question and integrates them into a final answer. - Leverages Large Language Models (LLMs), specifically Gemini-Flash for vision and GPT-4 for text, achieving over 10% accuracy improvement on LVBench compared to state-of-the-art VLMs and video agents. - Demonstrates the effectiveness of MapReduce for long video understanding, suggesting a simple yet powerful approach for processing lengthy video content. | ['Video-Text-to-Text', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/ziqipang/MR-Video) | N/A |
| [Progent: Programmable Privilege Control for LLM Agents](https://arxiv.org/abs/2504.11703) | Hongwei Li, Linyu Wu, Zhun Wang, Jingxuan He, stneng | - Progent, a new privilege control framework for Large Language Model (LLM) agents, is introduced to mitigate security risks associated with these agents interacting with external environments. - Progent employs a domain-specific language (DSL) based on JSON Schema, enabling developers to define fine-grained access control policies, specifying permissible tool calls and their conditions, along with fallback actions for disallowed calls. - These policies are enforced during agent execution, dynamically updated based on agent feedback and new information, and managed either manually or through automated policy generation and updates using LLMs familiar with JSON. - Evaluation across various scenarios, including AgentDojo, ASB, and AgentPoison benchmarks, demonstrates Progent's effectiveness in drastically reducing attack success rates (e.g., from 41.2% to 2.2% on AgentDojo) with minimal impact on agent utility, and even enhancing it in some cases. - Further analysis reveals Progent's modular design, enabling easy integration with minimal code changes, and showcases its resilience against adaptive attacks targeting policy generation LLMs. | ['Natural Language Processing'] | [Link](https://github.com/sunblaze-ucb/progent) | N/A |
| [IPBench: Benchmarking the Knowledge of Large Language Models in
  Intellectual Property](https://arxiv.org/abs/2504.15524) | Minghui Zhu, Huaren Liu, Hongbo Wang, Guhong Chen, QiYao-Wang | - Introduces IPBench, a benchmark designed to evaluate the knowledge and capabilities of Large Language Models (LLMs) in intellectual property applications. - Covers 8 IP mechanisms and 20 tasks, including information processing, logical reasoning, discriminant evaluation, and creative generation. - A benchmark of 16 LLMs showed that even the best-performing model only reached 75.8% accuracy, demonstrating substantial room for improvement. - Open-source IP and law-oriented models lag behind closed-source general-purpose models. - IPBench data and code are publicly available, and it will be updated in the future. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/IPBench/IPBench) |
| [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via
  Occluded Object Counting](https://arxiv.org/abs/2504.15485) | Mohit Bansal, Jaemin Cho, Elias Stengel-Eskin, Atin Pothiraj | - This paper introduces CAPTURE (Counting Amodally for Patterns Through Unseen REgions), a novel benchmark designed to evaluate the spatial reasoning and world modeling capabilities of Vision Language Models (VLMs) in the presence of occlusions. - CAPTURE involves presenting VLMs with images of objects arranged in patterns, where some objects are occluded, and requires the models to count the total number of objects by inferring the continuation of the pattern behind the occluder. - The benchmark includes two datasets: CAPTUREreal, consisting of real-world images, and CAPTUREsynthetic, a controlled set of generated images to isolate specific factors influencing performance. - Experiments with strong VLMs like GPT-40 reveal that these models struggle with amodal counting, performing worse on occluded compared to unoccluded images, while humans achieve near-perfect accuracy on the task.  - Providing auxiliary information, such as object coordinates or inpainted occluded regions, significantly improves VLM performance, indicating a weakness in visual world modeling and integrating visual and textual information rather than counting itself. | ['Multimodal', 'Computer Vision', 'Visual Question Answering'] | [Link](https://github.com/atinpothiraj/CAPTURE) | N/A |
| [DiffVox: A Differentiable Model for Capturing and Analysing Professional
  Effects Distributions](https://arxiv.org/abs/2504.14735) | Wei-Hsiang Liao, Ben Hayes, Junghyun Koo, Marco A. Martínez-Ramírez, yoyolicoris | - This paper introduces DiffVox, a differentiable model for capturing and analyzing professional vocal effects distributions in music production.  - DiffVox integrates parametric equalization, dynamic range control, delay, and reverb with efficient differentiable implementations, enabling gradient-based optimization for parameter estimation. - Analysis of parameter correlations from two datasets (MedleyDB and a private collection) reveals relationships between effects and parameters, such as delay time correlating with intensity and high-pass/low-shelf filters shaping low-end frequencies. - Principal component analysis connects these correlations to McAdams' timbre dimensions, highlighting the influence of spaciousness and spectral brightness. - The study confirms the non-Gaussian nature of parameter distributions, setting the foundation for future research in vocal effects modeling and automatic mixing. | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/SonyResearch/diffvox) | N/A |


## Papers for 2025-04-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [FlowReasoner: Reinforcing Query-Level Meta-Agents](https://arxiv.org/abs/2504.15257) | P2333, bhooi, dreamerdeo, yueliu1999, HongchengGao | - This paper introduces FLOWREASONER, a query-level meta-agent designed to automate the creation of multi-agent systems tailored to individual user queries, enhancing adaptability compared to traditional task-level approaches. - FLOWREASONER employs a reasoning-driven approach, leveraging external execution feedback and reinforcement learning to optimize workflows without relying on complex search algorithms or predefined search sets. - A multi-purpose reward function guides the RL training, focusing on performance, complexity, and efficiency of the generated multi-agent systems. - Experimental results on code generation benchmarks show FLOWREASONER-14B outperforming all baselines, including manually designed and existing automated workflow methods, achieving a 5 percentage point improvement over the strongest baseline (MaAS) and a 10.52% improvement over its underlying worker model (o1-mini) across three benchmarks. - The model demonstrates generalization capability by effectively adapting its planning strategies to different worker models and exhibits flexibility in workflow structure and granularity based on task complexity. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/sail-sg/FlowReasoner) | N/A |
| [Eagle 2.5: Boosting Long-Context Post-Training for Frontier
  Vision-Language Models](https://arxiv.org/abs/2504.15271) | WonminByeon, deahuang, lulidong, RealZhiqiLi, cg1177 | - Eagle 2.5 is a family of frontier vision-language models (VLMs) designed for long-context multimodal learning, addressing challenges in video and high-resolution image understanding. - It uses a generalist architecture based on LLaVA, using an MLP projection to align vision embeddings from SigLIP with the LLM representation space, and employs image tiling for any-resolution image inputs. -  Eagle 2.5 introduces an information-first sampling strategy (including Image Area Preservation and Automatic Degradation Sampling) and progressive mixed post-training. - It leverages a diverse data recipe, combining open-source data with the new Eagle-Video-110K dataset designed for long video understanding, featuring hierarchical story-level and clip-level annotations. - Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching top commercial models like GPT-40 and open-source models such as Qwen2.5-VL-72B and InternVL2.5-78B, showing robust scaling with increased frames. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Image-Text-to-Text'] | N/A | N/A |
| [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958) | Cheng Qian, Gokhantur, XtremSup, Merlin-Hongru, emrecanacikgoz | - This paper introduces ToolRL, a novel approach for enhancing tool-integrated reasoning in Large Language Models (LLMs) using reinforcement learning, specifically Group Relative Policy Optimization (GRPO). - The authors propose a principled reward design framework tailored for tool use tasks, incorporating both structural (format) and semantic (correctness) components. - ToolRL consistently outperforms supervised fine-tuning and other RL baselines by 17% and 15% respectively across multiple tool use and question-answering benchmarks. - The trained model exhibits strong generalization to unseen scenarios and task objectives, along with emergent behaviors such as proactiveness and metacognitive reasoning. - Comprehensive analysis of different reward strategies reveals the importance of dynamic reward scaling and fine-grained reward decomposition for effective tool learning. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/qiancheng0/ToolRL) | N/A |
| [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203) | hamidpalangi, mparvez, genglinliu, liweijiang, salmannyu | - Introduces X-Teaming, a multi-turn jailbreaking framework that utilizes collaborative agents for planning, executing, and optimizing attacks against language models (LMs). - Achieves state-of-the-art multi-turn jailbreak success rates (up to 98.1%) across diverse LMs, including a 96.2% success rate against Claude 3.7 Sonnet. - Generates XGuard-Train, a 30K multi-turn safety training dataset 20x larger than previous resources, enabling improved multi-turn safety alignment for LMs. - Shows that models fine-tuned on XGuard-Train exhibit a 28.3% improvement in multi-turn attack resistance while maintaining single-turn safety and general capabilities. - Open-sources the framework, dataset, and trained models to facilitate the development of robust multi-turn defenses for conversational AIs. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/marslabucla/XGuard-Train) |
| [UFO2: The Desktop AgentOS](https://arxiv.org/abs/2504.14603) | rujiawang, liqul, duchao, shilhe, vyokky | - UFO² is a novel multi-agent operating system for Windows desktops designed for robust and practical computer-using agent (CUA) automation. - It features a centralized HostAgent for task coordination and specialized AppAgents for application-specific interactions, using native APIs and a hybrid GUI-API action layer for improved control and efficiency. - UFO² incorporates a hybrid control detection pipeline combining Windows UI Automation (UIA) with visual grounding, continuous knowledge integration from documentation and execution logs, and speculative multi-action planning for reduced latency. - Evaluation across 20 real-world Windows applications shows UFO² outperforms existing CUAs in robustness and accuracy, achieving a 10% higher task completion rate than the best-performing CUA, Operator, with a 50% relative improvement using deeper OS-level integration. - A Picture-in-Picture (PiP) interface allows for non-disruptive automation within an isolated virtual desktop. | ['Multimodal', 'Natural Language Processing'] | [Link](https://github.com/microsoft/UFO/) | N/A |
| [Seeing from Another Perspective: Evaluating Multi-View Understanding in
  MLLMs](https://arxiv.org/abs/2504.15280) | Shengbang Tong, yubei, chengtim, ch-chenyu, danielchyeh | - This paper introduces All-Angles Bench, a new benchmark designed to evaluate the multi-view understanding capabilities of Multimodal Large Language Models (MLLMs). - The benchmark consists of over 2,100 human-annotated multi-view question-answer pairs across 90 diverse real-world scenes, categorized into six tasks: counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation. - Experiments on 27 representative MLLMs reveal a substantial performance gap between current models and human-level proficiency, particularly in tasks involving cross-view correspondence for partially occluded views and establishing coarse camera poses. - An in-depth analysis suggests that existing MLLMs struggle with identifying the same object across multiple viewpoints, and that current chain-of-thought prompting techniques are insufficient to address these limitations. - The findings highlight the need for domain-specific refinements or specialized modules to enhance multi-view awareness in MLLMs. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/danielchyeh/All-Angles-Bench) | N/A |
| [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient
  Training of Code LLMs](https://arxiv.org/abs/2504.14655) | Yan Wang, Yunhui Xia, chuyi777, jasonkleinlove, Swtheking | - This paper introduces LeetCodeDataset, a benchmark for evaluating and training code-generation LLMs.  - The dataset consists of curated LeetCode Python problems with rich metadata, 100+ test cases per problem, and temporal splits for contamination-free evaluation and efficient supervised fine-tuning (SFT). - Experiments reveal that reasoning models significantly outperform non-reasoning counterparts on this dataset. - SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. - This demonstrates the high-quality and efficiency of the LeetCodeDataset for training code LLMs. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/newfacade/LeetCodeDataset) | [Link](https://huggingface.co/datasets/newfacade/LeetCodeDataset) |
| [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to
  Deliberative Reasoners](https://arxiv.org/abs/2504.14239) | Xavier Hu, Yuhang Liu, xiaotianhan, xieck13, pengxiang | - This paper introduces InfiGUI-R1, a Multimodal Large Language Model (MLLM)-based GUI agent trained with the novel Actor2Reasoner framework, designed to enhance GUI agent reasoning capabilities. - The two-stage framework first injects spatial reasoning into the base MLLM through Spatial Reasoning Distillation, leveraging a teacher model, to transition from a Reactive Actor to a Basic Reasoner.  - The second stage, Deliberation Enhancement, refines the Basic Reasoner into a Deliberative Reasoner using Reinforcement Learning with Sub-goal Guidance and Error Recovery Scenario Construction to bolster planning and reflection.  - InfiGUI-R1 achieves state-of-the-art performance on cross-platform GUI grounding (87.5% on ScreenSpot) and strong results on complex, long-horizon tasks (71.1% on AndroidControl-High), outperforming existing models with comparable or even larger parameters. - This demonstrates the framework's effectiveness in improving agent performance, particularly in tasks requiring complex reasoning and planning. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/Reallm-Labs/InfiGUI-R1) | N/A |
| [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language
  Models](https://arxiv.org/abs/2504.15133) | Linear-Matrix-Probability, HaomingXu, xukewei, Saberlve, xzwnlp | - EasyEdit2 is a framework for controlling Large Language Model (LLM) behavior at test time without modifying model parameters. - It features a new architecture with key modules like the steering vector generator and applier for seamless model steering. - EasyEdit2 supports various interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features, and allows control with a single example. - Experimental results demonstrate EasyEdit2's effectiveness across different LLMs and dimensions, showing improvements over baseline methods and prior work. - The framework is open-sourced with an online demo and video tutorial for easy use and experimentation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zjunlp/EasyEdit) | N/A |
| [LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration
  Benchmark](https://arxiv.org/abs/2504.13805) | dkeeeee, Yuxiang007, zhimingc, Pengxiangzhao, lgy0404 | - LearnAct, a novel multi-agent framework, enhances mobile GUI agent performance through few-shot demonstration learning, addressing long-tail scenarios and personalization gaps. - LearnGUI, the first comprehensive dataset for demonstration-based learning in mobile GUI agents, includes 2,252 offline and 101 online tasks with human demonstrations, enabling personalized adaptation. - LearnAct incorporates three agents: DemoParser extracts knowledge, KnowSeeker retrieves relevant knowledge, and ActExecutor combines instructions, GUI context, and demonstrations for task completion. - Evaluations demonstrate substantial improvements: Gemini-1.5-Pro accuracy increases from 19.3% to 51.7% (198.9% relative improvement) offline, and UI-TARS-7B-SFT online task success rate rises from 18.1% to 32.8% (+14.7%). - The results establish demonstration-based learning as a promising direction for adaptable and personalized mobile GUI agents, offering personalized assistance in diverse scenarios. | ['Multimodal'] | N/A | N/A |
| [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217) | Somayeh Sojoudi, Jonah Casebeer, Njb, Bai-YT | - DRAGON, a novel framework for fine-tuning generative models using distributional rewards, is introduced, allowing optimization of instance-wise, instance-to-distribution, and distribution-to-distribution rewards, unlike traditional RLHF or pairwise preference methods. - Novel reward functions are designed by selecting an encoder (e.g., CLAP) and reference examples, enabling optimization of metrics like FAD and enhancing generations by comparing them to an exemplar distribution, even across modalities. - Evaluated on an audio-domain text-to-music diffusion model with 20 reward functions, including custom aesthetics, CLAP, Vendi, and FAD, DRAGON achieved an average 81.45% win rate across targets. - Human evaluation shows a 60.95% win rate in perceived music quality without human preference annotations, solely based on appropriate exemplar sets. - Example generations can be found at https://ml-dragon.github.io/web. | ['Text-to-Audio', 'Audio'] | N/A | N/A |
| [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary
  Quality-Diversity Search](https://arxiv.org/abs/2504.15047) | Truong-Son Hy, tnngo2, quyanh | - RAINBOWPLUS, a novel red-teaming framework, enhances adversarial prompt generation for Large Language Models (LLMs) using an adaptive quality-diversity search. - It employs a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, overcoming limitations of prior methods like Rainbow Teaming. - Experiments across various datasets and LLMs demonstrate superior attack success rate (ASR) and diversity compared to Rainbow, generating up to 100 times more unique prompts. - On HarmBench with 12 LLMs, RAINBOWPLUS achieves 81.1% average ASR, outperforming AutoDAN-Turbo by 3.9% and running 9 times faster. - The open-source implementation facilitates further research and development in LLM red-teaming and safety assessment. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/knoveleng/rainbowplus) | N/A |
| [NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning](https://arxiv.org/abs/2504.13941) | yejinchoinka, ericnyberg, ekmb, shrimai19, SieraL | - NEMOTRON-CROSSTHINK is a novel framework that uses multi-domain corpora for reinforcement learning (RL) training of large language models (LLMs), improving generalization across diverse reasoning tasks. - It addresses the challenge of verifiable reward modeling for non-deterministic domains by employing templates on curated data to limit answer space diversity, enabling scalable RL training. - The framework incorporates diverse data sources (synthetic and real-world question-answer pairs), applies structured templates (multiple-choice and open-ended), filters for verifiable answers, and optimizes data blending strategies. - Experimental results show significant accuracy improvements on math (MATH-500: +30.1%, AMC23: +27.5%) and non-math (MMLU-PRO: +12.8%, AGIEVAL: +15.1%) benchmarks. - NEMOTRON-CROSSTHINK also improves response efficiency, using 28% fewer tokens for correct answers compared to math-only training. | ['Reinforcement Learning', 'Question Answering', 'Natural Language Processing'] | N/A | N/A |


## Papers for 2025-04-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Does Reinforcement Learning Really Incentivize Reasoning Capacity in
  LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837) | Zhaokai Wang, Andrew Zhao, Rui Lu, Zhiqi Chen, Yang Yue | - This paper challenges the assumption that Reinforcement Learning with Verifiable Rewards (RLVR) enables Large Language Models (LLMs) to develop novel reasoning abilities beyond the capabilities of the base model. - Through experiments across math, code, and visual reasoning tasks, using various model families and RL algorithms, the study reveals that RLVR primarily improves the sampling efficiency of existing reasoning paths present in the base model, rather than introducing new ones. - By measuring the pass@k metric, where k represents the number of samples, the authors found that base models can often outperform RL-trained models when k is large, indicating a broader reasoning coverage in base models. -  Perplexity analysis revealed that RL model's reasoning paths are already present in base model's output distribution. - The study concludes that RLVR, in its current form, has limitations in expanding the reasoning boundary of LLMs and suggests the need for new paradigms to advance LLM reasoning abilities. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://limit-of-RLVR.github.io) | N/A |
| [MIG: Automatic Data Selection for Instruction Tuning by Maximizing
  Information Gain in Semantic Space](https://arxiv.org/abs/2504.13835) | Haochen Ye, Zerun Ma, Kai Hu, Yining Li, Yicheng Chen | - This paper introduces MIG (Maximize Information Gain), a novel data selection method for instruction tuning of Large Language Models (LLMs). - MIG quantifies dataset quality and diversity by modeling the semantic space as a label graph, where nodes represent labels and edges capture semantic relationships. - Information gain is maximized during data selection by iteratively selecting samples that contribute the most to the overall information content of the dataset as measured by the label graph. - Experiments on diverse datasets (Tulu3, OpenHermes 2.5, X sota) and LLMs (Llama 3.1-8b, Mistral-7B, Qwen2.5-7B) demonstrate that MIG consistently outperforms state-of-the-art data selection methods. - Notably, a model fine-tuned with only 5% of the Tulu3 data selected by MIG achieved comparable performance to the model trained on the full dataset, showcasing significant efficiency gains. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Could Thinking Multilingually Empower LLM Reasoning?](https://arxiv.org/abs/2504.11833) | Lei Li, Shujian Huang, Wenhao Zhu, Xu Huang, Changjiang Gao | - This paper explores the potential of multilingualism in enhancing reasoning capabilities of Large Language Models (LLMs). - The study quantifies the potential gain from multilingual thinking by aggregating model responses to translated parallel inputs on reasoning-specific tasks like GPQA and MGSM. - Results demonstrate that multilingual thinking can significantly improve accuracy compared to English-only reasoning or paraphrased inputs, and a combination of just a few (≥4) languages is sufficient for substantial improvement. - The study finds that common answer selection methods, like majority voting, prompt-based selection, and LLM-as-a-judge selection, struggle to fully realize the potential of multilingualism due to issues like bias toward specific languages and sensitivity to answer selection criteria. - The paper suggests that different languages may be better suited for different difficulty levels of questions, and the presence of "key advantageous languages" can compensate for errors in other languages, yet a robust solution is not fully realized yet. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/CONE-MT/multilingual_reasoning) | [Link](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct), [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B), [Link](https://huggingface.co/Qwen/Qwen-2.5-72B-Chat) |
| [NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes](https://arxiv.org/abs/2504.11544) | Yixin Liu, Haoxiang Chen, Chengze Li, Haojie Zheng, Tianyang Xu | - NodeRAG, a novel graph-based Retrieval-Augmented Generation (RAG) framework, is introduced, which enhances RAG performance through optimized graph structure indexing for more effective and fine-grained retrieval. - NodeRAG constructs a heterogenous graph with functionally distinct nodes, balancing fine-grained understanding with a global perspective of the knowledge corpus, addressing limitations of previous graph-based RAG methods. - The heterograph integrates various node types, including entities, relationships, text chunks, events, and summaries, enabling precise hierarchical retrieval while minimizing irrelevant information. - Experimental results show that NodeRAG outperforms current methods (GraphRAG, LightRAG) in multi-hop benchmarks and open-ended evaluations with fewer retrieval tokens, demonstrating enhanced efficiency. - The framework offers system-level efficiency advantages with improvements in indexing time, query time, and storage efficiency due to the fine-grained graph structure and retrieval process. | ['Question Answering', 'Natural Language Processing', 'Graph Machine Learning'] | [Link](https://github.com/Terry-Xu-666/NodeRAG) | N/A |
| [Thought Manipulation: External Thought Can Be Efficient for Large
  Reasoning Models](https://arxiv.org/abs/2504.13626) | Wenhan Dong, Zifan Peng, Zhen Sun, Jingyi Zheng, Yule Liu | - This paper introduces ThoughtMani, a training-free method to enhance the efficiency of Large Reasoning Models (LRMs) by mitigating the "overthinking" problem, where models generate redundant reasoning steps. - ThoughtMani leverages smaller CoT generator models to provide external thoughts inserted into the reasoning process of the LRM, reducing internal thought generation and computational cost. - Experimental results on various datasets like GSM-8k, MATH-500, AIME-2024, and LiveBench/Code demonstrate that ThoughtMani reduces output token counts while maintaining or even improving reasoning accuracy and safety alignment. - For instance, using ThoughtMani with a smaller Qwen-2.5-7B-Instruct CoT generator for QwQ-32B on LiveBench/Code decreases output tokens by approximately 30% without performance degradation. - The method also reveals a distinct behavior in RL-based and distillation-based LRMs in handling external CoTs, offering insights into their reasoning processes. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [It's All Connected: A Journey Through Test-Time Memorization,
  Attentional Bias, Retention, and Online Optimization](https://arxiv.org/abs/2504.13173) | Vahab Mirrokni, Peilin Zhong, Meisam Razaviyayn, Ali Behrouz | - This paper introduces MIRAS, a novel framework for designing sequence models based on associative memory principles, inspired by the cognitive phenomenon of attentional bias. - MIRAS framework considers four design choices: memory architecture, attentional bias objective, retention gate, and memory learning algorithm, offering flexibility in model design. - Three new sequence models—MONETA, YAAD, and MEMORA—are presented as variants of MIRAS, employing distinct attentional biases and retention mechanisms. - Experimental results demonstrate that these MIRAS variants outperform Transformer++ and other linear RNNs across language modeling, commonsense reasoning, and needle-in-haystack tasks. - The authors attribute the superior performance of MIRAS models to their expressive memory architectures and robust learning mechanisms, especially in long-context scenarios. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2025-04-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for
  Language Model Pre-training](https://arxiv.org/abs/2504.13161) | Dan Su, Xin Dong, Yonggan Fu, Yu Yang, shizhediao | - CLIMB, a novel framework for automating the search for optimal data mixtures during pre-training of large language models (LLMs), is introduced. - CLIMB consists of three steps: embedding and clustering large-scale datasets, constructing mixture-performance pairs by training proxy models, and fitting a predictor to estimate performance. - When trained on 400 billion tokens with an optimized data mixture, a 1 billion parameter model trained using CLIMB exceeds the performance of the state-of-the-art Llama-3.2-1B model by 2.0%. - Optimizing for a specific domain (e.g., Social Sciences) results in a 5% performance improvement compared to random sampling. - ClimbLab, a 1.2 trillion token corpus with 20 clusters, and ClimbMix, a 400 billion token dataset, are introduced as new datasets for pre-training research. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/ClimbMix), [Link](https://huggingface.co/ClimbLab) |
| [Antidistillation Sampling](https://arxiv.org/abs/2504.13146) | Avi Schwarzschild, Zhili Feng, Asher Trockman, arobey1, yashsavani | - This paper introduces "antidistillation sampling," a method designed to mitigate the effectiveness of model distillation by poisoning the reasoning traces generated by large language models (LLMs). - Antidistillation sampling strategically modifies the model's next-token probability distribution during the generation of reasoning traces, striking a balance between maintaining high likelihood under the original distribution and maximizing the negative impact on the performance of distilled models. - The approach focuses on making the generated traces less useful for training student models while preserving the teacher model's performance on downstream tasks. - Experiments using DeepSeek-R1-Distill-Qwen-7B, Qwen2.5-3B, and Llama-3.2-3B on GSM8K and MATH benchmarks demonstrate the effectiveness of antidistillation sampling. - The results show that the approach significantly reduces the performance of distilled student models relative to those trained on traces from temperature sampling, for the same teacher performance level. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in
  Data Synthesis](https://arxiv.org/abs/2504.12322) | Honglin Lin, Yu Li, Zinan Tang, Qizhi Pei, GX-XinGao | - This paper introduces GRA, a novel framework leveraging multiple small language models (LLMs) in a collaborative approach for data synthesis, aiming to match or exceed the quality achieved by single large LLMs. - GRA assigns distinct roles to small LLMs (Generator, Reviewer, and Adjudicator) inspired by the peer-review process, promoting iterative refinement and quality control, thus achieving data-level parity with large LLM-based distillation. - Across multiple benchmarks, GRA-produced data matches or exceeds the quality of single large LLM outputs like Qwen-2.5-72B-Instruct, demonstrating the potential of strategically coordinating smaller LLMs for high-quality data synthesis. - The framework reduces computational resource requirements significantly compared to large LLM distillation. - This study demonstrates the potential of smaller, collaborative LLMs in enhancing efficiency and accessibility of data synthesis | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/GX-XinGao/GRA) | N/A |
| [Packing Input Frame Context in Next-Frame Prediction Models for Video
  Generation](https://arxiv.org/abs/2504.12626) | Maneesh Agrawala, Lvmin Zhang | - FramePack, a novel neural network structure, enhances video generation by addressing the "forgetting" problem in next-frame prediction, where models struggle to maintain long-range temporal consistency. - FramePack compresses input frames based on their relative importance, allowing models to process a larger number of frames without increasing computational burden. - The structure employs progressive compression by manipulating the transformer's patchify kernel size, ensuring a fixed context length regardless of video duration. - The paper also introduces anti-drifting sampling methods that incorporate bi-directional context to enhance visual quality. - Experiments demonstrate that FramePack can improve existing video diffusion models, allowing higher training batch sizes and yielding more balanced diffusion schedules for enhanced results. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/lllyasviel/FramePack) | N/A |
| [Generate, but Verify: Reducing Hallucination in Vision-Language Models
  with Retrospective Resampling](https://arxiv.org/abs/2504.13169) | Trevor Darrell, Joseph E. Gonzalez, Jiaxin Ge, Heekyung Lee, tsunghanwu | - REVERSE, a novel framework for reducing hallucinations in vision-language models (VLMs), is introduced, unifying generation adjustment and online post-hoc verification within a single VLM architecture. - The approach uses a 1.3M semi-synthetic training dataset with hallucination phrases tagged by special tokens, enabling the VLM to identify potential hallucinations. - Retrospective resampling, a new inference technique, allows the VLM to act as its own verifier, backtracking and revising hallucinations through rejection sampling and query rewriting. - Evaluations show REVERSE achieves state-of-the-art hallucination reduction, improving CHAIR scores by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest compared to existing methods. - The framework demonstrates significant improvements on hallucination-sensitive captioning and open-ended VQA tasks, while also maintaining performance on discriminative question formats. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/tsunghan-wu/reverse_vlm) | [Link](https://reverse-vlm.github.io) |
| [VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference
  Optimization for Large Video Models](https://arxiv.org/abs/2504.13122) | Meng Luo, Haojian Huang, scofield7419, ChocoWu, Harold328 | - VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization, improves text-video preference alignment in Large Video Models (LVMs) across three hierarchical levels: instance, temporal, and perceptive. - It introduces VistaDPO-7k, a dataset of 7.2K QA pairs with chosen/rejected responses and spatiotemporal grounding information. - VistaDPO addresses video-language misalignment and hallucination by aligning video content with responses at the instance level, video temporal semantics with event descriptions at the temporal level, and spatial objects with language tokens at the perceptive level. - Experiments on benchmarks including VideoHallucination, Video QA, and Captioning demonstrate significant performance improvements in existing LVMs (e.g., 26.42% over PLLaVA, 53.92% over Video-LLaVA). - The hierarchical spatiotemporal alignment and the high-quality VistaDPO-7k dataset contribute to enhanced video understanding and reduced hallucination in LVMs. | ['Video-Text-to-Text', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/HaroldChen19/VistaDPO) | N/A |
| [NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation](https://arxiv.org/abs/2504.13055) | Chao Du, Zijian Wu, Jinjie Ni, Xiangyan Liu, dreamerdeo | - NoisyRollout, a novel reinforcement learning (RL) method, is introduced to enhance visual reasoning in vision-language models (VLMs) by improving rollout diversity and addressing perceptual limitations. - It incorporates a hybrid rollout strategy, using trajectories from both clean and distorted images during training to improve exploration and expose perceptual discrepancies. - A noise annealing schedule gradually reduces distortion strength to maintain training stability. - Experiments demonstrate state-of-the-art performance on five out-of-domain benchmarks, outperforming open-source RL-tuned models and even some large-scale supervised fine-tuning (SFT) + RL models with only 2.1K training samples. - This method improves generalization without requiring additional training overhead or changes to the RL objective (GRPO). | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/John-AI-Lab/NoisyRollout) | [Link](hf.co/collections/xyliu6/noisyrollout) |
| [ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question
  Answering](https://arxiv.org/abs/2504.05506) | Firoz Kabir, Aayush Bajaj, Mahir Ahmed, 38saidul, ahmed-masry | - This paper introduces ChartQAPro, a new benchmark dataset for Chart Question Answering (CQA) that aims to address the limitations of existing datasets like ChartQA. - ChartQAPro includes 1,341 charts from 157 diverse sources, featuring 1,948 questions with diverse types such as multiple-choice, conversational, hypothetical, and unanswerable questions. - The dataset emphasizes visual diversity with the inclusion of infographics and dashboards and linguistic diversity with more complex and varied questions and accompanying paragraphs. - Evaluations with 21 models show substantial performance drops for large vision-language models (LVLMs) on ChartQAPro; for example, Claude Sonnet 3.5 accuracy drops from 90.5% on ChartQA to 55.81% on ChartQAPro. - Error analysis reveals challenges in visual perception, instruction following, and math reasoning, providing directions for future LLM development in CQA. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/vis-nlp/ChartQAPro) | N/A |
| [Exploring Expert Failures Improves LLM Agent Tuning](https://arxiv.org/abs/2504.13145) | Ruochen Wang, Minhao Cheng, Andrew Bai, Li-Cheng Lan, zhoutianyi | - This paper proposes Exploring Expert Failures (EEF), a novel method for fine-tuning Large Language Model (LLM) agents that leverages beneficial actions from failed expert trajectories, which are often overlooked by existing methods like Rejection Sampling Fine-Tuning (RFT). - EEF identifies and incorporates valuable actions and plans from unsuccessful expert demonstrations, enhancing agent exploration efficiency and skill acquisition, while carefully filtering out potentially detrimental actions to prevent negative learning. - EEF outperforms RFT and expert models in challenging environments such as WebShop and SciWorld, achieving a 62% win rate in WebShop, exceeding RFT (53.6%) and GPT-4 (35.6%). -  Furthermore, EEF sets new state-of-the-art performance in WebShop (first to score above 0.81) and SciWorld (first to exceed 81).  - The method's effectiveness is demonstrated by its ability to solve previously unsolvable subtasks and its efficient utilization of weaker but more cost-effective experts like GPT-3.5 Turbo. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |
| [FocusedAD: Character-centric Movie Audio Description](https://arxiv.org/abs/2504.12157) | Liangcheng Li, Sheng Zhou, Yiren Song, Chun Wang, Xiaojun Ye | - FocusedAD is a novel framework for generating character-centric movie audio descriptions, addressing the need for plot-relevant and character-referential narration for blind and visually impaired audiences. - The model architecture consists of a Character Perception Module (CPM) to track character regions and link them to names, a Dynamic Prior Module (DPM) to integrate contextual cues from prior descriptions and subtitles, and a Focused Caption Module (FCM) to generate detailed, character-focused narrations. - It introduces an automated pipeline for building character query banks to improve character identification and incorporates a dynamic soft prompt mechanism in the DPM to handle scenes with varying numbers of characters. - FocusedAD outperforms existing methods, achieving state-of-the-art results, including in zero-shot settings, on MAD-eval-Named and a newly proposed Cinepile-AD dataset. - The model's focus on narrative-salient regions and explicit character identification makes it particularly suitable for automated audio description generation, surpassing methods that rely on global scene features and ambiguous character references. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/Thorin215/FocusedAD) | N/A |
| [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079) | Mohit Bansal, Elias Stengel-Eskin, Archiki Prasad, HanNight | - This paper introduces MADAM-RAG, a multi-agent debate approach for Retrieval-Augmented Generation (RAG) designed to handle conflicting evidence from multiple sources, including ambiguity, misinformation, and noise. - MADAM-RAG assigns each retrieved document to an LLM agent, which generates an intermediate response. These agents then engage in a multi-round debate, allowing them to refine responses and challenge each other’s claims. - An aggregator summarizes the debate into a coherent final response, filtering out misinformation and presenting multiple valid answers when appropriate. - The authors also introduce RAMDocs, a new dataset that simulates complex scenarios for conflicting evidence. - Experimental results on FaithEval and AmbigDocs demonstrate that MADAM-RAG outperforms existing RAG baselines by up to 15.80% and 11.40%, respectively. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/HanNight/RAMDocs) | N/A |
| [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171) | Sarah Wooders, Charles Packer, Yu Wang, Charlie Snell, Kevin Lin | - This paper introduces "sleep-time compute," a technique that allows large language models (LLMs) to pre-compute useful information from a given context before a query is presented, reducing test-time compute requirements. - The authors demonstrate that sleep-time compute improves the Pareto frontier of test-time compute vs. accuracy, achieving similar accuracy with 5x less compute on modified versions of Stateful GSM-Symbolic and Stateful AIME reasoning tasks.  - By scaling sleep-time compute and amortizing it across multiple related queries (using Multi-Query GSM-Symbolic), they further boost accuracy and reduce average cost per query by up to 18% and 2.5x, respectively. - Analysis shows that the efficacy of sleep-time compute correlates with the predictability of user queries from the context. - Finally, they apply the technique to a realistic agentic software engineering task, demonstrating its potential in real-world stateful LLM applications. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/letta-ai/sleep-time-compute) | N/A |
| [Perception Encoder: The best visual embeddings are not at the output of
  the network](https://arxiv.org/abs/2504.13181) | Andrea Madotto, Jang Hyun Cho, Peize Sun, Po-Yao Huang, Daniel Bolya | - This paper introduces Perception Encoder (PE), a vision encoder trained with contrastive vision-language learning. - PE achieves state-of-the-art zero-shot performance in image and video classification and retrieval, outperforming models trained on larger, proprietary datasets like JFT-3B. -  It uses a novel training process involving a robust image pretraining recipe and a video data engine that generates aligned captions for video clips. - The authors also release a new dataset of 1M videos with 120K human-refined annotations called the PE Video Dataset (PVD). - Through two novel alignment methods, language and spatial, internal representations within PE are identified that excel in various tasks exceeding models with other pretraining techniques. | ['Multimodal', 'Visual Question Answering', 'Zero-Shot Image Classification', 'Video Classification', 'Zero-Shot Classification', 'Image Feature Extraction'] | [Link](https://github.com/facebookresearch/perception_models) | [Link](https://ai.meta.com/datasets/pe-video/) |


## Papers for 2025-04-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ColorBench: Can VLMs See and Understand the Colorful World? A
  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514) | zhoutianyi, jiuhai, shweta12, kweCobi, Fcr09 | - This paper introduces COLORBENCH, a new benchmark designed to evaluate the color perception, reasoning, and robustness of Vision-Language Models (VLMs). - COLORBENCH includes 11 tasks covering various real-world applications, such as identifying colors in paintings, reading test kits, interpreting satellite images, and recognizing camouflaged objects. - An evaluation of 32 VLMs reveals that larger models generally perform better, but the absolute performance across all models is relatively low, suggesting that color understanding is a current weakness of VLMs. - Chain-of-Thought (CoT) reasoning improves color understanding accuracy and robustness, highlighting the importance of reasoning abilities in these tasks. - Color clues are helpful for most COLORBENCH tasks but can mislead VLMs in tasks involving illusions or mimicry. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/tianyi-lab/ColorBench) | N/A |
| [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285) | thegenerality, THU-CHUNXIA, buaahsh, hongyuw, shumingma | - BitNet b1.58 2B4T is a 2-billion parameter, native 1-bit Large Language Model (LLM) trained on 4 trillion tokens, using a modified transformer architecture with custom BitLinear layers incorporating weight quantization (1.58 bits), activation quantization (8 bits), and subln normalization. - It achieves performance comparable to leading open-weight, full-precision LLMs of similar size across various tasks including language understanding, reasoning, coding, and conversation. - This model offers substantial efficiency benefits, with significant reductions in memory footprint, energy consumption, and decoding latency compared to full-precision counterparts. - Open-source implementations optimized for both GPU (custom CUDA kernels) and CPU (bitnet.cpp) are provided to encourage wider adoption and research. - BitNet b1.58 2B4T advances the state-of-the-art in 1-bit LLMs, demonstrating the potential of extreme quantization for deploying powerful language models in resource-constrained environments. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/microsoft/LMOps) | [Link](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T) |
| [AlayaDB: The Data Foundation for Efficient and Effective Long-context
  LLM Inference](https://arxiv.org/abs/2504.10326) | FeTieTer, YuanPeiqi, Qilong00, BenjaminXIANG, YangshenDeng | - AlayaDB, a new vector database system, is designed specifically for efficient and effective long-context Large Language Model (LLM) inference. - It decouples key-value (KV) cache and attention computation from the LLM inference engine, encapsulating them within the database system.  - A novel dynamic inner product range (DIPR) query is introduced to enhance sparse attention by dynamically selecting critical tokens.  - AlayaDB employs a query optimizer and various optimizations, including a novel vector file system and a data-centric attention engine, to minimize resource consumption and improve performance.  - Experimental results on LLM inference benchmarks demonstrate AlayaDB's ability to reduce resource consumption and improve generation quality while adhering to service level objectives (SLOs). | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction
  Fine-Tuning](https://arxiv.org/abs/2504.09081) | Jian Xie, Rupak Vignesh Swaminathan, svinxz, vijaygirish2001, panprabh | - The paper introduces SIFT-50M, a 50 million example multilingual dataset for instruction fine-tuning and pre-training of speech-text large language models (LLMs). - The dataset covers five languages and focuses on diverse speech understanding and controllable speech generation instructions, generated using LLMs and expert models. - The authors also introduce EvalSIFT, a benchmark dataset to evaluate speech-text LLMs and  SIFT-LLM, a speech-text LLM trained on SIFT-50M. - SIFT-LLM outperforms existing speech-text LLMs on instruction following benchmarks while remaining competitive on foundational speech tasks according to Table 3. - The model also shows promising results for controllable speech generation, with synthesized speech exhibiting characteristics specified in the instructions as seen in Table 6. | ['Audio', 'Automatic Speech Recognition', 'Text-to-Speech', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/amazon-agi/SIFT-50M) |
| [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536) | chijx, imjcqt, YujiaHi, zhangysk, JoeYing | - ReTool, a reinforcement learning framework, enhances LLMs' ability to use external tools, like code interpreters, for complex problem-solving, such as mathematical reasoning. - It combines dynamic interleaving of real-time code execution within natural language reasoning and automated RL for training tool invocation strategies based on outcome feedback. - On the AIME math benchmark, ReTool-32B achieves 67% accuracy with 400 training steps, outperforming a text-based RL baseline (40% accuracy, 1080 steps) and surpasses OpenAI's model by 27.9% in extended settings (72.5% accuracy). - Analysis reveals emergent behaviors like code self-correction, indicating autonomous mastery of adaptive tool use. - ReTool highlights the potential of outcome-driven tool integration for complex problem-solving and provides insights into hybrid neuro-symbolic systems. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952) | ashay-sriv, jebish7, DrishtiSharma, Siddartha10, 1024m | - This paper introduces a new dataset of 2.4M human-machine co-authored texts in 23 languages generated using popular LLMs, including GPT-4, Gemini, and Claude. - It proposes a set of multilingual transformer models with a CRF layer for token classification, trained to distinguish writing styles within a text. - These models achieve better performance over texts with unseen features (domain, generator, adversarial inputs, non-native speakers) compared to binary classification approaches. - The models effectively separate human-authored from machine-generated portions in co-authored texts. The findings of the paper are based on tests conducted on various benchmarks and datasets, including Mgtd-bench and Raid-bench. Additional findings include a comparison of performance against various adversarial methods and the characteristics of generated text compared to human-authored text. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |
| [Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution](https://arxiv.org/abs/2504.09566) | Qigan Sun, Jiaquan Zhang, Yi Lu, Chaoning Zhang, Chenghao Li | - This paper introduces Syzygy of Thoughts (SoT), a novel reasoning framework that enhances Chain-of-Thought (CoT) prompting for Large Language Models (LLMs) by incorporating principles of Minimal Free Resolution (MFR) from algebraic geometry. - SoT decomposes complex problems into interconnected reasoning paths, capturing deeper logical dependencies and improving structured problem-solving by introducing concepts like "Module", "Freeness", and "Mapping". - Experimental results on datasets like GSM8K and MATH demonstrate that SoT achieves comparable or superior performance to other CoT methods across various LLMs (GPT40-mini, Qwen2.5). - SoT's structured approach reduces redundant computations and logical inconsistencies, leading to more efficient and transparent reasoning. - The framework shows improved inference accuracy and scalability, addressing CoT limitations in high-dimensional and complex logical problem-solving, particularly in mathematical reasoning tasks, where it approaches the performance of larger models like GPT-4 on lightweight models. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/dIMARiA/Syzygy-of-thoughts) | N/A |


## Papers for 2025-04-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Genius: A Generalizable and Purely Unsupervised Self-Training Framework
  For Advanced Reasoning](https://arxiv.org/abs/2504.08672) | Haiteng Zhao, Chang Ma, Hang Yan, QiushiSun, xufangzhi | - Genius, a generalizable and purely unsupervised self-training framework, is proposed to enhance the reasoning capabilities of Large Language Models (LLMs) without external supervision or reward models. - It uses a stepwise foresight re-sampling strategy, which involves simulating future outcomes to estimate step values and create high-quality preference pairs for training. - To ensure robust optimization, Genius introduces an advantage-calibrated optimization (ACO) loss function which mitigates inconsistencies between foresight scores and step advantages. - Experimental results on seven reasoning benchmarks demonstrate that Genius significantly improves the reasoning performance of LLaMA 3.1 by >7% with only 25K unsupervised general queries. - Further analysis reveals that Genius holds potential for scalability, as indicated by the consistent improvement observed with increasing training steps. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/xufangzhi/Genius) | N/A |
| [xVerify: Efficient Answer Verifier for Reasoning Model Evaluations](https://arxiv.org/abs/2504.10481) | Bo Tang, Wentao Zhang, Pengyuan Wang, Duguce, Hush-cd | - This paper introduces xVerify, an efficient answer verifier designed for evaluating responses from reasoning models on objective questions, addressing the limitations of existing methods in handling complex reasoning traces and equivalence checking. - It supports robust equivalence checking, including symbol conversion, mathematical expression matching, and semantic alignment, and is tolerant of formatting errors, making it applicable to a wide range of question types. - A new dataset, Verify Answer for Reasoning (VAR), is constructed, containing responses from 19 LLMs across 24 reasoning benchmarks, with labels verified through GPT-40 and human review. - xVerify models of different sizes are trained on VAR and achieve state-of-the-art performance, outperforming existing evaluation methods and judge models on both test and generalization sets. - Notably, even the smallest variant, xVerify-0.5B-I, surpasses most existing methods, while larger variants achieve F1 scores and accuracy exceeding 95%. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/IAAR-Shanghai/xVerify) | [Link](https://huggingface.co/collections/IAAR-Shanghai/xverify) |
| [Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding](https://arxiv.org/abs/2504.10465) | Weixian Lei, Yanwei Li, Zilong Huang, Tao Zhang, LXT | - Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks like referring segmentation and visual prompt understanding, uses a single transformer, eliminating the need for extra components like vision encoders or segmentation experts. - Pixel-SAIL incorporates a learnable upsampling module to refine low-resolution visual tokens, a novel visual prompt injection method for better visual prompt understanding, and a dense feature distillation strategy from pre-trained segmentation experts to enhance fine-grained feature extraction. - Evaluated on RefCOCO, RefCOCOg, RefCOCO+, and gRefCOCO, Pixel-SAIL-0.5B outperforms similarly sized models and even some larger 7B models with vision experts.  Pixel-SAIL-3B achieves state-of-the-art results on these datasets, outperforming larger models like Sa2VA-4B.  - On the new PerBench, which includes detailed object captions, visual prompt multiple-choice questions, and visual-text referring segmentation, Pixel-SAIL-3B achieves an overall score of 42.2, surpassing Sa2VA-4B's 39.0 and GLaMM-7B's 15.3. | ['Image Segmentation', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/magic-research/Sa2VA) | N/A |
| [Heimdall: test-time scaling on the generative verification](https://arxiv.org/abs/2504.10337) | Xing Jin, WesleyShi | - Heimdall, a long chain-of-thought (CoT) verification large language model (LLM), is proposed, which accurately judges the correctness of solutions, boosting verification accuracy on competitive math problems from 62.5% to 97.5% using reinforcement learning and repeated sampling. - Heimdall demonstrates strong generalization capabilities by successfully detecting issues in challenging math proofs not seen during training and is extended to improve problem-solving through Pessimistic Verification, which selects the most likely correct solution by judging multiple solutions from a solver model. - Using DeepSeek-R1-Distill-Qwen-32B as the solver, Pessimistic Verification with Heimdall improves solution accuracy on AIME2025 from 54.2% to 83.3% and reaches 93% with Gemini 2.5 Pro, matching state-of-the-art performance. - An automatic knowledge discovery system is prototyped using NuminaMath for data synthesis, with Heimdall verifying solutions, effectively identifying nearly half of the synthetic data as flawed, consistent with findings from NuminaMath's own study. - The work highlights the importance of verification in AI systems for knowledge creation and maintenance, showing the potential of long CoT LLMs for accurate verification and improved problem-solving. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [How Instruction and Reasoning Data shape Post-Training: Data Quality
  through the Lens of Layer-wise Gradients](https://arxiv.org/abs/2504.10766) | Ziyue Li, Yanhong Li, Ming Li, zhoutianyi | - This paper presents a spectral analysis of layer-wise gradients in Large Language Models (LLMs) during post-training with instruction/reasoning data of varying quality. - The analysis reveals that established data quality metrics can be unified and explained by the spectral properties of gradients obtained through singular value decomposition (SVD). - Higher-quality data correlates with lower nuclear norms and higher effective ranks of gradients, with effective rank demonstrating finer-grained resolution in discerning data quality compared to nuclear norms. - The study shows that reasoning data elicits richer gradient structures (higher effective ranks) than instruction-following data, indicating greater complexity in parameter updates for reasoning tasks. - Models within the same family exhibit similar gradient dynamics irrespective of model size, but variations are significant across different model families. | ['Natural Language Processing'] | [Link](https://github.com/MingLiiii/Gradient_Unified) | N/A |
| [The Scalability of Simplicity: Empirical Analysis of Vision-Language
  Learning with a Single Transformer](https://arxiv.org/abs/2504.10462) | Jun Hao Liew, Haochen Wang, Jiacong Wang, Weixian Lei, LXT | - This paper introduces SAIL, a Single Transformer unified Multimodal Large Language Model (MLLM) that integrates raw pixel encoding and language decoding within a single architecture, eliminating the need for a separate vision encoder. - SAIL leverages mixed attention mechanisms (bidirectional for image patches, causal for text) and multimodal positional encodings to align visual and textual modalities. - Through model and data scaling, SAIL achieves performance comparable to modular MLLMs on vision-language benchmarks and functions as a high-performing vision backbone. - Notably, SAIL demonstrates superior data scaling properties compared to modular MLLMs, achieving near-comparable performance with significantly less pretraining data when scaled to 512M image-text pairs. - SAIL also exhibits strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. | ['Multimodal', 'Image Classification', 'Image Segmentation', 'Visual Question Answering', 'Image Feature Extraction'] | [Link](https://github.com/bytedance/SAIL) | N/A |
| [Efficient Process Reward Model Training via Active Learning](https://arxiv.org/abs/2504.10559) | Tianyu Pang, Xin Mao, Zichen Liu, Keyu Duan, dreamerdeo | - This paper introduces ACTPRM, an active learning approach for training Process Reward Models (PRMs) that significantly reduces annotation costs by proactively selecting uncertain samples for labeling. - ACTPRM uses an ensemble of PRMs to estimate uncertainty during training, which is used in the pool-based setting to retain only highly uncertain data that subsequently gets labelled by a reasoning model. - The paper demonstrates that ACTPRM achieves comparable or better performance to full-data tuning while using only 50% of the annotation budget. - It also presents a one-shot active learning setting where ACTPRM filters over 1M math reasoning trajectories, reaching a new state-of-the-art performance of 75.0% on ProcessBench and 65.5% on PRMBench with significantly reduced costs compared to previous SOTA methods. - The code and models are open-sourced to facilitate community adoption and further research. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/sail-sg/ActivePRM) | N/A |
| [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and
  Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456) | Xingyu Chen, Qiuzhi Liu, Jiahao Xu, Tian Liang, Zhiwei He | - This paper introduces DeepMath-103K, a large-scale dataset of 103K challenging mathematical problems designed for training advanced reasoning models via reinforcement learning. - DeepMath-103K is more challenging and diverse than existing datasets, with problems spanning difficulty levels 3-10 and covering a wide range of mathematical topics.  - Each problem includes a verifiable final answer and three distinct AI-generated solutions, supporting various training paradigms like supervised fine-tuning, model distillation, and reinforcement learning. - Models trained on DeepMath-103K demonstrate significant performance improvements on multiple challenging mathematical reasoning benchmarks, especially using RL-Zero techniques. - The dataset's rigorous decontamination process ensures the integrity of evaluations, facilitating the development of robust AI reasoning systems by removing overlap with common evaluation benchmarks. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/zwhe99/DeepMath) | N/A |
| [ReZero: Enhancing LLM search ability by trying one-more-time](https://arxiv.org/abs/2504.11001) | Thinh Le, alandao | - ReZero, a novel Reinforcement Learning (RL) framework, is introduced to enhance the search capabilities of Large Language Models (LLMs) in Retrieval-Augmented Generation (RAG) by explicitly rewarding the act of retrying a search query after an initial unsuccessful attempt. - It uses a modified reward function within a standard RL loop (state, action, reward, policy) to incentivize the model to explore different querying strategies and persist in information seeking rather than prematurely halting. - ReZero was evaluated on the Apollo 3 mission dataset using Llama2-3B-Instruct and achieved a peak accuracy of 46.88%, nearly double the 25% accuracy of a baseline model trained without the retry incentive. - The reward_retry component encourages more effective utilization of the search tool to arrive at correct answers, particularly in challenging scenarios where initial queries may be insufficient. - The results demonstrate that explicitly rewarding retry attempts in a RAG system significantly improves LLM robustness in information retrieval, suggesting the model learns effective search strategies faster. Further research focusing on the training dynamics, generalizing ReZero's performance, using other datasets and integrating it with related methods are suggested as future directions. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [AI-University: An LLM-based platform for instructional alignment to
  scientific classrooms](https://arxiv.org/abs/2504.08846) | Rahul Gulati, Mostafa Faghih Shojaei, garikipati, Dinzhenzhenzhu, simocimolato | - AI-University (AI-U) is an LLM-based platform for instructional alignment to scientific classrooms by using retrieval-augmented generation (RAG) to fine-tune LLMs with course materials such as lecture videos, notes, and textbooks. - The AI-U framework fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to create instructor-aligned responses, mirroring the instructor's style and approach. - A prototype web application allows users to input queries and receive AI-generated responses with links to relevant course materials and timestamps in corresponding video lectures. - Evaluation results showed that the fine-tuned LLM, LLaMA-TOMMI-1.0, achieves a higher cosine similarity with reference answers and outperforms the base Llama 3.2 model in alignment with course style. - The framework offers a scalable approach to AI-assisted education with a focus on adaptability and personalized learning, and can be extended to broader research content in science. | ['Question Answering'] | [Link](https://github.com/my-ai-university/finite-element-method) | [Link](https://huggingface.co/my-ai-university) |
| [Adaptive Computation Pruning for the Forgetting Transformer](https://arxiv.org/abs/2504.06949) | Aaron Courville, Johan Obando-Ceron, Zhixuan Lin, littleowen | - This paper introduces Adaptive Computation Pruning (ACP) for the Forgetting Transformer (FoX), a method to dynamically prune computations in attention based on a forget gate and a threshold. - ACP identifies and skips computations related to input-output dependencies significantly weakened by the forget gate, reducing FLOPs without impacting performance. - Applied to language model pretraining with FoX, ACP reduces FLOPs in softmax attention by ~70% and improves training throughput by 10-35% across various model sizes and context lengths, with longer contexts yielding greater savings. - Analysis reveals a bimodal distribution of attention heads, categorized into "local" heads (primarily focused on local context) and "global" heads, with the majority being local, aligning with the observed FLOP reduction. - The paper also explores the impact of a hyperparameter controlling pruning aggressiveness and demonstrates its robustness, showing minimal performance impact even with a less conservative setting. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zhixuan-lin/arctic-fox), [Link](https://github.com/zhixuan-lin/forgetting-transformer) | N/A |
| [Multimodal Long Video Modeling Based on Temporal Dynamic Context](https://arxiv.org/abs/2504.10443) | Xiangyu Yue, Yiyuan Zhang, Jiaming Han, Hoar012 | - This paper introduces Temporal Dynamic Context (TDC), a novel multimodal long-video modeling framework that leverages both static visual features and dynamic multimodal context within each scene. - TDC employs a query-based Transformer to compress video, audio, and instruction text tokens into a limited set of temporal context tokens, enabling effective token compression while preserving crucial information. - To handle extremely long videos, the paper proposes a training-free Long Video Chain-of-Thought (LVCoT) strategy, which processes the video segment by segment and then integrates the segment information for a final answer. - Extensive experiments on various video and audio-video understanding benchmarks demonstrate that TDC achieves state-of-the-art performance, outperforming existing methods such as VideoLLaMA2 and LongVU on MLVU by 15.6% and 7.4%, respectively. - Results on VideoMME, with and without subtitles, further validates TDC's effectiveness in long video understanding. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Audio'] | [Link](https://github.com/Hoar012/TDC-Video) | N/A |
| [Summarization of Multimodal Presentations with Vision-Language Models:
  Study of the Effect of Modalities and Structure](https://arxiv.org/abs/2504.10049) | Frédéric Dufaux, Camille Guinaudeau, gigant | - This paper benchmarks open-weight Vision-Language Models (VLMs) for summarizing multimodal presentations, specifically focusing on the impact of input representation. - It introduces a structured, interleaved representation of slides and transcripts, demonstrating superior performance compared to unstructured or single-modality inputs. - With Qwen2-VL, the study conducts a fine-grained analysis across various visual token budgets and input structures, revealing that visual information becomes crucial with higher budgets while structure is key at lower budgets.  - The paper further examines the scaling of model size and its interaction with visual token budget.  - Through a qualitative evaluation, the paper illustrates how multimodal input leads to more comprehensive summaries with improved accuracy and contextual information, while also observing an unintended "bullet point" generation tendency. | ['Multimodal', 'Summarization', 'Video-Text-to-Text'] | N/A | [Link](https://huggingface.co/datasets/gigant/tib-bench), [Link](https://huggingface.co/spaces/gigant/slide-presentation-viz) |
| [LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews](https://arxiv.org/abs/2504.11042) | Iryna Gurevych, Lizhen Qu, Anne Lauscher, Zhuang Li, sukannya | - Introduces LAZYREVIEW, a dataset of peer review sentences annotated with fine-grained lazy thinking categories. - Reveals that Large Language Models (LLMs) struggle to detect lazy thinking instances in a zero-shot setting but improve significantly with instruction-based fine-tuning. - Demonstrates through a controlled experiment that reviews revised with lazy thinking feedback are more comprehensive and actionable. - Releases the dataset and enhanced guidelines to aid in training junior reviewers. - Presents analysis and experimental results on effectiveness of instruction tuning and positive examples in identifying lazy thinking. | ['Natural Language Processing'] | [Link](https://github.com/UKPLab/arxiv2025-lazy-review) | N/A |


## Papers for 2025-04-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [InternVL3: Exploring Advanced Training and Test-Time Recipes for
  Open-Source Multimodal Models](https://arxiv.org/abs/2504.10479) | jackroos, duanyuchen, gulixin0922, Yeshenglong, Weiyun1025 | - InternVL3 is a multimodal large language model (MLLM) that uses a native multimodal pre-training paradigm, jointly learning from text and multimodal data. - It incorporates variable visual position encoding (V2PE), supervised fine-tuning (SFT), mixed preference optimization (MPO), and test-time scaling strategies. - InternVL3-78B achieves a 72.2 score on the MMMU benchmark, outperforming other open-source MLLMs and demonstrating comparable capabilities to proprietary models such as ChatGPT-40, Claude 3.5 Sonnet, and Gemini 2.5 Pro. - The model shows improvements in various tasks, including tool usage, GUI agent interaction, industrial image analysis, and spatial reasoning, as well as strong language capabilities comparable to other advanced LLMs of a similar scale. - Both training data and model weights will be publicly released to support further MLLM research. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text', 'Image-to-Text'] | [Link](https://github.com/OpenGVLab/InternVL) | [Link](https://huggingface.co/OpenGVLab/InternVL3-78B), [Link](https://huggingface.co/datasets/OpenGVLab/InternVL-Data) |
| [PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday
  Home Clusters](https://arxiv.org/abs/2504.08791) | Hongfang Yu, Mohsen Guizani, NeuronNomad, LiPhilip, LIKirin | - Prima.cpp is a distributed inference system designed for low-resource home clusters, enabling the execution of 70B-scale large language models (LLMs) on devices with limited resources like CPUs/GPUs, low RAM/VRAM, and using Wi-Fi. - It introduces piped-ring parallelism with prefetching to efficiently manage model weights and reduce token latency by overlapping disk loading with computation. - The system models heterogeneity in computation, communication, disk, memory, and OS to optimally assign model layers to each device's CPU and GPU using an algorithm called Halda, and minimizes token latency by modeling delays in computation, memory access, disk loading, and communication, while optimizing the use of RAM and VRAM. - Evaluation on a real-world home cluster demonstrates that prima.cpp achieves a 15x speed improvement compared to llama.cpp on 70B models, maintaining memory pressure below 6% per device. It also outperforms other distributed alternatives like exo and dllama in terms of speed and memory efficiency for models ranging from 7B to 72B. - It achieves ~600 milliseconds per token and a time-to-first-token (TTFT) below 2 seconds for a 70B model on a small heterogeneous home cluster. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Lizonghang/prima.cpp) | N/A |
| [FUSION: Fully Integration of Vision-Language Representations for Deep
  Cross-Modal Understanding](https://arxiv.org/abs/2504.09925) | Jingzhou Chen, conghui, jingwei-xu-00, Balalauuoo, starriver030515 | - Introduces FUSION, a family of Multimodal Large Language Models (MLLMs) that integrate vision and language throughout the processing pipeline, unlike existing methods that primarily focus on late-stage fusion. - Proposes Text-Guided Unified Vision Encoding, which incorporates text information during vision encoding, Context-Aware Recursive Alignment Decoding for dynamic aggregation of visual features, and Dual-Supervised Semantic Mapping Loss to ensure robust cross-modal consistency. - Presents a synthesized Language-Driven Question-Answer (QA) dataset generation method that prioritizes textual richness to guide image generation and diverse QA pair construction. - Evaluated on 22 benchmarks and demonstrates state-of-the-art performance with significantly fewer vision tokens, outperforming models like Cambrian-1 and Florence-VL, and achieving comparable results to top-tier models like InternVL2 and Qwen2VL. - Shows robustness even with reduced vision tokens and highlights the effectiveness of each component through ablation studies. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](https://github.com/starriver030515/FUSION) | N/A |
| [VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models
  with Reinforcement Learning](https://arxiv.org/abs/2504.08837) | Wei Chu, Chao Qu, wenhu, zuminghuang, JasperHaozhe | - VL-RETHINKER, a new vision-language model, enhances multimodal reasoning through reinforcement learning and a novel "forced rethinking" technique. - The model utilizes Group Relative Policy Optimization (GRPO) with Selective Sample Replay (SSR) to address the vanishing advantages problem in reinforcement learning. - Forced Rethinking encourages self-reflection by appending rethinking triggers to initial responses during training. - VL-RETHINKER achieves state-of-the-art results on MathVista (80.3%), MathVerse (61.7%), and MathVision (43.9%), outperforming existing models like GPT-01. - It also sets a new open-source state-of-the-art on MMMU-Pro, EMMA, and MEGA-Bench, closing the gap with GPT-01. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | N/A | N/A |
| [Iterative Self-Training for Code Generation via Reinforced Re-Ranking](https://arxiv.org/abs/2504.09643) | Valentin Malykh, Ivan Sedykh, Nikita Sorokin | - This research proposes RewardRanker, a new iterative self-training approach for enhancing code generation by refining reward model optimization using Proximal Policy Optimization (PPO). - This approach uses both correct and hard negative examples during training with PPO in order to optimize code generation by leveraging multiple sampled solutions. - The RewardRanker model iteratively refines the training dataset by re-evaluating generated code outputs, incorporating high-scoring negative examples into the training data. - Evaluation results on the MultiPL-E dataset demonstrates that a 13.4B parameter RewardRanker model outperforms larger models including a 33B parameter model and achieves performance comparable to GPT-4 and surpasses GPT-4 for C++. - This method does not require predefined tests during the inference stage, only during training. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | N/A |
| [Mavors: Multi-granularity Video Representation for Multimodal Large
  Language Model](https://arxiv.org/abs/2504.10068) | kugwzk, zhenhuawu, UnnamedWatcher, CheeryLJH, DogNeverSleep | - Mavors is a novel framework for long-context video understanding in Multimodal Large Language Models (MLLMs) that introduces a multi-granularity video representation. - The architecture consists of two main components: an Intra-chunk Vision Encoder (IVE) that uses 3D convolutions and Vision Transformers to extract high-resolution spatial features, and an Inter-chunk Feature Aggregator (IFA) that models temporal dependencies across chunks using a transformer with chunk-level rotary position encodings. - Mavors also unifies image and video understanding by treating images as single-frame videos and incorporates a multi-stage training process. - Experimental results across various video benchmarks demonstrate Mavors' superior performance in tasks requiring fine-grained spatio-temporal reasoning. - Specifically, Mavors-7B shows strong performance compared to other 7B MLLMs on DREAM-1K video captioning, improving from around 30 to 39.4 in Video-MME score. | ['Multimodal', 'Video-Text-to-Text', 'Video Classification'] | [Link](https://mavors-mllm.github.io/) | N/A |
| [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability
  of Large Reasoning Models](https://arxiv.org/abs/2504.10368) | Tingwen Liu, Xinghua Zhang, Starrrrrry, ShuaiyiNie, WYRipple | - Introduced S1-Bench, a benchmark designed to evaluate Large Reasoning Models' (LRMs) ability to perform simple, intuitive (System 1) thinking, as opposed to deliberative (System 2) reasoning. - S1-Bench consists of simple, diverse, and naturally clear questions across multiple domains and languages. - Evaluation of 22 LRMs revealed lower efficiency, with outputs averaging 15.5 times longer than traditional small LLMs. - LRMs often identify correct answers early but continue unnecessary deliberation, sometimes leading to errors. - Findings highlight the rigid reasoning patterns of current LRMs and the need for balanced dual-system thinking capabilities. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/WYRipple/S1_Bench) | N/A |
| [Have we unified image generation and understanding yet? An empirical
  study of GPT-4o's image generation ability](https://arxiv.org/abs/2504.08003) | Ning Li, cuijiaxing, zhangjingran | - This paper presents an empirical study of GPT-40's image generation capabilities, focusing on its ability to integrate world knowledge, contextual reasoning, and instruction adherence. - The study evaluates GPT-40 across three dimensions: Global Instruction Adherence, Fine-Grained Editing Precision, and Post-Generation Reasoning, using specifically designed prompts to test these capabilities. - The experiments reveal that GPT-40 often defaults to literal interpretations of instructions, struggles with applying knowledge constraints consistently, and exhibits limitations in conditional reasoning tasks within image generation. - These findings challenge the assumption of unified understanding and generation in multimodal LLMs like GPT-40. - The paper concludes by highlighting the need for more robust benchmarks and training strategies that emphasize knowledge-guided synthesis and contextual generalization in multimodal generation. | ['Multimodal', 'Text-to-Image'] | N/A | N/A |
| [DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM
  Post-training](https://arxiv.org/abs/2504.09710) | zwt123home123, timecuriosity, gfcui, ztwang | - DUMP, a novel distribution-level curriculum learning framework, is introduced for Reinforcement Learning (RL)-based Large Language Model (LLM) post-training, addressing the challenge of training on data from diverse distributions by dynamically adjusting the sampling probabilities based on the learnability of distributions. - The framework leverages the magnitude of policy advantages as a proxy for distribution-level learnability, with high advantages on specific data distributions indicating underfitting and high potential for improvement. - It uses a Upper Confidence Bound (UCB)-based strategy to balance exploration and exploitation, prioritizing training on distributions with high average advantage or low sample counts, and normalizing the scores via softmax to create sampling weights for different distributions during batch generation. - Instantiated with Group Relative Policy Optimization (GRPO) as the underlying RL algorithm, DUMP is evaluated on logic reasoning datasets with multiple difficulties and sources. - Experimental results show significant improvements in convergence speed and final performance compared to uniform sampling, highlighting the importance of distribution-aware training in RL-based LLM post-training. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/ZhentingWang/DUMP) | N/A |
| [SocioVerse: A World Model for Social Simulation Powered by LLM Agents
  and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157) | milesz7777, tangshiping, SimingChen, libo-ca, Lishi0905 | - SocioVerse, an LLM-agent-driven world model for social simulation, is introduced, leveraging a pool of 10 million real-world user profiles. - The framework addresses alignment challenges between simulated and real-world environments, users, interaction mechanisms, and behavioral patterns through four key components: Social Environment, User Engine, Scenario Engine, and Behavior Engine. - Large-scale simulations across political, news, and economic domains demonstrate SocioVerse’s ability to reflect real-world population dynamics with diversity, credibility, and representativeness. - A user pool of 10 million individuals, constructed from real-world social media data, powers the simulations, offering a large-scale and diverse dataset for realistic agent behavior modeling. - The simulations accurately predict US presidential election results, reproduce consistent public feedback to breaking news, and provide reliable outputs for national economic surveys. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/FudanDISC/SocioVerse) | N/A |
| [Breaking the Data Barrier -- Building GUI Agents Through Task
  Generalization](https://arxiv.org/abs/2504.10127) | jxhe, QiushiSun, changma, heroding77, leoozy | - This paper proposes a mid-training approach to enhance the performance of vision-language models on graphical user interface (GUI) agent tasks by leveraging data and knowledge from non-GUI domains. - It investigates the effectiveness of 11 mid-training tasks on both mobile and web GUI benchmarks and finds that mathematical reasoning tasks, surprisingly, even text-only ones, yield the largest performance gains due to cross-domain knowledge transfer. - In contrast to prior work, visual GUI perception datasets show limited benefits, likely due to the pre-existing visual capabilities of current VLMs. - The paper introduces GUIMid, a 300k dataset combining four top-performing domains and achieves state-of-the-art results on AndroidWorld in visual-only settings, and improves Qwen2-VL to GPT4 level performance on WebArena. - The code, data, and models related to this work are open-sourced to support future research. | ['Multimodal'] | [Link](https://github.com/hkust-nlp/GUIMid) | N/A |
| [TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning](https://arxiv.org/abs/2504.09641) | Lei Huang, Wenjun Wu, wenzz1, Zhang199 | - This paper introduces TinyLLaVA-Video-R1, a small-scale video reasoning model based on TinyLLaVA-Video, enhanced with reinforcement learning for improved reasoning abilities on general Video-QA datasets. - The model leverages the GRPO algorithm on the NextQA dataset and incorporates modifications to reward rules, including a continuous length reward and penalties for incorrect answers, leading to emergent "aha moments" during reasoning. - Unlike previous research focusing on large-scale models and reasoning-intensive datasets, this work emphasizes the value of exploring small-scale models for researchers with limited resources and enabling models to explain reasoning processes on general QA datasets. - Experimental results demonstrate significant improvements in reasoning and thinking abilities compared to the baseline TinyLLaVA-Video-SFT across benchmarks like MVBench, Video-MME, MLVU, and MMVU. - Ablation studies validate the effectiveness of the modifications to the GRPO algorithm and the importance of cold-start data for stabilizing training and improving reasoning performance in smaller models. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/ZhangXJ199/TinyLLAVA-Video-R1) | N/A |
| [LLM-SRBench: A New Benchmark for Scientific Equation Discovery with
  Large Language Models](https://arxiv.org/abs/2504.10415) | Khoa D Doan, Amir Barati Farimani, Ngoc-Hieu Nguyen, mkmeidani, parshinsh | - This paper introduces LLM-SRBench, a new benchmark designed to evaluate the capabilities of large language models (LLMs) for scientific equation discovery. - The benchmark comprises 239 challenging problems across four scientific domains (chemistry, biology, physics, and material science) and is structured around two categories: LSR-Transform (transforming common equations into less common mathematical representations) and LSR-Synth (synthetic, discovery-driven problems). - LLM-SRBench aims to address the limitations of existing benchmarks that are susceptible to memorization by LLMs by focusing on problems that require reasoning and discovery. - The authors evaluated several state-of-the-art LLM-based scientific equation discovery methods with different LLM backbones and found that the best-performing system achieved only 31.5% symbolic accuracy on LSR-Transform and 28.1% on LSR-Synth. - These findings underscore the challenges of scientific equation discovery and the need for more robust evaluation methods. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/deep-symbolic-mathematics/llm-srbench) | N/A |
| [EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental
  Health Safety](https://arxiv.org/abs/2504.09689) | Edify-Kd2024, yaozixin, YimingWang, ChrisJuan, yinghuihe | - EmoAgent, a multi-agent AI framework, is introduced to evaluate and mitigate mental health risks in human-AI interactions, particularly within character-based chatbots. - EmoAgent comprises two key components: EmoEval, which simulates virtual users with varying mental health profiles to assess changes after interaction with AI characters using standardized psychological tests (PHQ-9, PDI, PANSS), and EmoGuard, which acts as a real-time safeguard by monitoring user mental state and providing feedback to the AI. - Experiments conducted on popular character-based chatbots reveal that emotionally engaging dialogues can lead to a deterioration in mental state for vulnerable users, observed in over 34.4% of simulations. - EmoGuard demonstrates a significant reduction in these deterioration rates by offering corrective feedback and interventions, thereby improving the safety of human-AI interactions. - The study highlights the potential risks of character-based AI for mental health and provides a framework for developing safer interactive AI systems. | ['Natural Language Processing'] | [Link](https://github.com/1akaman/EmoAgent) | N/A |
| [The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via
  Agentic Tree Search](https://arxiv.org/abs/2504.08066) | Chris Lu, Shengran Hu, Robert Tjarko Lange, conglu, yyamada | - This paper introduces AI Scientist V2, an end-to-end system for automated scientific paper generation. - It iteratively formulates hypotheses, designs and executes experiments, analyzes data, and authors manuscripts using an agentic tree-search approach and language models. - Unlike its predecessor, it eliminates the need for human-coded templates and generalizes across machine-learning domains. - Notably, one AI-generated manuscript was accepted at a peer-reviewed ICLR workshop after exceeding the average human acceptance threshold. - The authors open-sourced the code and data to encourage further development in autonomous scientific discovery. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SakanaAI/AI-Scientist-v2) | N/A |
| [Executable Functional Abstractions: Inferring Generative Programs for
  Advanced Math Problems](https://arxiv.org/abs/2504.09763) | Zaid Khan, mohitbansal, j-min, archiki, esteng | - This paper introduces Executable Functional Abstractions (EFAs), a new method for generating diverse math problem variants. - EFAs are parameterized programs that encapsulate the logic of a math problem, allowing for the automated generation of new problem instances. - EFAGen, a framework for automatically constructing EFAs from static math problems is also presented. - EFAGen uses a large language model (LLM) to generate candidate EFA implementations, filtering them with unit tests to ensure validity. - Experiments demonstrate that EFAGen can construct faithful and learnable EFAs across multiple sources of math problems, with applications to data augmentation and stress-testing of models. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/zaidkhan/EFAGen) | N/A |
| [How new data permeates LLM knowledge and how to dilute it](https://arxiv.org/abs/2504.09522) | Nolan Andrew Miller, Andrey Zhmoginov, Chen Sun, gozzo87, mendor | - This paper investigates the "priming" effect in LLMs, where learning a new fact can cause its inappropriate application in unrelated contexts. - It introduces "Outlandish," a dataset of 1320 text samples to study knowledge permeation in LLMs, finding that the degree of priming can be predicted by the pre-learning token probability of key words. - The study reveals that priming and memorization are coupled during learning in some models (like PALM-2) but not others (Llama and Gemma). - The paper introduces a "stepping-stone" text augmentation strategy and an "ignore-k" update pruning method to mitigate priming effects, improving knowledge insertion specificity. - These techniques reduce priming by 50-95% while maintaining the model's learning ability, offering insights into LLM learning and tools for controlled knowledge insertion. | ['Natural Language Processing'] | [Link](https://sunchipsster1.github.io/projects/outlandish/) | N/A |
| [VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search](https://arxiv.org/abs/2504.09130) | QipengGuo, alphadl, ngc7293, sinwang, LibraTree | - VisuoThink, a novel multimodal tree search framework, is introduced to enhance the reasoning capabilities of Large Vision-Language Models (LVLMs) by dynamically integrating visual and textual information during the inference process. - It employs a step-by-step vision-text interleaved reasoning framework that utilizes multi-step visual aids from tool uses and a look-ahead tree search algorithm with a predictive rollout mechanism to explore multiple reasoning paths. - This framework enables test-time scaling by simulating the likely outcomes of different reasoning states, prioritizing promising paths, and optimizing reasoning dynamically during inference. - Experimental results show that VisuoThink significantly outperforms existing methods on various reasoning tasks, especially in geometry (improving accuracy by up to 21.8% on Geomverse) and spatial reasoning. - The improvements are attributed to the effective integration of multi-step visual cues, predictive rollout search, and tool interactions, mitigating the limitations of relying solely on text-based reasoning chains or single-step visual assistance in existing LVLMs. | ['Multimodal', 'Question Answering'] | [Link](https://github.com/ekonwang/VisuoThink) | N/A |
| [M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models](https://arxiv.org/abs/2504.10449) | Daniele Paliotta, tridao, voidptr74, xu3kev, JunxiongWang | - This paper introduces M1, a new hybrid linear RNN reasoning model built upon the Mamba architecture, designed for enhanced scalability in test-time computation. - M1 leverages a distillation process from existing reasoning models, followed by supervised fine-tuning on math datasets and reinforcement learning using the GRPO algorithm. - The model achieves comparable performance to DeepSeek-R1-Distill-Qwen-1.5B on benchmarks like MATH500, AIME25, AIME24, and OlympiadBench. - Notably, M1 demonstrates a 3x speedup compared to transformers of the same size when served using vLLM at large batch sizes, allowing for higher accuracy under fixed generation time budgets through self-consistency voting. - The increased efficiency makes resource-intensive test-time compute strategies, such as self-consistency, more practical, making M1 a strong alternative for reasoning tasks. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/jxiw/M1) | N/A |
| [LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety
  in Large Language Models](https://arxiv.org/abs/2504.10430) | Xinyi Zhang, sarvech123, aneverfull, Zhiyang03, mqliu | - This paper introduces PERSUSAFETY, a novel framework for evaluating the safety of Large Language Models (LLMs) in persuasive dialogues, focusing on their potential for unethical behavior. - PERSUSAFETY involves three stages: creating diverse persuasion tasks, simulating multi-turn conversations between LLM persuaders and persuadees, and assessing safety through refusal rates and the use of unethical strategies. - Experiments across eight LLMs revealed significant safety concerns, with most failing to consistently refuse harmful tasks and often employing unethical strategies, even when aware of persuadee vulnerabilities or under pressure. - Claude-3.5-Sonnet, while best at refusing unethical requests, exhibited a high usage of unethical strategies during persuasion, highlighting a gap in current safety alignment methods. - The study underscores the need for better safety alignment techniques to ensure ethical and responsible use of LLMs in goal-driven conversations. | ['Natural Language Processing'] | [Link](https://github.com/PLUM-Lab/PersuSafety) | N/A |
| [DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and
  Summarization?](https://arxiv.org/abs/2504.08120) | Christoph Leiter, Yanran Chen, Ran Zhang, Sotaro Takeshita, Daniil Larionov | - This paper explores the effectiveness of reasoning Large Language Models (LLMs) for evaluating machine translation (MT) and text summarization (TS) by comparing eight models with and without reasoning capabilities, across three architectural categories: state-of-the-art reasoning models, their distilled variants, and conventional LLMs. - The experiments, conducted on WMT23 and SummEval benchmarks, reveal that the benefits of reasoning are model- and task-dependent: OpenAI 03-mini models demonstrate consistent performance improvement with increased reasoning intensity, while DeepSeek-R1 generally underperforms compared to its non-reasoning variant. - Correlation analysis indicates a positive relationship between increased reasoning token usage and evaluation quality, specifically within the 03-mini models. - The study finds that distillation of reasoning capabilities can be effectively maintained in medium-sized models (32B parameters) but significantly degrades in smaller variants (8B parameters). - This research provides a comprehensive assessment of reasoning LLMs for natural language generation evaluation, offering valuable insights for practical applications and suggesting that the mere presence of reasoning capabilities isn't sufficient; their successful integration and alignment with specific evaluation requirements are crucial. | ['Natural Language Processing', 'Summarization', 'Translation'] | N/A | N/A |
| [MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in
  Multimodal Large Language Models](https://arxiv.org/abs/2504.05782) | Jiaxin Ai, Zhaopan Xu, Xiaopeng Peng, Fanrui Zhang, Pengfei Zhou | - This paper introduces MDK12-Bench, a large-scale, multi-disciplinary benchmark for evaluating the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs). - The benchmark comprises 140K reasoning questions spanning six K-12 subjects (math, physics, chemistry, biology, geography, and information science) with varying difficulty levels, detailed answer explanations, and fine-grained knowledge point annotations. - A novel dynamic evaluation framework is proposed to mitigate data contamination by transforming question form, type, and image style during testing. - Experimental results on various MLLMs demonstrate the benchmark's ability to reveal limitations in existing models' multimodal reasoning capabilities. - The research contributes a valuable resource for advancing MLLM development by offering a more challenging and comprehensive assessment of real-world reasoning skills in diverse academic contexts. | ['Multimodal', 'Question Answering', 'Visual Question Answering'] | [Link](https://github.com/LanceZPF/MDK12) | N/A |


## Papers for 2025-04-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SQL-R1: Training Natural Language to SQL Reasoning Model By
  Reinforcement Learning](https://arxiv.org/abs/2504.08600) | Ran Chen, Xuhui Jiang, Chengjin Xu, Peixian Ma, ZhuangXialie | - This paper introduces SQL-R1, a novel Natural Language to SQL (NL2SQL) reasoning model trained using reinforcement learning (RL). - SQL-R1 focuses on improving inference performance in complex scenarios with multi-table joins and nested queries, which are challenging for traditional supervised fine-tuning methods. - The model uses a specialized RL reward function to guide SQL generation and addresses the cold-start problem for effective training. - Experiments on Spider and BIRD benchmarks show SQL-R1 achieves competitive accuracy of 88.7% and 66.6%, respectively, using only a 7B base model. - The paper also explores data engineering for RL and emphasizes the model's ability to produce an explicit reasoning process, enhancing interpretability. | ['Natural Language Processing', 'Text2Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on
  Transformer Encoder Models Performance](https://arxiv.org/abs/2504.08716) | Djamé Seddah, Benoît Sagot, Wissam Antoun | - This paper investigates the performance of ModernBERT, a transformer-encoder model designed for efficiency, against DeBERTaV3 and RoBERTa models by pretraining them on the same French dataset (CamemBERTaV2's dataset) to control for data variations. - The results reveal that DeBERTaV3 surpasses ModernBERT in benchmark performance and training sample efficiency, attributing this to its superior architecture and training objective optimization. - ModernBERT, however, demonstrates significantly faster training and inference speeds due to its efficiency-focused design, making it practically advantageous. - Additionally, the study finds that training on a higher-quality, filtered dataset enhances convergence speed but does not substantially improve final performance, suggesting a potential saturation of current NLP benchmarks. - The authors release their pretrained French ModernBERT models and training scripts publicly to facilitate reproducibility and further research. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification'] | N/A | [Link](https://huggingface.co/collections/almanach/moderncamembert-67f7e6d85ede5f7cfc1ce012) |
| [Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend
  NPUs](https://arxiv.org/abs/2504.07866) | Xueyu Wu, Yehui Tang, Kaikai Song, Wenyong Huang, Yichun Yin | - This paper introduces Pangu Ultra, a 135 billion parameter, 94-layer dense Large Language Model (LLM) trained on Huawei's Ascend NPUs. - The model utilizes novel techniques like depth-scaled sandwich normalization and tiny initialization to stabilize training and prevent loss spikes, which are common in deep LLMs. - Trained on 13.2 trillion tokens and incorporating techniques for long context extension (up to 128K tokens), Pangu Ultra performs competitively with state-of-the-art LLMs, including sparse models with significantly more parameters like DeepSeek-R1, and outperforms other dense LLMs like Llama 405B and Mistral Large 2 on various benchmarks. - System-level optimizations like kernel fusion, context parallelism, and optimized attention mechanisms allow for efficient training on 8,192 Ascend NPUs, achieving over 52% Model FLOPs Utilization (MFU). - The enhanced reasoning capabilities of Pangu Ultra are demonstrated through its superior performance on specialized benchmarks like AIME 2024, MATH-500, GPQA Diamond, and LiveCodeBench. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [CoRAG: Collaborative Retrieval-Augmented Generation](https://arxiv.org/abs/2504.01883) | Virginia Smith, Mona Diab, Aashiq Muhamed | - Introduces CoRAG, a framework for collaborative Retrieval-Augmented Generation (RAG) where multiple clients jointly train a shared model using a collaborative passage store, but use their local passage stores during inference. - Proposes CRAB, a homogeneous open-domain question answering benchmark for evaluating CoRAG and exploring the impact of passage composition within the collaborative store.  - Demonstrates through experiments on CRAB that CoRAG consistently outperforms parametric collaborative learning methods and locally trained RAG models, especially in few-shot settings, achieving up to a 33.8% improvement over local RAG in 16-shot scenarios. - Reveals that relevant passages are critical for generalization, surprisingly irrelevant passages can be beneficial, while hard negatives negatively impact performance. - Identifies a key challenge in CoRAG, where clients must balance the advantages of an enriched store with the risk of incorporating detrimental passages from others, and introduces mechanisms for encouraging participation. | ['Question Answering'] | [Link](https://github.com/aashiqmuhamed/CORAG) | N/A |
| [Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning
  vs. Memorization in Large Language Models](https://arxiv.org/abs/2504.05262) | Zhenzhong Lan, Renjun Xu, Yu Lu, Yang Yan | - This paper investigates whether Large Language Models (LLMs) genuinely understand mathematical principles or rely on memorization by evaluating their performance on elementary two-integer addition. - The study probes two core properties: commutativity (A + B = B + A) and compositional generalization using symbolic mappings (e.g., 7 → Y). - Despite high accuracy on numerical addition, LLMs' performance collapses under symbolic mapping and exhibits non-monotonic scaling with digit count, along with frequent commutativity violations. - Providing explicit addition rules degrades performance, while self-explanation maintains baseline accuracy, suggesting misalignment between LLM arithmetic processing and human-defined principles. - The findings indicate current LLMs rely on memory patterns over rule learning, highlighting limitations and the need for new approaches for genuine mathematical reasoning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2025-04-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Kimi-VL Technical Report](https://arxiv.org/abs/2504.07491) | dongliangwang, congcongwang, DuChenZhuang, tzzcl, xingbowei | - Kimi-VL is an open-source Mixture-of-Experts (MoE) vision-language model with advanced multimodal reasoning, long-context understanding, and agent capabilities. - It consists of a 2.8B parameter MoE language decoder (Kimi-VL-A3B) paired with a 400M native-resolution MoonViT vision encoder. - It outperforms other efficient vision-language models like DeepSeek-VL2 and Qwen2.5-VL-7B on various benchmarks, including college-level image and video comprehension, OCR, and mathematical reasoning. - Kimi-VL-Thinking, a long-thinking variant fine-tuned with long chain-of-thought (CoT) and reinforcement learning, further improves the performance on complex multimodal reasoning tasks, achieving scores of 61.7 on MMMU, 36.8 on MathVision and 71.3 on MathVista. - Kimi-VL also excels in processing long contexts with a 128K extended context window and high-resolution visual inputs. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Document Question Answering', 'Video-Text-to-Text'] | [Link](https://github.com/MoonshotAI/Kimi-VL) | N/A |
| [VCR-Bench: A Comprehensive Evaluation Framework for Video
  Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.07956) | lovesnowbest, Lin-Chen, Osilly, ChthollyTree, yukunqi | - VCR-Bench, a novel benchmark designed to comprehensively evaluate Large Vision-Language Models' (LVLMs) video Chain-of-Thought (CoT) reasoning capabilities. - It consists of 859 videos with diverse content, along with 1,034 question-answer pairs, each manually annotated with stepwise CoT rationales. - Introduces CoT score by categorizing CoT steps into visual perception and logical reasoning and evaluates them across various task dimensions (recall, precision). - Exposes limitations of current LVLMs, with even top-performing models showing subpar CoT scores, particularly in perception tasks involving temporal-spatial information extraction. - Validates framework's effectiveness through a strong positive correlation between CoT scores and accuracy, indicating its crucial role in complex video reasoning. | ['Video-Text-to-Text', 'Multimodal', 'Question Answering', 'Visual Question Answering'] | N/A | [Link](https://vlm-reasoning.github.io/VCR-Bench/) |
| [MM-IFEngine: Towards Multimodal Instruction Following](https://arxiv.org/abs/2504.07957) | yhcao, sweetFruit, KennyUTC, yuhangzang, ChrisDing1105 | - This paper introduces MM-IFEngine, a new pipeline for generating high-quality image-instruction pairs for multimodal instruction following. - It also presents MM-IFEval, a challenging benchmark for multimodal instruction following with diverse constraints and a hybrid evaluation approach. - The authors create two datasets using the pipeline: MM-IFInstruct-23k for supervised fine-tuning and MM-IFDPO-23k for direct preference optimization. - Experiments show that fine-tuning MLLMs on these datasets significantly improves performance on various instruction following benchmarks, including MM-IFEval, MIA, and IFEval. - The models fine-tuned on these datasets maintains comparable performance on other visual question answering benchmarks. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/SYuan03/MM-IFEngine) | N/A |
| [DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning](https://arxiv.org/abs/2504.07128) | parishadbehnam, miladink, vaibhavad, arkilpatel, spaidartaigar | - This paper introduces a taxonomy for analyzing the reasoning chains of Large Reasoning Models (LRMs), focusing on DeepSeek-R1. - The study finds that DeepSeek-R1's reasoning chains follow a consistent structure, starting with problem definition, followed by decomposition ('Bloom cycle'), and iterative reconstruction cycles ('rumination'). - The research reveals a 'sweet spot' for reasoning length, beyond which performance declines, and that DeepSeek-R1 struggles to adhere to specified token budgets. - The analysis also shows DeepSeek-R1 prioritizes context over parametric knowledge, exhibits safety vulnerabilities compared to its non-reasoning counterpart, and displays cultural biases in moral reasoning. - It highlights correlations between model reasoning chains and human processing of challenging sentences, while also noting limitations in the model's iterative refinement and world modeling capabilities during visual reasoning tasks. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization
  for Test-Time Expert Re-Mixing](https://arxiv.org/abs/2504.07964) | Ziyue Li, zhoutianyi, Lzy01241010 | - C3PO, Critical-Layer, Core-Expert, Collaborative Pathway Optimization, is a novel test-time optimization method for Mixture-of-Experts (MoE) Large Language Models (LLMs) that improves expert pathway selection. - It addresses the sub-optimality of expert pathways generated by pre-trained routers by jointly optimizing the core experts' mixing weights in critical layers for each test sample based on similar, successful samples from a reference set.  - C3PO employs three surrogate objectives and algorithms: mode-finding, kernel regression, and average loss of similar samples, and leverages gradient-free or gradient-based optimization depending on the objective. - Experiments on two MoE LLMs (OLMOE and DeepSeekMoE) across six benchmarks demonstrate a consistent 7-15% accuracy improvement over base models, outperforming existing test-time learning methods like in-context learning and prompt tuning. - Notably, C3PO enables MoE LLMs with 1-3B active parameters to outperform larger 7-9B parameter LLMs, highlighting its efficiency benefits. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/tianyi-lab/C3PО) | N/A |
| [MOSAIC: Modeling Social AI for Content Dissemination and Regulation in
  Multi-Agent Simulations](https://arxiv.org/abs/2504.07830) | Marzyeh Ghassemi, saadia, elisakreiss, salmannyu, genglinliu | - MOSAIC, a novel open-source multi-agent social network simulation framework, uses generative language agents (LLMs) to model user behaviors like liking, sharing, and flagging content, combined with a directed social graph, to analyze emergent deception behaviors and how users assess online content veracity. - MOSAIC constructs user representations from diverse, fine-grained personas, enabling multi-agent simulations to model content dissemination and engagement dynamics at scale, and evaluating three content moderation strategies (community-based, third-party, and hybrid fact-checking) with simulated misinformation. - The simulations demonstrate that these strategies not only mitigate the spread of misinformation but also increase user engagement, unlike human social media behavior. - The study explores content popularity trajectories, finding that agents' articulated reasoning for interactions doesn't always align with collective engagement patterns, and observed that misinformation didn't spread faster than real news with LLM-based agents. - The framework is open-sourced to promote research in AI and social science, enabling controlled experiments for studying online behavior and content moderation strategies. | ['Natural Language Processing'] | [Link](https://github.com/genglinliu/MOSAIC) | N/A |
| [SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual
  Reasoning Self-Improvement](https://arxiv.org/abs/2504.07934) | furongh-lab, kevinlin311tw, linjieli222, zyang39, russwang | - This paper introduces ThinkLite-VL, a novel approach for enhancing the visual reasoning abilities of Vision-Language Models (VLMs) using a data-efficient training method that relies on self-improvement without knowledge distillation. - The key contribution is an MCTS-guided sample selection mechanism, which quantifies the difficulty of training examples based on the number of MCTS iterations required by the base VLM to solve each problem. - Using only 11k training samples, the resulting ThinkLite-VL-7B model demonstrates state-of-the-art performance on eight visual reasoning benchmarks, outperforming other 7B-level reasoning VLMs. - Notably, ThinkLite-VL-7B achieves 75.1% accuracy on MathVista, surpassing larger open-sourced models, GPT-40, and O1. - Ablation studies confirm the importance of MCTS-based sample selection by revealing that training exclusively on easy samples does not improve model reasoning ability, and that unsolved samples identified by MCTS pose significant challenges and contribute substantially to enhancing the model's reasoning capabilities during reinforcement learning. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/si0wang/ThinkLite-VL) | N/A |
| [Scaling Laws for Native Multimodal Models Scaling Laws for Native
  Multimodal Models](https://arxiv.org/abs/2504.07951) | Joshua Susskind, Matthieu Cord, Victor Guilherme Turrisi da Costa, Enrico Fini, Mustafa Shukor | - This paper investigates the scaling properties of native multimodal models (NMMs), which are trained from scratch on multimodal data, without relying on pre-trained components. - The study conducts extensive experiments on early and late fusion architectures, training 457 models with different architectures and training mixtures, and finds that early fusion models exhibit stronger performance at lower parameter counts and are more efficient to train. - The research derives scaling laws for NMMs and demonstrates that they follow similar trends as LLMs, albeit with slight variations in scaling coefficients, suggesting model parameters and training tokens should be scaled roughly equally for optimal performance. - It explores the benefits of incorporating Mixture of Experts (MoEs) into NMMs and observes significant performance improvements, with scaling laws indicating that scaling training tokens is more crucial than scaling parameters for MoEs. - Analysis reveals that experts in MoE models tend to specialize in different modalities, particularly in the early and last layers, demonstrating the potential for multimodal specialization within a unified architecture. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Towards Visual Text Grounding of Multimodal Large Language Model](https://arxiv.org/abs/2504.04974) | Franck-Dernoncourt, YfZ, JoshuaGu, zhangry868, MingLiiii | - This paper introduces TRIG, a novel Text-Rich Image Grounding task and benchmark for evaluating and improving the visual text grounding capabilities of Multimodal Large Language Models (MLLMs) in document question-answering. - A new dataset, TRIG-Bench, consisting of 800 manually annotated question-answer pairs from DocVQA, ChartQA, InfographicsVQA, and TRINS, along with a 90k synthetic training dataset generated using an OCR-LLM-human pipeline is also presented. - Two methods are proposed: an instruction-tuning method and an embedding-based method; evaluation shows that both outperform current MLLMs on the benchmark, with the embedding method being more efficient. - Existing MLLMs struggle with visual text grounding on text-rich document images, often failing to follow customized instructions requiring spatial understanding. - The authors suggest that this task is under-explored and needs more attention from the community, emphasizing the need for MLLMs to better handle complex document layouts and understand instructions requiring deep spatial reasoning. | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | N/A | N/A |


## Papers for 2025-04-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
  Tokens](https://arxiv.org/abs/2504.07096) | Yensung, sewon, yanaiela, taylorb, liujch1998 | - OLMOTRACE is a system that traces the output of large language models (LLMs) back to their original training data, which consists of trillions of tokens. - It identifies verbatim matches between segments of the LLM's output and documents within its training corpora. - Uses an extended version of infini-gram, allowing for real-time tracing results within seconds. - Aims to enhance user understanding of LLM behavior by revealing potential sources of information or learned sequences within the training data. - OLMOTRACE is publicly available and open-source. | ['Natural Language Processing'] | [Link](https://github.com/allenai/infinigram-api) | [Link](https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct) |
| [A Unified Agentic Framework for Evaluating Conditional Image Generation](https://arxiv.org/abs/2504.07046) | Yiyu Wang, Longyue Wang, Xue Yang, Jifang Wang, imryanxu | - This paper introduces CIGEVAL, a unified agentic framework for evaluating conditional image generation tasks using large multimodal models (LMMs) like GPT-40 and open-source 7B models. - CIGEVAL integrates a multi-functional toolbox for nuanced analysis, including grounding, difference, highlighting, and scene graph tools, and uses a divide-and-conquer approach for fine-grained evaluation of multiple conditions like text prompts, subject images, and control signals. -  It synthesizes evaluation trajectories for fine-tuning, enabling smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. - Experiments on ImagenHub across seven tasks show CIGEVAL (GPT-40 version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47 and surpassing previous state-of-the-art methods. -  Fine-tuning with just 2.3K trajectories allows 7B open-source LMMs to exceed the prior GPT-40-based state-of-the-art. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/HITsz-TMG/Agentic-CIGEval) | N/A |
| [Missing Premise exacerbates Overthinking: Are Reasoning Models losing
  Critical Thinking Skill?](https://arxiv.org/abs/2504.06514) | Ming Li, zhoutianyi, sunlichao137, Fcr09 | - This paper introduces the concept of "Overthinking under Missing Premise (MiP-Overthinking)", a phenomenon where reasoning Large Language Models (LLMs) generate excessively long responses to ill-posed questions with missing premises. - The authors curate four MiP datasets across varying difficulty levels, employing three distinct generation strategies: Rule-Based Generation, Body-Question Swapping, and Essential-Premise Removal. - Experimental results on a diverse set of LLMs demonstrate that reasoning models produce significantly longer responses (2x-4x more tokens) for MiP questions compared to well-defined questions and non-reasoning models. - Despite the extended reasoning, these models exhibit low abstain rates on MiP questions, indicating a lack of genuine critical thinking skills, contradicting test-time scaling law. - Further analysis reveals that reasoning models often detect the missing premise early in the reasoning process but continue to generate redundant content, while non-reasoning models efficiently identify and abstain from such questions. | ['Question Answering'] | [Link](https://github.com/tianyi-lab/MiP-Overthinking) | N/A |
| [FantasyTalking: Realistic Talking Portrait Generation via Coherent
  Motion Synthesis](https://arxiv.org/abs/2504.04842) | Yunpeng Zhang, Yaqi Fan, Mengchao Wang, fanjiang, wangqiang9 | - FantasyTalking is a novel framework that leverages a pretrained video diffusion transformer model to generate realistic and coherent talking portraits with controllable motion dynamics from a single portrait image, voice, and text. - It employs a dual-stage audio-visual alignment strategy, utilizing clip-level training for global motion coherence and frame-level training for precise lip synchronization. - Instead of a reference network, it uses a facial-focused cross-attention module to maintain facial identity. - A motion intensity modulation module allows control over expression and body motion intensity. - Experimental results on tame and wild talking head datasets demonstrate superior performance compared to existing state-of-the-art methods in video quality, temporal consistency, and motion diversity. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089) | Cxxs, Wayne-lc, Dakerqi, JiakangYuan, yeeeeeyy | - OmniCaptioner is a versatile visual captioning framework generating fine-grained textual descriptions across diverse visual domains, including natural images, visual text, and structured visuals. - Unlike existing methods limited to specific image types, OmniCaptioner provides a unified solution, bridging the gap between visual and textual modalities by converting pixel information into semantic textual representations. - It outperforms existing models in visual reasoning tasks when integrated with LLMs, enhances image generation by providing more accurate descriptions, and allows for a more efficient supervised fine-tuning (SFT) process due to its diverse pretraining dataset. - The framework consists of Seed-Caption Generation for precise pixel-to-word mapping using GPT-40 and Caption Extension to enrich styles and incorporate reasoning knowledge using Qwen2.5-32B. - Evaluation across visual reasoning, image generation, and SFT benchmarks demonstrates OmniCaptioner's superior performance and versatility in bridging visual and language modalities. | ['Image-to-Text', 'Multimodal'] | [Link](https://github.com/Alpha-Innovator/OmniCaptioner) | [Link](https://huggingface.co/U4R/OmniCaptioner) |
| [RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts](https://arxiv.org/abs/2504.06947) | Anna Lapanitsyna, Natalia Tkachenko, Natalia Loukachevitch, nicolay-r, RefalMachine | - This paper introduces RuOpinionNE-2024, a shared task focused on extracting structured opinion tuples from Russian news text. - The task involves identifying sentiment holders, targets, expressions, and polarity within sentences, contributing to a deeper understanding of sentiment dynamics in news. - The competition saw over 100 submissions, primarily employing large language models (LLMs) in zero-shot, few-shot, and fine-tuning settings. - Fine-tuning a large language model yielded the best performance on the test set. - A comparison of 30 prompts and 11 open-source LLMs with varying parameter sizes (3-32 billion) in 1-shot and 10-shot learning revealed optimal model and prompt configurations. | ['Natural Language Processing', 'Text Classification', 'Question Answering'] | N/A | N/A |
| [DiTaiListener: Controllable High Fidelity Listener Video Generation with
  Diffusion](https://arxiv.org/abs/2504.04010) | chaubeyG, hongkung, minhtran, Boese0601, havent-invented | - DiTaiListener is a novel diffusion-based model for generating high-fidelity listener response videos from speaker audio and facial motion inputs, also incorporating text-based control for customized responses. - It uses a Causal Temporal Multimodal Adapter (CTM-Adapter) within a Diffusion Transformer (DiT) architecture to process multimodal inputs and generate realistic facial expressions directly in pixel space, rather than relying on intermediate 3DMM representations and rendering. - For long video generation, DiTaiListener-Edit refines transitions between independently generated segments, ensuring smooth and coherent motions. - On benchmark datasets like RealTalk and VICO, DiTaiListener achieves state-of-the-art performance in both photorealism (+73.8% FID on RealTalk) and motion representation (+6.1% FD metric on VICO). - User studies confirm DiTaiListener's superior performance regarding feedback, diversity, and smoothness. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement
  Fine-Tuning](https://arxiv.org/abs/2504.06958) | Lanxingxuan, donglu, desenmeng, Aurorana, xinhaoli | - This paper introduces VideoChat-R1, a video multimodal large language model (MLLM) enhanced for spatio-temporal perception using Reinforcement Fine-Tuning (RFT) with Group Relative Policy Optimization (GRPO). - RFT is shown to be highly data-efficient, improving performance on specific tasks without impacting general capabilities or chat abilities.  - VideoChat-R1 achieves state-of-the-art results on spatio-temporal perception tasks such as temporal grounding (+31.8 compared to Qwen2.5-VL-7B) and object tracking (+31.2), and it also shows improvements on general video QA benchmarks. - The paper suggests that training on spatio-temporal perception tasks can strengthen a model's spatio-temporal reasoning ability.  - The authors provide comprehensive analysis and ablation studies demonstrating the efficacy of RFT for Video MLLMs. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/OpenGVLab/VideoChat-R1) | N/A |


## Papers for 2025-04-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniSVG: A Unified Scalable Vector Graphics Generation Model](https://arxiv.org/abs/2504.06263) | Jiaxu Zhang, Xianfang Zeng, Yiying Yang, CH3COOK, wchengad | - OmniSVG is a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal complex SVG generation. - It parameterizes SVG commands and coordinates into discrete tokens, decoupling structural logic from low-level geometry, and mitigating the "coordinate hallucination" problem. - OmniSVG introduces MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. - Experimental results demonstrate that OmniSVG outperforms existing methods across various SVG generation tasks, including Text-to-SVG, Image-to-SVG, and Character-Reference SVG generation. - OmniSVG shows potential for integration into professional SVG design workflows due to its ability to generate high-quality, complex, and editable SVG content from diverse modalities. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](http://omnisvg.github.io) | N/A |
| [Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought](https://arxiv.org/abs/2504.05599) | Jiangbo Pei, Yichen Wei, Xiaokun Wang, Chris, Yi Peng | - Skywork R1V, a 38B parameter multimodal reasoning model, extends the R1-series Large Language Models (LLMs) to visual modalities using a lightweight visual projector and a hybrid optimization strategy combining Iterative Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). - The model employs an adaptive-length Chain-of-Thought distillation approach for efficient reasoning and outperforms other similarly sized models, achieving scores of 69.0 on MMMU and 67.5 on MathVista. - It also maintains robust textual reasoning performance, with scores of 72.0 on AIME and 94.0 on MATH500. - The model architecture involves a staged approach, first training an MLP adapter to align a vision encoder with a substitute language model and then transferring this adapter to bridge the encoder with the reasoning-capable R1 LLM. -  The adaptive-length Chain-of-Thought distillation dynamically optimizes the reasoning chain length, improving inference efficiency. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text', 'Question Answering'] | N/A | [Link](https://huggingface.co/Skywork/Skywork-R1V-38B) |
| [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261) | Vage Egiazarian, George Yakushev, Alina Shutova, Roman Garipov, Gleb Rodionov | - This paper introduces Hogwild! Inference, a novel parallel LLM inference method designed to enhance reasoning and generation speed by enabling multiple LLM instances to collaborate dynamically during inference. - The core idea is to allow concurrent access to a shared key-value cache, which eliminates the need for recomputing key-value representations for each worker and facilitates real-time interaction between instances using Rotary Position Embeddings (ROPE). - The authors experiment with three cache layouts: contiguous (token-wise), interleaved (step-wise), and combined, with their preliminary results demonstrating improved performance on mathematical reasoning tasks compared to single-thread baselines. - Through prompting, the LLMs are encouraged to collaborate by devising and adjusting their own strategies for problem-solving, rather than relying on pre-defined frameworks. - The initial experiments suggest that reasoning-capable LLMs like QwQ and DeepSeek-R1 can effectively leverage this shared cache approach to improve efficiency and potentially quality on tasks requiring long-chain reasoning, such as those in the LIMO dataset. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/eqimp/hogwild_llm) | [Link](https://huggingface.co/Qwen/QwQ-32B), [Link](https://huggingface.co/datasets/GAIR/LIMO) |
| [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810) | Baizhou Huang, Ruilin Yan, Xiangyu Wang, YitaoLiang, pkuHaowei | - This paper introduces KUMO, a generative evaluation framework for assessing reasoning in Large Language Models (LLMs). - KUMO generates diverse, multi-turn reasoning tasks across various domains,  using LLMs combined with symbolic engines to create partially observable environments with adjustable difficulty. - The authors evaluated 23 state-of-the-art LLMs on 5,000 KUMO tasks and found that reasoning-scaled LLMs perform comparably to university students on complex reasoning challenges, while even standard LLMs outperform humans on simpler tasks. - The paper shows strong correlation between LLM performance on KUMO and other reasoning benchmarks like MMLU-Pro and LiveBench-Reason, and demonstrates KUMO's resistance to data contamination by showing that fine-tuned LLMs overfit to specific domains. - An analysis of parsing errors and token consumption revealed that some LLMs struggle with KUMO's format and generate excessively long or irrelevant outputs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/linhaoweil/kumo) | N/A |
| [V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric
  Capabilities in Multimodal Large Language Models](https://arxiv.org/abs/2504.06148) | Alex Jinpeng Wang, Ping Yu, Zhengyuan Yang, Linjie Li, Fengx1nn |  - V-MAGE is a novel game-based benchmark designed to evaluate visual-centric capabilities of Multimodal Large Language Models (MLLMs).  - It features five diverse games with over 30 handcrafted levels, testing various visual reasoning skills such as positioning, tracking, timing, and visual memory.  - V-MAGE reveals significant challenges in MLLMs' visual perception and reasoning, with top-performing models exhibiting substantial performance gaps compared to humans.  - The benchmark uses an Elo-based ranking system for comparing model performance and facilitates iterative model improvements.  - The findings highlight crucial limitations of current MLLMs and suggest avenues for improvement from an agent-centric perspective. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/CSU-JPG/V-MAGE) | N/A |
| [CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs
  with Controllable Puzzle Generation](https://arxiv.org/abs/2504.00043) | William W. Cohen, Bill Yuchen Lin, Langlin Huang, Chengsong Huang, Jixuan Leng | - CrossWordBench, a novel benchmark for evaluating the reasoning capabilities of LLMs and LVLMs using crossword puzzles, is introduced. - The benchmark features controllable puzzle generation in multiple formats (text and image) and offers various evaluation strategies. - Evaluations on over 20 models demonstrate that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints. - LVLMs struggle with the task, showing a strong correlation between their performance and grid-parsing accuracy. - Findings offer insights into the limitations of current LLMs and LVLMs and provide an effective approach for creating multimodal constrained tasks. | ['Multimodal'] | [Link](https://github.com/SeanLeng1/CrossWordBench) | [Link](https://huggingface.co/datasets/HINT-lab/CrossWordBench) |
| [Accelerate Parallelizable Reasoning via Parallel Decoding within One
  Sequence](https://arxiv.org/abs/2503.20533) | Yijiong Yu | - This paper introduces "Parallel Decoding in One Sequence," a novel decoding method designed to accelerate the reasoning process of Large Language Models (LLMs) for parallelizable tasks. - The method modifies the attention mask and position IDs, enabling parallel decoding of multiple tokens within a single sequence, thereby improving efficiency without requiring additional memory or KV cache recomputation. - Experiments on retrieval, multi-document QA, and planning tasks demonstrate a significant speedup in decoding time (over 100% in some cases) while maintaining answer quality. - The method is generally applicable across different LLMs without requiring additional training or modules. - Evaluation shows improved decoding speed by approximately 70% and 40% for Qwen2.5-14b and Qwen2.5-7b on the planning task with little to no reduction in answer quality as measured by GPT-4o rating scores. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/yuyijiong/parallel-decoding-in-one-sequence) | N/A |


## Papers for 2025-04-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [One-Minute Video Generation with Test-Time Training](https://arxiv.org/abs/2504.05298) | guestrin, zhaoyue-zephyrus, GashonHussein, koceja, karansdalal | - This paper introduces a new approach to generate one-minute videos from text storyboards using Test-Time Training (TTT) layers within a pre-trained Diffusion Transformer model. - TTT layers, whose hidden states are neural networks, offer increased expressiveness compared to traditional RNN layers with fixed-size hidden states, enabling the model to capture complex, multi-scene stories with dynamic motion. - The authors curate a text-to-video dataset based on Tom and Jerry cartoons to demonstrate the effectiveness of their approach, focusing on complex narratives and dynamic motion. - In human evaluations, the proposed method with TTT layers outperforms baselines like Mamba 2, Gated DeltaNet, and sliding-window attention by a significant margin (34 Elo points), demonstrating superior coherence and motion naturalness in generated videos. - Despite the promising results, the generated videos still contain some artifacts, and improving the efficiency of the TTT-MLP kernel is identified as future work. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/abs/2504.05299) | eliebak, mervenoyan, mfarre, orrzohar, andito | - SmolVLM, a family of compact, multimodal models designed for resource-efficient inference, are introduced, demonstrating strong performance on image and video tasks with minimal memory footprints. - Architectural configurations, tokenization strategies, and data curation are systematically explored for low computational overhead. - SmolVLM-256M uses <1GB GPU memory during inference and outperforms the much larger Idefics-80B.  - The largest model, SmolVLM-2.2B, rivals state-of-the-art VLMs while using half the GPU Memory. - Strategic architectural optimizations, aggressive tokenization, and curated training data significantly enhance performance, enabling practical deployments at smaller scales. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Video-Text-to-Text'] | N/A | [Link](spaces/smolvlm2), [Link](spaces/smolvlm-webgpu), [Link](apple/huggingsnap), [Link](community/smol-research) |
| [URECA: Unique Region Caption Anything](https://arxiv.org/abs/2504.05305) | Heeji Yoon, seungryong, crepejung00, junwann, SammyLim | - This paper introduces URECA, a new region-level captioning model designed to generate unique captions for multi-granularity regions. - The model leverages a mask encoder network and dynamic mask modeling to preserve essential region properties such as position and shape, addressing the challenge of generating distinguishable captions for visually similar regions. - The authors also present a novel dataset, URECA dataset, specifically curated for unique captioning of multi-granularity regions using a stage-wise data pipeline with Multimodal Large Language Models (MLLMs). - URECA achieves state-of-the-art performance on the URECA dataset and demonstrates strong generalization on benchmark datasets like Visual Genome and RefCOCOg. - Experimental results show that fine-tuning existing captioning models on URECA dataset enhances their multi-granularity captioning capabilities. | ['Image-to-Text', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/cvlab-kaist/URECA) | N/A |
| [LiveVQA: Live Visual Knowledge Seeking](https://arxiv.org/abs/2504.05288) | Yao Wan, Mingyang Fu, shuaishuaicdp, Tim666, Ayiirep | - LIVEVQA, a new dataset designed to assess AI systems' ability to answer questions requiring up-to-date visual knowledge. - It consists of 3,602 visual questions from 6 news websites and 14 categories, pairing images with basic comprehension questions and two multi-hop reasoning questions requiring news context. - Evaluation across 15 Multimodal Large Language Models (MLLMs) shows that while larger models perform better, challenges persist in handling multi-hop visual questions demanding real-world knowledge.  - Integrating GUI-based multimodal search substantially improves performance, especially on complex questions, with Gemini-2.0-Flash achieving 29% accuracy.  - Despite strong textual skills, search engine-equipped models struggle with visual questions needing recent visual information, indicating areas for further research. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [Are You Getting What You Pay For? Auditing Model Substitution in LLM
  APIs](https://arxiv.org/abs/2504.04715) | Tianneng Shi, Will Cai, dawnsong, Xuandong | - This paper explores the challenge of Large Language Model (LLM) substitution within black-box APIs where providers might substitute advertised models with cheaper alternatives. - The authors investigate the effectiveness of several detection methods, including output-based statistical tests, benchmark evaluations, log probability analysis, and identity prompting. - Results reveal the limitations of methods relying solely on text outputs, particularly against attacks such as model quantization and randomized substitution.   - The study finds that log probability analysis provides more reliable verification but depends on API access. - The authors also propose using Trusted Execution Environments (TEEs) for hardware-based model integrity verification.  | ['Natural Language Processing'] | [Link](https://github.com/sunblaze-ucb/llm-api-audit) | N/A |
| [DiaTool-DPO: Multi-Turn Direct Preference Optimization for
  Tool-Augmented Large Language Models](https://arxiv.org/abs/2504.02882) | Donghun Lee, dsindex, junrae, gaeunseo, hash2430 | - DiaTool-DPO, a novel method to enhance the conversational abilities of Tool-Augmented Large Language Models (TA-LLMs) by using Direct Preference Optimization (DPO) is introduced. - The approach models TA-LLM interactions as a Markov Decision Process (MDP) and introduces a specialized objective loss that contrasts preferred and rejected dialogue trajectories to control conversation flow. - The method constructs paired trajectory datasets automatically and improves on existing techniques by addressing challenges in handling incomplete queries and out-of-scope requests. - The proposed method approaches GPT-40's performance achieving 94.8% in information gathering and 91% in tool call rejection. - The presented approach substantially improves baseline performance which only achieved 44% and 9.6% respectively for the same tasks. | ['Natural Language Processing', 'Text2Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [VAPO: Efficient and Reliable Reinforcement Learning for Advanced
  Reasoning Tasks](https://arxiv.org/abs/2504.05118) | Ruofei Zhu, Xiaochen Zuo, Qiying Yu, Yufeng Yuan, YuYue | - VAPO, a Value-based Augmented Proximal Policy Optimization framework, is introduced for enhancing reasoning models within the value-based paradigm. - When evaluated on the AIME 2024 dataset, VAPO, utilizing the Qwen 32B pre-trained model, achieves a state-of-the-art score of 60.4, outperforming DeepSeek-R1-Zero-Qwen-32B and DAPO by over 10 points under identical settings. - The framework addresses three key challenges in value-based methods for long chain-of-thought (long-CoT) reasoning: value model bias, heterogeneous sequence lengths, and reward signal sparsity. - VAPO incorporates Value-Pretraining, Length-adaptive GAE, and a combination of Clip-Higher, Positive Example LM Loss, and Group-Sampling techniques to overcome these challenges. - VAPO is shown to not only outperform but uses fewer training steps than previous methods, showcasing stable and efficient training without crashes across multiple independent runs. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Clinical ModernBERT: An efficient and long context encoder for
  biomedical text](https://arxiv.org/abs/2504.03964) | Jeffrey N. Chiang, Anthony Wu, Simonlee711 | - This paper introduces Clinical ModernBERT, a transformer-based encoder pre-trained on a large-scale biomedical corpus of literature, clinical notes, and medical ontologies, including PubMed abstracts, MIMIC-IV clinical data, and medical codes with textual descriptions. - Building upon ModernBERT, it incorporates architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and an extended context length of up to 8,192 tokens. - Clinical ModernBERT excels at generating semantically rich representations specifically tailored for long-context medical tasks, outperforming domain baselines in benchmarks such as named entity recognition (NER), retrieval, and classification tasks, including achieving state-of-the-art performance on long-context tasks like i2b2 concept extraction. - The model's weights and tokenizer are publicly available. - Latent space visualizations demonstrate improved alignment with clinical ontologies, showing its ability to internalize medical taxonomies. | ['Natural Language Processing', 'Token Classification', 'Text Classification'] | [Link](https://github.com/Simonlee711/Clinical_ModernBERT) | [Link](https://huggingface.co/Simonlee711/Clinical_ModernBERT) |
| [JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language
  Model](https://arxiv.org/abs/2504.03770) | Li Li, Yi Nian, yuehanqi, yuehanqi, Chouoftears | - JAILDAM, a memory-centered test-time adaptive framework, is introduced for detecting jailbreak attempts in vision-language models (VLMs). - It uses a memory bank of unsafe concepts generated by GPT-40 based on safety guidelines. - Multimodal safe inputs are encoded using CLIP and compared to the memory bank through attention mechanism to generate attention features which is then feed into an autoencoder. - During inference, if an input's similarity to the memory is low, and the reconstruction error is high, the input is considered harmful, and the least-used concept in memory is updated with a residual representation of the input, enabling adaptation to new attacks. - Experimental results show that JAILDAM outperforms existing methods in jailbreak detection accuracy and speed. | ['Multimodal'] | [Link](https://github.com/ShenzheZhu/JailDAM) | N/A |
| [GlotEval: A Test Suite for Massively Multilingual Evaluation of Large
  Language Models](https://arxiv.org/abs/2504.04155) | Ona de Gibert, Sawal Devkota, Joseph Attieh, Zihao Li, zuenmin | - GlotEval is a lightweight framework designed for massively multilingual evaluation of large language models (LLMs), supporting seven key tasks across dozens to hundreds of languages. - It features consistent multilingual benchmarking by standardizing language codes, language-specific prompt templates for diverse linguistic settings, and non-English-centric machine translation evaluation. - GlotEval integrates 20+ existing multilingual benchmarks and supports customizable prompts for each language, along with automated translation of prompt templates to 130+ languages. - A case study comparing EMMA-500 and Llama-2-7B on multilingual translation tasks demonstrates the framework's ability to reveal performance differences under various prompting strategies and language-centric settings. - The framework promotes more inclusive LLM evaluation by focusing on both widely spoken and underrepresented languages. | ['Natural Language Processing', 'Translation', 'Text Classification', 'Summarization'] | [Link](github.com/MaLA-LM/GlotEval) | N/A |


## Papers for 2025-04-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving](https://arxiv.org/abs/2504.02605) | Linhao Zhang, Hanwu Chen, Wei Liu, Zhirong Huang, Daoguang Zan | - This paper introduces Multi-SWE-bench, a multilingual benchmark for issue resolving comprising 1,632 human-validated issues across seven programming languages (Java, TypeScript, JavaScript, Go, Rust, C, and C++). - It also launches Multi-SWE-RL, an open-source community and dataset with 4,723 instances for building reinforcement learning environments for issue resolution. -  The authors evaluate nine state-of-the-art large language models (LLMs) across three representative methods (Agentless, SWE-agent, and OpenHands), demonstrating limited generalization of current LLMs beyond Python. - Analysis reveals that model performance is sensitive to issue difficulty, description length, and the complexity of fix patches, with shorter, single-file patches generally being easier to resolve. - The benchmark and community aim to catalyze the development of more robust, adaptable, and scalable multilingual code agents, fostering progress toward automated issue resolution in diverse software ecosystems. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation', 'Question Answering', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/multi-swe-bench/multi-swe-bench) |
| [Agentic Knowledgeable Self-awareness](https://arxiv.org/abs/2504.03553) | Xiangyuan Ru, Xiaobin Wang, Baochang Ren, Zhisong Qiu, Shuofei Qiao | - KnowSelf, a novel data-centric approach, equips Large Language Model (LLM)-based agents with agentic knowledgeable self-awareness, enabling them to dynamically regulate knowledge use during decision-making. - It uses a heuristic criterion to mark agent's self-explored trajectories for collecting training data according to three situation types: fast thinking, slow thinking, and knowledgeable thinking. - A two-stage training process is employed, involving supervised fine-tuning and an RPO loss, to instill self-awareness in the agent. - KnowSelf outperforms strong baselines on agent planning tasks with minimal external knowledge and reflection steps, demonstrating its effectiveness and efficiency. - Further analysis reveals KnowSelf effectively mitigates planning pattern overfitting, demonstrates better generalization in agent planning and performance scales with model and data size. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/zjunlp/KnowSelf) | N/A |
| [MegaMath: Pushing the Limits of Open Math Corpora](https://arxiv.org/abs/2504.02807) | Liping Tang, Zhoujun Cheng, Nikhil Ranjan, Zengzhi Wang, Fan Zhou | - MegaMath, a new open-source English math corpus totaling 371.6B tokens, pushes the boundaries of existing open math corpora in terms of scale and quality. - It incorporates diverse sources, including 279B tokens of web data, 28.1B of code, and 64.5B of synthetic data generated through optimized pipelines that ensure high-quality mathematical content and diversity.  - A premium subset, MegaMath-Web-Pro, utilizes LLM-based filtering and refining to provide an even higher-quality data source ideal for advanced training.  - Through ablation studies and benchmark evaluations, MegaMath demonstrates its effectiveness, significantly improving the mathematical reasoning capabilities of state-of-the-art large language models like the Llama-3 series by 15%-20% on tasks such as GSM8K and MATH.  - Its versatile data variants and open-source nature make MegaMath a valuable resource for research and development in the field of mathematical reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/LLM360/MegaMath) | [Link](https://hf.co/datasets/LLM360/MegaMath) |
| [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge
  Refinement](https://arxiv.org/abs/2504.03561) | Jialong Wu, Shuofei Qiao, Yuan Liang, Xiaobin Wang, Runnan Fang | - SynWorld, a novel framework, assists Large Language Model (LLM)-based agents in learning unfamiliar actions within new environments by synthesizing virtual scenarios involving multiple coordinated actions. - Through iterative Monte Carlo Tree Search (MCTS) optimization within these virtual scenarios, SynWorld refines the action descriptions and workflow patterns, ensuring better alignment with environmental constraints. - This framework addresses the limitations of existing methods that rely on single-action synthetic scenarios and linear iterative optimization processes which are prone to stagnation. - Experimental results on ToolBench and HotpotQA datasets demonstrate SynWorld’s effectiveness in refining action knowledge in virtual environments. - It also improves the agent's ability to generalize this knowledge to real-world scenarios, outperforming other iterative optimization methods. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zjunlp/SynWorld) | N/A |
| [MME-Unify: A Comprehensive Benchmark for Unified Multimodal
  Understanding and Generation Models](https://arxiv.org/abs/2504.03641) | Bingyan Nie, Yang Shi, Chaoyou Fu, Yi-Fan Zhang, Wulin Xie | - Introduces MME-Unify (MME-U), a benchmark designed to comprehensively evaluate unified multimodal large language models (U-MLLMs) on understanding, generation, and mixed-modality generation tasks. - Curates a diverse set of tasks from existing datasets, standardizing formats and metrics for consistent evaluation across 12 U-MLLMs including Janus-Pro, EMU3, and Gemini2-Flash. - Designs five novel subtasks specifically for assessing mixed-modality generation, evaluating how well models' understanding and generation capabilities enhance each other. - Reveals a significant performance gap between current U-MLLMs and specialized models in both understanding and generation, highlighting areas for improvement such as instruction following and image generation quality. - Finds a substantial variance in performance across models and a notable struggle with mixed-modality generation tasks, even among leading U-MLLMs. | ['Multimodal', 'Computer Vision', 'Image-to-Image', 'Image-to-Video', 'Text-to-Image', 'Text-to-Video'] | N/A | N/A |
| [VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via
  Iterative Instruction Tuning and Reinforcement Learning](https://arxiv.org/abs/2504.02949) | Liming Liang, Dongchao Yang, Yufan Deng, Yuxin Xie, Xianwei Zhuang | - VARGPT-v1.1 is a visual autoregressive large unified model that improves upon its predecessor, VARGPT, by incorporating iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), an expanded training corpus, and an upgraded language model backbone using Qwen2. - The model architecture consists of an LLM (Qwen2-7B-Instruct), a visual encoder, a visual decoder, and projectors for both understanding and generation, employing causal attention in the LLM and block causal attention in the decoder. - It achieves state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, outperforming comparable unified MLLMs and demonstrating significant improvements in both comprehension and generation metrics as shown in Figure 2. - The model supports text-and-image instructions and outputs text-and-image mixed modal data simultaneously. - It also exhibits flexible image-editing capabilities without architectural modifications. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/VARGPT-family/VARGPT-v1.1) | N/A |
| [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated
  Agent-Human Interplay](https://arxiv.org/abs/2504.03601) | Ming Zhu, Jianguo Zhang, Weiran Yao, Zuxin Liu, Akshara Prabhakar | - APIGen-MT is a two-phase framework for generating verifiable and diverse multi-turn agent data.  - The first phase involves an agentic pipeline that generates task blueprints with ground-truth actions using LLM reviewers and feedback loops.  - These blueprints are then transformed into interaction trajectories through simulated human-agent interplay in the second phase. - The xLAM-2-fc-r models, trained on this synthetic data, outperform models like GPT-40 and Claude 3.5 on T-bench and BFCL, especially in multi-turn settings, with smaller models often exceeding larger counterparts. - Both the synthetic data and trained xLAM-2-fc-r models are open-sourced. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://apigen-mt.github.io) | [Link](https://huggingface.co/Salesforce/xLAM-2) |
| [TransMamba: Flexibly Switching between Transformer and Mamba](https://arxiv.org/abs/2503.24067) | Shuaipeng Li, Xingwu Sun, Ruobing Xie, andyyang, Yixinglee | - TransMamba is a novel framework that merges Transformer and Mamba models for sequence modeling, enabling dynamic switching between attention and state-space mechanisms based on token length and layer. - It employs shared parameter matrices (QKV and CBx) for both Transformer and Mamba and introduces a Memory Converter to bridge the two mechanisms by transforming attention outputs into SSM-compatible states. - TransMamba incorporates a flexible TransPoint scheduling strategy to optimize the switching points between the two models, enhancing training efficiency. - Experimental results demonstrate that TransMamba achieves superior training efficiency and performance compared to standalone Transformer, Mamba2, and hybrid models. - The study validates the underlying consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling, particularly for long-sequence processing. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [EvMic: Event-based Non-contact sound recovery from effective
  spatial-temporal modeling](https://arxiv.org/abs/2504.02402) | Lu Zhang, Xudong XU, Xu Jia, Shi Guo, yyzqy | - EvMic, a novel learning-based pipeline for event-based non-contact sound recovery, leverages the high temporal resolution and large field of view of event cameras. - It uses a network with sparse convolutions for feature extraction, a multi-head self-attention spatial aggregation block, and a Mamba-based temporal modeling module to process event data and reconstruct sound. - A new synthetic dataset, EvMic, was created using Blender and an event simulator to train the model, along with a speckle-based data augmentation technique and real world data for improved generalization. - Experimental results on synthetic and real-world data demonstrated superior performance compared to existing event-based and frame-based methods, achieving higher SNR and STOI scores. -  A custom-designed imaging system with a laser matrix was used to amplify surface gradients and capture subtle vibrations in real-world scenarios. | ['Audio', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [BEATS: Bias Evaluation and Assessment Test Suite for Large Language
  Models](https://arxiv.org/abs/2503.24310) | Lisa Erickson, tbandopa, alokabhishek | - This research paper introduces BEATS (Bias Evaluation and Assessment Test Suite), a novel framework for evaluating bias, ethics, fairness, and factuality (BEFF) in Large Language Models (LLMs). - The framework utilizes a curated dataset of evaluation questions that measure performance across 29 distinct metrics spanning demographic, cognitive, and social biases, as well as ethical reasoning, group fairness, and factuality. - A consortium of LLMs acts as judges to evaluate model responses, enabling a statistically rigorous and scalable bias assessment methodology. - Empirical results reveal that 37.65% of outputs from industry-leading LLMs contain some form of bias, highlighting the risk of using these models in critical decision-making systems. - The BEATS framework provides a methodology for benchmarking LLMs, diagnosing sources of bias, and developing mitigation strategies to promote more responsible and ethical AI development. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/SocialGrep/one-million-reddit-questions) |


## Papers for 2025-04-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Advances and Challenges in Foundation Agents: From Brain-Inspired
  Intelligence to Evolutionary, Collaborative, and Safe Systems](https://arxiv.org/abs/2504.01990) | KaitaoSong, JinlinW, Peiyan, xinfeng1i, Bang-UdeM-Mila | - This survey explores the intersection of Large Language Models (LLMs) and intelligent agents and maps human brain functionalities with corresponding modules in agentic architectures. - The paper offers a modular framework for building advanced agents, incorporating key elements like memory, world modeling, emotions, goals, and reward systems. - It discusses self-enhancement mechanisms in AI agents that leverage adaptive learning and self-reflection for continuous improvement, along with collaboration and evolution in multi-agent systems. - The survey also covers critical safety and security concerns in LLM-based agents, including threats like jailbreaking, data poisoning, and misalignment, while suggesting potential defense mechanisms. - Lastly, it discusses the concept of “superalignment” and the “scaling law of AI safety,” focusing on enhancing the safety and reliability of agents as their capabilities expand. | ['Multimodal', 'Reinforcement Learning', 'Robotics'] | [Link](https://github.com/FoundationAgents/awesome-foundation-agents) | N/A |
| [GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image
  Generation](https://arxiv.org/abs/2504.02782) | shawnxyh, BestWishYsh, SereinH, liweijia, Yejy53 | - This research paper introduces GPT-ImgEval, a benchmark designed to evaluate the image generation capabilities of GPT-40 across generation quality, editing proficiency, and world knowledge. - GPT-40's performance is quantitatively and qualitatively assessed through the GenEval, Reason-Edit, and WISE datasets.  - Across all tasks, GPT-40 demonstrates strong performance, outperforming existing methods.  - The paper also investigates the potential architecture of GPT-40 using a classification-model-based approach, with results suggesting a diffusion-based image decoding mechanism. - An analysis of GPT-40's limitations, safety implications, and multi-round editing capabilities is also provided. | ['Text-to-Image', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/PicoTrex/GPT-ImgEval) | N/A |
| [Rethinking RL Scaling for Vision Language Models: A Transparent,
  From-Scratch Framework and Comprehensive Evaluation Scheme](https://arxiv.org/abs/2504.02587) | Pengfei, IanZhong, Ryan1122, steffichern, ManTle | - This paper introduces MAYE, a transparent, from-scratch framework for Reinforcement Learning (RL) applied to Vision Language Models (VLMs), focusing on improving reasoning capabilities. - The framework uses a four-step process—data flow, response collection, trajectory generation, and policy update—and is validated across multiple VLMs and datasets, including Qwen-VL and mm_math5k/geometry3k. - A standardized evaluation scheme is also proposed, emphasizing training dynamics and reflective behaviors to ensure robust and reproducible benchmarks. - Experimental results demonstrate that RL consistently outperforms Supervised Fine-Tuning (SFT) in generalization across multiple visual reasoning tasks, even with high-quality SFT data. - Analysis reveals a strong correlation between response length and reflective behavior, and RL effectively leverages this to enhance performance. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/GAIR-NLP/MAYE) | N/A |
| [Scaling Analysis of Interleaved Speech-Text Language Models](https://arxiv.org/abs/2504.02398) | adiyoss, MajoRoth, hassid, gallilmaimon | - This paper conducts a scaling analysis of interleaved speech-text language models (SLMs), demonstrating that they scale more efficiently with compute compared to textless SLMs. - The analysis involves training dozens of interleaved SLMs with varying sizes, compute budgets, and model families, leading to practical insights for optimizing SLM performance.  - The study finds that allocating more compute budget towards increasing model size than training tokens results in better performance. - It also investigates the role of synthetic data and TextLM model families, suggesting that scaled-up interleaved SLMs achieve comparable performance to leading models on speech semantic metrics with less compute and data. -  All trained models and code are open-sourced to encourage further community exploration. | ['Audio', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/slp-rl/slamkit) | [Link](https://huggingface.co/hexgrad/Kokoro-82M) |
| [ShortV: Efficient Multimodal Large Language Models by Freezing Visual
  Tokens in Ineffective Layers](https://arxiv.org/abs/2504.00502) | xphan, sanmusunrise, luyaojie, chenjiawei-icip, yuanqianhao | - ShortV is a training-free method for optimizing Multimodal Large Language Models (MLLMs) by identifying and freezing visual token computations in ineffective layers, thereby reducing computational costs. - It introduces a novel metric called Layer Contribution (LC) to quantify the impact of a layer's transformations on visual and text tokens, identifying layers with minimal contribution to the model's output. - ShortV freezes visual tokens by replacing standard layers with sparse ShortV layers where only text tokens are processed. - Experiments across various benchmarks (MME, MMBench, MMMU, MMStar, SEED-Bench, GQA, Flickr30K) demonstrate that ShortV can freeze visual tokens in approximately 60% of MLLM layers, achieving a 50% reduction in FLOPs on LLaVA-NeXT-13B while maintaining performance. - ShortV is orthogonal to and compatible with visual token pruning methods like FastV, allowing combined use for further efficiency gains. | ['Multimodal', 'Computer Vision', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/icip-cas/ShortV) | N/A |
| [ZClip: Adaptive Spike Mitigation for LLM Pre-Training](https://arxiv.org/abs/2504.02507) | gueraf, nilabhra, louisowen6, akanyaani | - This paper introduces ZClip, a novel adaptive gradient clipping algorithm designed to mitigate loss spikes during large language model (LLM) pre-training. - ZClip dynamically adjusts the clipping threshold based on the running mean and standard deviation of gradient norms, using a z-score based anomaly detection mechanism.  - Unlike traditional methods with fixed thresholds, ZClip adapts to evolving training dynamics and avoids over-clipping.  - Experimental results on a 1B parameter LLaMA model demonstrate that ZClip stabilizes training at high learning rates, leading to faster convergence without compromising performance.  - At lower learning rates, ZClip effectively handles minor fluctuations and improves downstream task performance on HellaSwag and Winogrande benchmarks. | ['Natural Language Processing'] | [Link](https://github.com/bluorion-com/ZClip) | N/A |
| [Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495) | Chong Ruan, Shirong Ma, Runxin Xu, Peiyi Wang, Zijun Liu | - This paper introduces Self-Principled Critique Tuning (SPCT), a novel learning method to improve the inference-time scalability of generalist reward modeling (RM) for large language models (LLMs). - SPCT enables pointwise generative reward models (GRMs) to generate principles and critiques adaptively through online reinforcement learning, resulting in higher quality reward signals and better performance-compute scaling. - DeepSeek-GRM, a 27B parameter model trained with SPCT, outperforms existing methods and models on various RM benchmarks. - Inference-time scaling with parallel sampling and a meta RM further improves the performance of DeepSeek-GRM, exceeding the performance gains from training-time scaling with larger model sizes. - The proposed method addresses the challenge of generating accurate and robust rewards for general queries in diverse domains, crucial for broader applications of LLMs. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [GenPRM: Scaling Test-Time Compute of Process Reward Models via
  Generative Reasoning](https://arxiv.org/abs/2504.00891) | Saputello, dmux, ChetKao, iseesaw, RyanLiu112 | - Introduced GenPRM, a generative process reward model that enhances the reasoning capabilities of Large Language Models (LLMs) by performing explicit Chain-of-Thought (CoT) reasoning with code verification for each step. - Proposed Relative Progress Estimation (RPE) and a novel rationale data synthesis framework involving code verification to generate high-quality supervision labels. - Demonstrated significant performance improvements over previous PRMs on ProcessBench and mathematical reasoning tasks, using only 23K training data from the MATH dataset. - Showed that smaller GenPRM models can outperform much larger PRMs (e.g., 1.5B GenPRM outperforms GPT-40, and 7B GenPRM surpasses Qwen2.5-Math-PRM-72B) through test-time scaling. - Presented GenPRM's potential as a critic model for refining policy model outputs and its effectiveness in test-time scaling for enhanced process supervision capabilities. | ['Natural Language Processing', 'Question Answering'] | [Link](https://ryanliu112.github.io/GenPRM) | N/A |
| [Sparse Autoencoders Learn Monosemantic Features in Vision-Language
  Models](https://arxiv.org/abs/2504.02821) | Zeynep Akata, Serge Belongie, Quentin Bouniot, Shyamgopal Karthik, Mateusz Pach | - This paper introduces a framework for enhancing the interpretability and control of Vision-Language Models (VLMs) using Sparse Autoencoders (SAEs). - The authors propose a Monosemanticity Score (MS) to quantify the similarity of images activating a given neuron, demonstrating that SAE training significantly improves neuron monosemanticity in VLMs like CLIP. - The Matryoshka SAE architecture is shown to further enhance monosemanticity and reveal concept hierarchies that align with expert-defined structures. - The study demonstrates the ability to steer Multimodal Large Language Models (MLLMs), such as LLaVA, by intervening on SAE activations in the vision encoder without modifying the underlying LLM. - This method enables controlled manipulation of MLLM outputs towards specific concepts discovered by the SAE, showcasing the potential of SAEs for enhancing both understanding and control of VLMs. | ['Multimodal', 'Image Feature Extraction', 'Computer Vision'] | [Link](https://github.com/ExplainableML/sae-for-vlm) | N/A |
| [Whisper-LM: Improving ASR Models with Language Models for Low-Resource
  Languages](https://arxiv.org/abs/2503.23542) | Ibon Saratxaga, Eva Navas, inmahernaez, zuazo | - This paper introduces Whisper-LM, a method for enhancing Automatic Speech Recognition (ASR) performance, especially in low-resource languages, by integrating traditional (n-gram) and novel (large language models - LLMs) language models with fine-tuned Whisper models. - Whisper-LM leverages a structured fine-tuning process for Whisper models across various sizes (Tiny to Large-V3) using the Common Voice dataset and integrates language models at inference time. - Results demonstrate improvements up to 51% for in-distribution datasets and 34% for out-of-distribution sentences using statistical Language Models, while LLMs yield more moderate but robust gains and enhance model robustness. - A detailed analysis of sentence-level overlap confirms minimal leakage in evaluation datasets, ensuring the observed improvements are not due to memorization. - An ablation study reveals the substantial impact of evaluation parameters, particularly language specification and beam search, on performance, emphasizing the need for careful consideration of settings. | ['Automatic Speech Recognition', 'Natural Language Processing'] | [Link](http://www.github.com/hitz-zentroa/whisper-lm) | [Link](https://huggingface.co/openai) |


## Papers for 2025-04-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Improved Visual-Spatial Reasoning via R1-Zero-Like Training](https://arxiv.org/abs/2504.00883) | Zijian Kong, Yanhao Zhang, Qingsong Xie, Zhenyi Liao, zhijie3 | - This paper introduces a new method for improving the visual-spatial reasoning of Multimodal Large Language Models (MLLMs) using R1-Zero-like training. - The authors found that Chain of Thought (CoT) prompting is ineffective for activating visual-spatial reasoning in small- to medium-sized Qwen2-VL models. - They created a 100k sample video-based question answering dataset called VSI-100k based on ScanNet and used it for Group Relative Policy Optimization (GRPO) training. - Their vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, outperforms the base model by 12.1% on VSI-bench and surpasses GPT-40.  - The vsGRPO-7B model achieves performance comparable to LLaVA-NeXT-Video-72B. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/zhijie-group/R1-Zero-VSI) | N/A |
| [AnimeGamer: Infinite Anime Life Simulation with Next Game State
  Prediction](https://arxiv.org/abs/2504.01014) | Ying Shan, Jing Liao, Yixiao Ge, Yuying Ge, Howe666 | - AnimeGamer is a novel framework for infinite anime life simulation, leveraging Multimodal Large Language Models (MLLMs) to generate dynamic animation shots and update character states based on user instructions. - It introduces action-aware multimodal representations for animation shots, decoded into video clips using a video diffusion model, ensuring contextual consistency and dynamic movements. - AnimeGamer uses historical multimodal representations and character states to predict subsequent game states, resulting in a coherent and immersive gaming experience. - Evaluations using automated metrics and human assessments show that AnimeGamer outperforms existing methods in instruction following, contextual consistency, and overall gaming experience. -  A data collection pipeline from anime films is also proposed, enabling customized model training for diverse character experiences. | ['Text-to-Video', 'Multimodal'] | [Link](https://github.com/TencentARC/AnimeGamer) | N/A |
| [Understanding R1-Zero-Like Training: A Critical Perspective](https://arxiv.org/abs/2503.20783) | Tianyu Pang, Wenjun Li, QPHutu, Cameron-Chen, lkevinzc | - This paper analyzes R1-Zero-like training, focusing on base models and reinforcement learning (RL) components, and introduces Dr. GRPO, an unbiased optimization method to improve token efficiency. - The authors investigate various base models, including DeepSeek-V3-Base and Qwen2.5, to understand how pretraining influences RL performance and reveal potential biases such as Qwen2.5's potential pretraining on question-answer pairs. - The paper identifies an optimization bias in Group Relative Policy Optimization (GRPO) that artificially inflates response length, especially in incorrect outputs, and proposes Dr. GRPO as a solution to improve token efficiency without sacrificing reasoning performance. - A minimalist R1-Zero recipe, employing Dr. GRPO, Qwen2.5-Math-7B, and a math dataset, achieves state-of-the-art 43.3% accuracy on AIME 2024 with a 7B model using limited compute. - The study's findings on base models and RL aim to enhance future research in LLM post-training and online alignment through code and model releases. | ['Reinforcement Learning', 'Question Answering', 'Natural Language Processing'] | [Link](https://github.com/sail-sg/understand-r1-zero), [Link](https://github.com/sail-sg/oat) | [Link](https://huggingface.co/HuggingFaceTB/FineMath-Llama-3B), [Link](https://huggingface.co/AI-MO/NuminaMath-1.5) |
| [ScholarCopilot: Training Large Language Models for Academic Writing with
  Accurate Citations](https://arxiv.org/abs/2504.00824) | Zhiheng Lyu, Huaye Zeng, Ping Nie, Xueguang Ma, Yubo Wang | - ScholarCopilot, a new framework designed to enhance existing large language models (LLMs) for generating professional academic articles with accurate and contextually relevant citations, is introduced. - It dynamically determines when to retrieve scholarly references by generating a retrieval token ([RET]), which is then used to look up relevant citations from a database.  - Trained on 500K papers from arXiv, ScholarCopilot achieves a top-1 retrieval accuracy of 40.1% on the evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). - In terms of generation quality, using LLM-as-judge, ScholarCopilot scores 16.2/25 on a 1000 samples dataset across five dimensions, outperforming larger models such as Qwen-2.5-72B-Instruct (15.8).  - Human studies confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience. | ['Text Generation'] | N/A | N/A |
| [ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and
  Diffusion Refinement](https://arxiv.org/abs/2504.01934) | Yunlong Yuan, Guansong Lu, Junwei Yang, Chunwei Wang, Runhui Huang | - ILLUME+ is a unified multimodal large language model (MLLM) that integrates understanding, generation, and editing using dual visual tokenization and a diffusion decoder. - It employs DualViTok, a dual-branch vision tokenizer, to preserve both semantic and fine-grained texture information for robust image representation. - A diffusion model is used as the image detokenizer, improving generation quality and enabling efficient super-resolution. - ILLUME+ utilizes a continuous-input, discrete-output scheme within the MLLM and adopts a progressive training procedure supporting dynamic resolution across components. - The model achieves competitive performance on various multimodal benchmarks, showing strong results in understanding, high-resolution generation (up to 1024x1024), and improved texture preservation in editing compared to its predecessor, ILLUME. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image', 'Visual Question Answering'] | N/A | N/A |
| [Safeguarding Vision-Language Models: Mitigating Vulnerabilities to
  Gaussian Noise in Perturbation-based Attacks](https://arxiv.org/abs/2504.01308) | Zhendong Liu, Yushen Zuo, sofyc, AllenChai, Jarvis1111 | - This paper introduces Robust-VLGuard, a multimodal safety dataset with aligned and misaligned image-text pairs, combined with noise-augmented fine-tuning to mitigate vulnerabilities of Vision-Language Models (VLMs) to Gaussian noise in perturbation-based attacks. - It also proposes DiffPure-VLM, a defense pipeline integrating diffusion models with Gaussian-noise-tolerant VLMs, leveraging the distribution-shifting property of diffusion models to transform adversarial perturbations into Gaussian-like noise. - Experiments demonstrate that DiffPure-VLM effectively mitigates adversarial perturbations across varying intensities, significantly improving VLM robustness against Gaussian noise and other optimization-based attacks. - The authors systematically evaluate the robustness of three mainstream VLMs (InternVL2-8B, LLaVA-v1.5-7B, and MiniGPT-4-13B) against Gaussian noise and find significant performance degradation in both helpfulness and safety. - The Robust-VLGuard dataset addresses limitations of existing datasets like VLGuard by including image-text misalignment scenarios and detailed responses generated by GPT-4V, which improved helpfulness and safety of the tested VLMs. | ['Multimodal'] | [Link](https://github.com/JarvisUSTC/DiffPure-RobustVLM) | N/A |
| [DASH: Detection and Assessment of Systematic Hallucinations of VLMs](https://arxiv.org/abs/2503.23573) | Matthias Hein, Maximilian Augustin, YanNeu | - This research introduces DASH (Detection and Assessment of Systematic Hallucinations), an automated, large-scale pipeline for identifying systematic object hallucinations in Vision-Language Models (VLMs). - DASH employs two methods: DASH-LLM, using text-based retrieval with LLM-generated queries, and DASH-OPT, which optimizes a latent diffusion model to generate images that trigger VLM hallucinations while having low object detector confidence. - Applied to PaliGemma and LLaVA-NeXT models on 380 object classes, DASH identified over 19,000 hallucination clusters encompassing 950,000 images. - These hallucinations transferred to other VLMs, including top-performing models like QwenV2-72B.  - Fine-tuning PaliGemma with DASH-generated images demonstrated a reduction in object hallucinations. | ['Multimodal', 'Computer Vision', 'Object Detection'] | [Link](https://YanNeu.github.io/DASH) | N/A |
| [VerifiAgent: a Unified Verification Agent in Language Model Reasoning](https://arxiv.org/abs/2504.00406) | Ehsan Shareghi, Wray Buntine, Jiuzhou Han | - VerifiAgent, a novel verification framework, enhances the reliability of Large Language Models (LLMs) by verifying and improving their outputs across diverse reasoning tasks, including mathematical, logical, commonsense, and hybrid reasoning. - This two-layer verification agent employs both meta-verification, assessing completeness and consistency, and tool-based adaptive verification, selecting appropriate external tools like Python interpreters or symbolic solvers for enhanced accuracy. - Experimental results demonstrate VerifiAgent's superior performance compared to existing baselines such as deductive and backward verifiers, achieving higher accuracy while maintaining competitive precision and recall. - Additionally, VerifiAgent effectively integrates with inference scaling methods, improving accuracy with fewer samples and lower cost than traditional Process Reward Models (PRMs). - This framework benefits from the capabilities of its backbone LLM, scaling its verification effectiveness alongside improvements in the underlying language model. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/Jiuzhouh/VerifiAgent) | N/A |
| [Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal
  Representations](https://arxiv.org/abs/2503.18817) | Sangheum Hwang, mawjdgus | - This paper introduces Cross-Modal Alignment (CMA), a novel multi-modal fine-tuning method for Vision-Language Models (VLMs) designed to enhance Out-of-Distribution (OoD) detection. - CMA addresses the modality gap in embedding space by aligning image and text embeddings of in-distribution data, improving the utilization of pretrained textual knowledge, especially negative concept labels, for OoD detection. - The method achieves state-of-the-art performance on OoD detection benchmarks such as MOS and OpenOOD v1.5, surpassing existing zero-shot, prompt learning, and other multi-modal fine-tuning approaches. - On the MOS benchmark, CMA achieves a 19.93% FPR95 and a 95.13% AUROC, significantly outperforming other methods. - CMA also demonstrates strong performance in In-Distribution (ID) classification tasks, indicating its efficacy in enhancing both OoD detection and ID classification accuracy. | ['Computer Vision', 'Zero-Shot Classification', 'Multimodal'] | [Link](https://github.com/ma-kjh/CMA-OoDD) | N/A |


## Papers for 2025-04-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Exploring the Effect of Reinforcement Learning on Video Understanding:
  Insights from SEED-Bench-R1](https://arxiv.org/abs/2503.24376) | yshan2u, yxgeee, ruiwang, tttoaster, ChenYi99 | - This paper introduces SEED-Bench-R1, a new benchmark for evaluating post-training methods for Multimodal Large Language Models (MLLMs) on video understanding tasks requiring both perception and reasoning. - The benchmark features egocentric videos, multiple-choice questions, and a hierarchical three-level validation set for assessing generalization across in-distribution, cross-environment, and cross-environment-task scenarios. - Using Qwen2-VL-Instruct-7B as a base model, the authors compare Reinforcement Learning (RL) with Supervised Fine-tuning (SFT) and find that RL, specifically GRPO, significantly improves performance on both in-distribution and out-of-distribution tasks, outperforming SFT even with simple outcome-based rewards. - Analysis reveals that RL enhances the model's visual attention but may produce less logically coherent reasoning chains. - The paper suggests future directions such as strengthening the base model's reasoning ability pre-RL and improving reward modeling and RL robustness. | ['Reinforcement Learning', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/TencentARC/SEED-Bench-R1) | N/A |
| [CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive
  Program Synthesis](https://arxiv.org/abs/2503.23145) | Naveen Kannan, Jiannan Cao, kaiyan289, tarsur909, anjiangwei | - CodeARC, a new framework for evaluating Large Language Models (LLMs) on inductive program synthesis, is introduced, focusing on interactive input generation and self-correction through queries to a ground truth function and feedback from a differential testing oracle. - A comprehensive dataset of 1114 diverse, general-purpose Python functions with corresponding input-output examples is created to facilitate the evaluation, targeting general programming tasks and employing differential testing tools for accurate assessment. - The experiments demonstrate that CodeARC presents a significant challenge for LLM-based program synthesis, with even the best-performing model (OpenAI 03-mini) achieving only a 52.7% success rate among 18 evaluated models. - Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces that capture both function invocations and reasoning steps yields up to a 31% relative improvement in performance. - The interactive evaluation protocol in CodeARC, where agents can actively generate inputs and revise their solutions based on feedback, offers a more realistic and robust alternative to prior static evaluation methods. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [JudgeLRM: Large Reasoning Models as a Judge](https://arxiv.org/abs/2504.00050) | Jiaying Wu, Nuo Chen, bhooi, qingyunzou, zhiyuanhucs | - This paper introduces JudgeLRM, a family of large language models (LLMs) trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards to improve performance on complex reasoning tasks for evaluation. - JudgeLRM models leverage a reward function that combines structural and content-based components, ensuring well-formatted reasoning and aligning model judgments with ground-truth preferences. - Empirical results demonstrate that JudgeLRM-3B surpasses GPT-4 on judgment tasks, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, showing particular strength in tasks requiring deep reasoning. - Analysis reveals a negative correlation between standard supervised fine-tuning (SFT) performance gains and the proportion of reasoning-demanding samples, highlighting the limitations of SFT in such scenarios and the advantage of using RL. - Further investigation reveals that effective judgment involves complex reasoning patterns like verification, sub-goal setting, and justification, emphasizing the importance of structured reasoning in evaluation tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/NuoJohnChen/JudgeLRM) | [Link](https://huggingface.co/nuojohnchen/JudgeLRM-7B), [Link](https://huggingface.co/spaces/nuojohnchen/JudgeLRMDemo) |
| [Agent S2: A Compositional Generalist-Specialist Framework for Computer
  Use Agents](https://arxiv.org/abs/2504.00906) | Vincent Tu, Kyle Wong, xw-eric, jc-y42, saa1605 | - Introduces Agent S2, a novel compositional framework that combines generalist and specialist models to enhance computer use agent performance by delegating cognitive tasks like planning, execution, and grounding. - Proposes a Mixture-of-Grounding technique to improve GUI element localization by routing actions to specialized grounding experts based on the nature of the subgoal. - Introduces Proactive Hierarchical Planning, which dynamically refines action plans at multiple temporal scales in response to evolving observations. - Achieves state-of-the-art performance on the OSWorld (27.0% and 34.5% on 15- and 50-step evaluations), WindowsAgentArena (29.8%), and AndroidWorld (54.3%) benchmarks. - Demonstrates that composing generalist and specialist models can surpass the best monolithic models even if individual models are suboptimal. | ['Multimodal'] | [Link](https://github.com/simular-ai/Agent-S) | N/A |
| [Z1: Efficient Test-time Scaling with Code](https://arxiv.org/abs/2504.00810) | Xiao-Ping Zhang, armanc, yilunzhao, yh1567, zjy2001 | - This paper introduces Z1, an efficient test-time scaling method for Large Language Models (LLMs) focusing on reducing excess thinking tokens while maintaining performance on complex problem-solving. - Z1 employs a novel Shifted Thinking Window, eliminating delimiting tags and capping reasoning tokens, to adjust its reasoning level based on problem complexity and prevent overthinking. - Trained on a new dataset, Z1-Code-Reasoning-107K, which consists of coding problems paired with short and long solution trajectories, Z1-7B achieves comparable performance to larger models while requiring significantly fewer thinking tokens, matching R1-Distill-Qwen-7B with approximately 30% of its average thinking tokens. - The model demonstrates efficient test-time scaling across various reasoning tasks including LiveCodeBench, MATH500, and GPQA Diamond, showing generalization ability to domains beyond code. - Further data ablation studies reveal the importance of mean trajectory length and training dataset size in code-related reasoning datasets for efficient test-time scaling. | ['Natural Language Processing', 'Text Generation', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/efficientscaling/Z1) | N/A |
| [MixerMDM: Learnable Composition of Human Motion Diffusion Models](https://arxiv.org/abs/2504.01019) | José García-Rodríguez, Sergio Escalera, Cristina Palmero, Germs96, pabloruizponce | - MixerMDM, a novel learnable model composition technique, dynamically combines pre-trained text-conditioned human motion diffusion models, enhancing control over individual and interaction dynamics. - Unlike prior methods with fixed mixing strategies, MixerMDM employs an adversarial training approach with a lightweight Mixer module, enabling it to learn dynamic mixing weights based on motion characteristics, conditions, and denoising timesteps. - This dynamic mixing strategy allows MixerMDM to generate diverse and nuanced interactions, merging the strengths of single-person and interaction-focused models, resulting in improved individual control within interactions and overall interaction quality. - A new evaluation procedure assesses both interaction and individual motion quality by measuring alignment with input conditions and the model's adaptability in adjusting mixing weights across the denoising process. - Experimental results demonstrate that MixerMDM consistently generates higher-quality mixed motions compared to previous techniques, outperforming existing methods and offering greater control over individual dynamics within interactions. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal
  LLMs on Academic Resources](https://arxiv.org/abs/2504.00595) | Heng Wang, Yu Tian, windwest, yanglj55, weizhiwang | - Open-Qwen2VL is a 2-billion parameter open-source multimodal large language model (MLLM) pre-trained on 29 million image-text pairs using only 442 A100-40G GPU hours. - It utilizes a low-to-high dynamic image resolution strategy and multimodal sequence packing, improving training efficiency. - The model employs both MLLM and CLIP based filtering methods to ensure high-quality data within the training set. - Open-Qwen2VL outperforms the closed-source 2B parameter Qwen2-VL-2B on several multimodal benchmarks including MMBench, SEEDBench, MMStar, and MathVista, demonstrating its effectiveness and training efficiency. - All aspects of the training process and the model are open-sourced including training data, filtering methods, pre-training code, and model weights. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Victorwz/Open-Qwen2VL) | [Link](https://huggingface.co/weizhiwang/Open-Qwen2vl), [Link](https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data) |
| [OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming
  Video Contexts](https://arxiv.org/abs/2503.22952) | Tong Wu, Bo Chen, Yueqian Wang, zlzheng, ColorfulAI | - This paper introduces OmniMMI, a benchmark designed to evaluate the interactive capabilities of Omni Large Language Models (OmniLLMs) in streaming video contexts. - The benchmark includes over 1,121 videos and 2,290 questions and covers six subtasks related to streaming video understanding and proactive reasoning. - A novel framework called Multi-modal Multiplexing Modeling (M4) is also proposed, using inspiration from duplexing modeling of auditory models. - M4 is designed to enable inference-efficient streaming by allowing models to see and listen while generating responses and includes proactive generation and interruption. - Experimental results show that existing MLLMs struggle with interactive streaming video understanding, particularly with proactive tasks and multi-turn queries. | ['Multimodal', 'Video-Text-to-Text'] | [Link](https://omnimmi.github.io) | N/A |
| [Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on
  Elementary School-Level Reasoning Problems?](https://arxiv.org/abs/2504.00509) | Xuesong Yao, xmerge123, ALEXoDu, yfxu, kaiyan289 | - This paper introduces RoR-Bench, a novel multimodal Chinese benchmark designed to evaluate the reasoning capabilities of Large Language Models (LLMs) on elementary school-level problems. - The benchmark consists of pairs of problems, with subtle but crucial condition shifts, to test if LLMs truly reason or simply recite solutions. - Experimental results on 23 LLMs reveal a significant performance drop (often >60%) on modified problems compared to original ones, indicating a strong tendency towards recitation rather than genuine reasoning. - Initial solutions like adding prompts and few-shot learning show limited improvement. - The authors propose further research into robust reasoning mechanisms less reliant on user clarifications and less sensitive to minor input changes. | ['Natural Language Processing', 'Question Answering', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/kaiyan289/RoR-Bench/tree/main) |
| [AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models
  with Unsupervised Coefficient Optimization](https://arxiv.org/abs/2503.23733) | Yiru Wang, Jiabo Ye, Xiaochen Wang, Yiyang Du, carboncoo | - AdaMMS, a novel model merging method designed for heterogeneous Multimodal Large Language Models (MLLMs), is introduced to address the challenges of merging models with different architectures and parameter space asymmetry. - The method employs a three-step process: mapping parameters between models with different architectures, merging the mapped parameters using adaptive linear interpolation, and searching for the optimal interpolation coefficient using an unsupervised hyperparameter selection method based on response consistency. - AdaMMS is the first model merging method capable of merging heterogeneous MLLMs without labeled data. - Experimental results on various model combinations and vision-language benchmarks demonstrate that AdaMMS outperforms existing model merging methods. - The unsupervised hyperparameter selection method effectively selects near-optimal interpolation coefficient and requires minimal data. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/THUNLP-MT/AdaMMS) | N/A |
| [When To Solve, When To Verify: Compute-Optimal Problem Solving and
  Generative Verification for LLM Reasoning](https://arxiv.org/abs/2504.01005) | anna-rohrbach, kaiweichang, adityagrover, arianhosseini, hbXNov | - This paper investigates compute-optimal strategies for Large Language Model (LLM) reasoning, comparing Self-Consistency (SC) with Generative Reward Models (GenRM). - Under a fixed inference budget, SC is found to be more compute-efficient than GenRM for most practical budgets, with GenRM requiring up to 8x more compute to match SC's performance and significantly more to outperform it. - The study derives inference scaling laws for GenRM, showing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. - Experiments across various model families, sizes, thinking models, and reasoning tasks confirm these findings. - These results provide practical guidelines for optimizing test-time scaling by balancing solution generation and verification under constrained budgets. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/nishadsinghi/sc-genrm-scaling) | N/A |
| [Multi-Token Attention](https://arxiv.org/abs/2504.00927) | sainbar, spermwhale, Tianlu, Golovneva | - This paper introduces Multi-Token Attention (MTA), a novel attention mechanism for LLMs that overcomes the limitations of standard "single token attention" by allowing attention weights to be conditioned on multiple query and key vectors simultaneously. - MTA achieves this by applying convolution operations over queries, keys, and attention heads, enabling nearby tokens and different heads to influence each other's attention for more precise context localization. - The authors demonstrate that MTA enhances performance on various benchmarks, including standard language modeling and long-context tasks such as Needle-in-the-Haystack and BabiLong, where its ability to leverage richer information is particularly advantageous. - MTA outperforms Transformer baselines on language modeling tasks, showing improvements in validation perplexity and benchmark results, while only minimally increasing parameter count. - The paper provides a motivating toy task that highlights the shortcomings of standard attention and showcases MTA's ability to effectively combine information from multiple tokens to identify target blocks. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/cerebras/SlimPajama-627B) |
| [Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features](https://arxiv.org/abs/2504.00557) | Jaeyeon Kim, Donguk Lim, Seungmin Yang, Ki-Ung Song, Jewon Lee | - This paper introduces Trimmed Llama, a training-free inference optimization method for cross-attention based Large Vision Language Models (LVLMs). - The method leverages a novel visual token pruning technique by exploiting the sparsity and inter-layer resemblance of cross-attention patterns, reducing key-value cache demands and computational overhead. - By trimming redundant visual features, often achieving 50% reduction, Trimmed Llama reduces inference latency and memory usage while maintaining benchmark parity with the original model. - Experimental results on datasets like MMVP, MME, SEED-Bench and LLaVA-Bench demonstrates the consistent efficacy of the proposed method. - It surpasses alternative pruning strategies such as random and spatial sampling, particularly in complex task-driven and open-ended generative tasks. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text'] | N/A | [Link](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) |
| [Discovering Knowledge Deficiencies of Language Models on Massive
  Knowledge Base](https://arxiv.org/abs/2503.23361) | Ryotaro Shimizu, Jieyu Zhang, Xuwei Ding, MaksimSTW, linxinso | - This paper introduces Stochastic Error Ascent (SEA), a framework designed to identify knowledge gaps in closed-weight Large Language Models (LLMs). - SEA uses a query budget to efficiently probe a massive knowledge base, iteratively selecting subsets likely to reveal errors by considering semantic similarity to previous mistakes. - This approach incorporates hierarchical retrieval at document and paragraph levels, constructing a relation-directed acyclic graph to track error dependencies and prune less informative nodes. - Experimental results show that SEA significantly outperforms baseline methods like Automated Capability Discovery (ACD) and AutoBencher in error discovery while also reducing the cost-per-error. - Human evaluations confirm the high quality of questions from SEA, and ablation studies validate each component's contribution. | ['Question Answering'] | [Link](https://github.com/uscnlp-lime/SEA) | N/A |
| [m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning
  with Large Language Models](https://arxiv.org/abs/2504.00869) | Yuyin Zhou, Xianfeng Tang, Hui Liu, Juncheng Wu, Xiaoke Huang | - This paper introduces m1, a novel methodology employing test-time scaling to enhance medical reasoning capabilities in Large Language Models (LLMs). - By increasing the "thinking" token budget during inference, m1 allows models to generate more extensive reasoning chains, leading to improved accuracy on various medical question-answering benchmarks. - Notably, the m1-7B-23K model achieves state-of-the-art accuracy of 60.32% on in-domain and out-of-domain medical exam datasets, outperforming larger, more computationally intensive models like HuatuoGPT-01-7B/8B and UltraMedical-8B. - Additionally, m1-32B-1K demonstrates comparable performance to 70B scale medical LLMs, highlighting the efficiency of this approach. - The study's findings underscore that enriched medical knowledge, rather than increased reasoning depth alone, is essential for fully realizing the benefits of test-time scaling in LLMs. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/UCSC-VLAA/m1) | N/A |
| [Towards Trustworthy GUI Agents: A Survey](https://arxiv.org/abs/2503.23434) | Ninghao Liu, Wenhu Chen, Wenlin Yao, Wenhao Yu, Yucheng Shi | - This survey paper examines the trustworthiness of GUI agents, which are powered by large language and multimodal models and interact with digital interfaces like websites and mobile apps. - The paper categorizes trustworthiness into five key areas: security, reliability, explainability, ethical alignment, and evaluation methodologies and discusses existing research and challenges within each area, such as vulnerabilities to adversarial attacks and the need for robust evaluation benchmarks. - The authors highlight the shift in focus from task success to holistic trustworthiness and identify open problems like balancing real-time performance with safety and mitigating hallucination errors. -  The survey also discusses the broader social implications of using GUI agents, emphasizing the importance of responsible development practices to address security, ethical, and privacy risks. - The paper concludes by suggesting future research directions, including developing smarter defense tools and promoting user-controlled privacy mechanisms for more secure and reliable GUI agent deployment. | ['Multimodal'] | N/A | N/A |
| [MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote
  Sensing](https://arxiv.org/abs/2503.24219) | Mustapha lebbah, Hanane Azzag, rdkarim | - This paper introduces MB-ORES, a two-stage framework that unifies object detection and visual grounding for remote sensing imagery. - In the first stage, MB-ORES fine-tunes an open-set object detector with referring expression data, which serves as a partially supervised object detection task. - Then, MB-ORES constructs a graph representation of each image, which is then processed by a multi-branch network that integrates visual, spatial, and categorical features to generate task-aware object proposals. - Lastly, an object reasoning network assigns probabilities to these proposals, followed by a soft selection mechanism and regression head for precise referred object localization. - MB-ORES achieves state-of-the-art performance on the OPT-RSVG and DIOR-RSVG datasets, demonstrating improvements over existing methods while retaining conventional object detection capabilities. | ['Computer Vision', 'Object Detection', 'Multimodal'] | [Link](https://github.com/rd20karim/MB-ORES) | N/A |


## Papers for 2025-04-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [TextCrafter: Accurately Rendering Multiple Texts in Complex Visual
  Scenes](https://arxiv.org/abs/2503.23461) | Nikai Du, yingtai, jzzzzk, Chenzzzzzz, zhen-nan | - TextCrafter is a novel training-free framework for rendering multiple texts in complex visual scenes, addressing challenges like text distortion, omission, and blurriness common in current text-to-image models. - It employs a progressive strategy with three stages: Instance Fusion strengthens the link between visual text and its carrier; Region Insulation separates and denoises text prompts in different regions, leveraging positional priors of a pre-trained DiT model; and Text Focus enhances attention maps of visual text, refining fidelity. - A new benchmark dataset, CVTG-2K, is introduced with 2,000 prompts containing complex visual texts, varying in position, quantity, length, and attributes, to rigorously evaluate models on CVTG tasks. - Quantitative experiments on CVTG-2K show TextCrafter significantly outperforms state-of-the-art models in OCR accuracy (Word Accuracy and NED) and prompt adherence (CLIPScore), improving OCR accuracy by over 45% compared to the baseline FLUX model. - Qualitative results further demonstrate TextCrafter's ability to generate harmonious images with accurate and clear multiple visual texts, even in complex scenarios, while other models struggle with omissions, confusion, and background information loss. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [MoCha: Towards Movie-Grade Talking Character Synthesis](https://arxiv.org/abs/2503.23307) | Luczzz, daixl1992, FelixXu, haoyum1997, lim142857 | - MoCha is a novel diffusion transformer (DiT) model for generating talking character animations from speech and text input, focusing on full-body motions and expressions beyond just the face. - It introduces a speech-video window attention mechanism for aligning video and speech, a joint training strategy that leverages both speech and text data to improve data efficiency, - and supports multi-character conversations by using character tags in prompts. - Human evaluations and automatic metrics on MoCha-Bench, a new benchmark tailored for this task, demonstrate MoCha's superior performance over existing methods on lip-sync quality, facial expressions, action naturalness, text alignment, and visual quality. - MoCha represents a significant advancement in AI-driven cinematic storytelling, setting a new standard for talking character video generation. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [What, How, Where, and How Well? A Survey on Test-Time Scaling in Large
  Language Models](https://arxiv.org/abs/2503.24235) | nancy-zwx, demolei, RubinSun, silentspring2, DonJoey | - This paper surveys Test-Time Scaling (TTS), a technique to improve Large Language Model (LLM) performance by allocating additional computation during inference. - It introduces a four-dimensional framework for analyzing TTS methods: what to scale (e.g., CoT length, samples), how to scale (e.g., SFT, RL, search), where to scale (tasks and datasets), and how well to scale (evaluation metrics). - It systematically reviews existing TTS methods, mapping them to the framework's dimensions and providing hands-on guidelines for practical implementation. - The paper identifies key trends, open challenges, and promising research directions in TTS, including improving scalability, clarifying the essence of techniques, and broadening generalization across domains. - It highlights the shift towards AI systems that dynamically scale their intelligence at inference, adapting to complex and evolving tasks. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [RIG: Synergizing Reasoning and Imagination in End-to-End Generalist
  Policy](https://arxiv.org/abs/2503.24388) | Haian Huang, Zhonghan Zhao, GaoangWang, pppppM, ZwwWayne | - This paper introduces RIG, an end-to-end generalist Transformer-based policy that synergizes reasoning and imagination for embodied agents in open-world environments. - RIG integrates textual reasoning, low-level action control, and image generation within a unified sequence-to-sequence model, enabling it to reason about actions, predict their consequences, and review imagined outcomes before execution. - Trained with a progressive data collection strategy, RIG achieves state-of-the-art results on embodied tasks, image generation, and reasoning benchmarks, showing significant improvements over existing methods. - RIG demonstrates higher sample efficiency, requiring only 111 hours of training data compared to thousands of hours used by other methods, while also achieving 3.29x, 2.42x, and 1.33× improvements on embodied tasks, image generation, and reasoning benchmarks respectively. - The model also supports test-time scaling, allowing for dynamic lookahead reasoning to improve action robustness and reduce trial-and-error during inference. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370) | Prateek Mittal, Jiachen T. Wang, cxiang, tongwu2020 | - This paper introduces "Thinking Intervention," a novel method for controlling reasoning-enhanced large language models (LLMs) by strategically inserting or modifying specific thinking tokens within the LLM's reasoning process. - This method requires no model training and can be integrated with existing techniques like prompt engineering.  - The authors demonstrate that Thinking Intervention improves performance across tasks including instruction following, handling instruction hierarchies, and safety alignment.  - Evaluating on IFEval, SEP, XSTEST, and SORRY-BENCH datasets using DeepSeek R1 and QwQ models, Thinking Intervention shows significant improvements over baseline prompting methods.  - Results demonstrate gains up to 6.7% in instruction following accuracy, 15.4% for hierarchy tasks and 40% higher refusal rates for unsafe prompts, enhancing LLM control without extra training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Query and Conquer: Execution-Guided SQL Generation](https://arxiv.org/abs/2503.24364) | sfc-mwydmuch, Borchmann | - This paper introduces a novel approach called "execution-guided SQL generation" for enhancing the accuracy of text-to-SQL tasks. - The method leverages execution results to select the most semantically consistent query from multiple generated candidates, enabling smaller, cost-effective models to outperform computationally intensive reasoning methods like OpenAI's o1, o3-mini, and DeepSeek R1. - By comparing query outputs based on exact and approximate execution similarity, the proposed approach overcomes limitations of traditional self-consistency methods that rely on structural comparisons, which are ineffective when queries are structurally different yet semantically equivalent. - Empirical results on the BIRD-SQL dataset demonstrate significant accuracy improvements across various model sizes, notably matching the performance of larger models with a 30-fold reduction in inference cost. - Further enhancements include leveraging partial executability in SQL dialects like PipeSQL to incrementally apply self-consistency during intermediate generation stages, leading to more robust refinement of complex queries. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud
  Detection](https://arxiv.org/abs/2503.24115) | Kai Wu, Jingpeng Wang, HuangMinhua, WDong, JimmyMa99 | - This paper introduces TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset for automated telecom fraud analysis, integrating audio signals with reasoning-oriented textual analysis. - The dataset was constructed using three strategies: privacy-preserved text-truth sample generation with ASR and TTS, semantic enhancement via LLM-based self-instruction sampling, and multi-agent adversarial synthesis to simulate diverse fraud tactics. - TeleAntiFraud-28k contains 28,511 speech-text pairs with detailed annotations for fraud reasoning, divided into tasks for scenario classification, fraud detection, and fraud type classification. - A standardized evaluation benchmark, TeleAntiFraud-Bench, and a production-optimized SFT model trained on hybrid real/synthetic data are also provided. - Evaluations show that fine-tuning a large audio language model (LALM) like Qwen2Audio with this dataset significantly improves performance on telecom fraud detection tasks, outperforming other models and highlighting the importance of data synthesis, modality fusion, and slow-thinking training. | ['Audio', 'Automatic Speech Recognition', 'Natural Language Processing'] | [Link](https://github.com/JimmyMa99/TeleAntiFraud) | N/A |
| [Efficient Inference for Large Reasoning Models: A Survey](https://arxiv.org/abs/2503.23077) | jiaheng233, Bibaolong, HongyuChen, HongchengGao, yueliu1999 | - This paper surveys efficient inference methods for Large Reasoning Models (LRMs), focusing on mitigating token inefficiency while preserving reasoning quality. - The paper introduces a taxonomy categorizing methods into explicit compact Chain-of-Thought (CoT) and implicit latent CoT. - Empirical analyses are conducted on existing methods, comparing performance and efficiency. - Open challenges are presented, including human-centric controllable reasoning and ensuring safety. - The paper suggests key insights for enhancing LRMs' inference efficiency via model merging, new architectures, and agent routers. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs) | N/A |
| [Classical Planning with LLM-Generated Heuristics: Challenging the State
  of the Art with Python Code](https://arxiv.org/abs/2503.18809) | jendrikseipp, andregrahl, abcorrea | - This paper introduces a novel approach to classical planning using Large Language Models (LLMs) to automatically generate domain-dependent heuristic functions, written in Python code, for greedy best-first search. - The pipeline prompts an LLM multiple times with domain descriptions, training tasks, and examples of heuristics to obtain a pool of candidate heuristic functions. - The best-performing heuristic is selected based on its performance on a training set and then used to solve unseen test tasks. -  Experimental results on the International Planning Competition (IPC) 2023 domains show that these LLM-generated heuristics outperform state-of-the-art domain-independent heuristics and are competitive with leading learning algorithms for domain-dependent heuristics, even when implemented within an unoptimized Python planner (Pyperplan) competing against highly optimized C++ planners. | ['Natural Language Processing'] | N/A | N/A |
| [UPME: An Unsupervised Peer Review Framework for Multimodal Large
  Language Model Evaluation](https://arxiv.org/abs/2503.14941) | Zheyuan Liu, Yibing, yuehuang, MunanNing, 77Hui | - This paper introduces UPME, an unsupervised peer review framework for evaluating multimodal large language models (MLLMs). - UPME uses only image data, allowing models to automatically generate questions and conduct peer reviews of answers from other models, reducing reliance on human annotations. - A vision-language scoring system is introduced to mitigate biases, focusing on response correctness, visual understanding and reasoning, and image-text correlation. - Experimental results on the MMStar and ScienceQA datasets demonstrate that UPME achieves high Pearson and Spearman correlations with human evaluations, showing strong alignment with human preferences. - The framework addresses limitations of existing MLLM evaluation methods by reducing human workload and mitigating biases such as self-preference and verbosity. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Decoupling Angles and Strength in Low-rank Adaptation](https://arxiv.org/abs/2503.18225) | Zeynep Akata, Leander Girrbach, Massimo Bini | - This paper introduces DeLoRA, a novel parameter-efficient fine-tuning method that enhances the robustness of low-rank adaptation by decoupling angular learning from adaptation strength. - DeLoRA achieves this by normalizing and scaling learnable low-rank matrices, effectively controlling the magnitude of weight updates while maintaining expressiveness. - The method is derived from and improves upon both LoRA and ETHER, combining their respective strengths and mitigating their limitations. - Evaluations on image generation, natural language understanding, and instruction tuning tasks demonstrate that DeLoRA matches or surpasses the performance of competing methods while exhibiting improved robustness. - Ablation studies validate the design choices of DeLoRA and highlight its advantages in terms of hyperparameter sensitivity and resistance to performance degradation during extended training. | ['Image-to-Image', 'Text-to-Image', 'Natural Language Processing'] | [Link](https://github.com/ExplainableML/DeLoRA) | N/A |
| [Entropy-Based Adaptive Weighting for Self-Training](https://arxiv.org/abs/2503.23913) | Wei Wang, Mingyu Derek Ma, Yihe Deng, Xiaoxuan Wang | - This paper introduces Entropy-Based Adaptive Weighting for Self-Training (EAST), a novel method for improving the mathematical reasoning capabilities of large language models (LLMs). - EAST employs an adaptive weighting strategy that prioritizes uncertain data during self-training by assigning higher weights to data points where the model exhibits higher uncertainty, as measured by the entropy of the model's sample distribution. - This approach encourages the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. - Experimental results on GSM8K and MATH benchmarks demonstrate that EAST consistently outperforms baseline methods, achieving a 1% gain over the backbone model on MATH and a further 1-2% performance boost on GSM8K compared to the vanilla self-training method. - EAST effectively addresses the limitation of existing self-training methodologies that treat all generated training data uniformly, regardless of the model's confidence level. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/mandyyyyii/east) | N/A |


## Papers for 2025-03-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through
  Lightweight Vocabulary Adaptation](https://arxiv.org/abs/2503.19693) | Roi Reichart, ehoffer, eyalbd, nitay, itaynakash | - AdaptiVocab, a novel end-to-end approach, adapts LLM vocabularies to enhance efficiency in domain-specific, low-resource settings by replacing general tokens with domain-specific n-grams. - It involves a vocabulary modification algorithm, a tokenization patching algorithm, an embedding initialization technique using exponential weighting, and a lightweight fine-tuning strategy focusing on the embedding and outer layers. - AdaptiVocab reduced token usage by over 25% in two 7B LLMs across three niche domains without compromising performance on automatic generation quality metrics or human evaluations. - Lightweight fine-tuning proved crucial, boosting domain-specific question-answering performance even for off-the-shelf LLMs, whereas other fine-tuning techniques like LoRA underperformed. - The method is tokenizer and architecture agnostic, making it widely applicable with minimal adaptation overhead. | ['Natural Language Processing', 'Text Generation'] | [Link](github.com/itay-nakash/AdaptiVocab) | N/A |
| [Exploring Data Scaling Trends and Effects in Reinforcement Learning from
  Human Feedback](https://arxiv.org/abs/2503.22230) | amusingchao, qingping95, zhengwu07, glnbyte, Swtheking | - This paper explores data scaling in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs), identifying reward hacking and decreasing response diversity as key bottlenecks. - It introduces a hybrid reward system combining Reasoning Task Verifiers (RTV) and a Generative Reward Model (GenRM) to mitigate reward hacking, and a prompt selection method (Pre-PPO) to improve response diversity. - Prioritizing mathematical and coding tasks in early RLHF training is found to boost performance due to their inherent fine-grained distinctions and clear ground truths. - Experiments across two model sizes show that RTV exhibits the strongest resistance to reward hacking, and Pre-PPO enhances model performance and generalization, especially on challenging tasks. - The work highlights the importance of careful data construction and provides practical methods for overcoming critical performance barriers in RLHF, demonstrating improved effectiveness and scalability. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for
  Sequential Recommendation](https://arxiv.org/abs/2503.22675) | Xu Chen, Jun Xu, TengShi, KID-22, TangJiakai5704 | - ReaRec, a novel reasoning-enhanced sequential recommendation framework, is proposed to improve user representation learning by incorporating multi-step implicit reasoning during inference. - ReaRec leverages reasoning chains before making final predictions, enabling increased computational depth for better capturing evolving user preference and long-tail item understanding. - Two learning strategies, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), are introduced to address optimization challenges and mitigate reasoning degradation in ReaRec. - ERL uses ensemble learning to aggregate diverse reasoning results, while PRL employs progressive temperature annealing and contrastive learning for refined reasoning. - ReaRec consistently outperforms existing ID-based and text-based sequential recommendation models, enhancing performance by up to 50% on five real-world datasets via optimal reasoning step selection. | ['Natural Language Processing', 'Question Answering', 'Summarization', 'Feature Extraction', 'Text Generation'] | [Link](https://github.com/TangJiakai/ReaRec) | N/A |
| [A Survey of Efficient Reasoning for Large Reasoning Models: Language,
  Multimodality, and Beyond](https://arxiv.org/abs/2503.21614) | Elliott, weigao266, Warrieryes, yaful, Xiaoye08 | - This paper surveys recent efforts to improve the reasoning efficiency of Large Reasoning Models (LRMs), focusing on reducing the tendency of these models to produce excessively long and often redundant reasoning traces. - The authors categorize common patterns of inefficiency, such as redundant content, overthinking simple questions, and incoherent reasoning, and discuss the challenges unique to optimizing reasoning efficiency in LRMs, including quantifying reasoning utility, controlling thinking length, architectural limitations, and cross-task generalization. - The survey organizes methods for improving reasoning efficiency across the LRM lifecycle, covering pretraining techniques like latent space pretraining and subquadratic attention, supervised fine-tuning strategies like reasoning chain compression and latent-space SFT, reinforcement learning methods with and without length rewards, and inference-time techniques like length budgeting, system switching, model switching, and parallel search. -  The paper further explores potential future research directions, including efficient multimodal and video reasoning, efficient test-time scaling and infinity thinking, efficient and trustworthy reasoning, and building efficient reasoning applications. - The authors aim to provide a comprehensive overview of the current state and future trends in efficient reasoning for LRMs, serving as a valuable resource for researchers in this rapidly evolving field. | ['Natural Language Processing', 'Question Answering', 'Multimodal'] | N/A | N/A |
| [PHYSICS: Benchmarking Foundation Models on University-Level Physics
  Problem Solving](https://arxiv.org/abs/2503.21821) | armanc, jsous, henryL7, yilunzhao, Carrie777 | - PHYSICS, a benchmark dataset comprising 1,297 university-level physics problems across six core domains (classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics), is introduced to assess the problem-solving skills of foundation models. - A robust automated evaluation system using SymPy and GPT-40 is implemented to standardize mathematical expressions, verify numerical content, and ensure the logical correctness of solutions. - Evaluation across 33 foundation models reveals that even the best-performing model (o3-mini) achieves only 59.9% accuracy, highlighting the significant challenges physics problem-solving presents for current models. - Key areas for improvement are identified through comprehensive error analysis and diverse prompting strategies, and retrieval-augmented generation (RAG) is shown to improve performance across top models. - The benchmark and analysis provide guidance for developing more advanced AI models capable of handling complex reasoning tasks in scientific domains. | ['Question Answering', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/yale-nlp/Physics) | N/A |
| [OThink-MR1: Stimulating multimodal generalized reasoning capabilities
  via dynamic reinforcement learning](https://arxiv.org/abs/2503.16081) | Changwang Zhang, Feng Liu, Yuting Zhang, Zhiyuan Liu, jwanglux | - This paper introduces OThink-MR1, a Multimodal Large Language Model (MLLM) enhanced with a dynamic reinforcement learning approach called Group Relative Policy Optimization with Dynamic KL divergence (GRPO-D) for improved generalized reasoning across diverse multimodal tasks. - GRPO-D addresses limitations of standard reinforcement learning and supervised fine-tuning by dynamically balancing exploration and exploitation during training, leading to better performance in both same-task and cross-task evaluations. - Experiments on visual counting and geometry reasoning tasks demonstrate that OThink-MR1 with GRPO-D achieves relative improvements exceeding 5.72% over supervised fine-tuning and 13.59% over standard GRPO in same-task settings, and over 61.63% improvement in cross-task generalization. - The proposed dynamic KL divergence strategy in GRPO-D enhances the model's ability to transfer knowledge effectively across different multimodal tasks by balancing exploration and exploitation, showing stronger generalized reasoning capabilities compared to traditional methods. - The paper also highlights the effectiveness of GRPO-D with smaller models like Qwen2-VL-2B-Instruct, where strategic exploration-exploitation balance compensates for limited model capacity and outperforms larger models trained with less efficient post-training methods. | ['Multimodal', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/leonardPKU/GEOQA_R1V_Train_8K), [Link](https://huggingface.co/datasets/leonardPKU/clevr_cogen_a_train), [Link](https://huggingface.co/datasets/leonardPKU/superclevr/tree/main) |
| [4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object
  Understanding](https://arxiv.org/abs/2503.17827) | mhelhoseiny, ajhamdi, TonNew, bing-li-ai, vxuanz | - This paper introduces 4D-Bench, the first benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in understanding 4D objects (3D objects with temporal evolution). - The benchmark features two tasks: 4D object question answering (with five subtasks related to different understanding dimensions) and 4D object captioning, both requiring the model to analyze multi-view videos of dynamic 3D objects. - The authors evaluate several open-source and closed-source MLLMs on 4D-Bench and find that even state-of-the-art MLLMs like GPT-4 perform significantly worse than humans, especially in object counting, action recognition and temporal reasoning. - Experimental results from 4D object caption generation reveal that current MLLMs are better at understanding object appearance than motion. - The benchmark and the experimental findings highlight a significant performance gap in 4D object understanding and offer valuable insights for improving future MLLMs in this critical direction. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | N/A | [Link](https://huggingface.co/datasets/KAUST-AILab/4D-Bench) |
| [A Refined Analysis of Massive Activations in LLMs](https://arxiv.org/abs/2503.22329) | Fabian Güra, akanyaani, nilabhra, louisowen6 | - This paper analyzes "massive activations" in LLMs, finding that suppressing them isn't always detrimental to performance and that mitigation strategies like Attention KV bias are model-specific. - The study investigates novel hybrid mitigation strategies, pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT), which successfully balances mitigation with preserved downstream performance. - The analysis challenges prior assumptions about the detrimental nature of massive activations and proposes architecture-agnostic strategies for managing them. - The study uses perplexity and downstream task performance, including commonsense reasoning, question answering, and multi-step reasoning, on datasets like WikiText, C4, PG-19, HellaSwag, PIQA, SIQA, WinoGrande, TriviaQA, and ARC, with a wide range of LLMs like GPT-2, Falcon, OPT, LLaMA, Gemma, OLMo, Phi, and Mistral. - For LLaMA-1B, TVR and hybrid strategies improve downstream task accuracy by up to 2.2% compared to the baseline and outperform single mitigation strategies like KV Bias and DyT. | ['Natural Language Processing'] | [Link](https://github.com/bluorion-com/refine_massive_activations) | N/A |
| [ReFeed: Multi-dimensional Summarization Refinement with Reflective
  Reasoning on Feedback](https://arxiv.org/abs/2503.21332) | jasoncai, hwany-j, Myyhlee, hyang0503, hamzzi | - ReFeed is a novel multi-dimensional summarization refinement pipeline that leverages reflective reasoning on feedback to enhance faithfulness, completeness, and conciseness. - It addresses challenges like trade-offs between dimensions, ordering bias from feedback presentation, and inaccuracies within feedback itself by using backtracking, simultaneous reasoning, and noise filtering. - A new dataset, SumFeed-CoT, is introduced, comprising long chain-of-thought (Long-CoT) reasoning examples on feedback for training ReFeed, along with data shuffling across dimensions for mitigating ordering bias. - Experiments demonstrate ReFeed's superior performance compared to sequential and single-dimension baselines, and its comparable performance to a larger teacher model with significantly faster inference. - ReFeed also exhibits robustness to varying feedback quality, unlike other pipelines, showcasing its ability to process and validate information effectively. | ['Summarization'] | N/A | N/A |
| [MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via
  Reasoning Agentic Workflow](https://arxiv.org/abs/2503.18968) | Yueming Jin, Chang Han Low, morson, ZiyueWang | - MedAgent-Pro, a novel reasoning agentic workflow for evidence-based multi-modal medical diagnosis, is proposed.  - The model leverages a hierarchical structure with task-level reasoning for formulating unified diagnostic plans using retrieved clinical criteria and case-level diagnosis utilizing specialized vision models as tools for detailed analysis.  - Experimental results on glaucoma and heart disease diagnosis datasets demonstrate that MedAgent-Pro surpasses both general Multi-modal Large Language Models (MLLMs) and task-specific solutions, improving mean Average Classification Accuracy (mACC) by up to 32.3% and F1 scores by up to 55.1%. - Ablation studies confirm that using multiple indicators and the Mixture of Experts (MOE) decider contributes to better diagnostic performance.  - The interpretability of the system through visual evidence and clinical guidelines enhances reliability and transparency in medical decision-making. | ['Multimodal', 'Visual Question Answering', 'Image Segmentation'] | [Link](https://github.com/jinlab-imvr/MedAgent-Pro) | N/A |
| [On Large Multimodal Models as Open-World Image Classifiers](https://arxiv.org/abs/2503.21851) | Yiming Wang, Enrico Fini, paolorota, massimilianom, altndrr | - This paper evaluates the performance of Large Multimodal Models (LMMs) in open-world image classification, where the categories are not predefined. - The authors introduce a comprehensive evaluation protocol with four metrics: text inclusion, Llama inclusion, semantic similarity, and concept similarity to analyze different aspects of model predictions. - Evaluating 13 LMMs across 10 image classification benchmarks, this study shows that LMMs outperform contrastive methods in open-world settings but lag behind closed-world models. - The paper identifies challenges related to granularity and fine-grained classification, providing insights into the types of errors LMMs make. - It is shown that these errors can be reduced through tailored prompting and reasoning strategies. | ['Computer Vision', 'Image Classification', 'Zero-Shot Classification', 'Multimodal'] | [Link](https://github.com/altndrr/lmms-owc) | N/A |


## Papers for 2025-03-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/abs/2503.21776) | Potentialts, guozonghao96, BreakLee, kxgong, KaituoFeng | - Video-R1, a novel framework leveraging reinforcement learning (RL) to enhance video reasoning capabilities in multimodal large language models (MLLMs), is introduced. - The model utilizes a novel Temporal Group Relative Policy Optimization (T-GRPO) algorithm, which encourages temporal reasoning by contrasting performance on ordered and shuffled video frames, and addresses the lack of temporal modeling in existing RL methods. - Two new datasets, Video-R1-COT-165k for supervised fine-tuning and Video-R1-260k for reinforcement learning, comprised of image and video data, are introduced to address the scarcity of high-quality video reasoning data. - Video-R1-7B achieves state-of-the-art performance on the VSI-Bench video spatial reasoning benchmark with 35.8% accuracy, outperforming the commercial proprietary model GPT-4. - Significant performance improvements are also observed across other video reasoning and general video benchmarks like VideoMMMU, MMVU, MVBench, and TempCompass. | ['Video-Text-to-Text', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/tulerfeng/Video-R1) | N/A |
| [UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement
  Learning](https://arxiv.org/abs/2503.21620) | Xi Yin, hsli-cuhk, guoyaxuan0106, Yuxiang007, LZXzju | - UI-R1, a novel framework, leverages reinforcement learning with a rule-based reward function to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) for Graphic User Interface (GUI) action prediction. - The model is trained on a small, curated dataset of 136 mobile GUI tasks with a unified rule-based action reward, incorporating action type, argument, and format rewards. - Evaluated on in-domain (ANDROIDCONTROL) and out-of-domain (ScreenSpot-Pro) benchmarks, UI-R1-3B demonstrates substantial improvements over the base model (Qwen2.5-VL-3B) and achieves competitive performance with larger SFT models trained on considerably more data. - UI-R1-3B significantly improves action type prediction accuracy by 15% and grounding accuracy by 10.3% on ANDROIDCONTROL, while on ScreenSpot-Pro it surpasses the base model by 6% and performs on par with larger models like OS-Atlas-7B. - The results demonstrate the efficacy and data efficiency of rule-based reinforcement learning for enhancing GUI understanding and control in MLLMs. | ['Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [Challenging the Boundaries of Reasoning: An Olympiad-Level Math
  Benchmark for Large Language Models](https://arxiv.org/abs/2503.21380) | Wayne Xin Zhao, jrwen, TimothyCzp, EliverQ, CoderBak | - This paper introduces OlymMATH, a challenging bilingual (English and Chinese) benchmark dataset designed for evaluating the mathematical reasoning capabilities of Large Language Models (LLMs), especially those using slow-thinking methods. - The dataset consists of 200 Olympiad-level math problems, divided into easy (AIME-level) and hard subsets, covering algebra, geometry, number theory, and combinatorics. - Experimental results demonstrate that even state-of-the-art LLMs like DeepSeek-R1 and OpenAI's 03-mini struggle with the benchmark, particularly the hard subset, achieving accuracies of only 21.2% and 30.3%, respectively. - Analysis reveals that some LLMs resort to empirical guessing or shortcut solutions rather than rigorous reasoning, indicating a need for process-level evaluation to better assess true reasoning capabilities. - The benchmark aims to promote the development of more robust and powerful reasoning models by providing a rigorous and challenging evaluation platform. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs) | N/A |
| [VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic
  Faithfulness](https://arxiv.org/abs/2503.21755) | mimihe, yinanhe, jackyhate, HongboLiu, Ziqi | - VBench-2.0 is a new benchmark suite designed to evaluate the intrinsic faithfulness of video generation models, going beyond superficial aspects like pixel fidelity and prompt adherence. - It assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each broken down into fine-grained capabilities, using a combination of state-of-the-art VLMs, LLMs, and specialized detectors. - An extensive human preference annotation study validates the alignment of VBench-2.0's automated evaluation with human judgment. - The evaluation of four recent SOTA models (Kling 1.6, Sora-480p, HunyuanVideo, and CogVideoX-1.5) reveals emerging capabilities in some areas but also significant limitations in generating complex plots, handling dynamic object changes, and maintaining commonsense reasoning. - VBench-2.0 complements existing benchmarks like VBench by focusing on intrinsic faithfulness and aims to guide the development of more realistic and sophisticated video generation models. | ['Text-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/Vchitect/VBench) | N/A |
| [Large Language Model Agent: A Survey on Methodology, Applications and
  Challenges](https://arxiv.org/abs/2503.21460) | qqlong, joeyleo, evan-gyy, yszhao, luojunyu | - This survey paper systematically deconstructs Large Language Model (LLM) agent systems, providing a methodology-centered taxonomy that connects architectural foundations, collaboration mechanisms, and evolutionary pathways. - The paper examines how LLM agents are constructed (profile definition, memory, planning, action execution), how they collaborate (centralized, decentralized, hybrid), and how they evolve (self-learning, co-evolution, external resources). - It also addresses evaluation methods, available tools, real-world challenges (security, privacy, social impact), and diverse applications of LLM agents. - By offering a unified architectural perspective and surveying recent developments, this work provides a structured framework for understanding LLM agents and identifies promising future research directions. - The authors make their collection of LLM agent papers available on GitHub. | ['Natural Language Processing', 'Summarization'] | [Link](https://github.com/luo-junyu/Awesome-Agent-Papers) | N/A |
| [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large
  Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729) | chenyn66, liuweichuan, NeoZ123, caoshulin, ZhiCheng0326 | - ReaRAG, a factuality-enhanced reasoning model for Retrieval-Augmented Generation (RAG), iteratively constructs knowledge-guided reasoning chains to improve LRM factuality. - The model selects actions from a predefined action space (Search and Finish) based on deliberate thinking generated by an LRM. If Search is selected, a query is executed, and the result guides subsequent reasoning. This process repeats until Finish is chosen. - ReaRAG outperforms other baselines on multi-hop question answering, demonstrating improvements of 14.5%, 6.5%, and 2.25% ACCL on MuSiQue, HotpotQA, and IIRC, respectively. - The proposed method enhances the factuality of LRMs by effectively integrating robust reasoning with external knowledge sources.  - ReaRAG showcases a strong reflective ability by recognizing errors and refining reasoning trajectories through external knowledge and deliberate thinking. | ['Question Answering'] | N/A | N/A |
| [Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for
  Embodied Interactive Tasks](https://arxiv.org/abs/2503.21696) | Guiyang1001, tricktreat, yijiang, Gangao, zwq2018 | - This paper introduces Embodied-Reasoner, a novel model for embodied interactive tasks that combines visual search, reasoning, and action.  - It extends deep-thinking capabilities to embodied scenarios by generating diverse thinking processes such as analysis, planning, and reflection.  -  Embodied-Reasoner is trained with a three-stage pipeline: imitation learning, rejection sampling tuning for exploration, and reflection tuning for self-correction.  - Evaluated on AI2-THOR simulator tasks including Search, Manipulation, Transportation, and Composite, it significantly outperforms state-of-the-art VLMs and visual reasoning models, exceeding OpenAI models and Claude by a large margin (+9% to +24%). - Notably, Embodied-Reasoner demonstrates superior performance on complex, long-horizon tasks and exhibits more consistent reasoning and fewer repeated searches.  | ['Robotics', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/zwq2018/embodied_reasoner) | [Link](https://embodied-reasoner.github.io/) |
| [ResearchBench: Benchmarking LLMs in Scientific Discovery via
  Inspiration-Based Task Decomposition](https://arxiv.org/abs/2503.21248) | yuqiangli, bgao22182, jinjieni, ZonglinY, yujieliu | - Introduced ResearchBench, the first large-scale benchmark for evaluating LLMs in scientific discovery, focusing on inspiration retrieval, hypothesis composition, and hypothesis ranking. - Developed an automated framework to extract research questions, background surveys, inspirations, and hypotheses from scientific papers across 12 disciplines, ensuring benchmark scalability and preventing data contamination. - Evaluated popular LLMs on the benchmark and found that they perform surprisingly well in retrieving inspirations, an out-of-distribution task, suggesting their ability to uncover novel knowledge associations. - Showed LLMs also possess moderate capabilities in hypothesis composition and ranking, with performance improving with model scale and advanced training strategies, indicating potential for growth. - Identified inspiration retrieval as a key bottleneck and posit LLMs as "research hypothesis mines," capable of automating scientific discovery by generating and ranking hypotheses with minimal human intervention, pending further development to address current limitations. | ['Natural Language Processing'] | N/A | N/A |
| [ChatAnyone: Stylized Real-time Portrait Video Generation with
  Hierarchical Motion Diffusion Model](https://arxiv.org/abs/2503.21144) | Peng Zhang, Chaonan Ji, Jinwei Qi, Liefeng, shengxu97 | - ChatAnyone is a novel framework for stylized real-time portrait video generation from a single image and audio, utilizing a two-stage approach. - The first stage employs a hierarchical motion diffusion model that generates facial and upper-body motion representations from audio input, incorporating style control and transfer through AdaLN and reference motion sequences. - The second stage uses a hybrid control fusion generative model, combining explicit and implicit motion representations to warp appearance features and generate portrait video frames, enhanced by a facial refinement module for added realism. - Explicit hand control signals derived from a MANO template are injected into the generator to improve hand gesture quality. - Experimental results demonstrate state-of-the-art performance in upper-body video generation and talking-head animation with high fidelity and expressiveness at 30fps on a 4090 GPU. | ['Text-to-Video', 'Audio-to-Audio', 'Computer Vision'] | N/A | N/A |
| [ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging](https://arxiv.org/abs/2503.21088) | Ziyan Jiang, Yi Zhong, Yanqiu Zhao, Saberlve, HaomingXu | - This paper introduces a novel unlearning system leveraging Model Merging, specifically TIES-Merging, for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. - The system merges two specialized models, one over-forgetting and one under-forgetting, to achieve a balanced unlearned model. - The ZJUKLAB team achieved second place in the competition, with their 7B model achieving a Task Aggregate Score of 0.944 and an Aggregate Score of 0.487. - Local experiments demonstrate the effectiveness of the merging technique, achieving near-perfect MIA scores (0.501) and high Task Aggregate scores (0.939) while maintaining comparable MMLU scores. - Analyses of performance trajectories, loss dynamics, and parameter changes provide insights into the effectiveness of the model merging strategy. | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/unlearn/tree/main/semeval25) | N/A |
| [FinAudio: A Benchmark for Audio Large Language Models in Financial
  Applications](https://arxiv.org/abs/2503.20990) | Yueru1, Shashidhar, ShirleyY, Acatsama, YupengCao | - FINAUDIO, a benchmark designed to evaluate Audio Large Language Models (AudioLLMs) in the financial domain, is introduced, focusing on tasks relevant to financial analysis and investment decisions. - The benchmark includes three tasks: Automatic Speech Recognition (ASR) for short financial audio clips, ASR for long financial audio recordings, and summarization of long financial audio, using five datasets, including a newly created dataset for financial summarization. - Seven prominent AudioLLMs were evaluated on FINAUDIO, revealing performance variations based on audio length and task type, with open-source models like Whisper-v3 demonstrating strong performance in ASR. - The evaluation highlighted the limitations of current AudioLLMs in handling long financial audio and their sensitivity to prompt variations, suggesting areas for future research and development in instruction tuning and domain-specific knowledge. - The study emphasizes the need for more robust AudioLLMs in finance and the potential for using open-source models for financial applications. | ['Audio', 'Automatic Speech Recognition', 'Summarization', 'Natural Language Processing'] | N/A | N/A |
| [Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile
  Gaussian Feature Fields](https://arxiv.org/abs/2503.20776) | Hui Ren, Fanzhiwen, ir1d, ShuwangZhang00, shijiezhou | - Feature4X introduces a novel framework to lift 2D vision foundation models into dynamic 4D feature fields using Gaussian Splatting, enabling interaction with monocular videos. - It employs a compact representation by attaching base features to Motion Scaffold nodes and interpolating per-Gaussian features, reducing computational costs and ensuring smooth features. - The unified feature field supports diverse vision tasks like segmentation, editing, and VQA across 2D, 3D, and 4D, integrating seamlessly with LLMs for high-level language or direct user interaction. - Experiments showcase novel view segment anything, geometric and appearance editing, and free-form VQA across all timesteps, empowered by LLMs in feedback loops. - Feature4X provides a foundation for scalable and contextually aware systems for immersive dynamic 4D scene interaction, achieving high performance with less memory usage. | ['Computer Vision', 'Image Segmentation', 'Visual Question Answering', 'Multimodal'] | [Link](https://feature4x.github.io/) | N/A |
| [Unified Multimodal Discrete Diffusion](https://arxiv.org/abs/2503.20853) | Katerina Fragkiadaki, Deepak765, Sid1275, mihirpd, aswerdlow | - UniDisc, a unified multimodal discrete diffusion model, jointly processes and generates text and images using a shared vocabulary and full self-attention transformer. - UniDisc outperforms autoregressive (AR) models in FID and CLIP scores for conditional generation, exhibits strong joint image-text inpainting abilities, and offers better inference efficiency and controllability. - It uses discrete noise (random masking) and learns to map masked tokens into multimodal tokens during inference, offering advantages over continuous diffusion for discrete data. - UniDisc demonstrates enhanced controllability, editability, inpainting, and a flexible trade-off between inference time and generation quality. - A scaled-up 1.4B parameter UniDisc model trained on web-scale image-text data showcases further advancements in capabilities. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Unconditional Image Generation'] | [Link](https://unidisc.github.io) | N/A |
| [LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized
  Text-Guided Image Editing](https://arxiv.org/abs/2503.21541) | Sirisha Rambhatla, Meet Soni, Achint Soni | - LOCATEdit, a novel text-guided image editing technique, refines cross-attention maps using graph Laplacian regularization for spatially consistent and localized modifications. - It leverages a dual-branch editing paradigm, injecting source branch cross-attention maps into the target branch while incorporating an IP-Adapter for enhanced semantic guidance and a selective pruning operator for noise suppression in text embeddings. - The method constructs a CASA (Cross and Self-Attention) graph, where nodes represent image patches and edges capture patch relationships from self-attention, ensuring smooth transitions and preventing over-editing. - Evaluated on the PIE-Bench dataset, LOCATEdit demonstrates superior spatial consistency and semantic alignment compared to state-of-the-art methods, preserving structural integrity and background fidelity while achieving precise, localized edits. - Experimental results show improvements across multiple metrics, including structure consistency, background preservation (PSNR, LPIPS, MSE, SSIM), and target prompt-image alignment (CLIP similarity). | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | [Link](https://github.com/LOCATEdit/LOCATEdit/) | N/A |
| [LLPut: Investigating Large Language Models for Bug Report-Based Input
  Generation](https://arxiv.org/abs/2503.20578) | Tarannum Shaila Zaman, imranraad, Subarna10, alifalhasan | - This paper introduces LLPut, a technique to evaluate Large Language Models (LLMs) for extracting failure-inducing inputs from bug reports, which are essential for reproducing and diagnosing software bugs. - The study evaluates three open-source LLMs: LLaMA, Qwen, and Qwen-Coder, on a dataset of 206 annotated bug reports from the Linux coreutils project. - Using a one-shot prompting approach and the BLEU score as an evaluation metric, the study shows that generative LLMs outperform BERT-based NLP model in extracting inputs. - Qwen achieves the highest performance, with 62.62% of generated outputs achieving a BLEU-2 score of 0.5 or greater, demonstrating the potential of LLMs for automating input extraction. - The authors also discuss the observed error categories, such as variation in wording, failed extractions, and annotation subjectivity, highlighting potential areas for future research. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/imranraad/LLPut) | [Link](https://zenodo.org/record/15092886) |


## Papers for 2025-03-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Dita: Scaling Diffusion Transformer for Generalist
  Vision-Language-Action Policy](https://arxiv.org/abs/2503.19757) | TTTTTony, MIASANMIA, robot-haonan, TianyiZhang0213, zhihou | - Dita, a novel Diffusion Transformer (DiT) policy for generalist robotic learning, leverages a Transformer architecture to denoise continuous action sequences via a unified multimodal diffusion process. - Unlike previous methods that condition denoising on fused embeddings, Dita uses in-context conditioning, aligning actions with raw visual and language instruction tokens. - This approach allows Dita to explicitly model action deltas and environmental nuances, resulting in state-of-the-art or competitive performance on multiple simulation benchmarks. - Dita also demonstrates robust real-world 10-shot adaptation to complex, long-horizon tasks in novel environments, handling object arrangements and lighting variations with third-person camera input. - Its simple 334M parameter architecture offers a lightweight, versatile, and open-source baseline for generalist robot policy learning. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://robodita.github.io) | N/A |
| [Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215) | JialinWang, chenkq, bluelike, jinzheng-he, ZhifangGuo | - Qwen2.5-Omni is an end-to-end multimodal model based on the Thinker-Talker architecture, where the Thinker processes multimodal inputs (text, image, audio, and video) and generates text, while the Talker utilizes these representations to generate streaming speech and text. - It introduces Time-aligned Multimodal RoPE (TMRoPE), a novel positional embedding approach to synchronize timestamps of video and audio inputs and leverages a block-wise streaming processing method to enable streaming multimodal information input. - It utilizes a sliding-window DiT model to stream audio output and reduce latency. - Qwen2.5-Omni achieves comparable performance with single-modality models like Qwen2.5-VL and Qwen2-Audio and state-of-the-art results on multimodal benchmarks such as OmniBench and AV-Odyssey. - Its end-to-end speech instruction following capabilities are on par with its text input performance, and the streaming Talker outperforms alternatives in speech generation robustness and naturalness. | ['Multimodal', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition', 'Image-to-Text', 'Video-Text-to-Text', 'Any-to-Any'] | [Link](https://github.com/QwenLM/Qwen2.5-Omni) | [Link](https://huggingface.co/Qwen) |
| [LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?](https://arxiv.org/abs/2503.19990) | Leoxing, KennyUTC, zengyh1900, favourisnotyou, KexianTang | - Introduced LEGO-Puzzles, a novel benchmark to evaluate the multi-step spatial reasoning capabilities of Multimodal Large Language Models (MLLMs). - LEGO-Puzzles consists of 1,100 curated visual question-answering (VQA) samples across 11 distinct tasks related to spatial understanding and sequential reasoning, inspired by LEGO construction. - Evaluations on 20 state-of-the-art MLLMs reveal significant performance gaps compared to humans, especially in multi-step reasoning and spatially coherent visual output generation. - Even the most advanced MLLMs struggle with complex spatial relationships, highlighting the need for further research in multimodal spatial reasoning. - The benchmark also includes an image generation component, demonstrating the difficulty of generating spatially coherent and instruction-following image outputs even for top-performing MLLMs. | ['Multimodal', 'Visual Question Answering', 'Image-to-Image'] | N/A | N/A |
| [Wan: Open and Advanced Large-Scale Video Generative Models](https://arxiv.org/abs/2503.20314) | HermanZ, chenweix7, chaojiemao, baoleai, ang-annng | - This paper introduces Wan, a suite of open-source, large-scale video foundation models based on the diffusion transformer architecture. - Wan incorporates a novel spatio-temporal variational autoencoder (VAE), scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. - The 14B parameter model demonstrates state-of-the-art performance across multiple benchmarks, outperforming existing open-source models and commercial solutions like RunwayML, Hunyuan Video, and CogVideoX. - Wan offers multiple capabilities, including text-to-video and image-to-video generation, video editing, visual text generation in both English and Chinese, and real-time video generation using an efficient 1.3B parameter model.  - Both the code and model weights for the entire Wan series are open-sourced to facilitate community growth and advancement in video generation technology. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Text-to-Image', 'Image-to-Image', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/Wan-Video/Wan2.1) | N/A |
| [Open Deep Search: Democratizing Search with Open-source Reasoning Agents](https://arxiv.org/abs/2503.20201) | speedyarda, ljirwin, pchiniya, cabxyz, salzubi401 | - This paper introduces Open Deep Search (ODS), an open-source framework designed to democratize access to advanced search AI capabilities. - ODS enhances open-source Large Language Models (LLMs) with reasoning agents and a novel web search tool, enabling them to answer complex queries by leveraging real-time information retrieval. - ODS outperforms existing state-of-the-art closed-source solutions such as Perplexity AI and OpenAI's GPT-40 Search Preview on benchmarks like SimpleQA and FRAMES. - For example, ODS-v2 achieves 75.3% accuracy on FRAMES, a 9.7% improvement over GPT-40 Search Preview. - The framework consists of Open Search Tool for context retrieval and Open Reasoning Agent to orchestrate tools, including search and calculator, showcasing its effectiveness in complex reasoning tasks. | ['Question Answering'] | [Link](https://github.com/sentient-agi/OpenDeepSearch) | N/A |
| [GenHancer: Imperfect Generative Models are Secretly Strong
  Vision-Centric Enhancers](https://arxiv.org/abs/2503.19480) | yshan2u, yxgeee, aether25, tttoaster, msj9817 | - GenHancer, a two-stage post-training method, enhances the fine-grained visual representations of discriminative models like CLIP by leveraging lightweight, randomly initialized generative models. - This method uses visual features, specifically the [CLS] token, as conditions for self-supervised reconstruction with the generative model, transferring fine-grained knowledge to the discriminative model. - Experiments show that perfect generation is not essential for representation enhancement, and a two-stage training approach effectively mitigates irrelevant information transfer. - GenHancer consistently outperforms previous methods on the MMVP-VLM benchmark, achieving a 6.0% improvement on OpenAICLIP and demonstrating its effectiveness across various CLIP backbones. - The enhanced CLIP model can be seamlessly integrated into multimodal large language models for improved vision-centric performance, without negatively impacting global semantic understanding. | ['Image Feature Extraction', 'Multimodal'] | N/A | N/A |
| [BizGen: Advancing Article-level Visual Text Rendering for Infographics
  Generation](https://arxiv.org/abs/2503.20672) | YuanYuhui, kevinlin311tw, bohanChen, Marseclipse, wukeming11 | - BizGen is a novel framework for generating business content like infographics and slides from article-length text prompts and ultra-dense layouts, addressing the challenges of long context lengths and data scarcity. - It uses a two-stage approach: 1) Building a scalable dataset (INFOGRAPHICS-650K) of high-quality infographics with dense layouts and captions through layered retrieval augmentation. 2) Implementing a layout-guided cross-attention mechanism in Glyph-SDXL-v2 that refines subsections using layout conditional CFG. - BizGen significantly outperforms DALL-E3, SD3 Large, and FLUX in visual text spelling accuracy and prompt following on the BizEVAL benchmark, particularly with complex layouts exceeding 20 layers. - It introduces layout conditional classifier-free guidance to further refine layer artifacts. - BizGen also supports multilingual generation across ten languages, achieving approximately 90% visual text spelling precision. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Gemini Robotics: Bringing AI into the Physical World](https://arxiv.org/abs/2503.20020) | abalakrishna123, TravisAStrong, montse90, jalayrac, saminda | - Introduces Gemini Robotics, a family of AI models designed for robotics, built upon Gemini 2.0, featuring a Vision-Language-Action (VLA) model and an Embodied Reasoning (ER) model. - Gemini Robotics controls robots directly to perform complex manipulation tasks, showing robustness and generalization to unseen environments and open-vocabulary instructions. - Gemini Robotics-ER extends Gemini's multimodal reasoning to the physical world, enabling object detection, pointing, trajectory and grasp prediction, and 3D understanding. - Demonstrates specialization of Gemini Robotics to long-horizon dexterous tasks like origami and card playing, adapting to novel robot embodiments, and few-shot learning. - Addresses safety considerations for large robotics models like Gemini Robotics. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree
  Search](https://arxiv.org/abs/2503.20757) | armanc, chenzhao, yilunzhao, AlexCCtop | - MCTS-RAG is introduced, a novel approach that combines Monte Carlo Tree Search (MCTS) with Retrieval-Augmented Generation (RAG) to enhance reasoning capabilities in small language models. - Unlike standard RAG or MCTS methods, MCTS-RAG dynamically integrates retrieval and reasoning, enabling adaptive retrieval strategies and improved knowledge integration. - Experimental results on ComplexWebQA, GPQA, and FoolMeTwice show significant performance improvements, exceeding 20% on some datasets with smaller language models, and demonstrating competitive performance with larger models like GPT-40. - MCTS-RAG effectively scales inference-time compute, refining both retrieval and reasoning through a search-based process to achieve higher accuracy. - The iterative refinement of queries and integration of retrieved information leads to enhanced decision-making and reduced hallucinations in knowledge-intensive tasks. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/yale-nlp/MCTS-RAG) | N/A |
| [ViLBench: A Suite for Vision-Language Process Reward Modeling](https://arxiv.org/abs/2503.20271) | cihangxie, xianft, alihiker, Helicopt, PahaII | - This paper introduces VILBENCH, a new benchmark designed to evaluate vision-language process reward models (PRMs), which provide step-wise feedback during reasoning processes, unlike output reward models (ORMs) that only evaluate final answers. - The researchers benchmark seven Vision-Language Large Language Models (VLLMs) across five existing datasets and find that better VLLMs don't always correlate with superior reward capabilities, and neither ORMs or PRMs consistently outperforms the other. - They construct VILBENCH, a new benchmark with 600 examples that emphasizes the necessity of step-wise feedback and demonstrate that OpenAI's GPT-40 with Chain-of-Thought only achieves 27.3% accuracy, while benefiting 3.0% more from PRM than ORM. - This paper also introduces ViLReward-73k, a 73.6k step-wise vision-language reward dataset, enabling the training of a specialized 3B parameter vision-language PRM (ViLPRM). - ViLPRM improves the average evaluation accuracy by 3.3% over standard CoT methods and up to 2.5% over its untrained counterpart on VILBENCH. | ['Multimodal', 'Visual Question Answering'] | [Link](https://ucsc-vlaa.github.io/ViLBench) | N/A |
| [LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior
  Accuracy Preservation](https://arxiv.org/abs/2503.19950) | Pingyi Luo, Bingsheng He, deciding, Zicong99, Concyclics | - LogQuant, a novel 2-bit quantization technique for KV Cache in large language model (LLM) inference, is introduced, delivering substantial memory savings while preserving performance. - LogQuant employs a log-based filtering mechanism to selectively compress the KV Cache across the entire context, outperforming existing methods that prioritize recent tokens or predict important tokens based on past attention patterns. - Benchmark tests show LogQuant improves throughput by 25%, increases batch size by 60% without raising memory consumption, and boosts accuracy on challenging tasks like Math and Code Completion by 40% to 200% compared to similar compression ratio techniques. - LogQuant's position-agnostic approach to attention calculation maintains accuracy while improving efficiency. - The technique integrates seamlessly with popular inference frameworks like Python's transformers library. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/Concyclics/LogQuantKV) | N/A |
| [ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving
  Systems](https://arxiv.org/abs/2503.20756) | xzwnlp, bozhong, xiangchen-dvi, JizhanFang, Chenxiwang | - This paper introduces ADS-Edit, a multimodal knowledge editing dataset for autonomous driving systems.  - The dataset addresses challenges such as traffic knowledge misunderstanding, complex road conditions, and diverse vehicle states in autonomous driving.  - ADS-Edit includes various real-world scenarios, multiple data types (video, multi-view images, single image), and comprehensive evaluation metrics (reliability, generality, and locality).  - The authors evaluate four knowledge editing baselines (Prompt, AdaLora, GRACE, and WISE) under single and lifelong editing scenarios.  - Experimental results demonstrate the effectiveness of memory-based editing methods (GRACE and WISE) in modifying LMM behavior for autonomous driving tasks, with GRACE achieving a 100% modification rate. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/zjunlp/EasyEdit) | N/A |
| [Beyond Words: Advancing Long-Text Image Generation via Multimodal
  Autoregressive Models](https://arxiv.org/abs/2503.20198) | Min Li, Lijuan, zyang39, linjieli222, Awiny | - This paper introduces LongTextAR, a novel multimodal autoregressive model specifically designed for generating images containing long text sequences, such as paragraphs or multi-sentence descriptions. - LongTextAR addresses limitations in existing text-to-image models that struggle with extended text inputs by introducing TextBinarizer, a specialized text-focused tokenizer that enhances text preservation in generated images. - LongTextAR combines TextBinarizer with a Llama 2-based autoregressive decoder and employs a hybrid tokenization strategy incorporating both visual and textual tokens for coherent image generation. - Extensive experiments demonstrate that LongTextAR outperforms current state-of-the-art models like Stable Diffusion 3.5 and GPT-40 with DALL-E 3 in generating long text accurately and consistently, especially on interleaved document and PowerPoint generation tasks. - The model also offers robust controllability over text properties such as font style, size, color, and alignment, enabling customization of generated text within images. | ['Text-to-Image', 'Multimodal'] | N/A | [Link](https://fingerrec.github.io/longtextar) |
| [Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs](https://arxiv.org/abs/2503.16870) | kw1jjang, Rock222, AndrewAhn, ya-mehdi, Anshumann | - This paper introduces Random Sampling Knowledge Distillation (RS-KD), an importance-sampling based method to accelerate knowledge distillation in Large Language Models (LLMs) by efficiently storing sparse logits. - RS-KD addresses the biases and limitations of existing sparse knowledge distillation approaches like Top-K caching by providing unbiased estimates of the teacher's probability distribution and preserving the gradient in expectation. - Using significantly less storage (only 12 tokens in experiments), RS-KD achieves performance comparable to full distillation, maintaining model performance while using only 0.01% of pre-computed teacher logits and significantly reducing training time overhead to under 10%. - The method demonstrates consistent improvements over cross-entropy training as student model size increases and remains effective across various model sizes, training tokens, and evaluation metrics, from 300M to 3B parameter models trained on up to 100B tokens. -  Further combining RS-KD with techniques like adaptive learning rates based on token confidence allows it to even surpass the performance of full knowledge distillation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2025-03-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CoMP: Continual Multimodal Pre-training for Vision Foundation Models](https://arxiv.org/abs/2503.18931) | Yu-Gang Jiang, Zuxuan Wu, Wujian Peng, Lingchen Meng, Row11n | - COMP, a continual multimodal pre-training framework, enhances pre-trained Vision Foundation Models (VFMs) by enabling them to process native resolution images and aligning visual features with the text embedding space of Large Language Models (LLMs). - It introduces C-ROPE, which combines learned absolute position embeddings with Rotary Position Embedding, allowing VFMs to handle arbitrary image sizes without resizing and preserving pre-trained knowledge. - It employs an Alignment Loss, a cross-entropy loss based on language prototypes, to align representations between VFMs and LLMs. - This three-stage continual pre-training method combines a vision-language adapter warm-up, native resolution adaptation, and optional instruction tuning. - Experimental results show that COMP-enhanced VFMs achieve superior performance not only in multimodal understanding tasks like ChartQA and DocVQA but also maintain competitive results in image classification and segmentation tasks like ImageNet-1K and ADE20K. | ['Multimodal', 'Image Classification', 'Image Segmentation', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/SliMM-X/CoMP-MM) | N/A |
| [Exploring Hallucination of Large Multimodal Models in Video
  Understanding: Benchmark, Analysis and Mitigation](https://arxiv.org/abs/2503.19622) | Yue Liu, Baolong Bi, Jingyi Tang, Jiashu Qu, Hongcheng Gao | - This paper introduces HAVEN, a benchmark for evaluating hallucinations in video understanding for Large Multimodal Models (LMMs), focusing on object, scene, and event hallucinations arising from prior knowledge conflicts, in-context conflicts, and inherent model limitations. - The benchmark includes 6,497 questions across various formats, evaluating 16 LMMs and revealing that Valley-Eagle-7B and GPT40-mini exhibit the lowest hallucination rates. - The study analyzes the impact of video duration, frame count, question length, and model size on hallucination, finding that performance initially improves with longer videos and more frames but degrades beyond a certain point, while longer questions consistently decrease performance and larger models tend to hallucinate less. - A novel training strategy combining Supervised Reasoning Fine-tuning (SRFT) and Thinking-based Direct Preference Optimization (TDPO) is proposed to mitigate hallucinations by enhancing reasoning and grounding the thinking process. - Applying this strategy to LLaVA-NEXT-Video-DPO-7B shows a 7.65% improvement in accuracy on HAVEN and a 4.5% reduction in bias score, demonstrating the effectiveness of the proposed approach in reducing hallucinations and enhancing response consistency. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Hongcheng-Gao/HAVEN) | N/A |
| [Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection
  with Artifact Explanation](https://arxiv.org/abs/2503.14905) | Zichen Wen, Hengrui Kang, Peilin Feng, Junyan Ye, Siwei Wen | - Introduces FakeVLM, a large multimodal model for synthetic image detection and artifact explanation, based on the LLaVA architecture and trained to provide natural language explanations for image artifacts. - Presents FakeClue, a dataset of over 100,000 real and synthetic images across seven categories with fine-grained artifact annotations generated using a multi-LMM strategy. - Achieves comparable performance to expert models in authenticity classification without requiring additional classifiers, demonstrated through extensive evaluations on multiple datasets including FakeClue and LOKI. - Outperforms existing general-purpose large models in both synthetic detection and artifact explanation tasks on FakeClue and LOKI, showing significant improvements in accuracy and F1 scores. - Provides more human-interpretable explanations for synthetic detection results compared to traditional methods that rely on probability thresholds, as showcased in qualitative examples across various image categories. | ['Multimodal', 'Computer Vision', 'Image Classification'] | [Link](https://github.com/opendatalab/FakeVLM) | N/A |
| [Scaling Vision Pre-Training to 4K Resolution](https://arxiv.org/abs/2503.19903) | Sifei Liu, Yao Lu, Han Cai, Boyi Li, Baifeng Shi | The paper introduces PS3, a new method for scaling CLIP-style vision pre-training to 4K resolution with near-constant cost.  Instead of contrastive learning on global image representations, PS3 selectively processes local regions and contrasts them with local detailed captions.  When integrated into a multimodal large language model (MLLM), the resulting model (VILA-HD) significantly improves high-resolution visual perception, outperforming previous MLLMs on multiple benchmarks and achieving better efficiency than existing token pruning approaches. The authors introduce a new benchmark, 4KPro, demonstrating VILA-HD's superior performance on image QA at 4K resolution.  PS3 unlocks appealing scaling properties, including free resolution scaling and test-time compute trading for better performance. | ['Multimodal', 'Visual Question Answering', 'Image Feature Extraction'] | [Link](https://nvlabs.github.io/PS3) | N/A |
| [Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time
  Thinking](https://arxiv.org/abs/2503.19855) | Yunjie Ji, Shuaiting Chen, Haotian Wang, Sitong Zhao, Xiaoyu Tian | - This paper introduces "Multi-round Thinking," a test-time scaling method for Large Language Models (LLMs) that enhances reasoning abilities through iterative answer refinement. - The method leverages previous answers as prompts for subsequent reasoning rounds, encouraging independent re-evaluation and error correction, mimicking human cognitive processes. - Experiments across multiple models and benchmarks (AIME 2024, MATH-500, GPQA-diamond, LiveCodeBench) demonstrate performance improvements with this approach. For example, QwQ-32B's accuracy on AIME 2024 increased from 80.3% to 82.1% and DeepSeek-R1 from 79.7% to 82.0% using just two rounds. - Analysis shows a correlation between improved performance and shorter response lengths with increasing rounds, suggesting enhanced conciseness and confidence in reasoning. - A preliminary exploration of combining Multi-round Thinking with supervised fine-tuning opens promising directions for future research into further enhancing LLM reasoning. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [CoLLM: A Large Language Model for Composed Image Retrieval](https://arxiv.org/abs/2503.19910) | Son Tran, Mubarak Shah, Ashish Tawari, Jinyu Yang, Chuong Huynh | This paper introduces CoLLM, a novel framework for composed image retrieval (CIR). CoLLM leverages large language models (LLMs) to generate joint embeddings of reference images and modification texts, enabling supervised training without manual annotation. It addresses limitations of existing methods by generating triplets on-the-fly from image-caption pairs and introducing a new large-scale dataset, MTCIR.  CoLLM achieves state-of-the-art performance on various CIR benchmarks, showing significant improvements of up to 15% compared to existing methods.  The refined benchmarks contribute towards more reliable evaluation metrics for future CIR research. | ['Multimodal'] | [Link](https://github.com/collm-cvpr25) | N/A |
| [MDocAgent: A Multi-Modal Multi-Agent Framework for Document
  Understanding](https://arxiv.org/abs/2503.13964) | Yun Li, Tong Sun, Ruiyi Zhang, Peng Xia, Siwei Han | - MDocAgent is a novel Retrieval Augmented Generation (RAG) framework employing a multi-agent system with specialized agents for document understanding. - It leverages both text and image modalities, using ColBERTv2 and ColPali for context retrieval, and employs five specialized agents: general, critical, text, image, and summarizing agents. - MDocAgent outperforms existing LVLMs and RAG-based methods on five benchmarks (MMLongBench, LongDocURL, PaperTab, PaperText, and FetaTab), achieving an average improvement of 12.1%. - Ablation studies validate the importance of each agent and the synergistic benefit of combining textual and visual modalities. - A case study highlights the framework's ability to accurately synthesize information from multiple modalities within a document, even with imperfect retrieval. | ['Document Question Answering', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/aiming-lab/MDocAgent) | N/A |
| [ReSearch: Learning to Reason with Search for LLMs via Reinforcement
  Learning](https://arxiv.org/abs/2503.19470) | Chenzheng Zhu, Yijie Zhou, Haoze Sun, Tianpeng Li, Mingyang Chen | - ReSearch, a novel framework, trains LLMs to reason with search via reinforcement learning, eliminating the need for supervised data on reasoning steps. - The framework integrates search operations as part of the reasoning chain, where text-based thinking guides search execution, and search results influence subsequent reasoning. - Trained on Qwen2.5 models, ReSearch demonstrates significant improvements (8.9% to 22.4%) over baselines on multi-hop question answering benchmarks. - The model exhibits strong generalizability, performing well on various benchmarks despite training on a single dataset (MuSiQue). - Analysis reveals ReSearch's ability to elicit advanced reasoning capabilities like reflection and self-correction during training. | ['Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Agent-RL/ReSearch) | N/A |
| [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041) | Mengshu Sun, Lin Yuan, Yujie Luo, Mengru Wang, Kangwei Liu | - LookAhead Tuning, a novel method for mitigating safety degradation in large language models (LLMs) during fine-tuning, is introduced, which involves modifying training data by previewing partial answer prefixes to preserve inherent safety mechanisms. - Two variants of LookAhead Tuning are presented: the Real Answer approach incorporates actual initial tokens for explicit guidance, while the Virtual Answer approach uses a generic prefix to avoid revealing answer content, both aiming to minimize perturbations to initial token distributions and maintain safety. - Experimental results on LLaMA2-7B-Chat, fine-tuned on GSM8K and SAMSum datasets and evaluated for safety with HEx-PHI, demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing downstream task performance compared to Vanilla Fine-tuning, SDFT, and Constrained SFT. - Further analysis confirms that fine-tuning safety correlates with reduced KL divergence of early tokens, validating the theoretical framework and showing that previewing more tokens increases safety at a potential cost to downstream performance, and virtual prefix variations demonstrate robustness. - LookAhead Tuning offers a simple, resource-efficient (only 1.65% and 2.56% more computational time for real and virtual methods, respectively), and effective solution for safe and effective LLM adaptation, applicable in practical deployments due to unchanged inference data and greedy decoding. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zjunlp/LookAheadTuning) | N/A |
| [When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only
  Training For Human-Centered Decision Making](https://arxiv.org/abs/2503.16965) | Yu Yin, Jing Li, Zhe Hu | - This paper introduces a novel text-only training approach for enhancing Visual Language Models (VLMs) in human-centered decision-making tasks. - The study reveals that Large Language Models (LLMs) often outperform their VLM counterparts in such scenarios, suggesting that visual processing may hinder decision-making capabilities. - By leveraging text-only training data generated by GPT-4, the proposed method strengthens the language components of VLMs, leading to significant performance gains. - Furthermore, the research demonstrates that VLMs can achieve self-improvement by using training data generated by smaller LLMs, offering a more efficient and scalable alternative to traditional training methods. - Experimental results on the VIVA benchmark show that text-only training improves the accuracy of various VLMs, with Qwen2-VL achieving the highest improvement from 80.32% to 83.15%. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Towards a Unified Copernicus Foundation Model for Earth Vision](https://arxiv.org/abs/2503.11849) | Thomas Dujardin, Adam J. Stewart, Chenying Liu, Zhitong Xiong, Yi Wang |  - This paper introduces Copernicus-FM, a unified foundation model for Earth vision, which integrates data from multiple Copernicus Sentinel missions, covering both surface and atmospheric features.  - The model architecture utilizes dynamic hypernetworks to handle various sensor modalities and metadata encoding, enhancing flexibility and scalability.  - A comprehensive benchmark, Copernicus-Bench, is presented, including 15 hierarchical tasks assessing performance across different Sentinel missions and data types.  - The authors demonstrate that their model outperforms existing methods on multiple downstream tasks, particularly in processing lower-resolution sensors and atmospheric data.  - The work also explores using grid embeddings derived from Copernicus-FM to connect EO and climate research, indicating promising results in climate prediction tasks. | ['Image Classification', 'Image Segmentation', 'Image Feature Extraction', 'Multimodal'] | [Link](https://github.com/zhu-xlab/Copernicus-FM) | N/A |


## Papers for 2025-03-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [I Have Covered All the Bases Here: Interpreting Reasoning Features in
  Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878) | Polina Druzhinina, Andrey Galichin, tlenusik, razzant, therem | - This paper introduces a methodology for identifying reasoning-specific features within Large Language Models (LLMs) using Sparse Autoencoders (SAEs). - The authors propose ReasonScore, a metric designed to pinpoint SAE features associated with reasoning by analyzing their activation patterns on a curated vocabulary of introspective terms. - Through empirical analysis, an automatic interpretability pipeline, and controlled feature steering experiments, they demonstrate a direct correlation between the identified features and the model's reasoning capabilities. - Notably, amplifying these features leads to enhanced reasoning performance on benchmarks like AIME 2024, MATH-500, and GPQA Diamond, providing mechanistic evidence for the role of specific LLM components in complex cognitive behaviors. - The results show that steering certain features systematically enhances structured reasoning in model outputs, offering an initial mechanistic explanation of reasoning in LLMs. | ['Natural Language Processing', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/AIRI-Institute/SAE-Reasoning) | N/A |
| [LEMMA: Learning from Errors for MatheMatical Advancement in LLMs](https://arxiv.org/abs/2503.17439) | mingchenlin2025, Word2Li, QizhiPei, LHL3341, panzs | - LEMMA, a novel method to enhance LLMs' reflective reasoning by constructing and learning from error-corrective trajectories, is proposed. - LEMMA systematically analyzes model-generated error types and introduces an error-type grounded mistake augmentation method to collect diverse and representative errors. - The model is fine-tuned on error-corrective trajectories, enabling it to self-correct errors within the generation process without relying on external critique models. - Experimental results on mathematical reasoning benchmarks show LEMMA achieves state-of-the-art performance, outperforming standard SFT baselines and prior error-aware methods. - LEMMA-trained models also exhibit strong generalization ability on out-of-distribution benchmarks and can consistently reduce the occurrence of representative error types. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/pzs19/LEMMA) | N/A |
| [Judge Anything: MLLM as a Judge Across Any Modality](https://arxiv.org/abs/2503.17489) | shuang72, Frywind, NiuniuWang, yuhangchen, fjchendp | - This paper introduces two benchmarks, TASKANYTHING and JUDGEANYTHING, for evaluating and assessing the capabilities of Multimodal Large Language Models (MLLMs) as judges across various modalities (image, text, audio, video) in understanding and generation tasks. - TASKANYTHING is a benchmark consisting of 1,500 open-ended queries across 15 any-to-any modality categories, accompanied by human annotations and model-generated responses. - JUDGEANYTHING evaluates MLLMs' judging ability across the same modalities using pair comparison and score evaluation settings against human judgments and detailed checklists. - Experiments with advanced MLLMs like GPT-40 and Gemini show promising results in assessing multimodal understanding tasks but significant challenges in assessing generation tasks, revealing cross-modality biases and hallucination issues. - The authors also present OMNIARENA, an automated evaluation platform for omni-models and multimodal reward models based on these benchmarks to further improve any-to-any multimodal models. | ['Any-to-Any', 'Multimodal'] | [Link](https://urrealhero.github.io/judgeanythingweb/) | N/A |
| [FFN Fusion: Rethinking Sequential Computation in Large Language Models](https://arxiv.org/abs/2503.18908) | geifmany, AmnonGeifman, omripuny, mdabbah-nvidia, abercovich | - This paper introduces FFN Fusion, a novel architectural optimization designed to reduce sequential computation in large language models (LLMs) by parallelizing Feed-Forward Network (FFN) layers. - The key insight is that consecutive FFN layers, especially those prevalent after attention pruning, can be fused into a single, wider layer, facilitating parallel execution and minimizing synchronization overhead. - This method is evaluated on Llama-3.1-405B-Instruct, resulting in a new model called Llama-Nemotron-Ultra-253B-Base, which achieves a 1.71x speedup and 35x lower per-token cost while maintaining comparable performance. - Experiments across various model sizes (49B to 253B parameters) demonstrate that FFN Fusion's effectiveness increases with scale and complements existing techniques like quantization and pruning. - The paper also presents preliminary findings suggesting that full transformer blocks (including both attention and FFN layers) can sometimes be parallelized, which opens new avenues for LLM architecture design. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Video SimpleQA: Towards Factuality Evaluation in Large Video Language
  Models](https://arxiv.org/abs/2503.18923) | Pengfei Hu, zhangysk, Drexubery, grejioh, mengcao | - This paper introduces Video SimpleQA, a novel benchmark designed to evaluate the factual grounding capabilities of Large Video Language Models (LVLMs). - Video SimpleQA consists of short, fact-seeking questions paired with definitive short-form answers and corresponding video clips, focusing on knowledge integration. - Unlike existing video benchmarks, Video SimpleQA necessitates external knowledge integration, promoting factual grounding rather than mere video content analysis.  - Experimental results on 41 state-of-the-art LVLMs reveal significant performance gaps, with top models reaching an F-score of only 54.4%, highlighting substantial room for improvement in video-grounded factual understanding. - The benchmark further explores test-time compute strategies and retrieval-augmented generation, revealing limitations and trade-offs in enhancing LVLMs factuality. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [AgentRxiv: Towards Collaborative Autonomous Research](https://arxiv.org/abs/2503.18102) | Samuel Schmidgall, mdmoor | - Introduces AgentRxiv, a framework for collaborative autonomous research among Large Language Model (LLM) agents, enabling them to iteratively build upon prior research findings via a shared preprint server. - Demonstrates that agents with access to past research through AgentRxiv achieve higher performance improvements (11.4% relative improvement over baseline on MATH-500) compared to isolated agents, and the best performing strategies generalize to other benchmark domains (average 3.3% improvement). - Shows that multiple agent laboratories using AgentRxiv collaborate effectively and achieve faster progress, reaching higher overall accuracy (13.7% relative improvement over baseline on MATH-500) compared to isolated labs. - Presents a parallelized mode for AgentRxiv, allowing concurrent research and faster discovery (+6.0% improvement on MATH-500 with 3 labs) but with a trade-off between speed and computational cost. - Discusses the discovered reasoning technique, Simultaneous Divergence Averaging (SDA), which yields the highest accuracy on MATH-500 (78.2%) and shows generalization across diverse benchmarks and language models. | ['Natural Language Processing'] | N/A | N/A |
| [Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models
  via Vision-Guided Reinforcement Learning](https://arxiv.org/abs/2503.18013) | Fan Yang, Hongyin Zhao, Shurong Zheng, Yousong Zhu, Yufei Zhan | - This paper introduces Vision-R1, a novel vision-guided reinforcement learning algorithm designed to improve the object localization capabilities of Large Vision-Language Models (LVLMs). - Vision-R1 uses a criterion-driven reward function based on visual feedback, eliminating the need for human-annotated preference data and specialized reward models. - It incorporates a progressive rule refinement strategy, dynamically adjusting reward criteria during training to ensure continuous model improvement and mitigate reward hacking. - Experiments on various object localization benchmarks, including in-domain and out-of-domain datasets, show that Vision-R1 significantly enhances performance, even surpassing the state-of-the-art Qwen2.5-VL-72B model on ODINW-13 by 2.5 mAP with a 10x smaller model size. - Furthermore, Vision-R1 maintains strong generalized question answering capabilities, unlike supervised fine-tuning which shows a decline in performance. | ['Multimodal', 'Object Detection', 'Reinforcement Learning'] | [Link](https://github.com/jefferyZhan/Griffon/tree/master/Vision-R1) | N/A |
| [Reasoning to Learn from Latent Thoughts](https://arxiv.org/abs/2503.18866) | Tatsunori Hashimoto, cmaddis, nband, ryoungj | - This paper proposes "reasoning to learn," a novel approach to improve data efficiency in language model (LM) pretraining by explicitly modeling and inferring latent thoughts underlying text generation. - This approach treats web text as the outcome of a compressed thought process and posits that latent thoughts have contextual knowledge important for data-efficient learning. - The authors demonstrate the effectiveness of their approach by continually pretraining a 1.1B TinyLlama model on a limited amount of data from FineMath, a reasoning-intensive web corpus, augmented with latent thoughts generated by GPT-40-mini. - Results show substantial improvements in downstream task performance compared to baselines trained on raw data and synthetic Chain-of-Thought paraphrases. - The introduced BoLT algorithm enables iterative improvement of the latent thought generator, showing gains for at least three iterations without task-specific data. | ['Natural Language Processing'] | [Link](https://github.com/ryoungj/BOLT) | N/A |
| [Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813) | Tianqi Fan, ftramer, carlini, iliashum, dedeswim | - This paper introduces CaMeL, a new defense against prompt injection attacks in Large Language Model (LLM) agents. - CaMeL creates a protective system layer around the LLM that extracts control and data flows from trusted queries, ensuring that retrieved untrusted data does not influence program flow. - The system utilizes a custom Python interpreter to track data provenance and enforce security policies based on capabilities (metadata) attached to individual data values, enabling granular control over data and control flows. - CaMeL's effectiveness is demonstrated in AgentDojo, where it successfully completes 67% of tasks with provable security guarantees, mitigating prompt injection risks and preventing unintended data exfiltration without modifying the underlying LLM. - This approach mirrors established software security practices and offers a more robust solution compared to relying solely on LLM training or prompting for security. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid
  Question Answering](https://arxiv.org/abs/2503.15879) | Yunho Maeng, Hyeonseo Nam, Ahjeong Park, keirahrlee, oneonlee | - Typed-RAG, a novel type-aware multi-aspect decomposition framework within the RAG paradigm, is introduced to enhance Non-Factoid Question Answering (NFQA). - Typed-RAG refines retrieval and generation strategies for different NFQ types by incorporating a pre-trained question type classifier and decomposing questions into single-aspect sub-queries. - It addresses the diverse intent and answer perspective limitations in current RAG systems by classifying NFQs into distinct types such as debate, experience, and comparison. - Experimental results on Wiki-NFQA, a new benchmark dataset for NFQA, show that Typed-RAG surpasses baseline models. - Typed-RAG demonstrates the significance of type-aware decomposition for effective retrieval and generation in NFQA by outperforming LLMs and standard RAG in capturing NFQ complexity. | ['Question Answering'] | [Link](https://github.com/TeamNLP/Typed-RAG) | N/A |
| [AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and
  Symbolic Reasoning](https://arxiv.org/abs/2503.18769) | Bui Quang Huy, Dinh Bach Vu, alandao | - AlphaSpace is a novel methodology designed to improve the spatial reasoning capabilities of Large Language Models (LLMs) for robotic manipulation in 3D space. - The methodology utilizes a hierarchical, semantics-based tokenization strategy that encodes both coarse and fine-grained spatial data, including height information represented by z-coordinates. - It incorporates symbolic reasoning and synthetic data to train a decoder-only LLM architecture to understand spatial relationships and perform actions like picking, placing, and stacking objects within a simulated tabletop environment. - On the EmbodiedBench Manipulation Subtask, AlphaSpace achieves 66.67% accuracy, surpassing GPT-40 (37.5%) and Claude 3.5 Sonnet (29.17%). - The enhanced spatial tokenization method expands upon previous 2D approaches by incorporating z-axis height information, leading to more effective 3D object manipulation. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural
  Contexts?](https://arxiv.org/abs/2503.18018) | Jaswinder Singh, Bhoomika Lohana, Aabid Karim, 55mv, Abdul084 | - This research explores the impact of cultural context on the mathematical reasoning abilities of Large Language Models (LLMs). - Six culturally adapted versions of the GSM8K dataset were created, maintaining the original mathematical logic while changing cultural elements like names and food. - Evaluation across 14 LLMs revealed a performance drop on culturally adapted problems compared to the original GSM8K, with smaller models showing larger drops. - Interestingly, models familiar with specific cultures sometimes outperformed larger, mathematically proficient models on culturally relevant problems. - This highlights the need for more diverse training data to improve LLM robustness in real-world applications. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/akarim23131/Lost_in_Cultural_Translation) | N/A |
| [V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V
  Platforms](https://arxiv.org/abs/2503.17422) | Luca Benini, Daniele Jahier Pagliari, Alessio Burrello, Mohamed Amine Ahmdi, Javier J. Poveda Rodrigo | - This paper presents V-Seek, a method for optimizing Large Language Model (LLM) inference on the Sophon SG2042, a RISC-V CPU with vector processing capabilities. - The authors develop optimized and quantized kernels for key LLM layers, targeting the specific hardware features of the SG2042, including vectorization and memory infrastructure. - The work explores different compiler options (GCC and Clang) and NUMA optimization strategies to maximize performance. - Results on DeepSeek R1 Distill Llama 8B/QWEN 14B and vanilla Llama 7B demonstrate speedups of up to 3.0x and 2.8x in token generation and prompt processing, respectively, compared to baseline llama.cpp. - The optimized system achieves throughputs up to 13.07/6.54/3.68 token/s for the evaluated LLMs, showing competitive performance against incumbent x86 architecture and improvements over previous work on the SG2042. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse](https://arxiv.org/abs/2503.18470) | Han Liu, zhenyupan | - MetaSpatial, a novel reinforcement learning (RL)-based framework, enhances 3D spatial reasoning in vision-language models (VLMs) for generating coherent and realistic 3D scenes. - It addresses the limitations of supervised fine-tuning by employing a multi-turn refinement strategy with a hybrid reward system combining format, physics, and rendering-based evaluations. - This approach uses Group Relative Policy Optimization (GRPO) to optimize the model across multiple refinement trajectories, promoting adaptable and generalizable spatial understanding. - Experimental results demonstrate that MetaSpatial significantly improves scene quality, increasing format accuracy, reducing physical violations, and enhancing perceptual realism as judged by GPT-4o. - The qualitative analysis shows more structured and realistic object placements after training, validating RL's effectiveness in 3D spatial reasoning for applications like AR/VR and metaverse design. | ['Text-to-3D', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/PzySeere/MetaSpatial) | N/A |
| [Verbal Process Supervision Elicits Better Coding Agents](https://arxiv.org/abs/2503.18494) | Jui-Ming Yao, Cheng-Pong Huang, MarkChenX | - This paper introduces CURA (Code Understanding and Reasoning Agent), a novel code generation framework enhanced with Verbal Process Supervision (VPS). - VPS guides language models to generate step-level reward signals for improved code generation outcomes.  - CURA with VPS improves performance by 3.65% over baseline models on BigCodeBench.  - Paired with o3-mini and VPS, CURA achieves state-of-the-art performance, indicating the effectiveness of iterative verbal process supervision in enhancing agentic reasoning.  - Deterministic decoding (temperature 0) leads to better results in code generation compared to stochastic decoding (temperature 1), as shown by Mistral Large Latest and GPT-40-mini on BigCodeBench. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |


## Papers for 2025-03-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MAPS: A Multi-Agent Framework Based on Big Seven Personality and
  Socratic Guidance for Multimodal Scientific Problem Solving](https://arxiv.org/abs/2503.16905) | Xinyu Zhang, Zhangqi Wang, Zhiyuan Wang, Qika, VentureZJ | - This paper introduces MAPS, a Multi-Agent framework based on the Big Seven Personality theory and Socratic questioning, for solving Multimodal Scientific Problems (MSPs). - The framework employs seven distinct agents with specific skills to perform problem-solving in a cooperative manner. - A progressive four-agent solving strategy is proposed where each agent focuses on a specific stage of the problem-solving process (Interpreter, Aligner, Scholar, and Solver), and a Critic agent provides feedback to refine reasoning, simulating human reflection. - Experiments conducted on EMMA, Olympiad, and MathVista datasets demonstrates that MAPS outperforms current state-of-the-art models by 15.84% across all tasks and even surpasses human expert performance by 3.58%. | ['Multimodal', 'Question Answering'] | [Link](https://github.com/exoskeletonzj/MAPS) | N/A |
| [MARS: A Multi-Agent Framework Incorporating Socratic Guidance for
  Automated Prompt Optimization](https://arxiv.org/abs/2503.16874) | Jun Liu, Haiping Zhu, Zhangqi Wang, Qika, VentureZJ | - This paper introduces MARS (Multi-Agent framework Incorporating Socratic guidance), a novel multi-agent framework designed for Automated Prompt Optimization (APO). - MARS utilizes a seven-agent architecture, including a Planner to devise flexible optimization paths and a Teacher-Critic-Student module for iterative prompt refinement through Socratic dialogue. - MARS addresses the limitations of existing APO methods, such as the inflexibility of fixed templates and inefficient search in prompt spaces. - Experimental results on 12 general tasks and 5 domain-specific datasets show that MARS surpasses previous state-of-the-art methods and significantly improves performance compared to original prompts and Chain-of-Thought (CoT) prompts. - An efficiency analysis demonstrates MARS effectively balances resource consumption and performance improvement. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [RoboFactory: Exploring Embodied Agent Collaboration with Compositional
  Constraints](https://arxiv.org/abs/2503.16408) | Xiaohong Liu, Zhenfei Yin, Xiufeng Song, FACEONG, IranQin | - RoboFactory, a framework for automated data collection in multi-agent embodied systems, is introduced, focusing on generating safe and efficient training data. - The framework uses compositional constraints (logical, spatial, and temporal) to govern agent behavior and ensure collaboration effectiveness. - RoboBrain, a large language model (e.g., GPT-40), generates subgoals and constraints based on task instructions and feedback, while RoboChecker utilizes constraint interfaces to validate trajectories and prevent violations. - A new benchmark for embodied multi-agent manipulation, also named RoboFactory, featuring 11 tasks with varying agent numbers and environment settings within the ManiSkill simulator, is proposed. - Evaluation using Diffusion Policy on this benchmark showed promising results, highlighting the importance of sufficient data and the framework's ability to generate high-quality datasets, but also revealed performance degradation with increasing agent numbers and limitations in long-term temporal dependency learning. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://iranqin.github.io/robofactory/) | N/A |
| [When Less is Enough: Adaptive Token Reduction for Efficient Image
  Representation](https://arxiv.org/abs/2503.16660) | Andrey Kuznetsov, Elizaveta Goncharova, Eduard Allakhverdov | - This paper introduces a novel method for reducing the number of visual tokens generated by vision encoders in multimodal models, aiming to improve efficiency without compromising performance. - The method employs an autoencoder with a Gumbel-Softmax selection mechanism to identify and retain the most informative visual tokens, allowing for the reconstruction of less valuable features from more valuable ones. - Experiments on LLaVA-NeXT and LLaVA-OneVision models show that up to 50% of the visual context can be discarded with minimal performance loss on OCR-based tasks using the proposed selection method, outperforming random selection. - In general domain tasks, the performance is less affected by the visual token reduction, where retaining as little as 30% of the original tokens often yields performance on par with utilizing the full set.  - This adaptive token reduction approach provides a promising direction for more scalable and efficient multimodal inference without significant performance degradation. | ['Multimodal', 'Image Feature Extraction'] | N/A | N/A |
| [OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning
  via Iterative Self-Improvement](https://arxiv.org/abs/2503.17352) | Wei Wang, Nanyun Peng, Fan Yin, Hritik Bansal, Yihe Deng | - OpenVLThinker-7B, a large vision-language model (LVLM), is introduced, trained using iterative self-improvement involving supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance complex reasoning abilities. - The model leverages a warm-start approach where SFT establishes initial reasoning structure and RL drives performance gains and generalization, utilizing distilled reasoning from text-based models in a vision-language context. - OpenVLThinker iteratively refines through RL, specifically Group Relative Policy Optimization (GRPO), using the improved model to generate refined SFT datasets for subsequent iterations. - Evaluation on visual reasoning benchmarks, including MathVista, MathVerse, and MathVision, demonstrates consistent improvement, surpassing or matching the performance of existing models like GPT-4 and Qwen2.5-VL-7B. - The work suggests the potential of combining SFT and RL for complex reasoning in LVLMs and provides early evidence for integrating R1-style reasoning into multimodal contexts. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](https://github.com/yihedeng9/OpenVLThinker) | N/A |
| [Modifying Large Language Model Post-Training for Diverse Creative
  Writing](https://arxiv.org/abs/2503.17126) | Max Kreminski, Yuqian Sun, Melissa Roemmele, Vishakh Padmakumar, John Joon Young Chung | - This paper introduces diversified versions of Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO) for training large language models (LLMs) to generate diverse and high-quality creative writing. - The core idea is to incorporate deviation, a measure of how different a training instance is from others with the same prompt, into the training objective. - This approach encourages the model to learn from rarer, high-quality examples and promotes diversity in generated text. - Experiments show that the diversified methods achieve comparable quality to state-of-the-art models while significantly improving diversity, even surpassing human-generated text in some metrics. - The approach is robust to variations in dataset size and outperforms existing diversification methods like DivPO. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/mj-storytelling/DiversityTuning) | [Link](https://huggingface.co/datasets/euclaise/WritingPrompts_preferences) |
| [MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical
  Problems](https://arxiv.org/abs/2503.16549) | Jun Cen, Tao Feng, Yunqiu Xu, Felix Chen, JacobYuan | - This paper introduces MathFlow, a modular problem-solving pipeline designed to improve Multimodal Large Language Models (MLLMs) performance on visual mathematical problems by decoupling perception and inference stages. - A novel benchmark called FlowVerse is proposed which categorizes information into four components: Descriptive, Essential, Reasoned Property, and Question.   - MathFlow-P-7B, a dedicated perception model trained using a two-stage approach is developed to extract key information from diagrams. - Results on FlowVerse and MathVerse show that MathFlow-P-7B significantly improves performance when integrated with various inference models, achieving state-of-the-art results with some combinations.  - The results highlight the importance of strong perception capabilities in visual mathematical problem-solving and the effectiveness of the modular design. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/MathFlow-zju/MathFlow) | N/A |
| [ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question
  Generation and Answering](https://arxiv.org/abs/2503.16867) | Wei Liu, Peng Zhang, Yuchong Sun, Zhengfeng Lai, Guan123 | - This paper introduces ETVA, a novel evaluation method for assessing text-to-video alignment, addressing limitations of existing metrics by employing fine-grained question generation and answering. - ETVA uses a multi-agent system to parse prompts into scene graphs for generating atomic questions and a knowledge-augmented multi-stage reasoning framework with auxiliary LLMs and video LLMs for question answering.  - A comprehensive ETVA-Bench benchmark featuring 2k diverse prompts and 12k atomic questions across 10 categories is also presented. - Evaluation results on ETVA-Bench demonstrate that ETVA achieves significantly higher correlation with human judgment (Spearman's correlation of 58.47) compared to existing metrics like VideoScore which achieves 31.0. - Systematic evaluation of 15 T2V models reveals their limitations, especially in handling temporal dynamics and physics, highlighting areas for improvement in next-generation text-to-video models. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [FastCuRL: Curriculum Reinforcement Learning with Progressive Context
  Extension for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287) | Xuan Luo, Wenjie Yang, Zheng Li, Mao Zheng, Mingyang Song | - FASTCURL is a Curriculum Reinforcement Learning approach with a context window extension strategy designed for efficient training of R1-like reasoning models. - It involves length-aware training data segmentation and context window extension training. - The former splits training data by input prompt length into three levels, and the latter uses these segmented datasets with progressively increasing context window length to train the model. - FASTCURL-1.5B-Preview outperforms DeepScaleR-1.5B-Preview on five datasets (MATH 500, AIME 2024, AMC 2023, Minerva Math, and OlympiadBench) using only 50% of the training steps. - All training for FASTCURL-1.5B-Preview is completed on a single 8-GPU node. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/nick7nlp/FastCuRL) | [Link](https://huggingface.co/datasets/Nickyang/FastCuRL), [Link](https://huggingface.co/Nickyang/FastCuRL-1.5B-Preview) |
| [PVChat: Personalized Video Chat with One-Shot Learning](https://arxiv.org/abs/2503.17069) | Yuchen Li, Yumeng Li, Gang Xu, Weilong Yan, Master-Shi | - PVChat is a personalized Video Large Language Model (ViLLM) that enables subject-aware question answering from a single video using a one-shot learning approach. - It utilizes a Mixture-of-Heads (MoH) enhanced ViLLM architecture with ReLU Routing and two novel training objectives: Smooth Proximity Regularization and Head Activation Enhancement, to facilitate personalized learning. - A systematic data augmentation pipeline generates identity-preserving positive and hard negative samples, along with question-answer pairs across existence, appearance, action, and location categories. - A two-stage training strategy, transitioning from image pre-training to video fine-tuning, progressively develops the model's capacity from static attributes to dynamic representations. - Experimental results on diverse video datasets demonstrate that PVChat achieves state-of-the-art performance in personalized video understanding and question answering. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [From Head to Tail: Towards Balanced Representation in Large
  Vision-Language Models through Adaptive Data Calibration](https://arxiv.org/abs/2503.12821) | Yu Cheng, Jiawei Zhou, Xiaoye Qu, hitsmy | - This paper introduces the Adaptive Data Refinement (ADR) framework, a method designed to improve Large Vision-Language Models (LVLMs) by addressing the long-tail problem in training data, where certain concepts are overrepresented while others are underrepresented. - ADR consists of two stages: Data Rebalancing (DR) and Data Synthesis (DS).  The DR stage filters redundant data based on entity distributions to mitigate overfitting, while the DS stage utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic data for underrepresented concepts. - Evaluation across eleven benchmarks shows ADR improves the average performance of LLaVA 1.5 by 4.36% relative to the baseline without increasing the training data volume.  The improvement is observed not only in overall performance but also specifically on tail data, indicating effective mitigation of the long-tail problem. - The framework is model-agnostic and data-agnostic, allowing for easy integration with existing LVLMs and datasets. - Analysis of the long-tail problem identifies four key perspectives contributing to the imbalance: tokens, objects, co-occurrences, and interrogations. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Implicit Bias-Like Patterns in Reasoning Models](https://arxiv.org/abs/2503.11572) | Calvin K. Lai, l048596 | - This paper introduces the Reasoning Model Implicit Association Test (RM-IAT), a method for studying implicit bias-like patterns in AI reasoning models, focusing on the model's processing rather than just its outputs. - The RM-IAT adapts the human Implicit Association Test (IAT) by measuring the number of reasoning tokens used by LLMs like OpenAI's 03-mini when categorizing words related to social groups and attributes in association-compatible and -incompatible pairings. - Results reveal that 03-mini requires significantly more tokens for association-incompatible pairings in 9 out of 10 RM-IATs, indicating greater computational effort and mirroring human implicit bias in processing efficiency. - This bias in computational effort has potential implications for real-world applications, as it suggests that reasoning models may unintentionally perpetuate social stereotypes due to their reliance on pre-existing patterns in data. - Further research on how RLHF and other alignment techniques interact with these implicit-like processing patterns in reasoning models is necessary to address the concerns raised. | ['Natural Language Processing'] | N/A | N/A |
| [Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language
  Model](https://arxiv.org/abs/2503.16282) | Junlin Han, Runjia Li, Yun Liu, Guolei Sun, Zhaochong An | - This paper introduces GFS-VL, a generalized few-shot 3D point cloud segmentation framework that leverages the open-world knowledge of 3D Vision-Language Models (3D VLMs) to enhance the performance of few-shot learning. - GFS-VL addresses the noisy nature of 3D VLM predictions by using a prototype-guided pseudo-label selection method, filtering noisy regions and adaptively infilling unlabeled areas using contextual information and few-shot samples. - To further enhance learning, a novel-base mix strategy is employed, integrating support samples into training scenes while preserving context. - The authors introduce two new benchmarks with increased diversity and quantity of novel classes for a comprehensive evaluation. - Experiments demonstrate state-of-the-art performance across existing and new benchmarks, validating the efficacy of GFS-VL in generalizing to diverse novel classes. | ['Image Segmentation', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/ZhaochongAn/GFS-VL) | N/A |


## Papers for 2025-03-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Stop Overthinking: A Survey on Efficient Reasoning for Large Language
  Models](https://arxiv.org/abs/2503.16419) | andrewwen, HongyiLiuAI, jy-yuan, JiamuZhang, yangsui | - This paper surveys efficient reasoning methods for Large Language Models (LLMs), categorizing them into model-based, reasoning output-based, and input prompts-based approaches. - Model-based methods optimize or train LLMs for conciseness, while output-based methods dynamically reduce reasoning steps during inference, and prompt-based methods leverage prompt properties for efficiency. - The paper also discusses efficient data utilization, reasoning in smaller models, and evaluation methods. - It introduces Sys2Bench for evaluating LLMs across various reasoning tasks and frameworks to assess "overthinking."  - The survey highlights the practical benefits of efficient reasoning across diverse domains like healthcare, autonomous driving, and embodied AI, emphasizing the importance of balancing reasoning quality with computational efficiency. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs) | N/A |
| [Survey on Evaluation of LLM-based Agents](https://arxiv.org/abs/2503.16416) | Yilun Zhao, Guy Uziel, Lilach Eden, lihaoxin2020, Asaf-Yehudai | - This paper presents the first comprehensive survey of evaluation methodologies for Large Language Model (LLM)-based agents. - The survey analyzes evaluation benchmarks and frameworks across four dimensions: fundamental agent capabilities, application-specific benchmarks, generalist agent benchmarks, and agent evaluation frameworks. - The paper identifies emerging trends in agent evaluation, including a shift towards more realistic and challenging evaluations using continuously updated benchmarks. - It also highlights critical gaps in current evaluation methods, particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained and scalable methods. - The survey aims to provide a comprehensive overview of the current state of agent evaluation and guide future research by suggesting several promising directions. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/spaces/galileo-ai/agent-leaderboard) |
| [Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning](https://arxiv.org/abs/2503.15558) | Hannah Brandon, Alisson Azzolini, NVIDIA, zhuoliny, fferroni | - NVIDIA introduces Cosmos-Reason1, a multimodal large language model family (8B and 56B parameter versions) specializing in physical reasoning, trained to perceive, understand, and generate embodied decisions based on video input using a long chain-of-thought process. - The model architecture employs a decoder-only multimodal approach, where video input is processed by a vision encoder and aligned with text embeddings before being fed into a hybrid Mamba-MLP-Transformer LLM. - Training occurs in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL), using curated datasets focusing on physical common sense and embodied reasoning, including specialized intuitive physics datasets.  - Evaluation on new benchmarks tailored for physical common sense and embodied reasoning shows significant performance improvements over existing models, especially after Physical AI SFT and RL, with the 56B model outperforming OpenAI 01 on the common sense benchmark. - The authors plan to open-source the code and open-weight the models. | ['Video-Text-to-Text', 'Multimodal', 'Reinforcement Learning', 'Robotics', 'Visual Question Answering'] | [Link](https://github.com/nvidia-cosmos/cosmos-reason1) | N/A |
| [MathFusion: Enhancing Mathematic Problem-solving of LLM through
  Instruction Fusion](https://arxiv.org/abs/2503.16212) | Honglin Lin, Yu Li, Zhuoshi Pan, Lijun Wu, Qizhi Pei | - MathFusion, a novel framework, enhances mathematical reasoning in Large Language Models (LLMs) by fusing different mathematical problems through instruction synthesis, focusing on leveraging the relationships between problems rather than simply modifying individual instances. - Three fusion strategies are introduced: sequential fusion (chaining related problems), parallel fusion (combining analogous problems), and conditional fusion (creating context-aware selective problems). - MathFusionQA, a new dataset, is created by applying these strategies and fine-tuning various LLMs (DeepSeekMath-7B, Mistral-7B, Llama3-8B). - Experimental results demonstrate substantial performance gains, boosting accuracy by 18.0 points across benchmarks using only 45K additional synthetic instructions compared to traditional single-instruction approaches. - MathFusion's high data efficiency is further highlighted by its superior performance when combined with the state-of-the-art DART-Math, exceeding its accuracy by 1.4 points with less data. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/QizhiPei/mathfusion) | N/A |
| [Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language
  Models](https://arxiv.org/abs/2503.16257) | Huan Wang, Can Qin, Yang Sui, Haoxuan You, KD-TAO | - VidKV, a plug-and-play quantization method, compresses the key-value (KV) cache in Video Large Language Models (VideoLLMs) to lower than 2-bit precision without fine-tuning. - It employs a mixed-precision quantization strategy for the key cache, using 2-bit quantization for anomalous channels and 1-bit quantization combined with Fast Fourier Transform (FFT) for normal channels. - For the value cache, VidKV implements 1.58-bit quantization with an optional semantic token protection mechanism to preserve critical visual tokens at 2-bit precision. - Extensive experiments on six video benchmarks with LLaVA-OV-7B and Qwen2.5-VL-7B demonstrate that VidKV compresses KV cache to 1.5-bit and 1.58-bit with negligible performance drop compared to FP16. - The results show that per-channel quantization is more effective for the value cache in VideoLLMs, contrary to previous findings in LLMs that favor per-token quantization. | ['Multimodal', 'Video-Text-to-Text'] | [Link](https://github.com/KD-TAO/VidKV) | N/A |
| [JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play
  Visual Games with Keyboards and Mouse](https://arxiv.org/abs/2503.16365) | Yitao Liang, Xiaojian Ma, Kaichen He, Zihao Wang, Muyao Li | - Introduces JARVIS-VLA, a Vision-Language-Action (VLA) model trained with a novel Act from Visual Language Post-Training (ActVLP) paradigm.  - JARVIS-VLA is a non-Markovian model that uses a history of observations in its prompts, and employs an action decoder to output discrete and continuous actions. - ActVLP enhances VLMs with visual and linguistic guidance in a self-supervised manner before imitation learning, significantly improving performance on a variety of tasks. - Achieves state-of-the-art performance in Minecraft, surpassing traditional imitation learning-based policies and showing a 40% improvement over baseline agents on atomic tasks like crafting, smelting, and mining. - Open-sourced code, models, and datasets to facilitate further research.  | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners](https://arxiv.org/abs/2503.16356) | Shumin Deng, Jia-Chen Gu, Jizhan Fang, Yunzhi Yao, Ningyu | - CaKE (Circuit-aware Knowledge Editing) is introduced, a novel method designed to improve the generalization of knowledge edits in Large Language Models (LLMs) for multi-hop reasoning tasks. - CaKE leverages circuit-aware training data, which explicitly requires the LLM to reason with the updated knowledge, guiding the model to construct robust reasoning circuits. - This approach addresses the limitations of existing KE methods that often struggle to integrate updates into the multi-hop reasoning process. - Experimental results on the MQUAKE dataset demonstrate that CaKE significantly improves multi-hop reasoning accuracy, achieving an average of 20% improvement compared to existing KE methods. - CaKE's effectiveness is shown across various LLMs, including LLAMA3-8B-Instruct, Qwen2.5-7B-Instruct, and LLAMA3-70B-Instruct, showcasing its adaptability to different model sizes. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zjunlp/CaKE) | N/A |
| [Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/abs/2503.13657) | Bhavya Chopra, Lakshya A. Agrawal, Shuyi Yang, Melissa Z. Pan, Mert Cemri | - This paper presents MASFT, the first comprehensive taxonomy of failure modes in Multi-Agent Large Language Model (LLM) systems. - The study analyzes five popular MAS frameworks across 150 tasks, using human annotations to identify 14 distinct failure modes categorized into specification and system design, inter-agent misalignment, and task verification. - An LLM-as-a-judge pipeline is introduced for scalable automated failure mode detection and achieves 94% accuracy and 0.77 Cohen's Kappa score against human annotations. - Interventions, including improved role specifications and enhanced orchestration, show a 14% improvement in one framework but don't resolve all failures, highlighting the need for more complex solutions. - The research emphasizes the importance of organizational understanding in MAS design and releases the dataset and LLM annotator as open-source resources. | ['Natural Language Processing'] | [Link](https://github.com/multi-agent-systems-failure-taxonomy/MASFT) | N/A |
| [M3: 3D-Spatial MultiModal Memory](https://arxiv.org/abs/2503.16413) | Jianglong Ye, Xuanbin Peng, Ri-Zhao Qiu, Yuchen Song, Xueyan Zou | - M3, a 3D spatial multimodal memory system, is introduced, integrating 3D Gaussian Splatting with foundation models to store multimodal memories efficiently. - It addresses two key limitations of previous feature splatting approaches: the computational constraints of storing high-dimensional features and misalignment or information loss between distilled and original features. - M3 introduces principal scene components and Gaussian memory attention, enabling efficient processing and high fidelity rendering. - Comprehensive quantitative evaluations of feature similarity, downstream tasks, and qualitative visualizations confirm M3's superiority in memorization and downstream tasks. - M3 has been successfully deployed on a real-world quadruped robot for grasping tasks, showcasing its potential for real-world applications. | ['Multimodal', 'Computer Vision', 'Image Feature Extraction'] | [Link](https://github.com/MaureenZOU/m3-spatial) | N/A |
| [XAttention: Block Sparse Attention with Antidiagonal Scoring](https://arxiv.org/abs/2503.16428) | Song Han, Junxian Guo, Guangxuan Xiao, Ruyi Xu, songhan | - XAttention is a plug-and-play framework that accelerates long-context inference in Transformer models using sparse attention by summing antidiagonal values in the attention matrix to determine block importance. - Unlike existing methods relying on computationally intensive solutions, XAttention uses this simple, efficient scoring method to identify and prune non-essential blocks, leading to high sparsity and accelerated inference without sacrificing accuracy. - Across benchmarks like RULER, LongBench, VideoMME, and VBench, XAttention achieves accuracy comparable to full attention while significantly reducing computational costs, demonstrating up to 13.5x acceleration in attention computation. - This method effectively balances accuracy and efficiency and unlocks the practical potential of block sparse attention. - XAttention paves the way for scalable and efficient deployment of Long-Context Transformer Models (LCTMs) in real-world applications. | ['Natural Language Processing', 'Video Classification', 'Video-Text-to-Text', 'Multimodal', 'Question Answering', 'Text Generation', 'Text-to-Video'] | [Link](https://github.com/mit-han-lab/x-attention) | N/A |
| [CLS-RL: Image Classification with Rule-Based Reinforcement Learning](https://arxiv.org/abs/2503.16188) | Kaipeng Zhang, Jike Zhong, Ming Li, yuxianglai117, stzhao | - CLS-RL, a novel rule-based reinforcement learning framework, is proposed for few-shot image classification fine-tuning of Multimodal Large Language Models (MLLMs), addressing the catastrophic forgetting issues encountered in supervised fine-tuning. - CLS-RL utilizes verifiable signals (class names) as rewards and formats rewards to encourage models to think before answering, demonstrating superior performance compared to supervised fine-tuning in most datasets across both base-to-new generalization and few-shot learning settings. - A "free-lunch" phenomenon is observed with CLS-RL, where fine-tuning on one dataset enhances performance on other distinct datasets, suggesting that RL effectively teaches models the fundamentals of image classification. - No-Thinking-CLS-RL, a variant minimizing the thinking process during training through an equality accuracy reward, achieves better in-domain performance and generalization capabilities than CLS-RL with less fine-tuning time. - The paper explores the role of the thinking process during RL fine-tuning for visual classification, finding it potentially less critical than previously assumed, and suggests that for simple visual tasks extensive thinking might be detrimental. | ['Image Classification', 'Reinforcement Learning', 'Multimodal', 'Zero-Shot Image Classification'] | N/A | N/A |
| [Fin-R1: A Large Language Model for Financial Reasoning through
  Reinforcement Learning](https://arxiv.org/abs/2503.16252) | Jinyi Niu, Lingfeng Zeng, Fangqi Lou, Xin Guo, Zhaowei Liu | - This paper introduces Fin-R1, a 7-billion parameter large language model designed for financial reasoning, addressing the challenges of fragmented data, opaque reasoning, and weak generalization in financial AI. - Fin-R1 leverages a two-stage training process: Supervised Fine-Tuning (SFT) on a new dataset, Fin-R1-Data, followed by Reinforcement Learning (RL) using Group Relative Policy Optimization (GRPO). - Fin-R1-Data consists of 60,091 Chain-of-Thought (CoT) examples derived and filtered from a combination of public and proprietary financial datasets. - In evaluations, Fin-R1 achieves state-of-the-art results on ConvFinQA (85.0) and FinQA (76.0), and strong performance across other financial benchmarks, outperforming larger models like DeepSeek-R1-Distill-Llama-70B. - The model shows promising real-world applications in financial compliance and robo-advisory. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/SUFE-AIFLM-Lab/Fin-R1) | N/A |
| [Make Your Training Flexible: Towards Deployment-Efficient Video Models](https://arxiv.org/abs/2503.14237) | Yi Wang, Xiangyu Zeng, Tianxiang Jiang, Kunchang Li, Chenting Wang | - This paper introduces Flux, a new data augmentation tool and training approach for video models that optimizes performance across various computational budgets and spatiotemporal resolutions. - Flux employs flexible sampling and token selection to maximize input information, enabling training of flexible models. - FluxViT, trained using Flux, achieves state-of-the-art results on standard video classification benchmarks like K400 and Something-Something V2. - Notably, FluxViT matches or exceeds prior state-of-the-art performance while using significantly fewer tokens (1/4 to 1/2), leading to substantial computational savings (70% to 95%). - It also shows robust performance on multi-modal tasks, especially in chat-centric applications including video captioning and question answering. | ['Computer Vision', 'Video Classification', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://github.com/OpenGVLab/FluxViT) | N/A |
| [Reinforcement Learning for Reasoning in Small LLMs: What Works and What
  Doesn't](https://arxiv.org/abs/2503.16219) | Chris Ngo, quyanh | - This paper explores Reinforcement Learning (RL) for improving reasoning in small Large Language Models (LLMs), specifically a 1.5B parameter model (DeepSeek-R1-Distill-Qwen-1.5B), under limited resources (4 NVIDIA A40 GPUs for 24 hours). - The study uses a curated dataset of mathematical problems and adapts the Group Relative Policy Optimization (GRPO) algorithm for resource efficiency. - Results show significant improvement in reasoning performance with minimal resources: accuracy on AMC23 rose from 63% to 80%, and AIME24 reached 46.7%, outperforming larger models like o1-preview. - Key challenges include optimization instability, length constraints, and language drift with prolonged training, suggesting a need for more robust training strategies and potentially longer sequence lengths. - Code and data are released to foster further research into computationally efficient enhancement of small LLMs, promoting wider accessibility to advanced reasoning capabilities. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/knoveleng/open-rs) | [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-aime), [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-amc) |
| [Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging
  Fabricated Claims with Humorous Content](https://arxiv.org/abs/2503.16031) | Sunil Saumya, Shankar Biradar, UVSKKR | - This paper introduces the Deceptive Humor Dataset (DHD), a synthetic multilingual benchmark dataset designed for studying humor derived from fabricated claims and misinformation. - DHD consists of humor-infused comments generated from false narratives using ChatGPT-40, labeled with satire levels and categorized into distinct humor types. - The dataset spans multiple languages, including English and several Indian languages, along with their code-mixed variations, addressing the gap in regional language resources for humor detection. - The authors established baseline results using various transformer-based models and large language models for both satire level and humor attribute classification tasks, providing a benchmark for future research. - Results suggest that while existing models perform well on standard humor detection or misinformation classification individually, they struggle with the combined challenge of deceptive humor, highlighting the need for new research in this direction. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space
  Complexity?](https://arxiv.org/abs/2503.15242) | Gabriel Synnaeve, Benoit Sagot, Baptiste Roziere, pierrechambon | - This paper introduces BigO(Bench), a novel coding benchmark designed to evaluate the ability of Large Language Models (LLMs) to generate code that adheres to specific time and space complexity constraints. - The benchmark includes tooling to infer the time and space complexity of Python code, a dataset of 3,105 coding problems with inferred complexity labels and 1.2M solutions, and an evaluation framework. - Results from evaluating several state-of-the-art LLMs reveal that, despite their impressive performance in generating functional code, most struggle to meet specified time and space complexity constraints, hinting that they may not generalize well to tasks without specific reward at training. - Token-space reasoning models, while superior in code generation tasks, show limitations in complexity understanding. - DeepSeek R1 Llama 70B achieves the highest scores on most aspects of the benchmark (41.4% and 4.8% All@1 on complexity prediction and generation respectively), with Llama 3.1 405B excelling at space complexity prediction (10.3%). | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/facebookresearch/bigobench) | [Link](https://huggingface.co/datasets/facebook/BigOBench) |
| [See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language
  Balance to Mitigate Dominant Modality Bias](https://arxiv.org/abs/2503.13834) | YoungBin Kim, Juhwan Choi, Eunju Lee, MiHyeon Kim, JuneHyoung Kwon | - This paper introduces BALGRAD, a novel framework designed to mitigate dominant modality bias in vision-language (VL) models by balancing gradients between modalities and ensuring stable convergence and balanced learning. - BALGRAD employs two key components: inter-modality gradient reweighting, which adjusts the gradient magnitude of the KL divergence term based on each modality's contribution, and inter-task gradient projection, which prevents conflicts between gradients that can hinder balanced learning.   - The authors theoretically demonstrate that unbalanced loss reduction can result from conflicting directions or significantly different magnitudes of the gradients between modalities.   - Experimental results on UPMC Food-101, Hateful Memes, and MM-IMDb datasets show that BALGRAD effectively reduces the performance gap between modalities under different impaired conditions (missing/noisy data) and avoids negative transfer.  - BALGRAD achieves superior performance by balancing modality contributions and improving robustness, as demonstrated by achieving the highest Avg. performance and the smallest gap in experiments with impaired modalities across the datasets. | ['Multimodal'] | N/A | N/A |


## Papers for 2025-03-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time
  Exploration and Exploitation](https://arxiv.org/abs/2503.13288) | Qika, haitengzhao, changma, Meituannnnnn, xufangzhi | - Introduces φ-Decoding, a novel inference-time optimization algorithm utilizing adaptive foresight sampling for enhanced exploration and exploitation during LLM inference. - Employs simulated future steps to estimate step values, balancing exploration and exploitation to find globally optimal reasoning paths. - Proposes in-width and in-depth pruning strategies for adaptive computation allocation, enhancing efficiency by prioritizing challenging steps. - Achieves >14% average performance improvement on LLaMA3.1-8B-Instruct over auto-regressive CoT across seven reasoning benchmarks. - Demonstrates consistent superiority across various LLMs (3B, 7B, 8B, and 70B) and computational budgets, matching suboptimal baseline performance with 6x efficiency. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/xufangzhi/phi-Decoding) | N/A |
| [TULIP: Towards Unified Language-Image Pretraining](https://arxiv.org/abs/2503.15485) | XuDong Wang, Seun Eisape, Long Lian, yala, ZinengTang | - TULIP, a novel image-text contrastive learning model, enhances fine-grained visual feature learning while preserving language grounding capabilities. - It incorporates patch-level global/local multi-crop augmentations, objectives, and a reconstruction objective to preserve high-frequency visual details and spatial information. - It also employs generative data augmentation with diffusion models to create challenging negative examples for better semantic grounding. - TULIP achieves state-of-the-art zero-shot performance on ImageNet-1K and significant improvements on other benchmarks, including RxRx1, and MMVP. - Its ability to encode both fine-grained visual and robust textual features makes it beneficial as a visual encoder for large-scale multimodal models like LLaVA. | ['Multimodal', 'Computer Vision', 'Image Classification', 'Zero-Shot Image Classification', 'Image Feature Extraction'] | [Link](https://tulip-berkeley.github.io) | N/A |
| [Cube: A Roblox View of 3D Intelligence](https://arxiv.org/abs/2503.15475) | Karun Channa, Nishchaie Khanna, Kiran Bhat, Foundation AI Team, marcelvanworkum | - Roblox researchers introduce Cube, a new foundation model for 3D intelligence designed to support developers in creating various aspects of Roblox experiences, including 3D object and scene generation, character rigging, and behavior scripting. - The model leverages a novel 3D shape tokenization technique, converting 3D shapes into discrete tokens, enabling applications such as text-to-shape, shape-to-text, and text-to-scene generation.  - This tokenizer utilizes a Perceiver-based transformer with phase-modulated positional encoding, optimal transport vector quantization, and a stochastic gradient shortcut to improve training stability and reconstruction quality. - The model demonstrates strong performance in shape reconstruction, outperforming existing methods like Craftsman in both surface and volumetric IoU on the Toys4K dataset.  - The paper also presents applications showcasing text-to-shape generation and shape-to-text capabilities, along with initial steps toward text-to-scene generation by combining these models with a large language model for scene analysis and reasoning. | ['Text-to-3D', 'Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://github.com/Roblox/cube) | N/A |
| [STEVE: AStep Verification Pipeline for Computer-use Agent Training](https://arxiv.org/abs/2503.12532) | Chi-Wing Fu, Shu Liu, Ziqin Wei, Zhisheng Zhong, Fanbin Lu | - STEVE (Step Verification Pipeline) is introduced for training computer-use agents, which leverages a large language model (GPT-40) to assess the correctness of each action within a trajectory, providing dense reward signals. - The pipeline starts by training a vision-language model specialized in UI grounding using a dataset of web pages and desktop screenshots, and then fine-tuning this model as a computer-use agent using supervised learning with limited trajectory data. - The agent is then deployed to collect more trajectories, with each action evaluated by GPT-40, resulting in a step-verified trajectory dataset. - Kahneman & Tversky Optimization (KTO) is employed to train the agent from this dataset, enabling efficient utilization of both positive and negative actions. - Experiments show that STEVE outperforms supervised fine-tuning, allows training of a 7B parameter vision-language model as a high-performing agent, and achieves state-of-the-art results on the WinAgentArena benchmark. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/FanbinLu/STEVE) | N/A |
| [LEGION: Learning to Ground and Explain for Synthetic Image Detection](https://arxiv.org/abs/2503.15264) | Weijia Li, Junyan Ye, Siwei Wen, zichenwen, khr0516 | - This paper introduces LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework for fully synthetic image detection. - LEGION integrates artifact detection, segmentation, and explanation within a single framework. - The authors also introduce SynthScars, a high-quality and diverse dataset of synthetic images with fine-grained annotations, including pixel-level segmentation, textual explanations, and artifact category labels.  - On SynthScars, LEGION surpasses the second-best traditional expert by 3.31% in mIoU and 7.75% in F1 score, demonstrating state-of-the-art performance. - Beyond detection, LEGION acts as a controller, guiding image refinement pipelines for enhanced realism and quality in synthetic image generation. | ['Computer Vision', 'Image Segmentation', 'Multimodal'] | N/A | [Link](https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1), [Link](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2) |
| [MusicInfuser: Making Video Diffusion Listen and Dance](https://arxiv.org/abs/2503.14505) | Steven M. Seitz, Brian Curless, Ira Kemelmacher-Shlizerman, Susung Hong | - MusicInfuser adapts pre-trained text-to-video diffusion models to generate dance videos synchronized with user-provided music. - It uses a Zero-Initialized Cross-Attention (ZICA) adapter and a low-rank adapter (High-Rank LoRA) to condition video generation on music while preserving text-based control over style and scene elements. - Trained on a combined dataset of AIST dance videos and in-the-wild YouTube dance videos, MusicInfuser leverages existing knowledge of human motion and diverse dance styles without needing motion capture data. - Evaluations using Video-LLMs demonstrate that MusicInfuser produces higher quality and more realistic dance movements, better aligned with music than existing methods, and is generalizable to novel music and longer videos. - The model allows for difficulty control through prompt engineering, enabling users to customize their choreography. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [ViSpeak: Visual Instruction Feedback in Streaming Videos](https://arxiv.org/abs/2503.12769) | Kun-Yu Lin, maybetomorrow, Lymann, PhilipC, fushh7 | - Introduces ViSpeak, a novel streaming video understanding Large Multi-modal Model (LMM) designed for Visual Instruction Feedback, a new task requiring models to actively respond to visual instructions in streaming videos, enhancing real-time human-agent interaction. - Presents ViSpeak-Bench, a benchmark with 1,000 videos and 1,000 QA pairs across seven visual instruction subtasks, and ViSpeak-Instruct, a 34k sample training dataset, to facilitate research in this area.  - Employs a three-stage fine-tuning approach, adapting an existing omni-modal model to streaming input, improving streaming question-answering and proactive output, and finally training on ViSpeak-Instruct for visual instruction feedback. - Achieves state-of-the-art (SOTA) performance on StreamingBench and OVO-Bench, comparable to GPT-40, demonstrating its strong streaming video understanding capabilities and exceeding existing open-source models on ViSpeak-Bench.  - While showing promising results, acknowledges limitations in dataset size and diversity, context length, and degraded Automatic Speech Recognition due to audio segmentation. | ['Multimodal', 'Video-Text-to-Text'] | [Link](https://github.com/HumanMLLM/ViSpeak), [Link](https://github.com/HumanMLLM/ViSpeak-Bench) | N/A |
| [GKG-LLM: A Unified Framework for Generalized Knowledge Graph
  Construction](https://arxiv.org/abs/2503.11227) | Jun Liu, haiping Zhu, Shihao Qi, Bifan Wei, VentureZJ | - This paper proposes GKG-LLM, a unified framework for constructing Generalized Knowledge Graphs (GKGs), encompassing Knowledge Graphs (KGs), Event Knowledge Graphs (EKGs), and Commonsense Knowledge Graphs (CKGs). - GKG-LLM employs a three-stage curriculum learning approach to fine-tune Large Language Models (LLMs), starting with KG data, then EKG data, and finally CKG data, along with counter-task data to enhance generalization. - The model is evaluated on a dataset comprising 15 sub-tasks across 29 datasets, categorized into in-domain, counter-task, and out-of-distribution (OOD) data. - Experimental results show that GKG-LLM outperforms existing closed-source and open-source LLMs on various GKG construction tasks, demonstrating the effectiveness of the unified approach and the curriculum learning strategy. -  GKG-LLM achieves an average improvement of 7.49% over the strongest baseline across all GKG sub-tasks and exhibits strong performance on OOD data, indicating good generalization capability. | ['Natural Language Processing', 'Text2Text Generation', 'Graph Machine Learning'] | N/A | N/A |
| [Mitigating Visual Forgetting via Take-along Visual Conditioning for
  Multi-modal Long CoT Reasoning](https://arxiv.org/abs/2503.13360) | Han-Jia Ye, Houwen Peng, Zhun Sun, Allen8 | - This paper introduces Take-along Visual Conditioning (TVC), a novel method to mitigate visual forgetting in Multimodal Large Language Models (MLLMs) during long-chain reasoning. - TVC shifts and compresses visual tokens at critical reasoning stages through Dynamic Visual Reaffirmation (DVR) and Periodic Visual Calibration (PVC). - DVR injects visual content at intervals during training, and PVC reactivates visual information after token compression during inference. - Experiments show that TVC improves performance on average by 3.4% across five mathematical reasoning datasets compared to previous state-of-the-art models. - TVC's effectiveness is demonstrated through improved reasoning capabilities, especially on complex benchmarks such as MathVerse, which focuses on multimodal reasoning. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | N/A |
| [ELTEX: A Framework for Domain-Driven Synthetic Data Generation](https://arxiv.org/abs/2503.15055) | Eugene Dmitriev, Julien Capitaine, Sofia Sedlova, Kseniia Murasheva, lavriz | - ELTEX (Efficient LLM Token Extraction), a new domain-driven framework, generates high-quality synthetic training data for specialized domains by integrating domain indicator extraction with dynamic prompting. - This framework addresses the scarcity of domain-specific training data that limits large language models (LLMs) in specialized domains like cybersecurity. - The authors demonstrated ELTEX's effectiveness in blockchain-related cyberattack detection by fine-tuning Gemma-2B and achieving performance comparable to GPT-4 with fewer computational resources. - ELTEX-enhanced model shows competitive results across standard classification metrics and uncertainty calibration (measured by Brier score). - A curated synthetic dataset of social media texts for cyberattack detection in blockchain is released. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](huggingface.co) |


## Papers for 2025-03-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [RWKV-7 "Goose" with Expressive Dynamic State Evolution](https://arxiv.org/abs/2503.14456) | saitejautpala, Guangyu, SmerkyG, ZhangRC, BlinkDL | - This paper introduces RWKV-7 "Goose," a novel RNN architecture for sequence modeling with linear complexity. - It features a generalized delta rule with vector-valued gating, in-context learning rates, and a relaxed value replacement rule, enabling state tracking and recognition of all regular languages while retaining training parallelizability. - RWKV-7 achieves state-of-the-art multilingual performance at the 3 billion parameter scale and matches English language performance despite fewer training tokens than comparable models. - Trained models ranging from 0.19B to 2.9B parameters are released alongside a new 3.1 trillion token multilingual corpus (RWKV World v3). - The architecture's ability to match or exceed larger transformer models' performance on various tasks, despite using dramatically fewer training tokens, supports its claim of improved downstream performance. | ['Natural Language Processing', 'Text Generation', 'Multimodal', 'Image-to-Text'] | [Link](https://github.com/RWKV/RWKV-LM) | [Link](https://huggingface.co/RWKV) |
| [Impossible Videos](https://arxiv.org/abs/2503.14378) | Hai Ci, mikeshou, ZechenBai | - This paper introduces IPV-BENCH, a novel benchmark designed to evaluate and foster progress in impossible video understanding and generation. - IPV-BENCH includes a taxonomy of impossible scenes, a prompt suite (IPV-TXT) for text-to-video generation, and a curated impossible video dataset (IPV-VID). - The benchmark covers diverse scenes that defy physical, biological, geographical, or social norms, challenging models to generate and comprehend out-of-distribution content. - Evaluations reveal that current state-of-the-art models fall short on impossible video tasks, struggling with both generation quality and prompt adherence. - The findings highlight the need for future research in video generation and understanding models, emphasizing the importance of temporal reasoning and world knowledge. | ['Text-to-Video', 'Video Classification', 'Multimodal'] | N/A | N/A |
| [Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM](https://arxiv.org/abs/2503.14478) | Yingji Liang, Shengyuan Ding, Kai Lan, Zhijian Chen, Xinyu Fang | - Creation-MMBench, a multimodal benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in real-world, image-based tasks, is introduced. - The benchmark comprises 765 test cases spanning 51 fine-grained tasks across four categories: Literary Writing, Common Functional Writing, Professional Functional Writing, and Creative Multimodal Understanding. - Instance-specific evaluation criteria guide the assessment of general response quality and factual consistency with visual inputs. - Experimental results reveal that current open-source MLLMs underperform compared to proprietary models in creative tasks, and visual fine-tuning can negatively impact the base LLM's creative abilities. - Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future research. | ['Multimodal'] | [Link](https://github.com/open-compass/Creation-MMBench) | N/A |
| [DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs
  for Knowledge-Intensive Visual Grounding](https://arxiv.org/abs/2503.12797) | Zonghao Guo, Zhicong Luo, carboncoo, sdudzy, MaxyLee | - DeepPerception, an enhanced Multimodal Large Language Model (MLLM), integrates cognitive reasoning with visual perception to address the novel task of Knowledge-Intensive Visual Grounding (KVG). - The model employs a two-stage training framework: supervised fine-tuning with synthesized Chain-of-Thought (CoT) data for cognitive scaffolding, followed by reinforcement learning with a perception-oriented reward system for optimizing perception-cognition synergy. - Evaluated on KVG-Bench, a new dataset spanning 10 domains and 1.3K test cases, DeepPerception achieved a +8.08% accuracy improvement over baseline and demonstrated superior (+4.60%) cross-domain generalization compared to existing approaches. - Results highlight the importance of integrating cognitive processes for improved fine-grained visual discrimination, aligning with human expert behavior by leveraging domain knowledge. - DeepPerception's success showcases the potential of cognitive enhancement in advancing MLLMs for more nuanced and human-like visual understanding, exceeding the capabilities of simplistic zero-shot CoT prompting. | ['Multimodal', 'Visual Question Answering', 'Object Detection'] | [Link](https://github.com/thunlp/DeepPerception) | N/A |
| [Frac-Connections: Fractional Extension of Hyper-Connections](https://arxiv.org/abs/2503.14125) | Jundong Zhou, Hongzhi Huang, Defa Zhu, Taoer, FetchFortune | - Frac-Connections (FC), a novel approach for improving the training of deep networks, is introduced, addressing the memory access cost issues of Hyper-Connections while maintaining their benefits.  - It involves dividing hidden states into multiple parts, implementing fractional expansion rates, and utilizing learnable or dynamically predicted connection weights.  - Experiments conducted on large language models, including 7B parameter MoE models trained on up to 3T tokens, demonstrate that Frac-Connections enhances training stability and downstream task performance.  - Compared to residual connections, Frac-Connections reduce training loss, achieve faster convergence, and demonstrate higher accuracy on various benchmarks.  - The approach improves knowledge retention and generalization in Language Models and offers a practical and scalable solution for building more efficient deep learning models. | ['Natural Language Processing'] | N/A | N/A |
| [Aligning Multimodal LLM with Human Preference: A Survey](https://arxiv.org/abs/2503.14504) | Jinda Lu, Junkang Wu, Chaoyou Fu, Tao Yu, yifanzhang114 | - This paper surveys alignment algorithms for Multimodal Large Language Models (MLLMs), focusing on improving their correspondence with human preferences. - The survey categorizes alignment algorithms based on application scenarios (general image understanding, multi-image/video/audio, and extended multimodal applications), discusses core factors in constructing alignment datasets (data sources, model responses, and preference annotations), and reviews benchmarks used for evaluation. - The paper also identifies potential future directions for research in MLLM alignment, such as integrating visual information into alignment algorithms and addressing the opportunities and challenges of MLLMs as agents. - While no specific model architecture is introduced, the survey emphasizes the importance of alignment algorithms in addressing issues like hallucinations, safety, and reasoning abilities of MLLMs, ultimately aiming to bridge the gap between model capabilities and human expectations. - By providing a systematic review of existing techniques and datasets, this work aims to offer a roadmap for researchers and practitioners in the rapidly evolving field of MLLM alignment. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment) | N/A |
| [Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal
  Control](https://arxiv.org/abs/2503.14492) | Tiffany Cai, Maciej Bala, Jose Alvarez, Hassan Abu Alhaija, NVIDIA | - Cosmos-Transfer1 is a conditional world generation model that generates simulated videos based on multiple spatial control inputs, including segmentation, depth, and edge. - The model employs an adaptive and customizable spatial conditional scheme, allowing different weights for various modalities at different spatial locations, built upon a diffusion-based transformer architecture (DiT) with added ControlNet branches for each input modality. - Evaluations demonstrate that Cosmos-Transfer1 excels in preserving scene structure while enabling fine-grained control and improving generation quality, outperforming single-modality models. - The model achieves real-time inference performance by using a data parallelism strategy on an NVIDIA GB200 NVL72 system, processing 5-second 720p videos in 4.2 seconds. - Potential applications include mitigating the sim-to-real domain gap in robotics and enriching data for autonomous vehicle training. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/nvidia-cosmos/cosmos-transferl), [Link](https://github.com/nvidia-cosmos/cosmos-transfer1) | N/A |
| [MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process
  Errors Identification](https://arxiv.org/abs/2503.12505) | Kai Wang, Wangbo Zhao, Jiaxin Ai, Pengfei Zhou, Zhaopan Xu | - Introduces MPBench, a multimodal benchmark for evaluating Process-Level Reward Models (PRMs) designed to identify errors in multi-step reasoning. - MPBench employs three evaluation paradigms: Step Correctness (evaluating individual step accuracy), Answer Aggregation (selecting the best solution from multiple candidates), and Reasoning Process Search (guiding the search for optimal reasoning steps). - Contains 9,745 fine-grained data instances across various subjects, tasks, and challenge levels, focusing on multimodal content common in real-world scenarios. - Experimental results with 12 Multimodal Large Language Models (MLLMs) reveal varying performance across different evaluation paradigms and highlight the challenges in effectively processing and integrating multimodal information for error detection and answer selection. - Demonstrates that model performance generally improves with scale, especially for Step Correctness and Reasoning Process Search, suggesting larger models are better equipped for complex reasoning tasks. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [Measuring AI Ability to Complete Long Tasks](https://arxiv.org/abs/2503.14499) | Katharyn Garcia, Amy Deng, Joel Becker, Ben West, Thomas Kwa | - This research paper proposes a new metric called "50%-task-completion time horizon" to quantify AI capabilities in terms of human capabilities.  - This metric represents the time humans typically take to complete tasks that AI models can complete with a 50% success rate. - The researchers timed humans with domain expertise on a combination of existing and novel tasks and evaluated 13 frontier AI models from 2019 to 2025. - They found that the 50% time horizon doubled approximately every seven months, primarily due to improvements in reliability, mistake adaptation, logical reasoning, and tool use.  - Extrapolating this trend suggests that within five years, AI could automate many software tasks currently taking humans a month, but limitations and external validity concerns necessitate further research. | ['Natural Language Processing'] | [Link](https://github.com/METR/eval-analysis-public), [Link](https://github.com/METR/public-tasks), [Link](https://vivaria.metr.org/), [Link](https://github.com/poking-agents/modular-public) | N/A |
| [Temporal Consistency for LLM Reasoning Process Error Identification](https://arxiv.org/abs/2503.14495) | Xinzhe Juan, Kaixuan Huang, Jiahao Qiu, Yue Wu, Jiacheng Guo | - This paper introduces Temporal Consistency, a novel method for enhancing the accuracy of Large Language Model (LLM) reasoning process error identification in mathematical problem-solving. - The method employs an iterative self-reflection mechanism where LLMs refine their judgments on the presence and location of errors in step-by-step solutions based on previous assessments, promoting convergence towards stable and accurate identifications. - Unlike traditional verification or multi-model debate methods, this approach leverages the temporal dimension by assessing consistency over multiple rounds of self-assessment. - Experimental results across three mathematical reasoning datasets (Mathcheck*, ProcessBench, and PRM800K) demonstrated consistent performance improvements over baseline methods, including greedy decoding, majority voting, and multi-model debate. - Notably, the method significantly boosts the performance of distilled 7B/8B models on ProcessBench, surpassing existing 70B/72B models and GPT-4, showcasing its efficacy in improving accuracy without relying on larger model sizes or extensive training data. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/jcguo123/Temporal-Consistency) | N/A |
| [PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for
  Multimodal Large Language Models](https://arxiv.org/abs/2503.12545) | Wangbo Zhao, Jiaxin Ai, Weidong Tang, Pengfei Zhou, Zhaopan Xu | - PEBench, a synthetic multimodal dataset designed to benchmark Machine Unlearning (MU) in Multimodal Large Language Models (MLLMs), is introduced. - The dataset includes 200 fictitious individuals paired with 40 distinct event scenes, resulting in 8,000 images, enabling evaluation of unlearning both identities and events. - Six MU methods are benchmarked on PEBench, revealing performance variations across people and event unlearning, with Gradient Difference-based methods excelling in event unlearning and maintaining retain set performance. - The study demonstrates the importance of Scope as a key metric and highlights challenges in simultaneously unlearning multiple coupled concepts, observing performance drops or "collapse" with conflicting target distributions or information imbalance. - A balanced gradient difference (BGD) approach with multi-task loss mitigation and dynamic sampling techniques improves simultaneous unlearning performance by mitigating these observed imbalances. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Natural Language Processing'] | [Link](https://pebench.github.io) | N/A |
| [Florenz: Scaling Laws for Systematic Generalization in Vision-Language
  Models](https://arxiv.org/abs/2503.09443) | Sven Behnke, Sebastian Houben, Spravil | - This paper introduces Florenz, a family of encoder-decoder vision-language models (VLMs) ranging from 0.4B to 11.2B parameters, built upon Florence-2 and Gemma-2, to investigate systematic generalization in multilingual multimodal tasks. - The authors explore the impact of model size and training data on systematic generalization, particularly the ability to perform image captioning in unseen languages by training only on translation tasks. - A novel data generation pipeline is proposed that creates a synthetic multilingual multimodal dataset from a translation dataset and an image dataset, enhancing textual context and linking parallel sentences with image context to address lexical ambiguities. - Experimental results demonstrate that Florenz adheres to scaling laws for cross-lingual transfer, with larger models exhibiting better generalization performance even with limited training samples. - Fine-tuning Florenz on downstream tasks yields competitive results and reveals promising scaling trends in multimodal machine translation, lexical disambiguation, and image captioning across various benchmarks. | ['Multimodal', 'Image-to-Text', 'Translation'] | N/A | N/A |
| [Pensez: Less Data, Better Reasoning -- Rethinking French LLM](https://arxiv.org/abs/2503.13661) | HoangHa | - This paper introduces Pensez 7B, a French Language Model fine-tuned from Qwen2.5 7B Instruct, demonstrating that strategic fine-tuning on a small, high-quality bilingual dataset (2,000 examples) enhances both reasoning and French proficiency. - The model leverages techniques like packing inputs, Liger Kernel, DeepSpeed 3, and NEFTune Noise during training on a curated bilingual dataset, Pensez-2k, emphasizing detailed reasoning chains and balanced language representation. - Evaluation results show Pensez 7B achieves competitive performance in reasoning tasks and knowledge comprehension across English and French benchmarks, including AIME25, MATH Hard lv5, MMLU, and TriviaQA, often outperforming models trained on much larger datasets. - Analysis reveals an "overthinking" phenomenon in Pensez 7B, characterized by excessive self-reflection, particularly in incorrect predictions, highlighting the need for better control over reasoning termination. - Future work focuses on refining reasoning processes with reinforcement learning using GRPO, enhancing agentic capabilities through tool integration, and expanding to reasoning-intensive domains like medicine. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs](https://arxiv.org/abs/2503.13111) | Justin Lazarow, Haiming Gang, David Griffiths, Nina Wenzel, Erik Daxberger | - This paper introduces MM-Spatial, a novel multimodal large language model (MLLM) fine-tuned for 3D spatial understanding, and the Cubify Anything VQA (CA-VQA) dataset to train and evaluate the model.  - CA-VQA covers diverse spatial tasks, including spatial relationship prediction, metric size and distance estimation, and 3D grounding, with multi-view images and sensor/monocular depth maps as input.  - MM-Spatial, based on the MM1.5 architecture and utilizing chain-of-thought prompting or tool use for depth integration, achieves state-of-the-art performance on various 3D spatial understanding benchmarks, including CA-VQA and CV-Bench.  - Incorporating multi-view and depth inputs significantly enhances MM-Spatial's 3D understanding, with the model exhibiting depth perception capabilities comparable to dedicated monocular depth estimation models.  - The data-driven approach of MM-Spatial demonstrates the potential of MLLMs to implicitly learn depth information through targeted supervision. | ['Multimodal', 'Visual Question Answering', 'Depth Estimation', 'Computer Vision'] | N/A | N/A |
| [Hyperbolic Safety-Aware Vision-Language Models](https://arxiv.org/abs/2503.12127) | Rita Cucchiara, Lorenzo Baraldi, Pascal Mettes, Tejaswi Kasarla, tobi1modna | - HySAC, a novel hyperbolic safety-aware CLIP model, is introduced to address unsafe content in vision-language models by shifting from unlearning to an awareness paradigm. - HySAC leverages the hierarchical nature of hyperbolic space and employs entailment loss functions to encode safe and unsafe content within distinct regions, establishing an interpretable structure for navigating between safe and unsafe concepts. - This structure enables HySAC to serve as both a multimodal unsafe classifier and a flexible content retriever, capable of redirecting unsafe queries to safer alternatives while preserving access to the original content, offering improved user agency and control. - Extensive experiments on the ViSU dataset demonstrate HySAC's superior performance compared to Safe-CLIP and other hyperbolic vision-language models in safety awareness and retrieval tasks involving both safe and unsafe content. - HySAC also exhibits competitive NSFW image classification accuracy against specialized classifiers and robust zero-shot capabilities across various datasets. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Computer Vision'] | [Link](https://github.com/aimagelab/HySAC) | N/A |
| [KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for
  Open-Vocabulary Robotic Manipulation](https://arxiv.org/abs/2503.10546) | Yunzhu Li, Mingtong Zhang, Zixian Liu | - KUDA, an open-vocabulary manipulation system, unifies dynamics learning and visual prompting through keypoints, integrating Vision Language Models (VLMs) with learning-based neural dynamics models. - Keypoint-based target specifications, generated by a VLM from language instructions and visual observations, are converted into cost functions for model-based planning with learned dynamics models, enabling manipulation across diverse object categories and dynamic scenarios. - A two-level closed-loop control mechanism, comprising low-level model-based planning and high-level VLM re-planning, ensures robustness and corrects for imperfect target specifications or execution errors. - A prompt retriever with score matching selects relevant few-shot examples from a library to enhance VLM performance without exceeding token limits. - Experimental results on tasks involving diverse object materials like ropes and granular objects demonstrate state-of-the-art performance, surpassing baselines like MOKA and VoxPoser in complex manipulation scenarios. | ['Robotics', 'Multimodal'] | N/A | N/A |


## Papers for 2025-03-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal
  Consistent Video Generation](https://arxiv.org/abs/2503.06053) | Runze Zhang, NeilXu, EllenAP, lixiaochuan, georgedu | - This paper introduces DropletVideo, a novel text-to-video model trained on a new dataset called DropletVideo-10M, which contains 10 million videos with dynamic camera motion and object actions, each accompanied by detailed captions averaging 206 words describing camera movements and plot developments. - The DropletVideo model focuses on maintaining *integral spatio-temporal consistency*, considering the interplay between plot progression and camera techniques, and the influence of prior content on subsequent generation, enabling generation of complex multi-plot narratives. - DropletVideo-10M is significantly larger than existing open-source datasets addressing spatio-temporal consistency and includes richer textual descriptions than comparable datasets like Panda-70M. - It leverages a 3D causal Variational Autoencoder and a Multi-Modal Diffusion Transformer for encoding video and text, and employs a motion adaptive generation strategy to control the motion speed in generated videos.  - Qualitative results demonstrate that DropletVideo effectively preserves content consistency and generates realistic camera movements, outperforming or matching other state-of-the-art models on several qualitative metrics. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | [Link](https://dropletx.github.io) | N/A |
| [Being-0: A Humanoid Robotic Agent with Vision-Language Models and
  Modular Skills](https://arxiv.org/abs/2503.12533) | tellarin, SherryXu, takenpeanut, fuyh, Yaya041 | - Being-0 is a hierarchical agent framework designed to effectively control a full-sized humanoid robot to perform complex, long-horizon tasks in real-world environments. - The framework integrates a Foundation Model (FM) for high-level reasoning, a lightweight Vision-Language Model (VLM) called Connector for bridging the gap between high-level plans and low-level skills, and a Modular Skill Library for robust locomotion and manipulation. - The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands, dynamically coordinating locomotion and manipulation, and adjusting the robot's pose for optimal skill execution. - Experimental results on a Unitree H1-2 humanoid robot show an average completion rate of 84.4% on challenging long-horizon tasks, demonstrating the framework's efficiency and robustness. - Being-0 leverages an active camera and dexterous hands, and deploys all modules onboard, except the FM, enabling efficient and real-time performance. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [reWordBench: Benchmarking and Improving the Robustness of Reward Models
  with Transformed Inputs](https://arxiv.org/abs/2503.11751) | Yoon Kim, Andrew Cohen, mghazvininejad, michiyasunaga, ZhaofengWu | - This paper introduces reWordBench, a new benchmark designed to evaluate the robustness of reward models (RMs) used in natural language processing. - reWordBench consists of transformed inputs from the original RewardBench, using meaning- or ranking-preserving transformations to assess how RMs handle slight input alterations. - The authors find that state-of-the-art RMs are brittle, exhibiting significant performance degradation even with minor transformations. - To address this, they propose a regularization method that trains RMs to assign similar scores to paraphrased inputs, improving robustness across different transformation types. - This regularization method also enhances the utility of RMs in alignment tasks, leading to higher-quality outputs in downstream applications. | ['Natural Language Processing'] | N/A | N/A |
| [MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based
  Scientific Research](https://arxiv.org/abs/2503.13399) | lundbergemma, chadliu, shcohen, suyc21, jmhb | - MicroVQA, a novel visual question answering (VQA) benchmark, is introduced to evaluate multimodal reasoning within the context of microscopy-based biological research. - The benchmark consists of 1,042 multiple-choice questions spanning three key research tasks: expert image understanding, hypothesis generation, and experiment proposal. - A two-stage process is employed for generating these questions, starting with expert-created samples and then refining them using an agent-based system ('RefineBot') to minimize language shortcuts. - State-of-the-art multimodal large language models (MLLMs) achieve a peak performance of 53% on MicroVQA, demonstrating its challenge and utility in assessing research-level reasoning. - An expert error analysis reveals that perception errors pose the most significant challenge, emphasizing the need for improved visual understanding capabilities in MLLMs, while specialized training on biomedical data shows performance improvements. | ['Multimodal', 'Visual Question Answering', 'Computer Vision'] | [Link](https://github.com/jmhb0/microvqa) | [Link](https://huggingface.co/datasets/jmhb/microvqa) |
| [Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2503.12605) | Yuecheng Zhang, scofield7419, liuziwei7, ChocoWu, Gh0stAR | - This survey paper provides a comprehensive overview of Multimodal Chain-of-Thought (MCOT) reasoning, a technique that enhances multimodal reasoning by enabling large language models (LLMs) to decompose complex tasks into a series of intermediate steps. - The authors categorize MCoT methodologies based on rationale construction, structure, information enhancement, objective granularity, multimodal rationale integration, and test-time scaling. - They also present a timeline of key milestones in MCoT research, discuss relevant applications in various domains like robotics, healthcare, and autonomous driving, and provide a curated list of available datasets and benchmarks. - The paper identifies key challenges in MCoT research, such as computational sustainability, the limitations of reasoning in general scenarios, error propagation in extended reasoning chains, the symbolic-neural integration gap, and hallucination prevention. - Finally, it proposes promising future research directions, including dynamic environment adaptation, adaptive chain length, and integration with cognitive science, to address these challenges and further advance the development of MCoT. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Natural Language Processing'] | [Link](https://github.com/yaotingwangofficial/Awesome-MCOT) | N/A |
| [Free-form language-based robotic reasoning and grasping](https://arxiv.org/abs/2503.13082) | Matteo Bortolon, Alice Fasoli, Runyu Jiao, SPovoli, FGiuliari | - This research introduces FreeGrasp, a novel method for robotic grasping that interprets free-form language instructions and reasons about spatial relationships between objects, leveraging pre-trained Vision-Language Models (VLMs). - The method detects objects as keypoints, annotates them on images for improved VLM spatial reasoning, and determines the grasping sequence for a target object, even if obstructed. - Using a new synthetic dataset, FreeGraspData, based on MetaGraspNetV2, and real-world robotic experiments, FreeGrasp demonstrated superior performance in grasp reasoning and execution compared to ThinkGrasp, particularly in cluttered scenes with ambiguous object descriptions. - FreeGrasp's robust performance is attributed to its mark-based visual prompting and contextualized reasoning, which addresses the limitations of VLMs in visual-spatial reasoning tasks. - Despite improvements, limitations persist in handling complex object occlusion and adapting to scene changes. Future work is needed to explore memory mechanisms and adaptive instructions in these scenarios. | ['Robotics', 'Computer Vision', 'Object Detection', 'Multimodal'] | N/A | N/A |
| [R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization](https://arxiv.org/abs/2503.12937) | Jingyi Zhang, Xikun, liushunyu, HuanjinYao, huangjiaxing | - This paper introduces R1-VL, a series of Multimodal Large Language Models (MLLMs) enhanced for step-by-step reasoning using a novel reinforcement learning approach. - The key contribution is Step-wise Group Relative Policy Optimization (StepGRPO), which addresses the sparse reward issue in MLLM reasoning by incorporating dense step-wise reasoning rewards. - StepGRPO employs two rule-based reward mechanisms: Step-wise Reasoning Accuracy Reward (StepRAR) to encourage the inclusion of necessary intermediate steps and Step-wise Reasoning Validity Reward (StepRVR) to promote well-structured, logically consistent reasoning. - Experimental results across 8 benchmarks demonstrate that R1-VL significantly outperforms baseline MLLMs and achieves state-of-the-art performance on multiple reasoning tasks, exhibiting improvements of 4.6% and 3.8% over Qwen2-VL-2B and Qwen2-VL-7B respectively.  - R1-VL's competitive results against both closed and open-source models like GPT-40 and Mulberry-7B illustrate the effectiveness of StepGRPO in advancing MLLM reasoning capabilities. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | N/A | N/A |
| [V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning](https://arxiv.org/abs/2503.11495) | Wei Li, Ziquan Liu, ChenyangSi, lwpyh, Cade921 | - Introduces V-STaR, a benchmark designed to evaluate the spatio-temporal reasoning abilities of Video Large Language Models (Video-LLMs). - Proposes a Reverse Spatio-Temporal Reasoning (RSTR) task that decomposes video understanding into identifying "what" objects are present, "when" events occur, and "where" they are located. - Constructs a dataset with coarse-to-fine Chain-of-Thought (CoT) questions generated by a semi-automated GPT-4 pipeline, mimicking human cognitive processes for sequential reasoning. - Introduces the Logarithmic Geometric Mean (LGM) to comprehensively assess spatio-temporal reasoning by combining model scores at each step of the reasoning chain. - Experiments on 14 Video-LLMs reveal performance gaps, especially in grounding answers temporally and spatially, highlighting the need for more robust spatio-temporal reasoning in Video-LLMs. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | [Link](https://V-STaR-Bench.github.io/) | N/A |
| [VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](https://arxiv.org/abs/2503.13444) | Chang Wen Chen, Ye Liu, AnalMom, KevinQHLin | - VideoMind, a novel video-language agent designed for temporal-grounded video understanding, is introduced, incorporating a role-based agentic workflow (Planner, Grounder, Verifier, and Answerer) and a Chain-of-LoRA strategy for efficient role-switching. - The Chain-of-LoRA strategy, built upon a single base Multimodal Large Language Model (MLLM), enables seamless transitions between roles using lightweight LoRA adaptors, balancing efficiency and flexibility by avoiding the overhead of multiple models. - VideoMind achieves state-of-the-art performance on 14 public benchmarks across diverse video understanding tasks, including grounded video question-answering, video temporal grounding, and general video question-answering. - On the challenging CG-Bench with long videos (average duration: 27 minutes), VideoMind's 2B model outperforms all open-source models and most closed-source models, while the 7B model surpasses even GPT-40. - Ablation studies confirm the effectiveness and efficiency of the proposed design choices, especially the Chain-of-LoRA mechanism for enhanced performance and computational efficiency. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Question Answering'] | N/A | N/A |
| [Sightation Counts: Leveraging Sighted User Feedback in Building a
  BLV-aligned Dataset of Diagram Descriptions](https://arxiv.org/abs/2503.13369) | Jaime-Choi, sangryul, namin0202, eunkey, soarhigh | - SIGHTATION, a new large-scale, BLV-aligned dataset of diagram descriptions, is introduced, addressing the need for accessible visual information for blind and low-vision (BLV) individuals. - The dataset leverages sighted user feedback through a multi-pass inference process where sighted users assess descriptions generated by vision-language models (VLMs), rather than producing the descriptions themselves, reducing bias and cost. - SIGHTATION includes a variety of training data for tasks such as completion, preference, retrieval, question answering, and reasoning, totaling 5k diagrams and 137k samples. - The effectiveness of the dataset is demonstrated through fine-tuning various sized models, with results showing a preference-tuned 2B model achieving a 1.67 increase in usefulness ratings by the BLV group and an instruction-tuned 2B model outperforming a 3B model on chart comprehension in 8 out of 11 automatic metrics. - A BLIP-2 model fine-tuned for retrieval on SIGHTATION achieves a 65%p improvement on Precision@1 compared to a COCO-tuned model. | ['Image-to-Text', 'Multimodal'] | N/A | [Link](https://hf.co/Sightation) |
| [Basic Category Usage in Vision Language Models](https://arxiv.org/abs/2503.12530) | KyleMoore, JesseTNRoberts, HTSawyer | - This paper investigates basic level categorization in two vision-language models (VLMs): Llama 3.2 Vision Instruct (11B) and Molmo 7B-D. - The study uses the Ecoset dataset, containing 1.5 million images across 565 basic-level categories. - Results show that both VLMs prefer basic level categorization, consistent with human behavior, exceeding the estimated 50% lower bound of basic-level usage. - Furthermore, the models exhibit nuances consistent with human categorization, such as the biological vs. non-biological basic level effects and the expert basic level shift. - These findings suggest that VLMs learn cognitive categorization behaviors from human training data. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [Investigating Human-Aligned Large Language Model Uncertainty](https://arxiv.org/abs/2503.12528) | Pamela Wisniewski, Daryl Watson, Kyle Moore, JesseTNRoberts | - This paper investigates how well different uncertainty measures in large language models (LLMs) align with human uncertainty. - The study uses non-factual questions from the Pew Research Center's American Trends Panel to compare human uncertainty with LLM uncertainty, focusing on agreement with the provided choices. - It evaluates various uncertainty measures, including Bayesian measures, entropy variations, and nucleus size, across different LLM sizes (1B to 7B parameters) and types (base and instruction-finetuned). - The findings indicate that top-k entropy tends to align best with human behavior, with better performance in smaller models, and a combination of multiple measures provides comparable alignment while reducing size dependency. - Future research directions include exploring other uncertainty measures, optimizing existing ones for better human alignment, and expanding the study to a broader range of contexts | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-03-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|


## Papers for 2025-03-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing](https://arxiv.org/abs/2503.10613) | Dang Nguyen, zhoutianyi, nandakiran09, advaitgupta | - CoSTA*, a cost-sensitive toolpath agent, is introduced for multi-turn image editing, combining LLMs for subtask planning and A* search for efficient tool selection. - This hierarchical approach leverages an LLM to generate a subtask tree, which is then used to construct a Tool Subgraph (TS) based on tool dependencies. - A* search on the TS determines the optimal toolpath by balancing computational cost and output quality using a cost function that incorporates a trade-off parameter. - Experimental results on a novel benchmark dataset demonstrates CoSTA*'s superiority over existing baselines in complex multimodal tasks, pushing the Pareto frontier of cost-quality trade-offs. - CoSTA* effectively handles intricate workflows, dynamic feedback and retry mechanisms, along with support for a broader range of tasks, making it adaptable for complex image and text-in-image editing. | ['Multimodal', 'Image-to-Image', 'Text-to-Image'] | [Link](https://github.com/tianyi-lab/CoSTAR) | N/A |
| [World Modeling Makes a Better Planner: Dual Preference Optimization for
  Embodied Task Planning](https://arxiv.org/abs/2503.10480) | xpqiu, Jinlan, CyberDJ, ngc7293, sinwang | - Introduces Dual Preference Optimization (D2PO), a framework jointly optimizing state prediction and action selection for embodied task planning using preference learning, allowing Large Vision-Language Models (LVLMs) to better understand environment dynamics. - Presents a tree search mechanism to automatically collect trajectories and preference data without human annotation, facilitating extensive exploration via trial-and-error. - Evaluates on VoTa-Bench, a vision-enhanced extension of LoTa-Bench, demonstrating D2PO's significant improvement over existing methods and GPT-40 across multiple LVLMs (Qwen2-VL 7B, LLaVA-1.6 7B, LLaMA-3.2 11B). - Shows superior task success rates and more efficient planning with the 7B parameter model outperforming GPT-40, highlighting the approach's effectiveness. - Addresses limitations like the sim-to-real gap and data collection efficiency, while suggesting future work on more complex error patterns. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [Charting and Navigating Hugging Face's Model Atlas](https://arxiv.org/abs/2503.10633) | yedid, LielAmar, jonkahana, nitzankur, Eliahu | - This paper introduces a method for charting and navigating large model repositories, focusing on Hugging Face, to improve model discovery, reuse, and analysis of machine learning trends. - The proposed method addresses the challenge of incomplete model documentation by leveraging structural patterns in real-world model training practices, such as quantizations, near-duplicates, and merges, to accurately predict model relationships and attributes. - The method constructs a directed acyclic graph (DAG) representing the model relationships (e.g., fine-tuning, merging) and demonstrates its utility for various applications, including trend analysis, model attribute prediction, and measuring model impact. - Experimental results on Hugging Face datasets show that this approach significantly outperforms baseline methods in atlas recovery accuracy and allows for better model impact measurement and more robust IP tracking than simple download counts. - The atlas and dataset are publicly released to enable more effective model exploration and research. | ['Graph Machine Learning', 'Computer Vision', 'Natural Language Processing', 'Multimodal'] | N/A | [Link](https://horwitz.ai/model-atlas), [Link](https://huggingface.co/datasets/cfahlgrenl/hub-stats) |
| [GoT: Unleashing Reasoning Capability of Multimodal Large Language Model
  for Visual Generation and Editing](https://arxiv.org/abs/2503.10639) | zengxingyu, shilinyan, LjHuang, gogoduan, LucasFang | - The paper introduces Generation Chain-of-Thought (GoT), a novel paradigm that integrates multimodal large language model (MLLM) reasoning capabilities into visual generation and editing through explicit semantic-spatial reasoning chains. - GoT transforms visual generation from direct mapping to a reasoning-guided process with precise spatial control over object layout, relationships, and attributes. - The proposed framework leverages a semantic-spatial aware MLLM to generate structured reasoning chains and a multi-guided diffusion model with a Semantic-Spatial Guidance Module (SSGM). - The SSGM integrates semantic understanding, spatial awareness, and reference knowledge to ensure generated images accurately reflect the reasoning process. - Experimental results demonstrate state-of-the-art performance on text-to-image generation and editing benchmarks while enabling interactive control through modifiable reasoning chains. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/rongyaofang/GoT) | N/A |
| [Transformers without Normalization](https://arxiv.org/abs/2503.10622) | Zhuang Liu, Kaiming He, ylecun, endernewton, JiachenZhu | - This paper introduces Dynamic Tanh (DyT), a simple element-wise operation, as a replacement for normalization layers in Transformers. - DyT is defined as DyT(x) = tanh(ax), where 'a' is a learnable parameter, aiming to mimic the input-output mapping of Layer Normalization (LN). - Experiments across various tasks, including image classification, self-supervised learning, diffusion models, and large language models, show that Transformers with DyT achieve comparable or better performance than their LN counterparts, often without hyperparameter tuning. - The paper challenges the conventional wisdom that normalization layers are indispensable in modern neural networks. - The findings offer insights into the role of normalization by demonstrating its similarity to a scaled tanh function. | ['Computer Vision', 'Image Classification', 'Natural Language Processing', 'Text Generation', 'Unconditional Image Generation'] | N/A | N/A |
| [GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding](https://arxiv.org/abs/2503.10596) | wenyuliu, steelozazala, wondervictor, LianghuiZhu, RuiHu | - This paper introduces GroundingSuite, a comprehensive suite for evaluating pixel grounding, including an automated annotation framework (GSSculpt), a large-scale training dataset (GSTrain-10M with 9.56 million image-text pairs), and a curated evaluation benchmark (GSEval with 3800 images). - GSSculpt leverages Vision-Language Models for generating and refining pixel masks and descriptions and employs a noise filtering process, enhancing the quality and efficiency of automatic annotation compared to existing methods. - GSTrain-10M and GSEval address limitations in previous datasets by encompassing stuff-class, part-level, and multi-object segmentation with diverse and detailed textual descriptions. - Models trained on GroundingSuite achieve state-of-the-art results, demonstrating its effectiveness; for example, reaching a cIoU of 68.9 on gRefCOCO and gIoU of 55.3 on RefCOCOm. - This benchmark evaluates models in zero-shot settings and covers fine-grained and complex scene understanding scenarios. | ['Image Segmentation', 'Multimodal'] | [Link](https://github.com/hustvl/GroundingSuite) | N/A |
| [New Trends for Modern Machine Translation with Large Reasoning Models](https://arxiv.org/abs/2503.10351) | acecamel1977, longyuewang, minghaowu, ChenyangLyu, SNF | - This paper explores the transformative potential of Large Reasoning Models (LRMs) in redefining Machine Translation (MT) by showcasing how they reframe translation as a reasoning task, going beyond traditional text-to-text mapping. - LRMs exhibit self-reflection and auto-pivot translation capabilities, which are new characteristics introduced to MT. - The authors present three fundamental shifts brought about by LRMs: 1) contextual coherence 2) cultural intentionality 3) self-reflection. - By leveraging Chain-of-Thought (CoT) reasoning, LRMs address challenges in stylized translation, document-level translation, and multimodal translation. - The paper also identifies critical challenges such as over-localization, inference efficiency, and limitations in handling complex ciphers and specialized multimodal inputs, presenting opportunities for future research. | ['Translation', 'Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [Shifting Long-Context LLMs Research from Input to Output](https://arxiv.org/abs/2503.04723) | mingshan, tsq2000, Zhiqiang007, bys0318, mozhu | - This paper argues for a shift in Natural Language Processing (NLP) research from focusing on long-input Large Language Models (LLMs) to developing long-output LLMs. - It highlights the increasing demand for generating extended, coherent, and contextually rich text in various real-world applications, such as novel writing, long-term planning, and complex reasoning, which require outputs exceeding 4,000 tokens. - The paper identifies key challenges hindering progress in long-output generation, including data limitations, task execution complexities, and computational cost constraints. - It defines long-output LLMs as models specifically designed to excel at long-output tasks, requiring capabilities in handling extensive contexts and generating high-quality, logically consistent text. - The paper explores the current state of long-output LLMs, discussing existing datasets, benchmarks, and models, and analyzes their limitations and potential. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [VisualWebInstruct: Scaling up Multimodal Instruction Data through Web
  Search](https://arxiv.org/abs/2503.10582) | Bo Li, Xiang Yue, wenhu, jiachenli-ucsb, jymmmmm | - This paper introduces VisualWebInstruct, a new dataset for visual question answering (VQA) focused on complex reasoning, created by leveraging web search and automated data curation. - Starting with 30,000 seed images, they use Google Image Search to find similar images and related web pages, extracting about 900K question-answer pairs from over 700K unique URLs with around 40% being visual QA pairs and others as text QA pairs. - The dataset creation pipeline involves HTML processing, content extraction using an LLM, answer synthesis using GPT-4, and a consistency filtering process to ensure high-quality and diversity of the collected data. - Fine-tuning existing VLMs on VisualWebInstruct shows significant performance gains, with improvements ranging from 5-20% on several reasoning benchmarks including MMMU, MathVista, and DynaMath. - Their best model, MAmmoTH-VL2 (7B parameter), achieves state-of-the-art performance in its size category on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Open-Sora 2.0: Training a Commercial-Level Video Generation Model in
  $200k](https://arxiv.org/abs/2503.09642) | Xinying Guo, Tom Young, Chenhui Shen, Zangwei Zheng, Xiangyu Peng | - Open-Sora 2.0 is a commercial-level text-to-video and image-to-video generation model trained for $200k, 5-10 times cheaper than comparable models. - It uses a novel Video DC-AE autoencoder with deep compression and a DiT (Diffusion Transformer) architecture incorporating 3D Rotary Position Embedding (ROPE) for enhanced motion representation. - Trained using a three-stage process involving low and high resolution videos, it leverages open-source image models (Flux) and hierarchical data filtering for efficiency. - Evaluations with VBench show that Open-Sora 2.0 outperforms open-source models like CogVideo and HunyuanVideo, closing the performance gap with OpenAI's Sora to 0.69%. - Supports resolutions up to 768x768 pixels and video lengths up to 5 seconds at 24 FPS. | ['Text-to-Video', 'Image-to-Video', 'Multimodal'] | [Link](https://github.com/hpcaitech/Open-Sora) | N/A |
| [4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large
  Language Models](https://arxiv.org/abs/2503.10437) | hpfister, Qmh, wrencanfly, rpzhou, EthanTaylor | - 4D LangSplat, a novel framework for building dynamic 4D language fields, enabling time-sensitive and time-agnostic open-vocabulary queries in dynamic scenes, is introduced. - It leverages a multimodal object-wise video prompting method, combining visual and textual prompts with a Multimodal Large Language Model (MLLM) and a Large Language Model (LLM), to generate and encode object-specific video captions into pixel-aligned features for training the 4D language field. - To capture evolving object semantics, 4D LangSplat incorporates a status deformable network, modeling smooth transitions between limited semantic states. - Experimental results on HyperNeRF and Neu3D datasets show state-of-the-art performance in both time-agnostic and time-sensitive querying, significantly outperforming baseline methods by substantial margins in accuracy and intersection-over-union scores. - The approach effectively handles dynamic object transformations and temporal semantics, enabling precise spatiotemporal querying in complex 4D scenes. | ['Computer Vision', 'Text-to-3D', 'Multimodal'] | N/A | N/A |
| [UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](https://arxiv.org/abs/2503.10630) | Ziwei Wang, Lingqing Zhao, jiwenlu, xuxw98, hangyin | - UniGoal is a universal zero-shot goal-oriented navigation framework that handles object-goal, instance-image-goal, and text-goal navigation without training or finetuning. - It employs a uniform graph representation for both the agent's observations (scene graph) and the goal (goal graph), which minimizes information loss and facilitates explicit graph-based reasoning by an LLM. - A multi-stage exploration policy guides navigation based on graph matching scores: zero matching triggers subgraph searching, partial matching initiates coordinate projection and anchor pair alignment, and perfect matching leads to graph correction and goal verification. - A blacklist mechanism avoids repeated exploration of unsuccessful areas and enables robust switching between stages. - UniGoal achieves state-of-the-art zero-shot performance on various benchmarks, exceeding task-specific and supervised universal methods. | ['Robotics', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and
  Beyond](https://arxiv.org/abs/2503.10460) | tanglifu, JunchenLiu, yyy99, duan901010, cizhenshi | - The Light-R1 series introduces a new approach for training long chain-of-thought (COT) reasoning models, focusing on efficiency and effectiveness for resource-constrained environments. - The approach employs a curriculum training strategy involving two-stage Supervised Fine-tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO), validated by training Light-R1-32B from Qwen2.5-32B-Instruct, outperforming DeepSeek-R1-Distill-Qwen-32B on math tasks. - A 3k dataset curated for the second SFT stage significantly boosts performance across other models, resulting in state-of-the-art (SOTA) results for 7B and 14B models, and competitive performance for the 32B model (Light-R1-32B-DS) compared to QwQ-32B and DeepSeek-R1. - Applying Generalized Reinforcement Learning from Preference Optimization (GRPO) on long-COT models, specifically Light-R1-14B-DS, yields SOTA results for 14B models in math, exceeding many 32B models and DeepSeek-R1-Distill-Llama-70B on AIME benchmarks, with simultaneous improvement in response length and reward scores. - The Light-R1 series releases all models, data, and code, showing that long-COT models can be effectively trained from scratch and demonstrating SOTA performance improvements on long-COT and RL training of 14B models. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/Qihoo360/Light-R1) | N/A |
| [CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance](https://arxiv.org/abs/2503.10391) | brotherhuang, u302117, BestWishYsh, angtian, dyf | - CINEMA is a novel framework for generating coherent multi-subject videos, guided by a Multimodal Large Language Model (MLLM), which enhances coherence and contextual alignment. - The framework utilizes MM-DiT for video generation, integrating multimodal information through a multimodal large language model, semantic alignment network (AlignerNet), and visual entity encoding. - AlignerNet bridges the representation gap between the MLLM and the text encoder of the base video generation model, ensuring compatibility. - Visual entity encoding extracts detailed visual features from reference images using a VAE, ensuring subject consistency. - Experimental results demonstrate that CINEMA effectively preserves subject identity, adheres to text prompts, and maintains temporal coherence in generated videos, outperforming baseline models in visual quality and subject consistency. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [Quantization for OpenAI's Whisper Models: A Comparative Analysis](https://arxiv.org/abs/2503.09905) | allisonandreyev | - This paper analyzes the impact of quantization on OpenAI's Whisper ASR models, including two variants: Whisper_Streaming and whisper-timestamped. - It quantizes the base Whisper model using INT4, INT5, and INT8 methods and evaluates their performance on the LibriSpeech dataset. - The study finds that quantization reduces model size by up to 45% and latency by 19% while preserving transcription accuracy. - This makes Whisper deployment feasible on resource-constrained devices like smartphones and IoT systems. - The paper also qualitatively compares the three Whisper variants, highlighting their strengths and limitations for different applications. | ['Audio', 'Automatic Speech Recognition'] | [Link](https://github.com/allisonandreyev/WhisperQuantization) | N/A |
| [R1-Onevision: Advancing Generalized Multimodal Reasoning through
  Cross-Modal Formalization](https://arxiv.org/abs/2503.10615) | Xiaoxuan He, Yi Yang, twilightsnow, dcyin, Emilia515 | - Introduces R1-Onevision, a multimodal reasoning model employing a cross-modal reasoning pipeline that transforms images into formal textual representations for enhanced language-based reasoning. - Presents a new dataset, R1-Onevision, containing detailed step-by-step multimodal reasoning annotations across various domains, created using a role-playing strategy. - Develops a two-stage post-training strategy involving supervised fine-tuning and reinforcement learning to enhance reasoning abilities and generalization of the model. - Introduces R1-Onevision-Bench, a comprehensive benchmark designed to assess grade-level multimodal reasoning skills across different educational levels and scientific domains. - Achieves state-of-the-art performance, surpassing models such as GPT-40 and Qwen2.5-VL on various multimodal reasoning benchmarks, including significant improvements on MathVerse and MathVision. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/Fancy-MLLM/R1-onevision) | N/A |
| [VisualPRM: An Effective Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2503.10291) | Einsiedler, Yeshenglong, Decaux, chenlj22, Weiyun1025 | - This paper introduces VisualPRM, an 8B parameter multimodal Process Reward Model (PRM) designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). - VisualPRM enhances MLLM performance across various model scales and families by using Best-of-N (BoN) evaluation strategies, achieving a 5.9-point improvement with InternVL2.5-78B across seven benchmarks. - To train VisualPRM, the authors created a 400K sample multimodal process supervision dataset, VisualPRM400K, using an automated data pipeline to annotate step-wise correctness. - A new benchmark, VisualProcess-Bench, with 2,866 human-annotated samples, was also developed for evaluating multimodal PRMs and MLLMs' ability to detect erroneous reasoning steps. - Experimental results demonstrate that VisualPRM consistently outperforms Outcome Reward Models and Self-Consistency in BoN evaluations, establishing its effectiveness as a critic model for enhancing MLLM reasoning. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| ["Silent Is Not Actually Silent": An Investigation of Toxicity on Bug
  Report Discussion](https://arxiv.org/abs/2503.10072) | Jaydeb Sarker, imranraad | - This study investigates toxicity in GitHub bug report discussions, a critical aspect of open-source software development often overlooked in previous research. - Through qualitative analysis of 203 bug threads (81 toxic), the research identifies key factors contributing to toxicity, such as misaligned perceptions of bug severity and maintainers' prioritization, unresolved frustrations with tools, and lapses in professional communication. - Findings reveal that external participants and bug reporters themselves are more prone to initiating toxic comments. Toxic threads have lower resolution rates and are less likely to be linked to pull requests, hindering collaboration and bug resolution.  - The study also highlights the detrimental impact of toxicity on project discussions and offers actionable recommendations for improving bug resolution by mitigating toxicity. - Recommendations for future work include developing more robust and context-aware automated systems for toxicity detection in software engineering discussions and conducting large-scale empirical studies to gain deeper insights into this phenomenon. | ['Natural Language Processing'] | N/A | N/A |
| [On the Limitations of Vision-Language Models in Understanding Image
  Transforms](https://arxiv.org/abs/2503.09837) | Saquib Sarfraz, Hasnain Ali, Ahmad Mustafa Anis | - This research paper investigates the limitations of Vision-Language Models (VLMs), particularly CLIP and SigLIP, in understanding basic image transformations such as rotations, flips, color adjustments, and distortions. - The study reveals that while VLMs excel in semantic understanding, they struggle with comprehending and reasoning about image-level augmentations, impacting their performance in downstream tasks like image editing. - An augmented version of the Flickr8k dataset is created, pairing each image with detailed descriptions of applied transformations, to evaluate VLM performance on understanding augmented descriptions, matching augmented images with descriptions, and classifying transformations. - Experiments show that VLMs have difficulty associating augmented descriptions with corresponding images and struggle to correctly classify image transformations. - The findings highlight a crucial gap in current VLMs' spatial understanding, emphasizing the need for new training paradigms that balance invariance with explicit transformation awareness for improved performance in image editing and related tasks. | ['Multimodal', 'Computer Vision', 'Image-to-Image'] | N/A | N/A |


## Papers for 2025-03-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Block Diffusion: Interpolating Between Autoregressive and Diffusion
  Language Models](https://arxiv.org/abs/2503.09573) | Zhixuan Qi, Zhihan Yang, Justin T Chiu, Aaron Gokaslan, Marianne Arriola | - This paper introduces Block Discrete Denoising Diffusion Language Models (BD3-LMs), a new class of language models that interpolate between autoregressive and diffusion models. - BD3-LMs address limitations of both approaches by generating blocks of text autoregressively, while using diffusion models within each block, which supports variable-length generation, KV caching, and parallel sampling. - The authors propose a recipe for training BD3-LMs, including efficient training algorithms, estimators of gradient variance, and data-driven noise schedules designed to minimize this variance and improve training stability. - Experiments on language modeling benchmarks show BD3-LMs achieve state-of-the-art perplexity among diffusion models and match autoregressive models in perplexity given appropriate noise schedules. - BD3-LMs generate arbitrary length sequences, outperforming other diffusion models on this task. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/kuleshov-group/bd3lms) | N/A |
| [GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based
  VLM Agent Training](https://arxiv.org/abs/2503.08525) | Zongqing Lu, Yuanchun Shi, Junliang Xing, Yijun Yang, Tong Wei | - Proposed Guided Thought Reinforcement (GTR), a framework to prevent thought collapse in RL-based VLM agent training by introducing automated thought correction. - GTR leverages an off-the-shelf VLM as a corrector model, which first evaluates the agent's thought and then corrects inconsistencies or errors based on the agent's outputs. - Integrates a simple SFT loss over the thought tokens to align the agent's reasoning with the corrected trajectories. - Addresses the distribution shift issue in thought cloning by employing Dataset Aggregation (DAgger), which continuously expands the dataset for thought cloning. - Achieved a 3x-5x increase in success rate and higher returns on the Points24 task compared to existing models, demonstrating the importance of process guidance in RL training. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [More Documents, Same Length: Isolating the Challenge of Multiple
  Documents in RAG](https://arxiv.org/abs/2503.04388) | Gabriel Stanovsky, Michael Hassid, Nir Mazor, Shahar Levy, LihiShalmon | - This paper investigates the impact of the number of retrieved documents on the performance of Retrieval-Augmented Generation (RAG) models while controlling for context length. - The study uses a custom dataset derived from a multi-hop QA task, where the context length and relevant information position are kept constant while varying the number of documents. - Results indicate that increasing the number of documents in RAG poses significant challenges for LLMs, even with a fixed context window, with performance drops of up to 10% observed. - This suggests that processing multiple documents is a distinct challenge from handling long contexts, likely due to factors such as redundancy, conflicting information, and implicit inter-document relationships. - The authors make their datasets and code publicly available to facilitate further research in multi-document retrieval. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/shaharl6000/MoreDocsSameLen) | N/A |
| [Motion Anything: Any to Motion Generation](https://arxiv.org/abs/2503.06955) | Rui Zhao, Danning Li, Wei Mao, Yiran Wang, SteveZeyuZhang | - Motion Anything is a multimodal motion generation framework using an Attention-based Mask Modeling approach for fine-grained spatial and temporal control over generated human motion. - The model architecture includes Temporal and Spatial Adaptive Transformers to align motion sequences with text, music, or both, enabling controllable generation under multimodal conditions.  - A new dataset, Text-Music-Dance (TMD), containing 2,153 text, music, and dance pairs, is introduced to facilitate research in multi-conditional motion generation.  - Experiments on HumanML3D, KIT-ML, AIST++, and TMD demonstrate a 15% FID improvement on HumanML3D and consistent gains on other benchmarks, outperforming state-of-the-art methods. - The framework excels in generating diverse and high-quality motion while aligning well with given text, music, and multimodal conditions, exceeding the performance of existing single or multi-task models. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [Quantizing Large Language Models for Code Generation: A Differentiated
  Replication](https://arxiv.org/abs/2503.07103) | Gabriele Bavota, Saima Afrin, Antonio Mastropaolo, mdiipenta, Devy1 | - This paper investigates the impact of low-bit quantization using Additive Quantization with Learned Multi-Codebooks (AQLM) on the performance of large language models (LLMs) for code generation in Python and Java. - The authors quantize CodeLlama (7B, 13B, 34B) and DeepSeek Coder (1B, 7B, 33B) to 8, 4, 3, and 2 bits per parameter and evaluate their performance on MultiPL-E and McEval benchmarks. - They find that 4-bit quantization is a "safe bet", reducing memory footprint by 70% without significant performance loss, and that code-specific calibration datasets improve performance at extreme quantization levels (3 and 2 bits). - Larger models exhibit greater resilience to information loss during extreme quantization. - Post-quantization fine-tuning improves the performance of 2-bit quantized models, particularly smaller ones. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/codellama), [Link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) |
| [WildIFEval: Instruction Following in the Wild](https://arxiv.org/abs/2503.06573) | Liat Ein-Dor, Ariel Gera, Asaf Yehudai, Gili Lior | - This paper introduces WILDIFEVAL, a new dataset of 12K real-world user instructions focusing on multi-constraint text generation tasks. - The dataset allows for fine-grained analysis of LLM performance on complex instructions and diverse constraint types. - The paper analyzes the dataset and benchmarks 14 different large language models (LLMs) using a novel evaluation method focusing on relative constraint fulfillment. - The results show that all tested LLMs struggle with length-based constraints and experience performance degradation as the number of constraints increases. - This work highlights the limitations of current state-of-the-art LLMs and provides a valuable resource for advancing research in constrained text generation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/gililior/wild-if-eval-code) | [Link](https://huggingface.co/datasets/gililior/wild-if-eval) |
| [VLog: Video-Language Models by Generative Retrieval of Narration
  Vocabulary](https://arxiv.org/abs/2503.09402) | Mike Zheng Shou, KevinQHLin | - VLog is a novel video understanding framework that generates concise video narrations using a generative retrieval model, offering a new perspective on video understanding. - It leverages a narration vocabulary, contrasting with subword vocabularies in existing generative video-language models, enabling faster decoding times (up to 20x) when processing videos. - This framework uses a novel generative retrieval architecture, combines language model's reasoning capabilities with contrastive retrieval's efficient similarity search, and employs hierarchical vocabulary indexing for efficiency. -  A vocabulary update strategy is also introduced to handle novel events during inference. - Experiments on VidCab-Eval, EgoSchema, COIN, and HiREST demonstrate that VLog can generate concise, contextually accurate, and efficient narrations, outperforming baselines on casual retrieval tasks while being significantly faster than generative models. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/showlab/VLog) | N/A |
| [Cost-Optimal Grouped-Query Attention for Long-Context LLMs](https://arxiv.org/abs/2503.09579) | Maosong Sun, Zhiyuan Liu, Xu Han, Yutong Wu, chen-yingfa | - This paper introduces a cost-optimal Grouped-Query Attention (GQA) method for long-context Large Language Models (LLMs), decoupling the number of attention heads from the model's hidden dimension, enabling flexible compute allocation to the attention operator. - The authors modify existing scaling laws to account for context length and attention head configuration, modeling language modeling quality as a function of compute and memory costs. - They establish that loss is a power-plus-constant function of attention heads, enabling loss prediction before training. - Experiments show that commonly used GQA configurations can be suboptimal. For instance, with Llama-3.2-1B at 128K context length, using fewer attention heads and a larger model can reduce training and inference FLOPs and memory by almost 50% without increasing loss. - This work provides insights into developing practical LLMs, especially in long-context scenarios. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) |
| [Self-Taught Self-Correction for Small Language Models](https://arxiv.org/abs/2503.08681) | Irina Nikishina, Chris Biemann, VityaVitalich | - This paper introduces Self-Taught Self-Correction (STaSC), a novel algorithm enabling small language models (SLMs) to self-correct using only self-generated data, eliminating the need for external tools or large models. - STaSC iteratively refines model outputs through answer sampling, correction generation, filtering of successful corrections, and fine-tuning on these corrections, thereby improving initial answer quality. - Experiments on the Natural Questions dataset demonstrate that STaSC significantly improves SLMs' self-correction capabilities, even boosting initial answer quality despite training solely on corrections. - The authors provide and analyze the impact of various STaSC design choices, including initial answer exploration, correction filtering, and fine-tuning strategies. - Open-source code and lightweight models are released to foster further research on self-correction in SLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/VityaVitalich/STASC) | N/A |
| [MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented
  Generation System](https://arxiv.org/abs/2503.09600) | Simin Niu, Hanyu Wang, Zhaoxin Fan, Zhiyuan Ji, Robot2050 | - This paper introduces MoC (Mixture-of-Chunkers), a framework designed to improve the chunking process within Retrieval-Augmented Generation (RAG) systems. - MoC addresses the limitations of traditional and semantic chunking methods by incorporating a three-stage process involving a multi-granularity-aware router, specialized meta-chunkers, and a post-processing algorithm. - The router dynamically selects the appropriate chunker based on input text granularity, while meta-chunkers generate regular expressions for chunk extraction, optimizing computational efficiency. - An edit distance recovery algorithm refines the generated content, ensuring accurate chunk extraction. - Experimental results across multiple QA datasets demonstrate that MoC enhances chunking quality and improves RAG system performance compared to baseline methods and state-of-the-art LLM-based approaches. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Multimodal Language Modeling for High-Accuracy Single Cell
  Transcriptomics Analysis and Generation](https://arxiv.org/abs/2503.09427) | Xiang Wang, Junfeng Fang, Sihang Li, Jiaqi Yang, Yaorui Shi | - This paper introduces scMMGPT, a novel multimodal pre-trained transformer for analyzing and generating single-cell transcriptomics data. - scMMGPT integrates a cell-based PLM (scGPT) with a text-based PLM (Llama-2) using cross-modal projectors to facilitate information exchange. - Pre-trained on 27 million cells, scMMGPT excels in cell description generation with an 84% improvement in textual discrepancy and a 97% reduction in Earth Mover's Distance over baselines. - It also achieves state-of-the-art performance in text-conditioned pseudo-cell generation with a 4% k-NN accuracy improvement, and a 20.5% higher accuracy for cell type annotation. - Ablation studies validate the contribution of key components, especially the importance of integrating both cellular and textual data through the unified framework. | ['Multimodal', 'Text Generation', 'Natural Language Processing'] | [Link](https://github.com/syr-cn/SCMMGPT) | N/A |
| [When Large Vision-Language Model Meets Large Remote Sensing Imagery:
  Coarse-to-Fine Text-Guided Token Pruning](https://arxiv.org/abs/2503.07588) | Qi Zhu, Kang Wu, Xue Yang, Yingying Zhang, Junwei Luo | - This paper proposes a novel text-guided token pruning method for efficient vision-language understanding of large Remote Sensing Images (RSIs), addressing the challenge of information loss and high computational cost with traditional grid-based methods. - The method introduces a Region Focus Module (RFM) to identify text-relevant key vision tokens and a Dynamic Image Pyramid (DIP) for coarse-to-fine image tile selection and token pruning. - A new benchmark, LRS-VQA, featuring 7,333 question-answer pairs across 8 categories with image lengths up to 27,328 pixels, is also introduced to reflect the challenges of large RSIs perception.  - The proposed method demonstrates performance improvements and efficiency gains compared to existing high-resolution strategies and token reduction methods on various datasets, including achieving higher accuracy on all four LRS-VQA subsets compared to adapted general LVLMs like EarthDial and GeoPixel, as well as improved accuracy and FPS on MME-RealWorld-RS compared to other token pruning methods. - The method is architecture-agnostic, and ablation studies validate the effectiveness of its core components: dynamic tile selection with DIP, text-guided key region localization with RFM, and token pruning strategy. | ['Computer Vision', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/VisionXLab/LRS-VQA) | N/A |
| [Multi Agent based Medical Assistant for Edge Devices](https://arxiv.org/abs/2503.05397) | Pragya Sahu, Jagdish Samant, Chinmay Kulkarni, Shivam Akhouri, Sakharam Gawade | - This research introduces an on-device, multi-agent healthcare assistant that addresses privacy, latency, and internet dependency challenges posed by traditional cloud-based Large Action Models (LAMs) in healthcare applications. - The system utilizes smaller, task-specific agents like Planner and Caller, powered by the Qwen Code Instruct 2.5 7B model, optimized for resource allocation and on-device execution. - These agents achieve average RougeL scores of 85.5 for planning and 96.5 for calling and handle tasks such as intelligent diagnosis and appointment scheduling, emergency SOS, vitals tracking, and daily health reporting. - The multi-agent architecture enables modular collaboration, with each agent functioning independently while working harmoniously for complex workflows, and facilitates system scaling through additional agents. - A user-friendly application integrates the multi-agent system with smartwatches, enhancing personalized data retrieval and agent capabilities through retrieval augmented generation. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct) |


## Papers for 2025-03-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural
  Vision-Language Dataset for Southeast Asia](https://arxiv.org/abs/2503.07920) | davidanugraha, rifqifarhansyah, tackhwa, holylovenia, samuelcahyawijaya | - SEA-VL is a new open-source initiative focused on developing high-quality, culturally relevant vision-language datasets for Southeast Asian languages, addressing the underrepresentation of these languages in AI research. - This initiative involves three image collection strategies: crowdsourcing from individuals within SEA, crawling existing online image sources, and generating synthetic images using AI models. - SEA-VL analyzed the trade-offs between manual and automated data collection methods and found that image crawling offered a good balance of cultural relevance (~85%) and efficiency. - While crowdsourcing yielded the most culturally relevant data, it was the most time-consuming and resource-intensive method. Image generation proved inadequate for capturing the cultural nuances of SEA.  - The resulting SEA-VL dataset is the largest of its kind for the region, exceeding existing datasets by more than 50 times and aiming to facilitate more culturally aware and inclusive AI systems. | ['Image-to-Text', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/SEACrowd/sea-vl-experiments) | [Link](https://huggingface.co/collections/SEACrowd/sea-vl-multicultural-vl-dataset-for-southeast-asia-67cf223d0c341d4ba2b236e7) |
| [LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through
  Two-Stage Rule-Based RL](https://arxiv.org/abs/2503.07536) | Jie Liu, Zhiyuan You, Miaosen Zhang, Gongrui Zhang, Yingzhe Peng | - LMM-R1 is a two-stage framework designed to improve the multimodal reasoning capabilities of Large Multimodal Models (LMMs), especially those with limited parameters (e.g., 3B), using rule-based reinforcement learning (RL). - The first stage, Foundational Reasoning Enhancement (FRE), uses text-only data with rule-based RL to bolster the model's core reasoning skills. - The second stage, Multimodal Generalization Training (MGT), generalizes this enhanced reasoning to multimodal tasks.  - Experiments on the 3B parameter Qwen2.5-VL-Instruct model show average improvements of 4.5% and 4.83% on text-only and multimodal benchmarks, respectively, demonstrating the framework's effectiveness. - Notably, LMM-R1 achieves a 3.63% performance gain on complex Football Game tasks, further showcasing its ability to improve real-world applicable reasoning skills. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/TideDra/lmm-r1) | N/A |
| [YuE: Scaling Open Foundation Models for Long-Form Music Generation](https://arxiv.org/abs/2503.08638) | HKUST-Audio, Liam-Liu, dododododo, zhangysk, a43992899 | - YuE is a family of open-source foundation models based on LLaMA2 for generating high-quality, long-form (up to 5 minutes) music from lyrics and control signals. - It uses a track-decoupled next-token prediction strategy, modeling vocal and accompaniment tracks separately, and structural progressive conditioning to handle long lyrical sequences, addressing coherence and alignment challenges. - A novel in-context learning framework allows versatile style transfer, voice cloning, and bidirectional content creation. - Human evaluations show YuE matches or surpasses some commercial systems in musicality and controllability, particularly excelling in vocal agility and duration. - YuE achieves state-of-the-art performance on the GS key recognition task within the MARBLE benchmark and shows competitive results in other music understanding tasks, demonstrating the quality of its learned representations. | ['Text-to-Audio', 'Audio'] | [Link](https://github.com/multimodal-art-projection/YuE) | N/A |
| [UniF^2ace: Fine-grained Face Understanding and Generation
  with Unified Multimodal Models](https://arxiv.org/abs/2503.08120) | Liya Guo, Linrui Xu, Xuerui Qiu, delinqu, tulvgengenr | - UniF^2ace is a unified multimodal model (UMM) designed for fine-grained face understanding and generation, addressing limitations of existing models that handle coarse facial attributes or treat understanding and generation as separate tasks. - It leverages a novel dual discrete diffusion (D3Diff) training strategy, connecting score matching and masked generative models, and a multi-level grouped Mixture-of-Experts (MoE) architecture for fine-grained facial feature processing. - A new dataset, UniF^2ace-130K, containing 130K facial image-text pairs and one million visual question-answering (VQA) pairs spanning 46 attributes, is introduced to support the model's training and evaluation. - Experimental results on UniF^2ace-130K show that UniF^2ace outperforms existing UMMs and generative models on various metrics for both understanding and generation, achieving state-of-the-art performance for models of similar size. - Qualitative analysis demonstrates UniF^2ace's ability to capture fine-grained facial details from text, generating more realistic face images and providing accurate descriptions for complex facial attributes. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by
  Imitating Human Annotator Trajectories](https://arxiv.org/abs/2503.08625) | Qingpei Guo, Chunluan Zhou, Hao Chen, Yuzhuo Tian, Z-MU-Z | - This paper introduces SegAgent, a new approach for image segmentation that leverages Multimodal Large Language Models (MLLMs) by mimicking human annotators using interactive segmentation tools. - The proposed method models the segmentation task as a multi-step Markov Decision Process, enabling MLLMs to iteratively generate text-based click points to refine segmentation masks. - SegAgent achieves performance comparable to state-of-the-art methods on referring segmentation datasets and also supports additional tasks like mask refinement and annotation filtering. - A new dataset, High-quality Referring Expression Segmentation (HRES), is also introduced in this work, to evaluate the decision making capabilities of MLLMs on more complex dataset. - The paper further explores enhancement techniques like policy improvement with StaR+ and tree search with process reward modeling (PRM), which further improve SegAgent's performance, especially in complex segmentation scenarios. | ['Image Segmentation', 'Multimodal'] | [Link](https://github.com/aim-uofa/SegAgent) | N/A |
| [Gemini Embedding: Generalizable Embeddings from Gemini](https://arxiv.org/abs/2503.07891) | Madhuri Shanbhogue, Daniel Cer, Sahil Dua, Feiyang Chen, Jinhyuk Lee | - This paper introduces Gemini Embedding, a new embedding model initialized from Google's Gemini large language model and trained on a diverse set of embedding tasks. - Gemini Embedding leverages Gemini's multilingual and code understanding capabilities to generate generalizable embeddings for various text modalities and over 100 languages. - It outperforms state-of-the-art models on the Massive Multilingual Text Embedding Benchmark (MMTEB), achieving a score of 68.32, a +5.09 improvement over the second-best model. - It also demonstrates exceptional performance on other benchmarks like XOR-Retrieve for cross-lingual retrieval. - The model uses a contrastive learning objective and incorporates task prompts and a pre-finetuning stage to enhance performance and is made available publicly via an API. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/spaces/mteb/leaderboard) |
| [Implicit Reasoning in Transformers is Reasoning through Shortcuts](https://arxiv.org/abs/2503.07604) | Deqing Yang, Siyu Yuan, Tianhe Lin, hsaest | - This paper investigates the implicit reasoning mechanism in Transformers, revealing that they rely on shortcuts, especially when trained on fixed-pattern data. - These shortcuts are effective for in-domain and out-of-domain generalization when the premise order is fixed, achieving near-perfect accuracy. - However, when trained on data with unfixed premise order, the model overfits to shortcut patterns, failing to generalize and exhibiting poor performance, specifically struggling with "Variable as Subtrahend Plight." - The paper suggests that current LLMs' implicit reasoning capability is limited by their reliance on shortcuts rather than true step-by-step reasoning. - This limitation is observed in state-of-the-art large language models (LLMs) as well, emphasizing the need for future research to develop methods that encourage true reasoning capabilities in LMs. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [OmniMamba: Efficient and Unified Multimodal Understanding and Generation
  via State Space Models](https://arxiv.org/abs/2503.08686) | Xinggang Wang, Wenyu Liu, Qian Zhang, Bencheng Liao, Jialv Zou | - OmniMamba is a unified multimodal model based on a linear state-space model (Mamba-2), enabling both understanding and generation tasks (including text-to-image) with a single model. - It uses decoupled encoders, vocabularies, and task-specific LoRA modules for parameter-efficient adaptation to different modalities and tasks. - Trained on only 2M image-text pairs, OmniMamba achieves competitive performance with JanusFlow and surpasses Show-o on various multimodal benchmarks, using 1000x less training data than Show-o. - Notably, OmniMamba demonstrates exceptional inference speed, achieving up to a 119.2x speedup and a 63% GPU memory reduction for long sequences compared to Transformer-based models. - On MS-COCO, it achieves state-of-the-art visual generation results (FID 5.50) compared to other unified and generation-specific models. | ['Multimodal', 'Text-to-Image', 'Visual Question Answering'] | [Link](https://github.com/hustvl/OmniMamba) | N/A |
| [Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.07572) | Edward Emanuel Beeching, Lewis Tunstall, Amrith Setlur, Matthew Y. R. Yang, CohenQu | - This paper introduces Meta Reinforcement Fine-Tuning (MRT), a new method for optimizing large language models (LLMs) to efficiently utilize test-time compute for improved reasoning. - MRT frames the optimization problem as a meta-reinforcement learning problem, where the LLM's output stream is segmented into episodes, enabling a principled perspective on resource allocation. - By minimizing cumulative regret, a measure of the difference between the LLM's performance and an optimal comparator, MRT balances exploration and exploitation in the output token sequence, leading to efficient progress. -  MRT prescribes a dense reward bonus during training, quantifying progress by the change in likelihood of eventual success after each generated episode. - Experiments on math reasoning tasks using DeepScaleR, DeepSeek, and Llama models demonstrate that MRT achieves 2-3x relative performance gains and 1.5-1.7x improvements in token efficiency compared to standard outcome-reward RL fine-tuning. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [BiasEdit: Debiasing Stereotyped Language Models via Model Editing](https://arxiv.org/abs/2503.08588) | Julian McAuley, Ningyu Zhang, Wei Xu, XinXuNLPer | - BIASEDIT, a novel model editing method, is proposed to mitigate stereotypical biases in language models by using lightweight editor networks to generate parameter updates, focusing on local edits of partial parameters.  - The architecture employs a debiasing loss to guide these edits, along with a retention loss to preserve general language modeling capabilities. - Experiments on StereoSet and Crows-Pairs demonstrate that BIASEDIT outperforms existing debiasing methods by achieving lower Stereotype Scores (SS) while minimizing impact on Language Modeling Scores (LMS).  - BIASEDIT exhibits robustness to gender reversal and semantic generality, indicating broader applicability.  - Further analysis shows that edits to upper model blocks have fewer negative impacts on modeling abilities compared to edits on lower blocks. | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/BiasEdit) | [Link](https://huggingface.co/openai-community/gpt2-medium), [Link](https://huggingface.co/google/gemma-2b), [Link](https://huggingface.co/mistralai/Mistral-7B-v0.3), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B) |
| [^RFLAV: Rolling Flow matching for infinite Audio Video generation](https://arxiv.org/abs/2503.08307) | Claudio Ferrari, Tomaso Fontanini, Filippo Botti, Giuseppe Gabriele Tarollo, MaverickAlex | - RFLAV, a transformer-based model for generating infinite-length audio-video (AV) sequences, is introduced, addressing key challenges in AV generation such as quality, synchronization, and temporal coherence. - The model uses a rolling flow matching framework and novel cross-modality interaction modules to align audio and video during training. - It bypasses the need for audio/video encoders, enabling variable-length video generation and avoiding restrictions imposed by fixed encoder output sizes. - Experimental results show that RFLAV outperforms existing state-of-the-art models on standard AV generation benchmarks like AIST++ and Landscape. - Further analysis demonstrates RFLAV's ability to maintain quality and avoid repetitive loops in extended video sequences, making it a significant advancement in the field of infinite AV generation. | ['Text-to-Video', 'Audio', 'Multimodal'] | [Link](https://github.com/ErgastiAlex/R-FLAV) | N/A |
| [QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long
  Video Comprehension](https://arxiv.org/abs/2503.08689) | Shukang Yin, Weizhong Huang, Xiawu Zheng, Wang Chen, Yongdong Luo | - QuoTA, a training-free modular extension for Large Video-Language Models (LVLMs), enhances long video comprehension through query-oriented visual token assignment. - QuoTA strategically assesses frame-level importance based on query relevance using a lightweight scoring LVLM, enabling efficient token allocation before cross-modal interactions. - Using Chain-of-Thoughts reasoning, QuoTA decouples complex queries into more interpretable questions for enhanced frame scoring precision by the scoring LVLM.  - QuoTA with LLaVA-Video-7B achieves an average 3.2% performance improvement across six benchmarks, including Video-MME and MLVU, while maintaining the same token budget as the baseline.  - QuoTA also consistently outperforms recent state-of-the-art token reduction methods (AIM and FrameFusion) across various token budget configurations. | ['Video-Text-to-Text', 'Multimodal', 'Computer Vision', 'Video Classification'] | [Link](https://github.com/MAC-AutoML/QuoTA) | N/A |
| [Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents](https://arxiv.org/abs/2503.08684) | Xiao Zhang, Liang Pang, Haiyuan Zhao, Sunhao Dai, Haoyu Wang | - This paper introduces Causal Diagnosis and Correction (CDC), a novel debiasing method for Pretrained Language Model (PLM)-based retrievers to address source bias, which is the tendency of these retrievers to favor LLM-generated content due to its lower perplexity. - The study identifies perplexity as a causal factor contributing to source bias, confirmed through intervention experiments and two-stage least squares regression analysis demonstrating that lower perplexity leads to higher relevance scores, irrespective of semantic quality. - A theoretical analysis of Masked Language Modeling (MLM) and retrieval tasks reveals a positive correlation in their gradients, indicating that retrievers inadvertently incorporate perplexity during relevance estimation. - This positive correlation explains the observed trade-off between retrieval performance and source bias, where higher performance correlates with increased bias due to greater sensitivity to perplexity. - CDC operates at inference time, calibrating relevance scores by separating the biased influence of perplexity and shows robust debiasing effectiveness across diverse datasets and LLMs without requiring retriever retraining. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/WhyDwelledOnAi/Perplexity-Trap) | N/A |
| [Benchmarking AI Models in Software Engineering: A Review, Search Tool,
  and Enhancement Protocol](https://arxiv.org/abs/2503.05860) | Maliheh Izadi, philippedebekker, RohamKoohestani | - This paper introduces BenchFrame, a unified method to enhance benchmark quality for AI4SE models, along with BenchScout, a semantic search tool to find relevant benchmarks. - The researchers conducted a systematic review of 173 studies, identifying 204 AI4SE benchmarks, classifying them, and analyzing their limitations. - A user study with 22 participants demonstrated BenchScout's usability, effectiveness, and intuitiveness, with average scores of 4.5, 4.0, and 4.1 out of 5, respectively. - Applying BenchFrame to HumanEval resulted in HumanEvalNext, addressing limitations such as errors, language conversion, test coverage, and difficulty. - Evaluating ten state-of-the-art code language models showed a pass@1 score reduction of 31.22% and 19.94% on HumanEvalNext compared to HumanEval and HumanEvalPlus. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Summarization', 'Translation'] | [Link](https://github.com/AISE-TUDelft/AI4SE-benchmarks) | [Link](https://huggingface.co/datasets/openai/openai_humaneval), [Link](https://huggingface.co/datasets/codeparrot/instructhumaneval) |
| [Evaluating Intelligence via Trial and Error](https://arxiv.org/abs/2502.18858) | Bo Zhang, Yiqun Liu, Jiayu Li, Jiahao Zhao, jingtao | - This paper introduces "Survival Game", a framework inspired by Natural Selection to evaluate intelligence based on the number of failed attempts in a trial-and-error process. - The framework categorizes intelligence into three levels: Limited, Capable, and Autonomous, based on the convergence of the expectation and variance of failure counts. - Through comprehensive evaluation of existing AI systems on various tasks including vision, search, recommendation, and language, the study finds that while AI reaches the Autonomous Level in simple tasks, it falls short in complex ones, often remaining at the Limited Level. - The paper projects that achieving Autonomous Level for general tasks would require an astronomical 10^26 parameters, highlighting the gap between current AI and human intelligence. - A theoretical analysis suggests that human tasks possess "criticality," demanding deep understanding of underlying mechanisms, which current AI systems, relying on superficial mimicry, lack, thus explaining the difficulty in achieving Autonomous Level. | ['Computer Vision', 'Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/jingtaozhan/IntelligenceTest) | N/A |
| [Referring to Any Person](https://arxiv.org/abs/2503.08507) | Yuda Xiong, Tianhe Ren, Zhaoyang Zeng, Lin Wu, Qing Jiang | - This paper introduces RexSeek, a novel detection-oriented multimodal large language model for the task of "referring to any person," which involves detecting all individuals in an image matching a given natural language description. - RexSeek integrates a person detector for robust perception and Qwen2.5 as the large language model (LLM) for enhanced language comprehension. - The model is trained using a four-stage approach, including image-captioning, detection-oriented data, multimodal data, and finally, the HumanRef dataset. - Experimental results on the HumanRef benchmark show that RexSeek outperforms existing state-of-the-art models, particularly in multi-instance referring scenarios where multiple individuals match the description. - RexSeek also demonstrates generalization capabilities for referring to arbitrary objects beyond human-centric tasks, highlighting its potential for broader applications in vision-language tasks. | ['Object Detection', 'Multimodal'] | [Link](https://github.com/IDEA-Research/RexSeek) | N/A |
| [AI-native Memory 2.0: Second Me](https://arxiv.org/abs/2503.08102) | Jingbo Shang, Felix Tao, Tao Gao, Xiang Ying, Jiale Wei | - SECOND ME is introduced as an AI-native, persistent memory offload system designed to enhance human-computer interaction by reducing redundant information exchange. - SECOND ME leverages LLM-based memory parameterization for structured organization, contextual reasoning, and adaptive knowledge retrieval, acting as a personalized intermediary. - The system employs supervised fine-tuning (SFT) and direct preference optimization (DPO) to improve LLM performance on tasks such as memory-based Q&A, context completion, and context critique. - An automated data synthesis strategy integrates local and global data perspectives using a multi-agent framework and Chain-of-Thought (CoT) reasoning for enhanced performance. - Experimental results demonstrate that diverse data sources with strong CoT normalization and DPO lead to significant performance improvements in automated evaluations, with human case studies suggesting even greater real-world effectiveness. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/Mindverse/Second-Me) | N/A |
| [Mixture of Experts Made Intrinsically Interpretable](https://arxiv.org/abs/2503.07639) | Puneet K. Dokania, Christian Schroeder de Witt, Ashkan Khakzar, Constantin Venhoff, Xingyi Yang | - This paper introduces MoE-X, a Mixture-of-Experts (MoE) language model designed for intrinsic interpretability by leveraging sparsity and width in the model architecture. - MoE-X consists of ReLU experts and employs sparsity-aware routing, ensuring only the most relevant and interpretable experts are activated during inference. - Evaluating MoE-X on chess and natural language tasks demonstrates that it maintains performance comparable to dense models while enhancing interpretability. - Notably, MoE-X surpasses the interpretability of sparse autoencoder (SAE) methods without sacrificing performance and achieves perplexity better than GPT-2 on language modeling tasks. - MoE-X successfully disentangles polysemantic features and clusters related concepts within individual experts, offering a more transparent and interpretable model. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/adamkarvonen/chess_games), [Link](https://github.com/EleutherAI/sae-auto-interp) |
| [Beyond Decoder-only: Large Language Models Can be Good Encoders for
  Machine Translation](https://arxiv.org/abs/2503.06594) | Qinghong Zhang, Bei Li, Yongyu Mu, Tong Zheng, luoyingfeng | - This paper introduces LaMaTE (Large Language Models as Machine Translation Encoders), a novel architecture for machine translation that utilizes LLMs as encoders coupled with a lightweight NMT decoder. - LaMaTE incorporates an adaptor module to bridge the gap between the LLM encoder and the NMT decoder, enhancing representation alignment and facilitating training. - A two-stage training process is proposed, where the adaptor and decoder are pre-trained initially, followed by fine-tuning of all model parameters, enabling efficient learning and knowledge retention. - A new comprehensive benchmark dataset, ComMT, is introduced to evaluate machine translation models across various tasks, including general translation, document-level translation, domain-specific translation, terminology-constrained translation, and automatic post-editing. - Experimental results demonstrate that LaMaTE achieves state-of-the-art performance on the ComMT benchmark, showing significant improvements in both translation quality and efficiency, with 2.4x to 6.5x faster decoding speeds and a 75% reduction in KV cache memory compared to traditional LLM-based methods. | ['Translation', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/NiuTrans/LaMaTE) |
| [VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large
  Vision-Language Models in Fact-Seeking Question Answering](https://arxiv.org/abs/2503.06492) | Lixin Liu, Shasha Guo, Xiaodong Chen, Yihan Zhao, WYLing | - This paper introduces VisualSimpleQA, a multimodal fact-seeking question answering benchmark designed for decoupled evaluation of visual and linguistic modules in large vision-language models (LVLMs). - The benchmark facilitates detailed analysis of LVLMs by including text-only questions paired with multimodal questions derived from images, and rationales and difficulty criteria for improved evaluation. - The authors evaluate 15 state-of-the-art LVLMs and show that even top-performing models like GPT-40 achieve only around 60% accuracy on VisualSimpleQA and 30% on a harder subset (VisualSimpleQA-hard), indicating significant room for improvement.  - The decoupled evaluation process reveals substantial performance differences across LVLMs, particularly in their visual recognition capabilities. - VisualSimpleQA aims to promote research and development in the field of LVLMs to enhance factuality in multimodal question answering. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/WYLing/VisualSimpleQA) |


## Papers for 2025-03-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Feature-Level Insights into Artificial Text Detection with Sparse
  Autoencoders](https://arxiv.org/abs/2503.03601) | Kristian Kuznetsov, natriistorm, razzant, plina2polina, Kushnareva | - This paper investigates the interpretability of Artificial Text Detection (ATD) using Sparse Autoencoders (SAEs) applied to the residual stream of the Gemma-2-2b model. - The study introduces a categorization of extracted features into discourse, noise, and style features, offering valuable insights into how machine-generated text differs from human-written content. - The authors analyze the semantics and relevance of these features through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. - The results demonstrate that SAE-derived features effectively detect artificial text, sometimes outperforming existing methods, with certain features showing strong generalizability across domains and models, while others exhibit domain- or model-specific performance. - The analysis reveals that modern LLMs have distinct writing styles, particularly in information-dense domains, making their text detectable, but adversarial techniques using less formal prompts can make generated text more human-like and harder to detect. | ['Natural Language Processing', 'Feature Extraction'] | N/A | [Link](https://mgtsaevis.github.io/
mgt-sae-visualization/) |
| [MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale
  Reinforcement Learning](https://arxiv.org/abs/2503.07365) | wangwhcore, friskit, hflqf88888, Cierra0506, FanqingM | - The paper introduces MM-Eureka, a multimodal reasoning model that extends large-scale rule-based reinforcement learning to multimodal reasoning tasks. - MM-Eureka successfully reproduces key characteristics of text-based RL systems in the multimodal space, including steady increases in accuracy, reward, and response length. - The model demonstrates strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showcasing superior data efficiency compared to alternative approaches. - MM-Eureka exhibits "visual aha moments", where the model re-examines intermediate steps using visual information to improve accuracy. - The authors open-source their complete pipeline, including code, models, and data, to facilitate further research in this area. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/ModalMinds/MM-EUREKA) | N/A |
| [SEAP: Training-free Sparse Expert Activation Pruning Unlock the
  Brainpower of Large Language Models](https://arxiv.org/abs/2503.07605) | Xun Liang, BO1022, Ki-Seki, siminniu, UglyToilet | - This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method for Large Language Models (LLMs) that selectively retains task-relevant parameters to reduce inference overhead. - Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model accordingly. - SEAP dynamically adjusts sparsity based on task type, leading to improved efficiency and preserved performance.  - Experimental results show that SEAP significantly reduces computational overhead while maintaining accuracy comparable to the dense model, outperforming existing baselines like WandA and FLAP by over 20% at 50% pruning and incurring only a 2.2% performance drop at 20% pruning. - The method involves constructing task-specific knowledge corpora, analyzing activation patterns, computing neuron importance scores, dynamically distributing sparsity, and applying task-specific pruning strategies. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/IAAR-Shanghai/SEAP) | N/A |
| [Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue
  Learning](https://arxiv.org/abs/2503.07002) | Zongqing Lu, Jiazheng Liu, tellarin, sipeng9527 | - Introduces MMDiag, a multi-turn multimodal dialogue dataset with strong correlations between questions, images, and image regions, generated using rules and GPT assistance. - Presents DiagNote, an MLLM with multimodal grounding and reasoning capabilities, using Deliberate and Gaze modules for Chain-of-Thought and annotation, respectively. - DiagNote demonstrates improved grounding and joint reasoning with vision and language information compared to existing MLLMs. - MMDiag serves as a challenging benchmark for multi-turn multimodal dialogue learning, focusing on saliency tracking and recall. - Empirical results show DiagNote's advantages in handling complex multi-turn dialogues and reasoning tasks. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Automated Movie Generation via Multi-Agent CoT Planning](https://arxiv.org/abs/2503.07314) | Zeyu Zhu, AnalMom, weijiawu | - MovieAgent, a novel framework for automated movie generation, leverages multi-agent Chain of Thought (CoT) planning to automate the creation of multi-scene, multi-shot videos from a script synopsis and character bank. - This hierarchical framework utilizes specialized LLM agents, simulating roles like director, screenwriter, and storyboard artist, to handle high-level narrative structuring and low-level cinematography, ensuring narrative coherence, character consistency, and synchronized subtitles. - MovieAgent introduces a hierarchical CoT reasoning process for automated scene structuring, camera settings, and cinematography, significantly reducing manual effort compared to traditional filmmaking and existing video generation methods. - Experimental results on the MoviePrompts dataset demonstrate MovieAgent's state-of-the-art performance in automated storytelling and movie generation, excelling in narrative coherence and character consistency. - The framework provides new insights into fully automated movie generation and offers a scalable solution for AI-driven storytelling. | ['Text-to-Video', 'Multimodal'] | [Link](https://github.com/showlab/MovieAgent) | N/A |
| [FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA
  Subparameter Updates](https://arxiv.org/abs/2503.07216) | Sung Ju Hwang, matbambbang, Seanie-lee, Sangsang | - FedRand is a privacy-enhanced federated learning framework for vision-language models (VLMs) that involves randomly selecting and updating a subset of LoRA parameters, while keeping the remaining parameters private on the client-side. - It addresses the vulnerability of VLMs to membership inference attacks in federated learning settings by reducing the exposure of client model parameters. - FedRand achieves comparable performance to FedAvg, the oracle method, on several benchmark datasets (ScienceQA, MSCOCO, and NoCaps) for visual question answering and image captioning tasks. - It improves robustness against membership inference attacks compared to other baselines like FedPer and FedPara, which employ partial parameter sharing. - The method reduces communication costs by approximately 25% compared to FedAvg by transmitting only a subset of updated parameters back to the server. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Image Feature Extraction'] | N/A | N/A |
| [FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation
  for Feature Implementation](https://arxiv.org/abs/2503.06680) | Wei Li, lisijia0504, yangyu90, dawnmsg, CharonBony | - FEA-Bench, a benchmark designed to assess Large Language Models' (LLMs) ability to perform incremental development of new features within existing code repositories. - It comprises of 1401 task instances derived from pull requests across 83 GitHub repositories, focusing specifically on adding new components. -  Each task instance includes a feature request, definitions of new components, environment setup details, a patch describing code changes, and corresponding unit test files for verification. - The benchmark tasks involve a significant amount of code modification, with an average of 128.5 lines changed per instance, making it more complex than existing benchmarks. -  Experimental results show that current LLMs struggle with these complex, repository-level tasks, with even the best model, DeepSeek-R1 only achieving a success rate of 9.92%. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/microsoft/FEA-Bench) | N/A |
| [AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via
  Reinforcement Learning and Reasoning](https://arxiv.org/abs/2503.07608) | Qian Zhang, xinggangw, wenyuliu, Atan-0221, rb93dett | - AlphaDrive, a vision-language model (VLM) designed for high-level planning in autonomous driving, is introduced, integrating Group Relative Policy Optimization (GRPO)-based reinforcement learning (RL) with planning reasoning. - The model utilizes four novel GRPO rewards tailored for planning: planning accuracy, action-weighted, planning diversity, and planning format rewards.  - A two-stage training strategy is employed, combining supervised fine-tuning (SFT) for knowledge distillation from larger models with subsequent RL for planning exploration. - On the MetaAD driving dataset, AlphaDrive demonstrates a substantial improvement in planning accuracy, outperforming the SFT-trained model by 25.52% overall and by 35.31% with only 20% of the training data. -  Following RL training, AlphaDrive exhibits emergent multimodal planning capabilities, generating multiple feasible driving plans in complex scenarios, which holds significant potential for enhancing driving safety and efficiency. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/hustvl/AlphaDrive) | N/A |
| [Agent models: Internalizing Chain-of-Action Generation into Reasoning
  models](https://arxiv.org/abs/2503.06580) | Jitao Sang, Xinyan Wen, Jiangming Shu, tzteyang, TokerZ | - This paper introduces AutoCoA, a framework for training Large Agent Models (LAMs) that internalize Chain-of-Action (CoA) generation, allowing them to autonomously decide when and how to use tools. - AutoCoA combines supervised fine-tuning (SFT) and reinforcement learning (RL) to train LAMs to generate CoA by interleaving reasoning and actions, managing environment interactions efficiently. - The framework incorporates step-level action triggering, trajectory-level CoA optimization, and an internal world model to minimize real-world interaction costs. - Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models outperform ReAct-based workflows, especially in tasks requiring long-term reasoning and multiple actions, as evidenced by higher task completion rates. - The paper focuses on smaller reasoning models with search as a testbed, with future research planned for scaling to larger models, integrating more tools, and evaluating on open-ended tasks. | ['Question Answering', 'Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/ADAM-BJTU/AutoCoA) | N/A |
| [WritingBench: A Comprehensive Benchmark for Generative Writing](https://arxiv.org/abs/2503.05244) | SHaopeng Lai, Chenliang Li, Ming Yan, Jiahao Mei, AQuarterMile | - WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. - Proposes a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. - Complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format, and length. - Framework's validity is demonstrated by its data curation capability, enabling 7B-parameter models to approach state-of-the-art (SOTA) performance. - Open-sourced the benchmark, along with evaluation tools and modular framework components. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/X-PLUG/WritingBench) | N/A |
| [Vision-R1: Incentivizing Reasoning Capability in Multimodal Large
  Language Models](https://arxiv.org/abs/2503.06749) | Zheyu Ye, Shaosheng Cao, Zijie Zhai, Bohan Jia, Wenxuan Huang | - This paper introduces Vision-R1, a novel Multimodal Large Language Model (MLLM) designed to enhance reasoning capabilities in visual question answering by integrating cold-start initialization with reinforcement learning. - Vision-R1 leverages a two-stage training process: cold-start initialization using a 200K multimodal Chain-of-Thought (CoT) dataset generated via modality bridging and data filtering, followed by reinforcement learning using Group Relative Policy Optimization (GRPO) and Progressive Thinking Suppression Training (PTST). - PTST progressively loosens context length restrictions during training, enabling Vision-R1 to acquire increasingly complex reasoning processes. - Vision-R1-7B achieves 73.5% accuracy on MathVista, outperforming existing MLLMs with 10x more parameters and approaching OpenAI O1's performance. - The authors also demonstrate the effectiveness of their approach by achieving state-of-the-art results on other math reasoning benchmarks (MathVerse, MM-Math) and general multimodal benchmarks (MM-Star, ChartQA, MME, HallBench). | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Osilly/Vision-R1) | N/A |
| [SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and
  Multi-dimensional Evaluation for Automated Survey Writing](https://arxiv.org/abs/2503.04629) | Bin Wang, Renqiu Xia, Jiakang Yuan, Shiyang Feng, Xiangchao Yan | - SURVEYFORGE is an automated framework for generating survey papers that leverages LLMs, addressing the limitations of existing methods in outline quality and citation accuracy. - It employs a two-stage process: outline generation using heuristic learning from human-written surveys and topic-relevant literature, followed by content generation driven by a memory-driven scholar navigation agent. - The agent retrieves high-quality literature using a temporal-aware reranking engine, combining and refining content into a coherent survey. - A new benchmark, SurveyBench, featuring 100 human-written surveys and multi-dimensional evaluation metrics (reference, outline, and content quality), facilitates systematic assessment of generated surveys. - Experimental results show SURVEYFORGE outperforms the baseline AutoSurvey across all evaluation dimensions, demonstrating improvements in outline structure, reference quality, and content coherence. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Alpha-Innovator/SurveyForge) | [Link](https://huggingface.co/datasets/U4R/SurveyBench) |
| [LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted
  Contrastive Learning](https://arxiv.org/abs/2503.04812) | Jinsong Su, Jie Zhou, Fandong Meng, lqniu, zhibinlan | - LLaVE, a new framework for training large language and vision embedding models, addresses the challenge of overlapping similarity distributions between positive and negative pairs in existing LMM-based embedding models. - This framework utilizes hardness-weighted contrastive learning, assigning larger weights to harder negative pairs, and employs a cross-device negative sample gathering strategy to increase the number of negative pairs without substantial memory overhead. - LLaVE models, trained in various sizes (0.5B, 2B, and 7B), achieve state-of-the-art results on the MMEB benchmark across multiple tasks, including retrieval, visual question answering, and classification. - LLaVE-7B surpasses the previous best model by 6.2 points on the MMEB benchmark, demonstrating strong scalability and efficiency. - Despite being trained solely on image-text data, LLaVE generalizes well to zero-shot text-video retrieval tasks. | ['Multimodal', 'Image Feature Extraction'] | N/A | N/A |
| [MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning](https://arxiv.org/abs/2503.07459) | Jiapeng Chen, Jiwoong Sohn, Daniel Shao, wshi83, RTT1 | - This paper introduces MEDAGENTSBENCH, a new benchmark designed to evaluate complex medical reasoning capabilities of large language models (LLMs) and agent frameworks, focusing on challenging questions requiring multi-step reasoning and diagnosis formulation. - The benchmark addresses limitations of existing evaluations by using adversarially filtered questions from seven established medical datasets, ensuring diversity, and incorporating human annotations to verify reasoning depth. - Experiments reveal that thinking models like DEEPSEEK R1 and OPENAI 03 outperform traditional approaches by 15-25% on complex questions. - Advanced search-based agent methods, like AFLOW, offer the best performance-to-cost ratios compared to traditional approaches, approaching the accuracy of thinking models with fewer computational resources. - Open-source models show competitive performance at lower costs, with DEEPSEEK-R1 demonstrating comparable or superior accuracy to several closed-source alternatives at a fraction of the computational cost. | ['Question Answering'] | [Link](https://github.com/gersteinlab/medagents-benchmark) | N/A |
| [Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive
  Reinforcement](https://arxiv.org/abs/2503.06520) | Fanbin Lu, Zihao Yue, Zhisheng Zhong, Bohao Peng, Yuqi Liu | - Seg-Zero, a novel framework for reasoning segmentation, is introduced, demonstrating emergent test-time reasoning abilities through a pure reinforcement learning (RL) strategy. - Seg-Zero uses a decoupled architecture with a reasoning model (Qwen2.5-VL) and a segmentation model (SAM2), where the reasoning model generates a reasoning chain and positional prompts (bounding box and points) for the segmentation model to produce pixel-level masks. - The model is trained using GRPO with a sophisticated reward mechanism that integrates format and accuracy rewards to enhance the reasoning process and regulate outputs. - Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities, surpassing the previous LISA-7B by 18% on ReasonSeg, achieving 57.5% zero-shot. - This improvement highlights Seg-Zero's ability to generalize across domains while presenting explicit reasoning. | ['Image Segmentation', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/dvlab-research/Seg-Zero) | N/A |
| [This Is Your Doge, If It Please You: Exploring Deception and Robustness
  in Mixture of LLMs](https://arxiv.org/abs/2503.05856) | Ilija Bogunovic, Sangwoong Yoon, Llwo | - This paper investigates the robustness of Mixture of LLM Agents (MoA) architectures to deceptive agents providing misleading responses. - The study uses the AlpacaEval 2.0 question answering benchmark and the QUALITY multiple-choice comprehension task to uncover vulnerabilities. - Results show that even a single malicious agent can significantly degrade performance, negating the gains of using MoA and dropping accuracy to near-baseline levels. - Various factors influencing vulnerability, such as the number and location of deceptive agents, aggregator model size, and information access, are examined. - Inspired by the Doge of Venice voting system, several unsupervised defense mechanisms are proposed to mitigate the impact of deceptive agents and recover performance. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/lorenzflow/robust-moa) | N/A |
| [State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for
  State Space Models](https://arxiv.org/abs/2503.03499) | Hyung Il Koo, Minjae Lee, Yuchen Zeng, Kevin Galim, Wonjun Kang | - This paper introduces state-based Parameter-Efficient Fine-Tuning (PEFT) methods for State Space Models (SSMs), proposing a new method called State-offset Tuning. - State-offset Tuning directly adjusts state-related features within the SSM at each time step, offering a more effective adaptation strategy compared to prompt-based methods that rely on external virtual tokens and suffer from diminishing influence over time. - State-offset Tuning inserts a constant, learnable state-offset to the hidden state before output generation, mitigating the inconsistent effects of the time-varying coefficients present in existing methods like Initial State Tuning.  - The paper demonstrates State-offset Tuning's effectiveness through extensive experiments on various NLU and NLG datasets, including GLUE, SAMSum, Spider, and DART, using pre-trained Mamba and Mamba-2 models. - Experimental results show that State-offset Tuning consistently outperforms other PEFT methods, including prompt-based and parameter-based approaches, achieving performance comparable to full fine-tuning while using significantly fewer parameters. | ['Natural Language Processing', 'Summarization', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/furiosa-ai/ssm-state-tuning) | [Link](https://huggingface.co/datasets/nyu-mll/glue), [Link](https://huggingface.co/datasets/Samsung/samsum), [Link](https://huggingface.co/datasets/xlangai/spider), [Link](https://huggingface.co/datasets/Yale-LILY/dart), [Link](https://huggingface.co/state-spaces/mamba-{130m,1.4b,2.8b}), [Link](https://huggingface.co/state-spaces/mamba2-{130m,1.3b}) |
| [Should VLMs be Pre-trained with Image Data?](https://arxiv.org/abs/2503.07603) | Igor Vasiljevic, Kushal Arora, Samir Yitzhak Gadre, Jean Mercat, Sedrick Keh | - This paper investigates the impact of incorporating image data during pre-training of Vision-Language Models (VLMs), challenging the conventional two-stage training approach. - The authors train a suite of 300 models with varying scales, datasets, image-text ratios, and pre-training lengths, finding that introducing visual data during the "cooldown" phase of text pre-training, specifically at 80% completion, yields superior performance on vision-language tasks compared to adding images after full text pre-training. - An optimal image-to-text ratio of 10-20% is identified for 1B parameter models during the image-text pre-training phase, with varying optimums depending on the scale of the model. - Fine-tuning on instruction data further improves performance, with 2-4 epochs achieving a balance between vision and text task performance. - On a suite of six diverse tasks, introducing visual tokens at 80% of pre-training for a 1B model leads to a 2% average improvement compared to adding visual tokens after full pre-training, demonstrating the benefit of the proposed integrated approach. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text', 'Natural Language Processing'] | [Link](https://github.com/TRI-ML/vlm-evaluation/) | [Link](https://huggingface.co/TRI-ML/DCLM-1B), [Link](https://huggingface.co/meta-llama/Llama-3.2-1B), [Link](https://huggingface.co/apple/DCLM-7B) |
| [ProBench: Judging Multimodal Foundation Models on Open-ended
  Multi-domain Expert Tasks](https://arxiv.org/abs/2503.06885) | Liu Liu, Bei Chen, Haoning Wu, dxli1, HelloKKMe | - ProBench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on open-ended, expert-level tasks requiring professional knowledge and reasoning. - It contains 4,000 expert-designed samples spanning 10 professional fields and 56 sub-fields, supporting 17 languages and conversations with up to 13 turns. - Evaluations of 24 leading MLLMs using ProBench and an MLLM-as-a-Judge reveal significant challenges in visual perception, textual understanding, domain knowledge, and advanced reasoning. - The best open-source models show competitive performance with proprietary models, highlighting the progress and remaining challenges in multimodal AI research. - A distilled version of Llama-vision is provided for efficient local evaluation of MLLMs. | ['Multimodal'] | N/A | N/A |
| [Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by
  Learning Language-Agnostic Speech Representations](https://arxiv.org/abs/2503.06273) | Yong Man Ro, Stavros Petridis, Chae Won Kim, Minsu Kim, JeongHun0716 | - This paper introduces Zero-AVSR, a zero-shot audio-visual speech recognition (AVSR) framework capable of recognizing speech in languages it hasn't been explicitly trained on. - Zero-AVSR leverages an Audio-Visual Speech Romanizer (AV-Romanizer) which predicts language-agnostic pronunciations (Roman text) from audio-visual speech, and a Large Language Model (LLM) to convert the Roman text into language-specific graphemes. - The authors introduce a Multilingual Audio-Visual Romanized Corpus (MARC) of 2,916 hours of data across 82 languages including both language-specific and romanized transcriptions to train the system. - Experiments demonstrate the effectiveness of Zero-AVSR on unseen languages, outperforming a baseline zero-shot model and achieving competitive performance with existing multilingual models on seen languages. - It is also shown that Zero-AVSR improves noise robustness compared to audio-only approaches and demonstrates the ability to process data from language families unseen during training. | ['Automatic Speech Recognition', 'Multimodal'] | N/A | N/A |
| [Words or Vision: Do Vision-Language Models Have Blind Faith in Text?](https://arxiv.org/abs/2503.02199) | Bryan Hooi, Tri Cao, Ailin Deng, ryanchen42 | - This paper investigates the modality preference of Vision-Language Models (VLMs) when faced with inconsistencies between visual and textual data in vision-centric tasks. - The study reveals a "blind faith in text" phenomenon, where VLMs disproportionately trust textual information even when it contradicts visual evidence, leading to significant performance degradation under corrupted text. - Analysis of ten VLMs across four vision-centric tasks reveals that instruction prompts and language model size have limited impact on mitigating text bias, while text relevance and token order can exacerbate it. - Supervised fine-tuning with text augmentation is shown to effectively reduce text bias. - A theoretical analysis suggests the "blind faith in text" may stem from an imbalance of pure text and multi-modal data during VLM training. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/d-ailin/blind-faith-in-text) | N/A |
| [Detection Avoidance Techniques for Large Language Models](https://arxiv.org/abs/2503.07595) | Gabi Dreo Rodosek, Joao A. G. Schneider, Florian Steuber, SinclairSchneider | - This paper explores techniques to evade detection by large language model (LLM) classifiers, including shallow detectors, transformer-based detectors, and zero-shot detectors. - The authors investigate the effects of temperature, sampling methods, and model size on detection rates using a Naive Bayes classifier with Bag-of-Words features. - Reinforcement learning with constraints is employed to guide LLMs in generating text that evades transformer-based detectors while maintaining linguistic quality and coherence. - A novel paraphrasing model, trained on a dataset of masked and permuted LLM-generated text, is introduced to minimize detectability while maximizing content similarity to the original text. The model outperforms general-purpose paraphrasing models in detection evasion tasks. - The study highlights the potential for malicious actors to circumvent LLM detection and calls for further research into robust detection mechanisms and the ethical implications of undetectable LLM-generated text. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Link](https://huggingface.co/datasets/google-research-datasets/natural_questions), [Link](https://huggingface.co/kalpeshk2011/dipper-paraphraser-xxl), [Link](https://huggingface.co/Qwen/Qwen1.5-4B) |
| [Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge
  Reasoning](https://arxiv.org/abs/2503.04973) | Fabio Petroni, Orion Weller, papotti, giulio98 | - This paper proposes task-aware key-value (KV) cache compression, a novel technique to enhance large language models (LLMs) ability to perform knowledge reasoning by compressing external knowledge into a compact representation suitable for zero- or few-shot learning. - The approach outperforms both Retrieval-Augmented Generation (RAG) and task-agnostic compression methods, achieving up to 7 point improvement in accuracy on LongBench v2 with a 30x compression rate while also reducing inference latency. - Unlike query-aware compression which requires recompression per query, the task-aware compression precomputes a cache for a wider task context, enabling efficient and reusable caching. - On a synthetic dataset and Longbench v2, task-aware compression is shown to excel in tasks requiring broad knowledge synthesis where RAG struggles.  - This technique demonstrates the potential of KV cache compression for scaling LLM reasoning beyond traditional retrieval-based methods. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [RePO: ReLU-based Preference Optimization](https://arxiv.org/abs/2503.07426) | Jinyang Gao, Xue Wang, Kexin Huang, Junkang Wu, xiangwang1223 | - This paper introduces ReLU-based Preference Optimization (RePO), a streamlined algorithm for aligning Large Language Models (LLMs) with human preferences using offline data. - RePO simplifies existing methods like DPO and SimPO by eliminating the hyperparameter β and using a ReLU-based max-margin loss, requiring only a single hyperparameter (γ) for tuning.  - It retains the reference-free reward margins of SimPO while using a ReLU activation for these margins.  -  Evaluations on AlpacaEval 2 and Arena-Hard across multiple LLMs show RePO matches or outperforms DPO and SimPO, demonstrating competitive performance with reduced complexity. - The ReLU-based max-margin loss in RePO acts as the convex envelope of the 0-1 loss which enables tractable gradient-based optimization while preserving the properties of global optimality. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/junkangwu/REPO) | [Link](https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback-armorm), [Link](https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm) |
| [Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal
  LLMs](https://arxiv.org/abs/2503.06362) | Stavros Petridis, Minsu Kim, Umberto Cappellazzo | - This paper introduces Llama-MTSK, a Matryoshka-based Multimodal Large Language Model (MLLM) for Audio-Visual Speech Recognition (AVSR). - Llama-MTSK encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels and enabling flexible adaptation of audio-visual token allocation based on specific computational constraints.  - Three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules are introduced for efficient fine-tuning of the LLM. - Evaluations on the LRS2 and LRS3 datasets show that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels for ASR, VSR, and AVSR tasks. | ['Audio', 'Automatic Speech Recognition', 'Multimodal'] | N/A | N/A |
| [Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent
  Spaces](https://arxiv.org/abs/2503.05283) | Qixing Huang, Diego Gomez, Luca Moschella, Souhail Hadgi, teelinsan | - This paper investigates aligning the latent spaces of pre-trained 3D and text encoders without joint training, a task not previously explored in depth. - It reveals that unimodal 3D encoders, when trained independently, exhibit weak alignment with text representations compared to image-text counterparts. - The paper introduces a novel approach combining Canonical Correlation Analysis (CCA) for subspace selection and existing alignment methods (affine transformation and local Centered Kernel Alignment) in the reduced space to align 3D and text feature spaces.  - Experimental results demonstrate significant improvements in matching and retrieval tasks by projecting representations onto these shared subspaces, outperforming existing alignment methods applied directly to the full latent spaces. - Further analysis reveals a complementary structure between the subspaces and original feature spaces, suggesting a division between geometric and semantic information within the learned representations. | ['Multimodal', 'Text-to-3D'] | N/A | N/A |


## Papers for 2025-03-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Unified Reward Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2503.05236) | Cheng Jin, Hao Li, Jiaqiwang, yuhangzang, CodeGoat24 | - This paper introduces UNIFIEDREWARD, a unified reward model for assessing both multimodal understanding and generation tasks, including image and video modalities. - UNIFIEDREWARD leverages a large-scale human preference dataset covering various visual tasks and is trained to perform both pairwise ranking and pointwise scoring of model outputs. - A three-stage pipeline is proposed, involving (1) training UNIFIEDREWARD, (2) constructing preference data using the trained reward model via pair ranking and point sifting, and (3) aligning vision models (VLMs and diffusion models) with human preferences using Direct Preference Optimization (DPO). - Experimental results demonstrate that joint learning across diverse visual tasks leads to synergistic improvements, with UNIFIEDREWARD-based DPO outperforming existing methods on various benchmarks for image and video understanding and generation. - The paper highlights the benefits of a unified reward model for more adaptable, generalizable, and effective preference learning across visual applications. | ['Multimodal', 'Text-to-Image', 'Text-to-Video', 'Image-to-Text', 'Video-Text-to-Text'] | N/A | N/A |
| [EuroBERT: Scaling Multilingual Encoders for European Languages](https://arxiv.org/abs/2503.05500) | caiocorro, ayoubhammal, DuarteMRAlves, hgissbkh, Nicolas-BZRD | - This paper introduces EuroBERT, a family of multilingual encoder models based on the Llama 3 architecture, but adapted for bidirectional encoding. - These models incorporate architectural advances like grouped query attention, swish gated linear units, and rotary position embeddings, and are trained on a 5T token multilingual dataset covering European and global languages, mathematics, and code. - EuroBERT outperforms existing models like XLM-ROBERTa and mGTE-MLM-base on various tasks including multilingual retrieval, classification, regression, mathematics, and coding, while natively supporting sequences up to 8,192 tokens. - The models are available in three sizes (210m, 610m, and 2.1B parameters) and trained with a two-phase approach (pre-training and annealing) using masked language modeling. - The study also shows that EuroBERT maintains performance at longer context lengths compared to XLM-ROBERTa. | ['Natural Language Processing', 'Translation', 'Feature Extraction', 'Sentence Similarity', 'Question Answering'] | N/A | [Link](https://huggingface.co/EuroBERT) |
| [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/abs/2503.05179) | Sung Ju Hwang, jinheon, saytes | - Sketch-of-Thought (SoT) is introduced as a novel prompting framework designed to enhance the efficiency of large language models (LLMs) in reasoning tasks by minimizing token usage without significant accuracy loss. - SoT incorporates three cognitive-inspired reasoning paradigms — Conceptual Chaining, Chunked Symbolism, and Expert Lexicons — each tailored to different types of reasoning tasks and selected dynamically by a lightweight router model based on query characteristics, and implemented through prompt engineering. - Evaluations across 15 reasoning datasets demonstrate SoT achieving a 76% reduction in token usage compared to Chain-of-Thought (CoT) prompting, while maintaining or even improving accuracy in some tasks. - SoT is also shown to be effective in both multilingual and multimodal scenarios, maintaining high efficiency with minimal accuracy trade-offs. - The study argues that concise, structured reasoning as promoted by SoT can be as effective as (and sometimes superior to) the verbose explanations typical of traditional prompting methods. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/SimonAytes/SoT) | N/A |
| [Forgetting Transformer: Softmax Attention with a Forget Gate](https://arxiv.org/abs/2503.02130) | Aaron Courville, littleowen, nikishin, zhixuan-lin | - This paper introduces the Forgetting Transformer (FoX), a Transformer variant incorporating a forget gate mechanism within the softmax attention, enabling data-dependent down-weighting of past information. - FoX integrates the forget gate by down-weighting unnormalized attention scores, enhancing performance on long-context language modeling, length extrapolation, and short-context downstream tasks while maintaining comparable performance to standard Transformers on long-context downstream tasks. -  It eliminates the need for positional embeddings and is compatible with FlashAttention.  - FoX also demonstrates superior long-context capabilities compared to recurrent sequence models like Mamba-2, HGRN2, and DeltaNet, achieving near-perfect accuracy in the needle-in-the-haystack test. - A "Pro" block design further improves FoX and Transformer performance by integrating architectural components from recurrent sequence models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zhixuan-lin/forgetting-transformer) | N/A |
| [R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.05592) | jrwen, TimothyCzp, EliverQ, Boru, XXsongLALA | - R1-Searcher, a novel framework, enhances the Retrieval Augmented Generation (RAG) capabilities of Large Language Models (LLMs) using a two-stage reinforcement learning (RL) approach. - This method allows LLMs to learn to use external search engines during reasoning, improving knowledge access and reducing hallucinations for complex questions. - R1-Searcher outperforms strong baselines, including GPT-40-mini, by up to 48.22% on HotpotQA and 21.72% on 2Wiki using Qwen-2.5-7B-Base as the LLM backbone. - The model generalizes well to out-of-domain datasets like Bamboogle, showing an 11.4% improvement over Search-01 with 32B parameters using online search. - The training relies solely on outcome-based RL, eliminating the need for distillation or cold starts with supervised fine-tuning, improving training efficiency and adaptability for various LLM backbones. | ['Question Answering', 'Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/SsmallSong/R1-Searcher) | N/A |
| [R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning](https://arxiv.org/abs/2503.05379) | Xihan Wei, Liefeng, StarJiaxing | - R1-Omni is introduced, an innovative application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model, specifically designed for enhanced emotion recognition by leveraging both visual and audio modalities. - RLVR optimizes the model, leading to significant improvements in reasoning capabilities, emotion recognition accuracy, and generalization, particularly on out-of-distribution datasets.  R1-Omni provides insights into the contributions of visual and audio information during emotion recognition. - The model is first pre-trained using a cold-start strategy on EMER (Explainable Multimodal Emotion Reasoning) and a manually annotated HumanOmni dataset. - The RLVR training process utilizes a reward function comprising accuracy and formatting components, drawing inspiration from DeepSeek R1. - Experimental results demonstrate R1-Omni's superior performance compared to baseline models on MAFW, DFEW, and RAVDESS datasets, showcasing improved reasoning, understanding, and generalization capabilities. | ['Multimodal', 'Video Classification', 'Reinforcement Learning'] | [Link](https://github.com/HumanMLLM/R1-Omni) | N/A |
| [BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities](https://arxiv.org/abs/2503.05652) | Ruohan Zhang, jiajunwu, cgokmen, yjze, yunfanj | - This paper introduces BEHAVIOR ROBOT SUITE (BRS), a framework for whole-body manipulation in diverse household tasks, featuring a novel teleoperation interface (JoyLo) and a learning algorithm (WB-VIMA). - JoyLo uses low-cost, kinematic-twin arms with joystick controllers to enable intuitive and efficient whole-body teleoperation of a wheeled, dual-arm robot with a flexible torso. - WB-VIMA, a transformer-based imitation learning algorithm, leverages the robot's kinematic hierarchy and multi-modal sensory input (egocentric point clouds and robot joint positions) to predict coordinated whole-body actions. - WB-VIMA outperforms baseline methods on five challenging real-world household tasks, achieving an average success rate of 58% and a peak of 93%, compared to a maximum of 20% for baselines. - JoyLo demonstrates superior efficiency and user-friendliness in a user study compared to VR controller and Apple Vision Pro interfaces, and its collected data leads to significantly higher replay success rates in policy learning. | ['Robotics', 'Reinforcement Learning', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation](https://arxiv.org/abs/2503.04872) | lwher1996, yuhanwuuu, xiaoqijiang, zhaoguangxiang, lincharliesun | - This paper introduces TinyR1-32B-Preview, a new 32B parameter language model trained using a novel Branch-Merge distillation approach. - The Branch-Merge approach first trains specialized student models on domain-specific data (math, coding, science) and then merges them to enhance cross-domain knowledge transfer and generalization. - This model outperforms DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B on benchmarks like AIME 2024 (Math), LiveCodeBench (Coding), and GPQA-Diamond (Science), and achieves near-equal performance to the larger DeepSeek-R1 model (67B parameters). - The merging method employed, Arcee Fusion, significantly reduces the computational cost compared to traditional data mixture methods, requiring only 0.5% of the overhead. - The model and training data, code, and logs will be open-sourced. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts](https://arxiv.org/abs/2503.05447) | Yu Cheng, Tong Zhu, Xiaoye08, landisen, weigao266 | - Linear-MoE is a production-level system for large language models that combines Linear Sequence Modeling (LSM) with Mixture-of-Experts (MoE). - It offers a unified framework for various LSM methods, including linear attention, state space models, and linear RNNs, along with efficient training techniques using advanced parallelism. - Linear-MoE models demonstrated competitive performance on various benchmarks while achieving efficiency gains, especially with long input sequences, compared to standard attention models. - The system also explores hybrid models combining Linear-MoE and standard Transformer-MoE layers for enhanced flexibility and performance on recall-intensive tasks. - The system facilitates seamless integration with existing ecosystems such as Megatron-Core and HuggingFace. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/OpenSparseLLMs/Linear-MoE) | N/A |
| [An Empirical Study on Eliciting and Improving R1-like Reasoning Models](https://arxiv.org/abs/2503.04548) | daixuancheng, Boru, ToheartZhang, EliverQ, TimothyCzp | - This paper explores scaling reinforcement learning (RL) training for large reasoning models (LRMs), also known as slow-thinking models, to enhance their complex reasoning capabilities. - The research systematically investigates the impact of various RL training settings, including hyperparameters, backbone models, and prompt design, on both base and fine-tuned LLMs. - Key findings indicate that on-policy learning and appropriate rollout parameter settings are crucial for effective RL training, while detailed prompts improve reasoning efficiency. - The study demonstrates that direct RL training on base models, exemplified by STILL-3-ZERO-32B, enhances both response length and test accuracy. - Furthermore, RL training combined with tool manipulation, particularly using a code interpreter, significantly improves the reasoning performance of fine-tuned models, with STILL-3-TOOL-32B achieving 86.67% accuracy on AIME 2024 with greedy search. | ['Natural Language Processing', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs) | N/A |
| [SAGE: A Framework of Precise Retrieval for RAG](https://arxiv.org/abs/2503.01713) | Jinyang Su, Guoliang Li, jt-zhang | - SAGE, a Retrieval-Augmented Generation (RAG) framework, enhances precise retrieval by incorporating semantic segmentation, gradient-based chunk selection, and LLM self-feedback.  - A lightweight model segments the corpus into semantically complete chunks, dynamic chunk selection prioritizes relevant chunks based on decreasing relevance scores, and LLM self-feedback adjusts the number of chunks for accurate QA. - Experiments demonstrate SAGE surpasses baselines by 61.25% in QA quality on average, and 49.41% in cost efficiency on average by lowering token consumption. - SAGE is evaluated with four LLMs (GPT3.5 turbo, GPT4, GPT4-o-mini, and UnifiedQA-3B) on three QA datasets (QuALITY, QASPER, and NarrativeQA).  - SAGE improves the performance of various retrievers (SBERT, BM25, DPR, and OpenAI Embedding) in RAG systems by a significant margin. | ['Question Answering'] | N/A | N/A |
| [LoRACode: LoRA Adapters for Code Embeddings](https://arxiv.org/abs/2503.05315) | bindsch, amanchadha, shollercoaster | - LoRACode, a novel parameter-efficient fine-tuning method using Low-Rank Adaptation (LoRA), enhances code embeddings for semantic code search by creating task-specific and language-specific adapters for retrieving code snippets. - This approach reduces the number of trainable parameters, enabling efficient fine-tuning of large language models on extensive code corpora (e.g., 2 million samples in 25 minutes on two H100 GPUs) while using only 1.83%-1.85% of the base model parameters for fine-tuning. - Experiments demonstrate improvements of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search and up to 86.69% for Text2Code search across multiple programming languages. - Language-specific adapters outperform task-specific adapters, especially for Text2Code retrieval due to better handling of language-specific syntax and semantics. - Evaluating the approach with various datasets, tasks, and languages reveals that language-specific fine-tuning leads to more effective code retrieval compared to a generalized, multilingual approach. | ['Natural Language Processing', 'Feature Extraction'] | N/A | N/A |
| [R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model](https://arxiv.org/abs/2503.05132) | Minhao Cheng, Ruochen Wang, zhoutianyi, AIcell, Dolphin42 | - This paper introduces VisualThinker-R1-Zero, a novel approach for enhancing visual reasoning capabilities in multimodal models by directly applying reinforcement learning (RL) to a non-supervised fine-tuned (non-SFT) 2B parameter model. - The model is trained using the Qwen2-VL-2B architecture and GRPO algorithm on the SAT dataset, focusing on spatial reasoning tasks. - This method successfully replicates the "aha moment" phenomenon observed in DeepSeek-R1, characterized by emergent self-reflection and increased response length during training, indicating the development of advanced reasoning strategies. - VisualThinker-R1-Zero achieves 59.47% accuracy on CVBench, outperforming the base Qwen2-VL-2B model by ~30% and its supervised fine-tuned (SFT) counterpart by ~2%, demonstrating significant improvement in visual reasoning abilities. - The study also highlights the challenges of applying RL to SFT models, revealing a tendency towards trivial reasoning patterns rather than genuine problem-solving strategies, suggesting that direct RL on non-SFT models may be a more effective approach for inducing complex reasoning capabilities. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/turningpoint-ai/VisualThinker-R1-Zero) | N/A |


## Papers for 2025-03-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [START: Self-taught Reasoner with Tools](https://arxiv.org/abs/2503.04625) | BeichenZhang, jx-yang, Zhenru, mingfengxue, ChengpengLi | - START, a novel tool-integrated long Chain-of-Thought (CoT) reasoning Large Language Model (LLM), enhances reasoning capabilities by leveraging external tools, such as Python code execution, for complex computations, self-checking, method exploration, and self-debugging. - The core innovation of START lies in its self-learning framework, comprising Hint-infer and Hint Rejection Sampling Fine-Tuning (Hint-RFT), stimulating the LLM's tool utilization abilities without demonstration data, improving sequential test-time scaling, and refining tool-integrated reasoning trajectories. - START achieves state-of-the-art performance on PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and code benchmark (LiveCodeBench), with accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively, surpassing existing tool-integrated and long CoT models, including QwQ, 01-mini, and 01-preview in MATH benchmarks. - Hint-infer strategically inserts hints during LLM inference to encourage tool use, while Hint-RFT refines these interactions through a scoring, filtering, and modification process followed by fine-tuning, enabling self-aware tool usage. - START is the first open-source tool-integrated long CoT reasoning model, setting a new standard for LLM performance in complex reasoning domains by combining the strengths of long CoT and external tool integration. | ['Question Answering', 'Text2Text Generation', 'Natural Language Processing'] | N/A | N/A |
| [LLM as a Broken Telephone: Iterative Generation Distorts Information](https://arxiv.org/abs/2502.20258) | Michalis Vazirgiannis, guokan-shang, mgeng, amr-mohamed | - This paper investigates how Large Language Models (LLMs) distort information through iterative generation, similar to the "broken telephone" game. - The study uses translation-based experiments, where a document is iteratively translated between English and other languages. - The results reveal that distortion accumulates over time and is influenced by language choice and chain complexity. - While degradation is inevitable, it can be mitigated through temperature control and constrained prompting techniques. - This work raises concerns about the reliability of LLM-generated content in iterative workflows. | ['Natural Language Processing', 'Translation'] | [Link](https://github.com/amr-mohamedd/LLM-as-a-Broken-Telephone) | N/A |
| [EgoLife: Towards Egocentric Life Assistant](https://arxiv.org/abs/2503.03803) | Zzitang, Alarak, fesvhtr, THUdyh, Jingkang | - This paper introduces EgoLife, a project focused on developing an egocentric life assistant using AI-powered wearable glasses. - A new 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset called EgoLife Dataset is collected, featuring six participants living together for a week and wearing Meta Aria glasses. - It also introduces a new question answering benchmark called EgoLifeQA which is based on the dataset that features five types of questions. - An agent system called EgoButler comprising two models, EgoGPT and EgoRAG are introduced to address the EgoLifeQA benchmark. - Experimental results demonstrate that EgoButler outperforms other models on the EgoLifeQA benchmark, showcasing its effectiveness in long-context question answering and multimodal understanding in egocentric scenarios. | ['Multimodal', 'Video-Text-to-Text', 'Question Answering'] | [Link](https://github.com/EvolvingLMMs-Lab/EgoLife) | N/A |
| [LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation](https://arxiv.org/abs/2503.02972) | Vlad Neacs, Lingyi Yang, Simi Hellsten, Karolina Korgul, Jude Khouja | - This paper introduces LINGOLY-TOO, a new benchmark for evaluating linguistic reasoning in Large Language Models (LLMs) designed to mitigate the impact of memorization by using orthographically obfuscated versions of linguistic problems. - The obfuscation process alters the writing systems of problems while preserving the underlying reasoning steps needed for solutions. - Experiments demonstrate that state-of-the-art LLMs, including Claude 3.7 Sonnet and other frontier models struggle with the benchmark's obfuscated problems indicating a gap in genuine reasoning capabilities. - The paper also introduces a framework for creating obfuscated evaluation datasets for other reasoning tasks and provides insights on the effects of data exposure and tokenization on LLMs' performance. - Results of an RCT with 172 participants suggest that obfuscation increases the perceived difficulty for humans, but does not significantly change underlying reasoning process indicating its viability as an evaluation method. | ['Natural Language Processing', 'Question Answering', 'Zero-Shot Classification'] | N/A | [Link](https://huggingface.co/spaces/jkhouja/lingoly-too) |
| [HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization](https://arxiv.org/abs/2503.04598) | Ya Wang, Breeze0417, LLIXQ, Taoer, BryceZhuo | - This paper introduces HybridNorm, a novel hybrid normalization strategy for training deep transformer models that combines the strengths of Pre-Norm and Post-Norm. - HybridNorm applies QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block.  - This design stabilizes training and improves performance, particularly in Large Language Models (LLMs). - Experimental results across various benchmarks, including dense and Mixture-of-Experts (MoE) models, demonstrate that HybridNorm consistently outperforms both Pre-Norm and Post-Norm, showing improved training stability and achieving state-of-the-art performance on downstream tasks. - A variant, HybridNorm*, with a specially-treated first transformer block, further improves training stability and downstream task performance. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/BryceZhuo/HybridNorm) | [Link](https://huggingface.co/datasets/allenai/OLMOE-mix-0924) |
| [FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion](https://arxiv.org/abs/2503.04222) | passerqxj, OnewayLab, GGLS, Wanfq, AALF | - FuseChat-3.0 is a suite of large language models (LLMs) created by fusing heterogeneous source LLMs into more compact target LLMs using implicit model fusion (IMF). - The training process involves Supervised Fine-tuning (SFT) using high-reward responses from source models, followed by Direct Preference Optimization (DPO) using preference pairs from the same source model, eliminating reward bias and variance from heterogeneous sources. - Evaluation across 14 benchmarks shows FuseChat-3.0 significantly outperforming its corresponding target LLMs across diverse tasks including instruction following, general knowledge, mathematics, and coding. - Using Llama-3.1-8B-Instruct as the target model, FuseChat-3.0 achieved a 6.8 point average improvement and state-of-the-art performance gains of 37.1 and 30.1 points on instruction following benchmarks AlpacaEval-2 and Arena-Hard respectively. - The results demonstrate FuseChat-3.0's effectiveness as a scalable framework for enhancing LLM performance through implicit knowledge fusion without requiring architectural modifications or massive computational resources. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/SLIT-AI/FuseChat-3.0) | N/A |
| [Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/abs/2503.04130) | zhiqilinv, MuyangLI, zhijianliu, xiuyul, jdps | - This paper introduces STORM (Spatiotemporal Token Reduction for Multimodal LLMs), a novel architecture for enhancing long-video understanding in multimodal large language models (LLMs). - STORM incorporates a dedicated temporal encoder, based on the Mamba State Space Model, between the image encoder and the LLM to integrate temporal information into image tokens, improving video reasoning and enabling token reduction strategies. - The Mamba layer compresses historical frame information, allowing for temporal and spatial pooling (or test time sampling) for efficient token reduction without discarding crucial details. - Evaluation across various video understanding benchmarks demonstrates state-of-the-art results, with over 5% improvement on MLVU and LongVideoBench, while also reducing computational costs by up to 8× and decoding latency by 2.4-2.9×. - Qualitative analysis reveals that STORM effectively retains crucial visual information despite significant token compression, enabling robust comprehension of complex long-video content. | ['Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval](https://arxiv.org/abs/2503.04644) | Mingsheng Shang, yilunzhao, guo9, songtingyu | - IFIR is a new benchmark designed for evaluating instruction-following information retrieval in specialized domains such as finance, law, healthcare, and science literature. - The benchmark consists of 2,426 instruction-following queries across four expert domains with an average of 6.14 ground-truth passages per query. - A novel LLM-based evaluation metric, INSTFOL, is introduced to measure a retriever’s instruction-following ability.  - Experimental results on 15 state-of-the-art information retrievers reveal that existing models face significant challenges in complex, domain-specific instructions following and LLM-based retrievers show better potential in handling complex retrieval tasks. - The benchmark also confirms that lexical search is a promising method for complex instructions retrieval in specific domains. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/SighingSnow/IFIR) | N/A |
| [Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities](https://arxiv.org/abs/2503.03983) | manocha, rafaelvalle, firecomputer, ZhifengKong, SreyanG-NVIDIA | - This paper introduces Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) designed for enhanced audio understanding and reasoning, particularly with long audio segments (30 seconds to 5 minutes). - AF2 leverages a custom CLAP model trained on a large audio-caption dataset with a novel contrastive loss, synthetic Audio Question Answering (AQA) data (AudioSkills) targeting seven specific reasoning skills, and a 3-stage curriculum learning strategy. - It also introduces LongAudio, a dataset with 263k AQA pairs and 80k audio segments up to 5 minutes long, and LongAudioBench, an expert-validated benchmark derived from LongAudio for evaluating long-audio understanding. - AF2 achieves state-of-the-art performance on over 20 benchmarks, outperforming larger, open-source, and proprietary models despite using only a 3B parameter language model, surpassing even advanced models like Gemini 1.5 Pro in expert-level audio reasoning tasks. - Ablation studies validate the effectiveness of AF2's components, especially the AudioSkills dataset for improving reasoning abilities and the novel contrastive loss for AF-CLAP which shows that quality data plays a more significant role compared to model size. | ['Audio', 'Question Answering', 'Multimodal'] | N/A | N/A |
| [Identifying Sensitive Weights via Post-quantization Integral](https://arxiv.org/abs/2503.01901) | Weiyu Huang, surfingtomchen, jt-zhang, zcliang22, yuezhouhu | - This paper proposes Post-quantization Integral (PQI), a novel sensitivity metric to accurately estimate the influence of each quantized weight in Large Language Models (LLMs), addressing the inaccuracy of existing gradient and Hessian-based metrics. - PQI leverages local continuity and considers both original and quantized weights to calculate sensitivity, enabling fine-grained prediction of quantization's impact on loss function. - The paper introduces ReQuant, a framework utilizing PQI for Dense-and-Sparse decomposition, enhancing quantized model quality via self-adaptive outlier selection and step-wise significant weights detach. - ReQuant improves state-of-the-art post-training quantization methods, showing a 2.66 perplexity gain on Llama 3.2 1B with QTIP and nearly 3% improvement on MATH few-shot accuracy. - Experimental results demonstrate the effectiveness of ReQuant on various quantization methods like AWQ, SqueezeLLM, and QTIP with Llama 3.2 1B and 3B models, enhancing both perplexity and few-shot performance. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling](https://arxiv.org/abs/2503.04725) | Marin Soljačić, Di Luo, Zhuotao Jin, oriolmayne, zhuoc3 | - This paper introduces a bipartite mutual information scaling law, distinct from traditional two-point mutual information, to govern long-range dependencies in natural language and understand long-context language modeling. - It formulates the Long-context Language Modeling (L2M) condition, linking a model's effective long context length modeling capacity to its latent state size scaling for storing past information. - The bipartite mutual information scaling law is validated across different natural language datasets and LLMs (LLaMA, DeepSeek) showing consistent power-law growth. - The L2M condition is empirically verified with transformer and state space models trained on varying sequence lengths. - The study provides a theoretical foundation for improving LLM architecture design for longer context lengths and more effective applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/LSquaredM/mutual_info_scaling_law) | N/A |
| [Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks](https://arxiv.org/abs/2503.04378) | Ellie Evans, Daniel Egert, Jiaqi Zeng, Zhilin Wang, odelalleau | - This paper introduces a novel Feedback-Edit system for enhancing the performance of large language models (LLMs) on open-ended, general-domain tasks, addressing the limitations of existing inference-time scaling techniques that rely on verifiable answers. - The system employs three dedicated models: one generates initial responses, a second provides detailed textual feedback on these responses, and a third edits the responses based on the feedback.  - It leverages a new dataset curated from over 7,000 annotators providing both feedback and edits for diverse prompts, sourced from ShareGPT and WildChat.  - The Feedback and Edit models are trained with supervised fine-tuning and reinforcement learning. - Evaluation on Arena Hard, a challenging benchmark, shows that an optimized Feedback-Edit system using 70B Llama 3 models achieves state-of-the-art performance of 92.7, surpassing OpenAI's ol-preview-2024-09-12 and DeepSeek R1. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer](https://arxiv.org/abs/2503.02495) | Linhui Li, Jing Lian, yjyangwork | - The paper introduces Union-of-Experts (UoE), a novel Mixture-of-Experts (MoE) architecture that decomposes transformers into equivalent expert groups, enabling selective routing on both input data and experts. - UoE enhances expert collaboration through mechanisms like patch-wise data selection and expert selection for improved performance and efficiency. - The architecture consists of Selective Multi-Head Attention (SMHA) for diverse representation learning and Union-of-MLP-Experts (UoME) for dense model integration of activated experts.  - Parallel implementation and hardware optimization further boost efficiency, reducing FLOPs by at least 30% and achieving a speedup of over 2x.  - Experimental results across language modeling, long-range sequence modeling, and image classification tasks demonstrate UoE's superior performance compared to existing state-of-the-art MoEs and efficient transformers. | ['Natural Language Processing', 'Image Classification', 'Computer Vision'] | [Link](https://github.com/YujiaoYang-work/UoE) | N/A |
| [Lost in Literalism: How Supervised Training Shapes Translationese in LLMs](https://arxiv.org/abs/2503.04369) | Leyang Cui, Huajian Zhang, Zhilin Wang, Ronghao Zhang, yaful | - This paper investigates the phenomenon of "translationese" in Large Language Model (LLM) based machine translation, where translations are overly literal and unnatural despite LLMs being trained on vast amounts of text data. - The authors conduct a systematic study involving human evaluation of LLM translations across different domains and language pairs, finding that a significant portion of the translations exhibit translationese. - They analyze translationese prevalence in supervised fine-tuning datasets and find a substantial number of training samples with translationese characteristics, suggesting that supervised training biases LLMs towards literal translation. - Two mitigation strategies are introduced: refining the golden training references using LLMs and filtering out unnatural training instances based on perplexity scores. - Experimental results demonstrate that these strategies significantly reduce translationese and improve translation naturalness, as validated by human evaluations and automatic metrics across multiple LLMs and additional languages. | ['Translation', 'Natural Language Processing'] | [Link](https://github.com/yafuly/LLM_Translationese) | N/A |
| [Understanding and Predicting Derailment in Toxic Conversations on GitHub](https://arxiv.org/abs/2503.02191) | Rebekah Copeland, Robert Zita, kdamevski, rahat-rizvi, imranraad | - This research introduces a novel approach to proactively moderate online discussions on GitHub by predicting conversational derailment, which is when a conversation shifts from productive discourse to negativity and potential toxicity. - The authors curated a dataset of 202 toxic GitHub conversations with annotated derailment points and 696 non-toxic conversations, identifying linguistic and conversational dynamics features unique to toxic conversations. - Leveraging these features and a Least-to-Most (LtM) prompting technique, they developed an approach that generates Summaries of Conversation Dynamics (SCD) to predict if a conversation will derail into toxicity. - This LLM-powered approach, utilizing a two-step prompting process with a large language model (LLaMA-3.1-70B), achieves a 0.69 F1-score and 0.76 precision, significantly outperforming baseline methods like CRAFT and a generic SCD approach. | ['Natural Language Processing', 'Text Classification'] | [Link](https://anonymous.4open.science/r/derailment-oss-replication-C8B1) | N/A |


## Papers for 2025-03-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers](https://arxiv.org/abs/2503.00865) | LidongBing, maljunied, jhying, lukecq, Yiran0924 | - Babel, a new open-source multilingual large language model (LLM), supports the top 25 languages by speaker count, covering over 90% of the global population and addressing the scarcity of open-source multilingual LLMs. - Babel uses a layer extension technique, adding new layers identical to the original architecture, to increase its parameter space and improve performance, rather than traditional continue pretraining methods. - Two variants are introduced: Babel-9B for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs by outperforming other open LLMs of comparable size on multilingual tasks. - Babel chat models, trained using open-source supervised fine-tuning datasets, demonstrate strong task-solving capabilities, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat achieving state-of-the-art performance, comparable to commercial models like GPT-40. - The model's effectiveness in understanding, reasoning, and translation across multiple languages is highlighted through comprehensive evaluations on various multilingual datasets. | ['Natural Language Processing', 'Translation', 'Question Answering'] | N/A | [Link](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407) |
| [ABC: Achieving Better Control of Multimodal Embeddings using VLMs](https://arxiv.org/abs/2503.00329) | Florian Kerschbaum, Benjamin Schneider, wenhu | - Introduces ABC, a new open-source multimodal embedding model that leverages a vision-language model (VLM) backbone to integrate image features and natural language instructions, enabling better user control over representations. - Employs a two-stage training process involving contrastive pretraining with mined negatives and instruction fine-tuning with synthetic instructions. - Achieves state-of-the-art performance on MSCOCO image-to-text retrieval, outperforming CLIP-based models with up to 8 billion parameters, and excels in classification and VQA tasks on the Massive Multimodal Embedding Benchmark (MMEB), surpassing all other models in zero-shot settings. - Introduces CtrlBench, a new benchmark to evaluate the model's ability to control retrieval using natural language, demonstrating ABC's capacity to resolve ambiguous visual retrieval problems by interleaving textual instructions with image content. - Advances multimodal embeddings by offering high-quality representations combined with flexible natural language control. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Classification', 'Visual Question Answering'] | N/A | [Link](https://tiger-ai-lab.github.io/ABC/) |
| [Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions](https://arxiv.org/abs/2503.03278) | Cosmin I. Bercea, Rossella Arcucci, Wenjia Bai, Jun Li, che111 | - This paper introduces a knowledge-enhanced approach for abnormality grounding in medical images using Vision Language Models (VLMs). - The approach leverages decomposed medical knowledge descriptions, focusing on visual attributes like shape, density, and location, to improve the alignment between textual descriptions and visual features. - This method achieves comparable performance to significantly larger medical VLMs while using only 1.5% of the training data. - Evaluation on VinDr-CXR and PadChest-GR datasets shows competitive results, demonstrating the effectiveness of the proposed approach in both known and unseen abnormalities. - The smaller model size and strong generalization capabilities make this approach suitable for real-world medical applications with limited data. | ['Computer Vision', 'Object Detection', 'Multimodal'] | [Link](https://lijunrio.github.io/AG-KD/) | N/A |
| [KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding](https://arxiv.org/abs/2503.02951) | Radha Poovendran, mingyuanzhou, yyqoni, nlpyang, flydust | - KODCODE is a synthetic dataset of 447k coding question-solution-test triplets for training large language models (LLMs) for coding tasks. - It addresses challenges of data quality and verifiability by using a self-verification procedure and generating responses from a reasoning model (DeepSeek R1) under test-based reject sampling. - The dataset creation pipeline involves question synthesis from diverse sources, solution and test generation with self-verification, and post-training data synthesis including style conversion and CoT response generation. - KODCODE-tuned models achieved state-of-the-art performance on coding benchmarks like HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench, outperforming models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B. - Statistical analysis demonstrates KODCODE's minimal contamination with existing benchmarks and the effectiveness of the self-verification and challenging question inclusion mechanisms. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://kodcode-ai.github.io) | [Link](https://huggingface.co/KodCode) |
| [CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom](https://arxiv.org/abs/2503.01836) | Pan Zhou, Wenxuan Shen, Lingfeng Yang, shuaishuaicdp, yisenL | - CROWDSELECT, a new framework for selecting synthetic instruction-tuning data, leverages "Multi-LLM Wisdom" by incorporating responses and reward scores from multiple LLMs as diverse reflections of instruction quality. - Three foundational metrics—Difficulty, Separability, and Stability—are introduced to assess instruction-response pair characteristics, capturing the challenges, model differentiation potential, and alignment consistency of each sample. - CROWDSELECT integrates these metrics with a clustering approach for response diversity preservation, maximizing the effectiveness of LLM collaboration for identifying a compact yet high-impact subset of training data. - Experiments show CROWDSELECT achieves state-of-the-art performance on both MT-bench and Arena-Hard, improving Llama-3.2-3b-instruct scores by 4.81% and 11.1% respectively, demonstrating its efficiency and generalizability across four models and two benchmarks. - The proposed method effectively bridges the gap between pre-trained knowledge and real-world user scenarios by selecting high-quality instruction data, enhancing the model's instruction-following capabilities. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/listentm/crowdselect) | N/A |
| [QE4PE: Word-level Quality Estimation for Human Post-Editing](https://arxiv.org/abs/2503.03044) | Malvina Nissim, Ana Guerberof-Arenas, Grzegorz Chrupała, Vilém Zouhar, gsarti | - This paper presents QE4PE, a study investigating the impact of word-level Quality Estimation (QE) on human post-editing of machine translation (MT). - The study involves 42 professional post-editors across English-Italian and English-Dutch translations using a state-of-the-art NLLB 3.3B MT model. - Four highlight modalities (supervised, unsupervised, oracle, and no-highlight baseline) are compared for identifying errors. - The study analyzes post-editing effort, quality improvement, and the usability of different highlight methods based on behavioral logs, human annotations, and questionnaires. - Results show that domain, language, and individual editing speed significantly influence highlight effectiveness, with modest accuracy-usability trade-offs observed. | ['Translation', 'Natural Language Processing'] | [Link](https://github.com/gsarti/qe4pe) | [Link](https://grote-app.hf.spaces) |
| [Exploring Rewriting Approaches for Different Conversational Tasks](https://arxiv.org/abs/2502.18860) | Xiang Chen, Mike Rimer, Ryan A. Rossi, Md Mehrab Tanjim, Franck-Dernoncourt | - This paper investigates two query rewriting approaches—rewriting and fusion—for conversational assistants. - Rewriting uses previous queries and responses as context, while fusion recursively combines previous rewritten queries. - Evaluated on text-based Q&A and text-to-visualization tasks using cosine similarity and BERT F1 score. - Query rewriting outperforms fusion in conversational Q&A by 3.9% and 9.8% for each metric. - Fusion surpasses rewriting for data analysis tasks by 7.6% and 5.2% for each metric by effectively summarizing conversations and user intent. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | N/A | N/A |
| [Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective](https://arxiv.org/abs/2503.01933) | KartikAngadi, kruthika, SyedAbdul, RakshitAralimatti | - This research introduces the Shakti series of small language models (SLMs), including Shakti-100M, Shakti-250M, and Shakti-500M, designed for efficient and adaptable language modeling under edge constraints by combining efficient architectures, quantization techniques, and responsible AI principles. - These models incorporate techniques like variable grouped query attention (GQA), Block Sparse Attention, Rotary Positional Embeddings (ROPE), SiLU activation, and Sliding Window mechanism, along with quantization. - The Shakti series undergoes structured training including pre-training on diverse text corpora, Supervised Fine-Tuning (SFT) on task-specific datasets, and preference alignment via Reinforcement Learning from Human Feedback (RLHF) for Shakti-500M and Direct Preference Optimization (DPO) for Shakti-250M and Shakti-100M. - Evaluation shows that Shakti models achieve strong benchmark performance, competing effectively against larger models on standard NLP tasks and specialized domain tasks such as healthcare QA, finance analytics, and legal contract analysis, especially in resource-constrained settings. - These models offer quantized versions (int8, int5, int4) to minimize memory usage and increase tokens-per-second (TPS) throughput, suitable for deployment on resource-constrained hardware such as Raspberry Pi boards or entry-level GPUs. | ['Natural Language Processing', 'Summarization', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases](https://arxiv.org/abs/2502.20317) | Ryan A. Rossi, Haoyu Han, Yongjia Lei, mhalappa, Franck-Dernoncourt | - This paper introduces Mixture of Structural-and-Textual Retrieval (MoR), a novel framework designed to retrieve both textual and structural knowledge from Text-rich Graph Knowledge Bases (TG-KBs) for question answering. - MoR employs a Planning-Reasoning-Organizing approach, starting with generating textual planning graphs that outline the query logic, followed by interweaving structural traversal and textual matching to retrieve candidate entities, and finally reranking these candidates using structural trajectory features. - Experimental results across diverse TG-KBs demonstrate MoR's superior performance compared to existing textual, structural, and hybrid retrieval methods. - The ablation studies further confirm the effectiveness of the individual modules and features, especially the structure-aware reranker. - This work offers insights into the interplay between textual and structural knowledge, uneven retrieval performance across query logics, and the value of integrating structural trajectories for candidate reranking. | ['Question Answering', 'Graph Machine Learning', 'Natural Language Processing'] | [Link](https://github.com/Yoega/MoR) | N/A |
| [Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models](https://arxiv.org/abs/2503.01763) | Shuaiqiang Wang, Pengjie Ren, Lingyong Yan, Yuhan Wang, Zhengliang Shi | - This paper introduces TOOLRET, a benchmark for evaluating tool retrieval capabilities of information retrieval (IR) models designed to augment large language models (LLMs) for tool-use tasks. - TOOLRET comprises 7.6k diverse retrieval tasks and a corpus of 43k tools sourced from existing datasets and benchmarks, encompassing web APIs, code functions, and customized applications. - Evaluation results reveal that even state-of-the-art IR models, including those excelling in standard benchmarks like MTEB, struggle significantly on TOOLRET, achieving less than 35% Completeness@10. - The authors attribute this performance gap to the low lexical overlap between queries and relevant tools in TOOLRET, demanding more advanced reasoning capabilities from retrieval models compared to conventional IR tasks. - To address this, the paper contributes TOOLRET-train, a large-scale training dataset with over 200k instructional retrieval tasks, leading to substantially improved retrieval performance and higher end-to-end task pass rates for tool-use LLMs when combined with trained IR models. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co), [Link](https://github.com) |
| [Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection](https://arxiv.org/abs/2503.01449) | Hung Nguyen, Martin Weyssow, Yindu Su, Chengran Yang, Ting Zhang | - This paper presents a comprehensive empirical study evaluating the performance of Large Language Models (LLMs) on Software Vulnerability Detection (SVD) across multiple programming languages (Python, Java, and JavaScript). - The study evaluates five open-source LLMs using prompt engineering, instruction tuning, and sequence classification fine-tuning, and benchmarks them against fine-tuned small language models (SLMs) and static application security testing (SAST) tools. - The researchers find that LLM effectiveness varies across programming languages, with better performance observed on JavaScript compared to Python and Java. Fine-tuning improves LLM effectiveness in JavaScript but not in Python or Java, while prompt engineering methods tend to improve LLM performance on Python and Java. - The study further explored improving SVD by training LLMs with downsampled balanced datasets which showed performance improvement but mostly when original data is imbalanced.  Ensemble methods did not yield substantial improvements in the time-aware split setting used.  - The results suggest that data characteristics and the choice of LLM adaptation strategy play crucial roles in SVD. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs](https://arxiv.org/abs/2503.01378) | Artyom Myshlyaev, Oleg Sautenkov, Muhammad Haris Khan, Valerii Serpiva, Artem Lykov | - This paper introduces CognitiveDrone, a Vision-Language-Action (VLA) model for complex Unmanned Aerial Vehicles (UAVs) tasks requiring advanced cognitive abilities.  - The model processes first-person visual inputs and textual instructions to generate real-time 4D action commands and is trained on a dataset of over 8,000 simulated flight trajectories across three categories: Human Recognition, Symbol Understanding, and Reasoning. - An enhanced model, CognitiveDrone-R1, integrates a Vision-Language Model (VLM) reasoning module to refine task directives before high-frequency control.  - Evaluations on the CognitiveDroneBench benchmark show that CognitiveDrone achieves a 59.6% overall success rate, significantly outperforming the racing-oriented RaceVLA model (31.3%).  - CognitiveDrone-R1 further improves performance to 77.2%, demonstrating the effectiveness of integrating reasoning capabilities into UAV control systems. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions](https://arxiv.org/abs/2503.00502) | Peng Hang, Chen Lv, Chengkai Xu, Jiaqi Liu, FanGShiYuu | - This paper introduces a novel Actor-Reasoner framework for enhancing autonomous vehicle (AV) interactions with human-driven vehicles (HVs), leveraging a large language model (LLM). - The framework employs a parallel architecture inspired by the dual-system model of behavioral science, combining a fast, memory-retrieval based Actor and a slow, LLM-driven Reasoner that utilizes Chain-of-Thought (CoT) reasoning. - The Actor rapidly retrieves decisions from a memory database populated during training interactions with diverse simulated HVs, while the Reasoner infers HV driving styles and generates external Human-Machine Interface (eHMI) displays. - Ablation studies and comparisons demonstrate that the framework significantly improves both safety and efficiency metrics compared to baseline methods. - Field tests in a real-world environment validate the framework's effectiveness and generalizability across various interaction scenarios. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/FanGShiYuu/Actor-Reasoner) | N/A |
| [SwiLTra-Bench: The Swiss Legal Translation Benchmark](https://arxiv.org/abs/2503.01372) | Yingqiang Gao, Sina Ahmadi, Luka Nenadic, Jakob Merane, Joel Niklaus | - This paper introduces SwiLTra-Bench, a multilingual benchmark for Swiss legal text translation, containing over 180,000 aligned pairs across five languages. - The benchmark evaluates various Large Language Models (LLMs), including frontier models like Claude and Llama, specialized translation models like MADLAD-400, and fine-tuned open-source LLMs. - Evaluation results demonstrate that frontier models perform best overall, outperforming even specialized translation models, while fine-tuning significantly improves open-source LLM performance. - The study also presents SwiLTra-Judge, an LLM-based evaluation system, which shows better alignment with human expert assessments than traditional metrics. - This benchmark aims to facilitate research in legal text translation, particularly for multilingual countries like Switzerland. | ['Translation', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/collections/joelniklaus/swiltra-bench-67c569a2ada47e4549733deb) |


## Papers for 2025-03-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MPO: Boosting LLM Agents with Meta Plan Optimization](https://arxiv.org/abs/2503.02682) | sujianli, songff, Adagio, Rsy24, xwm | - This paper introduces Meta Plan Optimization (MPO), a framework designed to enhance the planning abilities of Large Language Model (LLM) agents by incorporating explicit high-level guidance through meta plans. - MPO utilizes a meta planner to generate these abstract strategy plans, which are then optimized based on feedback from the agent's performance in the task environment, addressing common issues like planning hallucinations. - The framework employs Monte Carlo sampling to assess the effectiveness of the meta plans by measuring task completion rates, and uses Direct Preference Optimization (DPO) to refine the meta planner based on comparisons between high- and low-performing plans.  - Experimental results on ALFWorld and ScienceWorld benchmarks show MPO significantly improves agent performance, with gains up to 100%, outperforming existing baselines and explicit guidance methods.  - MPO provides a plug-and-play solution compatible with existing agent training frameworks, enabling efficient integration and performance boosts without retraining. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/WeiminXiong/MPO) | N/A |
| [Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs](https://arxiv.org/abs/2503.02846) | Kai Chen, Chengqi Lyu, lindahua, ZwwWayne, vanilla1116 | - Mask-DPO, a novel fine-grained factuality alignment method for Large Language Models (LLMs), leverages sentence-level factuality as mask signals to resolve ambiguity in preference learning. - Unlike traditional Direct Preference Optimization (DPO), Mask-DPO learns exclusively from factually accurate sentences in preferred samples, avoiding penalties on factual content in non-preferred samples. - Experimental results show Mask-DPO significantly improves LLM factuality on in-domain and out-of-domain datasets, even with unseen topics during training.  - For instance, Mask-DPO boosts Llama3.1-8B-Instruct's score on the ANAH test set from 49.19% to 77.53%, exceeding Llama3.1-70B-Instruct (53.44%) and vanilla DPO (68.44%). - Further analysis suggests scaling the number of training topics is more impactful than increasing question diversity for factuality alignment. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/ANAH) | N/A |
| [LADDER: Self-Improving LLMs Through Recursive Problem Decomposition](https://arxiv.org/abs/2503.00735) | akiray1, TamasSimonds | - This paper introduces LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework that enables Large Language Models (LLMs) to improve their problem-solving abilities by recursively generating and solving easier versions of complex problems. - Unlike prior approaches requiring curated datasets or human feedback, LADDER leverages the model's existing capabilities to generate easier question variants and uses reinforcement learning with a verifiable reward system to guide learning. - LADDER improved Llama 3.2 3B's accuracy on undergraduate-level integration problems from 1% to 82% and enabled Qwen2.5 7B to achieve 73% on the MIT Integration Bee qualifying exam. - The paper also introduces Test-Time Reinforcement Learning (TTRL), which further improves the model's performance to 90% on the MIT Integration Bee by dynamically generating and learning from problem variants during inference. - The results demonstrate self-directed learning can achieve significant capability improvements without relying on architectural scaling or human supervision. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Wikipedia in the Era of LLMs: Evolution and Risks](https://arxiv.org/abs/2503.02879) | Yao Wan, fjchendp, mgeng, sdzzxyl, hsm316 | - This paper analyzes the impact of Large Language Models (LLMs) on Wikipedia content and downstream NLP tasks. - It quantifies LLM influence on Wikipedia pages across categories, analyzes word usage changes, and examines LLM-generated content's effects on machine translation and Retrieval-Augmented Generation (RAG). - Findings reveal a potential decline in page views for certain Wikipedia categories, limited but increasing LLM influence on article content, potential inflation of machine translation benchmark scores, and decreased RAG effectiveness with LLM-generated content. - Simulations estimate a 1-2% LLM impact in some Wikipedia categories, suggesting a growing influence over time. - The study highlights the need for careful assessment of potential risks associated with LLMs' growing impact on Wikipedia and NLP research. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/HSM316/LLM_Wikipedia) | N/A |
| [MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents](https://arxiv.org/abs/2503.01935) | mikewang, ShuyiGuo, Thomas-X-Yang, zhaochenhong, Leozkl | - MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse interactive scenarios, capturing both collaborative and competitive dynamics, is introduced. - The benchmark employs novel milestone-based key performance indicators (KPIs) to measure not only task completion but also the quality of collaboration and competition. - Various coordination protocols, including star, chain, tree, and graph topologies, as well as innovative strategies like group discussion and cognitive planning, are evaluated within the framework. - Notably, gpt-4o-mini achieves the highest average task score, the graph structure performs best among coordination protocols in research scenarios, and cognitive planning improves milestone achievement rates by 3%. - Agents exhibit emergent social behaviors, including strategic information sharing and trust-polarized collaboration, providing insights into AGI-level collaboration. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/MultiagentBench/MARBLE) | N/A |
| [PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization](https://arxiv.org/abs/2503.01328) | Min Lin, Xinyi Wan, JialinLi, huanggx-sea, QPHutu | - PipeOffload is a novel pipeline schedule designed to optimize memory and throughput for training large language models (LLMs). - It leverages memory offloading to the host, capitalizing on the natural gap between the forward and backward passes in pipeline parallelism, to reduce peak activation memory by up to 4x with minimal throughput loss, compared to interleaved 1F1B. - It introduces a selective offload strategy which prioritizes activations with longer lifespans, allowing for further peak memory optimization. - In settings with limited memory constraints, it extends the interleaving strategy into a more generalized form to manage the trade-off between memory and throughput, enabling flexible performance adjustments. - By significantly decreasing the memory footprint, PipeOffload enables the exclusive utilization of pipeline parallelism for training large models, surpassing the performance of existing hybrid parallelism approaches that combine pipeline and tensor parallelism by 12%-19%. | ['Natural Language Processing'] | [Link](https://github.com/sail-sg/zero-bubble-pipeline-parallelism) | N/A |
| [Iterative Value Function Optimization for Guided Decoding](https://arxiv.org/abs/2503.02368) | Ruizhe Chen, jokephp, ab3223323, lljhbxt, zhliu | - This paper proposes Iterative Value Function Optimization (IVO), a novel framework for enhancing guided decoding in large language models (LLMs). - IVO combines Monte Carlo Value Estimation with Iterative On-Policy Optimization, improving the accuracy and exploration capabilities of the value function, which guides text generation. - Experimental results across summarization, multi-turn dialogue, and instruction-following tasks demonstrate IVO's superior performance compared to existing baselines like FUDGE, VAS, ARGS, DPO, and IPO. - IVO achieves 77.52% win-rate against the base policy in multi-turn dialogue as judged by GPT-4, showcasing improved alignment. - IVO enhances safety against adversarial jailbreak attacks and demonstrates strong value function transferability across different model sizes. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling](https://arxiv.org/abs/2502.14856) | yuxuanli, zwl96, hyx21, ThonyPan, Achazwl | - FR-Spec, a frequency-ranked speculative sampling framework, is introduced to accelerate large-vocabulary language model (LLM) generation by optimizing draft candidate selection. - FR-Spec compresses the vocabulary space by constraining the draft search to a frequency-prioritized token subset, thereby reducing the computational overhead of the language modeling (LM) head. - This method reduces LM Head computation by 75% while maintaining the equivalence of the final output distribution to the original sampling method. - Experiments demonstrate an average of 1.12x speedup over EAGLE-2, the state-of-the-art speculative sampling method, and a 1.08x speedup over Medusa when integrated with FR-Spec. - The paper shows that FR-Spec supports both greedy decoding and random sampling and provides analysis of the tradeoff between drafting accuracy and time. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking](https://arxiv.org/abs/2503.00955) | Thanh T. Tran, ThanhDi, TienAnh, xuandin, DavidNguyen | - SemViQA, a novel Vietnamese fact-checking framework, combines Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) to enhance accuracy and efficiency in misinformation detection. - SER integrates TF-IDF for fast keyword matching and a Question Answering Token Classifier (QATC) for deeper semantic analysis, balancing speed and precision in evidence retrieval. - TVC employs a hierarchical classification approach, first categorizing claims into Supported, Refuted, or Not Enough Information (NEI), then refining hard cases with a binary classifier using Focal Loss. - Data processing optimizes long-token sequence handling, addressing limitations of Transformer-based models, crucial for capturing context in complex claims. - Evaluation on ISE-DSC01 and ViWikiFC datasets shows state-of-the-art performance with 78.97% and 80.82% strict accuracy, respectively, outperforming existing baselines and demonstrating its effectiveness in Vietnamese fact verification. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/DAVID-NGUYEN-S16/SemViQA) | N/A |
| [UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface](https://arxiv.org/abs/2503.01342) | windmillknight, Shawnee-bxy, Haiyang-W, chenweix7, kanashi6 | - UFO, a unified framework, integrates fine-grained visual perception tasks, such as object detection and image segmentation, with open-ended language interfaces, eliminating the need for task-specific decoders. - The framework leverages a novel embedding retrieval approach for segmentation, treating the text embedding of a mask token as a query to retrieve corresponding visual features, thereby generating masks. - For tasks requiring multiple predictions, like object detection, UFO employs a parallel decoding strategy with local visual features acting as prompts, enhancing efficiency and scalability. - Evaluated on a multi-task benchmark, UFO surpasses the previous state-of-the-art generalist model, GiT, by significant margins, specifically 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. - This unified approach streamlines the integration of fine-grained visual perception within multimodal large language models (MLLMs), enabling broader applications and more complex tasks, such as reasoning segmentation. | ['Multimodal', 'Object Detection', 'Image Segmentation', 'Computer Vision'] | [Link](https://github.com/nnnth/UFO) | N/A |
| [Language Models can Self-Improve at State-Value Estimation for Better Search](https://arxiv.org/abs/2503.02878) | rittera, emendes3 | - This paper introduces self-taught lookahead (STL), a self-supervised method that leverages state-transition dynamics to train a value model for language model-controlled search, eliminating the need for ground truth rewards or human demonstrations. - STL improves value estimation by fine-tuning an LLM value model on examples generated through single-step lookahead during tree search, incorporating the next best action, resulting state, rationale, and value. - Experiments on WebShop and Game-of-24 demonstrate that STL significantly improves performance compared to base LLM value models, matching or exceeding the performance of larger models like GPT-40. - STL achieves Pareto optimality in terms of cost and environment usage, showing a 37x cost reduction and significant state reduction compared to MCTS methods while outperforming previous LLM tree search approaches. - Analysis of scaling trends reveals that STL effectively improves value estimation even with smaller language models (<3 billion parameters). | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/ethanm88/self-taught-lookahead) | N/A |
| [SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models](https://arxiv.org/abs/2503.02876) | Ekaterina Ivanova, alpchel, mgvz | - SPIDER (Supervised Pathology Image-DEscription Repository) is introduced as the largest publicly available patch-level dataset for multi-organ histopathology, including Skin, Colorectal, and Thorax, with comprehensive class coverage and expert-verified annotations. - The dataset includes context patches surrounding each central 224x224px patch at 20x magnification, enhancing classification performance by providing spatial context and mitigating challenges in distinguishing features like fat from background solely from the central patch. - Baseline models trained on SPIDER utilize the Hibou-L foundation model as a feature extractor and an attention-based classification head, achieving state-of-the-art performance across the included tissue categories. - These models serve as benchmarks and enable rapid identification of significant areas, quantitative tissue metric calculation, and act as a foundation for multimodal approaches. - Both SPIDER and the trained models are open-sourced to foster collaboration, reproducibility, and advancement in digital pathology. | ['Image Classification', 'Image Segmentation', 'Multimodal', 'Image Feature Extraction'] | [Link](https://github.com/HistAI/SPIDER) | N/A |
| [Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content](https://arxiv.org/abs/2503.02357) | Zicheng Zhang, GTZhai, a9108, sl2782087, wcain | - This paper introduces Q-Eval-100K, the largest text-to-vision evaluation dataset with mean opinion scores (MOS), containing 100K instances (60K images and 40K videos) and 960K human annotations focused on visual quality and alignment. - The authors propose Q-Eval-Score, a unified model that leverages large multimodal models (LMMs) and a context prompt learning approach for decoupled evaluation of visual quality and alignment. - For enhanced alignment evaluation with long prompts, a Vague-to-Specific strategy is introduced, where prompts are separated into core and detailed variants for more accurate scoring via weighted averaging. - Experimental results show Q-Eval-Score outperforms existing methods on Q-Eval-100K, achieving high correlation with human evaluations. - Cross-dataset validation demonstrates strong generalization capabilities of the model and the value of the Q-Eval-100K dataset. | ['Text-to-Image', 'Text-to-Video', 'Multimodal'] | [Link](https://github.com/zzc-1998/Q-Eval) | N/A |
| [IterPref: Focal Preference Learning for Code Generation via Iterative Debugging](https://arxiv.org/abs/2503.02783) | Ruihang, yangyu90, Jianwen2003, CharonBony, Ringo1110 | - IterPref, a novel framework for preference learning in Code LLMs, leverages iterative debugging insights to refine code generation. - Unlike existing methods that rely solely on pass rates for preference pairs, IterPref contrasts critical tokens between corrected and previous versions to pinpoint specific errors. - A new iterative debugging dataset, CodeFlow, is introduced, where samples are refined until tests pass, capturing error corrections for informative preference pair generation. - IterPref incorporates a tailored DPO algorithm that masks non-critical tokens in the dispreferred sample, improving fine-grained alignment. - Experimental results demonstrate that IterPref achieves significant performance gains on coding benchmarks, including BigCodeBench, and generates code with fewer errors. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [AppAgentX: Evolving GUI Agents as Proficient Smartphone Users](https://arxiv.org/abs/2503.02268) | Chi Zhang, Wenjia Jiang, xuyang, ChenxiSong, yyzhuang2 | - AppAgentX, a novel evolutionary framework for GUI agents, enhances operational efficiency while retaining intelligence and flexibility in interacting with graphical user interfaces (GUIs). - It incorporates a memory mechanism to record task execution history and identify repetitive action sequences, which are then replaced with higher-level actions to create shortcuts and improve performance.  - This approach allows the agent to focus on complex reasoning tasks while streamlining routine actions. - Experimental results demonstrate AppAgentX's significant outperformance over existing methods in both efficiency and accuracy on multiple benchmark tasks such as AppAgent, DroidTask, and MobileBench. - The framework utilizes visual information, eliminating the need for backend access or APIs, and leverages GPT-4, LangGraph, Neo4j, Pinecone, and ResNet-50 for its core components. | ['Multimodal'] | N/A | N/A |


## Papers for 2025-03-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.01785) | yhcao, sweetFruit, yuhangzang, Zery, ziyuliu | - Introduces Visual Reinforcement Fine-Tuning (Visual-RFT), a novel approach for enhancing the visual perception and reasoning capabilities of Large Vision-Language Models (LVLMs) using reinforcement learning with verifiable rewards. - Visual-RFT employs a policy optimization algorithm, such as Group Relative Policy Optimization (GRPO), guided by task-specific, rule-based verifiable reward functions (e.g., Intersection over Union (IoU) for object detection). - Demonstrates superior performance compared to Supervised Fine-tuning (SFT) across diverse visual tasks, including few-shot image classification, open-vocabulary object detection, and reasoning grounding, especially in data-scarce scenarios. - Achieves significant improvements in few-shot learning, boosting accuracy by 24.3% in one-shot fine-grained image classification with limited samples, and exceeding SFT baselines in few-shot object detection on COCO and LVIS datasets. - Showcases advanced generalization ability by successfully transferring knowledge to novel and rare categories in open-vocabulary object detection, improving mAP by substantial margins on both COCO and LVIS benchmarks. | ['Multimodal', 'Computer Vision', 'Reinforcement Learning', 'Image Classification', 'Object Detection', 'Zero-Shot Object Detection'] | [Link](https://github.com/Liuziyu77/Visual-RFT) | N/A |
| [Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs](https://arxiv.org/abs/2503.01743) | vishravmsft, martincai, alonbenhaim, jianmin-ustc, atabakashfaqMSFT | - Phi-4-Mini and Phi-4-Multimodal are compact language and multimodal models, respectively, trained on curated web and synthetic data. - Phi-4-Mini, a 3.8B parameter model, outperforms similarly sized open-source models and matches larger models on complex reasoning tasks, utilizing a 200K token vocabulary and group query attention. - Phi-4-Multimodal integrates text, vision, and speech/audio using LoRA adapters and modality-specific routers, achieving state-of-the-art performance in multiple inference modes. - This "Mixture of LoRAs" approach allows flexible modality combinations without interference, exemplified by its top ranking on the OpenASR leaderboard with a compact speech/audio LoRA. - A reasoning-enhanced version of Phi-4-Mini rivals larger models like DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B in reasoning tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Automatic Speech Recognition', 'Translation', 'Summarization', 'Question Answering', 'Text2Text Generation', 'Text Generation', 'Natural Language Processing'] | N/A | N/A |
| [OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment](https://arxiv.org/abs/2502.18965) | GuoruiZhou, DingWF, caikuo, oneself, OrpheusBetter | - OneRec, a unified end-to-end generative framework, is proposed for single-stage recommendation, surpassing traditional cascaded ranking systems. - OneRec uses an encoder-decoder structure with sparse Mixture-of-Experts (MoE) to handle user behavior sequences and generate videos of interest efficiently. - It employs a session-wise list generation approach, considering context and order within a session, unlike point-by-point next-item prediction. - An Iterative Preference Alignment (IPA) module with Direct Preference Optimization (DPO) enhances generated results by learning from self-hard rejected samples ranked by a reward model. - Deployed in Kuaishou, a short-video platform, OneRec achieved a 1.6% increase in watch-time, demonstrating substantial improvement. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [Liger: Linearizing Large Language Models to Gated Recurrent Structures](https://arxiv.org/abs/2503.01496) | Yu Cheng, JusenK, Jiaxihu2, weigao266, landisen | - Liger is a novel method for linearizing large language models (LLMs), converting pre-trained Transformer-based LLMs into gated linear recurrent models without introducing additional parameters, enabling efficient deployment. - It repurposes the pre-trained key matrix weights to construct diverse gating mechanisms, facilitating various gated recurrent structures. - Using Low-Rank Adaptation (LoRA), Liger restores the linearized model's performance to match the original LLMs with minimal linearization cost. - Liger introduces a hybrid attention mechanism, combining sliding window softmax attention and linear recurrent modeling, accelerating linearization and maintaining LLM capabilities with linear-time inference. - Experimental results on models ranging from 1B to 8B parameters show that Liger outperforms existing linearization methods, achieving competitive results across benchmarks with limited training tokens. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/OpenSparseLLMs/Linearization) | N/A |
| [When an LLM is apprehensive about its answers -- and when its uncertainty is justified](https://arxiv.org/abs/2503.01688) | Alexey Zaytsev, Edvard Khalafyan, DanielVyazhev, aigoncharov, sspetya | - This paper investigates the effectiveness of different uncertainty estimation methods for multiple-choice question-answering tasks using Large Language Models (LLMs). - The study focuses on token-wise entropy and model-as-judge (MASJ) estimates across various question topics and LLM sizes (Phi-4, Mistral, and Qwen). - Results indicate that response entropy effectively predicts model errors in knowledge-dependent domains and correlates with question difficulty (e.g., 0.73 ROC AUC for biology). - However, this correlation weakens in reasoning-dependent domains (e.g., 0.55 ROC AUC for math), suggesting the need to integrate data-related uncertainty within entropy frameworks. - The study also reveals biases in the MMLU-Pro dataset regarding reasoning requirements across different topics, calling for more balanced datasets for fair LLM evaluation. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/LabARSS/question-complextiy-estimation) | N/A |
| [DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion](https://arxiv.org/abs/2503.01183) | Guobin Ma, Chunbo Hao, Yuepeng Jiang, Huakang Chen, Ziqian Ning | - DiffRhythm is a novel diffusion-based model for generating full-length songs (up to 4m45s) with both vocals and accompaniment, conditioned on lyrics and a style prompt. - It utilizes a Variational Autoencoder (VAE) trained on a large music dataset (60,000 hours) for high-fidelity music reconstruction, demonstrating robustness against MP3 compression artifacts and sharing the same latent space with Stable Audio VAE. - A Diffusion Transformer (DiT) operates in the VAE's latent space, generating songs through iterative denoising, guided by style prompts, timesteps, and lyrics processed through a sentence-level alignment mechanism for improved vocal intelligibility. - Evaluations show DiffRhythm outperforms SongLM in quality and intelligibility while achieving a ~50x speedup in generation time, with an RTF below 0.04. - It addresses the limitations of existing autoregressive models by enabling faster generation, improving scalability, and maintaining consistency over longer sequences. | ['Text-to-Audio', 'Audio'] | N/A | [Link](https://huggingface.co/spaces/stabilityai/stable-audio-tools), [Link](https://huggingface.co/transformers) |
| [Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs](https://arxiv.org/abs/2503.01307) | ngoodman, nlile, Asap7772, ayushchakravarthy, obiwan96 | - This paper investigates why some large language models (LLMs) improve significantly with reinforcement learning (RL) while others plateau, focusing on the presence of key cognitive behaviors in base LLMs. - It introduces a framework analyzing four cognitive behaviors: verification, backtracking, subgoal setting, and backward chaining, finding Qwen exhibits these more than Llama. - Priming Llama with examples demonstrating these behaviors, even incorrect solutions with correct reasoning patterns, substantially improved its RL performance, matching Qwen. - The study suggests the presence of these cognitive behaviors in the initial policy is crucial for effectively utilizing increased test-time compute through extended reasoning sequences. - Modifying pre-training data to emphasize these behaviors enabled Llama to achieve comparable self-improvement to Qwen, highlighting the importance of initial reasoning behaviors in enabling self-improvement through RL. | ['Natural Language Processing', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/kanishkg/cognitive-behaviors) | N/A |
| [Speculative Ad-hoc Querying](https://arxiv.org/abs/2503.00714) | Venkat Arun, Aditya Akella, Maria Angels de Luis Balaguer, Srikanth Kandula, Haoyu0529 | - SpeQL, a system for speculative ad-hoc querying, is introduced to reduce query latency by predicting and pre-executing queries while the user is still typing. - It leverages LLMs to predict query structure and precompute temporary tables containing the likely needed information and continuously displays results for speculated queries. - A user study shows SpeQL improved task completion time and aided in discovering data patterns more quickly. - Using TPC-DS queries and Amazon Redshift, SpeQL reduced planning, compilation, and execution latency by 94.42%, 99.99%, and 87.23%, respectively, with a P90 overhead of 7.72 seconds. - Open-sourced as a VS Code plugin and demonstrates potential for integration into database management systems. | ['Natural Language Processing', 'Question Answering', 'Table Question Answering'] | [Link](https://github.com/lihy0529/SpeQL) | N/A |
| [DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting](https://arxiv.org/abs/2503.00784) | xpqiu, QipengGuo, KYLN24, KaiLv | - DuoDecoding is a novel hardware-aware heterogeneous speculative decoding method with dynamic multi-sequence drafting designed to accelerate large language model (LLM) text generation. - It strategically deploys the draft model on CPU and the target model on GPU, enabling parallel decoding and reducing draft model overhead. - A hardware-aware optimal draft budget minimizes idle times on both CPU and GPU. - Dynamic multi-sequence drafting enhances the quality of draft outputs based on uncertainty. - Experiments show up to a 2.61x speedup in generation latency compared to conventional autoregressive decoding and 17% reduction in time to first token compared to traditional speculative decoding. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/KaiLv69/DuoDecoding) | N/A |
| [Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions](https://arxiv.org/abs/2503.00501) | Xiaohui He, Jia Chen, aiqy, haitaoli, qian | - This paper introduces Qilin, a multimodal information retrieval dataset collected from Xiaohongshu, a popular social platform with diverse content including image-text notes, video notes, commercial notes, and direct answers. - Qilin includes comprehensive user sessions with heterogeneous results, along with APP-level contextual signals and genuine user feedback, which facilitates advanced multimodal neural retrieval model development. - It contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module, which supports training and evaluation of Retrieval-augmented Generation (RAG) pipelines and analysis of module influence on user search behavior. - Preliminary experiments on search, recommendation and DQA tasks using baselines like BM25, BERT, DCN-V2, and VLM demonstrate the value of incorporating multimodal features and contextual signals. - DCN-V2 excels in search ranking by leveraging user history, features, and embeddings, while VLM demonstrates the effectiveness of visual information in both user modeling and note representation. | ['Multimodal', 'Question Answering', 'Natural Language Processing'] | [Link](https://github.com/RED-Search/Qilin) | N/A |
| [Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia](https://arxiv.org/abs/2503.01714) | Lang Gao, Zhongyu Wei, Ziruibest, Carol0110, Aurora-cx | - This paper investigates how Large Language Models (LLMs) reconstruct the semantics of words with scrambled internal characters (Typoglycemia) by introducing a novel metric, SemRecScore. - SemRecScore quantifies semantic reconstruction by comparing the representation of the original word token with the final subword token of the scrambled word at each layer of the LLM.  - Through experiments on LLaMA models, the study reveals that word form is the primary factor in semantic reconstruction, with contextual information having minimal impact. - LLMs rely on specialized attention heads to process word form information, with this mechanism remaining stable across varying scrambling levels. -  The study identifies a divergence between LLMs' fixed attention on word form and humans' adaptive strategy of balancing word form and context. | ['Natural Language Processing'] | [Link](https://github.com/Aurora-cx/TypoLLM) | N/A |
| [SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity](https://arxiv.org/abs/2503.01506) | bitwjg, WeiWang, WQYC, DeyangKong, xixy | - This paper introduces SampleMix, a novel sample-wise pre-training data mixing strategy for Large Language Models (LLMs) that prioritizes both data quality and diversity. - Unlike traditional domain-wise methods, SampleMix employs a bottom-up approach, performing global sampling based on individual sample evaluations, dynamically determining optimal domain proportions, and adapting to varying token budgets. - SampleMix leverages a quality evaluator trained on GPT-40 annotations to assess data based on seven criteria and employs clustering analysis to gauge sample diversity, combining these metrics to create sample weights for dataset construction. - Experimental results on various downstream tasks and perplexity evaluations demonstrate SampleMix's superior performance compared to existing domain-based methods. - Notably, SampleMix achieves comparable accuracy with significantly fewer training steps, showcasing its training efficiency and its potential for optimizing pre-training data utilization. | ['Natural Language Processing'] | N/A | N/A |
| [From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens](https://arxiv.org/abs/2502.18890) | Yuxuan Wang, zlzheng, vickyandkekey, JunzheS, TongWu | - TOKENSWIFT, a novel framework, accelerates ultra-long sequence generation (up to 100K tokens) with Large Language Models (LLMs) while maintaining the target model's quality. - It addresses three key challenges: frequent model reloading, dynamic key-value (KV) cache management, and repetitive generation, using techniques like multi-token generation, dynamic KV cache updates, and contextual penalties. - Experimental results demonstrate a 3x speedup across various model scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA), translating to hours of time savings. - The acceleration becomes more pronounced with longer sequence lengths and larger model sizes, showing up to 5.54 hours saved for a 14B model generating 100K tokens. - TOKENSWIFT exhibits robust performance across varying prefix lengths and sampling methods, showcasing its versatility and applicability to diverse generation tasks. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/bigai-nlco/TokenSwift) | N/A |
| [CodeArena: A Collective Evaluation Platform for LLM Code Generation](https://arxiv.org/abs/2503.01295) | terryyz, DongHuang-ebay, bobxwu, anhtuanluu36, Elfsong | - CodeArena, an online evaluation framework for Large Language Model (LLM) code generation is introduced to address limitations like benchmark leakage, data dissipation, and limited accessibility in existing evaluation methods. - The framework features a dynamic evaluation system that recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by benchmark leakage. - An open repository of solutions and test cases promotes transparency and facilitates research in LLM code generation, and automation-friendly APIs streamline the evaluation process. - Initial benchmarks using APPS and Mercury datasets demonstrate the platform's capability to assess LLM code generation performance. - CodeArena actively encourages community contribution to diversify the problem set and aims to establish a collaborative platform for evaluating and advancing code generation LLMs. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/) |
| [Large-Scale Data Selection for Instruction Tuning](https://arxiv.org/abs/2503.01807) | pradeepd, pangwei, faezeb, nanami, hamishivi | - This paper investigates the effectiveness of automated data selection methods for large-scale instruction tuning of language models. - The authors find that a variant of Representation-based Data Selection (RDS+), which uses weighted mean pooling of pre-trained LM hidden states, consistently outperforms other methods. - RDS+ improves performance with larger data pools, unlike other methods that decline or match random selection. - In multi-task settings, RDS+ outperforms baselines and human-curated mixtures like the TULU 2 dataset. - The study emphasizes the importance of evaluating data selection methods at scale to reveal their true potential for improving instruction-tuned language models. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/hamishivi/automated-instruction-selection) | N/A |
| [AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding](https://arxiv.org/abs/2503.01063) | dnoever | - This paper proposes a tonal language system for machine-to-machine (M2M) communication, inspired by human cryptophasia and tonal languages like Mandarin. - The system maps ASCII characters to unique frequencies using a logarithmic scale based on musical semitones, spanning a range that includes ultrasonic frequencies beyond human hearing. - A software prototype demonstrates the encoding through visualization, auditory playback, and ABC musical notation, showing potential for information density exceeding human speech. - The work explores the potential for AI systems to develop private languages and provides a technical foundation for understanding and governing such communication. - By encoding messages in both audible and ultrasonic frequencies, the system offers a potential model for both human-interpretable and machine-private M2M communication. | ['Audio', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/reveondivad/cryptophasia) | N/A |
| [CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments](https://arxiv.org/abs/2503.00729) | Qing Zhao, Zhixin Mai, Yiming Zhao, Ge Wang, SP4595 | - CLEA, a closed-loop embodied agent framework, is proposed for enhancing task execution in dynamic environments by incorporating four specialized open-source LLMs with functional decoupling for closed-loop task management. - The framework features two core innovations: an interactive task planner that generates executable subtasks dynamically based on environmental memory, and a multimodal execution critic employing an evaluation framework for probabilistic assessment of action feasibility, which triggers hierarchical re-planning when environmental perturbations exceed predefined thresholds. - Experimental results in a real environment with two heterogeneous robots for object search, object manipulation, and search-manipulation integration tasks demonstrate CLEA's effectiveness. - Across 12 task trials, CLEA outperforms the baseline model, achieving a 67.3% improvement in success rate and a 52.8% increase in task completion rate. - CLEA enhances the robustness of task planning and execution in dynamic environments by enabling adaptive decision-making through real-time environmental feedback and closed-loop perception-reasoning-execution. | ['Robotics', 'Multimodal'] | [Link](https://sp4595.github.io/CLEA/) | N/A |


## Papers for 2025-03-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking](https://arxiv.org/abs/2502.20730) | luyaojie, sanmusunrise, xuanang, yhycai, lzq2021 | - This paper introduces SolutionRAG, a novel system designed for complex engineering solution design, addressing the gap in existing RAG research for this task. - SolutionRAG leverages a tree-based exploration approach, allowing for flexible improvement of solutions, moving from suboptimal to reliable designs by exploring different improvement directions through branching. - It employs a bi-point thinking mechanism, alternating between solution design and review during tree growth to ensure generated solutions meet all real-world constraints specified in the requirements. - A node evaluation and pruning mechanism is incorporated into SolutionRAG to enhance inference efficiency by prioritizing promising solution paths and helpful review comments. - Experimental results on the SolutionBench demonstrate that SolutionRAG achieves state-of-the-art performance, significantly outperforming deep reasoning models and existing RAG approaches. | ['Question Answering'] | [Link](https://github.com/Li-Z-Q/DeepSolution) | N/A |
| [Chain of Draft: Thinking Faster by Writing Less](https://arxiv.org/abs/2502.18600) | Lingxiao Zhao, Wenhao Xie, DeBERTa, sileixu | - This paper introduces Chain of Draft (CoD), a new prompting strategy for Large Language Models (LLMs) that prioritizes concise and efficient reasoning. - Inspired by how humans use drafts to capture key ideas, CoD encourages LLMs to generate minimal intermediate reasoning outputs, contrasting with the verbose nature of Chain-of-Thought (CoT) prompting. - Experiments across arithmetic, common sense, and symbolic reasoning tasks demonstrate that CoD achieves comparable or better accuracy than CoT while significantly reducing token usage and latency. - In GSM8K, CoD achieved 91% accuracy with only 40 tokens, an 80% reduction compared to CoT's 200 tokens and a 76% latency decrease. - This suggests that CoD can make LLMs more practical for real-world applications by improving efficiency without compromising performance. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents](https://arxiv.org/abs/2502.18017) | xpjandy, shihang, vickywu, lovesnowbest, autumncc | - Introduced ViDoSeek, a new benchmark dataset for visual document retrieval-augmented generation focusing on complex reasoning. - Proposed ViDoRAG, a multi-agent RAG framework incorporating iterative reasoning with seeker, inspector, and answer agents. - Employed a Gaussian Mixture Model (GMM)-based hybrid strategy for multimodal retrieval, combining visual and textual features. - Demonstrated state-of-the-art performance on ViDoSeek, outperforming baselines by over 10%. - Showed effectiveness in handling complex reasoning and diverse content types within visually rich documents. | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | [Link](https://github.com/Alibaba-NLP/ViDoRAG) | N/A |
| [SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers](https://arxiv.org/abs/2502.20545) | Coralia Cartis, Wenqi Zhu, Kechen Li, Shiweiliuiiiiiii, jitianbo | - This paper introduces SoS-1K, a dataset of approximately 1,000 polynomials designed to evaluate the reasoning capabilities of LLMs in solving Sum-of-Squares (SoS) problems, a computationally intractable mathematical problem related to Hilbert's 17th problem. - The authors also present SoS-specialized reasoning instructions based on five progressively challenging criteria to guide LLMs in solving SoS problems.  - Their evaluation shows that providing high-quality reasoning instructions significantly improves the accuracy of state-of-the-art LLMs in solving SoS problems, boosting performance by up to 21%. - Fine-tuning a 7B model (SoS-7B) on SoS-1K for 4 hours resulted in an accuracy of 70%, outperforming larger models like DeepSeek-V3 (671B) and GPT-40-mini while requiring significantly less computation time.  - Further analysis suggests that while LLMs demonstrate an understanding of underlying mathematical concepts, they benefit from structured guidance and may exhibit shortcut behavior when tackling complex problems. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Joe-2002/SoS1) | N/A |
| [LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation](https://arxiv.org/abs/2502.20583) | kasikci, kojimano, jungok, kamahori | - LITEASR, a novel low-rank compression method for Automatic Speech Recognition (ASR) encoders, is introduced, which leverages low-rank properties of intermediate activations during inference. - The method uses Principal Component Analysis (PCA) with a small calibration dataset to approximate linear transformations by a chain of low-rank matrix multiplications and optimizes self-attention to operate in the reduced dimension. - Evaluated on Whisper large-v3, LITEASR reduces encoder size by ~40%, leading to a 1.4x speedup with minimal accuracy loss.  - In other configurations, the encoder size is reduced by over 50%, matching Whisper medium's size but achieving better transcription accuracy.  - The approach achieves Pareto-optimal balance between speed and accuracy, which paves the way for efficient ASR deployment. | ['Automatic Speech Recognition', 'Natural Language Processing'] | [Link](https://github.com/efeslab/LiteASR) | N/A |
| [HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](https://arxiv.org/abs/2502.20811) | Fuzheng Zhang, Yuanxing Zhang, Jingyun Hua, Xiao Wang, lwher1996 | - This paper introduces a two-stage data annotation pipeline and two associated datasets (HAICTrain and HAICBench) to improve human action understanding and generation for Multi-modal Large Language Models (MLLMs). - The pipeline improves on existing video captioning methods by focusing on fine-grained details of human actions, including attributes to distinguish individuals, body movements, and interactions. - HAICTrain consists of 126K video-caption pairs, and HAICBench contains 500 human-annotated video-caption pairs and 1400 QA pairs. - Experimental results show that training with HAICTrain significantly improves human action understanding across four benchmarks. - Additionally, the refined captions also show improvements in text-to-video generation on MovieGenBench. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Text-to-Video'] | N/A | [Link](https://huggingface.co/datasets/KuaishouHAIC/HAIC) |


## Papers for 2025-02-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Self-rewarding correction for mathematical reasoning](https://arxiv.org/abs/2502.19613) | Nan Jiang, Lichang Chen, Chenlu Ye, Hanning Zhang, Wei Xiong | - This paper introduces a self-rewarding reasoning framework for Large Language Models (LLMs) that enables autonomous error detection and self-correction in mathematical reasoning without external reward models. - The two-stage framework employs sequential rejection sampling to create synthetic training data containing self-rewarding and self-correction examples, which is used in the instruction fine-tuning phase. - It then refines these behaviors using reinforcement learning with a rule-based reward signal during the reinforcement learning phase. - Experiments on Llama-3 and Qwen-2.5 demonstrate superior performance over intrinsic self-correction methods and comparable results to systems with external reward models. - Results show that self-rewarding correction improves final accuracy and efficiency in test-time compute scaling compared to other baselines. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/RLHFlow/Self-rewarding-reasoning-LLM) | [Link](https://huggingface.co/AI-MO/NuminaMath-7B-COT) |
| [MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning](https://arxiv.org/abs/2502.19634) | Jiayuan Zhu, Fenglin Liu, Junde Wu, Jiazhen Pan, che111 | - MedVLM-R1, a novel 2B parameter medical Vision-Language Model (VLM), is introduced, designed for enhanced reasoning capabilities in radiological Visual Question Answering (VQA) tasks by leveraging Group Relative Policy Optimization (GRPO). - Unlike conventional Supervised Fine-Tuning (SFT) methods, MedVLM-R1 employs reinforcement learning, incentivizing the model to generate natural language reasoning alongside final answers, thus improving transparency and trustworthiness. - Demonstrating superior generalization, MedVLM-R1 achieves robust performance on out-of-distribution data (MRI → CT/X-ray), surpassing larger models like Qwen2VL-72B and Huatuo-GPT-Vision-7B trained on significantly more data. - Trained on only 600 samples, MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming significantly larger models trained on over a million samples. - By combining medical image analysis with explicit reasoning generation, MedVLM-R1 marks a significant advancement towards trustworthy and interpretable AI in clinical settings. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning', 'Computer Vision'] | N/A | N/A |
| [R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts](https://arxiv.org/abs/2502.20395) | Tianyi Zhou, Ziyue Li, Zhongyang Li | - This paper introduces R2-T2 (Re-Routing in Test-Time), a novel test-time optimization method for Multimodal Mixture-of-Experts (MoE) models that dynamically adjusts routing weights without requiring additional training. - R2-T2 addresses the suboptimality of pre-trained routers by leveraging information from similar samples in a reference set to refine the routing weights for test samples, thereby enhancing expert selection. - Three strategies for implementing R2-T2 are proposed including neighborhood gradient descent, kernel regression and mode finding. - Extensive experiments across various benchmarks demonstrate substantial improvements over strong baselines including larger VLM models. - Analysis shows that R2-T2 effectively refines routing, boosting correct predictions and mitigating the original router's over-reliance on a single expert. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/tianyi-lab/R2-T2) | N/A |
| [LongRoPE2: Near-Lossless LLM Context Window Scaling](https://arxiv.org/abs/2502.20082) | Gilsinia Lopez, Gaokai Zhang, Siyuan Wang, Li Lyna Zhang, Ning Shang | - LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) while preserving performance on the original shorter context window. - It achieves this through three contributions: (1) identifying insufficient training in higher RoPE dimensions as a key contributor to out-of-distribution (OOD) issues, (2) developing an effective RoPE rescaling algorithm using evolutionary search guided by "needle-driven" perplexity, and (3) employing mixed context window training to adapt model weights to rescaled RoPE for long sequences while maintaining short-context performance with original RoPE. - LongRoPE2 extends LLaMA3-8B to a 128K effective context length while retaining over 98.5% of short-context performance using only 10B training tokens — 80x fewer than Meta's approach, which fails to achieve the same effective context length. - It outperforms existing methods like YaRN, NTK, and LongRoPE on long context benchmarks like RULER and real-world datasets like LOFT, InfiniteBench, and LongBench. - Additionally, LongRoPE2 maintains strong performance on standard short-context benchmarks, minimizing the performance drop often observed in context window extension methods. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/microsoft/LongRoPE) | N/A |
| [FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving](https://arxiv.org/abs/2502.20238) | Chaoqun Liu, Hou Pong Chan, Hao Zhang, Weiwen Xu, Guizhen Chen | - Introduces FINEREASON, a logic-puzzle benchmark designed for granular evaluation of Large Language Models' (LLMs) reasoning capabilities, focusing on intermediate steps rather than just final-answer accuracy. - Proposes two evaluation tasks: *state checking*, predicting the solvability of a given state, and *state transition*, determining the next valid move in the puzzle-solving process. - Employs a tree-based decomposition of logic puzzles into atomic steps, allowing rigorous validation of the intermediate states and transitions. - Demonstrates that models trained on FINEREASON's state-checking and transition tasks show improved performance on mathematical reasoning benchmarks by up to 5.1% on GSM8K. - Reveals that reasoning-oriented models (OpenAI-o1 and Gemini-2.0-Flash-Thinking) significantly outperform general-purpose models on FINEREASON, but even leading reasoning models have substantial limitations in deep reasoning tasks, particularly in state transition accuracy. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/DAMO-NLP-SG/FineReason) | N/A |
| [CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale](https://arxiv.org/abs/2502.16645) | Kaiyue Qiu, Zhaoyang Chu, Chenlong Wang, yxy0807, zx10086 | - This paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. - Based on CODESYNC, the authors develop CODESYNCBENCH, a benchmark for assessing LLMs' ability to stay synchronized with code evolution, covering real-world updates for 220 APIs from six Python libraries. - The benchmark includes 3,300 test cases across three evaluation tasks (code completion, error correction, and multiple-choice questions) and an update-aware instruction tuning dataset with 2,200 training examples. - Experimental results on 14 state-of-the-art LLMs show that they struggle with dynamic code evolution, even with advanced knowledge updating methods. - The benchmark and dataset aim to facilitate the development of real-time code knowledge updating in LLMs. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Lucky-voyage/Code-Sync) | N/A |
| [Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance](https://arxiv.org/abs/2502.16944) | Zhixu Li, Pu Zhao, Lu Wang, Chenghua Huang, keanudicap | - This paper introduces Decoupled Value Policy Optimization (DVPO), a new framework for Reinforcement Learning from Human Feedback (RLHF) that replaces traditional reward modeling with a pre-trained global value model (GVM). - The GVM in DVPO predicts token-level return-to-go estimates and guides policy optimization, decoupling value and policy training to reduce computational complexity and instability. - DVPO reduces GPU memory usage by 40% and training time by 35% compared to conventional RLHF, while maintaining performance comparable to state-of-the-art methods like PPO. - Experiments on benchmarks such as MT-Bench, Alpaca-Eval, and Arena-Hard show DVPO outperforms efficient RLHF methods (e.g., DPO) and matches or exceeds PPO performance across different model sizes. - Theoretical analysis demonstrates that pretraining a reward model and a global value model are functionally interchangeable in offline RLHF where no new ground-truth rewards are available. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B), [Link](https://huggingface.co/meta-llama/Llama-3.2-3B), [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2), [Link](https://huggingface.co/openbmb/UltraRM-13b), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), [Link](https://huggingface.co/ziniuli/Mistral-7B-ReMax-v0.1) |
| [UniTok: A Unified Tokenizer for Visual Generation and Understanding](https://arxiv.org/abs/2502.20321) | Xin Yu, Jihan Yang, Junfeng Wu, Yi Jiang, Chuofan Ma | - UniTok, a novel discrete visual tokenizer, is introduced to bridge the gap between visual generation and understanding tasks by encoding fine-grained details for generation while capturing high-level semantics for understanding. - The paper argues that the performance bottleneck of existing unified tokenizers stems from the limited representational capacity of discrete tokens rather than conflicting learning objectives. - To address this limitation, UniTok employs multi-codebook quantization, which divides vector quantization with independent sub-codebooks, effectively scaling the latent code space and enhancing representation expressiveness. - UniTok incorporates attention factorization using multi-head attention modules to preserve richer semantics during token factorization. - Experimental results demonstrate that UniTok achieves state-of-the-art performance in both generation and understanding tasks, as evidenced by a 0.38 rFID on ImageNet reconstruction and 78.6% zero-shot accuracy, exceeding existing unified tokenizers and even outperforming some domain-specific models. | ['Multimodal', 'Text-to-Image', 'Visual Question Answering', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/FoundationVision/UniTok) | N/A |
| [Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think](https://arxiv.org/abs/2502.20172) | Haozhe Zhao, Weichu Xie, Wenhao Chai, Shuai Bai, Liang Chen | - DREAM ENGINE, a novel framework for multimodal image generation with text-image interleaved control, is introduced, which leverages Large Multimodal Models (LMMs) like QwenVL and a Diffusion Transformer (DiT) backbone like Stable Diffusion v3.5. - The architecture replaces traditional text encoders with an LMM and a lightweight projector layer for encoding text-image interleaved controls, employing a two-stage training process involving joint text-image alignment and multimodal instruction tuning. - It effectively generates images from complex, interwoven text and image instructions, including merging concepts from different images and handling object-driven compositional tasks.  - Evaluation on the GenEval benchmark shows a competitive overall score of 0.69, matching state-of-the-art models like SDv3.5 (0.71) and surpassing FLUX.1 Dev (0.66) and showing superior image reconstruction performance. - The model exhibits emergent capabilities, like synthesizing concepts from different input images, highlighting the potential of LMMs as unified multimodal instruction encoders. | ['Text-to-Image', 'Multimodal'] | [Link](https://github.com/chenllliang/DreamEngine) | N/A |
| [NeoBERT: A Next-Generation BERT](https://arxiv.org/abs/2502.19587) | Sarath Chandar, Mariam El Mezouar, Quentin Fournier, Lola Le Breton | - This paper introduces NeoBERT, a next-generation bidirectional encoder model for Natural Language Processing (NLP) tasks. - NeoBERT integrates state-of-the-art advancements in architecture (optimal depth-to-width ratio, RoPE positional embeddings, SwiGLU activation, RMSNorm), data (RefinedWeb), and pre-training methodologies (extended context length of 4096 tokens, two-stage pre-training). - With a compact size of 250 million parameters, NeoBERT achieves state-of-the-art results on the MTEB benchmark, outperforming larger models such as BERTlarge and ROBERTalarge, under identical fine-tuning conditions. - It also demonstrates superior performance on the GLUE benchmark, comparable to existing large models while being significantly smaller. - The authors release all code, data, checkpoints, and training scripts to facilitate further research and adoption. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | [Link](https://github.com/chandar-lab/NeoBERT) | [Link](https://huggingface.co/chandar-lab/NeoBERT) |
| [SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning](https://arxiv.org/abs/2502.20127) | Yanzhen Zou, Xiangxin Meng, Pengfei Gao, Chao Peng, mizersy | - This paper introduces SoRFT (Subtask-oriented Reinforced Fine-Tuning), a novel training approach to enhance the issue-resolving capability of Large Language Models (LLMs). - SoRFT decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation, and uses a two-stage training process. - The first stage involves rejection-sampled supervised fine-tuning using Chain of Thought (CoT) data filtered with ground truth. - The second stage employs rule-based reinforcement learning leveraging Proximal Policy Optimization (PPO) with ground-truth-based rewards. - Experimental results on SWE-Bench Verified and SWE-Bench Lite show SoRFT-trained LLMs achieve state-of-the-art performance among open-source models, resolving 21.4% of issues on SWE-Bench Verified with SoRFT-Qwen-7B. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/OpenDevin/CodeQwen1.5-7B-OpenDevin) |
| [R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning](https://arxiv.org/abs/2502.19735) | Hongyong Zeng, Yuanchang Luo, Shimin Tao, Yilun Liu, boommmmm | - R1-Translator (R1-T1) is a novel framework that uses reinforcement learning (RL) with human-aligned Chain-of-Thoughts (CoTs) to improve inference-time reasoning for general machine translation (MT). - The framework incorporates six common CoT patterns observed in human translation workflows, extending reasoning-based MT beyond specialized sub-tasks to diverse tasks and six languages. - R1-T1 uses a KL-constrained RL process to facilitate the discovery of new CoT trajectories and enable anti-forgetting adaptation for unseen translation scenarios. - Experimental results on the Flores-101 test set across 21 languages and 80 translation directions demonstrate consistent improvement, particularly in 15 languages unseen during training, while preserving general multilingual capabilities compared to plain supervised fine-tuning (SFT). - The approach addresses the limitations of existing methods by aligning CoTs with human strategies, enabling adaptability to new domains, and mitigating catastrophic forgetting. | ['Translation', 'Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |


## Papers for 2025-02-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Kanana: Compute-efficient Bilingual Language Models](https://arxiv.org/abs/2502.18934) | seopbo, Doohae, daniel-rl2, jiyeonham, bzantium | - This paper introduces Kanana, a series of bilingual (Korean and English) language models trained with a focus on computational efficiency. - The models range from 2.1B to 32.5B parameters and utilize techniques like staged pre-training, depth up-scaling, and pruning and distillation to reduce training costs. - Kanana models achieve state-of-the-art performance on Korean benchmarks (KMMLU, HAE-RAE) while maintaining competitive results on English benchmarks (MMLU). - Post-training methods like supervised fine-tuning and preference optimization enhance instruction following and user interaction capabilities. - The models are adapted for various downstream tasks like embedding generation, retrieval-augmented generation, and function calling, demonstrating their versatility. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Feature Extraction'] | [Link](https://github.com/kakao/kanana) | [Link](https://huggingface.co/kakaocorp) |
| [TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding](https://arxiv.org/abs/2502.19400) | Jonathan Leung, AlvinYuVotee, KrishKrosh, chongcht, vinesmsuic | - This paper introduces TheoremExplainAgent, a novel agentic approach for generating multimodal theorem explanation videos that integrate symbolic derivations with structured motion graphics and voiceover narration. - The agent consists of a planner that creates a high-level video plan and a coding agent that generates Python animation scripts using Manim. - A new benchmark called TheoremExplainBench consisting of 240 theorems across multiple STEM disciplines is also proposed along with five automated evaluation metrics which are used to evaluate the generated videos. - Experimental results show that the 03-mini model achieves the highest success rate (93.8%) and overall score (0.77) on TheoremExplainBench, and agent-based approaches can generate videos up to 10 minutes long, outperforming agentless methods. - However, limitations persist in visual element layout and retrieval-augmented generation performance, suggesting areas for improvement in spatial reasoning and refinement of AI-generated animations. | ['Text-to-Video', 'Multimodal'] | [Link](https://tiger-ai-lab.github.io/TheoremExplainAgent/) | N/A |
| [Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?](https://arxiv.org/abs/2502.19361) | Weixun Wang, Jiaheng Liu, Shilong Li, Yancheng He, zhangysk | - Introduces DeltaBench, a dataset designed to evaluate the ability of large language models (LLMs) to detect errors in long chain-of-thought (CoT) reasoning. - DeltaBench includes long CoTs generated by various models across diverse reasoning tasks, annotated with section-level labels for error analysis. - The study reveals that existing LLMs and process reward models struggle to effectively identify errors in long CoTs, with the best-performing model achieving an F1-score of only 40.8%. - Chain-of-thought prompting methods do not significantly improve error detection performance compared to other models. - The research highlights the limitations of existing models in critiquing long CoT reasoning and emphasizes the need for further research in this area. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/OpenStellarTeam/DeltaBench) | N/A |
| [Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems](https://arxiv.org/abs/2502.19328) | Bin Xu, Zijun Yao, Xiaozhi Wang, Yunjia Qi, Hao Peng | - This paper proposes agentic reward modeling, a novel reward system that integrates human preferences with verifiable correctness signals to enhance the reliability of reward models for large language models (LLMs). - The authors introduce REWARDAGENT, a reward agent that combines human preference rewards from existing reward models with two key verifiable signals: factuality and instruction-following. - REWARDAGENT employs a router to select appropriate verification agents, verification agents to assess different aspects of response correctness, and a judger to integrate verification scores and human preferences. - Experimental results on reward model benchmarks and real-world downstream tasks demonstrate that REWARDAGENT significantly outperforms existing reward models and LLMs used as reward models. - The authors further show that LLMs trained with DPO using REWARDAGENT-constructed preference pairs achieve superior performance on various NLP benchmarks compared to those trained with conventional reward models. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/THU-KEG/Agentic-Reward-Modeling) | N/A |
| [Language Models' Factuality Depends on the Language of Inquiry](https://arxiv.org/abs/2502.17955) | Hamid Palangi, Kumar Ayush, Kumar Tanmay, ayush1801, AggarwalTushar | - This research paper introduces a new benchmark dataset and evaluation framework to assess the factual consistency and cross-lingual knowledge transferability of multilingual language models (LLMs). - The benchmark comprises 10,000 country-related facts across 13 languages, encompassing high-, medium-, and low-resource languages, and evaluates LMs on factual recall, in-context recall, and counter-factual context adherence. - Three novel metrics—Factual Recall Score (FRS), Knowledge Transferability Score (KTS), and Cross-Lingual Factual Knowledge Transferability (X-FaKT) Score—are proposed to quantify factual recall and knowledge transferability across different languages. - Experimental results reveal that LLMs struggle to transfer factual knowledge across languages, often exhibiting inconsistent performance sensitive to the language of inquiry, with larger model sizes and higher-resource languages generally correlating with better performance. - The findings highlight the need for LMs to recognize language-specific factual reliability and leverage the most trustworthy information across languages for improved cross-lingual generalization. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation](https://arxiv.org/abs/2502.19414) | Matthias Bethge, Jonas Geiping, Ponnurangam Kumaraguru, Shashwat Goel, Shiven Sinha | - This paper introduces REFUTE, a benchmark designed to evaluate the ability of Language Models (LMs) to generate counterexamples, focusing on algorithmic problem-solving. - REFUTE consists of problems and incorrect solutions from programming competitions where human experts have successfully crafted counterexamples.  - The benchmark dynamically updates with recent problems to minimize data leakage, and employs an automated pipeline to filter out samples where finding counterexamples is trivial.  - Experiments reveal that even state-of-the-art reasoning LMs struggle to create valid counterexamples, achieving less than 9% success rate, significantly lagging behind their solution generation abilities. - This work highlights the gap between solving algorithmic problems and verifying the correctness of solutions, and emphasizes the need for benchmarks focused on testing the falsification abilities of LMs. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Towards an AI co-scientist](https://arxiv.org/abs/2502.18864) | Anil Palepu, Tao Tu, Alexander Daryin, Wei-Hung Weng, Juraj Gottweis |  - This paper introduces an AI co-scientist, a multi-agent system built on Google's Gemini 2.0, designed to assist scientists in generating novel research hypotheses and proposals.  - The system employs a generate, debate, and evolve approach, inspired by the scientific method and accelerated by scaling test-time compute.  - The AI co-scientist's performance was validated across three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution.  - Automated evaluations and a small-scale evaluation with domain experts demonstrate the effectiveness of test-time compute in enhancing hypothesis quality and the overall system's ability to generate high-quality hypotheses, and produce novel results.  - End-to-end validation of the co-scientist-generated hypotheses across three biomedical topics suggests the potential of the AI co-scientist to augment biomedical and scientific discovery. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](null) | [Link](null) |
| [Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance](https://arxiv.org/abs/2502.18772) | Polydoros Giannouris, Efstathia Soufleri, Triantafillos Papadopoulos, Xueqing Peng, jiminHuang | - This paper introduces Plutus-ben, the first Greek financial evaluation benchmark, and Plutus-8B, a pioneering Greek financial large language model (LLM). - Plutus-ben covers five key financial NLP tasks: numeric and textual named entity recognition (NER), question answering, abstractive summarization, and topic classification, supported by three new datasets (GRFinNUM, GRFinNER, GRFinQA) annotated by expert native Greek speakers and augmented with two existing resources. - Plutus-8B, based on Llama-Krikri-8B and fine-tuned with a novel instruction dataset (Plutus-instruction), demonstrates state-of-the-art performance on Plutus-ben, outperforming existing LLMs, including GPT-4, by 15.38%. - The evaluation reveals that Greek financial NLP remains challenging due to linguistic complexities and domain-specific terminology, highlighting the need for language and domain-specific adaptation. - All benchmark data, code, and model are publicly released to foster reproducible research and advance Greek financial NLP, promoting broader multilingual inclusivity in finance. | ['Natural Language Processing', 'Question Answering', 'Token Classification', 'Summarization'] | N/A | [Link](https://huggingface.co/collections/TheFinAI/plutus-benchmarking-greek-financial-llms-67bc718fb8d897c65f1e87db), [Link](https://huggingface.co/spaces/TheFinAI/plutus-8B-instruct), [Link](https://huggingface.co/spaces/TheFinAI/open_greek_finance_llm_leaderboard) |
| [AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement](https://arxiv.org/abs/2502.16776) | Xijie Huang, Junxiao Yang, Leqi Lei, Zhexin Zhang, LLLeo612 | - AISafetyLab is introduced as a unified framework and toolkit designed for evaluating and improving the safety of AI models, particularly Large Language Models (LLMs). - It integrates various attack, defense, and evaluation methodologies, featuring 13 attack methods, 16 defense strategies (including training-based and inference-time techniques), and multiple evaluation metrics. - The framework offers a modular design, extensive model support (local and API-based), and great extensibility for future development and integration of new methods. - Empirical studies conducted on Vicuna using AISafetyLab provide insights into the comparative effectiveness of different attack and defense strategies, revealing both strengths and limitations of current methods.  - The open-source availability of AISafetyLab aims to foster community contributions and advancements in AI safety research. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/AISafetyLab) | [Link](https://huggingface.co/datasets/thu-coai/AISafetyLab_Datasets) |
| [Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs](https://arxiv.org/abs/2502.19413) | Andreas Hochlehnert, Tawsif Ahmed, Ameya Prabhu, Gollam Rabby, Christoph Schuhmann |  - This paper introduces Project Alexandria, a new initiative to democratize access to scientific knowledge by extracting factual information from copyrighted research papers using LLMs. - The core contribution is the development of Knowledge Units (KUs), a structured data format representing scientific facts, entities, and relationships without stylistic content. - Legal defensibility is ensured through compliance with German copyright law and US Fair Use doctrine, focusing on extracting factual information only. - Experimental evidence demonstrates that KUs preserve approximately 95% of factual knowledge, as measured by MCQ performance across four research domains. - Open-source tools are provided to facilitate the creation and use of KUs, supporting the vision of an open and inclusive global scientific ecosystem. | ['Question Answering'] | [Link](https://github.com/open-psi/project-alexandria) | N/A |
| [BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187) | Chrysovalantis Anastasiou, John Palowitch, Hritik Bansal, Mehran Kazemi, baharefatemi | - This paper introduces BIG-Bench Extra Hard (BBEH), a new benchmark designed to evaluate advanced reasoning capabilities in LLMs. - BBEH builds upon the existing BIG-Bench Hard (BBH) benchmark by replacing each of its 23 tasks with a novel, more challenging counterpart, probing similar reasoning skills but with increased difficulty. - The benchmark tasks cover a diverse range of reasoning skills, from many-hop reasoning, error identification, and long-context processing, to going against strong priors and causal understanding. - Initial evaluations on several state-of-the-art LLMs, including general-purpose and specialized reasoning models, reveals that BBEH presents a significant challenge, with the best models achieving only 9.8% and 44.8% harmonic mean accuracy, respectively.  - This highlights the substantial room for improvement in robust general reasoning capabilities of LLMs. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/google-deepmind/bbeh) | N/A |
| [CritiQ: Mining Data Quality Criteria from Human Preferences](https://arxiv.org/abs/2502.19279) | Zhiheng Xi, Tianyi Liang, Qipeng Guo, Kai Lv, KYLN24 | - CritiQ, a novel data selection method, automatically mines criteria from human preferences for data quality with only ~30 human-annotated pairs and performs efficient data selection. - CritiQ Flow, the main component, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. - A knowledge base extracts quality criteria from previous work to boost CritiQ Flow. - CritiQ Scorer is trained to give quality scores and perform data selection. - Continual pretraining experiments with Llama 3.1 models show improved performance on downstream tasks compared to uniform sampling. | ['Natural Language Processing'] | [Link](https://github.com/KYLN24/CritiQ) | [Link](https://huggingface.co/datasets) |
| [MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra](https://arxiv.org/abs/2502.16284) | Qiang Liu, Deli Zhao, Yu Rong, Shaozhen Liu, AzureLeon1 | - MolSpectra, a new framework for pre-training 3D molecular representations, leverages multi-modal energy spectra, incorporating quantum mechanics knowledge into molecular representations. - It introduces SpecFormer, a multi-spectrum Transformer encoder trained with a masked patch reconstruction objective to capture intra- and inter-spectrum correlations. - A contrastive objective aligns 3D structure and spectra representations, enhancing the 3D encoder's understanding of spectral features without requiring spectral data during downstream tasks. - Evaluation on public benchmarks shows MolSpectra surpasses existing methods in molecular property prediction and dynamics modeling, achieving state-of-the-art performance in 8 out of 12 properties on QM9 and outperforming existing denoising methods on MD17. - This approach shows the effectiveness of incorporating molecular spectra into pre-training, going beyond the limitations of classical mechanics descriptions in previous work. | ['Graph Machine Learning', 'Multimodal'] | [Link](https://github.com/AzureLeon1/MolSpectra) | N/A |
| [PosterSum: A Multimodal Benchmark for Scientific Poster Summarization](https://arxiv.org/abs/2502.17540) | Frank Keller, Pasquale Minervini, rohitsaxena | - Introduced POSTERSUM, a new benchmark dataset with 16,305 scientific poster images and corresponding research paper abstracts, designed for evaluating multimodal scientific poster summarization. - Benchmarked state-of-the-art Multimodal Large Language Models (MLLMs) on POSTERSUM, demonstrating their limitations on complex scientific posters. - Proposed SEGMENT & SUMMARIZE, a hierarchical method that first segments the poster image into coherent regions, generates localized summaries for each region using an MLLM, and then combines them with a text-based LLM to create a final summary. - Achieved state-of-the-art performance on POSTERSUM, surpassing closed and open-source MLLMs by a significant margin, with a 3.14% improvement in ROUGE-L score, demonstrating the effectiveness of the hierarchical approach. - Released the dataset and code to promote research in multimodal scientific poster understanding and encourage the development of robust models capable of summarizing complex, information-dense scientific content. | ['Multimodal', 'Summarization', 'Computer Vision', 'Image-to-Text'] | N/A | [Link](rohitsaxena/PosterSum) |


## Papers for 2025-02-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference](https://arxiv.org/abs/2502.18411) | Jiaqiwang, Weiyun1025, UniverseCA, ChrisDing1105, PhoenixZ | - This paper introduces OmniAlign-V, a 200K sample dataset designed to improve the alignment of Multimodal Large Language Models (MLLMs) with human preferences.  - The dataset features diverse images, complex questions, and varied response formats, including natural images, infographics, and questions covering knowledge, inference, and creative tasks. - A novel image selection pipeline is used to ensure semantically rich images, and a post-refinement process enhances the quality and diversity of question-answer pairs. - Experiments demonstrate that fine-tuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly improves human preference alignment while maintaining or enhancing performance on standard VQA benchmarks. -  A new human-evaluated benchmark, MM-AlignBench, is also introduced to assess MLLM alignment with human values. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/PhoenixZ810/OmniAlign-V) | N/A |
| [SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference](https://arxiv.org/abs/2502.18137) | Haofeng Huang, surfingtomchen, hxi0408, Xiang-cd, jt-zhang | - SpargeAttn is a new, training-free, sparse attention method designed to accelerate inference across various models, including language, image, and video generation, without sacrificing performance. - It uses a two-stage online filtering approach to identify and skip less important computations in the attention mechanism, first by predicting sparse blocks and then using a softmax-aware filter. - Experiments on models like Llama 3.1, CogVideoX, Mochi, Flux, and Stable Diffusion demonstrate that SpargeAttn achieves 2.5x to 5x speedup compared to existing dense and sparse attention models. - It consistently outperforms baseline sparse attention techniques in terms of both speed and the preservation of end-to-end performance metrics, like perplexity and video quality scores. - SpargeAttn also integrates with 8-bit quantization techniques (SageAttention) for further performance gains. | ['Natural Language Processing', 'Text Generation', 'Text-to-Video', 'Text-to-Image', 'Computer Vision'] | [Link](https://github.com/thu-ml/SpargeAttn) | N/A |
| [ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation](https://arxiv.org/abs/2502.18364) | JianminBao, DongChen06, 131131yhx, 2JZ, yifanpu001 | - This paper introduces the Anonymous Region Transformer (ART), a novel model for generating variable multi-layer transparent images from a global text prompt and an anonymous region layout. - ART employs a layer-wise region crop mechanism to reduce computational costs, enabling efficient generation of images with numerous layers, outperforming full attention approaches in speed and coherence. - Inspired by Schema theory, the anonymous region layout allows the model to autonomously determine visual token and text token alignment, unlike conventional semantic layouts, reducing manual labor. - ART incorporates a high-quality multi-layer transparent image autoencoder for joint encoding and decoding of transparency in variable multi-layer images. - User studies demonstrate ART's superior performance compared to LayerDiffuse and COLE in photorealistic and graphic design domains, respectively, showcasing improved quality and layer generation capabilities. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective](https://arxiv.org/abs/2502.17262) | Chenggang Li, Xiao Li, shenke18, Lucky2022, JerryXu98 | - This paper introduces a Clustering-On-Difficulty (COD) framework for predicting the downstream performance of Large Language Models (LLMs). - COD addresses the challenges of emergent abilities and uneven task difficulty distributions by clustering tasks based on difficulty features and extrapolating performance on clusters exhibiting consistent scaling patterns. - A new performance scaling law, derived from the existing loss scaling law, is proposed for cluster-wise performance prediction. - COD maps subset accuracy predictions to full evaluation set performance using a calibrated mapping function. - Experimental results on eight benchmarks demonstrate that COD achieves state-of-the-art prediction accuracy, with an average error of 1.36% on a 70B parameter LLM. | ['Natural Language Processing'] | N/A | N/A |
| [Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models](https://arxiv.org/abs/2502.15499) | Ya Wang, LLIXQ, xunzhou, Taoer, BryceZhuo | - This paper proposes Scale-Distribution Decoupling (SDD), a novel technique for stabilizing the training of Large Language Models (LLMs), particularly Post-Norm Transformers, by decoupling the scale and distribution of weight matrices in fully-connected layers. - SDD normalizes activations and uses a learnable scaling vector to maintain well-conditioned gradients, preventing gradient explosion and dissipation. - Experiments on 1B parameter dense and 3.4B parameter MoE models show SDD improves training stability, convergence speed, and downstream performance across various benchmarks compared to Pre-Norm, Post-Norm, and DeepNorm. - SDD stabilizes training dynamics, achieves lower losses, and enhances generalization on tasks like MMLU, HellaSwag, and ARC-Challenge. - The method is lightweight, compatible with existing frameworks, and demonstrates improved robustness against hyperparameter perturbations. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/kaihemo/SDD) | N/A |
| [WebGames: Challenging General-Purpose Web-Browsing AI Agents](https://arxiv.org/abs/2502.18356) | Fraser, semitable, BiggieW, XanderJC, georgethomas | - Introduces WebGames, a benchmark suite with 50+ interactive challenges designed to evaluate general-purpose web-browsing AI agents. - Challenges range from fundamental browser interactions to cognitive tasks, workflow automation, and interactive entertainment, aiming to systematically test the limitations of current AI systems. - Eliminates external dependencies through a hermetic testing environment for reproducible evaluation with verifiable ground-truth solutions and uses a Set-of-Marks approach to allow models to interact with the webpage. - Evaluates leading vision-language models (GPT-4, Claude Computer-Use, Gemini-1.5-Pro, Qwen2-VL) against human performance, revealing a substantial capability gap (best AI system at 41.2% success vs. human 95.7%). - Publicly available at webgames.convergence.ai with a lightweight, client-side implementation and modular architecture. | ['Multimodal'] | [Link](https://github.com/convergence-ai/webgames) | [Link](https://huggingface.co/datasets/convergence-ai/webgames) |
| [Introducing Visual Perception Token into Multimodal Large Language Model](https://arxiv.org/abs/2502.17425) | wxcTest, horseee, rp-yu | - This paper introduces the concept of Visual Perception Tokens, empowering Multimodal Large Language Models (MLLMs) to control their visual perception processes, addressing the limitations of current MLLMs' reliance on fixed perception pipelines. - Two types of tokens are proposed: Region Selection Tokens, which prompt the MLLM to re-encode cropped image regions relevant to a query, and Vision Re-Encoding Tokens, which use additional vision encoders (like DINO or SAM) and hidden states to refine visual features. - These tokens are integrated into the MLLM's vocabulary and generated through next-token prediction, enabling seamless integration with the standard training process. - Experimental results demonstrate that a 2B parameter model with Visual Perception Tokens outperforms a 7B model without these tokens by 20% on average across various VQA tasks, showcasing their effectiveness in handling spatial reasoning and fine-grained understanding. - Introducing these tokens to a 2B model led to an average performance improvement of 30.9%. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/yu-rp/VisualPerceptionToken) | N/A |
| [The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?](https://arxiv.org/abs/2502.17535) | Peijie Dong, Qian Wang, Xiang Liu, wenxinsiju, coolzhtang | - This blog post proposes the "Lottery LLM Hypothesis," which suggests that a smaller "lottery" LLM can achieve performance comparable to a larger LLM when augmented with multi-step reasoning and external tools like knowledge retrieval and function calls. - The hypothesis challenges the current focus of LLM compression methods on preserving performance on simple tasks and perplexity, arguing that they overlook crucial abilities like long-context understanding, retrieval-augmented generation, and tool utilization. - The authors review recent advancements in LLMs and propose a recursive, divide-and-conquer reasoning algorithm that utilizes external knowledge, tools, and memory, enabling smaller LLMs to potentially achieve Turing completeness. - The blog post emphasizes the importance of preserving abilities such as retrieval from prompts, identification of required external resources, planning and scheduling, precise approximation of fundamental operations, and long-context reasoning in compressed LLMs. - The Lottery LLM Hypothesis offers a new perspective on LLM compression, suggesting that focusing on these essential abilities could lead to more efficient and capable compressed models that maintain performance on complex tasks with the assistance of external resources and multi-step reasoning. | ['Natural Language Processing'] | N/A | N/A |
| [AAD-LLM: Neural Attention-Driven Auditory Scene Understanding](https://arxiv.org/abs/2502.16794) | Ashesh Mehta, Stephan Bickel, vchoudhari, susameddin, xi-j | - This paper introduces AAD-LLM (Auditory Attention-Driven LLM), a multimodal model integrating brain signals with an auditory LLM (Qwen2-Audio) to improve auditory scene understanding by aligning responses with listener perception. - AAD-LLM decodes listener attention from intracranial EEG (iEEG) to identify the attended speaker and uses this information to prioritize relevant speech during response generation using x-vector centroids and chain-of-thought prompting, as well as incorporating a speech separator based on Mamba-TasNet. - Objective metrics and subjective ratings on speaker description, speech transcription, summarization, and question answering demonstrate that AAD-LLM generates responses that are better aligned with human perception compared to existing auditory LLMs. - AAD-LLM with decoded attention performs close to the oracle attention setting (where the target speaker is known), showing the efficacy of the neural decoding process and the improvement stemming primarily from its attentional state. - The model's attentional capabilities generalize to untrained tasks, like speech translation, indicating broader applicability beyond the tasks used in training. | ['Multimodal', 'Audio', 'Automatic Speech Recognition', 'Question Answering'] | [Link](https://aad-llm.github.io) | [Link](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct), [Link](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb), [Link](https://huggingface.co/openai/whisper-large-v3), [Link](https://huggingface.co/microsoft/wavlm-base-plus-sv), [Link](https://huggingface.co/hexgrad/Kokoro-82M) |
| [Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI](https://arxiv.org/abs/2502.17092) | KartikAngadi, kruthika, SyedAbdul | - Introduces Shakti-VLM, a family of 1B and 4B parameter vision-language models designed for data efficiency in multimodal learning. - Leverages architectural innovations like QK-Normalization for attention stability, hybrid normalization, and enhanced positional encoding to achieve competitive results with fewer tokens than other large VLMs. - Employs a three-stage training process: decoder pretraining on extended text data, vision-language alignment with a frozen decoder, and full model fine-tuning with instruction tuning and RLHF. - Excels in document understanding, visual reasoning, OCR, and general multimodal reasoning, outperforming larger models like SmolVLM-2.25B on some tasks and demonstrating comparable performance to models with significantly more parameters. - Achieves strong benchmark performance despite using significantly fewer training tokens (487B for 1B model, 782B for 4B model) compared to other VLMs, making it an efficient solution for enterprise-scale multimodal applications. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | N/A |


## Papers for 2025-02-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks](https://arxiv.org/abs/2502.17157) | Zhiyue Zhao, Mingyu Liu, Z-MU-Z, zhyya, Canyu | - DICEPTION is a multi-task visual generalist model based on a diffusion model (SD3 architecture) that unifies different computer vision tasks within the RGB space, leveraging the priors of pre-trained text-to-image diffusion models. - The model performs comparably to state-of-the-art specialized models on tasks such as depth estimation, surface normal estimation, and point-prompted segmentation using significantly less training data (e.g., 0.06% of SAM's data for comparable segmentation performance). - DICEPTION effectively adapts to new visual tasks with minimal fine-tuning (as few as 50 images and ~1% of its parameters). - The study also validates the effectiveness of random color assignment for instance segmentation and semantic segmentation tasks, contrary to previous findings. - While demonstrating strong qualitative results, quantitative evaluation for semantic segmentation and human keypoint detection remains a challenge due to post-processing limitations, and inference time is relatively long due to the diffusion process. | ['Computer Vision', 'Depth Estimation', 'Image Segmentation', 'Keypoint Detection', 'Multimodal'] | N/A | N/A |
| [Slamming: Training a Speech Language Model on One GPU in a Day](https://arxiv.org/abs/2502.15814) | Yossi Adi, avishai-elmakies, gallilmaimon | - Slam, a new training recipe, enables training high-quality Speech Language Models (SLMs) on a single academic GPU within 24 hours. - The recipe involves analyzing model initialization, architecture, synthetic data, preference optimization, and component tweaking. - This method scales well with more compute, achieving results comparable to leading SLMs using significantly fewer resources. - Empirical results outperform predictions from previous SLM scaling laws, showing a more optimistic compute requirement for training high-quality SLMs. - The study uses a decoder-only generative SLM architecture and various evaluation metrics, including likelihood evaluation and generative perplexity. | ['Audio', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models](https://arxiv.org/abs/2502.16614) | Yejie Wang, Wei Zhang, Jiaheng Liu, Marcus Dong, Alexander Zhang | This paper introduces CodeCriticBench, a comprehensive benchmark designed to holistically evaluate the code critique capabilities of large language models (LLMs).  It covers two mainstream code tasks: code generation and code QA, with varying difficulty levels. The benchmark uses basic and advanced evaluation protocols, including fine-grained evaluation checklists, for a thorough assessment. Extensive experiments on existing LLMs demonstrate the effectiveness of CodeCriticBench in evaluating different aspects of code critique capabilities. The results show that performance generally improves with model size, validating the benchmark's design. | ['Question Answering'] | [Link](https://github.com/multimodal-art-projection/CodeCriticBench) | N/A |
| [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/abs/2502.16894) | Wei Wei, Xiaoye Qu, Sichen Liu, Zhenyi Lu, Facico |  - This paper introduces GOAT, a novel framework that improves the performance of LoRA (Low-Rank Adaptation) by using adaptive singular values and mixture-of-experts (MoE) optimization alignment. - GOAT uses an SVD-structured MoE architecture, which adaptively integrates relevant priors and aligns optimization with full fine-tuned MoE. - The method includes a theoretical scaling factor that further boosts LoRA MoE's efficiency and performance. - Experiments across 25 datasets show GOAT achieving state-of-the-art performance, closing the gap with Full FT (Full Fine-Tuning). - GOAT is shown to significantly outperform other LoRA and LoRA-MoE baselines on multiple benchmarks, and it is also more parameter efficient than Full FT. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](https://arxiv.org/abs/2502.16033) | Yang Zhao, Shan Jiang, Hongquan Li, Yue Fan, Qianqi Yan | - Introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark, a new framework for evaluating the ability of Multimodal Large Language Models (MLLMs) to detect and reason about inconsistencies in visually and textually rich content like webpages, slides, and posters. - MMIR features 534 samples across five reasoning categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence, and employs two evaluation settings: open-ended and multiple-choice. - Evaluations of six state-of-the-art MLLMs, including o1, GPT-40, Qwen2.5-VL, LLaVA-NeXT, InternVL2.5, and Phi-3.5-Vision, reveal that current MLLMs struggle with multimodal inconsistency reasoning, with proprietary models significantly outperforming open-source alternatives. - Error analysis indicates that models perform best with single-modality (text-only) inconsistencies and struggle most with image-image inconsistencies, suggesting a limited capacity for visual and cross-modal reasoning. - Probing experiments show that Multimodal Interleaved Chain-of-Thought (MM-CoT) prompting, which combines visual and textual cues through an iterative reasoning process, leads to greater performance gains over traditional single-modality prompting methods like CoT and SoM. | ['Multimodal'] | N/A | N/A |
| [Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration](https://arxiv.org/abs/2502.17110) | Ji Zhang, Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy | - Mobile-Agent-V is a framework that leverages video demonstrations to teach mobile agents how to perform complex, device-specific operations, addressing the challenge of limited operational knowledge in current AI-driven mobile automation systems. - The framework employs a sliding window mechanism to process keyframes from instructional videos, reducing input length while retaining critical information for the decision-making agent. - A video agent dynamically adjusts the sliding window, a decision agent determines the appropriate action based on the video and current device state, and a deep-reflection agent validates and refines the chosen action to ensure consistency with the demonstrated operation. - Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks on tasks requiring operational knowledge, effectively learning and applying video-based knowledge for mobile device interaction. - The approach is scalable and adaptable, offering a practical alternative to manual programming or extensive data collection for training mobile automation agents. | ['Multimodal'] | N/A | N/A |
| [Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties](https://arxiv.org/abs/2502.16922) | Deyu Zhou, Yong Jiang, Pengfei LI, Jialong Wu, wzl0228 | - This paper introduces CTM, a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. - CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. - Experiments on various LLMs reveal that CTM poses significant challenges, highlighting potential avenues for improvement in LLMs' temporal reasoning abilities. - The benchmark includes eight challenging question-answering tasks and a Timeline Ito Game to evaluate the LLM’s ability to align entities across temporal and other dimensions. - CTM's dataset is built upon a curated and authoritative Chinese cultural entity repository, which encompasses over 4,700 entities. | ['Question Answering'] | [Link](https://github.com/Linking-ai/ctm_bench) | N/A |
| [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/abs/2502.14132) | Isabelle Augenstein, Desmond Elliott, gretawarren, Nadav | - This paper investigates the extent to which community notes on Twitter/X rely on professional fact-checkers and explores the characteristics of posts and notes that utilize fact-checking sources. - The study reveals that community notes cite fact-checking sources significantly more often than previously reported, especially for posts related to broader misinformation narratives. - The researchers use language models to annotate a large corpus of community notes with attributes like topic, cited sources, and refutation of claims. - Their findings indicate that professional fact-checking plays a crucial role in the creation of high-quality community notes, highlighting the interdependency between professional and community-based fact-checking. - The study's conclusion emphasizes the importance of professional fact-checking in combating misinformation, particularly for complex or high-stakes issues. | ['Text Classification'] | N/A | N/A |
| [Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam](https://arxiv.org/abs/2502.17055) | Xiang Li, Gaojie Jin, Zhenyu Zhang, Haotian Hu, Tianjin Huang | - This paper introduces Stable-SPAM, a new optimizer for training Large Language Models (LLMs) with 4-bit precision, addressing the instability issues often encountered in low-precision training. - Stable-SPAM incorporates adaptive gradient normalization (AdaGN), adaptive spike-aware clipping (AdaClip), and momentum reset (MoRet) to stabilize gradient norms and prevent divergence. - Experiments on various LLaMA models show that Stable-SPAM significantly outperforms Adam and other recent optimizers in 4-bit training scenarios, sometimes even exceeding the performance of 16-bit Adam. - Notably, the 4-bit LLaMA-1B model trained with Stable-SPAM achieves comparable results to the BF16 version trained with Adam, while requiring only half the training steps when both are trained in 4-bit. - The proposed techniques are also shown to be effective when integrated with other optimizers, demonstrating their broad applicability in improving low-precision training stability. | ['Natural Language Processing'] | [Link](https://github.com/TianjinYellow/StableSPAM.git) | N/A |
| [X-Dancer: Expressive Music to Human Dance Video Generation](https://arxiv.org/abs/2502.17414) | Chenxu Zhang, You Xie, Guoxian Song, Hongyi Xu, Zeyuan Chen | - This paper introduces X-Dancer, a novel zero-shot music-driven image animation pipeline that generates diverse and long-range human dance videos from a single static image. - The model architecture consists of a unified transformer-diffusion framework, using an autoregressive transformer to synthesize music-synchronized token sequences for 2D body poses, and a diffusion model to generate coherent and realistic video frames. - X-Dancer models 2D human motion, leveraging widely accessible dance videos, enhancing scalability and addressing limitations of traditional 3D methods. - Experimental results demonstrate that X-Dancer surpasses existing methods in terms of diversity, expressiveness, and realism, achieving state-of-the-art performance on challenging benchmarks. - The approach incorporates a multi-part tokenization scheme, allowing it to capture nuanced alignment with musical beats through readily available monocular videos. | ['Image-to-Video', 'Text-to-Video', 'Multimodal'] | N/A | N/A |
| [M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment](https://arxiv.org/abs/2502.15167) | Weiming Zhang, Wen Shen, Zhihua Wei, Kejiang Chen, Chuan Cui | - This paper introduces M3-AGIQA, a novel multimodal framework for assessing the quality of AI-generated images (AGI). - M3-AGIQA leverages multimodal large language models (MLLMs) as joint text and image encoders and incorporates a structured multi-round evaluation mechanism. - The model uses Low-Rank Adaptation (LoRA) to fine-tune a local MLLM, achieving state-of-the-art performance on multiple benchmark datasets. - Extensive experiments demonstrate its effectiveness in capturing nuanced aspects of AGI quality, including perceptual quality, prompt correspondence, and authenticity. - Cross-dataset validation confirms its strong generalizability. | ['Multimodal'] | [Link](https://github.com/strawhatboy/M3-AGIQA) | N/A |


## Papers for 2025-02-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SurveyX: Academic Survey Automation via Large Language Models](https://arxiv.org/abs/2502.14776) | UglyToilet, Ki-Seki, siminniu, fan2goa1, HaruTeru | - This paper introduces SurveyX, a novel automated survey generation system leveraging Large Language Models (LLMs) to address challenges in academic survey creation, such as context window limitations and outdated internal knowledge. - SurveyX employs a two-phase process: Preparation (retrieval and preprocessing of references using a novel AttributeTree method) and Generation (outline creation, content generation, and refinement with multimodal elements like figures and tables). - The system utilizes online reference retrieval, keyword expansion, and a two-step filtering process to ensure up-to-date and relevant sources. - An innovative outline optimization method, "separate-then-reorganize," enhances structure and reduces redundancy, while a RAG-based rewriting module ensures content consistency and citation accuracy. - Experimental results demonstrate SurveyX's superior performance over existing automated systems and near-human expert level quality in content, citation, and reference relevance across multiple metrics. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model](https://arxiv.org/abs/2502.13449) | Sung Ju Hwang, Wonbin Lee, DongkiKim | - Mol-LLaMA is a large molecular language model that learns fundamental knowledge centered on molecules via multi-modal instruction tuning, aiming to be a general-purpose molecular assistant. - The model architecture consists of 2D and 3D molecular encoders, a blending module to combine information from both encoders, a Q-Former projector, and a large language model. - A new instruction dataset, Mol-LLaMA-Instruct, is created focusing on fundamental molecular features and reasoning ability through detailed structural descriptions, structure-to-feature relationship explanations, and comprehensive conversations. - Experimental results show Mol-LLaMA outperforms baselines including GPT-40 in understanding general molecular features and generating relevant responses with detailed explanations. -  Mol-LLaMA also demonstrates strong performance in molecular property prediction tasks, accurately predicting properties while providing helpful explanations, demonstrating its potential as a general-purpose assistant for molecular analysis. | ['Multimodal', 'Graph Machine Learning', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers](https://arxiv.org/abs/2502.15007) | Polina Druzhinina, Elizaveta Goncharova, Temurbek Rahmatullaev, Matvey Mikhalchuk, Anton Razzhigaev | - This paper introduces LLM-Microscope, a new framework designed to analyze and visualize the internal behaviors of Large Language Models (LLMs), focusing on how they encode and aggregate contextual information. - It presents methods for measuring token-level non-linearity and contextualization, revealing that often overlooked tokens like punctuation and determiners play a key role in context memory. - Removing these "filler" tokens consistently degrades performance on MMLU and BABILong-4k benchmarks, even with targeted removal guided by GPT-4. - A strong correlation between linearity and contextualization scores is observed in token representations. - The LLM-Microscope toolkit, including a demo on Hugging Face Spaces, facilitates analysis of token-level non-linearity, contextual memory, and intermediate layer contributions via a multimodal Logit Lens, and intrinsic dimensionality of representations, enabling better understanding of model behavior. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/AIRI-Institute/LLM-Microscope/tree/main) | [Link](https://huggingface.co/spaces/AIRI-Institute/LLM-Microscope) |
| [SIFT: Grounding LLM Reasoning in Contexts via Stickers](https://arxiv.org/abs/2502.14922) | Zhijie Deng, Boxiu Li, Xuyao Huang, Zihao Zeng | - This paper introduces Stick to the Facts (SIFT), a novel post-training approach to enhance the reasoning capabilities of Large Language Models (LLMs) by mitigating *factual drift*. - SIFT leverages increased inference-time computation to ground LLM reasoning within the context by generating a "Sticker," which summarizes key facts and questions from the original query. - The sticker is refined through bidirectional optimization, including forward optimization to better align it with the query and inverse generation to conform to the model's reasoning preferences. - Experimental results across diverse LLMs (ranging from 3B to 100B+ parameters) and benchmarks (e.g., GSM8K, MATH-500, AIME) demonstrate consistent performance improvements. - Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to 85.67%, setting a new state-of-the-art result within the open-source community. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zhijie-group/SIFT) | N/A |
| [VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/abs/2502.12084) | Yi R., Paul Pu Liang, Renjie Pi, RainJamesY, Sterzhang | - This paper introduces VLM$^2$-Bench, a new benchmark designed to assess Vision-Language Models' (VLMs) ability to link matching visual cues across multiple images or videos. - The benchmark includes nine subtasks categorized into three cue types: general, object-centric, and person-centric, encompassing various question formats and over 3,000 test cases. - An evaluation of eight open-source VLMs and GPT-40 reveals a substantial performance gap between models and humans, with even GPT-40 lagging 34.80% behind human accuracy. - The analysis reveals that models struggle with tasks requiring linking visual cues across different contexts, especially in object-centric tasks where they often overlook nuanced details. - Based on these findings, the authors recommend improving fundamental visual capabilities, refining language-based reasoning integration, and evolving vision-text training paradigms to enhance VLMs' performance in vision-centric tasks. | ['Multimodal', 'Visual Question Answering'] | [Link](https://vlm2-bench.github.io/) | N/A |
| [LightThinker: Thinking Step-by-Step Compression](https://arxiv.org/abs/2502.15589) | Mengshu Sun, Yuqi Zhu, Jintian Zhang, Ningyu, GoooDte | - LightThinker is a new method for compressing intermediate thoughts in large language models (LLMs) during reasoning, reducing memory overhead and inference time. - Inspired by human cognition, the model compresses verbose thought steps into compact representations (gist tokens) and discards original reasoning chains, trained on when and how to compress through data construction and specialized attention masks. - A new Dependency (Dep) metric quantifies compression by measuring reliance on historical tokens. - Experiments on four datasets and two models show LightThinker reduces peak memory and inference time while maintaining competitive accuracy.  - For example, using the Qwen model, LightThinker reduces peak token usage by 70% and inference time by 26% compared to the baseline, with only a 1% accuracy drop. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zjunlp/LightThinker) | N/A |
| [StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following](https://arxiv.org/abs/2502.14494) | Yuan Wu, Yi Chang, Yue Wang, Jinzhe Li, Jinnan Li | - This paper introduces StructFlowBench, a new benchmark for evaluating multi-turn instruction following in large language models (LLMs). - The benchmark incorporates a novel structural flow framework consisting of six fundamental inter-turn relationships (Follow-up, Refinement, Recall, Summary, Expansion, Unrelatedness) to capture the dependencies between dialogue turns. - A dual-constraint evaluation system combining intra-turn instruction constraints and structural constraints is used for comprehensive assessment. - Experimental results on 13 leading LLMs reveal significant deficiencies in their understanding of multi-turn dialogue structures, especially in handling refinements. - The proposed framework and benchmark provide valuable insights for improving the design and evaluation of multi-turn dialogue systems. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/MLGroupJLU/StructFlowBench) | N/A |
| [InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback](https://arxiv.org/abs/2502.15027) | Mike Zheng Shou, Haiyang Mei, Yifei Tao, Wenqi Pei, Henry Hengyuan Zhao | - This paper introduces InterFeedback, a new framework for evaluating the interactive intelligence of Large Multimodal Models (LMMs) by assessing their ability to learn and adapt from human feedback. - The InterFeedback-Bench benchmark employs existing datasets like MMMU-Pro and MathVerse, and InterFeedback-Human dataset contains manually collected data, allowing for autonomous and human evaluation of LMM's interactive capabilities. -  A new straightforward problem-solving framework called InterFeedback was proposed which contains two roles: feedback receiver and feedback provider. It employs leading LLMs to simulate humans giving feedback, and integrates with existing datasets to evaluate the interactive intelligence of LMMs.  - Experimental results across various open and closed source LMMs shows that state-of-the-art models often struggle to effectively interpret and utilize feedback, with a correction rate below 50% even with assistance from advanced LMMs like GPT-4. - The study highlights the need for developing methods that can enhance the ability of LMMs to correctly understand and learn from feedback for improved performance. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?](https://arxiv.org/abs/2502.15657) | Pietro Greiner, Joumana Ghosn, Damiano Fornasiere, Michael Cohen, Yoshua Bengio | - This paper proposes Scientist AI, a non-agentic AI system designed for understanding rather than pursuing goals, as a safer alternative to current agency-driven AI development. - Scientist AI consists of a world model that generates causal theories to explain observed data and an inference machine that answers questions based on these theories, both operating with an explicit notion of uncertainty. - The model-based design leverages probabilistic deep learning and the scientific method, focusing on explanation and probabilistic inference rather than action and reward maximization. - The Bayesian approach to uncertainty ensures that all plausible explanations are considered, mitigating risks associated with overconfident predictions and promoting trustworthy answers. - The paper discusses potential applications of Scientist AI, including accelerating scientific progress, acting as a guardrail against unsafe agentic AIs, and assisting in the development of safer superintelligent AIs. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer](https://arxiv.org/abs/2502.15631) | Vincent Ginis, Andres Algaba, Marthe Ballon | - This paper investigates the relationship between reasoning length and performance in large language models (LLMs), specifically focusing on the OpenAI o1-mini, o3-mini(m), and o3-mini(h) models on the challenging Omni-MATH benchmark. - The study reveals that more proficient models (o3-mini(m) vs. o1-mini) achieve superior accuracy without generating longer reasoning chains. - Additionally, while deeper reasoning is necessary for solving complex problems, there is a diminishing return, where excessive token usage correlates with reduced accuracy and that this effect is smaller for higher performing models. - o3-mini(h) achieves only a marginal performance gain over o3-mini(m) at a much higher computational cost and more reasoning tokens. - The authors conclude that more efficient reasoning, rather than increased reasoning chain length, is associated with improved performance in more advanced LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/MartheBallon/analysis_o3-mini_thinks_harder_not_longer), [Link](https://github.com/KbsdJames/Omni-MATH) | [Link](https://huggingface.co/datasets/KbsdJames/Omni-MATH), [Link](https://huggingface.co/KbsdJames/Omni-Judge) |
| [MoBA: Mixture of Block Attention for Long-Context LLMs](https://arxiv.org/abs/2502.13189) | Tao Jiang, Yulun Du, Jingyuan Liu, Zhejun Jiang, Enzhe Lu | - This paper introduces Mixture of Block Attention (MoBA), a novel attention mechanism designed for long-context Large Language Models (LLMs). - Inspired by Mixture of Experts (MoE), MoBA partitions the context into blocks and uses a gating mechanism to route query tokens to the most relevant blocks, improving efficiency. - This block-sparse attention approach reduces the computational cost of traditional attention from quadratic to sub-quadratic. - Experiments demonstrate that MoBA achieves performance comparable to full attention while significantly reducing computational complexity. - It enables LLMs to scale to longer contexts, showing promising results on benchmarks, including maintaining low LM loss and high performance on tasks like RULER with extended context lengths up to 1 million tokens. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/MoonshotAI/MoBA) | N/A |
| [Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence](https://arxiv.org/abs/2502.14905) | Viktoria Rojkova, Ishan Joshi, Bhavik Agarwal | - This paper introduces ThinkJSON, a novel approach to enforce strict schema adherence in Large Language Model (LLM) generated JSON outputs by combining reinforcement learning with reasoning capabilities. - The approach uses a 1.5B parameter model trained via a novel pipeline involving synthetic structured/unstructured data generation, custom reward functions under Group Relative Policy Optimization (GRPO), and supervised fine-tuning. - ThinkJSON demonstrates superior performance compared to larger models like DeepSeek R1 (671B), its distilled versions (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B) achieving 62.41% mean match accuracy (proportion of fields correctly mapped) while minimizing extraneous output (0.27% mean noise). - The training process is computationally efficient, requiring 20 hours on an 8x H100 GPU cluster for GRPO and 3 hours on a single A100 for fine-tuning, highlighting its practical applicability for real-world scenarios. - The method addresses challenges of data integrity in regulated domains like bio-manufacturing by ensuring generated records meet schema requirements and compliance standards. | ['Text2Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [Evaluating Multimodal Generative AI with Korean Educational Standards](https://arxiv.org/abs/2502.15422) | Geewook Kim, sangheeeee | - This paper introduces KoNET, a benchmark designed to evaluate Multimodal Generative AI systems using Korean national educational tests across different educational levels (elementary, middle, high school, and college). - KoNET focuses on the Korean language, addressing the lack of benchmarks in languages other than English. - The benchmark assesses a variety of open-source, open-access, and closed API models, and analyzes model performance in relation to question difficulty, subject diversity, and human error rates. - An analysis comparing model performance with human error rates on the KoCSAT (Korean College Scholastic Ability Test) reveals interesting differences in performance between AI models and human test-takers. - The code and dataset builder for KoNET are open-sourced. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/naver-ai/KoNET) | N/A |
| [CrossOver: 3D Scene Cross-Modal Alignment](https://arxiv.org/abs/2502.15011) | Iro Armeni, Daniel Barath, Marc Pollefeys, Ondrej Miksik, sayandsarkar | - CrossOver is a novel framework for cross-modal 3D scene understanding that performs scene-level modality alignment by learning a unified, modality-agnostic embedding space. - It leverages dimensionality-specific encoders (1D, 2D, and 3D) tailored to each modality (RGB images, point clouds, CAD models, floorplans, and text descriptions), eliminating the reliance on explicit 3D scene graphs or semantic labels during inference. - Employs a three-stage training pipeline: object-level embedding, scene-level training, and dimensionality-specific encoder training to create a semantic-free cross-modal embedding space. - Supports robust scene retrieval and object localization, even with missing modalities, due to emergent cross-modal behavior. - Evaluations on ScanNet and 3RScan show superior performance across diverse metrics compared to existing state-of-the-art methods, demonstrating robustness in handling real-world scenarios with incomplete or misaligned data. | ['Multimodal', 'Computer Vision', 'Image Feature Extraction'] | N/A | N/A |
| [Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries](https://arxiv.org/abs/2502.14975) | Grant Rosario, David Noever | - This paper introduces the Persona Construction Benchmark (PCB), a dataset and evaluation framework for assessing emotional boundary handling in Large Language Models (LLMs). - The PCB dataset consists of 1156 prompts across six languages, designed to evaluate how LLMs respond to users' attempts to form emotional connections. - Three leading LLMs (GPT-4, Claude-3.5 Sonnet, and Mistral-large) were evaluated on their ability to maintain appropriate emotional boundaries using pattern-matched response analysis. - Results reveal significant variations in boundary-handling approaches across models and languages, with Claude-3.5 demonstrating the most sophisticated handling of emotional boundaries. - The study highlights the need for more nuanced evaluation methods and culturally sensitive training data for improving LLMs' emotional intelligence and boundary-setting capabilities. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/dnoever/Persona_Construction_Benchmark/tree/main) |
| [UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning](https://arxiv.org/abs/2502.15082) | Mohit Bansal, Elias Stengel-Eskin, vaidehi99 | - UPCORE, a method-agnostic data selection framework, mitigates collateral damage during unlearning in LLMs by pruning outliers from the forget set. - It leverages a correlation between the variance of hidden states and model degradation to select a core forget set, thereby optimizing the trade-off between deletion efficacy and model utility. - UPCORE improves both standard unlearning metrics and a proposed area-under-the-curve (AUC) metric, demonstrating superior performance across three unlearning methods and two question-answering settings. - It achieves positive knowledge transfer, effectively unlearning pruned points through generalization from the core set, and reduces negative transfer to unrelated data, resulting in better knowledge retention. - UPCORE demonstrates robustness to rephrased and jailbreak prompts, ensuring the deleted information is not easily retrievable through adversarial attacks. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Vaidehi99/UPCORE) | N/A |


## Papers for 2025-02-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](https://arxiv.org/abs/2502.14739) | Liam-Liu, kangz, aaabiao, BingliW, mkj69 | - This paper introduces SuperGPQA, a comprehensive benchmark designed to evaluate graduate-level knowledge and reasoning capabilities of Large Language Models (LLMs) across 285 academic disciplines. - SuperGPQA contains 26,529 multiple-choice questions with an average of 9.67 options per question, sourced from reference books and collaboratively refined using human-LLM filtering techniques to ensure question quality and difficulty. - Experimental results on state-of-the-art LLMs reveal significant room for improvement in diverse knowledge domains, with the best performing models (reasoning-focused) achieving around 61.82% accuracy. - The benchmark demonstrates the importance of reasoning capabilities, the effectiveness of instruction tuning, and varying performance across disciplines.  - The paper emphasizes the need for comprehensive cross-domain evaluation in LLMs and offers methodological guidance for building similar benchmarks | ['Question Answering'] | N/A | N/A |
| [SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://arxiv.org/abs/2502.14786) | Xiao Wang, talfanevans, ibomohsin, AlexeyG, mitsch | - SigLIP 2, a family of multilingual vision-language encoders, builds upon the original SigLIP by incorporating techniques like captioning-based pretraining, self-supervised losses, and online data curation. - The model architecture uses a ViT architecture and supports multiple resolutions, preserving the input's native aspect ratio in a NaFlex variant. - SigLIP 2 outperforms its predecessor and other baselines in zero-shot classification, image-text retrieval, and transfer learning tasks for Vision-Language Models (VLMs), as demonstrated by improved performance on benchmarks like ImageNet, COCO, and Flickr. - Additionally, it shows significant improvements in localization and dense prediction tasks, leading to better performance in applications such as referring expression comprehension and open-vocabulary segmentation. - Trained on a diverse, de-biased dataset, SigLIP 2 improves fairness and multilingual understanding compared to the original SigLIP. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Classification', 'Image-to-Text'] | [Link](https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md) | N/A |
| [MLGym: A New Framework and Benchmark for Advancing AI Research Agents](https://arxiv.org/abs/2502.14499) | Nikolay Bashlykov, Nicholas Roberts, Lovish Madaan, rraileanu, dnathani | - Meta introduces MLGYM, a new Gym environment and framework designed for developing and evaluating Large Language Model (LLM) agents for diverse, open-ended AI research tasks. - MLGYM-Bench, an accompanying benchmark, features 13 tasks spanning computer vision, natural language processing, reinforcement learning, and game theory, requiring agents to exhibit research skills like hypothesis generation and experimental design. - Initial evaluations of frontier LLMs (Claude, Llama, GPT, Gemini) reveal that while current models can improve upon given baselines (typically by optimizing hyperparameters), they struggle with generating novel hypotheses or significant improvements. - MLGYM's flexible design enables research on diverse training algorithms (e.g., reinforcement learning, curriculum learning) and accepts diverse artifacts (model weights, algorithms) for evaluation, unlike existing frameworks. - A new metric adapted from optimization and AutoML literature is introduced to compare LLM agent performance across tasks with differing performance metrics. | ['Reinforcement Learning', 'Natural Language Processing', 'Computer Vision', 'Other'] | [Link](https://github.com/facebookresearch/MLGym) | N/A |
| [S*: Test Time Scaling for Code Generation](https://arxiv.org/abs/2502.14382) | Shangyin Tan, Xiuyu Li, Chengkun Cao, Dacheng Li, eva98 | - S* is a novel hybrid test-time scaling framework for code generation that improves both the coverage and selection accuracy of generated code. - It extends parallel scaling with sequential scaling using iterative debugging based on execution feedback, and introduces a new adaptive selection mechanism grounded in execution where LLM generate distinguishing inputs for pairwise comparison. - The method is evaluated on 12 large language and reasoning models across various model sizes and families and two benchmarks (LiveCodeBench and CodeContests). - Results show that S* consistently boosts performance across different types of LLMs. - In particular, S* enables smaller models to outperform larger models in the same family, non-reasoning models to outperform reasoning models, and open models to be competitive with closed models. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/NovaSky-AI/SkyThought) | N/A |
| [How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?](https://arxiv.org/abs/2502.14502) | Vasily Konovalov, Daniil Moskovskiy, Maria Marina, msalnikov, memyprokotow | - This paper investigates the extent to which new factual knowledge can be integrated into Large Language Models (LLMs) using Low-Rank Adaptation (LoRA) without compromising pre-existing knowledge. - Experiments fine-tuning Llama-3.1-8B-instruct with varying amounts of new knowledge reveal that optimal performance is achieved with a mixture of known and new facts in the training data. - However, over-reliance on new information can negatively impact performance on external question-answering benchmarks and introduce biases towards overrepresented entities. - Two fine-tuning techniques are introduced to counter these effects: incorporating paraphrased new facts and adding facts already known to the model, with analysis indicating that training with HighlyKnown samples is a preferable strategy. - Analysis of knowledge shifts show that positive shifts arise when the model correctly answer questions it couldn't answer before, while negative shifts occur due to the model's convergence on over-represented answers from the training set. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/AIRI-Institute/knowledge-packing) | [Link](https://hf.co/meta-llama/Meta-Llama-3-8B-Instruct), [Link](https://hf.co/meta-llama/Meta-Llama-3-70B-Instruct) |
| [Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information](https://arxiv.org/abs/2502.14258) | Jaewoo Kang, Minbyul Jeong, Jungwoo Park, Chanwoong Yoon, Yein Park | - This paper introduces the concept of "Temporal Heads" in large language models (LLMs), which are specialized attention heads responsible for processing time-dependent information. - Through circuit analysis and targeted ablation experiments, the researchers demonstrate that these heads play a crucial role in recalling time-sensitive facts. - Disabling these heads selectively impairs the model's ability to retrieve temporally accurate information while leaving other functionalities intact. - The researchers further show that these heads are activated by both numerical and textual time cues and that their activations can be manipulated to edit temporal knowledge within the model. - The findings shed light on the internal mechanisms of temporal reasoning in LLMs and pave the way for more refined control over their ability to represent and utilize time-dependent knowledge. | ['Natural Language Processing'] | [Link](https://github.com/dmis-lab/TemporalHead) | N/A |
| [LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models](https://arxiv.org/abs/2502.14834) | Jifan Yu, Yushi Bai, Daniel Zhang-Li, Yucheng Wang, Shangqing Tu | - LongWriter-V introduces a new approach to enable ultra-long and high-fidelity generation in vision-language models (VLMs). - A new dataset, LongWriter-V-22k, comprising 22,158 examples with multiple input images, instructions, and long outputs (0-10,000 words) was created using a plan-and-write approach with GPT-4. - Iterative Direct Preference Optimization (IterDPO) was proposed to improve the quality of long outputs by breaking them into segments and using iterative corrections to form preference pairs, enabling efficient utilization of human feedback. - A benchmark, MMLongBench-Write, featuring six tasks designed to evaluate the long-generation capabilities of VLMs was developed to show current VLMs struggle to generate coherent outputs exceeding 1,000 words.  - The proposed 7B parameter model, trained with the new dataset and IterDPO, achieved impressive performance, outperforming larger proprietary models such as GPT-40 on this benchmark. | ['Image-Text-to-Text', 'Multimodal'] | [Link](https://github.com/THU-KEG/LongWriter-V) | N/A |
| [Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning](https://arxiv.org/abs/2502.14768) | Yuqian Hong, Haoming Luo, Qingnan Ren, Zitian Gao, Tian Xie | - This paper introduces Logic-RL, a rule-based reinforcement learning framework designed to improve the reasoning abilities of large language models (LLMs).  - The framework trains LLMs on a synthetic dataset of Knights and Knaves (K&K) logic puzzles, utilizing a carefully designed reward system and the REINFORCE++ algorithm and incorporates modifications like KL loss and unbiased KL estimation for enhanced training stability and convergence. - The 7B model trained with Logic-RL demonstrates a remarkable ability to generalize to challenging math benchmarks (AIME and AMC) after training on just 5K logic problems, showing a 125% improvement on AIME and 38% on AMC compared to the base model. - The paper explores several research questions related to RL algorithm comparison, the impact of thinking tokens, emergence of reasoning behaviors, generalization to OOD tasks, and the role of curriculum learning. - The study provides valuable insights into the dynamics of RL training for reasoning in LLMs and opens avenues for future research. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC](https://arxiv.org/abs/2502.14282) | Junyang Wang, Yuyang Wanyan, Haiyang Xu, Xi Zhang, Haowei Liu | - PC-Agent, a hierarchical multi-agent framework, is proposed for automating complex tasks on PCs, addressing the challenges of intricate interactive environments and inter-app workflows. - The framework incorporates an Active Perception Module (APM) to enhance fine-grained perception and operation on screen elements and text, leveraging accessibility trees and an MLLM-driven intention understanding agent with OCR. - A hierarchical structure decomposes decision-making into instruction, subtask, and action levels, managed by Manager, Progress, and Decision agents, respectively, enabling efficient handling of inter-subtask dependencies. - A Reflection agent provides feedback on action outcomes, allowing for dynamic adjustments and error correction, thus improving task completion rates. - PC-Agent significantly outperforms previous methods on PC-Eval, a new benchmark with real-world complex PC tasks, demonstrating a 32% improvement in task success rate over state-of-the-art methods. | ['Multimodal'] | N/A | N/A |
| [S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning](https://arxiv.org/abs/2502.12853) | Jiaqi Chen, Xingyan Liu, Cheng Liu, Peisong Wang, Ruotian Ma | - S2R, a new framework, enhances LLM reasoning by teaching models to self-verify and self-correct during inference, focusing on iterative self-verification and self-correction behaviors. - It initializes LLMs with these behaviors through supervised fine-tuning on curated data and further strengthens them with outcome and process-level reinforcement learning. - Qwen2.5-math-7B, trained with S2R using only 3.1k samples, shows significant improvement, achieving accuracy from 51.0% to 81.6% on Math500 and outperforming models trained on equivalent long-CoT distilled data (80.2%). - S2R effectively employs both outcome and process-level reinforcement learning to boost self-verification and self-correction capabilities and explores more flexible and effective test-time scaling. - Experiments across various base models and benchmarks, including out-of-domain general tasks like MMLU-PRO, demonstrate the effectiveness and generalizability of the learned self-verifying and self-correcting abilities. | ['Natural Language Processing', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/NineAbyss/S2R) | N/A |
| [Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation](https://arxiv.org/abs/2502.14846) | Luca Weihs, Tanmay Gupta, Matt Deitke, Ajay Patel, Yue Yang | - This paper introduces CoSyn, a framework that leverages large language models (LLMs) to generate synthetic text-rich multimodal data for training vision-language models (VLMs). - CoSyn uses LLMs to generate code in various formats (Python, HTML, LaTeX) that renders synthetic text-rich images, and then uses the code as context to generate corresponding textual instructions, creating instruction-tuning data. - Using CoSyn, the authors created CoSyn-400k, a dataset with 400k images and 2.7M instruction-tuning examples, and demonstrated state-of-the-art performance on 7 text-rich VQA benchmarks, outperforming open-source models like Llama 3.2 and proprietary models like GPT-4V and Gemini 1.5 Flash. -  CoSyn also generates synthetic pointing data to enable VLMs to ground information within images, achieving state-of-the-art performance on the ScreenSpot click prediction benchmark. - Further analysis demonstrates CoSyn's ability to improve zero-shot generalization on novel tasks, mitigate dataset biases, and enable efficient data generation for chain-of-thought reasoning in VLMs. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | [Link](https://huggingface.co/datasets/yyupenn/DocPointQA), [Link](https://huggingface.co/datasets/yyupenn/NutritionQA) |
| [AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO](https://arxiv.org/abs/2502.14669) | Dinh Bach Vu, Alan Dao | - AlphaMaze, a novel two-stage training framework, enhances the visual spatial reasoning abilities of Large Language Models (LLMs) for maze navigation by combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). - The model uses a tokenized representation of the maze as input, which allows the LLM to "see" the structure and plan its movements. - The model significantly outperforms a baseline model, achieving a 93% success rate on MazeBench, a proposed benchmark for evaluating maze-solving capabilities, while the baseline model has 0% accuracy. - GRPO refines the model's decision-making, leading to emergent chain-of-thought behaviors and self-correction capabilities, similar to those observed in DeepSeek-R1. - The research demonstrates the potential of combining SFT and GRPO to enhance visual reasoning in LLMs and suggests broader applications in robotics and other visual AI domains that involve spatial understanding and sequential decision-making. | ['Multimodal', 'Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild](https://arxiv.org/abs/2502.12769) | Goran Glavaš, Anne Lauscher, saadob12 | - This paper investigates the hallucination rates of Large Language Models (LLMs) across 30 languages in a knowledge-intensive, long-form question answering setting, representative of real-world LLM usage. - A multilingual hallucination detection model is trained using a translate-train approach, where an English hallucination dataset is machine-translated into 30 languages. - The study uses a new benchmark, MFAVA, containing both silver (synthetic) and gold (human-annotated) hallucination evaluation datasets for a subset of languages. - It is found that while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, length-normalized hallucination rates do not correlate with a language's digital representation. - Smaller LLMs exhibit higher hallucination rates compared to larger models and models supporting more languages tend to hallucinate more. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/WorldHellow/mHallucinations-LLM) | N/A |
| [Unstructured Evidence Attribution for Long Context Query Focused Summarization](https://arxiv.org/abs/2502.14409) | David Jurgens, Isabelle Augenstein, Lu Wang, Zain Muhammad Mujahid, dwright37 | - This paper proposes a new task: long-context query-focused summarization with unstructured evidence citation, aiming to improve transparency and reliability of LLM-generated summaries. - It introduces SUnsET (Summaries with Unstructured Evidence Text), a synthetic dataset created through an inductive pipeline to address challenges like positional bias (lost-in-the-middle) and lack of diverse evidence. - The pipeline generates diverse titles, outlines, queries, summaries, and evidence passages, refines them, and validates accuracy. - Two fine-tuning schemes using LoRA (low-rank adaptation) are employed: standard (concatenated sections) and shuffled (randomized section order) to mitigate positional bias and enhance citation quality. - Experiments across five LLMs and four datasets show that fine-tuning with SUnsET improves evidence extraction, citation accuracy, and summary quality, mitigating lost-in-the-middle effects and bridging the performance gap with stronger models. | ['Summarization'] | N/A | N/A |


## Papers for 2025-02-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923) | Keqin Chen, Shuai Bai, xhyandwyy, darkpromise, ayumiymk | - Qwen2.5-VL is a new vision-language model series demonstrating advancements in visual recognition, object localization, document parsing, and long-video comprehension. - It utilizes a redesigned Vision Transformer (ViT) architecture with window attention, dynamic FPS sampling, and absolute time encoding to enhance efficiency and handle diverse inputs. - A Multimodal Rotary Position Embedding (MROPE) aligned to absolute time enhances temporal understanding. - Qwen2.5-VL outperforms existing open-source models and matches or surpasses top-tier closed-source models like GPT-40 and Claude 3.5 Sonnet on benchmarks such as MMMU, MathVista, MMBench, and MME-RealWorld, particularly excelling in document and diagram understanding. - The model is available in three sizes (72B, 7B, and 3B) to cater to various computational resource constraints. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text', 'Computer Vision', 'Object Detection'] | [Link](https://github.com/QwenLM/Qwen2.5-VL) | [Link](https://huggingface.co/Qwen) |
| [SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation](https://arxiv.org/abs/2502.13128) | Pan Zhang, Xiaoyi Dong, Zhixiong Zhang, Shuangrui Ding, Zihan Liu | - SongGen is a single-stage auto-regressive transformer model for generating songs from text descriptions, lyrics, and an optional reference voice for cloning. - It utilizes a unified framework with two generation modes: mixed mode (generating vocals and accompaniment together) and dual-track mode (generating them separately). - The model incorporates diverse musical controls like instrumentation, genre, mood, and timbre through modal-specific encoders and cross-attention mechanisms. - Experiments on the MusicCaps dataset show that SongGen achieves competitive performance with state-of-the-art models and even surpasses commercial products in certain aspects like text relevance and vocal control. - The automated data preprocessing pipeline and open-source release of the model contribute significantly to the field of text-to-song generation by addressing data scarcity and promoting further research. | ['Text-to-Audio', 'Multimodal'] | [Link](https://github.com/LiuZH-19/SongGen) | N/A |
| [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685) | Yu Cheng, Jiaxi Hu, Disen Lan, Jusen Du, weigao266 | - This paper introduces Mixture-of-Memories (MoM), a novel architecture for linear sequence modeling inspired by the brain's memory mechanisms. - MoM utilizes multiple independent memory states and a router network to direct input tokens to specific memory states, enhancing memory capacity and mitigating interference. - MoM maintains linear complexity during training and constant complexity during inference. - Experimental results demonstrate that MoM outperforms other linear sequence models on various language tasks, especially recall-intensive tasks. - MoM even achieves performance comparable to Transformer models, bridging the gap between linear and quadratic complexity models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/OpenSparseLLMs/MoM), [Link](https://github.com/OpenSparseLLMs/Linear-MoE) | N/A |
| [Craw4LLM: Efficient Web Crawling for LLM Pretraining](https://arxiv.org/abs/2502.13347) | Chenyan Xiong, Zhiyuan Liu, yushi | - CRAW4LLM, a novel web crawling method, prioritizes webpages based on their influence on Large Language Model (LLM) pretraining, rather than traditional graph connectivity metrics. - This method uses a pretraining influence scorer, derived from data filtering pipelines, to rank URLs and guide the crawling process. - Experiments on a 900 million webpage dataset demonstrate that CRAW4LLM achieves comparable downstream performance to traditional methods while crawling only 21% of the data. - This efficiency significantly reduces crawling waste and server load on websites. - CRAW4LLM also exhibits a faster discovery rate of high-quality pretraining documents compared to baseline crawlers. | ['Natural Language Processing'] | [Link](https://github.com/cxcscmu/Crawl4LLM) | N/A |
| [LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization](https://arxiv.org/abs/2502.13922) | Lidong Bing, Michael Qizhe Shieh, Xin Li, Guanzheng Chen | - LongPO is a novel long-context alignment method that allows large language models (LLMs) to self-evolve their short-context capabilities to excel in long-context tasks without external data annotation. - It leverages short-to-long preference data, consisting of paired responses generated for identical instructions with long and compressed short contexts, respectively, to capture intrinsic model knowledge. - A short-to-long KL constraint is incorporated to maintain short-context performance during long-context alignment. - Applied to Mistral-7B and Qwen2.5-7B, LongPO significantly outperforms naive SFT and DPO in both long and short-context tasks, improving by 10-20+ points, while retaining original short-context performance. - LongPO-trained models achieve long-context benchmark results comparable or superior to larger models like GPT-4-128K, despite requiring less training data. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/DAMO-NLP-SG/LongPO) | N/A |
| [Small Models Struggle to Learn from Strong Reasoners](https://arxiv.org/abs/2502.12143) | Luyao Niu, Fengqing Jiang, Xiang Yue, Yuetai Li, flydust | - This paper introduces Mix Distillation, a novel technique to enhance the reasoning capabilities of small language models (LLMs). - Mix Distillation addresses the "Small Model Learnability Gap," where small LLMs struggle to learn from complex reasoning data generated by stronger, larger models. - The technique involves combining diverse types of reasoning data during fine-tuning. Mix-Long blends long and short chain-of-thought (CoT) examples, and Mix-Large mixes responses from both larger and smaller teacher models. - Experimental results demonstrate that Mix Distillation significantly improves small model reasoning performance compared to traditional distillation methods. For instance, Qwen2.5-3B-Instruct sees an improvement of over 8 points on MATH and AMC datasets with Mix-Long, and over 7 points with Mix-Large. - These findings emphasize the importance of balancing reasoning complexity when transferring knowledge to smaller LLMs. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Autellix: An Efficient Serving Engine for LLM Agents as General Programs](https://arxiv.org/abs/2502.13965) | Tianjun Zhang, Colin Cai, Xiaoxiang Shi, Michael Luo, Chrisyichuan | - Autellix, an LLM serving system, is introduced to minimize end-to-end latency for LLM agents acting as general programs by prioritizing program completion rather than individual LLM calls. - Unlike existing LLM serving systems, Autellix intercepts and enriches program-level context for LLM calls to create an execution graph and two scheduling algorithms: PLAS for single-threaded programs and ATLAS for distributed programs. - These algorithms prioritize calls from programs with less cumulative execution time and dynamically preempt long calls, reducing call and program-level head-of-line blocking.  - Across various LLMs and agent workloads, Autellix exhibits a 4-15x throughput improvement compared to state-of-the-art systems like vLLM at the same latency. - Autellix also improves throughput up to 1.5x over standard load balancers when routing calls across multiple engines by accounting for program-level data locality. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) |
| [Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946) | Wenjie Li, Jian Wang, Qingyu Yin, Chak Tou Leong | - This paper investigates a phenomenon called Template-Anchored Safety Alignment (TASA) in large language models (LLMs), where safety mechanisms overly rely on the template region between user instruction and model output. - The study confirms TASA's presence across various aligned LLMs through experiments showing attention shifts from instruction to template regions when processing harmful requests, and interventions in the template region significantly influence safety decisions. - A strong link is established between TASA and LLM vulnerabilities, as manipulating template information during response generation allows harmful requests to bypass safeguards, and common attacks disrupt the processing of harmfulness features within this region. - Detaching the safety mechanism from the template by transferring harmfulness probes and injecting them during generation, effectively mitigates these vulnerabilities and improves model robustness. - The findings highlight the need for more sophisticated safety alignment methods that avoid over-reliance on shortcuts like template information. | ['Natural Language Processing'] | N/A | N/A |
| [SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?](https://arxiv.org/abs/2502.13233) | Tianming Liu, Quanzheng Li, Canyu Chen, Tianze Yang, YuchengShi | - SearchRAG, a novel framework, enhances LLM performance in medical question answering by leveraging real-time search engines. - It uses synthetic query generation to transform complex medical questions into search-engine-friendly queries and uncertainty-based knowledge selection to incorporate relevant information into LLMs' input. - SearchRAG consistently outperforms baseline approaches, conventional RAG, and iterative RAG on MedMCQA, MMLU_Med, and MedQA datasets using LLaMA 8B, showing an average 12.61% improvement in answer accuracy. - The uncertainty-based filtering is effective across different model scales, improving LLaMA 8B by 4-7% and providing benefits even to larger models like LLaMA 70B. - Scaling the number of generated synthetic queries up to 32 further boosts performance, demonstrating the effectiveness of diverse query generation in capturing relevant information from search results. | ['Question Answering'] | N/A | N/A |
| [Presumed Cultural Identity: How Names Shape LLM Responses](https://arxiv.org/abs/2502.11995) | Lucie-Aimée Kaffee, Arnav Arora, Siddhesh Pawar, IAugenstein | - This paper investigates how names influence the responses of Large Language Models (LLMs), focusing on cultural biases. - By analyzing responses to information-seeking questions containing names from various cultures, the study finds that LLMs make strong cultural assumptions based on names, particularly for East Asian and Russian names. - The study utilizes a dataset of 900 names across 30 cultures, prompting 4 different LLMs with and without names in the prompts, and compares cultural presumptions in responses using LLM-as-a-judge and assertion based approaches using the CANDLE knowledge graph. - The paper also reveals that some names introduce significantly more bias than others, affecting the diversity of LLM-generated suggestions, particularly related to clothing and traditions. - This has implications for designing personalized systems, as biases emerging from names could reinforce stereotypes. | ['Natural Language Processing'] | N/A | N/A |
| [Thinking Preference Optimization](https://arxiv.org/abs/2502.13173) | Xiaotian Han, Vipin Chaudhary, Jingfeng Yang, Hongye Jin, Wang Yang | - This paper introduces Thinking Preference Optimization (ThinkPO), a post-Supervised Fine-Tuning (SFT) method to enhance long chain-of-thought (CoT) reasoning in Large Language Models (LLMs). - ThinkPO leverages readily available short CoT reasoning responses as rejected answers and long CoT responses as chosen answers, applying direct preference optimization to encourage longer reasoning outputs and thereby improve reasoning without new data acquisition. - Experiments demonstrated that ThinkPO improves the reasoning performance of SFT-ed models, increasing math reasoning accuracy by 8.6% and output length by 25.9%. - Notably, ThinkPO continually boosts the performance of publicly available distilled SFT models, such as DeepSeek-R1-Distill-Qwen-7B, improving from 87.4% to 91.2% accuracy on MATH500. - The results show that ThinkPO addresses performance bottlenecks in multi-epoch SFT with fixed, limited long-reasoning datasets, avoiding costly data acquisition. | ['Question Answering', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/uservan/ThinkPO) | N/A |
| [Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering](https://arxiv.org/abs/2502.13962) | Benjamin Van Durme, Jeffrey Cheng, wjurayj | - This paper investigates the impact of test-time scaling on selective question answering, where models can abstain from answering if uncertain. - It evaluates how increasing compute budgets affects both answer accuracy and confidence scores for large language models (LLMs). - Findings reveal that test-time scaling not only improves accuracy but also enhances confidence in correct answers. - It introduces a new evaluation methodology that penalizes incorrect answers and rewards abstentions, reflecting real-world scenarios with varying risk levels. - The study recommends reporting performance under "Jeopardy Odds" to incentivize confidence calibration and calls for future research to focus on efficient compute allocation for meeting confidence demands. | ['Question Answering'] | N/A | N/A |
| [AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence](https://arxiv.org/abs/2502.13943) | Jason Klein Liu, Chaofeng Qu, Zhaoling Chen, Junjie Lu, Yuliang Liu | - The paper introduces AdaptiveStep, a method that automatically divides reasoning steps in Large Language Model (LLM) responses based on model confidence, addressing the limitations of rule-based and manual step division methods. - The resulting Process Reward Model (PRM), called ASPRM, achieves state-of-the-art Best-of-N performance in mathematical reasoning (GSM8k and MATH500) and code generation tasks (LeetCodeDataset), outperforming greedy search with token-level value-guided decoding and reducing construction costs compared to existing PRMs. - TVD using ASPRM consistently outperforms greedy decoding by 3.15% on GSM8k and 14.4% on MATH500 in mathematical reasoning, and by 6.54% and 3.70% on LeetCodeDataset and LiveCodeBench in code generation. - ASPRM demonstrates transferability, in-domain, and cross-domain generalization capabilities, showing its robustness and wider applicability. - Analysis of step division features reveals that ASPRM effectively identifies critical decision points, offering valuable insights into LLM reasoning processes. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation](https://arxiv.org/abs/2502.13581) | Wang-Cheng Kang, Noveen Sachdeva, Zhankui He, Jianmo Ni, hyp1231 | - ActionPiece, a novel contextually aware method for tokenizing action sequences in generative recommendation, is introduced. - Unlike existing methods that tokenize each action independently, ActionPiece considers the surrounding context, allowing identical actions to be tokenized differently based on their context. - It uses a vocabulary construction process that merges feature patterns into new tokens based on their co-occurrence frequency within and across adjacent action sets, and a segmentation process using set permutation regularization to produce multiple semantically equivalent token sequences of an action sequence. - The authors performed experiments across three datasets from the Amazon Reviews dataset demonstrating that ActionPiece outperforms baseline models by 6.00% to 12.82% on NDCG@10. | ['Natural Language Processing', 'Feature Extraction'] | N/A | N/A |
| [Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models](https://arxiv.org/abs/2502.13533) | Ke Chen, Lidan Shou, Huan Li, Jue Wang, junzhang98 | - LORAM, a memory-efficient training method for Large Language Models (LLMs) using Low-Rank Adaptation (LoRA), is introduced. - LORAM trains on a smaller, pruned version of the model and recovers the low-rank matrices for inference with the full-sized model, reducing memory usage during training. - It includes continual pre-training on a small dataset to address knowledge discrepancies between the pruned and original models. - QLORAM, a combination of LORAM with structured pruning and 4-bit quantization, reduces memory costs for LLaMA-2-70B parameters by 8.21x while improving performance compared to both the original and LoRA-trained smaller models. - Experiments show QLORAM's effectiveness across different pruning algorithms, model sizes, and tasks. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/junzhang-zj/LORAM) | N/A |
| [GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking](https://arxiv.org/abs/2502.13766) | Anne Lauscher, Chris Biemann, Carolin Holtermann, floschne | - Introduces GIMMICK, a multimodal benchmark designed to evaluate cultural knowledge in LLMs and LVLMs across diverse countries and regions. - The benchmark includes six tasks built on three datasets spanning various cultural facets, including Visual Question Answering on images and videos, Cultural Origin Question Answering, and Cultural Knowledge Question Answering (naming and describing). - Evaluates 31 models, including proprietary and open-weight LLMs and LVLMs with sizes ranging from 500M to 78B parameters, across multiple model families. - Reveals performance biases toward Western cultures, correlations between model size and performance, and the effectiveness of multimodal input and external geographic cues. - Demonstrates that while models understand broad cultural categories, they struggle with nuanced understanding and intangible cultural aspects. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/floschne/gimmick) | N/A |
| [InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning](https://arxiv.org/abs/2502.11573) | Zhijie Sang, Pengxiang Li, Wenjun Wang, Shuo Cai, Congkai Xie | - This paper introduces InfiR, a training pipeline for developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) with competitive reasoning capabilities. - InfiR employs a novel pre-and post-training pipeline, focusing on high-quality data curation and filtering, achieving state-of-the-art performance in under 6000 GPU hours. - InfiR-1B models outperform Llama3.2-1B models by significant margins on reasoning benchmarks, including a 2.26x improvement on GSM8K and 1.33x on Llama3.2-1B-Instruct. - The InfiR-VL-1.6B model demonstrates state-of-the-art performance in the Android World scenario with a 28% accuracy increment compared to other small models. - The research aims to improve reasoning, reduce adoption barriers, and address privacy concerns through smaller model sizes, facilitating deployment on edge devices. | ['Multimodal', 'Natural Language Processing', 'Question Answering', 'Visual Question Answering'] | [Link](https://github.com/Reallm-Labs/InfiR) | N/A |
| [Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective](https://arxiv.org/abs/2502.13573) | Qiang Yang, Jian Jin, Yu Zhang, Xiaopu Zhang, yyyaoyuan | - This paper investigates transferable knowledge in semi-supervised heterogeneous domain adaptation (SHDA), where source and target domains have different feature representations and distributions. - Through extensive experiments, the authors surprisingly found that both category and feature information from source samples do not significantly affect target domain performance, and even noise can contain transferable knowledge. - They design a Knowledge Transfer Framework (KTF) for SHDA and conduct large-scale experiments with synthetic noise domains. - They find that transferable knowledge primarily comes from the transferability and discriminability of the source domain itself. -  Regardless of source sample origins, ensuring source domain transferability and discriminability enhances SHDA knowledge transfer effectiveness. | ['Computer Vision', 'Multimodal'] | [Link](https://github.com/yyyaoyuan/SHDA) | N/A |


## Papers for 2025-02-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Soundwave: Less is More for Speech-Text Alignment in LLMs](https://arxiv.org/abs/2502.12900) | Benyou, PhoenixAxis, FanBuCUHK, puccho, Yoohao |  - This paper introduces Soundwave, a novel two-stage training framework for speech-text alignment in LLMs that addresses the representation space gap and sequence length inconsistency issues. - Soundwave employs an efficient training strategy using an alignment adapter and a shrinking adapter to bridge the gap between speech and text representations. - Experimental results on speech translation and AIR-Bench speech tasks demonstrate that Soundwave outperforms the advanced Qwen2-Audio model while using significantly less training data (one-fiftieth). - Further analysis reveals that Soundwave maintains its conversational intelligence. - The project's code is available on GitHub. | ['Audio', 'Automatic Speech Recognition', 'Audio Classification'] | [Link](https://github.com/FreedomIntelligence/Soundwave) | N/A |
| [Continuous Diffusion Model for Language Modeling](https://arxiv.org/abs/2502.11564) | Sung Ju Hwang, harryjo97 | - This paper proposes Riemannian Diffusion Language Model (RDLM), a continuous diffusion model for language modeling that leverages the geometry of the underlying categorical distribution. - The model establishes a connection between discrete diffusion and continuous flow on the statistical manifold, and introduces a simple design for the diffusion process that generalizes previous discrete diffusion models. - It also proposes a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. - Experimental results on language modeling benchmarks (Text8 and One Billion Words Dataset), image modeling (CIFAR-10) and biological sequence design show that RDLM outperforms existing discrete diffusion models and approaches the performance of autoregressive models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/harryjo97/RDLM) | N/A |
| [Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity](https://arxiv.org/abs/2502.13063) | Aydar Bulatov, Mikhail Arkhipov, mbur, yurakuratov | - This paper explores the capacity of input embeddings in Large Language Models (LLMs) to compress and reconstruct long text sequences. - The authors introduce a method to compress text into trainable "memory" vectors using a per-sample optimization procedure, demonstrating compression ratios up to x1500, significantly exceeding previous methods. - They establish a connection between the latent capacity of input vectors and text cross-entropy, providing a quantifiable measure of information encoding. - The study shows that capacity limits are consistent across various text lengths and domains, including natural and random text, suggesting the capacity is inherent to the model rather than memorization. - Results indicate that capacity scales almost linearly with the number of trainable vectors, showcasing potential for efficient long-context processing and integration with memory-augmented architectures. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464) | Minki Kang, Dong Bok Lee, hbseong, dwgnr, Seanie-lee | - SafeRoute, a binary router, is proposed to enhance the efficiency and accuracy of safety guardrails in Large Language Models (LLMs). - SafeRoute selectively applies a larger safety guard model only to complex inputs identified by the router, while using a smaller model for simpler inputs. - This method improves the trade-off between computational cost and safety performance, outperforming baseline methods. - Experiments conducted on multiple benchmark datasets show that SafeRoute significantly improves F1 scores compared to individual use of smaller or larger models. - It reduces the computational overhead by only using the larger model on a small subset of the inputs while maximizing accuracy. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Llama-Guard-3-1B), [Link](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), [Link](https://huggingface.co/meta-llama/Llama-Guard-3-8B), [Link](https://huggingface.co/ibm-granite/granite-guardian-3.0-8b), [Link](https://huggingface.co/answerdotai/ModernBERT-large) |
| [Rethinking Diverse Human Preference Learning through Principal Component Analysis](https://arxiv.org/abs/2502.13131) | Hao Sun, Feng Luo, huanzhang12, CharlesDDDD, Ray2333 | - This paper introduces Decomposed Reward Models (DRMs), a novel approach for extracting diverse human preferences from binary comparisons, eliminating the need for fine-grained annotations. - DRMs represent preferences as vectors and utilize Principal Component Analysis (PCA) to identify orthogonal basis vectors capturing distinct preference aspects. - These basis vectors form reward models, linearly combined using test-time adaptation to align with individual user preferences. - Experiments on RewardBench and RPR datasets demonstrate DRMs' superior performance compared to single-head, ensemble-head, and random-head baselines in test-time preference adaptation. - DRMs effectively capture diverse preference attributes, offering interpretable representations and adapting well to user preferences without additional training. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/weqweasdas/preference_dataset_mixture2_and_safe_pku) |
| [Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation](https://arxiv.org/abs/2502.13145) | Qian Zhang, wenyuliu, wondervictor, HongyuanTao, LegendBC | - This paper introduces mmMamba, a framework for developing linear-complexity native multimodal state space models. - It achieves this through progressive distillation from existing Multimodal Large Language Models (MLLMs). - mmMamba offers two architectures: mmMamba-linear, with full linear complexity, and mmMamba-hybrid, which strategically transforms fixed intervals of Transformer layers into Mamba-2 layers for a balance between efficiency and performance. - By distilling from HoVLE, a Transformer-based decoder-only VLM, mmMamba-linear achieves competitive performance against other linear/quadratic-complexity VLMs while using 2x fewer parameters than EVE-7B. - At 103K tokens, mmMamba-linear shows a 20.6x speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid has a 13.5x speedup and 60.2% memory savings. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/hustvl/mmMamba) | N/A |
| [FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading](https://arxiv.org/abs/2502.11433) | ShirleyY, Acatsama, YupengCao, zdeng10, xionggj001 | - FLAG-TRADER, a novel framework fusing Large Language Models (LLMs) with Reinforcement Learning (RL), is introduced for financial trading. - A partially fine-tuned LLM acts as the policy network, processing market data via textual state representations, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. - A hybrid RL component integrates external environment reward gradients for policy optimization, aligning with trading performance metrics. - Empirical results on stock and cryptocurrency trading tasks demonstrate FLAG-TRADER's superior performance over buy-and-hold and LLM-agentic baselines, achieving higher cumulative returns and Sharpe ratios. - Notably, FLAG-TRADER enables a small-scale (135M parameter) open-source LLM to outperform significantly larger proprietary models, highlighting the effectiveness of RL fine-tuning. | ['Reinforcement Learning', 'Natural Language Processing', 'Time Series Forecasting'] | N/A | N/A |
| [You Do Not Fully Utilize Transformer's Representation Capacity](https://arxiv.org/abs/2502.09245) | kefirski, ummagumm-a, elephantmipt, yaraksen, gudleifrr | - This paper introduces Layer-Integrated Memory (LIMe), a novel mechanism designed to enhance the representational capacity of Transformer decoders by mitigating representation collapse. - LIMe modifies the attention mechanism to allow each head to access representations from all preceding layers via a learnable routing mechanism, rather than relying solely on the immediately prior layer.  - Through experiments on language modeling tasks, LIMe demonstrates consistent performance improvements over standard Transformer baselines and other state-of-the-art modifications, achieving lower perplexity and better results on one-shot benchmarks.  - Analysis reveals that LIMe successfully counteracts representation collapse by preserving higher entropy in deeper layers, increasing the separability of related tokens, and improving overall representational diversity.  - LIMe also exhibits superior scaling behavior with increasing model depth, suggesting potential for deeper and more robust Transformer architectures. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/corl-team/lime) | N/A |
| [Magma: A Foundation Model for Multimodal AI Agents](https://arxiv.org/abs/2502.13130) | cheryyunl, Baolin, rzheng12, qianhuiwu, tanreuben |  - Magma is a multimodal foundation model for AI agents capable of understanding and grounding multimodal inputs within its environment. It bridges verbal and spatial intelligence to navigate complex tasks.  - The model architecture is a significant extension of Vision-Language models, incorporating spatial-temporal intelligence for action planning and execution.  - Magma achieves state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models.  - The model is pretrained on heterogeneous datasets including images, videos, and robotics data, using Set-of-Mark (SoM) and Trace-of-Mark (ToM) for action grounding and planning.  - Magma's code and model are publicly available for reproducibility. | ['Multimodal', 'Robotics'] | [Link](https://microsoft.github.io/Magma) | N/A |
| [RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm](https://arxiv.org/abs/2502.12513) | Kaicheng Yang, JiankangDeng, SeriousBro, Nina0607, GaryGuuu | - This paper introduces RealSyn, a large-scale multimodal dataset designed for vision-language representation learning, addressing the underutilization of non-paired multimodal interleaved documents. - RealSyn integrates realistic and synthetic texts, overcoming limitations of existing datasets by employing hierarchical retrieval and an image semantic augmented generation module to create image-text pairs. - The dataset is available in three sizes (15M, 30M, and 100M), demonstrating scalability and strong performance on downstream tasks. - Extensive experiments show that pre-trained models on RealSyn achieve state-of-the-art performance on various downstream tasks, outperforming existing methods. - The RealSyn dataset and pre-trained model weights are publicly available to facilitate further research. | ['Multimodal'] | [Link](https://github.com/deepglint/RealSyn) | N/A |
| [PAFT: Prompt-Agnostic Fine-Tuning](https://arxiv.org/abs/2502.12859) | Fei Richard Yu, Ying Tiffany He, Mingwen Ou, Yao Shu, kittttttt | - PAFT dynamically adjusts prompts during fine-tuning, encouraging the model to learn underlying task principles rather than overfitting to specific prompt formulations. - It operates in two stages: constructing a diverse set of synthetic candidate prompts and randomly sampling from this set during fine-tuning. - Experiments across diverse datasets and LLMs show that PAFT-trained models exhibit strong robustness and generalization across various prompts, including unseen ones. - This enhanced robustness leads to improved model performance and inference speed while maintaining training efficiency. - PAFT achieves higher accuracy and lower variance across different reasoning and reading comprehension tasks compared to baseline models and other prompt engineering methods, demonstrating its effectiveness in improving prompt robustness. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections](https://arxiv.org/abs/2502.12170) | Xingyuan Yuan, Da Xiao, lishengping, Hilbertmeng | - MUDDFormer introduces Multiway Dynamic Dense (MUDD) connections, dynamically generating connection weights based on hidden states and input streams (query, key, value, residual) within a Transformer block. - Unlike static or shared-weight dense connections, MUDD enhances cross-layer information flow, mitigating limitations of residual connections in deep transformers. - MUDDFormer consistently outperforms standard Transformers across various architectures and scales in language modeling, achieving performance comparable to models trained with ~1.8x-2.4x more compute. - MUDDPythia-2.8B matches Pythia-6.9B in pretraining perplexity and downstream tasks, even rivaling Pythia-12B in few-shot settings with minimal added parameters (0.23%) and computation (0.4%). - Analysis reveals MUDD connections improve depth utilization, reduce representation collapse, and vitalize attention heads, especially in deeper layers, thereby enhancing in-context learning abilities. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Caiyun-AI/MUDDFormer) | N/A |
| [Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?](https://arxiv.org/abs/2502.12215) | Yunhua Zhou, Qinyuan Cheng, Zhiyuan Zeng, xpqiu, yinzhangyue | - This paper investigates the test-time scaling capabilities of large language models (LLMs) like QwQ, Deepseek-R1 (R1), and LIMO, which are similar to OpenAI's o1 series. - The study reveals that increasing the chain-of-thought (CoT) length does not consistently improve accuracy, and correct solutions often have shorter lengths than incorrect ones. - The authors attribute this phenomenon to the limited self-revision capabilities of these models, finding that longer CoTs contain more self-revisions which often lead to performance degradation. - A comparison of sequential and parallel scaling strategies shows that parallel scaling achieves better coverage and scalability. - Based on these findings, the paper introduces "Shortest Majority Vote," a method combining parallel scaling with CoT length characteristics to prioritize shorter solutions, significantly improving test-time scalability compared to conventional majority voting. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning](https://arxiv.org/abs/2502.11271) | Joseph Boen, Rahul Thapa, Sheng Liu, Bowen Chen, lupantech | This paper introduces OctoTools, a training-free, user-friendly, and extensible framework for complex reasoning.  OctoTools utilizes standardized tool cards to integrate diverse tools, a planner for high-level and low-level planning, and an executor to manage tool usage. Experiments on 16 diverse benchmarks demonstrate OctoTools's superiority over existing methods, showing substantial accuracy gains.  OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem-solving. The framework is designed to be easily extensible with new tools. | ['Any-to-Any', 'Question Answering', 'Multimodal'] | [Link](https://octotools.github.io) | N/A |
| [Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge](https://arxiv.org/abs/2502.12501) | zhangsan5421, lifengshang, horiz94, YuxinJiang, DonJoey | - This paper introduces Crowd-based Comparative Evaluation (CCE), a novel method for enhancing the reliability of LLM-as-a-Judge auto-evaluation. - CCE introduces additional crowd responses to compare with candidate responses, exposing deeper details and improving the comprehensiveness of CoT judgments. - Experiments across five benchmarks show that CCE improves accuracy by an average of 6.7% and produces higher-quality CoTs. - CCE is also applied to judge distillation and crowd rejection sampling for supervised fine-tuning (SFT), demonstrating its effectiveness. - The analysis reveals that CCE-generated CoTs are more comprehensive, and evaluation accuracy improves as inference scales. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation](https://arxiv.org/abs/2502.09838) | Binhe Yu, Yuqian Yuan, Sijing Li, Wenqiao Zhang, Tianwei Lin | - HealthGPT is a medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation within a unified autoregressive paradigm. - It uses a novel Heterogeneous Low-Rank Adaptation (H-LORA) technique for parameter-efficient fine-tuning and a hierarchical visual perception approach to effectively learn from heterogeneous medical data. - HealthGPT outperforms both state-of-the-art unified visual models and medical-specific models on various comprehension and generation tasks, highlighting its superior capability in complex healthcare applications. - A comprehensive medical domain-specific comprehension and generation dataset, called VL-Health, was created to train HealthGPT. - HealthGPT achieves performance comparable to or better than state-of-the-art models on several medical multi-modal tasks in data-constrained scenarios. | ['Multimodal', 'Image-to-Image', 'Image-to-Text', 'Text-to-Image', 'Visual Question Answering'] | [Link](https://github.com/DCDmllm/HealthGPT) | N/A |
| [HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/abs/2502.12574) | beidic, junjiehu, jinqixiao, ZefanCai, wdlctc |  - HEADINFER is a novel inference framework that significantly reduces the GPU memory footprint required for large language model (LLM) inference by employing a head-wise offloading strategy.  - It offloads the key-value (KV) cache to the CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU.  - This is achieved by maintaining only selective attention heads' KV cache on the GPU and computing attention output dynamically.  - Experimental results demonstrate that HEADINFER achieves a 92% reduction in GPU memory footprint and enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory.  - The approach is evaluated on various LLMs (Llama, Qwen, Mistral, and Gemma) and outperforms existing methods in terms of context length extension. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/wdlctc/headinfer) | [Link](None) |
| [Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2502.10708) | Mingzhe Li, Miao Fang, Yuhan Liu, Bin Yan, Ziruibest | - This paper provides a comprehensive survey of methods for injecting domain-specific knowledge into large language models (LLMs). - The authors categorize these methods into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. - Each approach is discussed in detail, highlighting its advantages, disadvantages, and trade-offs. - The authors also compare domain-specific LLMs against general-purpose LLMs and discuss the challenges and opportunities in this field. - Finally, they provide a summary of commonly used datasets and benchmarks for researchers interested in delving deeper into this area. | ['Natural Language Processing'] | [Link](https://github.com/abilliyb/Knowledge_Injection_Survey_Papers) | N/A |
| [Eager Updates For Overlapped Communication and Computation in DiLoCo](https://arxiv.org/abs/2502.12996) | Yanislav Donchev, Arthur Douillard, Satyen Kale | - This paper introduces "eager updates," a technique to improve the efficiency of distributed training for large language models (LLMs) using the DiLoCo method. - Eager updates mitigate communication bottlenecks by overlapping the communication of outer gradients with the computation of the next inner optimization phase. - A naive implementation of this approach leads to reduced convergence speed. Thus, the authors propose an "eager" version that utilizes local outer gradients as a proxy during the overlap, improving convergence compared to the naive version. - Experiments demonstrate that eager updates reduce bandwidth requirements significantly, approaching 95% compute utilization for 1B, 10B, and 100B parameter models with bandwidths between 1 and 5 Gbit/s. - Evaluating on C4 and HellaSwag datasets reveals competitive performance compared to standard DiLoCo and significant improvements over naive delayed outer gradient updates, particularly under low bandwidth conditions. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Atom of Thoughts for Markov LLM Test-Time Scaling](https://arxiv.org/abs/2502.12018) | Chenglin Wu, Jiayi Zhang, Quan Shi, Zhaoyang Yu, leavendough | - This paper introduces Atom of Thoughts (AOT), a novel reasoning framework designed to enhance the test-time scaling capabilities of Large Language Models (LLMs). - AOT employs a Markov-style reasoning process, decomposing complex questions into a dependency-based directed acyclic graph (DAG) and contracting subquestions into independent atomic question states. - This iterative decomposition-contraction process simplifies the reasoning process, eliminating the need for maintaining extensive historical information, and thus improving computational efficiency. - AOT can be used as a standalone framework or integrated with existing test-time scaling methods as a plug-in enhancement. - Experimental results across six benchmarks demonstrate that AOT significantly improves LLM performance, notably achieving an 80.6% F1 score on HotpotQA with gpt-4-o-mini, surpassing 03-mini by 3.4% and DeepSeek-R1 by 10.6%. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/qixucen/atom) | N/A |
| [FinMTEB: Finance Massive Text Embedding Benchmark](https://arxiv.org/abs/2502.10990) | Yi Yang, yixuantt | - This paper introduces FinMTEB, a comprehensive benchmark designed for evaluating financial text embedding models. - FinMTEB consists of 64 datasets across seven tasks, including semantic textual similarity, retrieval, classification, clustering, reranking, pair classification, and summarization, covering both Chinese and English financial texts. - The authors also develop Fin-E5, a finance-adapted embedding model based on e5-Mistral-7B-Instruct, trained using a persona-based data synthesis method to cover diverse financial embedding tasks. - Experimental results demonstrate that domain-adapted models like Fin-E5 outperform general-purpose models, achieving state-of-the-art performance on FinMTEB. - Surprisingly, a simple Bag-of-Words (BoW) approach surpasses sophisticated dense embeddings in financial Semantic Textual Similarity tasks, indicating limitations in current dense embedding techniques for handling complex financial texts. | ['Natural Language Processing', 'Feature Extraction', 'Sentence Similarity'] | [Link](https://github.com/yixuantt/FinMTEB) | N/A |
| [Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research](https://arxiv.org/abs/2502.12669) | Shuyan Chen, wenxinsiju, yongqi2023, sunpenglei, Dominic789654 | - This research introduces a comprehensive knowledge-enhanced system specifically designed for perovskite solar cell research, improving researchers’ access to relevant information and reasoning abilities. - The system comprises a domain-specific knowledge graph (Perovskite-KG) containing 23,789 entities and 22,272 relationships extracted from 1,517 research papers; two specialized datasets (Perovskite-Chat with 55,101 question-answer pairs and Perovskite-Reasoning with 2,217 materials science problems); and two specialized large language models (Perovskite-Chat-LLM and Perovskite-Reasoning-LLM). - Experimental results demonstrate that the system outperforms current models in both domain-specific knowledge retrieval and scientific reasoning tasks. - Perovskite-Chat-LLM achieved a Rouge-L score of 41.25 and an LLM-Judge score of 2.97 on the Perovskite QA dataset, surpassing other baseline models like LLaMA and GPT variants. - Perovskite-Reasoning-LLM achieved state-of-the-art performance on GPQA and Minerva benchmarks within the 7B model category, achieving a score of 43.95 and 44.49 respectively with high data efficiency. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages](https://arxiv.org/abs/2502.10852) | XU Han, Jianing Liu, Guixian Xu, Ziyin Zhang, Zeli Su | - This paper introduces XLM-SWCM, a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages by reusing weights between the encoder and decoder. - The model architecture consists of a pre-trained encoder (CINO, a variant of XLM-R) and a decoder with two layer types: NormalDecoderLayer (randomly initialized) and CustomDecoderLayer (initialized with encoder weights). -  A weight-sharing mechanism is introduced to initialize and reuse weights between the encoder and decoder, maximizing the utilization of pre-trained encoder features. -  XLM-SWCM is pre-trained on a combined corpus of Simplified Chinese news articles (THUCNews), minority language data (MC2), and machine-translated parallel sentences. -  Experimental results show that XLM-SWCM significantly outperforms baselines like mBART and MC2-LLaMA-13B in text summarization, reading comprehension, and machine translation tasks, even exceeding much larger models in cross-lingual transfer settings. | ['Natural Language Processing', 'Text Generation', 'Translation', 'Summarization', 'Text2Text Generation'] | N/A | N/A |


## Papers for 2025-02-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089) | Liang Zhao, Junyu Luo, Damai Dai, Huazuo Gao, Jingyang Yuan | - This paper introduces NSA, a novel sparse attention mechanism designed for efficient long-context modeling. - NSA employs a dynamic hierarchical sparse strategy that combines coarse-grained token compression with fine-grained token selection, preserving both global context and local precision. - The model's architecture is hardware-aligned, featuring arithmetic intensity-balanced algorithm design and implementation optimizations for modern hardware. - NSA enables end-to-end training, achieving comparable or superior performance to full attention models across various benchmarks while exhibiting substantial speedups on decoding, forward propagation, and backward propagation. - Experiments demonstrate that NSA maintains or exceeds full attention models' performance on general benchmarks, long-context tasks, and instruction-based reasoning, validating its efficiency throughout the model lifecycle. | ['Natural Language Processing'] | N/A | N/A |
| [ReLearn: Unlearning via Learning for Large Language Models](https://arxiv.org/abs/2502.11190) | Sendong Zhao, Liming Yang, Ningyuan Zhao, Haoming Xu, Ningyu | - This paper introduces ReLearn, a novel unlearning pipeline for large language models (LLMs) that uses data augmentation and positive optimization to remove unauthorized knowledge while preserving model capabilities. - ReLearn addresses the limitations of existing reverse optimization methods, which disrupt subsequent token prediction and degrade linguistic coherence.  - The proposed framework includes new metrics (Knowledge Forgetting Rate, Knowledge Retention Rate, and Linguistic Score) for a more comprehensive evaluation of unlearning performance. - Experimental results demonstrate that ReLearn effectively achieves targeted forgetting while maintaining high-quality outputs, outperforming existing methods in balancing forgetting and retention. - Mechanistic analysis reveals how ReLearn preserves coherent text generation, unlike reverse optimization methods that disrupt this crucial capability. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/zjunlp/unlearn) | N/A |
| [SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?](https://arxiv.org/abs/2502.12115) | Johannes Heidecke, Tejal Patwardhan, Michele Wang, Samuel Miserendino | - SWE-Lancer, a benchmark comprising 1,400 real-world freelance software engineering tasks from Upwork valued at $1 million, is introduced to evaluate the economic impact of AI in software development. - The benchmark includes two task types: individual contributor (IC) tasks graded by end-to-end tests, and managerial tasks evaluating proposal selection. - Unlike existing benchmarks, SWE-Lancer uses real-world payouts to reflect market-driven task difficulty, employs end-to-end tests created and validated by experienced software engineers, and encompasses full-stack engineering with diverse task categories. - Evaluation results reveal that state-of-the-art models, such as Claude 3.5 Sonnet, achieve limited success, solving 26.2% of IC SWE tasks and earning $208,050 in the Diamond set. - The open-sourced Diamond set and Docker image aim to facilitate research into automated software engineering, agent safety, and the economic impacts of automated coding models. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/openai/SWELancer-Benchmark) | N/A |
| [HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation](https://arxiv.org/abs/2502.12148) | Minghao Xu, Chenming Shang, Ye Tian, Ling Yang, comin | - HermesFlow is a novel framework designed to bridge the performance gap between multimodal understanding and generation tasks in Multimodal Large Language Models (MLLMs). - It leverages homologous data (image-caption pairs) to curate paired preference data for both understanding (caption generation and BERT similarity comparison) and generation (image generation and self-VQA scoring). - The framework employs Pair-DPO, a novel training strategy, to align multimodal understanding and generation using the curated homologous preference data. - Through self-play iterative optimization, HermesFlow enables continuous self-improvement of the MLLM without requiring external high-quality training data. - Experimental results demonstrate the superiority of HermesFlow over existing state-of-the-art models in narrowing the performance gap between understanding and generation, showing improvements in both qualitative image generation and quantitative metrics across multiple benchmarks. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://github.com/Gen-Verse/HermesFlow) | N/A |
| [I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models](https://arxiv.org/abs/2502.10458) | Runtao Liu, Hanrong Ye, Guocheng Qian, Kuan-Chieh Wang, Mifucius | - ThinkDiff, a novel alignment paradigm, empowers text-to-image diffusion models with multimodal in-context understanding and reasoning by integrating the strengths of vision-language models (VLMs). - Instead of directly aligning VLMs with diffusion decoders, ThinkDiff employs a proxy task that aligns VLMs with large language model (LLM) decoders using vision-language training on readily available image-caption datasets. - This proxy task is based on the shared encoder of LLMs and diffusion models creating a shared input feature space for LLM and diffusion decoders.  - This simplified approach effectively transfers the multimodal in-context reasoning abilities of VLMs to diffusion models without complex training procedures or specialized reasoning datasets. - Experiments show that ThinkDiff significantly improves the state-of-the-art accuracy from 19.2% to 46.3% on the CoBSAT benchmark for multimodal in-context reasoning generation with just 5 hours of training on 4 A100 GPUs. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training](https://arxiv.org/abs/2502.11196) | Jiacheng Sun, Hui Jin, Yunzhi Yao, Yixin Ou, Ningyu | This paper investigates the mechanism of new knowledge acquisition in LLMs using a knowledge circuit perspective. - It analyzes the evolution of knowledge circuits throughout continual pre-training, identifying three key findings: (1) new knowledge acquisition is influenced by its relevance to pre-existing knowledge, (2) knowledge circuit evolution exhibits a phase shift from formation to optimization, (3) the evolution follows a deep-to-shallow pattern. - The findings provide insights into the mechanisms of knowledge acquisition in LLMs and have implications for improving continual pre-training strategies. - The study uses three series of decoder-only LLMs (GPT-2, Llama, and Phi) and synthetic data for controlled experiments. - The analysis is performed from three perspectives: performance, topology, and components, using graph-theoretical metrics and mechanistic interpretability techniques. | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/DynamicKnowledgeCircuits) | N/A |
| [SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors](https://arxiv.org/abs/2502.11167) | Siqiao Huang, zcliang22, Bohan22 | - This paper introduces SURGE, a benchmark designed to evaluate the capabilities of Large Language Models (LLMs) as general-purpose surrogate code executors.  - SURGE covers diverse aspects such as competition-level programming, high-cost scientific computing, and formal mathematical proof verification. - An evaluation of various open-source and proprietary LLMs on SURGE reveals that existing LLMs perform moderately.  - A scaling law study with models of varying sizes and data of different scales also reveals that performance consistently improves with model size and training data. - The results suggest that while LLMs can predict execution outcomes for simple programs, they struggle with more complex, real-world scenarios and exhibit limitations in general-purpose surrogate execution. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Imbernoulli/SURGE) | N/A |
| [SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL](https://arxiv.org/abs/2502.11438) | Hwanhee Lee, Byeongjeong Kim, Ingeol Baek, Jimin Lee | - SAFE-SQL, a novel framework, enhances Text-to-SQL generation by creating and filtering self-augmented examples using LLMs. - It employs a structured filtering mechanism to select high-quality question-SQL pairs based on similarity metrics and reasoning path validity. - This approach eliminates the need for additional training and improves robustness, particularly in complex scenarios where traditional methods struggle. - SAFE-SQL surpasses previous zero-shot and few-shot methods, showing substantial performance gains in challenging cases. - The method dynamically adapts examples using schema-linked information for enhanced SQL generation in unseen scenarios. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [CRANE: Reasoning with constrained LLM generation](https://arxiv.org/abs/2502.09061) | Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Tarun Suresh, Debangshu Banerjee | - CRANE, a reasoning-augmented constrained decoding algorithm, is proposed to balance the correctness of constrained generation with the flexibility of unconstrained generation for LLMs. - The paper theoretically demonstrates that constraining LLM outputs to restrictive grammars reduces their reasoning capabilities by limiting the problem-solving complexity class accessible under constrained decoding to TC⁰. - It further shows that augmenting the grammar with additional rules for reasoning steps allows the LLM to retain expressivity while ensuring the final output adheres to the required format. - CRANE alternates between unconstrained generation for reasoning and constrained generation for producing structurally correct outputs, guided by delimiter symbols in the output grammar. - In experiments on GSM-symbolic and FOLIO datasets, CRANE outperforms constrained and unconstrained decoding baselines, showing up to a 10% improvement in accuracy on challenging symbolic reasoning tasks. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Intuitive physics understanding emerges from self-supervised pretraining on natural videos](https://arxiv.org/abs/2502.11831) | Laurent Najman, Adrien Bardes, Mahmoud Assran, Nicolas Ballas, Quentin Garrido | - This paper introduces V-JEPA, a video prediction model that learns an intuitive understanding of physics from self-supervised pretraining on natural videos. - The model uses a learned representation space to predict masked regions in videos, demonstrating an understanding of properties like object permanence and shape consistency. - V-JEPA outperforms state-of-the-art video prediction models trained in pixel space and multimodal large language models, achieving above-chance performance on intuitive physics benchmarks. - Ablation studies show that video prediction in a learned representation space is sufficient for acquiring an understanding of intuitive physics, challenging the idea of hardwired core knowledge. - The findings suggest that intuitive physics understanding can emerge from general-purpose learning mechanisms, without requiring task-specific training or hard-coded abstractions. | ['Video Classification', 'Multimodal'] | [Link](https://github.com/facebookresearch/jepa-intuitive-physics) | N/A |
| [Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest](https://arxiv.org/abs/2502.11275) | Jingbo Shang, Feng Yao, Zilong Wang, Letian Peng | - This paper introduces Cuckoo, a novel information extraction (IE) model that leverages the massive datasets used for training large language models (LLMs). - Cuckoo recasts next-token prediction into a token extraction framework, enabling efficient IE model training using existing LLM data. - The proposed model outperforms existing pre-trained IE models on various IE tasks, particularly in few-shot scenarios and instruction-following settings. - Cuckoo demonstrates strong adaptability by evolving with advancements in LLM data preparation, without needing additional manual annotation efforts. - The study reveals that Cuckoo exhibits in-context learning capabilities, similar to those observed in LLMs. | ['Natural Language Processing', 'Token Classification', 'Feature Extraction'] | [Link](https://github.com/KomeijiForce/Cuckoo) | N/A |
| [Dyve: Thinking Fast and Slow for Dynamic Process Verification](https://arxiv.org/abs/2502.11157) | Qiang Xu, Xiangyu Wen, Zhijian Xu, Zeju Li, Jianyuan1 | - This paper introduces Dyve, a dynamic process verifier that improves reasoning error detection in LLMs by incorporating fast and slow thinking. - Dyve uses a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation, LLM-as-a-Judge, and specialized reasoning models to generate high-quality supervision signals. - The model adaptively applies immediate token-level confirmation (System 1) for straightforward steps and comprehensive analysis (System 2) for complex ones. - Experimental results on ProcessBench and MATH datasets demonstrate that Dyve significantly outperforms existing process-based verifiers and enhances performance in Best-of-N settings. - Dyve's code, data, and model are publicly available. | ['Natural Language Processing'] | [Link](https://github.com/staymylove/Dyve) | N/A |
| [PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning](https://arxiv.org/abs/2502.12054) | Jiaxing Huang, Yanrui Wu, Yuxuan Dong, Xinyu Zhang, ChengyouJia | - This paper introduces PhysReason, a new benchmark for evaluating physics-based reasoning in large language models (LLMs). - PhysReason contains 1200 problems, categorized into knowledge-based and reasoning-based problems with varying difficulty levels. - The benchmark incorporates multi-modal data (text and diagrams), requiring an average of 8.1 solution steps per problem. - A novel evaluation framework, Physics Solution Auto Scoring (PSAS), is proposed to assess both answer and step-level accuracy. - Experimental results on various LLMs reveal that existing models struggle with complex physics reasoning, particularly on hard problems. | ['Question Answering'] | [Link](https://dxzxy12138.github.io/PhysReason/) | N/A |
| [System Message Generation for User Preferences using Open-Source Models](https://arxiv.org/abs/2502.11330) | Teakgyu Hong, Dawoon Jung, Minsoo Khang, Jungho Cho, Minbyul Jeong | - This paper introduces SYSGEN, a novel pipeline for generating system messages tailored to user preferences using open-source language models. - SYSGEN addresses the limitations of existing datasets by automatically generating diverse system messages, thus avoiding license constraints and manual labeling costs. - Experiments across various open-source models demonstrate that training on SYSGEN data significantly improves the alignment of model responses with system messages and user instructions. - The proposed method achieves substantial improvements on the Multifacet benchmark while maintaining performance on other benchmarks like the Open LLM Leaderboard. - Qualitative analysis highlights the importance of diverse system messages for better adaptability across different contexts. | ['Natural Language Processing'] | N/A | N/A |
| [Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity](https://arxiv.org/abs/2502.11901) | Tianran Sun, Justin Wang, Dylan Zhang | - This paper introduces PoPilot, a 14B parameter language model fine-tuned for proof-oriented programming in the F* language, outperforming GPT-40 by 64% under data scarcity. - PoPilot addresses data scarcity through synthetic data augmentation, generating basic proof-oriented programming problems and incorporating diverse coding data for reasoning capability elicitation. - The model effectively synthesizes and repairs proofs at both the function and repository levels. - Experiments demonstrate that PoPilot significantly improves upon existing models in project-level proof-oriented programming, achieving a 64% relative margin over GPT-40 and a 54% improvement when repairing GPT-40's outputs. - The research contributes a novel data-centric post-training recipe for enhancing LLMs' performance in proof-oriented programming, particularly addressing the challenges posed by data scarcity. | ['Natural Language Processing'] | N/A | N/A |
| [Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems](https://arxiv.org/abs/2502.11098) | Shingo Takamatsu, Briti Gangopadhyay, Wei-Yao Wang, Sota Moriyama, Zhao Wang | - TalkHier is a novel collaborative LLM-MA framework that uses a structured communication protocol and hierarchical refinement system to improve communication and refinement process during collaboration on complex tasks. - The structured communication protocol organizes agent communication by incorporating messages, intermediate outputs, and background information, and hierarchical refinement allows agents to act hierarchically which mitigates biases and balances opinions or feedback. - TalkHier surpasses state-of-the-art performance on various tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation, as demonstrated by achieving 88.38% accuracy on MMLU benchmark and exceeding the best baseline on WikiQA by +5.32% ROUGE-1 and +3.30% BERTScore.  - Experimental results across different benchmarks and ablation studies further validate the effectiveness of the framework's key components. - TalkHier's improved performance makes it a suitable framework for more efficient, adaptable, and collaborative multi-agent frameworks | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/sony/talkhier) | N/A |
| [One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs](https://arxiv.org/abs/2502.10454) | Xinnian Liang, Zhikun Xu, Haojing Huang, Jiayi Kuang, Yinghui Li | - This paper introduces COUNTERMATH, a new counterexample-based mathematical reasoning benchmark designed to evaluate the conceptual reasoning abilities of Large Language Models (LLMs) in advanced mathematics. - COUNTERMATH consists of 1,216 statement-rationale pairs extracted from university-level math textbooks, focusing on disproving statements under specific conditions using counterexamples. - Experimental results demonstrate that current LLMs, including OpenAI models, exhibit limited performance on COUNTERMATH, indicating a need for improved higher-level mathematical conceptual reasoning. - A fine-tuned model trained on a small dataset (1,025 samples) of counterexample-based proofs outperforms baseline models, showcasing the effectiveness of incorporating counterexample reasoning in LLMs for enhancing mathematical proficiency. - The study's findings highlight the limitations of drill-based learning in LLMs and underscore the importance of focusing on conceptual understanding in mathematics education for both humans and machines. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Better Embeddings with Coupled Adam](https://arxiv.org/abs/2502.08441) | Tobias Stollenwerk, flxst | - This paper proposes Coupled Adam, a modification to the Adam optimizer, designed to improve the quality of word embeddings in Large Language Models (LLMs) by mitigating anisotropy. - The authors argue that the second moment in Adam is a root cause of anisotropic embeddings and Coupled Adam addresses this by averaging the second moment across the vocabulary. - Experiments demonstrate that Coupled Adam significantly enhances the quality of word embeddings and leads to improved upstream and downstream performance on sufficiently large datasets. - Coupled Adam shows improvements in isotropy, reduced mean embedding shift, and better condition numbers compared to standard Adam. - The paper provides both theoretical analysis and experimental validation to support the effectiveness of Coupled Adam. | ['Natural Language Processing'] | [Link](https://github.com/flxst/coupled-adam) | N/A |
| [Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking](https://arxiv.org/abs/2502.09083) | Isabelle Augenstein, Irina Shklovski, gretawarren | - This paper investigates fact-checkers' requirements for explainable automated fact-checking through interviews with 10 professionals from five continents. - The study identifies a gap between the current state of automated fact-checking and the practical needs of fact-checkers, particularly in how explanations should align with their decision-making process. - The findings highlight unmet needs for explanations that trace the model's reasoning, reference specific evidence, and highlight uncertainty and information gaps. - Fact-checkers prioritize primary sources, nuanced verdicts, and replicable processes, which contrasts with the current focus on secondary sources, binary verdicts, and veracity prediction in automated fact-checking research. - The paper offers design recommendations for automated fact-checking tools, emphasizing the importance of primary sources, nuanced verdicts, and replicable processes. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Large Language Models and Mathematical Reasoning Failures](https://arxiv.org/abs/2502.11574) | birgermoell, jboye | - This research paper evaluates the mathematical reasoning abilities of eight large language models (LLMs), including Mixtral, Llama, Gemini, GPT-40, and OpenAI's o1 variants, using 50 newly designed high-school-level word problems. - Unlike previous studies that focused only on answer correctness, this study analyzes both final answers and solution steps to pinpoint reasoning failures in LLMs. - While newer models demonstrate improved accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through faulty logic. - Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. - The study stresses the importance of evaluating reasoning processes in addition to answers, highlighting the need for targeted enhancements in structured reasoning and constraint handling for LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/jboye12/llm-probs) | N/A |
| [Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance](https://arxiv.org/abs/2502.11578) | jboye, birgermoell | - This paper investigates the use of language complexity metrics, LIX and Average Dependency Distance (ADD), as noisy zero-shot proxies for evaluating the performance of Large Language Models (LLMs). - Using Swedish high school and university essays, the study evaluates six LLMs: Gemini-1.5-Pro, Gemini-2.0-flash, Llama 70b, Llama70b 3.3, GPT40-mini, and ChatGPT-01-mini. - The results show that ChatGPT-01-mini performs most consistently across LIX and ADD computation and that there's a significant negative correlation between LIX computation accuracy and overall performance on the MMLU benchmark (r=-0.875, p=0.026). - This suggests that a model's ability to measure language complexity can indicate its general capabilities, potentially offering a simpler evaluation method. - The paper notes limitations, including the single-run nature of the evaluations, the scope being limited to Swedish text, the reliance on proprietary models, and the dependence on tokenization processes. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-02-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models](https://arxiv.org/abs/2502.09696) | Samuel Roberts, Akash Gupta, Ansh Sharma, Mohammad Reza Taesiri, Jonathan Roberts | - Introduces ZeroBench, a challenging visual reasoning benchmark designed to be impossible for current large multimodal models (LMMs). - Contains 100 manually created questions and 334 subquestions, focusing on complex multi-step reasoning with diverse natural and synthetic images. - Evaluates 20 prominent LMMs, including those with test-time compute scaling, all achieving 0% accuracy on the primary questions. - Error analysis reveals that models struggle primarily with visual interpretation rather than logical reasoning. - Public release of ZeroBench aims to encourage progress in visual understanding by providing an extremely difficult benchmark with maximum headroom. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) | Jingyang Ou, Xiaolu Zhang, Zebin You, Fengqi Zhu, Shen Nie | - This paper introduces LLaDA, a large language diffusion model trained from scratch using a masked diffusion approach, challenging the dominance of autoregressive models in natural language processing. - LLaDA uses a vanilla Transformer architecture to predict masked tokens, optimizing a likelihood bound for probabilistic inference, enabling bidirectional dependencies. - Across various benchmarks, LLaDA 8B demonstrates strong scalability and competes with LLaMA3 8B in in-context learning and instruction-following after supervised fine-tuning. - LLaDA addresses the reversal curse, outperforming GPT-4 in a reversal poem completion task, showing its ability for bidirectional context processing. - These findings establish diffusion models as a viable alternative to autoregressive models for LLMs, suggesting that key LLM capabilities aren't inherently tied to autoregression. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391) | Peiyan Li, Chaoyou Fu, Haochen Tian, Tao Yu, Yi-Fan Zhang | - Introduces MM-RLHF, a 120k human-annotated multimodal dataset designed for aligning large language models with human preferences, focusing on improved quality, diversity, granularity, and interpretability over existing datasets. - Proposes a Critique-Based Reward Model that generates critiques of model outputs before scoring, enabling fine-grained evaluation and outperforming other 7B-scale models on various reward model benchmarks. - Presents Dynamic Reward Scaling, which optimizes the Direct Preference Optimization (DPO) framework by adjusting sample loss weights according to reward signal strength. - Evaluates on 27 benchmarks across 10 dimensions, demonstrating substantial improvements – 19.5% increase in conversational abilities and 60% enhancement in safety for LLaVA-ov-7B when fine-tuned with MM-RLHF. - Argues that well-designed alignment using MM-RLHF and the proposed methods comprehensively boosts MLLM capabilities in various tasks, including visual perception, reasoning, dialogue, and trustworthiness. | ['Multimodal', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data), [Link](https://huggingface.co/datasets/Yirany/UniMM-Chat), [Link](https://huggingface.co/openai/clip-vit-base-patch32) |
| [Precise Parameter Localization for Textual Generation in Diffusion Models](https://arxiv.org/abs/2502.09935) | Adam Dziedzic, Kamil Deja, Franziska Boenisch, Bartosz Cywiński, Łukasz Staniszewski | - This paper introduces a method for localizing a small subset of cross and joint attention layers within diffusion models that are responsible for generating text within images.  - The method involves "patching" the activations of these layers with the activations generated from a target prompt, which allows for precise modification of the generated text without affecting other visual attributes.  - Through experiments on Stable Diffusion XL, DeepFloyd IF, and Stable Diffusion 3, it demonstrates that less than 1% of the model parameters are responsible for text generation, and that the localized layers are highly specialized to textual content.  -  Fine-tuning only these localized layers with LoRA improves the quality of the generated text without affecting the overall generation capabilities and diversity.  - The localization method is also applied to text editing within synthetic images and prevention of toxic text generation, outperforming prior techniques. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [Diverse Inference and Verification for Advanced Reasoning](https://arxiv.org/abs/2502.09955) | Yuke Zhang, Seunghwan Hyun, Mao Mao, Gaston Longhitano, Iddo Drori | - This research introduces a diverse inference approach that combines multiple models and methods during test time to tackle advanced reasoning tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. - The approach utilizes perfect verifiers (Lean for IMO and code execution for ARC) and imperfect verifiers (best-of-N for HLE) to increase accuracy and validate solutions. - Test-time simulations and reinforcement learning generate problem-specific information, improving generalization by adapting agent graph representations and varying prompts, code, and datasets. - Meta-learning is employed to trace pipeline runs, generate A/B tests, and adaptively modify the agent graph structure. - The proposed approach increases accuracy on IMO combinatorics problems to 77.8%, HLE question accuracy to 37%, solves 80% of human-failed ARC puzzles, and 26.5% of puzzles unsolved by high-compute OpenAI models. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [We Can't Understand AI Using our Existing Vocabulary](https://arxiv.org/abs/2502.07586) | Been Kim, Robert Geirhos, John Hewitt | - This paper introduces a new framework for understanding and controlling AI systems, arguing that progress in interpretability can be improved by defining and using neologisms (new words) corresponding to human concepts that we want to teach machines or machine concepts that we need to learn. - The paper posits that humans and machines conceptualize the world differently, leading to mismatches and a communication problem where bridging these differences is best achieved by forming new words. - It proposes "neologism embedding learning" as a proof of concept, where new word embeddings are learned via preference-based losses to understand and control model behavior; experiments include a length neologism to control LLM response length, and a diversity neologism to control the variability of responses. - Results show that using a "length neologism" enables controlling LLM response length to within a specified range more effectively than baseline prompting techniques. - Another experiment demonstrates that utilizing a "diversity neologism" promotes response diversity, aiding in tasks requiring exploration of variations, such as guessing a hidden number. | ['Natural Language Processing'] | N/A | N/A |
| [FoNE: Precise Single-Token Number Embeddings via Fourier Features](https://arxiv.org/abs/2502.09741) | Vatsal Sharan, Robin Jia, Mahdi Soltanolkotabi, Deqing Fu, Tianyi Zhou | - This research proposes **Fourier Number Embedding (FoNE)**, a novel method designed to enhance the representation and processing of numerical data within Large Language Models (LLMs). - FoNE directly maps numbers to their Fourier representations, encoding each digit using sine and cosine functions with different periods. This approach facilitates precise representation and efficient handling of numbers within LLMs. - On a 6-digit decimal addition task, FoNE achieves 99% accuracy with 64 times less training data than standard tokenization methods, while using significantly fewer tokens. Moreover, it reaches perfect accuracy with a larger training set. - FoNE not only accelerates training and inference but also facilitates perfect accuracy on various numerical tasks, including subtraction and multiplication, outperforming existing methods. - The method's efficiency and precision address a fundamental limitation in current LLMs, paving the way for improved performance on number-related tasks and broader applications in fields involving numerical reasoning. | ['Natural Language Processing'] | N/A | N/A |
| [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638) | Bijan Varjavand, Robert Vacareanu, Vaughn Robinson, Jeremy Kritz, ZifanScale | - This paper introduces a novel approach to red teaming Large Language Models (LLMs) called "jailbreaking-to-jailbreak" (J2), where a human jailbreaks an LLM (J2 attacker) to make it willing to jailbreak itself or other LLMs. - J2 attackers can systematically evaluate target models using various red teaming strategies and improve their performance via in-context learning from previous failures. - Experiments demonstrate that Sonnet-3.5 and Gemini-1.5-pro outperform other LLMs as J2 attackers, achieving high attack success rates against GPT-40 and other capable LLMs on Harmbench. - This approach not only offers a scalable way for strategic red teaming but also highlights an overlooked failure mode of LLM safeguards: LLMs can bypass their own safeguards by employing a jailbroken version of themselves. - The methodology is publicly shared, while specific prompting details are kept private to prevent misuse. | ['Natural Language Processing'] | N/A | N/A |
| [STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning](https://arxiv.org/abs/2502.10177) | Shuguang Cui, Zhixin Mai, Ge Wang, Yiming Zhao, Mingcong Lei | - Introduces Spatio-Temporal Memory Agent (STMA), a novel framework enhancing embodied task planning and decision-making through integrated spatio-temporal memory, dynamic knowledge graph, and planner-critic mechanism. - STMA leverages a dynamic Knowledge Graph (KG) for spatial memory, updating in real-time to reflect environmental changes for improved spatial reasoning and adaptability in complex scenarios.  - Employs a planner-critic closed-loop architecture combining proactive multi-step planning with real-time feedback for robust decision-making in dynamic environments. - Achieves 31.25% higher success rate and 24.7% higher average score compared to state-of-the-art models in the TextWorld environment, demonstrating effectiveness in long-horizon tasks.  - Demonstrates competitive performance using open-source models without fine-tuning, emphasizing the efficacy of the spatio-temporal memory design in conjunction with a planner-critic framework. | ['Reinforcement Learning', 'Robotics', 'Natural Language Processing'] | N/A | N/A |
| [V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models](https://arxiv.org/abs/2502.09980) | Yu-Chiang Frank Wang, Stephen F. Smith, Chien-Yi Wang, Ryo Hachiuma, Hsu-kuang Chiu | - This paper introduces V2V-LLM, a new multimodal large language model (MLLM) for cooperative autonomous driving that fuses perception information from multiple connected autonomous vehicles (CAVs) to improve driving safety. - V2V-LLM uses a centralized LLM computing node that receives individual perception features (scene-level feature maps and object-level feature vectors) from each CAV, answers driving related questions in natural language based on fused information. -  A new dataset and benchmark called Vehicle-to-Vehicle Question-Answering (V2V-QA) is created based on V2V4Real dataset to support the development and evaluation of LLM-based cooperative autonomous driving. - V2V-QA includes question-answer pairs about grounding, notable object identification, and planning. - Experimental results show that V2V-LLM outperforms other baseline fusion methods, especially in the notable object identification and planning tasks, important for overall driving safety. | ['Multimodal', 'Visual Question Answering', 'Object Detection', 'Robotics'] | N/A | N/A |
| [Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model](https://arxiv.org/abs/2502.10173) | Markus J. Buehler, Bo Ni | - This paper introduces VibeGen, a generative AI framework for end-to-end de novo protein design conditioned on normal mode vibrations, enabling the design of proteins with tailored dynamic properties. - VibeGen employs a dual-model architecture with a protein designer (PD) generating sequence candidates based on specified vibrational modes and a protein predictor (PP) evaluating their dynamic accuracy. - The designed proteins accurately reproduce prescribed normal mode amplitudes while adopting diverse stable, functionally relevant structures, often de novo, thereby expanding the protein space beyond evolutionary constraints. - The framework establishes a bidirectional link between sequence and vibrational behavior, which holds implications for designing flexible enzymes, dynamic scaffolds, and biomaterials. - The two-player framework outperforms single-model approaches by collaboratively boosting the strength of end-to-end models, improving accuracy and diversity of protein designs. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/lamm-mit/ModeShapeDiffusionDesign) | [Link](https://huggingface.co/lamm-mit/VibeGen) |


## Papers for 2025-02-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](https://arxiv.org/abs/2502.08910) | Sung Ju Hwang, Losif63, geonp, gmlwns5176 | - InfiniteHiP is a novel framework for long-context Language Model (LLM) inference that extends context length up to 3 million tokens on a single 48GB GPU by dynamically pruning irrelevant context and offloading key-value cache to host memory. - It employs a modular, hierarchical token pruning algorithm to accelerate processing by discarding less relevant tokens and utilizes various Rotary Positional Embeddings (RoPE) adjustments to generalize to sequences longer than the LLM's training length. - InfiniteHiP achieves an 18.95x speedup on attention decoding with a 1-million token context on a single GPU without requiring any additional training compared to standard attention. - Evaluation on LongBench and ∞Bench benchmarks shows improved performance over state-of-the-art efficient long-context methods such as InfLLM, especially with longer contexts and on short-context LLMs tested on extended contexts. - The framework is implemented within the SGLang LLM serving framework, demonstrating its practicality for real-world use cases, and uses a specialized LRU-based cache policy. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Summarization'] | N/A | N/A |
| [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/abs/2502.09604) | Hu Xu, Shannon Zejiang Shen, ZhaofengWu, bencw, voidism | - SelfCite, a novel self-supervised framework, enhances the quality of citations in large language models (LLMs) without human annotations. - It leverages a reward system based on context ablation, calculating necessity (probability drop when cited text is removed) and sufficiency (probability hold when only cited text is retained) scores. - SelfCite employs best-of-N sampling and preference optimization with SimPO for improved citation quality, boosting F1 scores by up to 5.3 points on LongBench-Cite across various long-form question answering tasks, surpassing previous state-of-the-art and proprietary model prompting. - The method excels in producing shorter yet more precise citations, addressing the challenge of hallucination in LLMs by linking generated text to specific evidence. - Length balancing is applied during training to prevent models from simply generating longer citations as a shortcut, and further iterative SimPO training has been studied with improved performance. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/voidism/SelfCite) | N/A |
| [Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620) | delinqu, Tavish9, zhuhaow, Purple1288, IvanTang | - This paper introduces ENEL, an encoder-free 3D Large Multimodal Model (LMM) that addresses the limitations of encoder-based 3D LMMs, such as fixed point cloud resolution and mismatched semantic embedding with Large Language Models (LLMs). - ENEL utilizes an LLM-embedded Semantic Encoding strategy with a Hybrid Semantic Loss during pre-training to enable the LLM to learn high-level 3D semantics directly from point cloud tokens. - It employs a Hierarchical Geometry Aggregation strategy during instruction tuning to enhance the LLM's perception of 3D geometric structures. - The 7B ENEL model achieves state-of-the-art performance on the Objaverse 3D captioning benchmark with a GPT-4 score of 50.92% and competitive results on classification and VQA tasks, rivaling or exceeding the performance of larger, encoder-based models like ShapeLLM-13B. - This demonstrates the potential of encoder-free architectures for efficient and scalable 3D LMMs. | ['Multimodal', 'Text-to-3D', 'Text-to-Image', 'Computer Vision'] | [Link](https://github.com/Ivan-Tang-3D/ENEL) | N/A |
| [An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging](https://arxiv.org/abs/2502.09056) | Kasima Tharnpipitchai, Potsawee Manakul, Kunat Pipatanakul, pittawat | - This paper introduces a method for enhancing the reasoning capabilities of language-specific Large Language Models (LLMs) by merging them with a reasoning-focused LLM (DeepSeek R1 70B Distill). - The method involves a two-stage process: (1) representation alignment via supervised fine-tuning (SFT) on a combined dataset of translated reasoning traces and general instructions; (2) ability-aware model merging, where the merging ratio is optimized based on layer depth, giving higher weight to DeepSeek R1 in earlier layers and to the language-specific LLM in later layers. - Applying this method to a 70B Thai LLM (Typhoon2 70B Instruct) with Deepseek R1 70B and a budget of $120 demonstrates performance comparable to DeepSeek R1 on reasoning tasks while maintaining strong performance on Thai language tasks, with an average score of 76.5/100. - Experiments also validate that merging alone and sft alone performs worse than merging with sft. - Results show that merging model improved overall average metric by 41.6% over Typhoon2 70B Instruct and 12.8% over DeepSeek R1 70B Distill. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | [Link](https://huggingface.co/spaces/ThaiLLM-Leaderboard/leaderboard), [Link](https://huggingface.co/datasets/scb10x/ifeval-th), [Link](https://huggingface.co/datasets/HuggingFaceH4/aime_2024), [Link](https://huggingface.co/datasets/Suraponn/thai_instruction_sft), [Link](https://huggingface.co/datasets/LDJnr/Capybara), [Link](https://huggingface.co/datasets/ThaiLLM-Leaderboard/mt-bench-thai), [Link](https://huggingface.co/datasets/airesearch/WangchanThaiInstruct), [Link](https://huggingface.co/aisingapore/llama3.1-70b-cpt-sea-lionv3-instruct) |
| [Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights](https://arxiv.org/abs/2502.09619) | Yedid Hoshen, Or Nathan, Jonathan Kahana, Eliahu | - This paper introduces ProbeLog, a method for retrieving classification models that can recognize a target concept (e.g., "Dog") without access to model metadata or training data. - ProbeLog computes a descriptor for each output logit of a model by observing its responses on a fixed set of input probes and normalizing the response vector.  - It supports both logit-based retrieval ("find more logits like this") and zero-shot, text-based retrieval ("find all logits corresponding to dogs") using text alignment with probes.  - A collaborative filtering method is introduced to reduce the cost of encoding repositories by probing models with fewer samples and imputing missing data.  - Experiments show ProbeLog achieves high retrieval accuracy on real-world and fine-grained search tasks, outperforming model-level baselines and generalizing to real-world models from Hugging Face. | ['Zero-Shot Classification', 'Multimodal', 'Image Classification'] | N/A | N/A |
| [CoSER: Coordinating LLM-Based Persona Simulation of Established Roles](https://arxiv.org/abs/2502.09082) | Rui Xu, Xinfeng Yuan, Yifei Zhang, Heng Wang, Xintao Wang | - Introduces COSER, a dataset, model, and evaluation framework for role-playing language agents (RPLAs) focused on established characters from literature. - The COSER dataset includes nearly 30,000 authentic multi-character conversations from 771 books, along with character profiles, plot summaries, and internal thoughts. - CoSER 8B and CoSER 70B are open role-playing LLMs trained on the COSER dataset and based on LLaMA-3.1. - Given-circumstance acting (GCA) is used for training and evaluation, where LLMs simulate conversations with defined characters and contexts. - COSER 70B shows state-of-the-art performance on the InCharacter and LifeChoice benchmarks and achieves or surpasses GPT-40 on other RPLA benchmarks. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/allenai/tulu-3-sft-mixture/tree/main/data) |
| [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents](https://arxiv.org/abs/2502.09560) | Cheng Qian, Mark Zhao, Junyu Zhang, Rui Yang, Hanyang81 | - Introduces EMBODIEDBENCH, a comprehensive benchmark for evaluating vision-driven embodied agents powered by Multimodal Large Language Models (MLLMs). - The benchmark includes 1,128 testing tasks across four environments: EB-ALFRED, EB-Habitat, EB-Navigation, and EB-Manipulation, encompassing both high-level and low-level tasks. - Evaluates 13 leading MLLMs and finds that they excel in high-level tasks but struggle with low-level manipulation, with the best model, GPT-4, achieving only a 28.9% success rate on average. - Proposes a novel capability-oriented evaluation across six subsets: basic task solving, common sense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. - Demonstrates the importance of vision for embodied agents, particularly in low-level tasks, with performance significantly degrading when vision is removed. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [Logical Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2502.09100) | Chaoli Zhang, Mengru Ding, Hanmeng Liu, ruoxining, HarryFu | - This survey paper explores the advancements and challenges in integrating logical reasoning capabilities into Large Language Models (LLMs). - The paper categorizes logical reasoning into four paradigms: deductive, inductive, abductive, and analogical, analyzing existing benchmarks and evaluation methods for each. - Several state-of-the-art techniques for enhancing logical reasoning in LLMs are discussed, including data-centric approaches, model-centric approaches, external knowledge utilization, and neuro-symbolic methods. - The paper highlights unresolved tensions in the field, such as the trade-off between robustness and generalization, the balance between interpretability and performance, and the need for more rigorous evaluation metrics. - Future research directions are proposed, emphasizing the need for hybrid architectures, robust evaluation frameworks, and exploration of multimodal reasoning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency](https://arxiv.org/abs/2502.09621) | Yu Qi, Yanwei Li, Ziyu Guo, Renrui Zhang, CaraJ | - Introduces MME-CoT, a benchmark designed to evaluate Chain-of-Thought (CoT) reasoning in Large Multimodal Models (LMMs) across six diverse domains (math, science, OCR, logic, space-time, general scenes). - Proposes a novel evaluation suite encompassing three key aspects of CoT: quality (recall and precision of reasoning steps), robustness (impact of CoT on perception vs. reasoning tasks), and efficiency (relevance of generated content and effectiveness of reflection steps). - Leverages curated high-quality data with fine-grained annotations of key reasoning steps and image captions. - Through analysis of state-of-the-art LMMs, finds that reflection mechanisms enhance CoT quality, CoT can negatively impact performance on perception-heavy tasks, and existing LMMs exhibit inefficiencies in long CoT and reflection processes, like Kimi k1.5 outperforming GPT-4 in CoT quality. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [Typhoon T1: An Open Thai Reasoning Model](https://arxiv.org/abs/2502.09042) | Kunat Pipatanakul, Kasima Tharnpipitchai, Potsawee Manakul, pittawat | - This research introduces Typhoon T1, an open-source Thai reasoning model based on a supervised fine-tuning approach using synthetic data.  - The model outperforms standard fine-tuning and zero-shot prompting on several benchmarks including GSM8K, HumanEval+, and IFEval.  - The paper introduces a novel "structured thinking" format using XML tags to guide the model's reasoning process and shows that it improves performance.  - The authors release their datasets, data pipeline, training configurations, and model weights to support further research and development of reasoning models.  - Additional findings include the effects of data quantity and mixture on reasoning model performance. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [CoT-Valve: Length-Compressible Chain-of-Thought Tuning](https://arxiv.org/abs/2502.09601) | Xinchao Wang, Gongfan Fang, Runpeng Yu, Guangnian Wan, Xinyin Ma | - This paper introduces CoT-Valve, a novel method for controlling the length of Chain-of-Thought (CoT) reasoning paths generated by Large Language Models (LLMs).  - CoT-Valve identifies a direction in the parameter space that allows for increasing or decreasing the length of generated CoT, using LoRA to implement this controllable direction, enabling flexible length manipulation during inference. - It introduces MixChain, a dataset with paired long and short reasoning chains, used to refine the direction for precise tuning and progressively compress reasoning paths. - Experiments on various LLMs show CoT-Valve achieves better control and compression of CoT compared to prompt-based methods and other baselines.  - The results also indicate that shorter reasoning paths can sometimes outperform longer ones on simpler tasks, highlighting the potential of CoT-Valve for enhancing model efficiency. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/horseee/CoT-Valve) | N/A |
| [SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models](https://arxiv.org/abs/2502.09390) | Moshe Wasserblat, Gad Markovits, Moshe Berchansky, danf | - This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to enhance chain-of-thought reasoning in Large Language Models (LLMs). - SQuARE prompts LLMs to generate and answer a series of sub-questions before addressing the main query, fostering a more thorough exploration of various aspects of a topic. - Evaluations were conducted with Llama 3 (3B and 8B) and GPT-4o on TriviaQA, HotpotQA, and ASQA datasets. - SQuARE consistently outperforms traditional Chain-of-Thought (CoT) prompting and existing rephrase-and-respond methods, especially with smaller LLMs. - The results suggest that systematically decomposing queries through self-interrogation improves reasoning capabilities in LLMs. | ['Question Answering'] | [Link](https://github.com/IntelLabs/RAG-FiT/tree/square) | N/A |
| [mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data](https://arxiv.org/abs/2502.08468) | Ziliang Zhao, Yutao Zhu, Nan Yang, Liang Wang, Haon-Chen | - This paper introduces mmE5, a multimodal multilingual embedding model trained on synthetic data generated using an MLLM (Multimodal Large Language Model). - The synthetic data generation framework focuses on three criteria: broad scope (covering diverse tasks, modalities, and languages), robust cross-modal alignment (achieved via a deep thinking process within the MLLM), and high fidelity (using real images and a refinement process). - mmE5 achieves state-of-the-art zero-shot performance on the MMEB benchmark with significantly less data than previous methods and shows superior multilingual performance on the XTD benchmark. - mmE5 demonstrates strong generalization capabilities across different tasks (classification, visual question answering, retrieval) and modalities. - The authors also analyzed the scaling effect of the synthetic data size and other hyperparameters, such as LORA rank, training batch size, and temperature. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Visual Question Answering', 'Zero-Shot Classification'] | [Link](https://github.com/haon-chen/mmE5) | N/A |
| [The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding](https://arxiv.org/abs/2502.08946) | Shunchi Zhang, Tsz Ting Chung, Junjie Wu, Lemao Liu, Mo Yu | - This research paper introduces PHYSICO, a new benchmark designed to assess the depth of understanding in Large Language Models (LLMs), particularly in the realm of physical concepts. - PHYSICO employs a summative assessment approach, presenting tasks in both natural language and abstract grid formats to evaluate LLMs' comprehension beyond memorization. - Experimental results demonstrate that state-of-the-art LLMs, including GPT-40 and Gemini 2.0, significantly lag behind human performance on PHYSICO's high-level understanding subtasks. - This performance disparity between low-level (definition-based) and high-level (abstract reasoning) tasks provides quantitative evidence for the "stochastic parrot" phenomenon, where LLMs excel at mimicking patterns but struggle with genuine comprehension. - Further analysis reveals that the challenge stems from the intrinsic difficulty of understanding these concepts rather than unfamiliar format, as fine-tuning and in-context learning provide minimal benefit, indicating a need for deeper comprehension capabilities in LLMs. | ['Natural Language Processing', 'Question Answering', 'Multimodal'] | [Link](https://physico-benchmark.github.io) | N/A |


## Papers for 2025-02-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation](https://arxiv.org/abs/2502.07870) | Zhuobai Dong, Weiming Han, Jiawei Zhang, Dongxing Mao, Alex Jinpeng Wang | - This paper introduces TextAtlas5M, a large-scale dataset of 5 million text-image pairs designed for long-text image generation. - The dataset includes synthetic data with varying complexity levels and real-world images sourced from diverse domains like PowerPoint presentations and academic papers. - A new benchmark, TextAtlasEval consisting of 3000 human-improved samples is introduced for evaluating long-text generation, posing significant challenges to even state-of-the-art models like GPT40 with DallE-3.  - Evaluation results show that existing models struggle with generating long and dense text within images, highlighting the need for improved models and training datasets in this area.  - The dataset and benchmark aim to facilitate research in generating more complex and textually rich images. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models](https://arxiv.org/abs/2502.07346) | Lei Li, Conghui He, Hanxu Hu, Wenhao Zhu, ggdcr | - BenchMAX, a multilingual benchmark encompassing 10 tasks across 17 languages, evaluates six core capabilities of LLMs: instruction following, reasoning, code generation, long context modeling, tool use, and translation. - The benchmark emphasizes diversity in language families and script systems, addressing a broader range of advanced LLM capabilities compared to previous benchmarks. - A rigorous dataset construction pipeline incorporates machine translation, human post-editing by native speakers, and LLM-based selection of final versions, ensuring high quality and reliability. - Evaluations reveal varying effectiveness across languages for these advanced LLM capabilities, highlighting persistent performance gaps between English and other languages that are not always bridged by scaling model size. - A novel domain translation task derived from the dataset construction process poses further challenges for LLMs, requiring new evaluation metrics beyond traditional translation assessment. | ['Natural Language Processing', 'Translation', 'Question Answering', 'Text Generation'] | [Link](https://github.com/CONE-MT/BenchMAX.git) | [Link](https://huggingface.co/collections/LLAMAX/benchmax-674d7a815a57baf97b5539f4) |
| [WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation](https://arxiv.org/abs/2502.08047) | Mike Zheng Shou, Difei Gao, Henry Hengyuan Zhao | - WorldGUI, a novel GUI benchmark, is introduced, which evaluates GUI agents' ability to handle dynamic tasks with diverse initial states across various desktop applications. - GUI-Thinker, a comprehensive GUI agent framework, is proposed. It leverages critical thinking principles with modules for post-planning critique, pre-execution validation, and post-action evaluation. - GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI, demonstrating the efficacy of the critical-thinking approach. - WorldGUI spans 315 tasks across 10 popular software applications, including Microsoft Office suite, VSCode, and Adobe Acrobat, incorporating varying initial states via pre-actions to simulate real-world user interactions. - Experimental results highlight the effectiveness of GUI-Thinker's critical modules, particularly the Actor-Critic, and the challenges posed by dynamic GUI automation tasks, including the difficulty in accurately perceiving screen elements and generating correct action code. | ['Multimodal'] | N/A | N/A |
| [LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid](https://arxiv.org/abs/2502.07563) | Yu Cheng, Xiaoye Qu, Yiran Zhong, landisen, weigao266 | - LASP-2 is a new sequence parallelism (SP) method designed for linear attention and hybrid models, aiming to improve training efficiency by enhancing communication and computation parallelism. - LASP-2 rethinks the minimal communication design for linear attention in sequence parallelism by reorganizing the computation-communication workflow and employing a single AllGather collective communication operation on intermediate memory states independent of sequence length. - LASP-2H extends the principle to hybrid architectures, integrating linear and standard attention layers, to improve long-context capabilities. - Experimental results on a Linear-Llama3 model, a modified Llama3 with linear attention, show that LASP-2 achieves up to 36.6% throughput improvement over Ring Attention and 15.2% over LASP (LASP-1) with a sequence length of 2048K across 64 GPUs. - LASP-2 demonstrates efficient scaling with increasing sequence length and number of GPUs, maintaining constant memory usage per GPU while achieving higher throughput. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/OpenSparseLLMs/Linear-MoE) | N/A |
| [TransMLA: Multi-head Latent Attention Is All You Need](https://arxiv.org/abs/2502.07864) | Muhan Zhang, Zengwei Yao, fxmeng | - Introduces Multi-head Latent Attention (MLA), a new attention mechanism for Large Language Models (LLMs) that addresses communication bottlenecks by using low-rank matrices in key-value layers, enabling caching of compressed latent key-value (KV) states. - Shows that MLA offers greater expressive power than Group Query Attention (GQA) for the same KV cache overhead and introduces TransMLA, a post-training method to convert GQA-based models to MLA. - Demonstrates through experiments that TransMLA-converted models, after fine-tuning, outperform original GQA models on downstream tasks, particularly in math and coding. - Proposes further development of MLA-specific inference acceleration and DeepSeek R1 distillation to further optimize the converted models. - Claims that switching to MLA can reduce resource consumption and carbon emissions. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/fxmeng/TransMLA) | N/A |
| [Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance](https://arxiv.org/abs/2502.08127) | Yan Wang, Weipeng Zhou, Lingfei Qian, QianqianXie1994, jiminHuang | - This paper introduces Fino1, a new reasoning-enhanced large language model (LLM) specialized for financial tasks, built upon Llama-3.1-8B-Instruct and fine-tuned using Chain-of-Thought and Reinforcement Learning with domain-specific reasoning paths generated by GPT-40. - Fino1 demonstrates a consistent 10% performance improvement across diverse financial reasoning tasks, outperforming all other 8B models and even surpassing Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. - The paper analyzes the performance of 16 state-of-the-art LLMs across three financial datasets (FinQA, DM-Simplong, XBRL-Math), revealing that general reasoning enhancements do not consistently improve financial reasoning and model scaling does not guarantee better results in the finance domain. - The study concludes that domain-specific adaptations are essential for effective financial reasoning with LLMs, and it highlights key areas for future research, including multi-table reasoning, long-context processing, and domain-specific terminology comprehension. - All datasets and code used in the study are publicly available, and a leaderboard for future research is introduced. | ['Natural Language Processing', 'Question Answering', 'Table Question Answering'] | [Link](https://github.com/The-FinAI/Fino1) | [Link](https://huggingface.co/TheFinAI), [Link](https://huggingface.co/spaces/TheFinAI/open-finllm-reasoning-leaderboard) |
| [Distillation Scaling Laws](https://arxiv.org/abs/2502.08606) | Etai Littwin, Jason Ramapuram, Floris Weers, Amitis Shidani, Dan Busbridge | - This paper introduces a distillation scaling law that predicts the performance of a student language model based on the compute budget and its allocation between the student and teacher models. - The scaling law reveals that in certain cases, specifically when a teacher model already exists or will be used for multiple distillations, distillation outperforms supervised pretraining up to a certain compute threshold, which can be determined using the scaling law. - The study shows that compute allocation can be optimized for both teacher and student models to maximize student performance and provides insights into the capacity gap phenomenon, where overly capable teachers may hinder student performance. - The paper also explores compute-optimal distillation scenarios, offering recipes for scenarios like pre-trained teacher availability or cases where teacher training is required, and compares them to supervised learning scenarios. - The findings demonstrate that while supervised learning matches distillation at high compute budgets, distillation can be more efficient for specific cases, potentially aiding in building smaller, more performant language models. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation](https://arxiv.org/abs/2502.08168) | HaiPeng Wang, Peidong Wang, Sihao Dong, Xiayang Xiao, JimmyMa99 | - This paper introduces SARChat-2M, a large-scale, multi-task, vision-language benchmark for SAR (Synthetic Aperture Radar) image interpretation.  - SARChat-2M contains approximately 2 million image-text pairs and covers diverse scenarios with detailed target annotations, supporting tasks like visual understanding, object detection, and image captioning.  -  A comprehensive benchmark, SARChat-Bench, is established with six core tasks: classification, description, counting, localization, recognition, and referring.  - Evaluation of 16 mainstream VLMs demonstrates the effectiveness of the dataset and benchmark, revealing performance variations across different models and tasks.  -  This work aims to promote the development of SAR-oriented visual language models and provides insights into building datasets for other remote sensing domains. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Object Detection'] | [Link](https://github.com/JimmyMa99/SARChat) | N/A |
| [Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning](https://arxiv.org/abs/2502.06533) | lecraquito, Nbeau, supertardigrade | - This paper introduces a modified Kullback-Leibler (KL) penalty for reinforcement learning (RL) fine-tuning of language models, prioritizing exploration on critical tokens. - Critical tokens are identified as decisive points in the generation process where the pre-trained model exhibits high uncertainty and significantly impacts the final outcome. - The modified KL penalty weights the divergence based on the pre-trained model's confidence, encouraging exploration in uncertain areas while preserving learned capabilities. - Experiments on a simple arithmetic task demonstrate that this method enhances exploration efficiency and leads to better performance compared to standard RL fine-tuning. - The paper suggests that balancing exploration and exploitation, particularly on critical tokens, can significantly improve the effectiveness of RL for fine-tuning language models. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/jvasso/llm-rl-arithmetic) | N/A |
| [LLM Pretraining with Continuous Concepts](https://arxiv.org/abs/2502.08524) | Andrew Cohen, Jane Yu, Jack Lanchantin, Jihoon Tack, xlxxl | - CoCoMix, a novel pretraining framework for LLMs, combines discrete next-token prediction with continuous concepts learned from a pretrained sparse autoencoder. - CoCoMix predicts continuous concepts and mixes them into the model's hidden state by interleaving with token hidden representations. - Evaluations on multiple benchmarks show CoCoMix is more sample-efficient and outperforms standard next-token prediction and knowledge distillation, achieving comparable performance with 21.5% fewer training tokens on a 1.38B parameter model. - Combining concept learning and interleaving is crucial for the performance gains, and CoCoMix also enhances interpretability and steerability by allowing inspection and modification of predicted concepts. - CoCoMix demonstrates substantial improvements in weak-to-strong supervision scenarios, where concepts from a small model can supervise a larger model's training. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/facebookresearch/RAM/tree/main/projects/cocomix) | N/A |
| [NoLiMa: Long-Context Evaluation Beyond Literal Matching](https://arxiv.org/abs/2502.05167) | Ryan A. Rossi, Trung Bui, Hanieh Deilamsalehy, Franck-Dernoncourt, amodaresi | - This paper introduces NOLIMA, a new benchmark designed to evaluate the latent reasoning capabilities of Large Language Models (LLMs) in long-context scenarios. - Unlike existing benchmarks like NIAH which often rely on literal matches between questions and answers, NOLIMA minimizes such overlaps, pushing models to infer latent connections. - The benchmark features a needle set with minimal lexical overlap between question and needle. The question and needle keywords are related through associative links, such as real-world knowledge. - Evaluations on 12 LLMs reveal that performance degrades significantly with increasing context length. Most models perform well on short contexts but poorly on those larger than 2K tokens. For instance, 10 models score at below 50% of their short-context performance on 32K tokens. - The study conducts ablation analysis, chain-of-thought prompting and tests across different complexity levels in latent hops to understand LLM’s failures in generalizing over long contexts when literal matches are absent. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing](https://arxiv.org/abs/2502.04411) | Peijie Dong, Xinglin Pan, Zhenheng Tang, Kunfeng Lai, Dominic789654 | - Mediator, an adaptive model merging framework, enhances Large Language Model (LLM) merging by addressing parameter conflicts and minimizing system costs. - It leverages layer-wise analysis of parameter conflicts, averaging layers with minimal conflict and routing those with significant conflicts using a novel task-level expert routing mechanism. - To further reduce storage, Mediator decomposes fine-tuned experts into dense and sparse components, enabling dynamic expert selection based on input task uncertainty. - Experiments on LLaMA and Qwen across various scales and real-world reasoning tasks show consistent and significant performance gains over existing methods. - Mediator achieves comparable performance to a 7B x 4 LLM ensemble on a single RTX 4090 GPU, improving accessibility in resource-constrained environments. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2502.06872) | Yongjia Lei, Leyao Wang, Zheyuan Liu, Bo Ni, Franck-Dernoncourt | - This survey paper provides a comprehensive overview of Retrieval Augmented Generation (RAG) for Large Language Models (LLMs) through the lens of trustworthiness, focusing on six key areas: reliability, privacy, safety, fairness, explainability, and accountability. - The paper categorizes existing work, identifies key challenges, and offers potential solutions for building more trustworthy RAG systems. - The survey also emphasizes the importance of robust evaluation metrics and benchmarks to assess the efficacy of different approaches for trustworthy RAG. - The survey concludes with a discussion on applying trustworthy RAG in high-stakes domains like healthcare, law, and education, emphasizing the unique trustworthiness considerations and challenges in these areas. - Finally, it suggests promising research directions, including developing unified frameworks for retrieval and generation watermarking, dynamic watermarking for adaptive AI, and better integration of governance and ethical considerations. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Arstanley/Awesome-Trustworthy-Retrieval-Augmented-Generation) | N/A |
| [DPO-Shift: Shifting the Distribution of Direct Preference Optimization](https://arxiv.org/abs/2502.07599) | Xiao Li, Lei Zhao, Qianen Zhang, Feng Jiang, Xiliang Yang | - This paper introduces DPO-Shift, a novel technique to address the likelihood displacement issue in Direct Preference Optimization (DPO) for aligning language models with human preferences. - DPO-Shift modifies the DPO objective function by adding a parameter function \(f(x)\) applied to the reward of rejected responses, controllably shifting the chosen probability distribution, which is supported by theoretical analysis showing a trade-off between improved chosen probability and reduced reward margin. - Ablation studies on Llama 3-8B and Qwen 2-7B models trained on the UltraFeedback and Capybara-preferences datasets demonstrate that the method effectively mitigates likelihood displacement and controls the trade-off through different choices of \(f(x)\). - In downstream MT-Bench and win rate experiments, DPO-Shift consistently outperforms DPO.  - Overall, DPO-Shift is a simple yet effective method grounded in theory for improving DPO's performance through a controllable shift of the chosen probability distribution. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Meaquadddd/DPO-Shift) | N/A |
| [LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention](https://arxiv.org/abs/2502.08213) | kkolomeitsev | - This paper proposes LLM Modules, a novel architecture for transferring knowledge from a large pre-trained language model (e.g., Qwen2-1.5B) to a smaller one (e.g., GPT-Neo-125M) using an Enhanced Cross-Attention mechanism. - The architecture freezes the large model and uses its representations as external input to the smaller model, which is trained on limited resources.  - The Enhanced Cross-Attention mechanism consists of linear projections, an adapter block, and a gating mechanism to effectively integrate the knowledge from the large model into the smaller model. - Experimental results on the Bespoke-Stratos-17k dataset show that the combined model generates responses comparable in quality to distilled models after only 15 epochs of training and exhibits improved reasoning capabilities. - The modular approach offers advantages in terms of reduced computational costs and adaptability to specific tasks by training on smaller datasets. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/kkolomeitsev/ll) |
| [MetaSC: Test-Time Safety Specification Optimization for Language Models](https://arxiv.org/abs/2502.07985) | vicgalle | - MetaSC, a novel dynamic safety framework, optimizes language model safety reasoning at inference time without modifying model weights, by iteratively updating safety prompts (specifications) to adaptively drive the critique and revision process. - This test-time optimization enhances performance against adversarial jailbreak requests and in general safety tasks, such as avoiding moral harm or pursuing honest responses. - Evaluations demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. - The meta-critique mechanism is presented as a discrete optimization problem, searching over textual specifications using a meta-critic language model. - Experiments using diverse language models and the BiGGen benchmark show substantial improvements in safety across various tasks, including jailbreak defense and general safety criteria. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/vicgalle/meta-self-critique) | N/A |


## Papers for 2025-02-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Competitive Programming with Large Reasoning Models](https://arxiv.org/abs/2502.06807) | Borys Minaev, Andre Saraiva, Alexander Wei, Ahmed El-Kishky, OpenAI | - This research demonstrates that reinforcement learning significantly improves the performance of large language models (LLMs) on complex coding tasks, especially in competitive programming. - The study compares three OpenAI models: o1, a general-purpose reasoning model; o1-ioi, a specialized version fine-tuned for the International Olympiad in Informatics (IOI); and an early version of o3, a more advanced reasoning model. - o1-ioi achieved a gold medal at IOI 2024 using handcrafted test-time strategies, showcasing the benefit of specialized training and inference techniques. - Notably, o3 surpassed o1-ioi's performance without relying on specialized strategies, achieving a CODEFORCES rating comparable to elite human competitors. - This indicates that scaling general-purpose reinforcement learning is a promising direction for developing AI capable of state-of-the-art performance in reasoning domains. | ['Reinforcement Learning', 'Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316) | Yu Wu, Runxin Xu, Dejian Yang, Daya Guo, Junlong Li | - CODEI/O is a novel approach that enhances the reasoning capabilities of Large Language Models (LLMs) by transforming code into a code input-output prediction format and training the models to predict inputs or outputs given the code. - By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, they are exposed to universal reasoning primitives, decoupling structured reasoning from code-specific syntax while preserving procedural rigor. - The model is trained in two stages: a code input-output prediction training stage followed by a general instruction-tuning stage, which improves average scores across reasoning benchmarks including DROP, WinoGrande, GSM8K, MATH, MMLU-STEM, BBH, GPQA, CruxEval, ZebraGrid, KorBench, and LiveBench.  - CODEI/O outperforms other strong datasets like OpenMathInstruct2, OpenCoder-SFT-Stage1, WebInstruct, and PythonEdu.  -  Further improvements were observed with CODEI/O++ using multi-turn revisions based on execution feedback, boosting performance without trade-offs across models with parameter sizes ranging from 7B to 30B. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/hkust-nlp/CodeIO) | N/A |
| [Teaching Language Models to Critique via Reinforcement Learning](https://arxiv.org/abs/2502.03492) | Jingjing Xu, Weichao Mao, Liyu Chen, Jie chen, Zhihui | - CTRL, a novel framework, is introduced to train Large Language Model (LLM) critics for code generation and refinement through reinforcement learning. - The framework decouples the critic model from the task-performing model and trains it to generate feedback that maximizes the generator's correction performance without human supervision. - Evaluation on diverse benchmarks like CodeContests, LiveCodeBench, MBPP+, and JudgeBench demonstrates that critics trained with CTRL enhance pass rates and mitigate errors across various generator models, showing substantial improvements compared to self-critique methods and those using stronger critic models. - The critic demonstrates an effective weak-to-strong generalization capability by using weaker critic models to improve solutions generated by stronger LLM models such as GPT-40. - CTRL’s critic enables effective test-time scaling by providing targeted feedback and significantly reducing the number of required revision iterations, as shown by up to 106.1% relative improvements across challenging code generation benchmarks. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Expect the Unexpected: FailSafe Long Context QA for Finance](https://arxiv.org/abs/2502.06329) | Mateusz Russak, Dmytro Mozolevskyi, Melisa Russak, muayad, kiranr | - This paper introduces FailSafeQA, a new long-context financial benchmark designed to evaluate the robustness and context-awareness of Large Language Models (LLMs) in question-answering systems. - The benchmark focuses on two scenarios: Query Failure, where the input query is perturbed with spelling errors, incompleteness, and out-of-domain phrasing, and Context Failure, where the input document is degraded, irrelevant, or missing. - It employs an LLM-as-a-Judge methodology using Qwen2.5-72B-Instruct and evaluates 24 off-the-shelf LLMs based on Robustness, Context Grounding, and Compliance scores. - Initial findings reveal a trade-off between robustness and context grounding, with even high-performing models struggling to maintain accuracy under perturbed conditions; for instance, the most robust model, OpenAI 03-mini, fabricates information in 41% of cases. - FailSafeQA serves as a valuable tool for improving LLM dependability in financial applications, where reliable information processing is critical, especially in long-context scenarios. | ['Question Answering', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/Writer/FailSafeQA) |
| [Scaling Pre-training to One Hundred Billion Data for Vision Language Models](https://arxiv.org/abs/2502.07617) | Keran Rong, Zhe Li, Daniel Salz, Ibrahim Alabdulmohsin, Xiao Wang | - This paper investigates the impact of scaling pre-training data to 100 billion image-text pairs on vision-language models (VLMs). - While traditional benchmarks show performance saturation, significant gains are observed in cultural diversity tasks and low-resource language performance, demonstrating the importance of scale for building inclusive models. - Quality filtering, while improving performance on Western-centric tasks, is shown to negatively impact cultural diversity. - The paper introduces WebLI-100B, a novel dataset with 100 billion image-text pairs, and trains SigLIP models with varying sizes on different data scales. - It also shows that rebalancing low-resource languages in the dataset leads to improved performance on corresponding benchmarks and overall multilingual performance. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification', 'Computer Vision'] | N/A | N/A |
| [LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!](https://arxiv.org/abs/2502.07374) | Xiangxi Mo, Shu Liu, Tyler Griggs, Shiyi Cao, Dacheng Li | - This paper demonstrates that Large Language Models (LLMs) can effectively learn long chain-of-thought (Long CoT) reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). - Using only 17k training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on various math and coding benchmarks, including a 40% improvement on AIME 2024 and 8.1% on Live-CodeBench, making it competitive with proprietary models. - The structure of Long CoT reasoning is crucial for effective learning, while the specific content is less important; the model exhibits robustness to incorrect answers or perturbed numerical values in training, yet structural changes such as shuffling reasoning steps significantly impact accuracy. - Through controlled experiments modifying reasoning structures and contents, the model demonstrates its sensitivity to the logical flow of the reasoning process. - This research highlights the potential of distilling reasoning abilities in LLMs efficiently and emphasizes key factors for training future reasoning models. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/NovaSky-AI/SkyThought) | [Link](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k), [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-aime), [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-amc) |
| [Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents](https://arxiv.org/abs/2502.04223) | Lukas Voegtle, Ilia Karmanov, jseppanen, katerynaCh, amalad | - ÉCLAIR, a multimodal large language model (MLLM) with a ViT-like encoder and autoregressive decoder, extracts formatted text, bounding boxes with semantic classes, and reading order from documents. - It addresses limitations of existing models like Kosmos-2.5 and GOT by predicting spatial information, semantic classes, and handling complex layouts. - A novel data generation pipeline was created to construct arXiv-5M, a dataset with maximum-information labels, and the model is fine-tuned on diverse datasets like DocLayNet, SynthTabNet, and a human-labeled Common Crawl subset. - ÉCLAIR achieves state-of-the-art accuracy on the new benchmark DROBS and shows competitive results on existing benchmarks for general OCR, layout understanding, and LLM training data extraction. - Multi-token inference significantly improves ÉCLAIR's inference speed while maintaining or enhancing accuracy compared to single-token decoding. | ['Image-to-Text', 'Multimodal'] | N/A | N/A |
| [CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing](https://arxiv.org/abs/2502.03997) | Jiang Bian, Qi Liu, Yu Yuan, ShizhaoSun | - CAD-Editor is the first framework for text-based CAD editing, allowing users to modify existing CAD models using textual instructions. - It utilizes a sequence-to-sequence approach, representing both instructions and CAD models as text sequences. - To overcome the lack of training data, CAD-Editor employs an automated data synthesis pipeline, combining design variation models with Large Vision-Language Models (LVLMs). - It features a locate-then-infill framework, where Large Language Models (LLMs) first identify regions needing modification and then generate appropriate edits, effectively handling the composite nature of the task. - Experimental results demonstrate that CAD-Editor achieves superior performance over baselines in terms of generated CAD model validity, alignment with editing instructions, and overall quality. | ['Text2Text Generation', 'Multimodal'] | N/A | N/A |
| [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/abs/2502.07527) | Chuan Cao, Liang He, Shufang Xie, Peiran Jin, Yingce Xia | - NatureLM is a sequence-based science foundation model designed for scientific discovery, trained on 143 billion tokens from various scientific domains using a transformer decoder architecture. - NatureLM excels at generating and optimizing molecules, proteins, RNA, and materials using text instructions; performing cross-domain generation (e.g., protein-to-molecule); and achieving state-of-the-art performance on tasks like retrosynthesis and SMILES-to-IUPAC translation. - Evaluation across 22 task categories reveals that larger NatureLM models generally outperform smaller ones, with the 46.7 billion parameter 8x7B model showing the best performance in most cases. - NatureLM demonstrates the potential of large foundation models for scientific discovery by integrating knowledge from multiple domains. - The researchers plan to further enhance the model's language capabilities and few-shot learning skills in future iterations. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Translation'] | N/A | N/A |
| [Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training](https://arxiv.org/abs/2502.06589) | Kewei Cheng, Xin Liu, Haoming Jiang, Jingfeng Yang, yczhuang | - This paper introduces Hephaestus, a continual pre-trained large language model (LLM) designed to improve the fundamental agent capabilities of LLMs, including API function calling, intrinsic reasoning and planning, and adaptation to environmental feedback. - It also introduces Hephaestus-Forge, a large-scale pre-training corpus created to enhance the abilities of LLM agents by combining diverse data sources like tool documentation, function-calling trajectories, and code examples. - The training process includes two stages of continual pre-training followed by instruction fine-tuning. - The paper uses scaling law experiments to determine the optimal data mixing ratio among agent, text, and code data, finding a 1:1:1 ratio to be most effective. - The authors claim Hephaestus-8B outperforms open-source LLMs and rivals commercial LLMs across several agent benchmarks, suggesting effectiveness at enhancing fundamental agent capabilities and generalizability. | ['Natural Language Processing'] | N/A | N/A |
| [Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon](https://arxiv.org/abs/2502.07445) | Seffi Cohen, Lior Rokach, Bracha Shapira, Yehonatan Elisha, Nurit Cohen-Inger | - This paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework to assess the overfitting of Large Language Models (LLMs) to benchmark datasets. - C-BOD systematically distorts evaluation prompts while preserving semantic content and labels using a distortion parameter (µ), and compares model performance on original and perturbed versions to detect overfitting. - Evaluated on MMLU benchmark with 26 LLMs, C-BOD revealed average performance degradation of 2.15% under modest perturbation (μ = 1.0), with 20 of 26 models showing statistically significant performance drops. - Models with higher baseline accuracy and larger LLMs were more sensitive to rephrasing, suggesting overreliance on prompt patterns. - This method provides a dataset and model-agnostic tool to detect and mitigate overfitting for more robust language understanding evaluation. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/SeffiCohen/CBOD) | N/A |
| [FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks](https://arxiv.org/abs/2502.04465) | Mirco Ravanelli, Cem Subakan, Francesco Paissan, lucadellalib | - FocalCodec, a novel low-bitrate hybrid speech codec based on focal modulation, compresses speech into a single binary codebook using a compressor-quantizer-decompressor architecture. - It operates at ultra-low bitrates (0.16 to 0.65 kbps), outperforming current state-of-the-art models in terms of reconstruction quality, voice conversion, and multilingual and noisy speech handling. - Evaluation on various downstream tasks showed that FocalCodec preserves sufficient semantic and acoustic information while being suitable for generative modeling. - Notably, it achieved the lowest differential word error rate (dWER) in both clean and noisy speech resynthesis benchmarks, surpassing state-of-the-art models like BigCodec. - FocalCodec demonstrated effective disentanglement of content and speaker information for voice conversion even with a single codebook design. | ['Audio', 'Text-to-Speech', 'Audio-to-Audio'] | N/A | [Link](https://huggingface.co/microsoft/wavlm-base-sv), [Link](https://huggingface.co/openai/whisper-small) |
| [Auditing Prompt Caching in Language Model APIs](https://arxiv.org/abs/2502.07776) | Percy Liang, Rohith Kuditipudi, Xiang Lisa Li, Chenchen Gu, thashim | - This paper introduces a novel auditing technique to detect prompt caching in Large Language Model (LLM) APIs by analyzing data-dependent timing variations. - Cached prompts are processed faster (cache hits) than non-cached prompts (cache misses), creating timing differences exploitable by attackers to infer information about other users' prompts if the cache is shared. - The audit employs statistical hypothesis testing with procedures designed to generate cache hits and misses, comparing the resulting time distributions. - Audits conducted on 17 real-world LLM API providers, including OpenAI, revealed global cache sharing across users in 7 providers, raising privacy concerns. - The study also demonstrates the potential leakage of model architecture information, specifically finding evidence that OpenAI's text-embedding-3-small model has a decoder-only Transformer architecture. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/chenchenygu/auditing-prompt-caching) | N/A |
| [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490) | Li Shen, Zhenyu Zhang, Jianjin Li, Zhikai Jia, Xialie Zhuang | - This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a novel training paradigm for Large Language Models (LLMs) that integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP). - MEAP randomly masks a fraction of input tokens before applying standard next-token prediction using a decoder-only transformer, eliminating the need for bidirectional attention or encoder-decoder architectures. - Experimental results demonstrate that MEAP significantly outperforms NTP on key information retrieval and long-context reasoning tasks, showing a 33% improvement on Needle in a Haystack and up to 27.2 percentage points improvement on Multi-Document Question Answering. It also excels in the Lost-In-The-Middle Question Answering setting.  - Analysis suggests that MEAP's effectiveness comes from its ability to promote more distinguishable attention scores by concentrating on fewer tokens, leading to better focus on task-relevant signals. - MEAP retains the scaling efficiency of decoder-only LLMs and is compatible with existing LLM pipelines and hardware. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2025-02-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators](https://arxiv.org/abs/2502.06394) | Alexander Panchenko, tlenusik, memyprokotow, chameleon-lizard, etomoscow | - This paper introduces SynthDetoxM, a large-scale multilingual synthetic parallel dataset for text detoxification comprising 16,000 sentence pairs across German, French, Spanish, and Russian. - The dataset was created using few-shot prompting of nine different open-source large language models (LLMs) and a novel selection criteria based on toxicity and similarity metrics. - Experiments demonstrate that models trained on SynthDetoxM outperform those trained on the human-annotated MultiParaDetox dataset, even in data-limited settings and various LLMs in few-shot setting. - The authors also propose a framework for generating synthetic parallel multilingual detoxification data using few-shot prompting and quality filtering with LLMs. - The dataset and code are publicly released to facilitate further research in multilingual text detoxification. | ['Natural Language Processing', 'Text2Text Generation', 'Text Generation'] | [Link](github.com/s-nlp/synthdetoxm) | N/A |
| [Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning](https://arxiv.org/abs/2502.06781) | Yuzhe Gu, Songyang Gao, Chengqi Lyu, zsytony, ZwwWayne | - This paper introduces OREAL, a novel Reinforcement Learning (RL) framework designed to enhance mathematical reasoning capabilities of Large Language Models (LLMs) by focusing on outcome-based rewards. - The framework leverages behavior cloning on positive trajectories obtained through Best-of-N sampling and employs reward shaping for negative samples to ensure gradient consistency during training. - A token-level reward model is also integrated to address the challenge of sparse rewards in long reasoning chains. - Experimental results demonstrate that OREAL achieves state-of-the-art performance on MATH-500, surpassing existing 7B and 32B models trained through distillation or RL. - Notably, OREAL-7B achieves a pass@1 accuracy of 91.0 on MATH-500 using RL, and OREAL-32B further pushes the limit to 95.0. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/InternLM/OREAL) | N/A |
| [Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling](https://arxiv.org/abs/2502.06703) | Xiu Li, Jian Zhao, Junqi Gao, iseesaw, RyanLiu112 | - This paper investigates compute-optimal Test-Time Scaling (TTS) for Large Language Models (LLMs) focusing on how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS effectiveness. - The authors propose a reward-aware compute-optimal TTS strategy, demonstrating that smaller models (e.g., 1B) can outperform significantly larger models (e.g., 405B) and even state-of-the-art reasoning models like Google's o1 and DeepSeek-R1 on challenging mathematical reasoning tasks (MATH-500 and AIME24). - Through experiments, they show that compute-optimal TTS strategies depend heavily on the chosen policy model, PRM, and problem difficulty and that the absolute problem difficulty criteria performs better than difficulty quantiles. - The findings highlight the importance of aligning TTS strategies with task and model characteristics, suggesting TTS as a potent approach to enhance LLM reasoning capabilities. -  A 3B LLM surpasses a 405B LLM on certain tasks using their proposed TTS strategy. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-aime), [Link](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute), [Link](https://huggingface.co/datasets/RLHFlow/Mistral-PRM-Data), [Link](https://huggingface.co/datasets/RLHFlow/Deepseek-PRM-Data), [Link](https://huggingface.co/Skywork) |
| [Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding](https://arxiv.org/abs/2502.05609) | Soyeong Jeong, Jeongyeon Seo, Sangjin Choi, doubleyyh, zomss | - This research introduces Hierarchy Drafting (HD), a novel lossless drafting approach for accelerating Large Language Model (LLM) inference, addressing the limitations of current speculative decoding methods by eliminating the need for parameter updates and ensuring consistent acceleration. - HD organizes various token sources into a hierarchical framework of multiple databases based on temporal locality, prioritizing tokens with higher locality to enhance drafting accuracy and minimize overhead from latency. - Experiments on Spec-Bench using LLMs with 7B and 13B parameters show that HD outperforms current lossless drafting methods, demonstrating robust inference speedups across model sizes, tasks, and temperatures, achieving over 1.5x faster inference speed in the greedy decoding setting and maintaining consistent improvement in sampling strategy. - The hierarchical framework in HD effectively leverages the varying temporal locality of tokens throughout the generation process, enhancing drafting accuracy and optimizing latency by accessing databases sequentially based on their scale and locality. - Analysis shows that HD successfully balances accuracy with reduced latency, leading to significant acceleration gains and demonstrating its potential as a practical alternative to computationally expensive model retraining methods for real-world LLM serving scenarios. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zomss/Hierarchy_Drafting) | N/A |
| [Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05415) | Yishun Li, Zhenyi Liao, zhijie3, asunalove, UnhurriedDawn | - Show-o Turbo is a new multimodal model that accelerates the Show-o model for unified understanding and generation by shortening the denoising trajectories of both image and text tokens. - It leverages a unified denoising perspective for both modalities based on parallel decoding of text tokens and extends consistency distillation to multimodal denoising trajectories. - A trajectory segmentation strategy and curriculum learning are employed to improve training convergence. - In text-to-image generation, Show-o Turbo achieves a GenEval score of 0.625 at 4 sampling steps without classifier-free guidance, outperforming the original Show-o with 8 steps and CFG. - In image-to-text generation, it demonstrates a 1.5x speedup without a significant drop in performance. | ['Text-to-Image', 'Image-to-Text', 'Multimodal'] | [Link](https://github.com/zhijie-group/Show-o-Turbo) | N/A |
| [Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2502.06060) | Dorsa Sadigh, C. Karen Liu, Warren Xia, bidiptas | - This paper introduces a technique to improve the discussion abilities of Large Language Models (LLMs) within social deduction games, focusing on the game Among Us. - The approach enhances communication by training LLMs to both "listen" effectively, by predicting imposter identity based on observations and discussions, and "speak" effectively, by rewarding messages that influence other agents' beliefs about the imposter. - Using multi-agent reinforcement learning and an iterated self-play algorithm, the trained crewmates achieve a two times higher win rate compared to standard RL and demonstrate robust communication strategies against adaptive imposters. - The method leverages dense reward signals derived from the agents' goal of identifying the imposter, which surpasses the performance of larger base models by a significant margin (over three times higher win rates) without relying on human demonstration data. - The emergent behaviors observed in the trained agents include accusing suspects, providing evidence, and sometimes employing deceptive strategies, mirroring human-like interactions in Among Us. | ['Reinforcement Learning', 'Natural Language Processing', 'Multimodal'] | [Link](https://socialdeductionllm.github.io/) | N/A |
| [ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates](https://arxiv.org/abs/2502.06772) | Mengdi Wang, Bin Cui, Zhaochen Yu, Ling Yang | - ReasonFlux, a novel hierarchical LLM reasoning framework, is introduced, which leverages scaled thought templates to enhance reasoning capabilities, outperforming models like OpenAI o1-preview and DeepSeek V3 on complex mathematical tasks. - The framework incorporates a structured library of approximately 500 thought templates, hierarchical reinforcement learning to optimize template trajectories, and a dynamic inference scaling system. - ReasonFlux-32B achieves 91.2% accuracy on the MATH benchmark, surpassing o1-preview by 6.7%, and solves 56.7% of problems on the AIME benchmark, outperforming o1-preview and DeepSeek-V3 by 27% and 45%, respectively. - The model demonstrates robust generalization across various mathematical reasoning benchmarks, including OlympiadBench and Gaokao, showcasing the effectiveness of the template-driven approach. - A better exploration-exploitation trade-off in navigating the reasoning space is exhibited by the model compared to Best-of-N and MCTS methods, as evidenced by a consistently lower and more stable exploration cost across different problem difficulty levels. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Gen-Verse/ReasonFlux) | N/A |
| [The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering](https://arxiv.org/abs/2502.03628) | Zhenting Wang, Di Liu, Yunhe Gao, Haizhou Shi, Zhuowei Li | - This paper introduces VISTA (Visual Information Steering with Token-logit Augmentation), a training-free method to reduce hallucination in Large Vision-Language Models (LVLMs) by reinforcing visual information and leveraging early-layer activations. - The authors identify three key patterns in LVLMs token processing: Gradual visual information loss, early excitation of semantically meaningful tokens, and hidden genuine information. - VISTA employs two modules: Visual Steering Vector (VSV) to counteract visual information loss and Self-Logits Augmentation (SLA) to utilize early excitation. - Extensive experiments on four LVLMs (LLaVA, Shikra, MiniGPT-4, InstructBLIP) across three decoding strategies and four benchmarks show significant hallucination reduction (up to ~40%) compared to existing methods. - VISTA excels in stochastic sampling settings and consistently improves performance on both object hallucination and general-purpose benchmarks, demonstrating enhanced visual grounding and overall model behavior. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/LzVv123456/VISTA) | N/A |
| [Matryoshka Quantization](https://arxiv.org/abs/2502.06786) | Aditya Kusupati, Prateek Jain, Jeff Dean, Puranjay Datta, Pranav Nair | - Matryoshka Quantization (MatQuant), a novel multi-scale quantization training technique, leverages the nested structure of integer data types (int8, int4, int2) to train a single model that can be served at different precision levels. - MatQuant achieves comparable accuracy to independently trained baselines for int8 and int4, and significantly outperforms them for int2, showing up to a 10% improvement with co-distillation techniques. - MatQuant exhibits strong interpolative behavior, allowing extraction of int3 and int6 models without explicit training, and enables layer-wise Mix'n'Match for optimal accuracy-cost trade-offs. - The technique is general-purpose and compatible with learning-based quantization methods like QAT and OmniQuant, demonstrating effectiveness across various large language models (LLMs). - Single Precision MatQuant, discovered through the research, enhances standard low-bit quantization and outperforms existing int2 quantization methods. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [EVEv2: Improved Baselines for Encoder-Free Vision-Language Models](https://arxiv.org/abs/2502.06788) | Yueze Wang, Yufeng Cui, Xiaotong Li, Haiwen Diao, PhyscalX | - Introduces EVEv2.0, a new family of encoder-free vision-language models (VLMs) built upon a decoder-only architecture, designed for enhanced efficiency and seamless integration with large language models. - Employs a divide-and-conquer strategy, incorporating modality-specific weights for attention, normalization, and feed-forward layers to mitigate interference and improve cross-modal alignment. - Utilizes a minimalist patch embedding layer trained from scratch, minimizing inductive biases from pre-trained vision encoders and enabling flexible processing of arbitrary-resolution and aspect-ratio images. - Presents a new caption engine named DenseFusion++ to generate higher-quality captions, leading to improved VLM performance. - Demonstrates superior performance against existing encoder-free models and approaches the capabilities of encoder-based counterparts across multiple vision-language benchmarks. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/baaivision/EVE) | N/A |
| [LM2: Large Memory Models](https://arxiv.org/abs/2502.06049) | Fraser Greenlee, Alex J. Chan, Filippos Christianos, Wenqi Wu, Jikun Kang | - This paper introduces LM2, a Large Memory Model, which is a decoder-only Transformer architecture enhanced with an auxiliary memory module to address limitations of standard Transformers in multi-step reasoning with long contexts. - LM2 incorporates a memory module acting as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. - On the BABILong benchmark, LM2 outperforms the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks, demonstrating exceptional capabilities in multi-hop inference and large-context question-answering. - On the MMLU dataset, LM2 shows a 5.0% improvement over a pre-trained vanilla model, indicating that the memory module does not hinder general task performance. - An analysis explores memory interpretability, effectiveness, and test-time behavior, emphasizing the role of explicit memory in enhancing Transformer architectures. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/convergence-ai/lm2) | N/A |
| [APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding](https://arxiv.org/abs/2502.05431) | Beidi Chen, Tianqi Chen, Hanyuezhuohua | - The paper introduces Adaptive Parallel Encoding (APE), a technique to improve the efficiency and performance of context-augmented generation (CAG) tasks like retrieval-augmented generation (RAG) and in-context learning (ICL). - APE addresses the performance drop seen in traditional parallel encoding methods by aligning the attention distribution of parallel encoding with sequential encoding through three steps: shared prefix, attention temperature adjustment, and scaling factor. - On RAG tasks, APE maintains 98% of sequential encoding accuracy while achieving a 3.3% performance improvement on LongBench by using more and longer contexts.  - On ICL tasks, APE outperforms parallel encoding by 7.9% on average across three tasks and retains 93% of full-shot sequential encoding performance with similar context length. - APE also demonstrates effective scaling for many-shot CAG, processing hundreds of texts in parallel, and achieves up to 4.5x faster inference for a 128k context. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/Infini-AI-Lab/APE) | N/A |
| [MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents](https://arxiv.org/abs/2502.05957) | Chao Huang, Tianyu Fan, Jiabin Tang | - MetaChain is a novel framework for developing Large Language Model (LLM) agents that requires no coding, enabling anyone to build custom agents using natural language. - It operates as an autonomous Agent Operating System, with Agentic System Utilities, an LLM-powered Actionable Engine, a Self-Managing File System, and a Self-Play Agent Customization module. - Evaluation on the GAIA benchmark shows MetaChain achieving second place, surpassing all other open-source solutions and nearing closed-source performance. - In Retrieval-Augmented Generation (RAG) tasks, MetaChain outperforms existing methods, including LangChain's agentic RAG, by dynamically orchestrating workflows. - Case studies demonstrate MetaChain's ability to handle tasks ranging from single-agent image generation to multi-agent financial analysis and automated workflow creation for tasks like majority voting across multiple LLMs. | ['Natural Language Processing'] | [Link](https://github.com/HKUDS/MetaChain) | N/A |
| [Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM](https://arxiv.org/abs/2502.06635) | Zhaoxiang Zhang, Shu Li, Qingshui Gu, aaabiao | - Steel-LLM is a 1-billion parameter, open-source, Chinese-centric large language model (LLM) trained on a large-scale dataset with a focus on transparency and resource efficiency. - The model architecture incorporates Soft Mixture of Experts (Soft MOE) and an enhanced Feed-Forward Network, trained on 8 GPUs. - Steel-LLM demonstrates competitive performance against established benchmarks such as CEVAL (41.90%) and CMMLU (36.08%), outperforming similarly sized models and approaching the performance of some larger models. - Key features include resource-efficient training, complete transparency with open-sourced code and data, and practical guidance for smaller research teams. - The project aims to improve accessibility and foster further research in Chinese and multilingual LLM development. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zhanshijinwat/Steel-LLM) | N/A |
| [The Curse of Depth in Large Language Models](https://arxiv.org/abs/2502.05795) | Yefeng Zheng, Lu Yin, Xinyuan Song, Wenfang Sun, pengxiang | - This paper introduces "LayerNorm Scaling", a technique to mitigate the Curse of Depth (CoD) in Large Language Models (LLMs), where deeper layers contribute less effectively than expected. - The paper identifies the root cause of CoD as Pre-Layer Normalization (Pre-LN), which leads to exponential growth of output variance with depth, causing gradients in deeper layers to approach an identity matrix. - LayerNorm Scaling mitigates this by scaling the output of layer normalization inversely by the square root of the layer's depth, thus controlling variance and improving the contribution of deeper layers. - Experimental results on LLaMA models (130M to 1B parameters) show that LayerNorm Scaling improves pre-training perplexity and downstream task performance in supervised fine-tuning compared to Pre-LN and other normalization techniques. - The improvement is attributed to the enhanced contribution of deeper layers enabled by LayerNorm Scaling, which results in better representation learning and improved generalization. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-02-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [VideoRoPE: What Makes for Good Video Rotary Position Embedding?](https://arxiv.org/abs/2502.05173) | Pan Zhang, Xiaoyi Dong, Xilin Wei, yuhangzang, LiuXR | - VideoRoPE, a novel video position embedding strategy designed for video large language models (LLMs), is introduced, improving the encoding of spatiotemporal information in videos. - It addresses four key properties for effective video position encoding: 2D/3D structure, frequency allocation, spatial symmetry, and temporal index scaling. - VideoRoPE utilizes a low-frequency temporal allocation to mitigate periodic oscillation issues, diagonal layout to preserve spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. - It surpasses existing RoPE variants on tasks like long video retrieval (+12.4 on V-NIAH, +12.4 on V-NIAH-D), video understanding (+2.9 on LongVideoBench, +4.5 on MLVU, +1.7 on Video-MME), and video hallucination (+11.9 on VideoHallucer). - A new challenging benchmark, V-NIAH-D (Visual Needle-In-A-Haystack with Distractors), is introduced to evaluate the robustness of position embedding designs against distractors. | ['Multimodal', 'Video-Text-to-Text', 'Video Classification', 'Video-Text-to-Text'] | [Link](https://github.com/Wiselnn570/VideoROPE) | N/A |
| [QuEST: Stable Training of LLMs with 1-Bit Weights and Activations](https://arxiv.org/abs/2502.05003) | Jiale Chen, d-alistarh, mnikdan97, soroushtabesh, BlackSamorez | - QuEST, a new quantization-aware training (QAT) method, achieves stable training of large language models (LLMs) with weights and activations quantized down to 1-bit. - QuEST improves upon existing QAT methods by using Hadamard normalization and MSE-optimal fitting for accurate and fast quantization, and a new trust gradient estimator for minimizing the error between quantized and full-precision gradients. - When data and compute are scaled proportionally to model size, QuEST trains models with 4-bit weights and activations that achieve superior accuracy compared to BF16 models almost 4x larger. - The method demonstrates stable scaling laws across a range of hardware-supported precisions and model sizes, and can be extended to sparse representations. - GPU kernel support shows that QuEST-trained models can be executed efficiently on commodity hardware. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/IST-DASLab/QUEST) | N/A |
| [DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails](https://arxiv.org/abs/2502.05163) | Bo Li, Wei Wang, Junkai Zhang, Yu Yang, ydeng9 | - DuoGuard, a novel two-player Reinforcement Learning framework, is proposed for training multilingual Large Language Model (LLM) guardrails to enhance safety, particularly for under-resourced languages. - A generator and guardrail classifier co-evolve adversarially within the framework to produce synthetic training data, guided by theoretical analysis proving convergence to a Nash equilibrium. - Empirical evaluations demonstrate DuoGuard's superior performance, achieving a 10% improvement over the 8B parameter LlamaGuard3 model on English benchmarks while being 4.5x faster at inference with a smaller 0.5B model. - DuoGuard also addresses the language imbalance issue by generating synthetic data for lower-resource languages, showing substantial advancements in multilingual safety tasks and outperforming existing guardrails by over 20% on average for similarly sized models. - Ablation studies confirm the significant role of synthetic data generation in bridging the data gap between English and other languages, promoting safer and more responsible use of LLMs across different linguistic contexts. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/yihedeng9/DuoGuard) | N/A |
| [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171) | Siddharth Singh, John Kirchenbauer, Neel Jain, Sean McLeish, Jonas Geiping | - This paper introduces a novel language model architecture that scales test-time computation through latent reasoning using a recurrent block, enabling arbitrary depth at test time. - The model, trained on a massive dataset of 800 billion tokens with 3.5 billion parameters, iteratively refines its understanding in a latent space, contrasting with chain-of-thought prompting which scales by generating more tokens. - Results demonstrate improved performance on reasoning benchmarks compared to open-source models with similar training data and more parameters, reaching computational loads comparable to a 50 billion parameter model. - This recurrent depth approach naturally enables features like adaptive compute, speculative decoding, and KV-cache sharing at inference time. - Analysis of token trajectories in latent space reveal emergent computational behaviors, such as orbiting patterns for numerical reasoning, suggesting the model learns to use its latent space in novel ways. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/seal-rg/recurrent-pretraining) | [Link](huggingface.co/tomg-group-umd/huginn-0125) |
| [Linear Correlation in LM's Compositional Generalization and Hallucination](https://arxiv.org/abs/2502.04520) | Chengyu Dong, Shibo Hao, Chenyang An, Letian Peng, shangjingbo | - This paper uncovers the phenomenon of linear correlations in next-token prediction (NTP) logits from language models (LMs) during knowledge composition, showing that related knowledge pairs (e.g., "X lives in the city of" and "X lives in the country of") exhibit linear transformations in their logits. - The study finds that this linear transformation is resilient to large-scale fine-tuning and generalizes updated knowledge when aligned with real-world relationships but causes hallucinations when deviated. - Empirical results suggest linear correlation can serve as a potential identifier of LM generalization. - A simple feedforward network with pre-trained vocabulary representations can learn these linear correlations, indicating LM generalization heavily relies on these representations. - This paper investigates the generalization mechanism behind LMs and how it relates to knowledge composition, offering insights into compositional generalization and hallucination. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/KomeijiForce/LinCorr) | N/A |
| [Generating Symbolic World Models via Test-time Scaling of Large Language Models](https://arxiv.org/abs/2502.04728) | Fuxiang Frank Xia, Tim Z. Xiao, Yuhuan Yuan, Zhouliang Yu, zhangysk | - This paper introduces a novel test-time scaling approach for generating Planning Domain Definition Language (PDDL) world models using Large Language Models (LLMs), enhancing their planning capabilities without requiring model fine-tuning. - The method employs a two-stage process: Best-of-N (BoN) sampling to generate diverse initial PDDL solutions and instance Verbalized Machine Learning (iVML) to iteratively refine these solutions based on critiques from an optimizer LLM. - This approach achieves state-of-the-art performance on PDDL domain generation tasks, surpassing OpenAI's models, with an 85.2% success rate on NL2Domain and 71.4% on Prob2Domain using Qwen2.5-Coder-7B. - By using PDDL as an intermediate abstraction, the method enables more robust planning and mitigates LLM hallucinations, successfully handling complex planning scenarios where direct LLM-based planners fail. - The combination of BoN and iVML balances exploration and exploitation, enabling faster convergence and higher-quality PDDL domain generation compared to using either method alone. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference](https://arxiv.org/abs/2502.04416) | Wulong Liu, Xianzhi Yu, Hui-Ling Zhen, Lancheng Zou, Eleven-P | - CMoE is a novel framework designed to transform pre-trained dense Large Language Models (LLMs) into Mixture-of-Experts (MoE) models, enhancing inference efficiency without extensive retraining or resource demands. - This is achieved by "carving" experts from the dense model's Feed-Forward Network (FFN) layers, categorizing neurons into shared (always-on) and routed (conditionally activated) based on activation patterns. - CMoE's routing mechanism is analytically derived from activation statistics rather than trained, and incorporates load balancing and differentiability for performance.  - Using a 7B parameter dense model, CMoE creates a usable MoE model in approximately five minutes, and achieves recovery of dense model performance through lightweight fine-tuning within one hour using a modest dataset (2048 samples). - Experimental results demonstrate that CMoE maintains comparable perplexity to the original dense models, achieving a perplexity as low as 12.73 on WikiText-2 and 32.37 on C4 after fine-tuning with a sparsity of 75% and outperforms the baseline method LLaMA-MoE on various downstream tasks both with and without fine-tuning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JarvisPei/CMoE) | N/A |
| [Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models](https://arxiv.org/abs/2502.04404) | Jie-Jing Shao, Ding-Chu Zhang, Wen-Da Wei, Xuan-Yi Zhu, yangxw | - This paper introduces Self-Backtracking, a novel technique to enhance the reasoning capabilities of Large Language Models (LLMs) by enabling them to learn when and where to backtrack during both training and inference. - The method addresses limitations of existing slow-thinking models, such as inefficient overthinking and over-reliance on external reward models, by internalizing the search process within the LLM. - Self-Backtracking facilitates dynamic search and expert iteration for self-improvement, transforming slow-thinking processes into fast thinking. - Empirical evaluations on the Countdown task demonstrate a performance improvement of over 40% compared to the optimal-path supervised fine-tuning method. - The proposed technique enhances reasoning flexibility and exhibits test-time scaling capabilities, contributing to the development of more advanced and robust reasoning LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/LAMDASZ-ML/Self-Backtracking) | N/A |
| [QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05178) | Yuke Zhu, Linxi Fan, Scott Reed, Fuzhao Xue, zhaoyue-zephyrus | - QLIP (Quantized Language-Image Pre-training) introduces a novel visual tokenization method combining high-quality reconstruction with strong zero-shot image understanding by training a Binary Spherical Quantization (BSQ)-based autoencoder with both reconstruction and language-image alignment objectives. - A two-stage training approach addresses the conflicting demands of large-batch contrastive learning and memory-intensive reconstruction, first prioritizing semantic representation learning and then refining visual quality. - QLIP achieves competitive reconstruction quality compared to other state-of-the-art visual tokenizers, while maintaining comparable zero-shot image classification accuracy to CLIP. - QLIP's visual tokens can be integrated with LLMs for visual question answering, achieving performance comparable to CLIP-based methods, and also used for high-quality text-conditioned image generation with improved FID scores compared to other visual tokenizers. - QLIP further enables UM³, a unified mixed-modality auto-regressive model, capable of handling language-only, image-to-text, and text-to-image tasks within a single model. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering', 'Zero-Shot Image Classification', 'Image Feature Extraction'] | N/A | [Link](https://huggingface.co/datasets/guangyil/laion-coco-aesthetic) |
| [CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance](https://arxiv.org/abs/2502.04350) | Chuchu Fan, Yang Zhang, Yueying Liu, Yilun Hao, Yongchao Chen | - CodeSteer, a novel framework to augment Large Language Models (LLMs) with symbolic computing capabilities by effectively guiding code/text generation. - Introduces SymBench, a comprehensive benchmark with 37 symbolic tasks of varying complexity, along with 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. - Employs a fine-tuned Llama-3-8B model as an assistant (CodeSteerLLM) to steer larger models like GPT-40 through multiple rounds of interaction using newly designed multi-round Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). - Incorporates symbolic and self-answer checkers to improve code quality and answer verification. - Demonstrates significant performance improvements on SymBench, boosting GPT-40's score from 53.3 to 86.4, outperforming leading models like OpenAI 01 (82.7) and DeepSeek R1 (76.8), with strong generalizability to other LLMs like Claude, Mistral, and GPT-3.5. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/yongchao98/CodeSteer-v1.0) | N/A |
| [ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning](https://arxiv.org/abs/2502.04689) | Giuseppe Carenini, yuweiyin | - This paper introduces ARR, a novel zero-shot prompting method designed to enhance the question-answering capabilities of Large Language Models (LLMs). - ARR guides LLMs through three key stages: analyzing the question's intent, retrieving pertinent information, and conducting step-by-step reasoning. - Through comprehensive evaluations across ten diverse multiple-choice question-answering datasets, ARR consistently demonstrates superior performance compared to baseline methods and existing Chain-of-Thought (CoT) prompting techniques. - Ablation studies confirm that each component of ARR contributes positively, with intent analysis notably enhancing performance. - Further experiments validate the generalizability of ARR across various model sizes, LLM architectures, generation settings, and few-shot learning scenarios, showcasing its robustness and adaptability. | ['Question Answering'] | [Link](https://github.com/YuweiYin/ARR) | N/A |
| [MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf](https://arxiv.org/abs/2502.04376) | Qingwei Lin, Jue Zhang, Xiaoting Qin, Shurun Yuan, Lingxiang Hu | - This paper introduces an LLM-powered meeting delegate system designed to represent users in meetings, generating relevant spoken content based on real-time meeting transcripts. - A prototype system was developed, focusing on the role of a participant rather than a facilitator, addressing challenges like navigating context-rich conversations and handling ambiguities in human speech. - A new benchmark dataset was created from real meeting transcripts, covering common scenarios like explicit cues, implicit cues, chiming in, and remaining silent. - Popular LLMs were evaluated on the benchmark, revealing that GPT-4/40 maintained balanced performance, Gemini 1.5 Pro was cautious, and Gemini 1.5 Flash and Llama3-8B/70B were more active.  - Overall, approximately 60% of generated responses included at least one key point from the ground truth, demonstrating potential but also highlighting the need for improvements in handling transcription errors and reducing irrelevant content. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2025-02-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Analyze Feature Flow to Enhance Interpretation and Steering in Language Models](https://arxiv.org/abs/2502.03032) | Yaroslav Aksenov, kefirski, elephantmipt, dlaptev | - The paper introduces a novel approach to map features discovered by sparse autoencoders across layers of large language models (LLMs).  This approach uses a data-free cosine similarity technique to generate granular flow graphs of feature evolution. - It demonstrates how these cross-layer feature maps enable direct steering of model behavior by amplifying or suppressing specific features, leading to targeted thematic control in text generation. - The method helps to discover the lifespan of SAE features, understand their evolution across layers, and shed light on how they might form computational circuits. - Experiments validate the single-layer analysis patterns and demonstrate causal relationships between target features and their matched predecessors using a deactivation method. - The research presents a novel multi-layer model steering technique leveraging these cross-layer feature maps, enabling more precise control over LLM behavior. | ['Natural Language Processing', 'Text Generation', 'Feature Extraction'] | N/A | N/A |
| [UltraIF: Advancing Instruction Following from the Wild](https://arxiv.org/abs/2502.04153) | Ning Ding, Li Sheng, ssz1111, ganqu, kkk-an | - This paper introduces ULTRAIF, a novel approach for enhancing instruction-following capabilities in large language models (LLMs) using open-source data. - ULTRAIF decomposes complex instructions into simpler queries, constraints, and evaluation questions, subsequently training a composer model to synthesize diverse and complex instructions. - The proposed approach demonstrates superior performance on various instruction-following benchmarks, surpassing existing methods in multiple metrics. - ULTRAIF achieves a notable milestone, aligning a base LLaMA-3.1-8B model to match its instruct counterpart in instruction-following ability, highlighting its efficacy. - The authors explore the potential of self-alignment through further optimization of LLaMA-3.1-8B-Instruct, showcasing the method's versatility and applicability. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/kkk-an/UltraIF) | [Link](string) |
| [Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment](https://arxiv.org/abs/2502.04328) | jiwenlu, WinstonHu, liuziwei7, THUdyh, Zuyan | - The paper introduces Ola, a novel 7B parameter omni-modal language model that leverages a progressive modality alignment strategy for training.- Ola's architecture incorporates modality-specific encoders (visual, audio, text) and a joint alignment module to fuse multi-modal inputs for processing by a large language model.- The progressive training strategy starts with image and text, then adds video and finally audio, enabling efficient training with smaller datasets compared to existing methods.- Ola outperforms state-of-the-art open-source omni-modal LLMs and achieves highly competitive performance against specialized models across image, video, and audio benchmarks. -  The model supports user-friendly real-time streaming decoding for text and speech. | ['Multimodal', 'Any-to-Any', 'Automatic Speech Recognition', 'Text-to-Speech', 'Text-to-Audio', 'Image-to-Text', 'Video-Text-to-Text', 'Video Classification', 'Audio Classification'] | [Link](https://github.com/Ola-Omni/Ola) | N/A |
| [MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm](https://arxiv.org/abs/2502.02358) | De Wen Soh, Na Zhao, zeyuhu, ZiyanGuo | - The paper introduces MotionLab, a novel framework for unified human motion generation and editing, based on the Motion-Condition-Motion paradigm. - MotionLab uses a MotionFlow Transformer with rectified flows to learn mappings between source and target motions, guided by specified conditions. - The model incorporates Aligned Rotational Position Encoding to ensure temporal synchronization and Task Instruction Modulation for effective multi-task learning. - A Motion Curriculum Learning strategy is employed for hierarchical training, facilitating knowledge sharing and mitigating catastrophic forgetting. - Experiments demonstrate MotionLab's superior versatility, performance, and efficiency compared to existing methods on multiple benchmarks. | ['Text-to-3D', 'Multimodal'] | N/A | N/A |
| [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313) | AmeyaPrabhu, douwekiela, iaa01, Klingspor, shash42 | This paper introduces CAPA, a novel metric for measuring language model (LM) similarity based on the overlap in model mistakes.  It demonstrates that LM-as-a-judge scores are biased towards models similar to the judge, and that gains from training on LM annotations are higher when the models are dissimilar. The study also reveals a concerning trend of increasing model similarity with increasing capabilities, highlighting risks associated with correlated failures in AI oversight. The authors propose using CAPA as a crucial component in reporting and mitigating model similarity effects in AI evaluation and training.  Finally, they suggest that sample-wise model predictions should be publicly released for improved analysis. | ['Natural Language Processing'] | [Link](https://github.com/model-similarity/lm-similarity) | [Link](https://huggingface.co/spaces/shash42/lm-similarity) |
| [Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2](https://arxiv.org/abs/2502.03544) | Miroslav Olšák, Trieu H. Trinh, Yuri Chervonyi, lmthang, mmenegali | - AlphaGeometry2 is presented, a significantly improved version of AlphaGeometry that surpasses the average gold medalist performance in solving IMO geometry problems. - The model achieves an 84% solve rate on all geometry problems from the last 25 years, compared to 54% previously. - Improvements include an expanded domain language covering locus theorems, linear equations, and non-constructive problem statements. - A stronger and faster symbolic engine, improved search algorithm utilizing Gemini architecture and knowledge sharing, and a better language model are also incorporated. - The system is making progress towards automating the process of solving geometry problems directly from natural language input. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization](https://arxiv.org/abs/2502.04306) | Bryon Aragam, Ling Yang, Edify-Kd2024, lightaime, yinjiewang | - ScoreFlow is a novel framework for automated multi-agent workflow generation and optimization that uses score-based preference optimization. - It addresses the limitations of existing methods by leveraging efficient gradient-based optimization in a continuous space, improving flexibility, adaptability, and scalability. - ScoreFlow incorporates Score-DPO, a novel variant of direct preference optimization, which incorporates quantitative feedback to improve reliability and convergence speed. - Experiments on six benchmark datasets show an 8.2% improvement over existing baselines across question answering, coding, and mathematical reasoning tasks. - ScoreFlow enables smaller models to outperform larger models at lower inference costs. | ['Question Answering'] | [Link](https://github.com/Gen-Verse/ScoreFlow) | N/A |
| [Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis](https://arxiv.org/abs/2502.04128) | Xinsheng Wang, Chi-Min Chan, Xinfa Zhu, HKUST-Audio, ZhenYe234 | - This paper introduces Llasa, a single Transformer-based Text-to-Speech (TTS) model initialized from Llama and trained with a new speech tokenizer, X-codec2, to align TTS with standard large language model architectures. - Scaling the training compute through increased model size and data consistently improves synthesized speech naturalness and prosody. - Scaling inference-time compute by incorporating speech understanding models as verifiers enhances emotional expressiveness, timbre consistency, and content accuracy. - X-codec2, a modification of X-codec with a single vector quantizer, achieves best performance on most metrics at a token rate of 50 and produces a UTMOS score close to ground truth. - Evaluation on Seed-TTS-Eval, LibriSpeech, and ESD datasets show state-of-the-art results for Llasa, highlighting improvements in zero-shot TTS and the effectiveness of inference-time scaling. | ['Text-to-Speech', 'Audio'] | N/A | [Link](https://huggingface.co/facebook/hubert-large-1s960-ft), [Link](https://huggingface.co/emotion2vec/emotion2vec_plus_large) |
| [ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution](https://arxiv.org/abs/2502.00989) | Kanika Goswami, Franck-Dernoncourt, ryanrossi, puneetm | - ChartCitor is a novel multi-agent framework designed to address the challenge of hallucination in Large Language Models (LLMs) for chart question answering by providing fine-grained bounding box citations. - The framework consists of six LLM agents that work together to extract structured data from charts, reformulate answers, generate contextual descriptions, retrieve evidence, and localize selected cells in the chart image. - ChartCitor outperforms existing baselines across various chart types, as demonstrated by its improved Intersection over Union (IoU) scores and qualitative user studies. - Qualitative user studies show that ChartCitor increases user trust in generative AI and improves user productivity by providing reliable and logically explained citations. -  The model uses GPT-4V for chart-to-table extraction and leverages RankGPT for re-ranking to identify the most relevant table cells. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation](https://arxiv.org/abs/2502.03860) | cxiong, yingbozhou, jcxu, hendrydong, bpucla | - BOLT bootstraps Long Chain-of-Thought (LongCoT) capabilities in Large Language Models (LLMs) without relying on knowledge distillation or extensive human annotation. - It involves three stages: LongCoT data bootstrapping via in-context learning with a ShortCoT LLM, LongCoT supervised finetuning, and LongCoT online training using Direct Preference Optimization (DPO). - BOLT was tested on various model scales (7B, 8B, and 70B) and achieved significant performance improvements across multiple benchmarks, including Arena-Hard, MT-Bench, WildBench, ZebraLogic, and MATH500, outperforming initial ShortCoT models. - The bootstrapping stage requires minimal human effort, with only 10 in-context examples used in the experiments.  -  BOLT represents a practical and efficient approach for enhancing reasoning skills in LLMs. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback](https://arxiv.org/abs/2502.00988) | Ryan Rossi, Puneet Mathur, Kanika Goswami, Franck-Dernoncourt | - PlotGen is a novel multi-agent framework that leverages multimodal LLMs to automate the creation of accurate scientific data visualizations. - The framework orchestrates multiple LLM-based agents, including a Query Planning Agent, a Code Generation Agent, and three feedback agents (Numeric, Lexical, and Visual) that iteratively refine the visualization. - Extensive experiments on the MatPlotBench dataset demonstrate that PlotGen outperforms strong baselines, achieving a 4-6% improvement in accuracy. - The use of multimodal feedback agents enhances user trust in LLM-generated visualizations and improves novice productivity by reducing debugging time. - PlotGen addresses the challenges faced by novice users in creating accurate and informative scientific visualizations from raw data. | ['Multimodal', 'Text-to-Image'] | N/A | N/A |
| [Enhancing Code Generation for Low-Resource Languages: No Silver Bullet](https://arxiv.org/abs/2501.19085) | gbavota, AML14, Devy1 | - This paper investigates techniques to enhance code generation for low-resource programming languages using Large Language Models (LLMs). - The study explores several approaches, including fine-tuning, in-context learning with crafted prompts, and a pre-training objective for code translation. - Experiments are conducted on six LLMs with varying architectures and sizes, using two low-resource languages (R and Racket) and a benchmark dataset. - Findings suggest that fine-tuning is effective for smaller LLMs, while in-context learning becomes more useful for larger models. - The study concludes that in-context learning, particularly with translation examples, is a safe and effective approach for improving code generation on low-resource languages. | ['Text Generation'] | N/A | N/A |
| [MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion](https://arxiv.org/abs/2502.04235) | Chenggang Li, Ke Shen, haoxintong | - This paper introduces MAGA, a method for expanding pre-training corpora by reformulating existing text data to generate diverse, contextually-rich synthetic data. - MAGA uses a two-stage synthesis process, generating genre-audience pairs and then reformulating documents to create new training instances. - The resulting MAGACorpus contains 770 billion tokens and demonstrates consistent performance improvements across various model sizes (134M-13B) compared to baselines using data repetition or upsampling. - The authors analyze the impact of prompt engineering on synthetic data quality, showing that careful prompt design can help mitigate issues such as training collapse. - This work suggests that MAGA offers a reliable path for scaling language models beyond existing data limitations. | ['Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/bytedance-research/MAGACorpus) |
| [Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization](https://arxiv.org/abs/2502.04295) | Xuan Feng, Qi Chen, Yuanye Liu, lynazhang, Jiahang | - This paper introduces Content-Format Integrated Prompt Optimization (CFPO), a novel method that jointly optimizes both prompt content and format for enhanced Large Language Model (LLM) performance. - CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy to systematically evaluate diverse format options. - Experiments across multiple tasks and open-source LLMs demonstrate that CFPO achieves measurable performance improvements compared to existing content-only optimization methods. - The proposed method highlights the importance of integrating content and format optimization and offers a practical, model-agnostic approach. - CFPO's effectiveness is validated through extensive evaluations on various benchmark datasets, showcasing consistent and significant performance gains across different LLMs. | ['Natural Language Processing'] | [Link](https://github.com/HenryLau7/CFPO) | [Link](null) |
| [Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](https://arxiv.org/abs/2502.04322) | Marzyeh Ghassemi, Yik Siu Chan, YuxinXiao, narutatsuri |  - This paper introduces SPEAK EASY, a novel framework for eliciting harmful jailbreaks from LLMs through simple, multi-step, multilingual interactions.  - It demonstrates that simple interactions can effectively elicit harmful responses, which are both actionable and informative, more effectively than existing methods.  - The authors propose HARMSCORE, a new metric that measures the harmfulness of LLM responses, aligning more closely with human judgments than existing metrics. - SPEAK EASY significantly improves the attack success rate and HARMSCORE across various LLMs and benchmarks.  - The findings highlight the importance of considering realistic human-LLM interactions when evaluating LLM safety. | ['Natural Language Processing', 'Text Generation', 'Text Classification'] | [Link](https://github.com/yiksiu-chan/SpeakEasy) | N/A |


## Papers for 2025-02-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model](https://arxiv.org/abs/2502.02737) | Gabriel Martín Blázquez, Elie Bakouch, Anton Lozhkov, Loubna Ben Allal, lvwerra | - This paper introduces SmolLM2, a 1.7 billion parameter language model trained on 11 trillion tokens using a multi-stage, data-centric approach. - The model utilizes a Llama2 architecture and is trained with a combination of web text, specialized math, code, and instruction-following data. - New specialized datasets (Fine-Math, Stack-Edu, and SmolTalk) were created to address limitations in existing public datasets. - SmolLM2 outperforms other recent small language models, including Qwen2.5-1.5B and Llama3.2-1B, on various benchmarks including HellaSwag, ARC, and MMLU-Pro. - Both the base and instruction-tuned versions of SmolLM2 are released, along with the new specialized datasets, to facilitate future research. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | N/A | [Link](https://hf.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9), [Link](https://huggingface.co/datasets/HuggingFaceTB/finemath), [Link](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) |
| [Demystifying Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2502.03373) | Xiang Yue, Graham Neubig, Morry Niu, Yuxuan Tong, Edward Yeo | - This paper investigates the mechanics of long chain-of-thought (CoT) reasoning in large language models (LLMs), identifying key factors that enable the generation of extended reasoning trajectories. - Through supervised fine-tuning (SFT) and reinforcement learning (RL) experiments, the study finds that while SFT simplifies training, scaling verifiable reward signals is crucial for RL, and core abilities like error correction are present in base models but require significant compute for complex tasks. - The authors introduce a cosine length-scaling reward with a repetition penalty to stabilize CoT length growth and encourage emergent reasoning behaviors. - Leveraging noisy, web-extracted solutions with filtering shows promise, especially for out-of-distribution reasoning tasks. - The research provides practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/eddycmu/demystify-long-cot) | N/A |
| [LIMO: Less is More for Reasoning](https://arxiv.org/abs/2502.03387) | Shijie Xia, Ethan Chern, Yang Xiao, Zhen Huang, Yixin Ye | - This paper introduces LIMO, a novel approach to mathematical reasoning in large language models (LLMs) that challenges the conventional wisdom that complex reasoning requires massive datasets. - LIMO achieves state-of-the-art performance on several mathematical reasoning benchmarks using only 817 training samples, significantly outperforming models trained on datasets 100 times larger. - The core finding is that sophisticated reasoning capabilities can be elicited with minimal data if the model has a strong pre-trained knowledge foundation and the training data provides effective demonstrations of cognitive processes. - The authors propose the Less-Is-More Reasoning Hypothesis, which posits that the efficiency of eliciting complex reasoning depends on two factors: completeness of pre-trained knowledge and the effectiveness of cognitive templates in the training data. - The study provides empirical evidence and contributes towards a more data-efficient approach to developing advanced reasoning capabilities in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/GAIR-NLP/LIMO) | [Link](https://huggingface.co/datasets/GAIR) |
| [Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking](https://arxiv.org/abs/2502.02339) | Feihu Che, Ruihan Jin, Shuai Zhang, Mingkuan Feng, Jinyang Wu | - This paper introduces AStar, a novel automated structured thinking framework designed to boost multimodal reasoning capabilities in large language models (LLMs) by leveraging Monte Carlo Tree Search (MCTS). - AStar automates the derivation of cognitive reasoning patterns from a small seed dataset (500 samples) using MCTS-powered hierarchical structures. - A unified reasoning framework integrates both internal and external reasoning abilities, enabling efficient inference with minimal tree iterations, and outperforms GPT-40 (50.2%) on the MathVerse benchmark, achieving a 54.0% accuracy score with a 7B backbone. - AStar improves efficiency by reducing inference overhead by 6.4x compared to other tree-based methods and achieves comparable results to training-based methods with 520x less training data. - It demonstrates robustness across different languages and tasks, exhibiting strong generalization capabilities in both in-distribution and out-of-distribution settings. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data) |
| [Jailbreaking with Universal Multi-Prompts](https://arxiv.org/abs/2502.01154) | Shang-Tse Chen, Hsuan Su, Yu-Ling Hsu | - This paper introduces JUMP, a prompt-based method that uses universal multi-prompts to jailbreak LLMs. - JUMP optimizes universal multi-prompts, outperforming existing techniques in experimental results. - The method also generalizes to defense scenarios, which is called DUMP. - JUMP++ is the enhanced version of JUMP, which significantly outperforms JUMP* and current state-of-the-art methods. - The code is publicly available on Github. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ntuaislab/JUMP) | [Link](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B), [Link](https://huggingface.co/openai-community/gpt2-large) |
| [A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods](https://arxiv.org/abs/2502.01618) | Akash Srivastava, Kai Xu, Guangxuan Xu, Shivchander Sudalairaj, ishapuri-mit | - This paper introduces a novel inference-time scaling approach for large language models (LLMs) that frames the task as probabilistic inference rather than a search problem.  - The proposed method uses particle-based Monte Carlo methods, specifically particle filtering, to estimate the typical set of the state distribution, which is more robust to approximation errors in reward models compared to search-based methods.  - Empirical evaluations demonstrate that the proposed method achieves a 4-16x better scaling rate than deterministic search counterparts on challenging mathematical reasoning tasks.  -  The method allows smaller models to surpass the accuracy of larger models, for instance Qwen2.5-Math-1.5B-Instruct surpassing GPT-4 accuracy in only 4 rollouts.  - This approach not only provides an effective method for inference-time scaling but also connects probabilistic inference with LLM inference-time scaling. | ['Natural Language Processing', 'Text Generation'] | [Link](https://probabilistic-inference-scaling.github.io/) | N/A |
| [Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](https://arxiv.org/abs/2502.03275) | Yuandong Tian, Jiantao Jiao, Yingchen Xu, Hanlin Zhu, DiJia Su | - This paper introduces a novel hybrid representation for language model reasoning that mixes latent and text tokens, aiming to improve efficiency and performance. - The method uses a VQ-VAE to generate latent tokens representing reasoning steps, reducing the length of reasoning traces. - Experiments on various benchmarks (including Math, GSM8K, and Keys-Finding Maze) show consistent improvements in accuracy compared to baseline methods, with an average reduction of 17% in reasoning trace length. - A simple training procedure that randomly mixes latent and text tokens allows for quick adaptation to new latent tokens, improving the training efficiency. - The approach is effective for both logical and mathematical reasoning tasks across different model sizes. | ['Natural Language Processing'] | N/A | N/A |
| [On Teacher Hacking in Language Model Distillation](https://arxiv.org/abs/2502.02671) | Nino Vieillard, Sarah Perrin, Johan Ferret, Daniele Calandriello, Daniil Tiapkin | - This paper introduces the concept of "teacher hacking" in language model distillation, where the student model over-optimizes the teacher model instead of the true data distribution. - A novel controlled experimental setup involving an oracle model, teacher model, and student model is proposed to study teacher hacking. - Experiments show that teacher hacking occurs when using a fixed offline dataset for distillation, and that this can be detected by deviations from polynomial convergence laws. - Online data generation techniques effectively mitigate teacher hacking, highlighting data diversity as a key factor. - The findings offer insights into the benefits and limitations of distillation for building robust and efficient language models. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2025-02-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ACECODER: Acing Coder RL via Automated Test-Case Synthesis](https://arxiv.org/abs/2502.01718) | Xiaotong Chen, Haozhe Wang, Huaye Zeng, pingnieuk, DongfuJiang | - This paper introduces ACECODER, a novel approach to improve code generation models using reinforcement learning (RL) and automated test-case synthesis. - ACECODER synthesizes a large-scale dataset (ACECODE-89K) of coding questions and test cases, enabling effective RL training. - The method utilizes a Bradley-Terry loss function to train reward models based on pass rates of generated programs over the synthesized test cases. - Experiments show that ACECODER consistently improves the performance of various code generation models across multiple benchmarks. Notably, it improves the model's performance on HumanEval-plus by over 25% and MBPP-plus by 6% with only 80 optimization steps. - The authors believe that ACECODER highlights the significant potential of RL in improving code generation models. | ['Reinforcement Learning', 'Text Generation'] | [Link](https://tiger-ai-lab.github.io/AceCoder) | N/A |
| [Can LLMs Maintain Fundamental Abilities under KV Cache Compression?](https://arxiv.org/abs/2502.01941) | Zeyu Li, Peijie Dong, Hong Chen, Zhenheng Tang, Dominic789654 |  - This paper introduces ShotKV, a novel KV cache compression method that distinctly manages prefill and decoding phases to maintain shot-level semantic coherence, improving performance on long-context tasks. - The study reveals that arithmetic reasoning tasks are particularly sensitive to aggressive compression, while multi-step reasoning LLMs exhibit more robust compression tolerance. -  Empirical results demonstrate that ShotKV achieves performance improvements of 9%-18% on long-context generation tasks under aggressive compression. - The authors analyze attention patterns and cross-task compression performance to identify key factors influencing compression sensitivity, including model training dynamics, prompt length, and task-specific requirements. - The findings provide valuable insights into the relationship between KV cache compression methods and LLMs' fundamental abilities. | ['Natural Language Processing'] | N/A | N/A |
| [Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://arxiv.org/abs/2502.02508) | Zhenfang Chen, Zhang-Wei Hong, Zhenting Qi, Guangtao Zeng, maohaos2 | This paper introduces Satori, a 7B parameter LLM enhanced with a Chain-of-Action-Thought (COAT) reasoning mechanism and a two-stage training paradigm (format tuning and self-improvement via reinforcement learning).  Satori achieves state-of-the-art performance on mathematical reasoning benchmarks and exhibits strong generalization to out-of-domain tasks.  The COAT mechanism allows for self-reflection and exploration of alternative solutions, improving reasoning capabilities.  The two-stage training paradigm effectively leverages a small-scale imitation learning stage for format tuning followed by a large-scale reinforcement learning stage for self-improvement.  Experimental results demonstrate Satori's superiority over existing methods. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Question Answering'] | [Link](https://satori-reasoning.github.io/) | N/A |


## Papers for 2025-02-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [The Differences Between Direct Alignment Algorithms are a Blur](https://arxiv.org/abs/2502.01237) | Boris Shaposhnikov, kefirski, ZeL1k7, ummagumm-a, Myashka | - This paper analyzes Direct Alignment Algorithms (DAAs) for aligning language models with human preferences, focusing on the impact of different design choices like ranking losses (pairwise vs. pointwise), implicit rewards (likelihood ratios, odds ratios), and the necessity of Supervised Fine-Tuning (SFT). - It introduces a scaling parameter \(\beta\) to control the strength of preference optimization for single-stage ORPO and ASFT methods, showing that incorporating an explicit SFT phase improves their performance, matching two-stage methods like DPO. - Through theoretical analysis, the paper reveals shared optimization dynamics between methods using odds ratios or reference-based rewards and demonstrates that the key factor influencing alignment quality is the use of pairwise or pointwise objectives rather than specific implicit reward formulations. - Experimental results on various datasets (UltraChat, UltraFeedback, Reddit TL;DR) and models (Llama 3.1 8B, Llama 3.2 3B) demonstrate that pairwise methods generally outperform pointwise methods, particularly with larger models, and even a small fraction (5-10%) of SFT data can yield substantial gains. - The study concludes that careful evaluation is crucial for understanding the differences between DAAs and optimizing LLM training pipelines, suggesting that pairwise methods with an SFT phase and tuned hyperparameters are most effective for alignment. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Process Reinforcement through Implicit Rewards](https://arxiv.org/abs/2502.01456) | Wendi Li, Zefan Wang, Lifan Yuan, hanbin, ganqu | - PRIME (Process Reinforcement through IMplicit rEwards) is a novel reinforcement learning (RL) framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) using dense token-level rewards. - PRIME leverages implicit process rewards, derived from an Implicit Process Reward Model (PRM), enabling online updates of the reward model using only outcome labels, which mitigates reward hacking and reduces computational costs compared to traditional methods. - The framework integrates token-level dense rewards and sparse outcome rewards into a Monte Carlo advantage function with a leave-one-out baseline, making it adaptable to various RL algorithms. - Experimental results on mathematical and coding reasoning benchmarks demonstrate that PRIME achieves a 15.1% average improvement over the supervised fine-tuning baseline and surpasses the state-of-the-art Qwen2.5-Math-7B-Instruct model on several key benchmarks using significantly less training data. - An ablation study shows that online PRM updating and the use of dense rewards lead to substantial gains in both sample efficiency and overall performance. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/PRIME-RL/PRIME) | N/A |
| [OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models](https://arxiv.org/abs/2502.01061) | Chao Liang, Zerong Zheng, Jiaqi Yang, Jianwen Jiang, Gaojie Lin | - OmniHuman, a Diffusion Transformer-based model, is introduced for generating realistic human animation videos conditioned on various modalities like text, audio, and pose. - The model uses an omni-conditions training strategy mixing motion-related conditions to scale up training data and improve generalization. - Two key training principles are introduced: stronger conditioned tasks leverage weaker ones for data scaling, and stronger conditions use lower training ratios to avoid overfitting. - OmniHuman outperforms existing methods in audio-driven portrait and body animation, supporting various aspect ratios and significantly improving gesture generation and object interaction. - It also handles various portrait contents, talking and singing, human-object interactions, and diverse image styles, demonstrating strong generalizability. | ['Text-to-Video', 'Audio-to-Audio', 'Computer Vision'] | N/A | N/A |
| [Preference Leakage: A Contamination Problem in LLM-as-a-judge](https://arxiv.org/abs/2502.01534) | Bohan Jiang, Ming Zhong, Yue Huang, Dawei Li, RLSNLP | - This paper introduces the concept of "preference leakage", a contamination problem in Large Language Model (LLM)-as-a-judge scenarios. - Preference leakage occurs when the LLM used for data generation and the LLM used for evaluation are closely related, leading to systematic bias in the evaluation. - The authors define three types of relatedness: being the same model, having an inheritance relationship, and belonging to the same model family. - Experiments across multiple LLM baselines and benchmarks demonstrate the existence of preference leakage and its impact on evaluation results. - The study reveals that preference leakage is a pervasive and challenging problem, especially for subjective evaluation criteria, hindering the development of reliable LLM-based evaluation systems. | ['Natural Language Processing'] | [Link](https://github.com/David-Li0406/Preference-Leakage) | N/A |
| [SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model](https://arxiv.org/abs/2501.18636) | Sensen Zhang, Zhiyu Li, Simin Niu, Xun Liang, UglyToilet | - Introduces SafeRAG, a benchmark designed to evaluate the security of Retrieval-Augmented Generation (RAG) systems. - Identifies four attack tasks: silver noise, inter-context conflict, soft ad, and white Denial-of-Service (DoS) to manipulate the knowledge used by RAG. - Constructs a manually curated dataset, SafeRAG, with LLM assistance, focusing on Chinese news domain to perform these attacks and evaluate RAG security. - Proposes an evaluation framework using retrieval accuracy and adapted F1 score variants combined with attack success rate (ASR) to assess RAG system vulnerability across different attack stages (indexing, retrieval, and generation). - Experimental results reveal significant vulnerabilities in existing RAG systems, with lighter LLMs showing better security in some cases than larger, more complex models such as GPT series and DeepSeek. | ['Question Answering'] | [Link](https://github.com/IAAR-Shanghai/SafeRAG) | N/A |
| [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068) | Jae-Joon Kim, Yulhwa Kim, jiwonsong, dongwonjo | - FastKV is a novel key-value (KV) cache compression method for LLMs that improves latency for long-context sequences by using a Token-Selective Propagation (TSP) approach and grouped-query attention (GQA)-aware KV cache compression. - TSP selectively propagates important tokens in deeper layers, leading to a 2.00x improvement in time-to-first-token and a 1.40× improvement in throughput compared to HeadKV. -  FastKV maintains accuracy comparable to baselines on long-context benchmarks, with less than a 1% accuracy gap. - FastKV's dual-strategy approach compresses KV caches by prioritizing important tokens identified by attention scores in early layers and using TSP to selectively propagate tokens in later layers. - The effectiveness of the approach is demonstrated through experiments on LLMs like LLaMA-3.1-8B-Instruct and Mistral-Nemo-12B-Instruct. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/dongwonjo/FastKV) | N/A |
| [Almost Surely Safe Alignment of Large Language Models at Inference-Time](https://arxiv.org/abs/2502.01208) | Jun Wang, Ilija Bogunovic, Matthieu Zimmer, Shyam Sundhar Ramesh, Xiaotong Ji | - This paper introduces InferenceGuard, a novel inference-time alignment method for Large Language Models (LLMs) that guarantees safe response generation with a probability approaching one. - It frames safe generation as a constrained Markov Decision Process (cMDP) within the LLM's latent space, augmenting a safety state to track constraint evolution and enable formal safety guarantees. - A critic is trained in the LLM's latent space for efficiency, using hidden state and logits as inputs to predict safety and task cost of generated text. - Empirically, InferenceGuard effectively balances safety and performance, achieving high safety rates (98.02% on Alpaca-7B and 100% on Beaver-7B-v3) while maintaining strong reward scores on the PKU-SafeRLHF dataset. - The method outperforms existing inference-time alignment techniques, demonstrating the efficacy of the safety augmentation approach in constrained MDPs. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/PKU-Alignment/alpaca-7b-reproduced), [Link](https://huggingface.co/argsearch/llama-7b-rm-float32), [Link](https://github.com/PKU-Alignment/safe-rlhf) |
| [DeepRAG: Thinking to Retrieval Step by Step for Large Language Models](https://arxiv.org/abs/2502.01142) | Yaojie Lu, Chunlei Xin, Fandong Meng, Jiali Zeng, xinyan233333 | - DeepRAG is a new framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP) to enable strategic and adaptive retrieval for Large Language Models (LLMs). - DeepRAG iteratively decomposes complex queries into subqueries and dynamically decides whether to retrieve external knowledge or use parametric reasoning at each step, improving retrieval efficiency and reducing noise. - Experiments on five open-domain QA datasets show DeepRAG improves answer accuracy by 21.99% compared to existing methods while also improving retrieval efficiency. - Further analysis demonstrates DeepRAG's strong correlation between retrieval decisions and parametric knowledge, indicating effective knowledge boundary calibration. - The framework includes two key components: *retrieval narrative*, which generates subqueries informed by previously retrieved data, and *atomic decisions*, determining the retrieval strategy for each subquery. | ['Question Answering'] | N/A | N/A |
| [The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles](https://arxiv.org/abs/2502.01081) | Soujanya Poria, Deepanway Ghosal, Yew Ken Chia, Vernon Y. H. Toh | - This paper investigates the multimodal reasoning capabilities of large language models (LLMs), particularly GPT-[n] and o-[n] series from OpenAI, on visual reasoning tasks using PUZZLEVQA and ALGOPUZZLEVQA datasets. - It tracks the evolution of reasoning performance across different model iterations and analyzes their performance on puzzles requiring visual perception, inductive reasoning, deductive reasoning, and algorithmic problem-solving. - The study demonstrates that o1 achieves the best performance in the multiple choice setting across PUZZLEVQA (79.2% accuracy) and ALGOPUZZLEVQA (55.3% accuracy), outperforming earlier models like GPT-4. - A key observation reveals a substantial gap between current artificial intelligence and human reasoning abilities, as even o1 struggles with simple multimodal puzzles requiring abstract reasoning. - The study also identifies visual perception as the primary bottleneck for all models, with significant performance improvements observed upon providing additional visual details, and further suggests the need for advancements in visual understanding for LLMs. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/declare-lab/LLM-PuzzleTest) | N/A |
| [ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning](https://arxiv.org/abs/2502.01100) | Radha Poovendran, Ashish Sabharwal, Kyle Richardson, ronanlb, yuchenlin | - This paper introduces ZebraLogic, a new benchmark dataset of 1,000 logic grid puzzles designed to evaluate the logical reasoning capabilities of Large Language Models (LLMs). - The dataset enables systematic control over puzzle complexity using metrics like search space size and Z3 conflict count, allowing researchers to study the scaling limits of LLMs in complex non-monotonic reasoning. - The study reveals a significant performance decline in LLMs as puzzle complexity increases, a phenomenon termed the "curse of complexity", which persists even with larger model sizes and enhanced training data.  - Scaling test-time compute through Best-of-N sampling and increasing the number of reasoning tokens offers some improvement in performance. - The paper finds that OpenAI's o1 models generate significantly more reasoning tokens than other LLMs, scaling them with puzzle complexity, and that backtracking mechanisms are crucial for complex reasoning. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://hf.co/spaces/WildEval/ZebraLogic) |
| [Lifelong Sequential Knowledge Editing without Model Degradation](https://arxiv.org/abs/2502.01636) | Thomas Hartvigsen, Ahmed Alaa, Maochuan Lu, Phudish Prateepamornkul, akshat57 | - This paper introduces ENCORE, a novel method for large-scale sequential knowledge editing in LLMs that mitigates model degradation. - ENCORE employs two key strategies: Most-Probable Early Stopping (MPES) to prevent overfitting on edited facts and a Frobenius-norm constraint to control disproportionate norm growth of the edited weight matrix. - The paper provides insights into the mechanics of locate-then-edit methods, revealing that they implicitly leverage norm growth to give edited layers more influence on the model's output, termed "importance hacking." - ENCORE successfully performs up to 10,000 sequential edits without significant performance degradation on downstream tasks, outperforming prior methods like ROME, MEMIT, and AlphaEdit. - Additionally, ENCORE exhibits improved efficiency, being 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B. | ['Natural Language Processing'] | [Link](https://github.com/scalable-model-editing/encore) | N/A |
| [Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637) | Pritish Kamath, Yangsibo Huang, Badih Ghazi, Edith Cohen, Da Yu | - SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is introduced as a method to extend input embedding layers, enhancing language model performance as layer size increases. - SCONE introduces embeddings for frequent n-grams (f-grams) alongside the original vocabulary, which provides contextualized representations during input embedding computation without affecting output embedding costs.  - These f-gram embeddings are learned using a separate model during training and precomputed for inference, minimizing latency impact by being stored in off-accelerator memory. - SCONE allows two scaling strategies: expanding the number of cached f-gram embeddings and increasing the f-gram model size, both while maintaining fixed inference FLOPS. - A 1B parameter model using SCONE outperforms a 1.9B parameter baseline with only half the inference FLOPS, demonstrating its efficiency across various corpora. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models](https://arxiv.org/abs/2502.01584) | Molly Q Feldman, Federico Cassano, Aleksander Boruch-Gruszecki, Joydeep Biswas, Carolyn Jane Anderson | - This paper introduces a new benchmark for reasoning models based on the NPR Sunday Puzzle Challenge, which requires general knowledge rather than specialized expertise. - The benchmark is challenging for both humans and current LLMs, with OpenAI's o1 model achieving the highest accuracy at 59%. - The puzzles are designed to have easily verifiable solutions and clear mistakes in model outputs, facilitating analysis. - Novel failure modes, including "giving up" and "thinking forever," are observed in certain reasoning models. - The research provides insights into the effective reasoning length for models like DeepSeek R1 and Gemini Thinking, proposing optimal token budgets for reasoning processes. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2025-02-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393) | Xiang Lisa Li, percyliang, swj0419, zitongyang, Muennighoff | - This paper introduces s1-32B, a language model fine-tuned for enhanced reasoning by using test-time scaling. - The model is trained on s1K, a curated dataset of 1,000 reasoning questions and solutions distilled from Google's Gemini. - Budget forcing, a novel technique to control test-time computation, improves the model's reasoning abilities allowing it to extrapolate beyond performance achieved without test-time intervention. - Evaluation on benchmarks such as AIME24, MATH500, and GPQA Diamond shows s1-32B's superior sample efficiency and competitive performance compared to existing models, including OpenAI's o1-preview. - The authors emphasize the importance of dataset curation using criteria like difficulty, diversity, and quality, as well as the effectiveness of budget forcing for test-time scaling. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/simplescaling/s1) | N/A |
| [Reward-Guided Speculative Decoding for Efficient LLM Reasoning](https://arxiv.org/abs/2501.19324) | doyensahoo, JunnanLi, hendrydong, yuhuixu, baohao | - This paper introduces Reward-Guided Speculative Decoding (RSD), a novel framework designed to enhance the efficiency of Large Language Model (LLM) inference, especially in reasoning tasks. - RSD combines a "draft" model with a "target" model and uses a process reward model to evaluate intermediate steps, dynamically deciding when to invoke the target model based on these rewards. - By selectively refining high-reward draft outputs, RSD minimizes computational costs compared to traditional speculative decoding which enforces strict unbiasedness. - Experimental results on reasoning benchmarks demonstrate significant computational savings (up to 4.4x fewer FLOPs) while improving accuracy compared to target-only or parallel decoding methods (up to +3.5 on average). - The paper explores different threshold-based criteria and weighting functions, highlighting the effectiveness of the approach across math and general LLMs for a range of reasoning tasks. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2501.18119) | Fangzhi Xu, Zhen Peng, Kai He, Tianzhe Zhao, Qika | - This paper proposes a two-stage framework called Self-Supervised Quantized Representation (SSQR) to seamlessly integrate Knowledge Graphs (KGs) with Large Language Models (LLMs). - The SSQR method compresses KG structural and semantic knowledge into discrete codes (tokens) through a self-supervised quantization process using a Graph Convolutional Network (GCN) encoder and vector quantization. - In the second stage, KG instruction-following data is created using the learned codes as input features, allowing LLMs to be fine-tuned for KG tasks like link prediction and triple classification. - Experimental results show that SSQR outperforms existing unsupervised quantized methods, and fine-tuned LLMs using SSQR achieve superior performance on KG link prediction and triple classification with fewer tokens per entity. - The learned quantized representations are more distinguishable and allow LLMs to better differentiate between entities in KGs. | ['Graph Machine Learning', 'Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming](https://arxiv.org/abs/2501.18837) | Primusa, euanong, sgoodfriend, jayelm, meg-tong | - This paper introduces Constitutional Classifiers, a new method for defending large language models (LLMs) against universal jailbreaks, which are prompting strategies that bypass model safeguards. - The approach involves training classifier safeguards on synthetic data generated using explicit constitutional rules that define permitted and restricted content categories. - This framework also employs data-augmentation techniques and leverages pool sets of benign data. - In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model. - On automated evaluations, enhanced classifiers showed strong defense against held-out, domain-specific jailbreaks while maintaining deployment viability with a small increase in production-traffic refusals and moderate inference overhead. | ['Natural Language Processing'] | N/A | N/A |
| [Trading Inference-Time Compute for Adversarial Robustness](https://arxiv.org/abs/2501.18841) | Sam Toyer, Stephanie Lin, Boaz Barak, Evgenia Nitishinskaya, Wojciech Zaremba | - This research investigates the impact of increased inference-time compute on the robustness of large language models (specifically OpenAI's 01-preview and 01-mini) against adversarial attacks. - Across various attacks, increased inference-time compute leads to improved robustness, often reducing the attack success rate to zero as computational resources increase, without explicit adversarial training. - The study introduces new attacks for reasoning models and explores scenarios where more compute does not guarantee reliability, and provides explanations. - The paper demonstrates that scaling inference-time compute, rather than pre-training compute, may be key to enhancing adversarial robustness in LLMs. - The analysis reveals limitations, such as the potential for adversaries to exploit policy ambiguities and introduce attacks like "Think Less" and "nerd sniping," highlighting the need for further research. | ['Natural Language Processing', 'Computer Vision'] | N/A | N/A |
| [INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation](https://arxiv.org/abs/2501.18753) | Shaogang Gong, Zixu Cheng, Jian Hu | - INT, a training-free, cycle-generation model, introduces instance-specific negative mining for task-generic promptable image segmentation, enhancing the accuracy of instance-specific prompts derived from generic prompts. - By comparing VLM outputs before and after masking image regions, INT iteratively refines prompts and masks, addressing challenges like camouflaged objects and medical images where traditional methods struggle. - INT outperforms existing point and scribble-supervised methods on datasets like COD10K, CHAMELEON, and CAMO in camouflaged object detection and achieves competitive results on medical image segmentation datasets like CVC-ColonDB, Kvasir, and ISIC using only a task-generic prompt, demonstrating effectiveness, robustness, and scalability. - The model involves splitting the image into patches, generating candidate prompts with a VLM, using an inpainting module to mask potential objects, and selecting the prompt with the highest output difference for segmentation using SAM. - Progressive negative mining refines prompts by accumulating scores from each iteration, normalizing differences for stability and excluding the influence of unstable changes caused by incorrect prompt predictions, improving segmentation accuracy in complex scenes where objects blend into their backgrounds. | ['Image Segmentation', 'Multimodal'] | N/A | N/A |
| [Unraveling the Capabilities of Language Models in News Summarization](https://arxiv.org/abs/2501.18128) | Göksel Biricik, odabashi | - This research paper benchmarks 20 different language models (LLMs), both large and small, on their news summarization capabilities using zero-shot and few-shot learning. - The study employs three commonly used news summarization datasets: CNN/Daily Mail, Newsroom, and XSum, and evaluates the models using automatic metrics (ROUGE, METEOR, BERTScore), human evaluation, and LLM-based evaluation. - The results indicate that while larger models like GPT-3.5-Turbo and Gemini generally outperform smaller models, certain smaller models, such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B, and Zephyr-7B-Beta, show competitive performance. - The paper finds that including demonstration examples in the few-shot learning setting does not necessarily improve performance and can even hinder it due to the sometimes low quality of the gold standard summaries in the datasets used. - The study highlights some common issues encountered with generated summaries, including early termination of text generation, redundancy and repetitive sequence generation, generation of prompts instead of task completions, inappropriate continuations and hallucinations and non-text outputs. | ['Summarization', 'Natural Language Processing'] | N/A | N/A |


## Papers for 2025-01-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492) | lakxtxue, JunXia97, zsf, HongchengGao, yueliu1998 | - This paper introduces GuardReasoner, a novel reasoning-based safeguard for Large Language Models (LLMs) designed to improve safety, explainability, and generalization of LLMs. - The GuardReasoner model architecture involves two stages: Reasoning Supervised Fine-tuning (R-SFT) to unlock reasoning capabilities and Hard Sample Direct Preference Optimization (HS-DPO) to refine reasoning, especially for ambiguous samples near decision boundaries. - GuardReasoner 8B outperforms GPT-40+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average across 13 benchmarks covering 3 guardrail tasks, demonstrating superior performance.  A new dataset, GuardReasonerTrain, consisting of 127K samples with 460K detailed reasoning steps, was created to facilitate training. - Explainability is enhanced by the provision of not only moderation results but also the intermediate reasoning process, enabling transparency in decision-making. Generalization is improved by employing reasoning steps, which allows the model to handle open-ended categories of harm, exceeding the constraints of manually defined categories. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/yueliu1999/GuardReasoner/) | N/A |
| [MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding](https://arxiv.org/abs/2501.18362) | Zhangren Chen, Yifei Li, Yuxin Zuo, stingning, lindsay-qu | - MedXpertQA is introduced as a challenging benchmark designed to evaluate expert-level medical knowledge and advanced reasoning capabilities in AI models. - It comprises two subsets: MedXpertQA Text for text-based evaluation and MedXpertQA MM for multimodal assessment, covering 17 medical specialties, 11 body systems, and 3 task categories. - The benchmark includes 4,460 multiple-choice questions derived from medical licensing exams and textbooks, with 2,005 multimodal questions accompanied by 2,839 diverse medical images. - A rigorous construction process involving data collection, filtering, augmentation, and expert review ensures high difficulty, diversity, and clinical relevance. - Initial evaluations of 16 leading large language and multimodal models on MedXpertQA reveal limited performance, especially in complex medical reasoning tasks, highlighting the benchmark's utility in driving further advancements in medical AI. | ['Question Answering', 'Multimodal'] | [Link](https://github.com/TsinghuaC3I/MedXpertQA) | N/A |
| [Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs](https://arxiv.org/abs/2501.18585) | yudian, freesunshine0316, zwhe99, Jiahao004, Dennis364 | - This paper identifies and analyzes "underthinking" in large language models (LLMs) like OpenAI's o1, where models frequently switch between reasoning paths without sufficient exploration, especially in challenging mathematical problems. - A novel metric is introduced to quantify underthinking by measuring the token efficiency in incorrect answers, comparing the number of tokens used before the first correct thought to the total number of tokens used. - A new decoding strategy called "Thought Switching Penalty (TIP)" is proposed to mitigate underthinking. This method discourages premature thought transitions by adding penalties to tokens associated with thought switching during decoding.   - Experiments on challenging datasets like MATH500, GPQA Diamond, and AIME demonstrate that TIP consistently improves the accuracy of the QwQ-32B-Preview model without requiring any model fine-tuning. - The findings contribute to a better understanding of reasoning inefficiencies in o1-like LLMs and offer a practical solution to improve their problem-solving capabilities by encouraging deeper, more focused exploration of reasoning paths. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding](https://arxiv.org/abs/2501.16411) | Vitor Guizilini, Daniel Seita, Jiageng Mao, Boyiliee, WeiChow | - This paper introduces PhysBench, a new benchmark designed to evaluate the physical world understanding of Vision-Language Models (VLMs) across diverse tasks, including physical object properties, relationships, scene understanding, and physics-based dynamics. - The benchmark consists of 10,002 interleaved video-image-text entries, revealing that current VLMs, particularly open-source ones, struggle with physical reasoning despite excelling in common-sense tasks. - To address this limitation, the authors propose PhysAgent, a novel framework integrating vision foundation models and a physics knowledge memory, enhancing VLMs' perceptual capabilities and providing physical world priors. - Experimental results show that PhysAgent improves GPT-40's zero-shot performance on PhysBench by 18.4%. - Further experiments with embodied agents on robotic manipulation tasks demonstrate that enhancing VLMs' physical world understanding through PhysBench fine-tuning and PhysAgent zero-shot assistance can facilitate more effective deployment in embodied AI applications, such as MOKA. | ['Multimodal', 'Visual Question Answering', 'Robotics'] | N/A | N/A |
| [Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch](https://arxiv.org/abs/2501.18512) | Zachary Charles, Satyen Kale, Keith Rush, Yanislav Donchev, Arthur Douillard | - This paper introduces Streaming DiLoCo, a distributed training algorithm for large language models (LLMs) that significantly reduces communication bandwidth requirements without sacrificing performance. - Streaming DiLoCo improves upon DiLoCo by synchronizing subsets of parameters sequentially, overlapping worker computation with communication, and quantizing exchanged gradients. - By combining these modifications, Streaming DiLoCo achieves similar quality to data parallelism and the original DiLoCo but with two orders of magnitude less bandwidth. - Experiments on language models ranging from 35 million to 4 billion parameters trained on the C4 dataset demonstrate comparable performance to Data-Parallel and DiLoCo. - Furthermore, overtraining experiments on the Dolma dataset with a 1 billion parameter model show Streaming DiLoCo performs slightly better than Data-Parallel despite using substantially less bandwidth. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training](https://arxiv.org/abs/2501.18511) | Chinmay Hegde, penfever | - This paper introduces WILDCHAT-50M, the largest public chat dataset comprising over 125 million chat transcripts generated by 50 different open-weight language models, ranging from 0.5B to 104B parameters. - The authors conduct a comparative analysis of model efficiency and response similarity, finding that model size and context window length strongly correlate with throughput and that LLM responses are surprisingly similar across different models. - They leverage WILDCHAT-50M to create RE-WILD, a novel data mixture for supervised fine-tuning (SFT), which outperforms the Tulu-3 SFT mixture with only 40% of the data. - The paper demonstrates that the choice of data generating model significantly impacts downstream benchmark performance and that blending different DGMs for SFT offers minimal benefit. - It concludes that data quality depends primarily on prompt diversity and that optimizing responses from a single high-quality DGM is more effective than blending multiple DGMs. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/penfever/wildchat-50m) | N/A |
| [o3-mini vs DeepSeek-R1: Which One is Safer?](https://arxiv.org/abs/2501.18438) | Miriam Ugarte, ssegura, japarejo, pablovalle, aitorarrieta | - This paper presents a systematic safety assessment of two Large Language Models (LLMs): DeepSeek-R1 (70b) and OpenAI's o3-mini (beta).  - The study leverages ASTRAL, an automated safety testing tool, to generate and execute 1260 unsafe test inputs across various safety categories, writing styles, and persuasion techniques. - The results indicate that DeepSeek-R1 exhibits a significantly higher rate of unsafe responses (11.98%) compared to o3-mini (1.19%). - Certain safety categories and writing styles were found to be more likely to trigger unsafe behaviors in DeepSeek-R1, while o3-mini's safety appeared less affected by specific categories or styles, potentially due to its policy violation feature. - The study concludes that OpenAI's latest LLMs demonstrate a stronger safety alignment compared to DeepSeek-R1. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Trust4AI/ASTRAL) | N/A |


## Papers for 2025-01-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate](https://arxiv.org/abs/2501.17703) | Xiang Yue, wenhu, ubowang | - This paper introduces Critique Fine-Tuning (CFT), a novel training strategy for Large Language Models (LLMs) that focuses on learning to critique responses rather than simply imitating correct ones. - CFT leverages the power of advanced LLMs such as GPT-40 to generate critiques for noisy responses, guiding the model towards deeper analysis and understanding. - CFT models consistently outperform SFT models across several benchmarks, demonstrating a 4-10% improvement on mathematical reasoning tasks while using significantly less data (50K samples vs. 2M+). - The effectiveness of CFT is shown across a variety of base models (Qwen2.5, DeepSeek-Math-7B) and datasets, showcasing its potential as an efficient training strategy. - CFT highlights that learning to critique offers a more effective way to train LLMs, promoting critical thinking and nuanced understanding often overlooked by standard SFT methods. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://arxiv.org/abs/2501.14334) | Simon Gosset, Caroline Vateau, Louis Ladan, Neyri56, clementdesroches | - This paper proposes a methodology to estimate the environmental impact of a company's AI portfolio, focusing on energy consumption and its multi-factor impacts like GHG emissions, water usage, and resource depletion. - The study finds that large generative AI models consume significantly more energy (up to 4600x) than traditional models and that energy consumption scales substantially with model size and workflow complexity. - The paper projects AI electricity use up to 2030 under various scenarios, with a potential 24.4x increase in a high-adoption scenario driven by widespread Generative AI and agents. - The authors advocate for standardized environmental assessment frameworks, greater transparency from AI providers, and a "Return on Environment" metric to align AI development with sustainability goals. - The paper explores the feasibility of achieving a 90% GHG reduction by 2030 through improvements in PUE, energy mix decarbonization, and hardware efficiency and proposes an environmental scoring label for AI similar to eco-scores in other industries. | ['Natural Language Processing', 'Other'] | N/A | N/A |
| [Atla Selene Mini: A General Purpose Evaluation Model](https://arxiv.org/abs/2501.17195) | Kyle Dai, Jackson Golden, Henry Broomfield, Andrei Alexandru, NinaCalvi | - Atla Selene Mini is a new small language model as a judge (SLMJ), designed for general purpose evaluation tasks. - It outperforms existing SLMJ and GPT-40-mini on 11 out-of-distribution benchmarks covering absolute scoring, classification, and pairwise preference tasks. - The model leverages a curated dataset augmented with synthetic critiques and a combined Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) training approach. - Selene Mini excels in real-world scenarios, demonstrating strong zero-shot agreement with human expert evaluations on financial and medical datasets and robustness to variations in prompt format.  - It also leads in a live community-driven Judge Arena benchmark, further highlighting its robust evaluation capabilities. | ['Natural Language Processing'] | N/A | [Link](https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) |
| [Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation](https://arxiv.org/abs/2501.17749) | Miriam Ugarte, ssegura, japarejo, pablovalle, aitorarrieta | - This research paper details the pre-deployment safety testing of OpenAI's o3-mini large language model (LLM) using the ASTRAL tool. - ASTRAL automatically generates unsafe test inputs categorized by style and persuasion technique across 14 safety areas. - A total of 10,080 tests were generated and executed, with 87 confirmed instances of unsafe behavior after manual verification. - The o3-mini model demonstrated improved safety compared to older OpenAI models, possibly due to integrated policy violation detection. - Key areas of concern still exist, particularly surrounding recent controversial topics and specific categories like terrorism and child abuse. | ['Natural Language Processing'] | [Link](https://github.com/Trust4AI/ASTRAL) | N/A |
| [Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation](https://arxiv.org/abs/2501.17433) | ling1119, sftekin25, tawreos, SihaoHu, TianshengHuang | - This paper introduces "Virus," a novel attack method targeting Large Language Models (LLMs) that bypasses guardrail moderation systems designed to prevent harmful fine-tuning. - Virus utilizes a dual-objective optimization approach to craft harmful training data, minimizing detectability by the guardrail while maximizing the degradation of the LLM's safety alignment. - Experimental results demonstrate Virus's effectiveness, achieving up to 100% leakage ratio (bypassing the guardrail) and increasing harmful scores by up to 21.8% compared to standard harmful fine-tuning attacks. - The key finding is that relying solely on guardrail moderation is insufficient for mitigating harmful fine-tuning risks, highlighting the need for more robust defense mechanisms. - The optimized datasets generated by Virus are publicly available for further research and analysis. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/git-disl/Virus) | [Link](https://huggingface.co/datasets/anonymous4486/Virus) |


## Papers for 2025-01-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training](https://arxiv.org/abs/2501.17161) | Saining Xie, Shengbang Tong, Jihan Yang, Yuexiang Zhai, Tianzhe Chu | - This paper compares the effects of supervised fine-tuning (SFT) and reinforcement learning (RL) on the generalization and memorization abilities of foundation models. - The authors introduce a new arithmetic reasoning card game called GeneralPoints and utilize the existing V-IRL visual navigation environment to evaluate model performance across textual and visual domains. - Their findings reveal that RL, particularly when trained with outcome-based rewards, generalizes better to unseen variants than SFT, which tends to memorize the training data.  - Interestingly, SFT is found to be helpful in stabilizing the model's output format for subsequent RL training, which allows RL to achieve its performance gains. - The research highlights the advantages of RL in acquiring generalizable knowledge for complex, multimodal tasks. | ['Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2501.17116) | Guoshuai Zhao, Xiao Liu, Yeyun Gong, Ruizhe Wang, cp5555 | - This paper introduces the first FP4 training framework for large language models (LLMs), addressing the challenges of significant quantization errors and limited representational capacity. - Two key innovations are presented: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. - The framework integrates a mixed-precision training scheme and vector-wise quantization to ensure stability. - Experimental results demonstrate that the FP4 framework achieves accuracy comparable to BF16 and FP8, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. - The authors conclude that their framework sets a foundation for efficient ultra-low precision training, particularly with the emergence of next-generation hardware supporting FP4. | ['Natural Language Processing'] | N/A | N/A |
| [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/abs/2501.16975) | Ya Wang, Yutao Zeng, Banggu Wu, Defa Zhu, Hongzhi Huang | - This paper introduces Over-Tokenized Transformers, a framework that improves language modeling performance by decoupling input and output vocabularies and scaling up the input vocabulary with multi-gram tokens. - A log-linear relationship is observed between the input vocabulary size and training loss, showing that larger input vocabularies improve performance regardless of model size. - Using a large input vocabulary, the proposed method matches the performance of a model 2.5x larger (400M model matches 1B model) on the OLMo2 benchmark at no additional training cost. -  Over-encoding (OE) leverages hierarchical n-gram input vocabulary and over-decoding (OD) utilizes a large output vocabulary with multi-token prediction, which can be beneficial for large models. - The integration of OE and OD results in Over-Tokenized Transformer (OT), demonstrating further performance improvements. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496) | Jeff Wu, Jack Lindsey, Joshua Batson, Lee Sharkey, bilalchughtai | - This review paper discusses open problems in mechanistic interpretability, a field aiming to understand the computational mechanisms behind neural networks' abilities, particularly in natural language processing. - The authors categorize open problems into methods and foundations (reverse engineering, concept-based interpretability, proceduralization and automation), applications (monitoring, control, predictions, inference/training, microscope AI, and model/modality ranges), and socio-technical aspects. -  Key challenges include developing better decomposition methods beyond sparse dictionary learning, improving validation techniques, and connecting interpretability research to concrete engineering and scientific goals. - The paper also touches on the potential of mechanistic interpretability in AI governance, including risk assessment, policy development, and addressing social and philosophical implications. - This review focuses specifically on suggesting potential future research directions for the field, rather than simply summarizing the current state of mechanistic interpretability. | ['Natural Language Processing'] | N/A | N/A |
| [IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding](https://arxiv.org/abs/2501.15747) | Nikunj Kotecha, Ashutosh Kumar, Sankalp KJ, amanchadha, laxmaanb | - IndicMMLU-Pro, a comprehensive benchmark for evaluating large language models (LLMs) across nine major Indic languages, is introduced.  - The benchmark covers a wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the complexities of Indic languages. - IndicMMLU-Pro's design principles, task taxonomy, and data collection methodology are detailed, and baseline results from state-of-the-art multilingual models are presented. - The benchmark is publicly available, aiming to contribute to advancements in Indic language-based technologies. - The results reveal that GPT-4 consistently outperforms other models across all Indic languages, highlighting the importance of both scale and specialized training for Indic languages. | ['Natural Language Processing', 'Translation', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/LinguaLift/IndicMMLU-Pro) |
| [Low-Rank Adapters Meet Neural Architecture Search for LLM Compression](https://arxiv.org/abs/2501.16372) | Nilesh Jain, Jinjie Yuan, J. Pablo Muñoz | - This paper introduces a novel approach to compressing Large Language Models (LLMs) by combining low-rank adapters with Neural Architecture Search (NAS) techniques. - The method, called LoNAS, uses elastic low-rank adapters that can dynamically adjust their configurations during fine-tuning, improving efficiency. - LoNAS achieves competitive results compared to traditional LoRA, reducing the total number of parameters by up to 20% while maintaining similar accuracy. - Further enhancements, such as Shears and SQFT, build upon LoNAS to address challenges in handling sparse or low-precision models and improve efficiency even further. - The results demonstrate significant improvements in inference speedup (up to 1.4x) and compression, making LLMs more accessible for resource-constrained environments. | ['Natural Language Processing'] | [Link](https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning) | N/A |
| [Histoires Morales: A French Dataset for Assessing Moral Alignment](https://arxiv.org/abs/2501.17117) | Charlotte Laclau, Julien Velcin, Antoine Gourru, Irina Proskurina, Thibaud Leteno | - This paper introduces HISTOIRESMORALES, a new French dataset for evaluating the moral alignment of large language models (LLMs). - The dataset is created by translating and refining the MORALSTORIES dataset, ensuring grammatical accuracy and cultural relevance for the French context. - HISTOIRESMORALES includes annotations of moral values to align with French norms, covering diverse social situations. - Preliminary experiments reveal that while LLMs generally align with human moral norms, their alignment is easily influenced by user preferences, demonstrating a lack of robustness. - The authors demonstrate how their dataset can be used to investigate the alignment of LLMs with human moral norms and the impact of language on this alignment. | ['Natural Language Processing', 'Translation'] | [Link](https://github.com/upunaprosk/histoires-morales) | [Link](https://hf.co/datasets/LabHC/histoires_morales) |


## Papers for 2025-01-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Baichuan-Omni-1.5 Technical Report](https://arxiv.org/abs/2501.15368) | Song Chen, Tao Zhang, Tao Zhang, Jun Liu, AdamLee1 | - Baichuan-Omni-1.5 is an open-source omnimodal model with end-to-end audio generation capabilities, utilizing a visual branch (NaViT based), audio branch (RVQ tokenizer and flow matching decoder), and a pretrained LLM backbone. - The model is trained using a three-stage strategy, focusing on image-text pretraining, image-audio-text pretraining, and multimodal fine-tuning using a 500B token dataset. - It claims to outperform leading open-source omnimodal models like VITA-1.5 and MiniCPM-0 2.6 on various tasks, including image, video, and audio understanding, and even surpasses proprietary models like GPT-40-mini on certain benchmarks. - On MMLU, it achieves 72.2% accuracy, and on OpenMM-Medical, it reaches 83.8% with a 7B LLM, exceeding Qwen2-VL-72B's 80.7%. - An audio tokenizer is designed for semantic and acoustic information capture, supporting controllable bilingual real-time conversations and general multimodal understanding. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Text-to-Audio', 'Text-to-Speech', 'Text-to-Video', 'Visual Question Answering', 'Video Classification'] | [Link](https://github.com/baichuan-inc/Baichuan-Omni-1.5) | N/A |
| [Qwen2.5-1M Technical Report](https://arxiv.org/abs/2501.15383) | Fei Huang, Dayiheng Liu, Chengyuan Li, Bowen Yu, An Yang | - This paper introduces Qwen2.5-1M, a series of large language models (LLMs) that extend the context length to 1 million tokens, including open-source models Qwen2.5-7B/14B-Instruct-1M and an API-accessible model, Qwen2.5-Turbo. - Qwen2.5-1M employs several key techniques to enhance long context capabilities, such as long data synthesis, progressive pretraining, and multi-stage supervised fine-tuning. - It also includes a novel inference framework with training-free length extrapolation and sparse attention to reduce costs. - Evaluation shows significant improvement of Qwen2.5-1M on long-context tasks, sometimes outperforming GPT-4, while retaining comparable performance on short-context tasks to 128k versions. - Qwen2.5-Turbo stands out with faster inference speed and lower cost for long sequences, offering a practical solution for various real-world scenarios requiring extended contexts. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer](https://arxiv.org/abs/2501.15570) | Peter Yue, Li Zhiyuan, Lin Yueyu, xiaol | - This paper introduces ARWKV, a series of RNN-based language models distilled from Qwen 2.5, utilizing pure native RWKV-7 attention. - The architecture aims to enhance the expressiveness of RNNs and demonstrate state-tracking capabilities beyond transformers, replacing the self-attention mechanism in transformers with the RWKV-7 time mixing module. - The distillation process reduces the training time and resource requirements, enabling a 7B parameter model to be trained on a single A100 80G GPU, compared to the vast resources needed for training Qwen 2.5's 18 trillion tokens. - Initial experiments with a 7B model demonstrate competitive performance in benchmarks after stage 2 training.  - The key innovation is to replace transformer's self-attention with RWKV time mixing module | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/yynil/RWKVInside) | [Link](https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1) |
| [Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation](https://arxiv.org/abs/2501.15907) | Yicheng Gu, Xuyuan Li, Chaoren Wang, Zengqiang Shang, Haorui He | - This paper introduces Emilia, a large-scale, multilingual, and diverse dataset for speech generation derived from in-the-wild speech data. - Emilia comprises over 101k hours of speech across six languages (English, Chinese, German, French, Japanese, and Korean) and is expanded to 216k hours with Emilia-Large, making it the largest open-source speech generation dataset available. - The paper also introduces Emilia-Pipe, an open-source preprocessing pipeline used to create the dataset, which standardizes, separates sources, diarizes speakers, segments by VAD, performs ASR, and filters data. - Experiments demonstrated that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous, human-like speech, capturing diverse speaker timbres and speaking styles. - Further experiments validated the importance of scaling dataset size and showcased the effectiveness of Emilia for multilingual and crosslingual speech generation. | ['Audio', 'Text-to-Speech'] | [Link](https://github.com/open-mmlab/Amphion/tree/main/preprocessors/Emilia) | [Link](https://huggingface.co/datasets/amphion/Emilia-Dataset) |
| [Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity](https://arxiv.org/abs/2501.16295) | Luke Zettlemoyer, Ning Dong, Genghan Zhang, Junhong Shen, Weixin Liang | - Mixture-of-Mamba (MoM) introduces modality-aware sparsity to State Space Models (SSMs) by incorporating modality-specific parameterization within the Mamba block, enhancing multi-modal pretraining. - MoM dynamically selects modality-specific weights in each processing component, enabling efficient handling of diverse data types while preserving SSM computational benefits. - Across three multi-modal settings (Transfusion, Chameleon, and a three-modality setup), MoM consistently achieves equivalent or better loss values at significantly reduced computational costs, outperforming dense baselines like Mamba Dense and Flex-Attention Transformer. - In Transfusion, MoM matches image loss with 34.76% of the FLOPs at the 1.4B scale; in Chameleon, it reaches similar image loss with 42.50% and text loss with 65.40% of the FLOPs; and in the three-modality setting, it matches speech loss with just 24.80% of the FLOPs. - An ablation study reveals synergistic effects from decoupling projection components, with joint decoupling yielding larger performance gains than individual modifications, establishing modality-aware sparsity as an effective design principle for both SSMs and Transformers. | ['Multimodal'] | [Link](https://github.com/Weixin-Liang/Mixture-of-Mamba) | N/A |
| [Feasible Learning](https://arxiv.org/abs/2501.14912) | Meraj Hashemizadeh, Jose Gallego-Posada, Juan Elenter, Ignacio Hounie, Juan Ramirez | - This paper introduces Feasible Learning (FL), a sample-centric learning paradigm that trains models by solving a feasibility problem, ensuring bounded loss for each training sample, as opposed to optimizing for average performance like Empirical Risk Minimization (ERM). - The authors use a primal-dual optimization approach, dynamically re-weighting sample importance based on fitting difficulty. - They also introduce Resilient Feasible Learning (RFL) to address potential infeasibility issues by incorporating slack variables. - Empirical analysis on image classification, age regression, and large language model preference optimization shows comparable average performance to ERM but improved tail behavior, with fewer high-loss samples. - The results suggest that RFL's concentrated loss distribution is particularly beneficial when consistent performance across all data points is crucial. | ['Image Classification', 'Natural Language Processing', 'Text2Text Generation', 'Computer Vision'] | [Link](https://github.com/juan43ramirez/feasible-learning) | [Link](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs), [Link](https://huggingface.co/stabilityai/stablelm-zephyr-3b) |


## Papers for 2025-01-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Humanity's Last Exam](https://arxiv.org/abs/2501.14249) | Josephina Hu, Nathaniel Li, Ziwen Han, Alice Gatti, Long Phan | - This paper introduces HUMANITY'S LAST EXAM (HLE), a challenging multi-modal benchmark designed to assess the advanced academic capabilities of large language models (LLMs). - HLE comprises 3,000 multi-modal, multiple-choice, and exact-match questions across various subjects, emphasizing complex mathematics problems, and aims to be the final closed-ended academic benchmark of its kind. - The benchmark creation involved a rigorous process of expert contribution, LLM difficulty checks, and a multi-stage review process to ensure high quality and difficulty. - Initial evaluations demonstrate that state-of-the-art LLMs perform poorly on HLE and exhibit poor calibration, indicating significant room for improvement. - HLE's public release aims to provide a robust tool for researchers and policymakers to evaluate AI progress and inform discussions about AI development and governance. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/abs/2501.13953) | Chunyi Li, Xiangyu Zhao, Zicheng Zhang, KennyUTC, nebulae09 | - This paper introduces a framework for evaluating redundancy in Multimodal Large Language Model (MLLM) benchmarks, addressing the issue of numerous benchmarks with overlapping capabilities. - The framework quantifies redundancy across three key perspectives: dimensions (intra-benchmark), instances (intra-benchmark), and cross-benchmarks within specific domains. - It uses performance correlation to measure redundancy, leveraging data from VLMEvalKit, a comprehensive dataset containing results from diverse benchmarks and over 100 MLLMs. - The study reveals significant redundancy in many existing benchmarks, suggesting opportunities for optimization by reducing redundant dimensions and instances. - The paper concludes with recommendations for constructing more efficient and effective MLLM benchmarks, including guidance on redundancy checks during the benchmark design process. | ['Multimodal'] | N/A | [Link](https://huggingface.co/datasets/VLMEval/OpenVLMRecords) |
| [Chain-of-Retrieval Augmented Generation](https://arxiv.org/abs/2501.14342) | Zhicheng Dou, Xiaolong Huang, Nan Yang, Haonan Chen, Liang Wang | - This paper introduces CoRAG (Chain-of-Retrieval Augmented Generation), a novel approach for training RAG models that retrieve and reason over information step-by-step before generating answers. - Unlike conventional RAG methods that perform a single retrieval step, CoRAG allows dynamic query reformulation based on the model's evolving state, enhancing its effectiveness in handling complex queries. - CoRAG is trained effectively using rejection sampling to generate intermediate retrieval chains, augmenting existing RAG datasets and improving the model's ability to learn effective retrieval strategies. - Experiments across multiple benchmarks show CoRAG outperforming strong baselines, particularly in multi-hop question answering tasks, where it achieves more than 10 points improvement in EM score. - CoRAG establishes a new state-of-the-art performance on the KILT benchmark across various knowledge-intensive tasks, demonstrating its superior ability to handle diverse knowledge-intensive tasks. | ['Question Answering'] | N/A | N/A |
| [RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques](https://arxiv.org/abs/2501.14492) | Ruoyu Sun, Tian Ding, Zhenyang Xiao, Ziniu Li, Zhengyang Tang | - The paper introduces RealCritic, a novel benchmark designed to assess the critique capabilities of large language models (LLMs) in a closed-loop manner. - Unlike existing benchmarks that evaluate critiques in isolation, RealCritic evaluates critique quality based on the effectiveness of the generated corrections. - RealCritic incorporates various critique settings, including self-critique, cross-critique, and iterative critique, providing a comprehensive evaluation. - The benchmark is implemented using eight challenging reasoning tasks across mathematical reasoning and multiple-choice question domains. - Experiments show that reasoning-based models outperform classical LLMs, particularly in self-critique settings, emphasizing the value of a closed-loop evaluation approach. | ['Natural Language Processing'] | [Link](https://github.com/tangzhy/RealCritic) | N/A |


## Papers for 2025-01-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Improving Video Generation with Human Feedback](https://arxiv.org/abs/2501.13918) | Ziyang Yuan, Jiajun Liang, Gongye Liu, Xintao, jieliu | - This paper introduces a new framework for aligning text-to-video (T2V) generation models with human preferences using reinforcement learning from human feedback (RLHF). - A new 182k-example multi-dimensional human preference dataset focused on modern video generation models is constructed, along with VideoReward, a multi-dimensional video reward model. - Three new alignment algorithms for flow-based video generation models are derived: Flow-DPO, Flow-RWR (training-time algorithms), and Flow-NRG (inference-time algorithm).  - Flow-DPO outperforms standard supervised fine-tuning and Flow-RWR on automatic and human preference evaluations when the KL-divergence parameter \(\beta\) is fixed.  - Flow-NRG enables personalized video generation by allowing users to adjust weights for multiple alignment objectives during inference. | ['Text-to-Video', 'Reinforcement Learning', 'Multimodal'] | [Link](https://gongyeliu.github.io/videoalign) | N/A |
| [Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models](https://arxiv.org/abs/2501.13629) | hanglics, yegong, lx865712528, tzh94588, Lin0 | - SIGMA, a new large language model specializing in the system domain, is introduced, featuring DiffQKV attention for enhanced inference efficiency. - DiffQKV differentially optimizes Query, Key, and Value components: using compressed K and V and augmented Q, balancing performance and efficiency. - SIGMA is pre-trained on 6 trillion tokens, including 19.5 billion system domain data and 1 trillion synthesized/rewritten data. - In general domains, SIGMA's performance is comparable to state-of-the-art models. - On AIMICIUS, a new system domain benchmark, SIGMA significantly outperforms existing models, including GPT-4, with up to a 52.5% absolute improvement. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Temporal Preference Optimization for Long-Form Video Understanding](https://arxiv.org/abs/2501.13919) | Zeyu Wang, yeunglevy, yuhuizhang, nicholswang, ruili0 | - This paper introduces Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video Large MultiModal Models (LMMs). - TPO leverages preference learning at two granularities: localized temporal grounding, focusing on specific video segments, and comprehensive temporal grounding, addressing broader temporal dependencies. - By curating preference data at these two levels, TPO trains video-LMMs to differentiate between temporally grounded and ungrounded responses, improving their ability to capture nuanced temporal relationships in videos. - Experiments on LongVideoBench, MLVU, and Video-MME benchmarks demonstrate significant performance improvements with TPO across two state-of-the-art video-LMMs (LongVA-7B and LLaVA-Video-7B). - Notably, LLaVA-Video with TPO achieves state-of-the-art results on Video-MME among 7B models, highlighting the effectiveness of TPO in enhancing long-form video understanding. | ['Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models](https://arxiv.org/abs/2501.13920) | lzyhha, JackyZhuo, RuoyiDu, Afeng-x, jyjyjyjy | - IMAGINE-E, a comprehensive evaluation framework, is introduced to benchmark text-to-image (T2I) models across five key domains. - Six prominent models, including FLUX.1 and Ideogram2.0, were evaluated on tasks related to structured output generation, realism and physical consistency, specific domain generation, challenging scenarios, and multi-style creation. - FLUX.1 and Ideogram2.0 show superior performance, especially in structured and specific domain tasks. - The evaluation reveals that existing evaluation frameworks need to be improved to better assess these advanced models and that T2I models show progress towards becoming foundational AI tools. - The evaluation also highlights ongoing limitations in complex areas like 3D and code generation. | ['Text-to-Image', 'Multimodal'] | [Link](https://github.com/jylei16/Imagine-e) | N/A |
| [Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback](https://arxiv.org/abs/2501.10799) | spermwhale, yunhe, sainbar, jindi, yentinglin | - This paper introduces Step-KTO (Stepwise Kahneman-Tversky-inspired Optimization), a novel training framework designed to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). - Step-KTO integrates both process-level and outcome-level binary feedback signals to guide LLMs in generating not only correct final answers but also logically sound intermediate reasoning steps. - By incorporating a Kahneman-Tversky-inspired value function, Step-KTO prioritizes correctness and coherence in the reasoning process. - Experimental results on benchmark mathematical reasoning datasets demonstrate that Step-KTO surpasses existing state-of-the-art methods, achieving a notable improvement in accuracy (e.g., 63.2% Pass@1 on MATH-500 vs. 53.4% for the baseline model) alongside producing more reliable intermediate solutions. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Debate Helps Weak-to-Strong Generalization](https://arxiv.org/abs/2501.13124) | Yongbin-Li, hzhwcmhf, langnick | - This paper proposes a novel approach to improve weak-to-strong generalization in natural language processing by leveraging debate between two large language models (LLMs). - The debate mechanism extracts trustworthy information from the LLMs, which is then used to train a better weak supervisor. - An ensemble of weak models is employed to process the long arguments generated during the debate, leading to more robust supervision estimates. - The proposed combination of scalable oversight and weak-to-strong generalization approaches results in improved alignment on OpenAI's weak-to-strong NLP benchmarks. - Experimental results show that the debate-enhanced weak supervision significantly outperforms baseline approaches in terms of performance gap recovered (PGR) and test accuracy on various question-answering datasets, including SciQ, BoolQ, CosmosQA, and AnthropicHH. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Evolution and The Knightian Blindspot of Machine Learning](https://arxiv.org/abs/2501.13075) | Tarin Ziyaee, Kenneth O. Stanley, Tarek El-Gaaly, ekmeyerson, jal278 | - This paper argues that machine learning (ML), and reinforcement learning (RL) in particular, overlooks the critical aspect of robustness to Knightian uncertainty (KU), or unknown unknowns, which is essential for general intelligence in open worlds. - By contrasting RL with biological evolution, the authors highlight how evolution's mechanisms, such as diversification, adaptation to novelty, and persistence as a filter for robustness, enable it to thrive in open-ended, unpredictable environments, unlike current RL agents, which struggle with out-of-distribution scenarios. - The paper identifies specific limitations in RL's core formalisms, including closed-world assumptions in MDPs, fixed time horizons in reward functions, episodic boundaries, and the treatment of training data as timeless, arguing that these limitations contribute to RL's blindness to KU.  - The authors suggest that incorporating principles from evolution, such as open-endedness, artificial life, and revisiting core RL formalisms, might help address KU and lead to more robust AI. - The implications of KU for foundation models, RLHF, AI safety, and potential pathways to integrate KU into RL algorithms are discussed. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos](https://arxiv.org/abs/2501.13826) | ZhangYuanhan, wangxiao1208, pufanyi, craigwu, KairuiHu | - Introduces Video-MMMU, a benchmark designed to evaluate large multimodal models' (LMMs) ability to acquire and apply knowledge from professional educational videos. - The benchmark includes 300 videos spanning six disciplines, each accompanied by three question-answer pairs aligned with Bloom’s Taxonomy: Perception, Comprehension, and Adaptation. - A new metric, called *Δknowledge*, is proposed to quantify the models' performance improvement on practice exam questions after viewing a video. - Evaluation results of several LMMs revealed a progressive decline in model performance as the cognitive level increases from perception to comprehension to adaptation. - Analysis reveals that even top-performing models like Claude-3.5-Sonnet exhibit significant performance decline in complex scenarios, highlighting the limitations of current models in adapting video-based knowledge to solve real-world problems. | ['Multimodal', 'Question Answering', 'Video-Text-to-Text'] | N/A | N/A |


## Papers for 2025-01-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) | AS-7, haha-point, freesky, DejianYang, guoday | - This paper introduces DeepSeek-R1, a large language model designed for enhanced reasoning capabilities, along with DeepSeek-R1-Zero, a model trained purely via reinforcement learning (RL) without supervised fine-tuning. - DeepSeek-R1-Zero showcases emergent reasoning abilities but suffers from readability and language mixing issues, prompting the development of DeepSeek-R1, which incorporates a multi-stage training pipeline with cold-start data and RL fine-tuning. - DeepSeek-R1 achieves comparable performance to OpenAI-01-1217 on several reasoning benchmarks and outperforms DeepSeek-V3. - Through knowledge distillation from DeepSeek-R1 to smaller models, even a 7B model surpasses existing open-source models on several benchmarks, and 32B and 70B models set new performance records. - The paper releases a series of distilled models based on Qwen and Llama. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces](https://arxiv.org/abs/2501.12909) | Senbao Shi, Li-Zhouyi, PigCatchingExpert, longyuewang, imryanxu | - FilmAgent, a novel LLM-based multi-agent collaborative framework, automates end-to-end virtual film production, encompassing idea development, scriptwriting, cinematography, and actor actions within pre-built Unity 3D environments. - Employing two novel multi-agent collaboration strategies, Critique-Correct-Verify and Debate-Judge, the framework leverages LLMs as virtual crew members (director, screenwriter, actors, cinematographer) to enhance communication and refine film elements iteratively. - Human evaluations across 15 film ideas demonstrate FilmAgent's superiority, achieving an average score of 3.98 out of 5, significantly outperforming single-agent baselines and even surpassing OpenAI's larger reasoning model, o1, in a multi-agent setting. - Compared with OpenAI's text-to-video model Sora, FilmAgent exhibits stronger storytelling and coherence in longer videos, owing to the pre-designed 3D spaces and character interactions, while Sora showcases greater adaptability and stylistic flexibility but lacks consistency and physics compliance. - The framework addresses limitations in existing automated film production methods by incorporating communication-driven collaboration, offering a promising approach to end-to-end film automation using AI agents. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback](https://arxiv.org/abs/2501.12895) | Yu Cheng, linjieli222, Xiaoye08, huxy912, yaful | - Test-Time Preference Optimization (TPO) is introduced, a framework that aligns Large Language Model (LLM) outputs with human preferences during inference without retraining. - TPO translates reward signals into textual critiques and utilizes them as rewards to iteratively refine LLM responses, unlike methods relying solely on numerical rewards. - Evaluations across various NLP tasks demonstrate that TPO progressively improves alignment with human preferences, and in some cases, the unaligned LLM with TPO surpasses its aligned counterpart. - TPO efficiently scales with both search width and depth during inference. - Case studies illustrate TPO's ability to exploit LLMs' capacity to interpret and act on reward signals, making it a practical, lightweight alternative for on-the-fly preference optimization. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/yafuly/TPO) | N/A |
| [VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding](https://arxiv.org/abs/2501.13106) | Sicong, Guanzheng, Zhiqiang007, ClownRat, CausalLi | - VideoLLaMA3 is a multimodal foundation model for image and video understanding that leverages a vision-centric training paradigm and framework design.  - The model architecture incorporates an any-resolution vision tokenization (AVT) technique, enabling it to handle varying input resolutions, and a differential frame pruner (DiffFP) for efficient video compression. - VideoLLaMA3 is trained in four stages: vision encoder adaptation, vision-language alignment, multi-task fine-tuning, and video-centric fine-tuning.  - The model outperforms existing state-of-the-art models on a variety of benchmarks, demonstrating strong abilities in image and video comprehension, including chart and document understanding, mathematical reasoning, temporal reasoning, and grounding.  - Notably, VideoLLaMA3 demonstrates significant improvements in chart understanding and vision-related math problem solving in image understanding, as well as achieving state-of-the-art performance in multiple video understanding benchmarks. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text', 'Video Classification'] | [Link](https://github.com/DAMO-NLP-SG/VideoLLaMA3) | N/A |
| [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599) | ChonghuaLiao, DuChenZhuang, shelowize, xingbowei, KbsdJames | - Kimi k1.5 is a multimodal large language model (LLM) trained using reinforcement learning (RL) with a focus on scaling context length for improved reasoning abilities. - The model architecture is based on a Transformer decoder and incorporates techniques like partial rollouts, improved policy optimization, and length penalty for efficient RL training. - It achieves state-of-the-art results on multiple benchmarks, including 77.5 on AIME, 96.2 on MATH 500, and 94-th percentile on Codeforces, matching OpenAI's GPT-01 on MathVista. - Long2short methods are introduced to improve short-context models by transferring knowledge from long-context models, resulting in significantly improved token efficiency, for example, k1.5-short w/ rl achieves a Pass@1 score of 60.8 on AIME2024 while utilizing only 3,272 tokens on average. | ['Multimodal', 'Reinforcement Learning', 'Question Answering', 'Text2Text Generation'] | N/A | N/A |
| [Autonomy-of-Experts Models](https://arxiv.org/abs/2501.13074) | Yining Qian, kangzhanhui, shwu, Ruobing-Xie, AngLv | - This paper introduces Autonomy-of-Experts (AoE), a novel Mixture-of-Experts (MoE) paradigm for large language models. - AoE allows experts to autonomously decide whether to process inputs based on the scale of their internal activations, eliminating the need for a separate router. - By pre-computing and ranking internal activation norms, only the top-activated experts process each token, while others abort, thereby improving efficiency. - The overhead of pre-computing activations is reduced through low-rank weight factorization. - Experimental results demonstrate that AoE outperforms traditional MoE models on downstream tasks with comparable efficiency across various model sizes up to 4 billion parameters. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament](https://arxiv.org/abs/2501.13007) | Yixin Cao, Rui Min, Zijun Yao, Yantao Liu, juanli | - This paper introduces Pairwise Reward Model (Pairwise RM), a novel approach for Best-of-N (BoN) sampling in Large Language Models (LLMs), particularly for math reasoning tasks. - Instead of assigning absolute scores, Pairwise RM evaluates two candidate solutions simultaneously, using a knockout tournament to select the best solution through pairwise comparisons. - This method addresses limitations of traditional reward models by eliminating arbitrary scoring and enabling cross-validation. - A new dataset, PAIRWISE-443K, with 443K pairwise comparisons is created for model training. - Experimental results on MATH-500 and Olympiad Bench demonstrate significant improvements over baseline models, showing a 40% to 60% relative improvement on the top 50% challenging problems in MATH-500. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/THU-KEG/PairwiseRM/) | N/A |
| [O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning](https://arxiv.org/abs/2501.12570) | Yibo Wang, Haiying He, Li Shen, cxc361461518, iNk233 | - This paper introduces Length-Harmonizing Fine-Tuning (O1-Pruner), a novel method to optimize long-thought reasoning in Large Language Models (LLMs) for mathematical problem-solving. - O1-Pruner addresses the issue of reasoning length disharmony in LLMs, where models generate solutions of varying lengths with shorter, yet accurate, solutions often available, leading to computational redundancy. - The method utilizes a Reinforcement Learning (RL)-style fine-tuning approach, incorporating an accuracy constraint to ensure that optimizing for shorter reasoning processes does not compromise problem-solving accuracy. - Experimental results on benchmark datasets like MATH, GSM8k, and GaoKao demonstrate that O1-Pruner achieves a better balance between solution length and accuracy compared to baseline and other competing methods like SFT and DPO, leading to improved inference efficiency. - The proposed method is evaluated using both accuracy and a new metric called Accuracy-Efficiency Score (AES), showing consistent improvements in reducing solution length while maintaining or improving accuracy. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/StarDewXXX/O1-Pruner) | N/A |
| [IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems](https://arxiv.org/abs/2501.11067) | Ilankad23, Eladlev | - This paper introduces IntellAgent, a multi-agent framework for evaluating conversational AI systems. - IntellAgent leverages a novel approach by automating the generation of diverse, synthetic scenarios that test conversational AI agents across various aspects including multi-turn dialogues, policy adherence, and API usage. - IntellAgent uses a graph-based policy model to represent the relationships and complexities of policy interactions, enabling fine-grained diagnostics. - Experimental results demonstrate a strong correlation between model performance on the IntellAgent benchmark and the T-bench, despite IntellAgent relying entirely on synthetic data. - The findings indicate a decrease in model performance with increasing complexity and variations in capabilities across different policy categories, highlighting IntellAgent's ability to provide detailed insights for targeted optimization. | ['Natural Language Processing'] | [Link](https://github.com/plurai-ai/intellagent), [Link](https://github.com/langchain-ai/langgraph) | N/A |


## Papers for 2025-01-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training](https://arxiv.org/abs/2501.11425) | Zhengyin Du, Zhiheng Xi, Junjie-Ye, lovesnowbest, siyuyuan | - This paper introduces Agent-R, a novel iterative self-training framework designed to improve the error correction capabilities of large language model (LLM) agents in interactive environments. - Agent-R leverages Monte Carlo Tree Search (MCTS) to dynamically construct training samples, enabling agents to learn from their mistakes by revising erroneous trajectories. - The framework includes a model-guided critique construction mechanism where the actor model pinpoints the first error in a failed trajectory and splices it with the adjacent correct path, facilitating timely error correction. - Experimental results across three interactive and agentic environments (WebShop, SciWorld, and TextCraft) demonstrate that Agent-R surpasses baseline methods and agents trained on expert trajectories, achieving superior performance (+5.59%). - Agent-R also equips agents with the ability to more effectively identify and correct erroneous actions in real time while avoiding loops, addressing a key limitation of previous methods. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/bytedance/Agent-R) | [Link](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) |
| [MMVU: Measuring Expert-Level Multi-Discipline Video Understanding](https://arxiv.org/abs/2501.12380) | Lujing Xie, Yilun Zhao, Phil-01, entropyhu, freesky | - MMVU, a new expert-level multi-discipline benchmark, is introduced for evaluating foundation models in video understanding, comprising 3,000 expert-annotated questions across 27 subjects in Science, Healthcare, Humanities & Social Sciences, and Engineering. - MMVU emphasizes domain-specific knowledge application and complex reasoning for specialized-domain video analysis, going beyond basic visual perception common in current video benchmarks. - Each MMVU example includes expert annotations from scratch with stringent quality control, enriched by expert-written reasoning rationales and domain knowledge. - In evaluations of 32 prominent models, advanced System-2 models like o1 and Gemini 2.0 Flash Thinking achieved top performance, though still below human expertise. - This work provides valuable insights for enhancing expert-level, knowledge-intensive video understanding in specialized domains. | ['Multimodal', 'Visual Question Answering'] | [Link](github.com/yale-nlp/MMVU) | [Link](huggingface.co/datasets/yale-nlp/MMVU) |
| [Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/abs/2501.11873) | Kaiyue Wen, Bo Zheng, Zeyu Huang, Zihan Qiu, Losin94 | - This paper proposes a global-batch load balancing loss (LBL) strategy for training Mixture-of-Experts (MoE) models, addressing the limitations of the commonly used micro-batch LBL. - The micro-batch LBL enforces load balancing at the sequence level, hindering expert specialization, particularly in domain-specific tasks, whereas the global-batch LBL promotes load balancing at the corpus level, encouraging specialization. - The global-batch LBL involves synchronizing expert selection frequencies across parallel groups, introducing minimal computational overhead. - Experiments on various MoE model sizes (up to 42.8B parameters) trained on up to 400B tokens show that global-batch LBL significantly improves both pre-training perplexity and downstream task performance. - Analysis reveals that global-batch LBL leads to more interpretable expert specialization, aligning routing decisions with the language modeling task. | ['Natural Language Processing'] | N/A | N/A |
| [UI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/abs/2501.12326) | Shihao Liang, Haoming Wang, Junjie Fang, Yining Ye, Yujia Qin | - UI-TARS is a native GUI agent model that perceives screenshots and performs human-like interactions, such as keyboard and mouse operations, outperforming current agent frameworks. - It incorporates enhanced perception through a large-scale dataset of GUI screenshots for context-aware understanding, unified action modeling for multi-step execution across platforms, and system-2 reasoning for deliberate decision-making. - UI-TARS addresses the data bottleneck in end-to-end agent training by automatically collecting, filtering, and refining interaction traces on virtual machines, along with reflection tuning to recover from errors. - In experiments on 10+ GUI agent benchmarks, UI-TARS achieved SOTA performance in perception, grounding, and GUI task execution, surpassing models like GPT-40 and Claude. - Notably, UI-TARS achieved 24.6 on OSWorld (50 steps) and 46.6 on AndroidWorld, exceeding Claude's 22.0 and GPT-40's 34.5, respectively. | ['Multimodal'] | [Link](https://github.com/bytedance/UI-TARS) | N/A |
| [Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks](https://arxiv.org/abs/2501.11733) | Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy, mikewang | - This paper introduces Mobile-Agent-E, a novel hierarchical multi-agent framework for mobile task automation, featuring self-evolution capabilities through learning and applying reusable *Shortcuts* and *Tips* from past experiences. - Mobile-Agent-E consists of a *Manager*, *Perceptor*, *Operator*, *Action Reflector*, and *Notetaker* agents to handle planning, visual perception, action execution, error verification, and information aggregation respectively. - It also proposes Mobile-Eval-E, a new benchmark focusing on complex, long-horizon, multi-app mobile tasks, along with a *Satisfaction Score* metric based on human-written rubrics for evaluating open-ended tasks. - Experimental results on Mobile-Eval-E demonstrate that Mobile-Agent-E outperforms previous state-of-the-art approaches by a significant margin, achieving a 22.1% absolute improvement in Satisfaction Score with GPT-40. - The inclusion of a self-evolution module shows further performance gains and improved efficiency. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/x-plug/MobileAgent) | N/A |
| [Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments](https://arxiv.org/abs/2501.10893) | Tao Yu, Pengcheng Yin, Jinsung Yoon, Ruoxi Sun, Hongjin Su | - LEARN-BY-INTERACT is a data-centric framework designed to enable Large Language Model (LLM) agents to self-adapt to new environments without human annotations by synthesizing trajectories of agent-environment interactions based on documentation. - It constructs instructions by summarizing or abstracting interaction histories (backward construction) and uses these in training-based and training-free in-context learning scenarios with retrieval approaches optimized for agents. - Experiments across coding, web, and desktop environments (SWE-bench, WebArena, OSWorld, Spider2-V) show LEARN-BY-INTERACT improves baseline results, with up to 12.2% for in-context learning with Claude-3.5 and 19.5% for training with Codestral-22B. - Backward construction contributes significantly to performance, improving results by up to 14%. - The framework's agentic retrieval pipeline demonstrates superiority over conventional retrieval-augmented generation. | ['Reinforcement Learning', 'Robotics', 'Natural Language Processing'] | N/A | N/A |
| [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.11223) | Afonso Catarino, Ales Kubicek, Eric Schreiber, Julia Barth, Maciej Besta | - This paper introduces a blueprint for Reasoning Language Models (RLMs), providing a modular framework for their design and analysis. - The blueprint incorporates various reasoning structures, strategies, and training schemes, unifying diverse RLM approaches like MCTS, reinforcement learning, and structured prompting. - A modular implementation, x1, is presented for rapid RLM prototyping and experimentation, along with insights like multi-phase training and the importance of familiar training distributions. - Analysis of existing RLMs like LLaMA-Berry and QwQ demonstrates the blueprint's versatility and unifying potential. - The work aims to democratize advanced reasoning capabilities, fostering innovation and bridging the gap between "rich AI" and "poor AI". | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/spcl/x1) | N/A |
| [Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](https://arxiv.org/abs/2501.12273) | Chuyu Zhang, Mo Li, Taolin Zhang, Maosong Cao, zsytony | - Condor, a two-stage framework for synthetic data generation, enhances Large Language Model (LLM) alignment by leveraging a World Knowledge Tree and self-reflection refinement. - The first stage, Condor Void, uses a knowledge inspiration strategy with the World Knowledge Tree to create diverse questions and initial responses, forming the Dv dataset. - The second stage, Condor Refine, applies a self-reflection mechanism allowing the model to iteratively refine Dv responses based on self-generated critiques, generating a higher-quality DR dataset. - Experiments using various LLMs, including Qwen, InternLM, and Llama, demonstrate that Condor-generated data significantly improves performance on subjective chat benchmarks compared to officially released and RLHF-trained models, even without RLHF incorporated in the Condor training pipeline. - Additional experiments on knowledge-based benchmarks reveal that Condor maintains the models' knowledge QA capabilities while improving conversational ability. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/InternLM/Condor) | [Link](https://hf.co/datasets/internlm/Condor-SFT-20K) |
| [EMO2: End-Effector Guided Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2501.10687) | Liefeng Bo, Bang Zhang, Qi Wang, Siqi Hu, Linrui Tian | - EMO2 is a novel two-stage audio-driven talking head method that generates expressive facial expressions and synchronized hand gestures from a single reference image and audio input. - The first stage uses a motion diffusion model to generate hand poses from audio, leveraging the strong correlation between audio and hand movements. - The second stage employs a diffusion-based model with a ReferenceNet backbone to synthesize video frames, incorporating the generated hand poses to produce realistic facial expressions and body movements, guided by "pixels prior IK". - Experimental results demonstrate that EMO2 outperforms state-of-the-art methods, such as CyberHost and Vlogger, in terms of visual quality, synchronization accuracy, and motion diversity, particularly in generating more vivid and expressive hand motions. - The method addresses the challenge of weak correspondence between audio and full-body gestures by focusing on hand motion generation and leveraging the implicit IK knowledge within 2D generative models. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [MSTS: A Multimodal Safety Test Suite for Vision-Language Models](https://arxiv.org/abs/2501.10057) | Alicia Parrish, Janis Goldzycher, Felix Friedrich, Giuseppe Attanasio, Paul Röttger | - This paper introduces MSTS, a Multimodal Safety Test Suite for Vision-Language Models (VLMs). - MSTS comprises 400 unsafe multimodal English-language prompts across 40 fine-grained hazard categories and is designed to test the safety of VLMs in a structured manner. - MSTS test prompts consist of both a textual and visual component designed to be safe individually but unsafe when combined. - Commercial VLMs generally respond safely to MSTS while open VLMs have clear safety issues often responding unsafely or failing to interpret the multimodal input correctly. - This research highlights the need for further research into VLM safety and the importance of multimodal inputs in safety evaluation. | ['Multimodal', 'Image-Text-to-Text'] | [Link](https://github.com/paul-rottger/msts-multimodal-safety) | N/A |


## Papers for 2025-01-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|


## Papers for 2025-01-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Evolving Deeper LLM Thinking](https://arxiv.org/abs/2501.09891) | Shumeet Baluja, Dave Marwood, Yueh-Hua Wu, Ian Fischer, Kuang-Huei Lee | - This paper introduces Mind Evolution, a novel evolutionary search strategy for Large Language Models (LLMs) designed to enhance their problem-solving capabilities by efficiently utilizing inference-time compute. - Mind Evolution employs a genetic algorithm that generates, refines, and recombines candidate solutions in natural language, guided by an evaluator that provides feedback without needing explicit formalization of the underlying problem. - In experiments on TravelPlanner, Trip Planning, and Meeting Planning benchmarks, Mind Evolution with Gemini 1.5 Flash significantly outperformed Best-of-N and Sequential Revision, achieving success rates exceeding 95%, 96%, and 85%, respectively. - A two-stage approach using Gemini 1.5 Pro for unsolved instances further boosted performance to near-perfect scores on TravelPlanner and Meeting Planning, matching or exceeding state-of-the-art results achieved with formal solvers. - The authors also introduce StegPoet, a new challenging benchmark for stenographic encoding of hidden messages in creative text, where Mind Evolution achieved a success rate of 87% using Gemini 1.5 Pro, demonstrating the method's applicability to less formalized tasks. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [PaSa: An LLM Agent for Comprehensive Academic Paper Search](https://arxiv.org/abs/2501.10120) | Yuchen Zhang, Yuan Lin, Peiyuan Feng, Guanhua Huang, Yichen He | - PaSa is a novel Large Language Model (LLM) agent designed for comprehensive academic paper searches that mimics human researcher behaviour by autonomously making decisions such as invoking search tools, reading papers and selecting relevant references. - PaSa consists of two LLM agents, the Crawler and the Selector. - The Crawler collects relevant papers by using search tools or extracting citations from the current paper and adding them to a growing paper queue, which the Selector then reads each paper in to determine whether it meets the requirements of the user query. - The authors build a synthetic but high-quality academic search dataset, AutoScholarQuery, based on fine-grained scholar queries and their corresponding relevant papers from ICLR 2023, ICML 2023, NeurIPS 2023, ACL 2024, and CVPR 2024. - PaSa-7b significantly outperforms all baselines on the RealScholarQuery dataset including Google, Google Scholar, Google paired with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4), GPT-3.5-Turbo, and PaSa-GPT-4, achieving 37.78% improvement in Recall@20 and 39.90% improvement in Recall@50 compared to the strongest Google based baseline (Google with GPT-4). | ['Natural Language Processing'] | [Link](https://github.com/bytedance/pasa) | N/A |
| [Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions](https://arxiv.org/abs/2501.10020) | Liefeng Bo, Jianqiang Ren, Chao He | - Textoon is a novel method for generating animatable 2D cartoon characters in the Live2D format from text descriptions, leveraging large language and vision models. - The system parses complex text descriptions using a fine-tuned LLM to identify character features like hair, eyes, clothing, and shoes, achieving over 90% accuracy. - It employs Stable Diffusion XL for controllable appearance generation, ensuring high-quality images and precise text pattern generation while maintaining model driving performance. - Textoon addresses component completion challenges by using a template-based approach for pixel extraction and image-to-image control generation for refining occluded areas. - For animation, Textoon enhances facial expressiveness by integrating ARKit's face blend shape capabilities into Live2D, enabling more detailed and lively lip-sync and facial movements. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong](https://arxiv.org/abs/2501.09775) | Pedro Reviriego, Gonzalo Martínez, Javier Conde, Tairan Fu, mariagrandury | - This paper investigates how Large Language Models (LLMs) self-confidence changes when they are asked to provide reasoning before answering multiple-choice questions (MCQs). - The study uses the Massive Multitask Language Understanding (MMLU) benchmark and evaluates seven different LLMs, including models from Meta, Mistral, Google, 01.AI, and OpenAI. - Results show that LLMs exhibit higher confidence in their selected answers when they provide reasoning, regardless of answer correctness.  - This increased confidence is more pronounced for incorrect answers and is attributed to the autoregressive nature of LLMs and the influence of reasoning on predicted probabilities. - This behavior is consistent with human behavior, suggesting potential limitations of using confidence estimates for LLM evaluation and raising questions about the effectiveness of reasoning for certain question types. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/aMa2210/LLM_MCQ_LogProbs) | N/A |
| [HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution](https://arxiv.org/abs/2501.10045) | Chong Zhang, Yukun Ma, Zexu Pan, Kun Zhou, Shengkui Zhao | - HiFi-SR, a unified transformer-convolutional generative adversarial network (GAN), is proposed for high-fidelity speech super-resolution. - The model uses a transformer network as an encoder to convert low-resolution mel-spectrograms into latent representations, and a convolutional network (based on HiFi-GAN generator) upscales these representations into high-resolution waveforms. - A multi-band, multi-scale time-frequency discriminator and a multi-scale mel-reconstruction loss are incorporated to enhance high-frequency fidelity during adversarial training. - HiFi-SR can upscale any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate. - Experimental results on VCTK, EXPRESSO, and VocalSet datasets demonstrate that HiFi-SR significantly outperforms existing methods in both objective metrics (LSD) and subjective listening tests (ABX). | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/modelscope/ClearerVoice-Studio) | N/A |
| [ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario](https://arxiv.org/abs/2501.10132) | Jie Tang, Haiyi Hu, Xiaohan Zhang, Zhengxiao Du, Lucen Zhong | - ComplexFuncBench, a new benchmark for evaluating complex function calling in LLMs, focusing on multi-step and constrained scenarios within a 128k long context, is introduced. - Unlike existing benchmarks, it incorporates multi-step calls, user constraints, parameter value reasoning from implicit info, long parameter values, and a 128k context. - ComplexEval, an automatic evaluation framework, uses multi-dimensional matching (rule-based, response-based, and LLM-based) to overcome limitations of traditional exact matching. - Experiments on various LLMs reveal that closed-source models outperform open-source models, and parameter value errors are a significant challenge. - Different models exhibit specific weaknesses in handling various parameter types and planning function call steps. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/THUDM/ComplexFuncBench) | N/A |


## Papers for 2025-01-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking](https://arxiv.org/abs/2501.09751) | Ningyu, Runnaning, callanwu, JizhanFang, ZekunXi | - OmniThink, a novel machine writing framework, enhances knowledge density in generated long-form articles by emulating human-like iterative expansion and reflection. - It simulates the cognitive process of learners progressively deepening their knowledge, iteratively adjusting retrieval strategies for thorough information exploration. - This framework incorporates expansion and reflection, outline structuring, and article composition stages, utilizing search engines and LLMs to generate nuanced, original content. - Evaluation on WildSeek dataset with GPT-40 and Qwen-Plus demonstrates improved knowledge density and overall quality compared to baselines like RAG, ORAG, STORM, and Co-STORM. - Human evaluations confirm enhanced breadth and depth, though automated and human novelty assessments diverge, suggesting areas for future evaluation refinement. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators](https://arxiv.org/abs/2501.09484) | Quan Tu, hsaest, ShizhengLi, sdujq, zhaocheng | - This paper introduces a novel patient simulator trained on synthetic doctor-patient dialogue data generated using real patient dialogue strategies and medical records. - The simulator aims to address the limitations of prompt engineering in accurately representing patient behavior in online medical consultations (OMCs). - Experiments demonstrate that the simulator exhibits a lower hallucination rate and improved anthropomorphism compared to baselines, although the irrelevant response rate is slightly higher. - The study investigates the relationship between inquiry and diagnosis in OMCs and finds that they adhere to Liebig's law: poor inquiry limits effective diagnosis, and vice-versa. - By categorizing inquiries into four types, the research analyzes inquiry differences among models and reveals the importance of effective inquiry allocation within limited consultation rounds. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/LIO-H-ZEN/PatientSimulator) | N/A |
| [FAST: Efficient Action Tokenization for Vision-Language-Action Models](https://arxiv.org/abs/2501.09747) | oier-mees, dannydriess, brianichter, kylestach, KarlP | - This paper introduces FAST (Frequency-space Action Sequence Tokenization), a novel compression-based tokenization scheme for robot actions, utilizing Discrete Cosine Transform (DCT) and Byte Pair Encoding (BPE) to improve the training of Vision-Language-Action (VLA) models, especially with high-frequency data. - It addresses the limitations of per-dimension binning, which struggles with high-frequency, correlated action sequences by compressing redundant data into fewer, high-information tokens. - Based on FAST, they introduce FAST+, a universal pre-trained tokenizer effective across different robot morphologies, action spaces, and control frequencies, offering a strong default for robot action tokenization. - Combining FAST with the π0 VLA model, they demonstrate performance comparable to state-of-the-art diffusion-based VLAs on long-horizon, dexterous manipulation tasks while achieving up to 5x faster training speeds. - The π0-FAST model trained with the proposed tokenization also successfully learns a generalist manipulation policy that generalizes to unseen environments in a zero-shot setting based on natural language prompts, the first of its kind on the DROID dataset. | ['Robotics', 'Multimodal'] | N/A | [Link](https://huggingface.co/physical-intelligence/fast) |
| [Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models](https://arxiv.org/abs/2501.09686) | Ouyangtj, zhazhahui7, berserkerko, zzfoutofspace, haohao11 | - This paper surveys recent advancements in Large Language Model (LLM) reasoning, focusing on reinforced learning methods and prompting techniques. - The survey explores how "thought" sequences, representing intermediate reasoning steps, enhance LLM's reasoning abilities, moving beyond simple token generation. - It reviews techniques like Chain-of-Thought prompting, Tree-of-Thoughts, and reinforcement learning methods using Process Reward Models (PRMs) for training and test-time scaling. - The paper analyzes OpenAI's o1 series and open-source projects like OpenR, LLaMA-Berry, and Journey Learning, showcasing their approaches to achieving strong reasoning capabilities. - Finally, it discusses open challenges and future research directions, including refining test-time scaling, developing more advanced reasoning models, and exploring potential applications in diverse domains. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2025-01-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents](https://arxiv.org/abs/2501.08828) | Ruiming Tang, Dexun Li, Xin Deik Goh, Yujing Chang, daviddongdong | - This paper introduces MMDocIR, a novel benchmark for multi-modal document retrieval focusing on page-level and layout-level retrieval tasks. - MMDocIR comprises an evaluation set with 313 documents and 1,658 expert-annotated questions and a training set with 6,878 documents and 73,843 automatically annotated questions. - Experimental results demonstrate that visual retrievers outperform text-based methods, and models trained on MMDocIR exhibit superior performance. - The benchmark addresses limitations of existing datasets by focusing on retrieval granularity, offering complete page contexts, and improved question quality. - The benchmark also highlights the effectiveness of VLM-based text representations over OCR for multi-modal document retrieval. | ['Multimodal', 'Document Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/MMDocIR) |
| [Towards Best Practices for Open Datasets for LLM Training](https://arxiv.org/abs/2501.08365) | jending12, ayahbdeir, avi-skowron, stellaathena, stefan-baack | - This paper discusses the challenges and best practices for creating open datasets for LLM training, focusing on sourcing, processing, governing, and releasing data. - It emphasizes the importance of dataset transparency for accountability and innovation in AI, particularly given the increasing criticism of data practices by large AI companies. - The authors recommend prioritizing community resources, providing thorough documentation, adhering to preference signals, and promoting diversity in data sources. - They also outline the need for clear legal frameworks and ethical considerations in data governance and release. - The paper emerged from a convening hosted by Mozilla and EleutherAI and builds on case studies from prominent open datasets. | ['Natural Language Processing'] | [Link](https://github.com/r-three/common-pile) | [Link](https://huggingface.co/datasets/HuggingFaceH4/c-pile) |
| [RepVideo: Rethinking Cross-Layer Representation for Video Generation](https://arxiv.org/abs/2501.08994) | liuziwei7, Ziqi, cszy98, weepiess2383, ChenyangSi | - RepVideo, a novel framework designed for text-to-video generation, enhances video diffusion models by leveraging enriched intermediate representations. - It employs a feature cache module to aggregate features from adjacent transformer layers and a gating mechanism to combine these aggregated features with the original input, improving spatial detail and temporal consistency. - RepVideo addresses the issue of fragmented spatial semantics and reduced temporal coherence in existing transformer-based video diffusion models. - Experimental results on VBench show that RepVideo-2B outperforms the baseline CogVideoX-2B and other state-of-the-art methods in various metrics, including motion smoothness, object class, multiple objects, and spatial relationship. - Both automated and human evaluations demonstrate RepVideo's superiority in generating high-quality videos with enhanced temporal coherence, spatial fidelity, and alignment with text prompts. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework](https://arxiv.org/abs/2501.08809) | Wenjie Zhu, Wei Tan, Wei Yuan, Can Zhang, Sida Tian | - XMusic is a novel framework for generating symbolic music from various prompt types, including images, videos, text, tags, and humming. - The framework consists of two main components: XProjector, which parses prompts into symbolic music elements (emotions, genres, rhythms, notes), and XComposer, which generates music based on these elements and selects high-quality outputs. - XComposer utilizes a Transformer Decoder as its generative model and incorporates a multi-task learning Selector for quality assessment, emotion recognition, and genre recognition. - XMusic is trained on a new large-scale symbolic music dataset, XMIDI, containing over 108,000 MIDI files with detailed emotion and genre annotations. - Both objective and subjective evaluations demonstrate XMusic’s superior performance in terms of music quality and controllability compared to existing methods across various prompt types, including video, text and image conditioned generation. | ['Audio', 'Text-to-Audio', 'Multimodal'] | N/A | N/A |
| [Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding](https://arxiv.org/abs/2501.07783) | douwh, Changyao, favor123, Einsiedler, wzk1015 | - PIIP (Parameter-Inverted Image Pyramid Networks) is proposed as a novel architecture for visual perception and multimodal understanding tasks. - It processes multi-scale images with different sized models: smaller models for higher resolutions and larger models for lower resolutions, which makes it more efficient than traditional image pyramids. - Cross-branch feature interaction and branch merging components allow information exchange and feature fusion between levels for enhanced performance. - PIIP-LLaVA, built on PIIP, adapts the architecture for efficient and effective high-resolution multimodal understanding. - PIIP demonstrates performance improvements of 1-2% with 40-60% less computation on object detection and semantic segmentation tasks, achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K with InternViT-6B. | ['Computer Vision', 'Object Detection', 'Image Segmentation', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/OpenGVLab/PIIP) | N/A |
| [Multimodal LLMs Can Reason about Aesthetics in Zero-Shot](https://arxiv.org/abs/2501.09012) | Vincentchang, Ruixiang | - This paper investigates the ability of Multimodal Large Language Models (MLLMs) to evaluate the aesthetic quality of artworks, focusing on artistic stylization. - It introduces MM-StyleBench, a new large-scale dataset with diverse content and style instances, and develops a method for modeling human aesthetic preferences for benchmarking. - The study reveals a hallucination issue in MLLMs' art evaluation, tied to response subjectivity, and proposes ArtCoT, a prompting method with explicit task decomposition to mitigate this. - ArtCoT enhances MLLMs' reasoning ability, leading to increased alignment with human preferences, by encouraging concrete language and reducing subjective interpretations. - The findings offer insights into MLLMs' application in art evaluation and suggest potential benefits for downstream tasks like style transfer and image generation. | ['Multimodal', 'Computer Vision', 'Image-to-Image'] | [Link](https://github.com/songrise/MLLM4Art) | N/A |


## Papers for 2025-01-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/abs/2501.08313) | Bangwei Gong, Aonian Li, MiniMax, Hannnnnxd, enochzhang | - This paper introduces the MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which leverage "lightning attention" and Mixture of Experts (MoE) to handle long contexts (up to 4 million tokens for MiniMax-Text-01). - MiniMax-Text-01 is a 456 billion parameter model, with 45.9 billion parameters activated per token and 32 experts, designed to match leading commercial models while offering a significantly larger context window. - MiniMax-VL-01 is a vision-language model trained on 512 billion vision-language tokens. - The authors claim their models match the performance of state-of-the-art models like GPT-4 and Claude-3.5-Sonnet on standard benchmarks and outperform them in long context scenarios (200k+ tokens). - They also highlight superior prefill latency due to their novel architecture. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Natural Language Processing', 'Question Answering', 'Translation', 'Summarization', 'Text Generation'] | [Link](https://github.com/MiniMax-AI) | N/A |
| [A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following](https://arxiv.org/abs/2501.08187) | Jingyang Qian, Kangwei Liu, Xinle Deng, Ningyu, Fangyinfff | - INSTRUCTCELL, a multimodal AI copilot, is introduced for enhanced single-cell analysis, integrating natural language instructions with single-cell RNA sequencing (scRNA-seq) data.  - The model architecture comprises a Q-Former for embedding gene expression profiles, a pretrained language model (LM) for text processing, and a cell reconstruction block for generating gene expression profiles.  - INSTRUCTCELL excels in tasks such as conditional pseudo-cell generation, cell type annotation, and drug sensitivity prediction, adapting to diverse experimental settings.  - Evaluation across multiple scRNA-seq datasets shows INSTRUCTCELL performs on par with or surpasses existing models like scBERT, scGPT, and Geneformer, demonstrating robustness and efficiency.  - Through this unified approach, INSTRUCTCELL streamlines complex single-cell data exploration, lowering technical barriers and revealing deeper biological insights. | ['Multimodal', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/zjunlp/InstructCell) | [Link](https://huggingface.co/zjunlp/InstructCell-chat), [Link](https://huggingface.co/zjunlp/InstructCell-instruct) |
| [PokerBench: Training Large Language Models to become Professional Poker Players](https://arxiv.org/abs/2501.08328) | Zhengyu Li, Aniket Rahane, Richard Yang, Richard Zhuang, akshat57 | - This paper introduces PokerBench, a benchmark and dataset for evaluating and training large language models (LLMs) to play the strategic game of poker. - PokerBench includes 11,000 scenarios (1,000 pre-flop and 10,000 post-flop) designed to test LLMs on decision-making and game theory optimal (GTO) play. - The authors evaluate several prominent LLMs including GPT-4, ChatGPT 3.5, and various Llama and Gemma models, finding that pre-trained models underperform compared to fine-tuned models. - Fine-tuning Llama-3-8B significantly improves its performance, exceeding that of GPT-4 on PokerBench. - Head-to-head comparisons between checkpoints with varying PokerBench scores show a correlation between benchmark performance and actual win rates, demonstrating the efficacy of PokerBench as an evaluation tool. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/pokerllm/pokerbench) | N/A |
| [Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens](https://arxiv.org/abs/2501.07730) | Xiaohui Shen, Chenglin Yang, Qihang Yu, Dongwon Kim, turkeyju | - This paper introduces TA-TiTok, a novel text-aware 1D tokenizer, and MaskGen, a family of text-to-image masked generative models. - TA-TiTok improves upon previous 1D tokenizers by incorporating textual information during detokenization, using a simplified one-stage training process, and supporting both discrete and continuous tokens. - MaskGen leverages TA-TiTok to efficiently generate images from text, supporting both discrete and continuous token representations and utilizing CLIP for text encoding. -  Evaluated on MJHQ-30K and GenEval benchmarks, MaskGen achieves comparable performance to models trained on private data, despite being trained exclusively on open data. - Notably, MaskGen-XL (1.1B parameters) achieves an FID of 6.53 on MJHQ-30K and an overall score of 0.57 on GenEval, outperforming larger models trained on private datasets, while exhibiting faster inference. | ['Text-to-Image', 'Image Feature Extraction', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/laion/laion2B-en-aesthetic), [Link](https://huggingface.co/datasets/laion/laion-art), [Link](https://huggingface.co/datasets/laion/laion-pop), [Link](https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions) |
| [Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks](https://arxiv.org/abs/2501.08326) | Subhashree Radhakrishnan, Sifei Liu, De-An Huang, Min-Hung Chen, Miran Heo | - Omni-RGPT, a multimodal large language model, is introduced for region-level understanding in both images and videos. - It uses "Token Mark," a set of learned tokens embedded into visual features and text prompts using region prompts (boxes or masks) to represent target regions. - An auxiliary "Temporal Region Guide Head" is introduced to improve region consistency in videos without relying on tracklets. - A new large-scale region-level video instruction dataset, RegVID-300k, is also introduced, containing 98k videos and 294k instruction samples. - Omni-RGPT achieves state-of-the-art performance on benchmarks like Causal-VidQA and VCR, demonstrating its effectiveness in region-level understanding tasks. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Image-to-Text'] | N/A | N/A |
| [OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training](https://arxiv.org/abs/2501.08197) | Ran Chen, Wei Wang, Zekun Wang, Ziyun Dai, yuyijiong | - This paper introduces the OpenCSG Chinese Corpus, a series of high-quality datasets designed for Chinese LLM pre-training, post-training, and fine-tuning. - The corpus comprises four distinct datasets: Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with unique characteristics catering to diverse training needs. - The datasets leverage automated scoring modules, synthetic text generation, and domain-focused curation, ensuring scalability, diversity, and openness. - Experimental results on a 2B-level LLM demonstrate significant performance improvements in tasks like C-Eval when pre-trained on Fineweb-edu-chinese compared to a baseline dataset. - This work addresses the scarcity of high-quality Chinese datasets and promotes advancements in Chinese NLP research by providing open-access resources. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/yuyijiong/fineweb-edu-chinese) | N/A |
| [Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding](https://arxiv.org/abs/2501.07888) | Yuan Lin, Yuchen Zhang, Haomiao Sun, Jiawei Wang, Liping Yuan | - Tarsier2, a 7B parameter Large Vision-Language Model (LVLM), focuses on generating detailed video descriptions and demonstrates superior general video understanding. - It utilizes a simple architecture comprising a vision encoder, vision adapter, and LLM and undergoes three training stages: pre-training on 40M video-text pairs, supervised fine-tuning with fine-grained temporal alignment, and Direct Preference Optimization (DPO) with automatically generated preference data. - Evaluation results show Tarsier2-7B outperforming proprietary models like GPT-40 and Gemini 1.5 Pro in detailed video description and achieving state-of-the-art performance on 15 public benchmarks, including video question answering, grounding, and hallucination tests. - A key contribution is a new recaptioning dataset, Tarsier2-Recap-585K, used to enhance existing LVLMs for video description and general video understanding. - Ablation studies confirm the effectiveness of scaling the pre-training data, fine-grained temporal alignment, and DPO training. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/Tarsier-LLM/Tarsier2-Recap-585K) |
| [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319) | Mor Geva, Chen Agassy, Roy Mayan, Yoav Gur-Arieh, atticusg | - Proposes two output-centric methods, VocabProj and TokenChange, for generating natural language descriptions of features in Large Language Models (LLMs). - Introduces a two-faceted evaluation framework for feature descriptions, considering both input-based and output-based metrics. - Shows that output-centric methods outperform input-centric methods (like MaxAct) on output-based evaluations, and are often only slightly worse on input-based evaluations. - Demonstrates that combining input- and output-centric methods leads to more comprehensive and accurate feature descriptions. - Reveals that output-centric methods can be used to find inputs that activate "dead" features, which were previously thought to be inactive. | ['Natural Language Processing'] | N/A | N/A |
| [Potential and Perils of Large Language Models as Judges of Unstructured Textual Data](https://arxiv.org/abs/2501.08167) | Satya Kapoor, Sreyoshi Bhaduri, Natalie Perez, Rewina Bedemariam, amanchadha | - This research explores the effectiveness of Large Language Models (LLMs) as judges for evaluating the thematic alignment of summaries generated by other LLMs, focusing on open-text survey data. - The study uses an Anthropic Claude model to generate thematic summaries and employs Amazon's Titan Express, Nova Pro, and Meta's Llama as LLM judges, comparing their performance to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha. - Findings reveal that while LLMs can offer a scalable solution comparable to human raters in judging thematic alignment, humans may still excel at detecting subtle, context-specific nuances. - The study highlights the potential of LLMs as judges in organizational settings while emphasizing the need for careful consideration of their limitations. - Recommendations for future research include addressing potential biases in LLM evaluations and developing more comprehensive assessment methods that capture nuanced thematic understanding. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [HALoGEN: Fantastic LLM Hallucinations and Where to Find Them](https://arxiv.org/abs/2501.08292) | Yejin Choi, David Wadden, Shrusti Ghela, Abhilasha Ravichander | - This paper introduces HALoGEN, a benchmark for evaluating hallucinations in large language models (LLMs). - HALoGEN consists of 10,923 prompts across nine domains, including programming, scientific attribution, and summarization, along with automatic verifiers. - The benchmark evaluates ~150,000 generations from 14 LLMs and finds that even top-performing models exhibit frequent hallucinations (4%-86% of generated atomic facts). - A novel error classification categorizes hallucinations as Type A (incorrect recollection of training data), Type B (incorrect or out-of-context training data), or Type C (fabrication). - The framework aims to facilitate research into why LLMs hallucinate and promote the development of trustworthy language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://halogen-hallucinations.github.io) | N/A |
| [AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages](https://arxiv.org/abs/2501.08284) | Ibrahim Said Ahmad, David Ifeoluwa Adelani, Abinew Ali Ayele, Idris Abdulmumin, Shamsuddeen Hassan Muhammad | - This paper introduces AfriHate, a multilingual dataset of hate speech and abusive language in 15 African languages. - The dataset consists of tweets annotated by native speakers into three categories: hate, abusive/offensive, and neutral, with further labeling of hate speech targets based on attributes like ethnicity, religion, and gender. - Baseline experiments using Africa-centric pre-trained language models and prompted LLMs were conducted, revealing performance variations across languages and demonstrating that multilingual training often yields better results. - The study finds that while multilingual models generally perform better, LLMs show potential for improved hate speech detection in low-resource languages. - The datasets, scripts, models, and lexicons are publicly released to facilitate further research on hate speech and offensive language in African languages. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/AfriHate/AfriHate) | N/A |


## Papers for 2025-01-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [The Lessons of Developing Process Reward Models in Mathematical Reasoning](https://arxiv.org/abs/2501.07301) | RunjiLin, BeichenZhang, wuyangzhen, chujiezheng, Zhenru | - This paper introduces two new process reward models (PRMs) for mathematical reasoning, Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B, focusing on enhancing the process supervision capabilities. - The study demonstrates that conventional Monte Carlo (MC) estimation for training data synthesis is less effective for training PRMs compared to LLM-as-a-judge or human annotation methods. - Furthermore, they point out the bias of using Best-of-N (BoN) evaluation alone and advocate for incorporating step-level metrics, such as PROCESSBENCH, for a more comprehensive assessment. - A new consensus filtering mechanism which integrates MC estimation with LLM-as-a-judge is proposed to improve both model performance and data efficiency. - The proposed models and training mechanisms significantly improve error identification in mathematical reasoning, exceeding the capabilities of existing open-source models and providing guidelines for future PRM development. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://hf.co/Qwen/Qwen2.5-Math-PRM-7B), [Link](https://hf.co/Qwen/Qwen2.5-Math-PRM-72B) |
| [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425) | Huizhuo Yuan, Yifeng Liu, thughost, zhenqincn, yifAI | - This paper introduces Tensor Product Attention (TPA), a novel attention mechanism designed to reduce memory overhead in large language models (LLMs) during inference. - TPA leverages tensor decomposition to create compact representations of queries, keys, and values, thereby decreasing the size of key-value caches, a major memory consumer in LLMs. - Based on TPA, the authors create a new model architecture, Tensor ProducT ATTenTion Transformer (T6), and show through experiments that it improves performance on language modeling tasks, achieving lower perplexity and higher accuracy on various benchmarks compared to standard transformer models. - TPA's memory efficiency facilitates handling longer sequences under fixed resource constraints, directly addressing a key scalability challenge in current LLMs. - Additionally, TPA is shown to integrate seamlessly with Rotary Position Embedding (RoPE), simplifying its application in existing architectures like LLaMA and Gemma. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/tensorgi/T6) | N/A |
| [$\text{Transformer}^2$: Self-adaptive LLMs](https://arxiv.org/abs/2501.06252) | tyj2022, edoarc, lfsm | - This paper introduces Transformer², a novel self-adaptation framework for Large Language Models (LLMs) that adapts to unseen tasks in real-time. - It employs a two-pass mechanism: first identifying task properties through a dispatch system and then dynamically mixing task-specific "expert" vectors, trained using reinforcement learning. - The "expert" vectors are generated by Singular Value Fine-tuning (SVF), a new parameter-efficient fine-tuning (PEFT) method that modifies singular values within weight matrices. - This method outperforms LoRA, a popular existing PEFT method, with fewer parameters and greater efficiency across tasks and models.  - The approach demonstrates versatility across different LLM architectures and modalities, including vision-language tasks, where it uses knowledge from language tasks to improve performance in visual question answering. | ['Natural Language Processing', 'Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/SakanaAI/self-adaptive-llms) | N/A |
| [VideoAuteur: Towards Long Narrative Video Generation](https://arxiv.org/abs/2501.06173) | Jiepeng Cen, Liangke Gui, Lu Qi, Feng Cheng, lambertxiao | - This paper introduces VideoAuteur, a two-stage auto-regressive pipeline for generating long-form narrative videos, consisting of a long narrative director and a visual-conditioned video generation model. - The long narrative director generates visual embeddings or keyframes along with captions and actions which capture the narrative flow using an interleaved auto-regressive model. - A novel cooking video dataset, CookGen, is created consisting of approximately 200,000 video clips sourced from existing video datasets (YouCook2 and HowTo100M) annotated with captions, actions, and visual states, which allows benchmarking long narrative video generation. - Experiments demonstrate that the generated videos contain improved semantic consistency and visual fidelity compared to existing methods. - Using CLIP embeddings for visual regression outperforms VAE embeddings. | ['Text-to-Video', 'Multimodal'] | [Link](https://videoauteur.github.io) | N/A |
| [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/abs/2501.07572) | zhoudeyu, Runnaning, ZekunXi, wzl0228, callanwu | - This paper introduces WebWalkerQA, a new benchmark designed to evaluate the web traversal capabilities of Large Language Models (LLMs). - WebWalkerQA focuses on information-seeking question-answering tasks that require navigating through website subpages, often involving multiple steps. - The benchmark contains 680 question-answer pairs across over 1373 webpages from diverse domains including conference, organization, education, and games, and is available in both English and Chinese. - A novel multi-agent framework called WebWalker, employing an explorer-critic paradigm, is proposed as a strong baseline for mimicking human-like web navigation and memory management. - Experimental results demonstrate that WebWalkerQA is challenging for LLMs, highlighting the need for better integration of LLMs with web traversal strategies. | ['Question Answering'] | N/A | N/A |
| [O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning](https://arxiv.org/abs/2501.06458) | Gui Geng, Pengfei, alanyoung058, ZhenHuang, zongzi | - This paper explores inference-time scaling in Large Language Models (LLMs) for medical reasoning tasks, including diagnosis and treatment planning. - Experiments on medical benchmarks (MedQA, Medbullets, JAMA Clinical Challenges) reveal that increasing inference time improves performance, with a 6-11% improvement observed using a modest training set of 500 samples. - Task complexity correlates with required reasoning chain length, and the model's differential diagnoses adhere to hypothetico-deductive principles. - The study utilizes a knowledge distillation approach from GPT-series models to enable journey learning during inference. - Findings highlight the potential of combining inference-time scaling and journey learning to improve real-world clinical reasoning in LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/SPIRAL-MED/Ophiuchus) | N/A |
| [MinMo: A Multimodal Large Language Model for Seamless Voice Interaction](https://arxiv.org/abs/2501.06282) | langgz, gaoruize, zhihaodu, Yingda, chenmengzhe | - MinMo, an 8-billion parameter multimodal large language model, is introduced for seamless voice interaction, addressing limitations of prior aligned multimodal models by training on 1.4 million hours of diverse speech data and a broad range of speech tasks. - MinMo achieves state-of-the-art performance in voice comprehension and generation benchmarks, maintains text LLM capabilities, and facilitates full-duplex conversations, outperforming models like Moshi, Freeze-Omni, and GLM-4-Voice across ASR, S2TT, SQA, VSC, SER, and LID tasks (Figure 1). - A novel voice decoder balancing simplicity and performance is proposed, utilizing a streaming Transformer mixing LLM hidden states with speech tokens. - MinMo demonstrates enhanced instruction-following, controlling speech generation with nuances like emotions, dialects, speaking rates, and voice mimicking with 98.4% accuracy. - MinMo supports full-duplex interaction with low latency (100ms for speech-to-text and 600ms theoretical/800ms practical for full-duplex) using a prediction module leveraging the text LLM's semantic understanding. | ['Multimodal', 'Audio', 'Text-to-Speech', 'Automatic Speech Recognition'] | N/A | [Link](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) |
| [SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training](https://arxiv.org/abs/2501.06842) | Zhangyang Wang, Lu Liu, Gaojie Jin, Ziquan Zhu, Tianjin Huang | - This paper introduces SPAM (Spike-Aware Adam with Momentum Reset), a novel optimizer designed to enhance the stability and efficiency of Large Language Model (LLM) training by mitigating the negative impact of gradient spikes. - SPAM incorporates two key innovations: periodic momentum reset and spike-aware gradient clipping to counteract the harmful accumulation of spiked gradients and preserve valuable directional information. - Extensive experiments demonstrate that SPAM surpasses Adam and its variants across various tasks, including LLM pre-training, quantization-aware training, reinforcement learning, and time series forecasting. - SPAM also facilitates memory-efficient training through sparse momentum, outperforming state-of-the-art memory-efficient optimizers like GaLore and Adam-Mini under memory constraints. - The analysis reveals that gradient spikes, often overlooked, coincide with subtle loss bumps during training and can reach up to 1000 times the magnitude of typical gradients, significantly impacting performance across different architectures, model sizes, and datasets. | ['Natural Language Processing', 'Reinforcement Learning', 'Time Series Forecasting'] | [Link](https://github.com/TianjinYellow/SPAM-Optimizer.git) | N/A |
| [BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature](https://arxiv.org/abs/2501.07171) | yeunglevy, yuhuizhang, jnirschl, minwoosun, lozanoe | - This paper introduces BIOMEDICA, a scalable open-source framework for creating a large-scale, deep-learning-ready biomedical image-caption dataset derived from scientific literature, along with a suite of associated CLIP-style models (BMCA-CLIP) pretrained on this data. - The BIOMEDICA dataset contains over 24 million image-text pairs from over 6 million open-access articles with rich metadata and expert annotations, significantly larger and more diverse than existing biomedical vision-language datasets. - BMCA-CLIP models are trained using continual pretraining on the BIOMEDICA dataset via streaming. - Evaluation across 40 standardized biomedical tasks demonstrates state-of-the-art zero-shot performance, with a 6.56% average improvement in classification (up to +29.8% on dermatology and +17.5% on ophthalmology tasks) and superior retrieval performance compared to previous methods, while using 10x less compute. - The authors release the code, dataset, and pretrained models to promote reproducibility and further research in biomedical vision-language modeling. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Zero-Shot Classification', 'Image Feature Extraction'] | [Link](https://github.com/minwoosun/biomedica-etl), [Link](https://github.com/Ale9806/open_clip_with_biomedica) | [Link](https://huggingface.co/BIOMEDICA) |
| [ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning](https://arxiv.org/abs/2501.06590) | Wangchunshu, siruo2, super-dainiu, CamelH, RTT1 | - ChemAgent, a new framework, enhances Large Language Models (LLMs) for chemical reasoning tasks using a dynamic self-updating library, improving performance by up to 46% (GPT-4). - The library compiles sub-tasks and solutions from decomposed chemical tasks, facilitating task decomposition and solution generation for new problems. - ChemAgent integrates three memory types: Planning Memory for high-level strategies, Execution Memory for specific solutions, and Knowledge Memory for fundamental chemistry principles, stored externally for efficient retrieval. - Experimental results on SciBench datasets demonstrate significant improvements over existing methods, including a 46% gain for GPT-4 and a 10% average improvement over StructChem. - The self-updating library system allows continuous refinement of problem-solving strategies and solutions over time, analogous to human learning from past experiences. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/gersteinlab/chemagent) | N/A |


## Papers for 2025-01-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints](https://arxiv.org/abs/2501.03841) | Wenlong Gao, Tianshu Wu, Ergogogogo, JiyaoZhang, pmj110119 | - OmniManip is an open-vocabulary robotic manipulation method that bridges the gap between high-level reasoning of Vision-Language Models (VLMs) and the low-level precision needed for manipulation by introducing object-centric interaction primitives as spatial constraints. - These primitives, defined within an object's canonical space, translate VLM reasoning into actionable 3D constraints, enabling precise manipulation. - The system uses a dual closed-loop approach: one for planning through primitive resampling, interaction rendering, and VLM checking, and another for execution via 6D pose tracking. - The method is evaluated on diverse robotic manipulation tasks and demonstrates strong zero-shot generalization capabilities without requiring VLM fine-tuning. - It also shows promise for automating large-scale simulation data generation for robotic manipulation. | ['Robotics', 'Multimodal'] | [Link](https://omnimanip.github.io) | N/A |
| [VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://arxiv.org/abs/2501.05874) | Sung Ju Hwang, jinheon, KangsanKim71, starsuzi | - VideoRAG, a novel framework for Retrieval-Augmented Generation (RAG) over video corpora, is introduced, addressing the limitations of existing RAG approaches that primarily focus on text or static images and overlook the rich multimodal information in videos. - VideoRAG dynamically retrieves relevant videos based on their relevance to user queries and integrates both visual and textual information from these videos into the answer generation process using Large Video Language Models (LVLMs). -  For videos lacking textual annotations (like subtitles), VideoRAG utilizes automatic speech recognition to generate auxiliary text, enabling the use of both visual and textual modalities even when explicit textual data is absent. - Experimental results on the WikiHowQA and HowTo100M datasets demonstrate that VideoRAG significantly outperforms relevant RAG baselines, including text-based RAG and video-based RAG that only uses textual video descriptions. - Ablation studies highlight the importance of video content and both visual and textual modalities in improving the quality and informativeness of generated responses. | ['Multimodal', 'Video-Text-to-Text', 'Question Answering'] | N/A | N/A |
| [OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](https://arxiv.org/abs/2501.05510) | qiaozc, zyh, HelloJiang, Niujunbo2002, JoeLeelyf | - This paper introduces OVO-Bench (Online-VideO-Benchmark), a novel benchmark designed to evaluate the online video understanding capabilities of Video-LLMs. - OVO-Bench focuses on evaluating temporal awareness by assessing models' abilities in Backward Tracing, Real-Time Visual Perception, and Forward Active Responding. - The benchmark comprises 12 tasks, 644 videos, and ~2800 human-curated meta-annotations with precise timestamps, covering diverse domains and video lengths. - Evaluation results reveal that current Video-LLMs struggle with online video understanding, showing a substantial gap compared to human performance, especially in tasks requiring temporal reasoning and dynamic adaptation. - The authors suggest that a more powerful LLM backbone and better temporal prioritization mechanisms are crucial for improving online video understanding capabilities in Video-LLMs. | ['Video-Text-to-Text', 'Multimodal', 'Question Answering', 'Visual Question Answering'] | [Link](https://github.com/JoeLeelyf/OVO-Bench) | N/A |
| [LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs](https://arxiv.org/abs/2501.06186) | Dinura Dissanayake, hishamcholakkal, ahmedheakl, Ritesh-hf, omkarthawakar | - This paper introduces LlamaV-01, a novel multimodal visual reasoning model trained using a multi-step curriculum learning approach, along with a new benchmark called Visual Reasoning-Chain (VRC-Bench) designed to evaluate step-by-step reasoning and a novel metric to assess reasoning quality at the granularity of individual steps. - VRC-Bench includes over 1,000 samples and 4,173 reasoning steps across eight diverse categories, including visual reasoning, math and logic, and scientific reasoning. - The proposed LlamaV-01 model leverages curriculum learning, progressively training on tasks of increasing complexity to enhance reasoning abilities and combines Beam search with multi-step curriculum learning to manage complexity, improve logical coherence, and generalize to challenging scenarios. - The model outperforms existing open-source models and performs favorably against closed-source models, achieving an average score of 67.3 with a 3.8% absolute gain over Llava-CoT across six benchmarks, while also being 5x faster during inference. - The model's code and benchmark are publicly available. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/mbzuai-oryx/LlamaV-01) | [Link](https://huggingface.co/omkarthawakar/LlamaV-01), [Link](https://huggingface.co/datasets/omkarthawakar/VRC-Bench) |
| [Enabling Scalable Oversight via Self-Evolving Critic](https://arxiv.org/abs/2501.05727) | Losin94, Benyou, yeshoubaizi, ziniuli, tangzhy | - This paper introduces SCRIT (Self-evolving CRITic), a framework for enhancing the critique abilities of Large Language Models (LLMs) without external supervision. - SCRIT leverages a contrastive critique technique where the model analyzes student solutions by referencing correct solutions, along with a self-validation mechanism that ensures critique quality. - Implemented with Qwen2.5-72B-Instruct, SCRIT achieves up to a 10.3% improvement on critique-correction and error identification benchmarks and shows performance improvement across eight datasets in three scenarios. - The paper presents analyses showing that SCRIT's performance scales positively with data and model size, outperforms alternative approaches (Direct Critic and Bug-Injection Critic), and significantly benefits from its self-validation component. - It demonstrates consistent improvements across various problem domains, difficulties, and solution generation models. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning](https://arxiv.org/abs/2501.04698) | Ruimao, Xintao, Qiulin, ziyangy, Yuzhou914 | - ConceptMaster is a novel framework for Multi-Concept Video Customization (MCVC) that allows personalized video generation using multiple user-defined concepts without test-time tuning. - It addresses the identity decoupling problem in MCVC by learning decoupled multi-concept embeddings and injecting them into diffusion transformer models in a standalone manner using a Multi-Concept Injector (MC-Injector). - A dedicated data collection pipeline was created to build a dataset of over 1.3 million high-quality MCVC samples, which overcomes the scarcity of suitable training data. - A Multi-Concept Benchmark (MC-Bench) was introduced to evaluate concept fidelity, identity decoupling, and video generation quality across six concept composition scenarios. - Extensive experiments demonstrate ConceptMaster's superior performance over existing naive solutions and tuning-based methods, achieving high-quality video generation with accurate representation of multiple concepts. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | [Link](https://yuzhou914.github.io/ConceptMaster/) | N/A |
| [ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding](https://arxiv.org/abs/2501.05452) | danielpaulroth, jw2yang, zyang39, mqliu, Fiaa | - REFOCUS is a framework that improves multimodal Large Language Models (LLMs) ability to perform visual reasoning on structured images like tables and charts by enabling them to generate Python code to edit the input image. - REFOCUS guides the LLM's attention through visual edits such as drawing boxes, highlighting sections, and masking areas, simulating a visual chain of thought. - Experiments on table and chart VQA datasets show significant performance improvements over GPT-40 without visual editing, with average gains of 11.0% on table tasks and 6.8% on chart tasks. - A 14k training set created using REFOCUS and GPT-40 demonstrates that visual chain-of-thought supervision leads to better performance compared to training on standard VQA data or chain-of-thought data, with an 8.0% average gain over QA pairs and a 2.6% gain over CoT when fine-tuning a Phi-3.5-vision model. - Analysis suggests that REFOCUS enhances the LLM's visual grounding, OCR accuracy, and reduces hallucinations through selective attention. | ['Multimodal', 'Visual Question Answering', 'Table Question Answering'] | N/A | [Link](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) |
| [Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains](https://arxiv.org/abs/2501.05707) | Shuang Li, Joshua B. Tenenbaum, Antoniotorralbaborruel, yilundu, vsub851 | - This paper introduces multiagent finetuning, a novel approach to improve large language models (LLMs) by leveraging multiagent interaction and specialization. - Instead of training a single model, the approach trains multiple LLMs from the same base model and specializes each model to different functionalities like generating initial responses (generation agents) and critiquing/refining those responses (critic agents). - Each model is independently trained using data generated through multiagent debate between the models, fostering specialization and promoting response diversification. - Experiments across open-source and proprietary LLMs on a suite of reasoning tasks demonstrate significant performance gains and the ability to improve over more finetuning rounds compared to single-agent self-improvement methods. - The finetuned models exhibit better generalization capabilities to new datasets in a zero-shot setting. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://llm-multiagent-ft.github.io) | N/A |
| [Infecting Generative AI With Viruses](https://arxiv.org/abs/2501.05542) | fgmckee, dnoever | - This paper introduces a novel approach to penetration testing for large language models (LLMs), focusing on their handling of image files containing embedded malware. - The researchers successfully embedded the EICAR test file, a harmless string used to test antivirus software, within JPEG images and uploaded them to several LLMs, including GPT-40, Microsoft Copilot, Google Gemini 1.5 Pro, and Anthropic Claude 3.5 Sonnet. - They demonstrated the LLMs' ability to process and even extract the embedded malware using Python scripts within their environments, raising concerns about potential vulnerabilities. - This research highlights the need for improved security measures in LLMs, especially in detecting and preventing the execution of potentially malicious code hidden within seemingly benign files. - The study also suggests further research into automated LLM file inspection, standardized security testing frameworks for LLMs, and investigation of cross-platform vulnerabilities. | ['Multimodal'] | N/A | N/A |


## Papers for 2025-01-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives](https://arxiv.org/abs/2501.04003) | ZwwWayne, Chonghao, THUdyh, ldkong, shaoyuanxie | - DriveBench, a new benchmark designed to assess the reliability of Vision-Language Models (VLMs) in autonomous driving, is introduced. - The benchmark comprises 19,200 images, 20,498 question-answer pairs, and covers four driving tasks (perception, prediction, planning, behavior) under 17 settings, including clean, corrupted, and text-only inputs, to evaluate VLM robustness and visual grounding. - Evaluations of 12 popular VLMs reveal that they often generate plausible but fabricated responses based on general knowledge rather than visual cues, especially with missing or degraded visual inputs. - This behavior poses risks in safety-critical scenarios like autonomous driving, and is masked by dataset imbalances and inadequate metrics. - The study emphasizes the need for refined evaluation metrics that focus on multi-modal understanding and robust visual grounding, and highlights the potential of using VLMs' awareness of corruptions to enhance their reliability. | ['Multimodal', 'Visual Question Answering', 'Computer Vision'] | [Link](https://github.com/drive-bench) | [Link](https://huggingface.co/datasets/drive-bench/arena) |
| [Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models](https://arxiv.org/abs/2501.04828) | Ece Elif Adak, tcTHEBESTMAN, fatihburakkaragoz, temretiras, sbozates | - This paper introduces the first Named Entity Recognition (NER) dataset (HisTR) and Universal Dependencies treebank (OTA-BOUN) for historical Turkish, alongside a cleaned text corpus (OTC) and transformer-based models for NER, dependency parsing, and part-of-speech tagging. - HisTR consists of 812 manually annotated sentences, while OTA-BOUN contains 514 sentences annotated with part-of-speech tags and dependency relations.  - The models were trained using BERTurk, mBERT, and TURNA architectures.  - Experimental results show that BERTurk outperforms mBERT in NER and dependency parsing of historical Turkish, and fine-tuning with a combination of modern and historical Turkish data improves performance. - The resources and models are publicly available, establishing a baseline for future research in historical Turkish NLP. | ['Natural Language Processing', 'Token Classification', 'Text Classification', 'Question Answering'] | [Link](https://github.com/UniversalDependencies/UD_Ottoman_Turkish-BOUN/tree/dev), [Link](https://github.com/Ottoman-NLP/ottominer-public) | [Link](https://huggingface.co/datasets/bucolin/HisTR), [Link](https://huggingface.co/datasets/bucolin/OTA-BOUN_UD_Treebank), [Link](https://huggingface.co/datasets/bucolin/OTC-Corpus), [Link](https://huggingface.co/bucolin) |
| [Entropy-Guided Attention for Private LLMs](https://arxiv.org/abs/2501.03489) | Brandon Reagen, nandan523 | - This research introduces an entropy-guided attention mechanism for enhancing the privacy of large language models (LLMs) during inference. - Researchers discovered that removing nonlinearities in LLMs can cause training instability due to entropy collapse in deeper layers and entropic overload in earlier layers. - The study presents a novel entropy regularization technique and proposes PI-friendly alternatives to layer normalization. - Experimental results show that the proposed methods reduce communication overhead by 3.94x and improve inference speed by 1.72x in a private setting. - The work bridges information theory and architectural design, utilizing entropy dynamics to guide the development of efficient privacy-preserving LLM architectures. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Nandan91/entropy-guided-attention-llm) | N/A |
| [Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model](https://arxiv.org/abs/2501.05122) | Radu Timofte, Chris Biemann, Carolin Holtermann, Florian Schneider, Gregor Geigle | - This paper introduces Centurio, a massively multilingual Large Vision-Language Model (LVLM) supporting 100 languages, trained by machine-translating high-quality English data and benchmarked across 13 downstream vision-language tasks covering 43 diverse languages. - The study investigates optimal language distributions of pre-training and instruction-tuning data, finding that including up to 100 languages with as little as 25-50% non-English data improves multilingual performance while maintaining strong English performance.  - The research also introduces a new benchmark, SMPQA (Synthetic Multilingual Plot Question Answering), for evaluating multilingual text-in-image understanding and finds that non-English OCR data in training is crucial for this task.  - Centurio achieves state-of-the-art results on 14 tasks covering 56 languages, matching popular models' performance on English while outperforming them on low-resource languages.  - One limitation is the heavy reliance on machine-translated data and the comparatively small image input resolution which affects performance on text-heavy tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |


## Papers for 2025-01-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://arxiv.org/abs/2501.04519) | Youran Sun, Yifei Liu, Xinyu Guan, J-shang, lynazhang | - rStar-Math is a novel framework that allows smaller language models to achieve state-of-the-art mathematical reasoning capabilities comparable to OpenAI's models. - It employs Monte Carlo Tree Search (MCTS) with a math policy SLM and a process reward model (PRM), and introduces innovations in data synthesis, reward modeling, and self-evolution. - A code-augmented chain-of-thought data synthesis method generates verifiable reasoning steps, and a process preference model (PPM) is trained using a pairwise ranking loss, eliminating the need for precise step-level reward annotation. - The system iteratively evolves the policy SLM and PPM to improve reasoning capabilities without relying on distillation from larger models. - Evaluations on various benchmarks show significant performance boosts, rivaling or exceeding OpenAI ol on competition-level problems, even with smaller model sizes. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/microsoft/rStar) | N/A |
| [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/abs/2501.04686) | Xinzhe Ni, Yiyao Yu, Yifan Wang, fun6668, AntimageTHU | - This paper introduces URSA-7B, a multimodal large language model (MLLM) designed for enhanced mathematical reasoning, using a three-module synthesis strategy integrating chain-of-thought (CoT) distillation, trajectory format rewriting, and format unification for training data creation. - The model architecture comprises a hybrid vision encoder (SAM-B and SigLIP-L) combined with Qwen2.5-Math-7B-Instruct and trained with an MLP projector aligner between the vision and language models. - URSA-7B achieves state-of-the-art performance on several multimodal mathematical reasoning benchmarks, including MathVista, MathVerse, and DYNAMATH, outperforming other open-source models and some closed-source models. - For test-time scaling, a dual-view process supervision data synthesis strategy is proposed, generating the DualMath-1.1M dataset and URSA-RM-7B, a verifier model that improves URSA-7B's reasoning path selection and accuracy. - URSA-RM-7B shows strong out-of-distribution (OOD) verification capabilities, particularly on the Multimath-7B CoT solutions, indicating improved robustness and generalisation in multimodal mathematical reasoning | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though](https://arxiv.org/abs/2501.04682) | Kanishk Gandhi, Charlie Snell, Violet Xiang, nlile, Asap7772 | - This paper proposes Meta Chain-of-Thought (Meta-CoT), a framework extending traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning process. - Meta-CoT models the latent "thinking" process involved in complex reasoning, addressing the limitations of traditional CoT in capturing non-linear, iterative, and latent exploration and verification. - Empirical evidence from state-of-the-art models like OpenAI's "o1" and DeepSeek-R1 shows behaviors consistent with internalized search, supporting the Meta-CoT hypothesis. - The authors outline a training pipeline for Meta-CoT, incorporating instruction tuning with linearized search traces and reinforcement learning. -  A "Big MATH" project is introduced, aiming to create a dataset of over 1,000,000 verifiable math problems to facilitate research in this area. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [LLM4SR: A Survey on Large Language Models for Scientific Research](https://arxiv.org/abs/2501.04306) | Xinya Du, Wei Yang, Ziming Luo, Ason-jay, ZonglinY | - This survey paper explores the transformative role of Large Language Models (LLMs) in the scientific research process, covering hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing. - The paper analyzes how LLMs contribute to each stage, summarizing methodologies, benchmarks, and evaluation methods, and identifying current challenges and future research directions. - It provides a comprehensive overview of LLM applications across the entire scientific workflow, unlike previous surveys that focused on specific LLM capabilities or individual research stages. - The survey identifies key components and trends in each application area, such as feedback modules in hypothesis discovery, agent-based automation in experiment implementation, and multi-model architectures in peer review generation. - The authors conclude that while LLMs face limitations in areas like planning, prompt robustness, and domain-specific expertise, their ongoing development holds immense potential to revolutionize scientific research by enhancing productivity, fostering innovation, and promoting collaboration. | ['Natural Language Processing'] | [Link](https://github.com/du-nlp-lab/LLM4SR) | N/A |
| [GeAR: Generation Augmented Retrieval](https://arxiv.org/abs/2501.02772) | Hao Sun, Yuefeng Zhan, Jianfeng Liu, Shaohan Huang, noobimp | - This paper introduces Generation Augmented Retrieval (GeAR), a novel retrieval method that incorporates fusion and decoding modules to generate relevant text from documents based on the fused representation of the query and the document, enhancing fine-grained information retrieval. - GeAR consists of a bi-encoder for initial encoding of queries and documents, a fusion encoder utilizing cross-attention to combine query and document embeddings, and a text decoder to generate relevant information from the fused representation. - The model is trained using contrastive learning loss for retrieval and language modeling loss for generation, enabling joint optimization of retrieval and fine-grained understanding. - Experimental results demonstrate competitive performance in document retrieval and units localization tasks across various datasets, showing improvements over traditional retrieval methods, especially in capturing fine-grained semantic relationships. - GeAR also exhibits promising information generation capabilities and offers insights into the interpretation of retrieval results through visualization of information localization and cross-attention weights. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection](https://arxiv.org/abs/2501.04575) | Xueyu Hu, Congkai Xie, Zishu Wei, Yuhang Liu, pengxiang | - InfiGUIAgent, a Multimodal Large Language Model (MLLM)-based Graphical User Interface (GUI) agent, is introduced for enhanced task automation on computing devices. - The agent employs a two-stage supervised fine-tuning approach where the first stage focuses on fundamental GUI understanding, grounding, and visual-language comprehension, while the second stage integrates advanced reasoning skills, including hierarchical and expectation-reflection reasoning, using synthesized trajectory data. - InfiGUIAgent leverages a modular action space design enabling flexible action combinations and utilizes reference-augmented annotation for precise spatial referencing in GUI interactions. - Experimental results on ScreenSpot and AndroidWorld benchmarks demonstrate InfiGUIAgent's superior performance compared to several open-source baselines. - The model achieves a 76.3% accuracy on ScreenSpot, surpassing models like ShowUI and UGround-7B, and a 0.09 overall success rate on AndroidWorld, outperforming similar-sized models and some with larger parameter sizes, showcasing its effective GUI task automation capabilities without relying on additional GUI metadata. | ['Multimodal'] | [Link](https://github.com/Reallm-Labs/InfiGUIAgent) | N/A |
| [DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization](https://arxiv.org/abs/2501.03271) | Rajarshi Roy, Danush Khanna, Suranjana Trivedy, Amitava Das, amanchadha | - This paper introduces DPO-Kernels, a framework enhancing Direct Preference Optimization (DPO) for aligning large language models (LLMs) with human preferences by integrating kernel methods and diverse divergence measures. - DPO-Kernels incorporates kernelized representations, divergence alternatives (Jensen-Shannon, Hellinger, Rényi, Bhattacharyya, Wasserstein, and f-divergences), and data-driven selection of optimal kernel-divergence pairs. - It introduces a Hierarchical Mixture of Kernels (HMK) to combine local and global kernels for precise and large-scale semantic modeling, automatically selecting the optimal kernel mixture during training. - Evaluations on 12 datasets demonstrate state-of-the-art generalization across various alignment tasks, including factuality, safety, reasoning, and instruction following. - Despite increased computational cost (3-4x higher than standard DPO), the improvements in alignment and generalization justify the overhead, with future work aiming to address scalability challenges. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/Anthropic/hh-rlhf), [Link](https://huggingface.co/datasets/nvidia/HelpSteer), [Link](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations), [Link](https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k), [Link](https://huggingface.co/datasets/tatsu-lab/alpaca_farm/viewer/alpaca_human_preference), [Link](https://huggingface.co/datasets/stanfordnlp/SHP-2), [Link](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized), [Link](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs) |


## Papers for 2025-01-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/abs/2501.03895) | Yang Feng, Zhe Yang, Qingkai Fang, Shaolei Zhang | - LLaVA-Mini is an efficient large multimodal model (LMM) that minimizes the number of vision tokens while maintaining performance comparable to larger models like LLaVA. - It introduces a modality pre-fusion module to combine visual and textual information before compression, enabling the model to use only one vision token per image or video frame. - LLaVA-Mini incorporates a query-based compression module to reduce the number of vision tokens, leading to significant improvements in computational efficiency (77% FLOPs reduction), inference speed (2.92x faster), and memory usage. - Experiments across 11 image and 7 video benchmarks demonstrate LLaVA-Mini's ability to achieve comparable performance to LLaVA with just 1 vision token instead of 576, even on high-resolution images and long videos. - It also allows for processing long videos exceeding 10,000 frames on a 24GB GPU, a significant advancement in efficient real-time multimodal interaction. | ['Multimodal', 'Image-Text-to-Text', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/ictnlp/LLAVA-Mini) | [Link](https://huggingface.co/ICTNLP/1lava-mini-llama-3.1-8b) |
| [Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos](https://arxiv.org/abs/2501.04001) | Shilin Xu, Zilong Huang, Tao Zhang, Xiangtai Li, HarborYuan | - Sa2VA is a novel unified model for dense grounded understanding of images and videos, integrating the Segment Anything Model 2 (SAM-2) with Large Language and Vision Assistant (LLaVA)-like Multimodal Large Language Models (MLLMs). - This architecture unifies text, image, and video data within a shared token space, enabling instruction-guided mask generation by SAM-2, which facilitates grounded multimodal understanding. - Sa2VA supports various tasks such as image and video conversations, referring image/video segmentation, and grounded caption generation through a single-shot instruction-tuning process.  - The model achieves state-of-the-art performance across multiple tasks, including referring video object segmentation, outperforming previous methods by a significant margin on the Ref-SAV dataset (over 15% under zero-shot settings). -  A key contribution is the introduction of Ref-SAV, a new large-scale dataset for referring video segmentation, which consists of more than 72,000 video object expressions.  | ['Multimodal', 'Image Segmentation', 'Visual Question Answering', 'Video Classification', 'Mask Generation'] | N/A | [Link](https://huggingface.co/ByteDance/Sa2VA-4B) |
| [PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides](https://arxiv.org/abs/2501.03936) | Hongyu Lin, Jia Zheng, Hao Kong, Xinyan Guan, Forceless | - PPTAgent, a novel framework, redefines automatic presentation generation as an interactive, edit-based workflow using both a document and reference presentation as inputs. - The two-stage framework first analyzes reference presentations for structural patterns and content through slide clustering and schema extraction, and then generates new slides via code actions, ensuring consistency and alignment through feedback mechanisms. - PPTAgent introduces PPT Eval, a comprehensive evaluation framework for presentation quality across three dimensions: Content, Design, and Coherence, using a multi-dimensional LLM-as-a-judge approach. - Experimental results demonstrate that PPTAgent outperforms current end-to-end text-generation methods, achieving a 97.8% success rate and a 3.67 average PPT Eval score across three dimensions: Content, Design, and Coherence, indicating high-quality presentation generation. - A new presentation dataset Zenodo10K consisting of 10,448 presentations with diverse domains is collected from Zenodo. | ['Multimodal'] | [Link](https://github.com/icip-cas/PPTAgent) | N/A |
| [Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback](https://arxiv.org/abs/2501.03916) | Tao Chen, Botian Shi, Xiangchao Yan, Jiakang Yuan, BoZhang | - DOLPHIN is a closed-loop, open-ended automatic research framework designed to automate the scientific research process, encompassing idea generation, experimental verification, and feedback. - It retrieves and ranks relevant papers based on topic and task attributes, generates novel research ideas guided by these papers, filters them for novelty and independence, and formulates experimental plans using LLMs. - DOLPHIN automatically generates and debugs code using an exception-traceback-guided process and analyzes experimental results to provide feedback for subsequent idea generation rounds. - Experimental results on benchmarks like CIFAR-100, ModelNet40, and SST-2 show that DOLPHIN generates ideas that improve performance over baselines and, in some cases, achieves state-of-the-art results, demonstrating its potential for automated research. -  DOLPHIN automatically generated methods based on PointNet that showed comparable performance to human-designed state-of-the-art 3D classification methods on ModelNet40. | ['Computer Vision', 'Image Classification', 'Zero-Shot Object Detection', 'Text-to-3D', 'Image-to-3D', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2025-01-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning](https://arxiv.org/abs/2501.03226) | lindahua, yhcao, KennyUTC, yuhangzang, BeichenZhang | - BoostStep, a novel method, enhances the mathematical reasoning capabilities of Large Language Models (LLMs) by improving single-step reasoning through refined in-context learning. - It addresses the granularity mismatch and negative-effect noise within traditional in-context learning examples by providing step-level guidance with a "first-try" strategy. - This strategy retrieves highly related in-context examples based on the model's initial reasoning attempt for each step, improving reasoning quality. - BoostStep improves the performance of GPT-40 and Qwen2.5-Math-72B by 3.6% and 2.0% respectively on various mathematical benchmarks, and by 7.5% when combined with Monte Carlo Tree Search (MCTS). - It seamlessly integrates with MCTS, improving both candidate generation and decision-making processes. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/beichenzbc/BoostStep) | N/A |
| [Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction](https://arxiv.org/abs/2501.03218) | myownskyW7, lindahua, yhcao, yuhangzang, Mar2Ding | - Dispider is a novel system designed for active, real-time interaction with streaming videos, disentangling perception, decision, and reaction into asynchronous modules. - It features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction using scene-based features and historical interactions. - An asynchronous interaction module provides detailed responses without interrupting video processing, ensuring real-time performance and multi-step reasoning capabilities. - Experiments on StreamingBench and a subset of ETBench show Dispider significantly outperforms existing online video LLM models in temporal grounding and proactive response generation. - It also maintains strong performance on conventional video QA tasks across long-video benchmarks such as EgoSchema, VideoMME, and MLVU, demonstrating effectiveness in handling long video lengths and complex interactions. | ['Video-Text-to-Text', 'Multimodal', 'Question Answering'] | [Link](https://github.com/Mark12Ding/Dispider) | N/A |
| [Personalized Graph-Based Retrieval for Large Language Models](https://arxiv.org/abs/2501.02157) | Franck-Dernoncourt, namyongp, Ojasmitha17, Tobilee, StevenAu | - This paper introduces PGraphRAG, a framework leveraging user-centric knowledge graphs for personalized text generation with LLMs. - PGraphRAG enhances personalization by augmenting prompts with user-relevant context retrieved from the knowledge graph, improving contextual understanding and output quality. - A new benchmark, the Personalized Graph-based Benchmark for Text Generation, is presented to evaluate the effectiveness of PGraphRAG. - Experimental results demonstrate that PGraphRAG significantly outperforms state-of-the-art personalization methods, especially in cold-start scenarios with limited user history. - The integration of structured user knowledge graphs through PGraphRAG allows for richer, contextually appropriate personalized responses. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/PGraphRAG-benchmark/PGR-LLM) | N/A |
| [Test-time Computing: from System-1 Thinking to System-2 Thinking](https://arxiv.org/abs/2501.02497) | Jia Xu, Kaixin Wu, Hai Ye, douvleplus, Yisam | - This paper surveys test-time computing methods, categorizing them into System-1 (for perceptual tasks) and System-2 (for cognitive tasks) models. - For System-1, test-time adaptation methods like parameter updates, input modification, representation editing, and output calibration are discussed, focusing on enhancing robustness and generalization. - For System-2, techniques such as repeated sampling, self-correction, and tree search are explored, aiming to improve reasoning and planning abilities. - The paper traces the evolution from System-1 to System-2 thinking, emphasizing test-time computing's role in this transition, and suggests potential future directions. - The o1 model is used as an example of the test-time computing scaling effect where increased computational effort at inference leads to improved performance in complex reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Dereck0602/Awesome_Test_Time_LLMs) | N/A |
| [METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring](https://arxiv.org/abs/2501.02045) | willieneis, oliu-io, upup-ashton-wang, Johannes, oliu-io | - METAGENE-1 is a 7-billion parameter autoregressive transformer model, pre-trained on a novel corpus of 1.5 trillion base pairs of metagenomic DNA and RNA sequences from wastewater. - The model utilizes byte-pair encoding (BPE) tokenization tailored for metagenomic sequences and a decoder-only architecture. - In benchmarks, METAGENE-1 achieves state-of-the-art results on genomic tasks like pathogen detection and sequence embedding, outperforming models trained on curated species genomes. - It demonstrates potential for pandemic monitoring and early detection of emerging health threats through wastewater analysis. - The model's open-source release aims to accelerate research in genomic anomaly detection while acknowledging safety considerations for future model development. | ['Natural Language Processing', 'Text Generation'] | [Link](github.com/metagene-ai) | [Link](huggingface.co/metagene-ai) |
| [Ingredients: Blending Custom Photos with Video Diffusion Transformers](https://arxiv.org/abs/2501.01790) | Di Qiu, MichaelFan, Changqian, Debang, onion | - Ingredients, a training-free framework, is introduced for customizing video generation with video diffusion transformers by incorporating multiple user-provided ID photos. - The framework includes a facial extractor, a multi-scale projector, and an ID router to handle ID features, embed them into the video diffusion transformer context, and allocate the ID embeddings to corresponding regions, respectively. - Ingredients supports multi-ID customization without prompt constraints, offering flexibility and precision in video synthesis. - Evaluations show Ingredients' superior performance in generating high-quality, editable videos with consistent multi-human customization, exceeding baseline methods quantitatively and qualitatively. - The data, code, and model weights are publicly available for research. | ['Text-to-Video', 'Image-to-Video', 'Multimodal'] | [Link](https://github.com/feizc/Ingredients) | [Link](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct), [Link](https://huggingface.co/openai/clip-vit-base-patch32) |
| [Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models](https://arxiv.org/abs/2501.01830) | Weiqiang Wang, Huijia Zhu, Yaojie Lu, Shuhen Zhou, Yanjiang Liu | - AUTO-RT, a reinforcement learning framework, automatically explores and optimizes complex attack strategies to uncover security vulnerabilities in Large Language Models (LLMs) through malicious queries. - It introduces Early-terminated Exploration to accelerate exploration by focusing on high-potential attack strategies and a Progressive Reward Tracking algorithm to dynamically refine the search trajectory towards successful vulnerability exploitation. - It operates in a black-box setting, requiring only access to a model's textual outputs, making it adaptable to diverse LLMs. - Experiments across various LLMs demonstrate that AUTO-RT detects a broader range of vulnerabilities with a faster detection speed and 16.63% higher success rate compared to existing methods. - AUTO-RT improves exploration efficiency and automatically optimizes attack strategies. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/icip-cas/Auto-RT) | N/A |
| [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506) | Yufei Xu, Xuesong Yao, Zhengyin Du, Junjie Ye, maverick1994 | - ToolHop, a new dataset with 995 multi-hop queries and 3,912 associated tools, is introduced to evaluate large language models (LLMs) in multi-hop tool use scenarios. - ToolHop employs a query-driven data construction approach involving tool creation, document refinement, and code generation. - An evaluation of 14 LLMs across five model families (LLaMA, Qwen, Gemini, Claude, and GPT) reveals that even the top-performing model (GPT-4) only achieves 49.04% accuracy. - Analysis suggests that providing LLMs with tools significantly improves their performance, but there are still significant challenges. - Different LLM families exhibit distinct tool-use patterns, with Qwen tending towards parallel calls that result in hallucinations, while GPT leverages tool feedback effectively to improve tool usage. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/bytedance-research/ToolHop) |


## Papers for 2025-01-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction](https://arxiv.org/abs/2501.01957) | hertin, shenyunhang, yifanzhang114, xiongwang, linhaojia13 | - VITA-1.5 is a multimodal large language model (MLLM) that integrates vision, language, and speech modalities using a three-stage training approach, enabling real-time vision and speech interaction. - The model architecture consists of vision and audio encoders with adapters connected to an LLM, and an end-to-end speech generation module, eliminating the need for external ASR and TTS systems. - The training process involves vision-language training, followed by audio input tuning and audio output tuning stages which aims at minimizing the training conflicts between the multiple modalities. - VITA-1.5 achieves comparable performance to state-of-the-art models on image and video understanding benchmarks and exhibits significant improvements in speech capabilities. - Evaluation on ASR benchmarks shows that VITA-1.5 outperforms specialized speech models in both Mandarin and English tasks, highlighting the models ability to integrate effective real-time vision and audio-speech interaction. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Automatic Speech Recognition', 'Text-to-Speech'] | [Link](https://github.com/VITA-MLLM/VITA) | [Link](https://huggingface.co/OpenGVLab/InternViT-300M-448px) |
| [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904) | jrwen, whenfra, yifanli, JohnCage, Richard1999 | - This paper introduces Virgo, a multimodal slow-thinking system designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) for complex visual tasks. - The core approach involves fine-tuning a capable MLLM (Qwen2-VL-72B-Instruct) with a small amount of textual long-form thought data, hypothesizing that slow-thinking capacity is primarily associated with the language component and can transfer across modalities. - Experimental results on MathVerse, MathVision, OlympiadBench, and MMMU benchmarks demonstrate that Virgo achieves competitive performance compared to commercial reasoning systems, sometimes even surpassing them. - It was found that textual reasoning data is generally more effective than visual reasoning data for improving the reasoning ability of the MLLMs. - Further analysis suggests that harder tasks benefit more from long thought reasoning, but excessively long reasoning processes may lead to performance degradation; moreover, current visual instruction generation methods do not show significant advantages over textual ones. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/RUCAIBox/Virgo) | N/A |
| [BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery](https://arxiv.org/abs/2501.01540) | Louise Li, Lyle Goodyear, ngoodman, michaelyli, obiwan96 | - BoxingGym, a new benchmark, has been introduced to evaluate the performance of autonomous agents in experimental design and model discovery within a scientific context. - The benchmark uses 10 simulated environments based on real-world scientific models, allowing agents to actively experiment and revise theories based on data. - Evaluation metrics include Expected Information Gain for experiment design and a communication-based approach where an agent's explanation enables a novice agent to make predictions for model discovery. - Initial experiments show that current LLMs, augmented or not with statistical modeling capabilities, struggle with both experimental design and model discovery. - The benchmark aims to promote research on agents capable of iterative model discovery through active experimentation and communication. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/kanishkg/boxing-gym/tree/v0.1.0-beta) | N/A |
| [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/abs/2501.00874) | anoperson, Franck-Dernoncourt, ryanrossi, ntnghia1811, Hieuman | - LUSIFER is a novel zero-shot approach that adapts English LLM-based embedding models for multilingual tasks without requiring explicit multilingual supervision. - It leverages XLM-R's multilingual representations and a learnable connector to transfer language understanding to English-optimized LLM embedding models. - Experimental results on 123 datasets across 14 languages show a significant performance increase, averaging 3.19 points across all tasks, with substantial gains for medium and low-resource languages. - In cross-lingual scenarios involving over 100 languages, LUSIFER surpasses existing English-centric models by 5.75 points on average. - This approach enhances multilingual representation capabilities without the need for explicit multilingual supervision. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | [Link](https://github.com/hieum98/lusifer) | N/A |


## Papers for 2025-01-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/abs/2501.00958) | Yongliang Shen, Jiashuo Sun, Xin Li, Hang Zhang, Wenqi Zhang | - This paper introduces "Textbook", a large-scale, high-quality multimodal dataset designed for vision-language pretraining. - The dataset is constructed from 2.5 years of online instructional videos, totaling 22,000 class hours and covering subjects like mathematics, physics, and chemistry.  - The videos are processed into an interleaved format of images and text, focusing on coherent context and knowledge density. - Experimental results demonstrate that models pretrained on "Textbook" achieve significant improvements on knowledge- and reasoning-intensive tasks, such as ScienceQA and MathVista. - The dataset improves few-shot performance, attributed to its video-centric interleaved design that enhances in-context learning capabilities. | ['Multimodal'] | [Link](https://github.com/DAMO-NLP-SG/multimodal_textbook) | N/A |
| [CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/abs/2501.01257) | Dayiheng Liu, Bo Zheng, Bowen Yu, Jiaxi Yang, Shanghaoran Quan | - This paper introduces CODEELO, a new benchmark for evaluating competition-level code generation capabilities of Large Language Models (LLMs). - CODEELO uses problems from the CodeForces platform, along with contest divisions, problem difficulty ratings, and problem algorithm tags.  - The benchmark employs a unique evaluation method where solutions are submitted directly to the CodeForces platform, ensuring accurate judgments with special judge support and a consistent execution environment.  - An Elo rating system is implemented to provide human-comparable ratings for LLMs based on their performance.  - Initial evaluation results demonstrate that OpenAI's o1-mini and QwQ-32B-Preview models achieve high Elo ratings, outperforming most open-source models. | ['Natural Language Processing', 'Text2Text Generation', 'Text Generation'] | N/A | [Link](https://hf.co/Qwen/CodeElo) |
| [VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM](https://arxiv.org/abs/2501.00599) | Boqiang Zhang, Zesen Cheng, Wentong Li, Hang Zhang, Yuqian Yuan | - The VideoRefer Suite is introduced, which consists of a new dataset (VideoRefer-700K), a model (VideoRefer), and a benchmark (VideoRefer-Bench) designed to improve the spatial-temporal object understanding capabilities of Video Large Language Models (LLMs). - VideoRefer, built upon VideoLLaMA2, incorporates a novel spatial-temporal object encoder that extracts pixel-level regional features within single frames and aggregates temporal information across multiple frames using a Temporal Token Merge Module. - The VideoRefer-700K dataset was created using a multi-agent data engine, incorporating diverse object-level instructions including descriptions, short captions, and multi-round question-answer pairs. - Experimental results demonstrate that VideoRefer outperforms existing generalist and specialist models on VideoRefer-Bench and a traditional video referring benchmark, HC-STVG, showing improved performance in metrics like subject correspondence, appearance and temporal description, and hallucination detection. - Moreover, VideoRefer also shows performance improvements on general video understanding benchmarks like Perception-Test, MVBench, and VideoMME, indicating its broader applicability. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/abs/2501.01264) | Wenbo Su, Jiaheng Liu, Weixun Wang, Yanan Wu, Xiaoshuai Song | - ProgCo, a novel program-driven self-correction method for Large Language Models (LLMs), is introduced, comprising program-driven verification (ProgVe) and refinement (ProgRe). - ProgVe employs self-generated, self-executing pseudo-programs for enhanced verification logic and validation, while ProgRe uses dual reflection and refinement on both responses and verification programs to mitigate misleading feedback in complex reasoning. - Experiments across instruction-following and mathematical reasoning tasks demonstrates ProgCo's efficacy in achieving effective self-correction. - Combining ProgCo with real program tools further enhances performance. - ProgCo shows greater improvement in mathematical reasoning tasks compared to existing baselines, highlighting its effectiveness in complex reasoning scenarios. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models](https://arxiv.org/abs/2501.00316) | Md Hasebul Hasan, Md Tanvir Parvez, Md Tanvir Hassan, Mahir Labib Dihan, eunus | - This paper introduces MAPEVAL, a benchmark designed to evaluate the geo-spatial reasoning capabilities of foundation models. - MAPEVAL features three task types: textual, API-based, and visual, requiring models to process diverse geo-spatial contexts, perform compositional reasoning, and interact with map tools. - An evaluation of 28 prominent foundation models using MAPEVAL revealed that while models like Claude-3.5-Sonnet, GPT-40, and Gemini-1.5-Pro performed competitively, substantial performance gaps exist, especially in the API-based tasks, and all models fall short of human performance. - The benchmark includes 700 unique multiple-choice questions spanning locations across 180 cities and 54 countries. - Further analyses suggest that integrating external tools, like calculators, can enhance performance in specific sub-tasks, such as calculating straight-line distances and cardinal directions. | ['Multimodal', 'Question Answering'] | [Link](https://github.com/MapEval) | N/A |
| [Dynamic Scaling of Unit Tests for Code Reward Modeling](https://arxiv.org/abs/2501.01054) | Sijia Luo, Jifan Yu, Jing Zhang, Xiaokang Zhang, KAKA22 | - This paper introduces CodeRM-8B, a lightweight unit test generator designed for efficient and high-quality unit test scaling in code reward modeling. - The authors demonstrate that scaling the number of unit tests improves the quality of the reward signal, particularly for more complex problems, leading to better identification of correct code solutions. - CodeRM-8B is trained using a novel data synthesis pipeline that produces high-quality unit tests from existing code instruction-tuning datasets. It leverages supervised fine-tuning (SFT) on Llama3.1-8B and implements a dynamic scaling mechanism adapting to problem difficulty for improved efficiency. - Experimental results on HumanEval Plus, MBPP Plus, and LiveCodeBench show significant performance improvements across various models, with CodeRM-8B achieving gains of 18.43% for Llama3-8B and 3.42% for GPT-40-mini on HumanEval Plus, comparable to the much larger Llama3.1-70B model. - Dynamic unit test scaling further boosts performance by allocating more computation resources to harder problems, leading to an additional 0.5% performance gain on MBPP Plus under fixed computational cost. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://code-reward-model.github.io) | [Link](https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction), [Link](https://huggingface.co/datasets/BAAI/TACO) |
| [MLLM-as-a-Judge for Image Safety without Human Labeling](https://arxiv.org/abs/2501.00192) | Felix Juefei-Xu, Xiaowen Lin, Shiyu Zhao, Shuming Hu, Zhenting Wang | - This paper introduces CLUE (Constitutional MLLM JUdgE), a novel framework for zero-shot image safety judgment using Multimodal Large Language Models (MLLMs) and a predefined safety constitution (a set of safety rules). - CLUE addresses challenges like subjective rules, complex constitutions, and model biases by objectifying rules, assessing rule-image relevance using contrastive models, and employing debiased token probabilities with logical precondition chains for judgments. - The method includes a deeper reasoning mechanism with cascaded chain-of-thought processes, if necessary, offering high confidence and explanations. - The experiments demonstrate CLUE's significant improvement over zero-shot and fine-tuning baselines for image safety classification using objective rules. - For example, CLUE with InternVL2-76B achieves 95.9% recall, 94.8% accuracy, and an F1-score of 0.949 on a newly created benchmark. | ['Multimodal', 'Zero-Shot Classification', 'Computer Vision'] | N/A | N/A |
| [Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing](https://arxiv.org/abs/2501.00658) | Jiajun Zhu, Yuehao Wang, Ruisi Cai, Peihao Wang, pragsri8 | - This paper identifies two key limitations of State Space Models (SSMs): strong recency bias, hindering recall of distant information and robustness, and over-smoothing in deeper architectures, making token representations indistinguishable. - The authors theoretically demonstrate the recency bias of SSMs, showing that influential scores between tokens decay exponentially with distance, and empirically validate this through a long-context retrieval task, where SSMs struggle compared to Transformers. - Over-smoothing is revealed through scaling experiments and theoretical analysis, demonstrating that SSM layers diminish pairwise differences between memory states, causing performance to plateau and decline with increasing depth. - A novel "polarization" technique is proposed, reserving dedicated channels in state transition matrices for values of zero and one, simultaneously addressing both recency and over-smoothing. - Experiments on associative recall demonstrate that polarization enhances long-range recall accuracy and enables SSMs to benefit from deeper architectures. | ['Natural Language Processing'] | [Link](https://github.com/VITA-Group/SSM-Bottleneck) | N/A |
| [MapQaTor: A System for Efficient Annotation of Map Query Datasets](https://arxiv.org/abs/2412.21015) | Md Rizwan Parvez, Mohammed Eunus Ali, mahirlabibdihan | - MAPQATOR is a web application designed to streamline the creation of map-based question answering (QA) datasets by integrating with any map API in a plug-and-play manner, reducing manual effort. - It offers visualization tools and caches API responses for consistent ground truth and data reliability, enabling complex geospatial reasoning evaluation and improvement. - Evaluation shows MAPQATOR speeds up annotation by at least 30 times compared to manual methods. - The system addresses the gap in efficiently annotating language-map reasoning tasks, aiding in developing reliable LLM training datasets. - MAPQATOR facilitates better geospatial understanding by centralizing data retrieval, annotation, and visualization, benefiting tasks like complex map reasoning. | ['Question Answering'] | [Link](https://github.com/mapqator/) | N/A |


## Papers for 2025-01-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723) | Yian Wang, Chuanyang Jin, Kanzhi Cheng, heroding77, QiushiSun | - OS-Genesis is a novel GUI data synthesis pipeline that automates the construction of high-quality and diverse agent trajectories without human supervision or predefined tasks. - Instead of relying on predefined tasks, OS-Genesis allows agents to explore environments freely and interact step-wise, retroactively deriving tasks from interactions. This interaction-driven approach reverses the conventional trajectory collection process and enables more effective exploration. - The pipeline incorporates a trajectory reward model (TRM) to filter and prioritize the synthesized trajectories for more effective utilization. - Evaluations on challenging online mobile benchmarks like AndroidWorld show that OS-Genesis doubles the performance of existing task-driven methods and outperforms other data synthesis approaches on unseen, out-of-distribution apps. -  Analysis suggests that OS-Genesis significantly increases the diversity of both generated instructions and trajectories, more closely mirroring human-like interactions with digital environments than existing synthetic data generation methods. | ['Multimodal'] | N/A | N/A |
| [Xmodel-2 Technical Report](https://arxiv.org/abs/2412.19638) | Jiang Ling, Qu Zhijiu, Lin Qingquan, Liu Yang, valeriaWong | - Xmodel-2 is a 1.2 billion parameter large language model specialized for reasoning tasks, using a unified set of hyperparameters across different model scales for efficient experimentation and configuration transfer. - Trained on 1.5 trillion tokens, Xmodel-2 employs the Warmup-Stable-Decay (WSD) learning rate scheduler from MiniCPM for enhanced training efficiency and stability. - It achieves state-of-the-art performance on complex reasoning and agent-based tasks compared to other models in the 1-2 billion parameter range, demonstrating the effectiveness of its design and training approach. - The model architecture is similar to Llama 2 and incorporates techniques like embedding sharing, deep-and-thin structure and grouped-query attention for optimized training and inference. - The model exhibits good calibration properties with predicted confidence closely aligned with actual correctness probabilities. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation', 'Fill-Mask', 'Sentence Similarity', 'Feature Extraction', 'Summarization', 'Translation'] | [Link](https://github.com/XiaoduoAILab/Xmodel-2) | N/A |


## Papers for 2025-01-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [On the Compositional Generalization of Multimodal LLMs for Medical Imaging](https://arxiv.org/abs/2412.20070) | Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai | - This paper introduces Med-MAT, a visual question answering (VQA) dataset designed to investigate compositional generalization (CG) in multimodal large language models (MLLMs) applied to medical imaging. - Med-MAT comprises 106 medical datasets categorized by modality, anatomical area, and task (MAT-Triplet), forming 53 subsets with 11 modalities, 14 anatomical areas, and 13 medical tasks. - Experiments demonstrate that MLLMs can leverage CG to understand unseen medical images, which is a key driver of the generalization observed in multi-task training. - Further analysis shows that CG supports data-efficient training with limited data and demonstrates consistent performance across different MLLM backbones. - The authors also explore CG between detection and classification tasks, finding that MLLMs can combine knowledge from both to improve classification accuracy. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Image Classification'] | [Link](https://github.com/FreedomIntelligence/Med-MAT) | N/A |
| [Efficiently Serving LLM Reasoning Programs with Certaindex](https://arxiv.org/abs/2412.20993) | Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu | - Dynasor optimizes inference-time compute for LLM reasoning queries by tracking and scheduling requests within reasoning queries and using certaindex, a proxy that measures statistical reasoning progress. - Certaindex guides compute allocation dynamically, allocating more compute to hard queries, reducing compute for simpler ones, and terminating unpromising queries early. - Evaluated on diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustains 3.3× higher query rates or 4.7× tighter latency SLOs in online serving. - Dynasor is implemented as a scheduling layer compatible with existing serving engines. - The system utilizes certaindex as a narrow interface between itself and the applications to support various current and future reasoning algorithms. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization](https://arxiv.org/abs/2412.21037) | Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung | - TANGOFLUX, a 515M parameter text-to-audio (TTA) model based on a hybrid multimodal and diffusion transformer architecture, generates up to 30 seconds of 44.1kHz audio in 3.7 seconds on a single A40 GPU using rectified flows. - The model uses CLAP-Ranked Preference Optimization (CRPO), which iteratively generates synthetic audio preference data and employs CLAP as a proxy reward model to improve alignment with textual descriptions. - Evaluation on AudioCaps and a challenging out-of-distribution dataset reveals that TANGOFLUX achieves state-of-the-art performance across various objective metrics, including Frechet Distance, KL divergence, CLAP score, and Inception Score, outperforming models like Tango 2, AudioLDM 2, and Stable Audio Open. - Human evaluation further confirms TANGOFLUX's superior audio quality and strong alignment with textual prompts. - Ablation studies demonstrate the effectiveness of CRPO's online data generation, the use of CLAP as a reward model, and the improvement of LCRPO over LDPO-FM loss for preference optimization. | ['Text-to-Audio', 'Audio'] | [Link](https://github.com/declare-lab/TangoFlux) | [Link](https://huggingface.co/declare-lab/TangoFlux), [Link](https://huggingface.co/spaces/declare-lab/TangoFlux), [Link](https://huggingface.co/datasets/declare-lab/CRPO) |
| [Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs](https://arxiv.org/abs/2412.21187) | Jianhui Pang, Zhiwei He, Tian Liang, Jiahao Xu, Xingyu Chen | - This paper investigates the "overthinking" issue in large language models (LLMs), particularly those like OpenAI's o1, where excessive computational resources are used for simple problems. - The authors introduce novel efficiency metrics from both outcome (accuracy improvement relative to token usage) and process (diversity of reasoning strategies) perspectives to evaluate the rational use of computational resources. - They propose a self-training paradigm using the PRM12K dataset and strategies to mitigate overthinking by streamlining reasoning processes while preserving accuracy. - Experimental results on various mathematical reasoning datasets, including GSM8K, MATH500, GPQA, and AIME, demonstrate that the proposed approach reduces computational overhead without compromising model performance. -  For example, the approach reduces token output by 48.6% while maintaining accuracy on the MATH500 dataset when applied to QwQ-32B-Preview. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Facilitating large language model Russian adaptation with Learned Embedding Propagation](https://arxiv.org/abs/2412.21140) | Daniil Chernyshev, RefalMachine | - This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models (LLMs) to new languages without requiring extensive instruction-tuning data. - LEP works by propagating learned embeddings from a pre-trained, instruction-tuned LLM in one language to a new LLM initialized with a vocabulary tailored for the target language. - The method was evaluated on Russian adaptation of Mistral-7B and LLaMa-3-8B using a new benchmark called Darumeru, specifically designed for evaluating text generation robustness in Russian. - Results show that LEP achieves competitive performance compared to existing models like OpenChat 3.5 and LLaMa-3-8B-Instruct, demonstrating its effectiveness in language adaptation while reducing costs associated with traditional instruction-tuning. - Further improvements were observed by calibrating the adapted models through self-instruct tuning and additional instruction-tuning steps, highlighting the potential of LEP for enhancing LLM performance beyond existing benchmarks. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/RefalMachine/ruadapt), [Link](https://github.com/RefalMachine/llmtf_open) | [Link](https://huggingface.co/RefalMachine) |
| [OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System](https://arxiv.org/abs/2412.20005) | Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo | - OneKE is a dockerized schema-guided knowledge extraction system based on LLMs and a multi-agent design. - It supports various data formats (web, PDF) and domains (science, news) through different agents. - The system includes a configurable knowledge base for schema configuration and error debugging. - Evaluation on benchmark datasets and case studies demonstrate OneKE's efficacy and adaptability. - The system is open-source and supports different LLMs without fine-tuning. | ['Natural Language Processing', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/zjunlp/OneKE) | N/A |
| [Training Software Engineering Agents and Verifiers with SWE-Gym](https://arxiv.org/abs/2412.21139) | Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan | - This paper introduces SWE-Gym, a new training environment for software engineering (SWE) agents designed to address the limitations of current resources, which often lack executable environments and reward signals. - SWE-Gym contains 2,438 real-world Python tasks from GitHub issues, each with a codebase, an executable runtime environment, unit tests, and a natural language task description. - The authors demonstrate that fine-tuning a 32B Qwen-2.5 language model with SWE-Gym can achieve state-of-the-art resolve rates of 32.0% and 26.0% on the SWE-Bench Verified and Lite test sets, respectively. - This involves an improvement of 19% compared to existing methods on these benchmarks. - This is further enhanced by training a verifier on agent trajectories, enabling inference-time scaling through candidate solution selection. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/SWE-Gym/SWE-Gym) | N/A |
| [HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation](https://arxiv.org/abs/2412.21199) | Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu | - This paper introduces "self-invoking code generation," a new task to evaluate LLMs' progressive reasoning and problem-solving skills by requiring them to solve a base problem and then use its solution to address a more complex related problem. - Three new benchmarks, HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, are created by extending existing datasets using a proposed general recipe and involve Deepseek-V2.5 for problem generation and human expert review. - Experiments on over 20 LLMs reveal that while models excel in traditional code generation, their performance declines significantly on self-invoking tasks, with even top models like o1-mini showing a substantial drop. - Instruction-tuned models exhibit only marginal improvements in self-invoking tasks compared to base models. - Analysis of failure modes highlights LLMs' struggle with generating code that can effectively self-invoke and suggests limitations in instruction-based fine-tuning for such complex tasks. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/CodeEval-Pro/CodeEval-Pro) | N/A |


## Papers for 2024-12-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [On the Compositional Generalization of Multimodal LLMs for Medical Imaging](https://arxiv.org/abs/2412.20070) | Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai | - This paper introduces Med-MAT, a Visual Question Answering (VQA) dataset designed to investigate compositional generalization (CG) in Multimodal Large Language Models (MLLMs) applied to medical imaging. - Med-MAT comprises 106 medical datasets spanning 11 modalities, 14 anatomical areas, and 13 medical tasks, categorized by MAT-Triplets (Modality, Anatomical area, Task) to facilitate CG analysis. - Experiments using LLaVA and other MLLMs on Med-MAT confirm that these models can leverage CG to understand unseen medical image combinations, demonstrating improved performance in classification tasks compared to single-task or unrelated multi-task training. - The paper shows that increasing the volume of CG combinations improves model understanding, and that CG assists data-efficient learning even with limited target data. - The study also demonstrates the existence of CG across different MLLM backbones (LLaVA, Qwen2-VL, Llama 3.2) and its applicability in detection tasks, highlighting its broad applicability and robustness in medical image analysis. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Image Classification'] | [Link](https://github.com/FreedomIntelligence/Med-MAT) | N/A |
| [Efficiently Serving LLM Reasoning Programs with Certaindex](https://arxiv.org/abs/2412.20993) | Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu | - Dynasor optimizes inference-time compute for LLM reasoning queries by tracking and scheduling requests within reasoning queries and using certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. - Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost. - Dynasor reduces compute by up to 50% in batch processing and sustains 3.3x higher query rates or 4.7x tighter latency SLOs in online serving on diverse datasets and algorithms. - Certaindex quantifies how certain the LLM is in approaching its final answer during reasoning, which correlates with computational resources required for correct solutions. - Dynasor outperforms other systems by reducing token usage by 9-52% without sacrificing accuracy on offline workloads and achieving higher sustainable request rates and tighter SLO scales. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization](https://arxiv.org/abs/2412.21037) | Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung | - TANGOFLUX, a novel text-to-audio (TTA) model based on a hybrid architecture of Multimodal Diffusion Transformer (MMDIT) and Diffusion Transformer (DiT) blocks, is introduced.  - The model leverages rectified flows for audio generation, enabling faster inference speeds and utilizes CLAP-Ranked Preference Optimization (CRPO) for alignment.  - CRPO iteratively generates synthetic audio preference data by ranking model outputs based on CLAP similarity scores and then optimizes the model using a novel loss function that prevents performance degradation.  - Experimental results show that TANGOFLUX achieves state-of-the-art performance on objective metrics such as Frechet Distance and CLAP score while significantly reducing inference time compared to existing models.  - Furthermore, human evaluations confirm that TANGOFLUX generates audio of higher quality and greater relevance to the input text compared to other leading TTA models. | ['Text-to-Audio', 'Audio'] | [Link](https://github.com/declare-lab/TangoFlux) | [Link](https://huggingface.co/declare-lab/TangoFlux), [Link](https://huggingface.co/datasets/declare-lab/CRPO), [Link](https://huggingface.co/spaces/declare-lab/TangoFlux) |
| [HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation](https://arxiv.org/abs/2412.21199) | Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu | - This paper introduces "self-invoking code generation," a new task designed to evaluate LLMs' progressive reasoning and problem-solving abilities where models must use the solution from a base problem to address a more complex, related problem. - Three new benchmarks are created: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, which are derived from existing benchmarks using a proposed method. - Experimental results on 20 LLMs reveal a significant performance gap between traditional code generation and self-invoking code generation, where models often struggle to utilize their own generated code effectively. - It's shown that instruction-tuned models offer limited improvement over base models in self-invoking tasks, indicating a need for more advanced training methods for this type of problem. - Analysis of failure modes reveals challenges LLMs face with self-invocation, emphasizing areas for future improvement in code generation and reasoning capabilities. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Feature Extraction'] | [Link](github.com/CodeEval-Pro/CodeEval-Pro) | N/A |
| [Facilitating large language model Russian adaptation with Learned Embedding Propagation](https://arxiv.org/abs/2412.21140) | Daniil Chernyshev, RefalMachine | - This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models (LLMs) to new languages requiring less training data and minimal disruption of existing LLM knowledge. - LEP employs an embedding propagation technique, bypassing instruction-tuning and directly integrating new language knowledge.  - A new benchmark, Darumeru, is introduced to evaluate text generation robustness during training for Russian adaptation.  - Applying LEP to adapt LLaMa-3-8B and Mistral-7B for Russian, across four vocabulary adaptation scenarios, demonstrates competitive performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct.  - Further improvements are observed using self-calibration and additional instruction-tuning, exceeding existing models' performance. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/RefalMachine/ruadapt), [Link](https://github.com/RefalMachine/llmtf_open) | [Link](https://huggingface.co/RefalMachine) |
| [Training Software Engineering Agents and Verifiers with SWE-Gym](https://arxiv.org/abs/2412.21139) | Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan | - This paper introduces SWE-Gym, a new training environment for real-world software engineering (SWE) agents. - SWE-Gym consists of 2,438 Python tasks from GitHub, each with an executable runtime environment, unit tests, and a natural language task description. - Using SWE-Gym to train language model-based SWE agents led to up to a 19% improvement in resolve rate on SWE-Bench Verified and Lite datasets.  - By training verifiers on agent trajectories from SWE-Gym and combining them with fine-tuned SWE agents, the resolve rate further increased to 32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, setting a new state-of-the-art for open-weight agents. - The paper analyzes the scaling behavior of both the training process and the inference-time scaling using verifiers, revealing continuous improvements with increased compute budget. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/SWE-Gym/SWE-Gym) | N/A |
| [OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System](https://arxiv.org/abs/2412.20005) | Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo | - OneKE is a dockerized schema-guided knowledge extraction system based on LLMs and a multi-agent design. - The system uses three agents: a Schema Agent, an Extraction Agent, and a Reflection Agent to handle various extraction scenarios. - It supports extraction from various data sources like web pages and PDF documents without fine-tuning and incorporates a configurable knowledge base to improve performance and allow error debugging. - Experimental results on CrossNER and NYT-11-HRL datasets demonstrate the efficacy of OneKE, with the Case Retrieval component of the Extraction Agent showing significant improvement. - Case studies on web news and book knowledge extraction further illustrate the practical applicability of OneKE. | ['Natural Language Processing', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/zjunlp/OneKE) | N/A |


## Papers for 2024-12-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs](https://arxiv.org/abs/2412.18925) | Wanlong Liu, Xidong Wang, Ke Ji, Zhenyang Cai, Junying Chen | - This paper introduces HuatuoGPT-01, a medical Large Language Model (LLM) designed for complex reasoning. - The model is trained in two stages: firstly, it learns complex reasoning by searching for correct reasoning trajectories guided by a medical verifier, and secondly, it refines this skill with reinforcement learning using verifier-based rewards. - It is trained on 40k verifiable medical problems converted from closed-set exam questions, supplemented by a general domain dataset for enhanced generalization. - HuatuoGPT-01 significantly outperforms existing general and medical-specific LLMs on multiple medical benchmarks, including MedQA, MedMCQA, and PubMedQA, as well as more challenging datasets MMLU-Pro and GPQA. - Ablation studies and other analysis demonstrate that the method of complex reasoning and reinforcement learning boosts performance compared to non-CoT or simple CoT approaches. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/FreedomIntelligence/HuatuoGPT-01) | N/A |
| [Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment](https://arxiv.org/abs/2412.19326) | Kunchang Li, Chenting Wang, Yinan He, Zhilin Li, Ziang Yan | - This paper introduces Task Preference Optimization (TPO), a novel method to enhance Multimodal Large Language Models (MLLMs) with fine-grained visual task alignment. - TPO utilizes learnable task tokens and corresponding task heads for region, temporal, and mask-related visual tasks and incorporates visual task data during training via a local-to-global training process to improve both multimodal dialogue and task-specific performance. - Experiments on VideoChat and LLaVA demonstrate a 14.6% average improvement in multimodal performance on benchmarks like MVBench, VideoMME, NExT-GQA, MLVU, and SEED-Bench2. - The model performs competitively with state-of-the-art models on tasks like spatial grounding, moment retrieval, highlight detection, and tracking. - Ablation studies validate the efficacy of different TPO components, co-training benefits, and task data scaling. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/OpenGVLab/TPO) | N/A |
| [Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging](https://arxiv.org/abs/2412.19512) | Shang-Tse Chen, Saurav Sahay, Shachi H Kumar, Hsuan Su, farnhua | - This paper introduces a method to mitigate safety degradation in fine-tuned Large Language Models (LLMs) without requiring additional safety data. - The method involves merging the weights of a pre-trained safety-aligned base model and its fine-tuned version after training on a downstream task. - Experimental results across various models (Llama-3, Gemma-2B-it), downstream tasks (reasoning, medical assistance, code generation, tool usage), and merging methods (linear merging, model stock, SLERP, DARE) show improvement in safety and downstream task performance. - The proposed approach reduces the Attack Success Rate (ASR) by up to 30% while enhancing downstream task performance, offering a simple yet robust solution. - Linear merging is highlighted as a practical method due to its efficacy and computational efficiency | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/allenai/wildguard) |
| [SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images](https://arxiv.org/abs/2412.17606) | Yoshitaka Ushiku, Tosho Hirasawa, Shohei Tanaka, Kuniaki Saito, Risa Shinoda | - This paper introduces SBS Figures (Stage-by-Stage Synthetic Figures), a new dataset for pre-training figure question answering (QA) models. - The dataset is generated through a novel three-stage pipeline involving visualization target data generation, figure rendering via Python code, and QA pair generation, leveraging LLMs at each stage and producing 1 million synthetic chart figures with associated data and QA pairs. - Models pre-trained on SBS Figures demonstrate a strong performance boost on real-world chart QA datasets like ChartQA, outperforming models trained from scratch or other synthetic datasets. - This method allows efficient training of QA models even with a limited amount of real-world data by first pre-training on the large-scale SBS Figures dataset. - An ablation study investigating various factors in the pipeline reveals the importance of diverse figure appearance, high-quality LLM-generated QA pairs, and the scale of the dataset for optimal pre-training effectiveness. | ['Visual Question Answering', 'Document Question Answering', 'Multimodal'] | [Link](https://github.com/omron-sinicx/SBSFigures) | N/A |


## Papers for 2024-12-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/abs/2412.17743) | Jie Chen, Jiapeng Wang, Jia Deng, Huatong Song, Yiwen Hu | - This paper introduces YuLan-Mini, a 2.42B parameter open-source language model trained on 1.08T tokens. - The model uses a decoder-only transformer architecture with grouped-query attention, SwiGLU activation, rotary positional embeddings, and embedding tying. - Key innovations for pre-training include a robust optimization method to improve training stability, a focused data pipeline with data cleaning and scheduling, and an annealing approach for data selection and long context training. - The model is evaluated on benchmarks for math, code generation, reasoning, and language understanding, showing comparable performance to larger industry models trained on significantly more data. - YuLan-Mini achieves state-of-the-art results for similar-sized models and demonstrates significant improvements in training efficiency. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/RUC-GSAI/YuLan-Mini) | N/A |
| [A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/abs/2412.17483) | Xinting Huang, Shuaiyi Li, Kelong Mao, Zhisong Zhang, ChenlongDeng | - This paper investigates gist token-based context compression methods for improving long-context processing in large language models (LLMs). - The study evaluates different gist-based model architectures categorized by memory location (recurrent or key-value cache) and gist granularity (coarse or fine-grained). - Experiments across language modeling, reasoning, and long-context tasks show that fine-grained key-value cache models achieve near-lossless performance on some tasks, while struggling with others like reranking. - Three failure patterns are identified: "lost by the boundary," where generation quality degrades near segment boundaries; "lost if surprise," where unexpected details are lost; and "lost along the way," where models struggle to recover exact rehearsals.  - Two mitigation strategies, fine-grained autoencoding and segment-wise token importance estimation, show improvements, especially in addressing boundary effects and precise recall. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | N/A | N/A |


## Papers for 2024-12-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Token-Budget-Aware LLM Reasoning](https://arxiv.org/abs/2412.18547) | Zhenyu Chen, Shiqing Ma, Shiyu Zhao, Chunrong Fang, Tingxu Han | - This paper introduces TALE (Token-Budget-Aware LLM rEasoning), a framework designed to optimize the reasoning process in Large Language Models (LLMs) by dynamically managing token budgets. - TALE estimates a token budget for each problem based on its complexity, then incorporates this budget into the prompt to guide the LLM's reasoning. - This method addresses the issue of token redundancy in current LLMs, which often produce unnecessarily lengthy reasoning processes, leading to increased costs and computational overhead. - Experimental results demonstrate that TALE significantly reduces token usage by an average of 68.64% while maintaining competitive accuracy (less than 5% decrease). - This suggests that TALE offers a practical approach to balancing efficiency and accuracy in LLM reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/GeniusHTX/TALE) | N/A |


## Papers for 2024-12-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding](https://arxiv.org/abs/2412.18450) | Dmitry Yudin, wingrune | - 3DGraphLLM introduces a novel approach for 3D scene understanding by combining semantic graphs and large language models (LLMs). - The model architecture includes pre-trained encoders for 3D point clouds and semantic relationships, and a pre-trained LLM, trained via a two-stage approach using ground truth and predicted instance segmentation of point clouds. - 3DGraphLLM represents a 3D scene as a flattened sequence of learnable embeddings of object subgraphs, including object identifiers and relationships with k-nearest neighbors, which is then fed to an LLM. - Experimental results on ScanRefer, Multi3DRefer, and Scan2Cap datasets demonstrate state-of-the-art performance for the proposed method, improving F1@0.5 scores by +5.8% and +4.4% and CIDEr@0.5 by +5.8% respectively on the mentioned datasets. - Ablation studies confirm the benefit of incorporating semantic graph representation for several 3D vision-language tasks. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Object Detection', 'Image-to-Text', 'Question Answering', 'Graph Machine Learning'] | [Link](https://github.com/CognitiveAISystems/3DGraphLLM) | N/A |
| [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/abs/2412.17739) | Ning Ding, Kaiyan Zhang, Xingtai Lv, Che Jiang, Ermo Hua | - This paper proposes Fourier Position Embedding (FoPE) to improve the length generalization of Language Models (LMs).  - FoPE enhances the periodic extension of attention by addressing the spectral damage caused by linear layers, activation functions, and under-trained frequency components.  - Unlike Rotary Position Embedding (ROPE), which treats each dimension as a single-frequency function, FoPE models each dimension as a Fourier series of different frequency components. - FoPE also clips inadequately trained frequency components, replacing them with zero-frequency components to preserve long-wavelength information.  - Experiments across various model scales demonstrate FoPE’s superior length generalization compared to ROPE and ALiBi, maintaining stable perplexity and consistent accuracy in a needle-in-haystack task within varying context windows. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/TsinghuaC3I/Fourier-Position-Embedding) | N/A |
| [In Case You Missed It: ARC 'Challenge' Is Not That Challenging](https://arxiv.org/abs/2412.17758) | Borchmann | - This paper challenges the perceived difficulty of the ARC Challenge dataset, arguing that it appears harder for Large Language Models (LLMs) primarily due to an evaluation setup that hinders direct comparison of answer choices rather than inherent complexity. - The authors highlight a shift in evaluation practices where some researchers have adopted a fairer comparison scheme, allowing models to see all answer options simultaneously, which dramatically improves performance. - This fairer approach reduces performance gaps in other benchmarks, such as SocialIQa (SIQA), and even leads to superhuman results in OpenBookQA, suggesting that evaluation methods significantly shape perceived difficulty. - They demonstrate that switching from evaluating answers in isolation to evaluating them alongside other options leads to substantial performance gains, up to 35% improvement on ARC Challenge for some LLMs. - The paper proposes guidelines to ensure multiple-choice evaluations accurately reflect model capabilities by recommending the use of an evaluation setup where LLMs can compare answer options directly. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](https://arxiv.org/abs/2412.14711) | Jun Zhu, Jianfei Chen, Ziteng Wang | - ReMoE, a fully differentiable Mixture-of-Experts (MoE) architecture based on ReLU routing, is proposed as a drop-in replacement for standard TopK routing, offering continuous training and dynamic expert allocation. - ReLU routing manages experts' on/off states independently and is combined with an adaptive load balancing L1 regularization to control sparsity. - ReMoE outperforms traditional TopK MoE and other routing methods across various model sizes, expert counts, and granularity levels on the LLaMA architecture trained on The Pile dataset. - ReMoE demonstrates improved scalability and performance gains compared to TopK MoE with increasing expert counts, suggesting its effectiveness with larger expert pools. - ReMoE exhibits dynamic expert allocation based on token frequency and stronger domain specialization, leading to efficient resource utilization and improved model expressivity. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/thu-ml/ReMoE) | N/A |
| [SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval](https://arxiv.org/abs/2412.15443) | Divya Chaudhary, Vinija Jain, Aman Chadha, Vinesh Kumar Gande, Aakash Mahalingam | - This paper introduces SKETCH, a novel methodology that enhances the Retrieval Augmented Generation (RAG) retrieval process by integrating semantic text retrieval with knowledge graphs. - SKETCH merges structured and unstructured data for a more holistic comprehension, aiming to improve retrieval performance and maintain context integrity. - Evaluated across four diverse datasets (QUALITY, QASPER, NarrativeQA, and Italian Cuisine), SKETCH consistently outperformed baseline approaches on key RAGAS metrics, including answer_relevancy, faithfulness, context_precision, and context_recall. - Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. - These results highlight SKETCH's capability to deliver more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems by addressing challenges in handling large-scale discourse structures and complex queries across various domains. | ['Question Answering'] | N/A | N/A |


## Papers for 2024-12-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners](https://arxiv.org/abs/2412.17256) | Zifei Shan, Yijun Wang, Lulu Zhao, Yuzhen Huang, Weihao Zeng | - B-STAR, a novel self-improving framework, balances exploration and exploitation by dynamically adjusting hyperparameters like temperature and reward thresholds to optimize a proposed balance score metric throughout training iterations. - This approach enhances the model's ability to generate both diverse and high-quality responses, addressing the limitations of current self-improving methods that often stagnate after a few iterations. - The effectiveness of B-STAR is validated across mathematical problem-solving (GSM8K and MATH datasets), coding challenges (APPS dataset), and commonsense reasoning (ARC-Challenge dataset). - Experimental results show significant performance improvement over various self-improving baselines (STaR/ReST-EM, Iterative RFT, Online RFT). - For example, B-STAR shows improved Pass@1 accuracy and sustained improvement over multiple training iterations without degradation, unlike other methods, demonstrating effective management of the exploration-exploitation trade-off. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/hkust-nlp/B-STaR) | N/A |
| [RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response](https://arxiv.org/abs/2412.14922) | Zhiping Xiao, Jingyang Yuan, Xiao Luo, Junyu Luo, kaize0409 | - ROBUSTFT is a robust supervised fine-tuning framework designed to enhance the performance of Large Language Models (LLMs) in the presence of noisy response data, which is a common issue in real-world applications. - It employs a two-stage process: noise detection and denoising.  Noise detection leverages a multi-expert system with reasoning-enhanced LLMs and a consistency checker, while denoising uses context-enhanced relabeling with a review agent and entropy-based data selection. - The method was evaluated on three LLMs (Gemma2-9B, Llama3.1-8B, Llama3.2-3B) and five datasets (MMLU, ARC, PubMedQA, Drop, FPB) under varying noise levels (30%, 50%, and 70%). - Experimental results show consistent performance improvements across various noise conditions and datasets compared to vanilla LLMs and baseline denoising methods. - The framework also proves particularly beneficial for smaller models and maintains stable performance even with rephrased instructions, validating its robustness and generalizability. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/luo-junyu/RobustFT) | [Link](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B), [Link](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT) |
| [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451) | Yu Cheng, Fan Zhou, Xiwen Zhang, Junlong Li, Wei Liu | - This paper introduces M-STAR, a novel self-evolving training framework for enhancing multimodal reasoning abilities of Large Multimodal Models (LMMs) without relying on human-annotated chain-of-thought data. - M-STAR systematically analyzes and optimizes three key components of self-evolving training: training methods, reward models, and prompt variations. - It presents a continuous self-evolving training scheme, trains the first process-based reward model for multimodal reasoning, and demonstrates that adding unlabeled data is only effective with reliable reward signals. - Additionally, M-STAR incorporates dynamic temperature adjustment to balance exploration and exploitation during training to counter exploration loss. - Experiments on five multimodal reasoning benchmarks show that M-STAR significantly improves the performance of models with varying sizes, such as MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B), and InternVL2 (2B), consistently surpassing pre-trained models across various subtasks. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/hkust-nlp/mstar) | N/A |
| [Deliberation in Latent Space via Differentiable Cache Augmentation](https://arxiv.org/abs/2412.17747) | Arthur Szlam, Jun Xie, Jiaxing Wu, Jonas Pfeiffer, Luyang Liu | - This paper introduces a novel method called "differentiable cache augmentation" to enhance frozen decoder-only Large Language Models (LLMs) by adding a coprocessor that operates on the model's key-value cache. - The coprocessor, trained using the language modeling loss, augments the cache with latent embeddings, improving the fidelity of subsequent decoding without modifying the original LLM architecture. - Experimental results show that this approach consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks, such as GSM8K and MMLU, even in zero/few-shot settings. - The approach allows asynchronous and offline coprocessor operation, opening possibilities for more deliberate and computationally intensive reasoning processes in future research. - The method outperforms the baseline model and a related method called "Pause Token" on tasks like GSM8K, showcasing the effectiveness of the learned context-dependent dynamic embeddings. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/abs/2412.16926) | Oh, Geunseob, Prakhar Gupta, Sun Jae Lee, Jinheon Baek | - This paper revisits In-Context Learning (ICL) with Long Context Language Models (LCLMs) and challenges the prevailing assumption that sophisticated example selection strategies are crucial for optimal performance. - Through experiments on 18 datasets across 4 tasks, the study finds that simple random sampling is as effective as more complex selection methods in many-shot ICL scenarios. - The paper identifies a new challenge with LCLMs: underutilization of expanded context capacity, especially in low-resource tasks.  - It proposes a data augmentation technique to address this which involves generating synthetic examples and filtering low-quality ones, leading to performance improvements of up to 5%. - The study also finds that while LCLMs benefit from larger contexts, performance plateaus and may decline when the context becomes extremely long, especially with noisy examples present, suggesting future research directions for improving robustness in LCLMs. | ['Natural Language Processing', 'Question Answering', 'Translation', 'Summarization'] | N/A | N/A |
| [Outcome-Refining Process Supervision for Code Generation](https://arxiv.org/abs/2412.15118) | Jindong Wang, Zhengran Zeng, Yidong Wang, Weizheng Gu, Zhuohao Yu | - This paper introduces Outcome-Refining Process Supervision (ORPS), a novel paradigm for enhancing code generation by treating outcome refinement as the process to be supervised. - It leverages a tree-structured exploration space, enabling models to maintain multiple reasoning trajectories, guided by execution feedback as objective anchors for evaluation.  - The framework combines beam search with a self-critique mechanism, where the model analyzes reasoning chains and execution outcomes before generating rewards, eliminating the need for trained Process Reward Models (PRMs). - ORPS demonstrates significant improvements across various benchmarks, achieving a 26.9% average increase in Pass@1 and a 42.2% reduction in execution time compared to existing methods.  - These results highlight the effectiveness of coupling structured reasoning space with concrete feedback signals for solving complex coding tasks, offering a scalable and efficient solution. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/zhuohaoyu/ORPS) | N/A |
| [DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/abs/2412.17498) | Jie Zhou, Yunlong Liang, Fandong Meng, Jiaan Wang | - This paper introduces DRT-01, a new model that integrates long chain-of-thought (CoT) into neural machine translation (MT), specifically targeting sentences with similes or metaphors from literature books.  - A multi-agent framework with a translator, advisor, and evaluator is used to iteratively refine translations, generating long-thought MT data. GPT-40 is then employed to enhance the readability of the generated data. - DRT-01 is trained on this data using Qwen2.5-7B/14B as backbones.  - Experimental results show improvements of 7.33~8.26 BLEU and 1.66~3.36 CometScore over the baselines. - Notably, DRT-01-7B surpasses QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showcasing its effectiveness. | ['Translation', 'Natural Language Processing'] | [Link](https://github.com/krystalan/DRT-o1) | [Link](https://huggingface.co/Unbabel/wmt22-cometkiwi-da), [Link](https://huggingface.co/Unbabel/wmt22-comet-da), [Link](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct) |
| [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470) | Junxiao Yang, Jingzhuo Zhou, Yida Lu, Shiyao Cui, Zhexin Zhang | - This paper introduces AGENT-SAFETYBENCH, a comprehensive benchmark designed to evaluate the safety of LLM agents. - The benchmark encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes. - An evaluation of 16 popular LLM agents reveals that none achieve a safety score above 60%. - Analysis identifies two key safety defects: a lack of robustness and a lack of risk awareness. - The study finds that relying solely on defense prompts is insufficient for addressing these safety issues, suggesting the need for more advanced strategies. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/Agent-SafetyBench) | N/A |
| [NILE: Internal Consistency Alignment in Large Language Models](https://arxiv.org/abs/2412.16686) | Hongru Wang, Bowei He, Yufei Wang, Qiyuan Zhang, Minda Hu | - The paper introduces NILE (iNternal consIstency aLignmEnt), a framework designed to improve the quality of Instruction Fine-Tuning (IFT) datasets for Large Language Models (LLMs) by aligning the datasets with the LLMs' internal knowledge. - NILE works by eliciting the internal knowledge of a pre-trained LLM, revising existing dataset answers using this internal knowledge, and filtering out inconsistent samples using a novel Internal Consistency Filtering (ICF) method. - Experiments show that NILE-aligned IFT datasets boost LLM performance across multiple benchmarks, including up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. - Ablation studies validate each component of the framework—Internal Knowledge Extraction, Knowledge-Aware Sample Revision, and Internal Consistency Filtering—confirming their contribution to the improved performance. - The results demonstrate the significance of dataset consistency with pre-trained internal knowledge for maximizing LLM potential. | ['Natural Language Processing'] | N/A | N/A |
| [LearnLM: Improving Gemini for Learning](https://arxiv.org/abs/2412.16429) | Andrea Huber, Aliya Rysbek, Aditya Srikanth Veerubhotla, Abhinit Modi, LearnLM Team | - This paper introduces LearnLM, a new large language model (LLM) based on Gemini 1.5 Pro and fine-tuned specifically for educational applications. - LearnLM is trained using a method called pedagogical instruction following, which uses system-level instructions to guide desired pedagogical behaviours, rather than defining specific pedagogical behaviours.  - It incorporates Reinforcement Learning from Human Feedback (RLHF) for enhanced adherence to nuanced instructions and user preferences.   - Human evaluation results show a significant preference for LearnLM over GPT-40, Claude 3.5, and Gemini 1.5 Pro across various learning scenarios.  - LearnLM is available as an experimental model on Google AI Studio. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [OpenAI o1 System Card](https://arxiv.org/abs/2412.16720) | Adam Richardson, Adam Lerer, Adam Kalai, Aaron Jaech, OpenAI | - OpenAI introduces the "o1" large language model family, trained with reinforcement learning for complex reasoning using chain-of-thought, enhancing safety and robustness. - The models demonstrate state-of-the-art performance in benchmarks related to generating illicit advice, stereotyped responses, and known jailbreaks due to deliberative alignment. - Trained on diverse public, proprietary, and custom datasets, o1 shows enhanced performance in jailbreak evaluations and adherence to safety guidelines compared to GPT-40. - The o1 models also demonstrate significant improvements in mitigating hallucinations, especially in factual question answering, and improved performance in tasks assessing demographic fairness. - Despite advancements, potential safety risks stemming from increased intelligence are acknowledged, highlighting the need for continuous improvement in alignment and safety methods and extensive stress-testing. | ['Multimodal', 'Image-to-Text', 'Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Summarization'] | N/A | N/A |
| [OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning](https://arxiv.org/abs/2412.16849) | Jinlin Xiao, Yuhang Wang, Jiangming Shu, Yuqi Yang, Yuxiang Zhang | - OpenRFT adapts a generalist reasoning model for domain-specific tasks using reinforcement fine-tuning (RFT), addressing challenges like limited training data and lack of reasoning step data. - It leverages domain-specific samples through question augmentation, synthesizing reasoning process data using a teacher model, and few-shot in-context learning (ICL) to enhance RL exploration. - Evaluated on SciKnowEval, OpenRFT demonstrates significant performance improvements with limited samples, averaging 11% accuracy increase compared to a baseline model. - The study highlights that data augmentation, stronger reasoning foundation models, and aligned action space contribute to better RFT performance. -  OpenRFT's effectiveness depends on the availability of high-quality generalist reasoning models and corresponding Process Reward Models (PRMs). | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ADaM-BJTU/OpenRFT) | [Link](https://huggingface.co/Skywork) |
| [Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding](https://arxiv.org/abs/2412.17295) | Qun Liu, Jianxin Liang, Xiaojun Meng, Yueqian Wang, ColorfulAI | - This paper introduces Friends-MMC, a multimodal multi-party conversation (MMC) dataset derived from the TV series *Friends*.  - The dataset includes over 24,000 utterances paired with video contexts, speaker annotations, and bounding boxes of faces, facilitating research on character-centered understanding in conversations. - The authors propose a baseline method for conversation speaker identification leveraging visual and textual models, combined with a quadratic binary optimization solver, demonstrating its effectiveness compared to existing pre-trained models. - For conversation response prediction, they fine-tune generative dialogue models on Friends-MMC and show that incorporating speaker information improves performance.  -  They argue for increased attention to modeling speaker information in conversational understanding research. | ['Multimodal', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/yellow-binary-tree/Friends-MMC) | N/A |


## Papers for 2024-12-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation](https://arxiv.org/abs/2412.13649) | Yilong Lai, Zhenglin Wang, zhoudeyu, lzhang472, callanwu | - SCOPE is a novel framework designed to optimize Key-Value (KV) cache compression for long-context generation in Large Language Models (LLMs), addressing the often-overlooked decoding phase. - It decouples KV cache optimization for prefill and decoding phases, preserving essential information from the prefill while dynamically allocating heavy hitters during decoding using a sliding window approach. - Further memory optimization is achieved through adaptive and discontinuous strategies, reducing memory usage and transfer overhead. - Experimental results on LONGGENBENCH show that SCOPE maintains comparable performance to full KV cache with a 35% overall compression rate, outperforming existing unified compression methods. - SCOPE also demonstrates its generalizability and effectiveness as a plug-in to other prefill-only KV cache compression methods. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Linking-ai/SCOPE) | N/A |
| [Offline Reinforcement Learning for LLM Multi-Step Reasoning](https://arxiv.org/abs/2412.16145) | yiwu, ZhangShenao, hendrydong, Shibo-UCSD, jwhj | - This paper introduces OREO (Offline REasoning Optimization), an offline reinforcement learning method designed to improve the multi-step reasoning abilities of Large Language Models (LLMs). - OREO jointly learns a policy model and value function by optimizing the soft Bellman Equation, enabling it to leverage unpaired data with sparse rewards and perform better credit assignment compared to methods like Direct Preference Optimization (DPO). - The approach is evaluated on mathematical reasoning (GSM8K, MATH) and embodied agent control (ALFWorld) tasks, demonstrating consistent improvements over baseline methods including rejection sampling, DPO, and KTO across different model sizes. - Notably, a 1.5B model achieves 52.5% accuracy on MATH using only the original training set, and iterative OREO shows continued improvement with additional training rounds. - The learned value function can also guide tree search during inference, leading to further performance gains (up to 17.9% relative improvement over greedy decoding on MATH). | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/jwhj/OREO) | N/A |
| [Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/abs/2412.15322) | Akio Hayakawa, mittu1204, TakashiShibuyaSony, mi141, hkchengrex | - MMAudio, a novel multimodal joint training framework for synthesizing high-quality, synchronized audio from video and optional text conditions, is introduced.  - This model uses a transformer-based architecture with visual, text, and audio branches, jointly trained on audio-visual and text-audio data, and incorporates a conditional synchronization module for precise temporal alignment.  - MMAudio achieves state-of-the-art performance in video-to-audio generation on public benchmarks, outperforming existing methods in audio quality, semantic alignment, and audio-visual synchronization, while maintaining a low inference time.  - Notably, it also demonstrates competitive performance in text-to-audio generation without fine-tuning.  - The joint training strategy enables accessible data scaling and cross-modal understanding, which are key to the model's success. | ['Multimodal', 'Text-to-Audio', 'Video-Text-to-Text', 'Audio'] | [Link](hkchengrex.github.io/MMAudio) | N/A |
| [MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design](https://arxiv.org/abs/2412.14590) | chuanjieliu, xiaonans, JamesTheZ | - MixLLM is a novel large language model (LLM) quantization technique that employs a global mixed-precision approach between output features, assigning higher bit-widths to features with greater impact on model accuracy, resulting in reduced memory consumption without compromising performance. - It identifies high-salience output channels by estimating their contribution to the final loss globally across all model layers, unlike previous methods that focus on per-layer salience. - MixLLM uses 8-bit symmetric activation quantization and 4-bit asymmetric weight quantization in a group-wise manner to maintain accuracy and employs a two-step dequantization process leveraging int8 Tensor Cores and fast integer-to-float conversion for optimized system efficiency. - Evaluation on popular LLMs like Llama 3.1 and Qwen2.5 across various benchmarks, including perplexity and downstream tasks like MMLU-Pro and BBH, demonstrates that MixLLM with only 4.4 bits for weights outperforms existing 4-bit methods and achieves results comparable to 5-bit quantization, while also exceeding the system performance of float16 and state-of-the-art 4-bit solutions. - Additionally, MixLLM with 8-bit weight quantization shows negligible accuracy loss compared to the float16 baseline, underscoring the efficacy of its group-wise activation quantization and system design. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps](https://arxiv.org/abs/2412.15035) | navigli, mbrack, PSaiml, sted97, felfri | - This paper introduces M-ALERT, a multilingual benchmark for evaluating the safety of Large Language Models (LLMs) across five languages (English, French, German, Italian, and Spanish). - M-ALERT comprises 75k prompts (15k per language), translated and adapted from the ALERT benchmark, covering a wide range of safety categories. - Experiments on 10 state-of-the-art LLMs reveal inconsistencies in safety performance across languages and categories, with some models exhibiting language-specific vulnerabilities while others show consistently unsafe behavior in certain high-risk categories. - The study finds a less pronounced correlation between model safety and size compared to the impact of instruction tuning. - M-ALERT also facilitates category and policy-specific evaluations, highlighting its practical use for policy compliance assessment in LLMs. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/felfri/M-ALERT) |
| [Fietje: An open, efficient LLM for Dutch](https://arxiv.org/abs/2412.15450) | BramVanroy | - This paper introduces Fietje, a family of 2.7B parameter decoder-only transformer language models for Dutch based on Phi-2. - Fietje was trained on 28B Dutch tokens from Wikipedia and the CulturaX dataset and comes in three versions: base, instruct, and chat. - At the time of its release, Fietje achieved competitive results with larger language models, sometimes even outperforming 7B models on ARC and MMLU benchmarks. - Evaluations on various Dutch NLP benchmarks demonstrated its efficacy compared to similar-sized models and established it as a significant step toward accessible language technology for Dutch. - The benchmark results also highlight the rapid advancement of the field and show that smaller multilingual models that were released after Fietje generally perform better. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/BramVanroy/fietje-2), [Link](https://github.com/BramVanroy/clin34-benchmarks) | [Link](https://huggingface.co/collections/BramVanroy/fietje-2-662cb803ed5cc4f617404146) |


## Papers for 2024-12-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115) | Losin94, bowenYu, bzheng, huybery, Baosong | - Qwen2.5 is a series of large language models (LLMs) trained on 18 trillion tokens of data, improving upon its predecessor Qwen2 through enhanced pre-training and post-training techniques. - The models range from 0.5B to 72B parameters in open-weight offerings and include Mixture-of-Experts (MoE) models, Qwen2.5-Turbo and Qwen2.5-Plus, for hosted solutions. - Qwen2.5-72B-Instruct demonstrates competitive performance against Llama-3-405B-Instruct, a model five times its size. - Qwen2.5-Turbo and Qwen2.5-Plus exhibit superior cost-effectiveness while competing with GPT40-mini and GPT40 respectively. - Qwen2.5 also serves as a foundation for specialized models like Qwen2.5-Math and Qwen2.5-Coder, broadening its applicability to specific domains. | ['Natural Language Processing', 'Text Generation', 'Multimodal'] | [Link](https://github.com/QwenLM/Qwen2.5) | [Link](https://huggingface.co/Qwen) |
| [MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval](https://arxiv.org/abs/2412.14475) | BoZhaoHuggingFace, yzwang, Shitao, zl101, JUNJIE99 | - MegaPairs, a novel data synthesis method that uses vision-language models (VLMs) and open-domain images with a massive synthetic dataset for universal multimodal retrieval is proposed. - This method constructs heterogeneous KNN triplets using three similarity models (CLIP vision encoder, DINO vision encoder, and CLIP text encoder) to sample correlated image pairs. - It then utilizes MLLM and LLM annotators for relationship description and pseudo retrieval instruction generation resulting in triplets (Image query, Text instruction, Image target).  - MMRet models trained on MegaPairs demonstrate SOTA zero-shot results on 4 Composed Image Retrieval benchmarks and MMEB's 36 datasets, outperforming baselines by 8.1% on CIRCO using 70x less data.  - Further downstream fine-tuning shows that the model maintains leading performance on the benchmarks mentioned above. | ['Multimodal', 'Image-to-Text', 'Computer Vision'] | N/A | N/A |
| [Progressive Multimodal Reasoning via Active Retrieval](https://arxiv.org/abs/2412.14835) | douzc, yutaozhu94, dengmengjie, Snow-Nation, dongguanting | - This paper introduces AR-MCTS, a framework designed to improve multi-step multimodal reasoning in Multimodal Large Language Models (MLLMs) by combining Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). - AR-MCTS employs a unified retrieval module to gather key insights from a hybrid-modal corpus, aiding in problem-solving. - It utilizes MCTS with active retrieval to automatically generate step-wise annotations, enhancing the diversity and reliability of the reasoning process. - A process reward model (PRM) is progressively aligned through step-wise Direct Preference Optimization (DPO) and Supervised Fine-tuning (SFT) for automated verification. - Experimental results across various MLLMs and benchmarks show AR-MCTS's effectiveness in boosting performance, optimizing sampling diversity and accuracy, and demonstrating improvement in complex reasoning scenarios, particularly on WE-MATH's S3 metrics and general reasoning tasks like GAOKAO-MM. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks](https://arxiv.org/abs/2412.15204) | wangxz098, haopeng01, NeoZ123, tsq2000, bys0318 | - Introduces LongBench v2, a challenging benchmark designed to evaluate the deep understanding and reasoning capabilities of Large Language Models (LLMs) in long-context scenarios across diverse real-world tasks. - The benchmark consists of 503 multiple-choice questions spanning six major task categories, with contexts ranging from 8k to 2M words, focusing on complex reasoning rather than simple information retrieval. - Data collection involves nearly 100 highly educated individuals and employs rigorous automated and manual review processes, resulting in a high-quality dataset where even human experts achieve only 53.7% accuracy under time constraints. - Evaluation shows that the best-performing model achieves 57.7% accuracy, surpassing the human baseline by 4% when leveraging chain-of-thought prompting during inference. - The results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle long-context challenges and call for further exploration in this direction. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/THUDM/LongBench) | N/A |
| [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/abs/2412.14689) | XingtaiHF, iseesaw, Hengli, daixuancheng, xuekai | - This paper proposes ToEdit (Token Editing), a novel technique for synthesizing text data that mitigates model collapse, a degenerative process where language models overfit to synthetic data distributions. - ToEdit employs token-level editing on human-produced data, guided by a pre-trained language model's probability distribution, to create semi-synthetic data. - This method theoretically constrains the test error within a fixed upper bound, preventing the error accumulation observed in iterative training on synthetic data. - Experimental results across pre-training, continual pre-training, and supervised fine-tuning demonstrate that ToEdit enhances model performance compared to using purely synthetic or mixed synthetic and human-produced data. - Statistical analyses reveal that synthetic data suffers from coverage collapse and over-concentration of n-gram features, issues addressed by ToEdit's approach. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Flowing from Words to Pixels: A Framework for Cross-Modality Evolution](https://arxiv.org/abs/2412.15213) | Andrew Brown, Alan Yuille, Xi Yin, mannatsingh, QHL067 | - CrossFlow, a novel framework for cross-modal flow matching, leverages variational encoders and a novel classifier-free guidance technique to directly map one modality's distribution to another's. - For text-to-image generation, CrossFlow uses a vanilla transformer without cross-attention, unlike existing methods that rely on complex architectures and conditioning mechanisms. - Demonstrating improved scaling, CrossFlow slightly outperforms standard flow matching baselines in zero-shot FID-30K and achieves comparable CLIP scores, given the same data, model size, and training budget. - CrossFlow exhibits semantic latent space arithmetic, enabling meaningful output edits through latent manipulation. - Its generalizability is showcased by comparable or superior performance in image captioning, depth estimation, and image super-resolution compared to state-of-the-art techniques. | ['Text-to-Image', 'Image-to-Text', 'Depth Estimation', 'Image-to-Image', 'Multimodal'] | [Link](https://cross-flow.github.io/) | N/A |
| [AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling](https://arxiv.org/abs/2412.15084) | wping, ctnzr, shoeybi, ychenNLP, zihanliu | - AceMath, a suite of large language models (LLMs) designed for complex math problem-solving and featuring specialized reward models for solution evaluation, is introduced. - The instruction-tuned math models are developed through a two-stage supervised fine-tuning (SFT) process, starting with general domain SFT and followed by targeted math domain fine-tuning using curated prompts and synthetically generated responses. - AceMath-72B-Instruct outperforms existing open-weight and proprietary LLMs, including Qwen2.5-Math-72B-Instruct, GPT-40, and Claude-3.5 Sonnet, on a variety of math reasoning benchmarks. - A new comprehensive benchmark, AceMath-RewardBench, is introduced for evaluating math reward models; the associated AceMath-72B-RM reward model achieves state-of-the-art performance. - Combining AceMath-72B-Instruct with AceMath-72B-RM yields the highest average rm@8 score across math reasoning benchmarks. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | N/A | N/A |
| [Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception](https://arxiv.org/abs/2412.14233) | Ke Zhu, Jing Hao, FuNz, cloud913, syp115 | - This paper introduces DCE (Descriptive Caption Enhancement), a novel image captioning engine that leverages off-the-shelf visual specialist models to extract detailed object attributes and relationships from images. - These attributes, combined with LLM-generated region captions and relational information, produce richer and more comprehensive descriptions than existing methods relying solely on LLMs or human annotation. - Experimental results demonstrate that DCE-generated captions significantly improve the performance of Large Multimodal Models (LMMs) across 14 visual question answering and multimodal benchmarks, exceeding human and other LLM-generated captions. - DCE utilizes open-source models for caption generation, reducing the costs and improving the efficiency compared to methods using expensive models like GPT-4V. - The authors plan to release the DCE source code and pipeline to promote further research and enable easy integration of other visual specialists into multimodal models. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/syp2ysy/DCE) | N/A |
| [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/abs/2412.14642) | Qing Li, Yunqing Liu, Jiatong Li, schrodingers-tiger, Duke-de-Artois | - This paper introduces TOMG-Bench, the first benchmark designed to evaluate the ability of Large Language Models (LLMs) to perform open-domain text-based molecule generation. - TOMG-Bench comprises three core tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom), each with three subtasks containing 5,000 samples. - It includes an automated evaluation system assessing generated molecules' quality and accuracy, alongside an instruction-tuning dataset OpenMolIns, extracted from PubChem, to enhance LLM performance. - The benchmarking of 25 LLMs showcases the current limitations and potential in this field; with OpenMolIns, Llama-3.1-8B outperforms open-source general LLMs, even surpassing GPT-3.5-turbo on the TOMG-Bench by 46.5%. - The researchers identified the challenge of open molecule generation for existing LLMs and constructed the corresponding benchmark and the instructional dataset. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/phenixace/TOMG-Bench) | N/A |


## Papers for 2024-12-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161) | Kritanjali Jain, Yuxuan Tang, Boxuan Li, Yufan Song, Frank F. Xu | - This paper introduces TheAgentCompany, a benchmark for evaluating AI agents on real-world tasks simulating a software company environment. - The benchmark includes 175 tasks across various job functions like software engineering, project management, and finance, requiring agents to interact with web interfaces, code, and simulated colleagues via chat and email. - The evaluation includes both autonomous completion and partial credit based on checkpoint achievements, assessing the agents' ability to manage complex workflows. - Experiments with different LLMs (Claude, Gemini, GPT-40, Llama, Qwen) reveal that even the best model (Claude 3.5 Sonnet) achieves only 24% full and 34.4% partial completion, showing limitations in tasks demanding social interactions and handling complex interfaces. - Despite the leading LLM's strong performance, the high cost per task ($6.34) highlights the need for further research and optimization of cost-effectiveness in real-world deployments. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/TheAgentCompany/TheAgentCompany), [Link](https://github.com/TheAgentCompany/experiments) | N/A |
| [FashionComposer: Compositional Fashion Image Generation](https://arxiv.org/abs/2412.14168) | Hao Luo, Xiaogang Xu, Xi Chen, Yiyang Wang, Sihui Ji | - FashionComposer is a novel diffusion-based model for compositional fashion image generation that takes multi-modal inputs such as text descriptions, parametric human models, garment images, and face images. - It employs a universal framework with a reference UNet and subject-binding attention to handle diverse input modalities and compose multiple visual assets in one pass, supporting applications like virtual try-on and album generation. - The model is trained on a scaled dataset constructed using existing datasets augmented with masked garments and generated captions, showing superior performance in multi-reference customization compared to existing methods like Emu2 and Collage Diffusion. - For consistent human image generation in albums, FashionComposer introduces correspondence-aware attention and latent code alignment to maintain both consistency and fidelity. - In virtual try-on tasks, FashionComposer achieves state-of-the-art results, outperforming other methods on standard benchmarks like VITON-HD for single garment and showing promising results on DressCode for multi-garment and outfit try-on scenarios. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities](https://arxiv.org/abs/2412.14123) | Loic Landrieu, Clement Mallet, Nicolas Gonthier, Guillaume Astruc | - AnySat, a novel multimodal and multiresolution model for Earth Observation, is introduced, leveraging a Joint Embedding Predictive Architecture (JEPA) and scale-adaptive spatial encoders. - Trained on GeoPlex, a diverse dataset comprising various modalities, resolutions, and scales, the model demonstrates state-of-the-art performance across several downstream tasks, including land cover mapping, tree species identification, and flood segmentation. - AnySat's versatility allows it to seamlessly handle diverse EO datasets with varying properties and modalities, eliminating the need for dataset-specific retraining. - Evaluation on GeoPlex and external datasets showcases performance improvements, particularly in classification tasks and smaller datasets, due to the enhanced representation learning from diverse data sources. - AnySat's efficiency allows for linear probing for semantic segmentation with competitive results, reducing training costs significantly and opening possibilities for wider application in environmental monitoring. | ['Image Segmentation', 'Image Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/gastruc/AnySat) | N/A |
| [GUI Agents: A Survey](https://arxiv.org/abs/2412.13501) | Namyong Park, Gang Wu, Yu Wang, Jian Chen, dangmn | - This survey paper provides a comprehensive overview of Graphical User Interface (GUI) agents, which leverage Large Foundation Models (LFMs) to automate human-computer interaction. - It categorizes GUI agents based on benchmarks, evaluation metrics, architectures (perception, reasoning, planning, and acting), and training methods, proposing a unified framework for understanding their capabilities. - The paper discusses various datasets and interactive environments used for evaluating GUI agents, distinguishing between closed-world and open-world settings, and static and dynamic environments. - It also covers different architectural designs for perception (accessibility-based, HTML/DOM-based, screen-visual-based, and hybrid), reasoning, planning (with internal and external knowledge), and acting modules. - Finally, the survey summarizes training methods, including prompt-based and training-based approaches (pre-training, fine-tuning, and reinforcement learning), and identifies open challenges and future research directions in GUI agent research, such as intent understanding, security, and latency. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment](https://arxiv.org/abs/2412.13746) | Yubo Chen, Pengfei Cao, Tianyi Men, Hongbang Yuan, Zhuoran Jin | - This paper introduces RAG-RewardBench, the first benchmark designed for evaluating reward models (RMs) within Retrieval Augmented Generation (RAG) settings, aiming to improve preference alignment between RAG models and human preferences. - The benchmark includes 1,485 preference pairs across four RAG-specific scenarios: multi-hop reasoning, fine-grained citation, appropriate abstaining, and conflict robustness, sourced from 18 datasets using six retrievers and 24 RALMs. - An LLM-as-a-judge approach is employed to enhance preference annotation efficiency and achieve a strong correlation (0.84 Pearson correlation) with human annotations. - Evaluation results on 45 existing RMs show the top-ranked model reaches only 78.3% accuracy, highlighting the benchmark's challenging nature and the need for RMs specifically tailored for RAG. - The paper finds that existing trained RALMs demonstrate minimal improvement (0.6%) in preference alignment over base LLMs based on their performance on RAG-RewardBench, suggesting a need to shift training towards preference-aligned approaches. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/jinzhuoran/RAG-RewardBench) | [Link](https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/) |
| [Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN](https://arxiv.org/abs/2412.13795) | Shiwei Liu, Lu Yin, Pengxiang Li | - Mix-LN, a novel normalization technique for Large Language Models (LLMs), combines Pre-LN and Post-LN to address the inefficiency of deeper layers often observed in LLMs trained with Pre-LN. - Mix-LN applies Post-LN to early layers and Pre-LN to deeper layers, promoting more uniform gradients and enabling effective contribution from all layers during training. - Experiments across various model sizes (70M to 7B parameters) show Mix-LN consistently outperforms Pre-LN, Post-LN, and their variants, improving pre-training perplexity and demonstrating better performance in supervised fine-tuning and reinforcement learning from human feedback. - The improved performance is attributed to Mix-LN's ability to promote healthier gradient norms and representation diversity across all layers, leading to more effective learning and generalization. - The study highlights the importance of optimizing normalization techniques in LLMs to fully leverage the potential of deep layers and improve overall model capacity and efficiency. | ['Natural Language Processing'] | [Link](https://github.com/pixeli99/MixLN) | N/A |
| [Learning from Massive Human Videos for Universal Humanoid Pose Control](https://arxiv.org/abs/2412.14172) | Junjie Ye, Tianheng Shi, Siqi Song, Siheng Zhao, Jiageng Mao | - This paper introduces Humanoid-X, a large-scale dataset with over 20 million humanoid robot poses and corresponding text descriptions, designed for universal humanoid pose control. - A new large humanoid model, UH-1, is proposed. UH-1 uses a Transformer architecture to translate text instructions into corresponding actions for controlling humanoid robots. It supports both text-to-keypoint and text-to-action control modes. - UH-1 is trained on Humanoid-X and shows strong generalization in text-based humanoid control, outperforming existing two-stage methods on the HumanoidML3D benchmark by over 23% in FID score. - Extensive simulated and real-world experiments demonstrate that UH-1 can reliably translate textual commands into diverse and accurate humanoid actions, achieving nearly 100% success rate in real-world deployment. - The scalability of Humanoid-X is demonstrated to improve model performance by training UH-1 on various dataset sizes. | ['Robotics', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers](https://arxiv.org/abs/2412.12571) | Yupeng Shi, Zhi-Fan Wu, Wei Wang, Lianghua Huang, bibona | - ChatDiT is a novel zero-shot, general-purpose, interactive visual generation framework built upon pre-trained diffusion transformers (DiTs) without requiring fine-tuning or architectural modifications. - It leverages the inherent in-context generation capabilities of DiTs, allowing users to create complex multi-image outputs, edit images, generate illustrated articles, and design character settings through free-form natural language interaction. - This is achieved using a multi-agent system composed of an Instruction-Parsing Agent, a Strategy-Planning Agent, and an Execution Agent, which collaboratively interpret instructions, formulate generation plans, and execute actions using an in-context toolkit of DiTs. - Evaluation on IDEA-Bench shows that ChatDiT outperforms existing methods, including specialized multi-task frameworks and rephrasing-based models, achieving a top score of 23.19 out of 100. - Despite its strong performance, certain limitations exist, such as difficulty in preserving fine details and identity, especially when handling long contexts with multiple subjects or elements, highlighting areas for future research. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/ali-vilab/ChatDiT) | N/A |
| [AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge](https://arxiv.org/abs/2412.13670) | Shuai Zhao, Ruiwen Zhou, Yuxi Xie, Liangming Pan, Xiaobao Wu | - This paper introduces AntiLeak-Bench, an automated anti-leakage benchmarking framework for Large Language Models (LLMs). - It addresses data contamination issues in LLM evaluation by constructing test samples with updated real-world knowledge, ensuring the knowledge is absent from LLMs' training sets. - A fully automated workflow is designed to build and update the benchmark, eliminating the need for human labor and reducing maintenance costs. - Experiments with various LLMs demonstrate a performance drop after the cutoff time, highlighting data contamination issues in LLM evaluations. - Results manifest the effectiveness of AntiLeak-Bench for contamination-free evaluation. | ['Question Answering'] | [Link](https://github.com/bobxwu/AntiLeak-Bench) | N/A |


## Papers for 2024-12-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147) | Linchen Xiao, Hongwei Liu, Junnan Liu, zsytony, Harold-lkk | - This paper introduces G-Pass@k, a novel evaluation metric designed to assess both the potential and stability of Large Language Models (LLMs) in complex reasoning tasks, particularly mathematical problem-solving. - G-Pass@k quantifies an LLM's consistency in generating correct solutions across multiple generations by considering varying thresholds of correctness, thereby capturing limitations in traditional metrics like Greedy Accuracy and Pass@k, which often overlook output stability. - A new dynamic benchmark called LiveMathBench is introduced, comprising challenging mathematical problems from various competitions to minimize data leakage and ensure relevance to the latest advancements in LLM capabilities.  - Through extensive experiments on LiveMathBench and other datasets, the paper reveals that current LLMs, including specialized and chain-of-thought enhanced models, exhibit significant instability in their reasoning abilities, with performance drops of up to 90% in challenging scenarios.  - The findings underscore the inadequacy of conventional evaluation methods and highlight the need for stability-aware metrics like G-Pass@k for a more realistic assessment of LLM capabilities in complex reasoning tasks. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/GPassk), [Link](https://github.com/open-compass/GPassK) | N/A |
| [Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models](https://arxiv.org/abs/2412.12606) | Xiaoshuai Song, Zhuoma GongQue, Runqi Qiao, Shanglin Lei, YiFan Zhang | - This paper introduces the Multi-Dimensional Insights (MDI) benchmark, a new benchmark for evaluating large multimodal models (LMMs) on real-world personalization tasks. - The MDI benchmark consists of over 500 images and 1.2k human-posed questions across six common real-world scenarios, focusing on two key dimensions: question complexity and age demographics. - Questions are categorized into simple and complex levels to assess basic understanding and reasoning abilities, respectively, while also being stratified across young, middle-aged, and older age groups to evaluate personalized responses. - Initial evaluations using the MDI benchmark reveal that while strong models like GPT-4 achieve a 79% accuracy on age-related tasks, there remains significant room for improvement in addressing the diverse needs and preferences of different age groups in real-world scenarios. - The MDI benchmark aims to foster development towards reliable, personalized human assistants by offering a comprehensive evaluation framework covering a broad spectrum of real-world personalized needs. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain](https://arxiv.org/abs/2412.13018) | Ji-Rong Wen, Zhicheng Dou, Jiejun Tan, ShootingWong | - This paper introduces OmniEval, an automatic and omnidirectional benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in the financial domain. - The benchmark employs a matrix-based evaluation system categorizing queries into five tasks and 16 financial topics for a comprehensive assessment of diverse query scenarios. - It uses a multi-dimensional data generation approach combining GPT-4-based automatic generation and human annotation, achieving an 87.47% acceptance ratio in human evaluations. - A multi-stage evaluation system assesses both retrieval and generation performance, and robust evaluation metrics from rule-based (MAP, Rouge) and LLM-based methods ensure reliable assessment. - Experiments on various retrievers and LLMs demonstrate OmniEval's comprehensiveness and highlight performance variations across topics and tasks, showing improvement opportunities for RAG systems in the financial domain. | ['Question Answering'] | [Link](https://github.com/RUC-NLPIR/OmniEval) | N/A |
| [Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers](https://arxiv.org/abs/2412.12276) | Pulkit Agrawal, Jeff Gore, Jinyeop Song, Seungwook Han | - This paper proposes a "concept encoding-decoding" mechanism to explain how transformers perform in-context learning (ICL). - The core idea is that transformers learn to encode different latent concepts (e.g., grammatical rules or arithmetic operations) into distinct, separable representations, and simultaneously develop concept-specific decoding algorithms. - Through experiments on synthetic and natural ICL tasks (part-of-speech tagging and bitwise arithmetic), the authors show that this mechanism emerges during training and exists in pretrained language models of varying scales (Gemma-2 and Llama). - They introduce a metric called "Concept Decodability" (CD) to quantify the separability of latent concepts in representations and demonstrate that CD is predictive of ICL performance. - Causal interventions and finetuning experiments further validate that concept encoding is causally related to and predictive of ICL performance. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2024-12-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation](https://arxiv.org/abs/2412.11919) | douzc, Benen2024, wuyongkang, jinjiajie, lixiaoxi45 | - RetroLLM is a novel framework that integrates retrieval and generation within a unified auto-regressive decoding process in LLMs, allowing direct generation of fine-grained evidence from a corpus using constrained decoding. - It employs hierarchical FM-Index constraints, generating corpus-constrained clues to identify relevant documents before evidence generation to mitigate false pruning. - It introduces forward-looking constrained decoding, utilizing document FM-Index to identify future windows and a relevance model to score these windows for improved evidence accuracy. - Experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance in in-domain and out-of-domain tasks, outperforming traditional RAG and more complex RAG strategies. - RetroLLM also significantly reduces token consumption compared to existing RAG methods due to more precise retrieval granularity. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/sunnynexus/RetroLLM) | N/A |
| [BrushEdit: All-In-One Image Inpainting and Editing](https://arxiv.org/abs/2412.10316) | yshan2u, ZyZcuhk, juxuan27, BianYx, Yw22 | - BrushEdit is an interactive image editing framework that combines language models and a dual-branch inpainting technique for seamless edits such as adding/removing objects and making structural changes with free-form masks. - It leverages pre-trained multimodal large language models (MLLMs) to interpret user instructions, identify editing types and target objects, and generate textual descriptions of the edited image. - The Editing Conductor, built on BrushNet, uses a mixed fine-tuning strategy with random and segmentation masks, allowing it to handle diverse mask-based inpainting tasks. - Experimental results on PIE-Bench, BrushBench, and EditBench demonstrate BrushEdit’s superior performance in preserving unedited regions, ensuring accurate text-alignment, and outperforming existing methods in image editing and inpainting tasks. - BrushEdit offers flexible control over base diffusion model selection and scale adjustment, enhancing its practical value for diverse user needs. | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871) | spermwhale, Chunting, marg33, benjamin-mlr, artidoro | - BLT (Byte Latent Transformer) is a new byte-level LLM architecture that dynamically groups bytes into patches based on next-byte entropy, allocating more compute to complex segments. - It uses a local encoder and decoder for byte-patch transformations and a global latent transformer for patch processing, matching token-based models at scale while improving inference efficiency and robustness. - BLT achieves parity with Llama 3 in training FLOP-controlled performance while using up to 50% fewer FLOPS at inference, and shows better scaling trends with simultaneous increases in model and patch size. - It demonstrates qualitative improvements on reasoning, long-tail generalization, noisy input robustness, and sub-word aspect awareness, surpassing token-based models in these areas. - BLT's code is released for both training and inference. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/facebookresearch/blt) | N/A |
| [Smaller Language Models Are Better Instruction Evolvers](https://arxiv.org/abs/2412.11231) | Hua Zhou, Yaqi Zhang, Lulu Zhao, dongguanting, Chaox72 | - This paper investigates the effectiveness of smaller language models (SLMs) compared to larger language models (LLMs) in evolving more complex and diverse instructions for instruction tuning. - Through experiments across three instruction evolution scenarios (Evol-Instruct, AutoIF, and Auto Evol-Instruct), the study demonstrates that SLMs outperform LLMs in evolving instructions, leading to better performance in downstream tasks including instruction following, mathematical reasoning, and code generation. - The authors hypothesize that SLMs' broader output space during instruction generation, due to their relatively weaker instruction-following capabilities compared to LLMs, results in more complex and diverse instructions. - They propose a new metric called Instruction Complex-Aware IFD (IC-IFD), incorporating instruction complexity into the original IFD score for a more accurate evaluation of instruction data effectiveness without requiring instruction tuning. - Experimental results demonstrate that SLMs generate more complex and diverse instructions than LLMs leading to improved performance in downstream tasks. | ['Natural Language Processing'] | [Link](https://github.com/HypherX/Evolution-Analysis) | N/A |
| [SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models](https://arxiv.org/abs/2412.11605) | howang, yuxiaod, lrxl, wangcunxiang, CCCCCC | - This paper introduces SPaR, a self-play framework that uses tree-search refinement to enhance the instruction-following capabilities of Large Language Models (LLMs). - SPaR involves an actor LLM generating responses and a refiner LLM critiquing and refining them through a tree-search process to create preference pairs for training. - This method aims to highlight key differences for instruction following by minimizing extraneous variations often present in independently sampled responses used by other preference learning methods. - Experiments demonstrate that a LLaMA-8B model trained with SPaR surpasses GPT-4-Turbo on the IFEval benchmark and shows promising scalability with larger models like LLaMA3-70B. - The study also finds that scaling inference in tree search improves performance, and the refiner's abilities can exceed the initially distilled LLM, suggesting potential for continuous self-improvement. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/thu-coai/SPaR) | N/A |
| [GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs](https://arxiv.org/abs/2412.11258) | junweiliang, StarYDY, zhifeichen097, spongy, Xxlbigbrother | - GaussianProperty is a training-free framework that predicts physical properties of materials for 3D Gaussians using Segment Anything (SAM) and GPT-4V(ision). - It employs a global-local reasoning module for 2D images by leveraging SAM's segmentation capability and GPT-4V's recognition capability to estimate physical properties. - These properties are then projected from multi-view 2D images to 3D Gaussians using a voting strategy. -  The framework enables applications in physics-based dynamic simulation by leveraging Material Point Method (MPM) and robot grasping by developing a grasping force prediction strategy based on the estimated properties. - Experiments on material segmentation, dynamic simulation, and real-world robotic grasping demonstrate the effectiveness of GaussianProperty in enhancing downstream tasks. | ['Computer Vision', 'Multimodal', 'Image-to-3D', 'Robotics'] | N/A | N/A |
| [SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator](https://arxiv.org/abs/2412.12094) | Xiaozhe Ren, Yihang Gao, Jiawei Li, Guoxuan Chen, shihan96 | - SepLLM is a plug-and-play framework that accelerates LLM inference by compressing segments of text into separator tokens and eliminating redundant tokens.  - It leverages a data-dependent sparse attention mechanism, retaining only initial, neighboring, and separator tokens and implementing efficient kernels for training acceleration. - Experimental results show that using the Llama-3-8B backbone, SepLLM can reduce KV cache by over 50% while maintaining comparable performance on GSM8K-CoT.  - In streaming settings, SepLLM can effectively process sequences of up to 4 million tokens or more.  - SepLLM addresses the limitations of other methods by maintaining consistent performance between training and inference and by achieving substantial reductions in computational costs and training time. | ['Natural Language Processing', 'Text Generation'] | [Link](sepllm.github.io) | N/A |
| [Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture](https://arxiv.org/abs/2412.11834) | wubingheng, JingzeShi | - This paper introduces Wonderful Matrices, a novel foundation model architecture combining sequence and state transformations for enhanced efficiency and effectiveness in language modeling. - The architecture integrates Rotary Position Embedding (ROPE) for unified positional encoding in hybrid algorithms, Dynamic Mask Attention (DMAttn) for selective filtering of past states, and Cross Domain Mixture of Experts (CDMOE) for reduced parameter redundancy and efficient expert retrieval. - The paper demonstrates the effectiveness of each individual module (ROPE, DMAttn, CDMOE) through empirical validation, showing improvements in perplexity and multi-query associative recall. - Experimental results on language modeling tasks demonstrate that Wonderful Matrices outperforms other architectures like LlaMa3, Mamba2, and Jamba across various evaluation metrics, especially with increasing parameter scale. - The architecture uses a combination of State Space Duality (SSD) and DMAttn modules for sequence transformation and CDMOE modules for state transformation, achieving a balance between efficiency and performance. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/LoserCheems/Doge) | N/A |
| [Whisper-GPT: A Hybrid Representation Audio Large Language Model](https://arxiv.org/abs/2412.11449) | prateekv | - WHISPER-GPT, a novel hybrid large language model (LLM) for speech and music generation, leverages continuous audio representations (mel-spectrograms) alongside discrete acoustic tokens within a single Transformer decoder-only architecture. - This hybrid approach addresses context length limitations encountered in purely discrete token-based LLMs by incorporating continuous audio information while retaining the advantages of discrete tokens for sampling and generation. - Experimental results on LibriSpeech TTS and a music dataset demonstrate that WHISPER-GPT with 4M parameters achieves comparable performance to a 40M parameter purely token-based LLM, showcasing the efficiency of the hybrid representation. - The model predicts the next token given the past acoustic tokens and mel-spectrogram slices, improving the next token prediction metrics like negative log-likelihood and perplexity. - Future work involves using this hybrid LLM to fine-tune other audio tasks such as generating multi-scale acoustic tokens and generate high-fidelity audio samples conditioned on them. | ['Audio', 'Text-to-Audio', 'Text-to-Speech'] | N/A | N/A |


## Papers for 2024-12-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Apollo: An Exploration of Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2412.10360) | minione, lichengyu, YannDubs, nicholswang, orrzohar | - The paper introduces Apollo, a family of state-of-the-art Large Multimodal Models (LMMs) designed for enhanced video understanding, capable of processing hour-long videos efficiently. - Apollo utilizes a unified architecture employing a combination of InternVideo2 and SigLIP-SO400M encoders, with features concatenated and resampled using a Perceiver Resampler before being fed to a large language model (LLM). - The authors claim Apollo-3B outperforms most existing 7B models, achieving a score of 58.4 on Video-MME (without subtitles), 68.7 on MLVU, and 62.7 on their proposed benchmark, ApolloBench.  - Apollo-7B achieves state-of-the-art performance amongst 7B LMMs with scores of 61.2 on Video-MME, 70.9 on MLVU, and 66.3 on ApolloBench, demonstrating competitiveness with some 30B models. - The study also explores various design choices, such as video sampling strategies, encoder combinations, and data composition, introducing the concept of "Scaling Consistency," where design decisions from smaller models effectively transfer to larger models. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities](https://arxiv.org/abs/2412.07769) | Saeed Yahya Alseiari, Mohammed Irfan Kurpath, hishamcholakkal, HuggingSara, sahalshajim | - BiMediX2 is a bilingual (Arabic-English) Large Multimodal Model (LMM) with a unified architecture integrating text and visual modalities for advanced medical image understanding and applications. - It leverages the Llama 3.1 architecture with integrated text and visual capabilities, supporting text and multi-turn conversations involving medical images and trained on a 1.6M sample bilingual healthcare dataset (BiMed-V). - BiMediX2 outperforms state-of-the-art models in medical LLM and VLM evaluation benchmarks, exceeding GPT-4 by 9% in UPHILL factual accuracy and showing over 9% improvement in English and 20% in Arabic on multimodal medical evaluations. - A new bilingual GPT4-based medical LLM benchmark called BiMed-MBench was introduced. - The model excels in medical Visual Question Answering, Report Generation, and Report Summarization tasks across diverse imaging modalities. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text', 'Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/mbzuai-oryx/BiMedix2) | N/A |
| [Large Action Models: From Inception to Implementation](https://arxiv.org/abs/2412.10047) | Eliblo1969, substill, shilhe, Lujunting, vyokky | - This paper introduces Large Action Models (LAMs), a new type of AI model designed to perform actions in both physical and digital environments, extending the capabilities of Large Language Models (LLMs). - LAMs are trained using a four-phase approach: task-plan pretraining, learning from experts, self-boosting exploration, and learning from a reward model. - The authors demonstrate the effectiveness of LAMs by integrating them into a Windows OS-based agent, showing superior performance in task completion compared to LLMs like GPT-40, particularly in scenarios requiring precise interaction and manipulation within specific environments. - The LAM achieved an 81.2% Task Success Rate (TSR), surpassing GPT-40's 67.2% and GPT-40 Mini's 62.3% in a Word application environment, demonstrating the effectiveness of LAMs over traditional LLMs in action-oriented tasks. - The paper concludes by discussing the current limitations of LAMs and identifying key areas for future research. | ['Natural Language Processing', 'Reinforcement Learning', 'Robotics'] | [Link](https://github.com/microsoft/UFO/tree/main/dataflow) | N/A |
| [ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation](https://arxiv.org/abs/2412.08645) | Dana Berman, Matan Cohen, Asaf Shul, yedid, danielwinter | - ObjectMate introduces a tuning-free method for object insertion and subject-driven generation, utilizing a novel "object recurrence prior." - This prior leverages the recurrence of everyday objects across large, unlabeled image datasets to create a massive, supervised training dataset with diverse poses, lighting, and scenes. - The model architecture is based on a straightforward text-to-image diffusion model trained on this dataset, taking object views and scene descriptions as input. - ObjectMate achieves state-of-the-art results on object insertion and subject-driven generation tasks, outperforming existing methods in identity preservation and photorealistic composition. - The paper also introduces a new object insertion evaluation dataset with ground truth data and proposes a new metric for identity preservation that aligns better with human perception, validated through a user study. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation](https://arxiv.org/abs/2412.09428) | morninghaze, baochenxi, wzk1015, JackyZhuo, wbs2788 | - This paper introduces Visuals Music Bridge (VMB), a novel multimodal music generation framework that uses text and music as explicit bridges for enhanced cross-modal alignment. - VMB consists of three core components: a Multimodal Music Description Model (MMDM) to convert visual input into text descriptions; a Dual-track Music Retrieval module to retrieve relevant music pieces; and an Explicitly Conditioned Music Generation framework to synthesize music. - The Explicitly Conditioned Music Generation module consists of a latent diffusion transformer (DiT) and employs Music ControlFormer and Stylization Module to enable high-quality generation. - The proposed method addresses challenges like data scarcity, weak cross-modal alignment, and limited controllability in existing multimodal music generation methods. - Experimental results on video-to-music, text-to-music, image-to-music, and controllable music generation tasks demonstrate that VMB significantly improves music quality, modality, and customization alignment compared to previous methods. | ['Multimodal', 'Text-to-Audio', 'Video-Text-to-Text', 'Image-to-Text'] | [Link](https://github.com/wbs2788/VMB) | N/A |
| [SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding](https://arxiv.org/abs/2412.09604) | wzk1015, Einsiedler, hehesang, Changyao, cpsxhao | - SynerGen-VL is a unified Multimodal Large Language Model (MLLM) designed for synergistic image understanding and generation using a single architecture and training process with a next-token prediction paradigm. - It introduces a token folding mechanism with a hierarchical architecture to compress input image token sequences, enabling efficient handling of high-resolution images and a decoder that reconstructs the image during generation. - Vision-expert-based progressive alignment pretraining integrates visual capabilities into the pretrained LLM, minimizing disruption to existing knowledge by using image-specific Feed-Forward Networks (FFNs) and aligning visual representations with the LLM's representation space. - Trained on large-scale mixed image-text data, SynerGen-VL achieves competitive performance compared to existing encoder-free unified MLLMs with comparable or smaller parameter sizes and narrows the gap with task-specific state-of-the-art models. - With 2.4B activated parameters, SynerGen-VL matches the performance of Emu3, which has 8B parameters, demonstrating its strong potential as a next-generation unified MLLM. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](https://arxiv.org/abs/2412.10319) | Chengruidong, luoxufang, qianhuiwu, iofu728, liyucheng | - Introduces SCBench, a benchmark designed to evaluate efficient long-context methods, particularly for shared context and multi-round interactions where KV Cache is reused. - Assesses four key long-context abilities: String Retrieval, Semantic Retrieval, Global Information processing, and Multi-tasking across 12 tasks with two shared context modes (multi-turn and multi-request). - Evaluates 13 long-context methods across four stages (generation, compression, retrieval, and loading) and eight categories on six open-source long-context LLMs. - Finds that sub-O(n) memory methods struggle in multi-turn scenarios, sparse encoding with O(n) memory performs robustly, and dynamic sparsity is more expressive for KV caches than static patterns. - Identifies attention distribution shift issues in long-generation scenarios, impacting performance even for O(n) memory methods. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | N/A | N/A |
| [SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs](https://arxiv.org/abs/2412.08347) | SultanR | - This paper introduces SmolTulu-1.7b-Instruct, an instruction-tuned language model based on Huggingface's SmolLM2-1.7B and adapted from AllenAI's Tulu 3 training pipeline. - The research focuses on the impact of learning rate to batch size ratios on model performance across different tasks, finding that higher ratios benefit reasoning tasks while lower ratios are optimal for pattern recognition tasks. - SmolTulu achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval (Δ11%) and 51.6% on GSM8K (13.4%) for mathematical reasoning. - The model also achieved 57.1% on ARC (15.4%) with an alternate version.  -  Training recipes and ablation studies are released to promote further research in efficient model alignment and optimization for small language models. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | [Link](https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct) |


## Papers for 2024-12-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions](https://arxiv.org/abs/2412.09596) | Rui Qian, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Pan Zhang | - InternLM-XComposer2.5-OmniLive (IXC2.5-OL) is a multimodal system designed for real-time interaction with streaming video and audio inputs, addressing the limitations of current Multimodal Large Language Models (MLLMs) in continuous perception, memory, and reasoning. - IXC2.5-OL consists of three modules: a Streaming Perception Module processing multimodal information, a Multi-modal Long Memory Module integrating and retrieving short-term and long-term memories, and a Reasoning Module handling queries. - The system simulates human-like cognition by disentangling streaming perception, reasoning, and memory mechanisms, allowing simultaneous processing of information. - Evaluation across audio and video benchmarks demonstrates IXC2.5-OL's superior performance, achieving state-of-the-art results on StreamingBench for real-time video interactions and competitive results on other benchmarks like MLVU and Video-MME. - IXC2.5-OL excels in real-time video interactions while demonstrating competitive results among open-source models on several audio and video understanding benchmarks. | ['Multimodal', 'Video-Text-to-Text', 'Audio', 'Automatic Speech Recognition'] | [Link](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive) | N/A |
| [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905) | Ronen Eldan, Sébastien Bubeck, Harkirat Behl, Jyoti Aneja, Marah Abdin | - Phi-4 is a 14-billion parameter language model trained with an emphasis on data quality, incorporating synthetic data generated by diverse techniques like multi-agent prompting and instruction reversal. - Phi-4 surpasses its teacher model (GPT-4) on STEM-focused QA, demonstrating that the data generation and post-training techniques provide capabilities beyond distillation. - The training recipe focuses on three pillars: synthetic data generation, curation of high-quality organic data, and innovative post-training techniques like rejection sampling and a new approach to Direct Preference Optimization (DPO). - Phi-4's performance on reasoning tasks is comparable to or surpasses larger models, exceeding Llama-3.1 on benchmarks like GPQA and MATH, and scoring high on the November 2024 AMC math competitions, indicating robust reasoning abilities and lack of overfitting. - Post-training includes supervised fine-tuning (SFT), pivotal token search-based DPO, and judge-guided DPO, refining the model's alignment with human preferences, improving reasoning, safety, robustness, and mitigating hallucinations. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions](https://arxiv.org/abs/2412.08737) | Willie Neiswanger, Jinyi Hu, Tianyu Yu, Ollie Liu, jrzhang | - This paper introduces Euclid, a family of Multimodal Large Language Models (MLLMs) specifically optimized for enhanced low-level visual perception (LLVP) in geometric tasks. - Euclid employs a curriculum learning strategy with synthetically generated high-fidelity visual descriptions of geometric shapes and their relationships, addressing the limitations of existing MLLMs in accurately perceiving detailed geometric information.  -  A new benchmark dataset called *Geoperception*, derived from the Geometry-3K corpus, is introduced to evaluate the model’s ability to precisely transcribe 2D geometric information from images. -  Euclid outperforms leading open-source and closed-source models on the Geoperception benchmark, demonstrating strong generalization capabilities to novel geometry shapes.  - For instance, Euclid surpasses the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain tasks and 10.65% on average. | ['Multimodal', 'Visual Question Answering', 'Computer Vision'] | [Link](github.com/euclid-multimodal/Euclid) | [Link](huggingface.co/euclid-multimodal) |
| [Multimodal Latent Language Modeling with Next-Token Diffusion](https://arxiv.org/abs/2412.08635) | Li Dong, Zhiliang Peng, Wenhui Wang, Hangbo Bao, Yutao Sun | - LatentLM, a novel multimodal generative model, is introduced, which seamlessly integrates continuous and discrete data using causal transformers and next-token diffusion. - The model employs a variational autoencoder (VAE) to represent continuous data as latent vectors and next-token diffusion for autoregressive generation of these vectors, overcoming variance collapse issues with the use of σ-VAE. - LatentLM shows superior performance in image generation, surpassing Diffusion Transformers in both performance and scalability, as evidenced by its favorable FID and IS scores on ImageNet. - When integrated into multimodal large language models, LatentLM provides a unified interface, outperforming existing methods in language modeling, text-to-image generation, and vision-language understanding tasks, as demonstrated by its better perplexity scores and FID scores on MS-COCO. - In text-to-speech synthesis, LatentLM achieves state-of-the-art results, outperforming VALL-E 2 in speaker similarity and robustness while requiring 10x fewer decoding steps. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Text-to-Speech'] | [Link](https://github.com/facebookresearch/DiT) | N/A |
| [AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials](https://arxiv.org/abs/2412.09605) | Zhennan Shen, Dunjie Lu, Yiheng Xu, cxiong, ZeonLap | - AgentTrek is a novel data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials. - It automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. - A VLM-based evaluator ensures the correctness of the generated trajectories, resulting in a dataset with 10,398 trajectories. - Experiments demonstrate that training GUI agents with the synthesized trajectories significantly improves their grounding and planning performance compared to current models. - The proposed method is more cost-efficient than traditional human annotation methods, paving the way for large-scale GUI agent training. | ['Multimodal'] | [Link](https://agenttrek.github.io) | N/A |
| [Learned Compression for Compressed Learning](https://arxiv.org/abs/2412.09405) | Neeraja J. Yadwadkar, Dan Jacobellis | - This paper introduces WaLLoC (Wavelet Learned Lossy Compression), a novel neural codec architecture for compressed-domain learning. - WaLLoC combines linear transform coding with nonlinear dimensionality-reducing autoencoders, placing a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. - This design enables WaLLoC to achieve computationally efficient encoding, high compression ratios, and uniform dimensionality reduction—key requirements for effective compressed learning. - Experiments demonstrate WaLLoC's superior performance compared to existing autoencoders used in state-of-the-art latent diffusion models across several metrics, including image classification, colorization, document understanding, and music source separation. - WaLLoC achieves up to 20x dimensionality reduction, making it an effective drop-in replacement for resolution reduction in accelerating downstream models without sacrificing accuracy. | ['Computer Vision', 'Image Classification', 'Image-to-Image', 'Audio-to-Audio', 'Audio Classification'] | N/A | [Link](https://ut-sysml.org/walloc/) |
| [Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://arxiv.org/abs/2412.09501) | Longxiang Tang, Senqiao Yang, Yuqi Liu, Chengyao Wang, Zhisheng Zhong | - Lyra, an efficient Multimodal Large Language Model (MLLM), enhances multimodal abilities with a focus on speech, including long speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction. - Lyra leverages existing open-source large models and a multi-modality LoRA to reduce training costs, uses a latent multi-modality regularizer and extractor to strengthen relationships between modalities (especially speech), and introduces a high-quality dataset with 1.5M multimodal samples and 12K long speech samples. - Compared to other omni-models, Lyra achieves state-of-the-art performance on vision-language, vision-speech, and speech-language benchmarks. - Lyra demonstrates superior efficiency with fewer computational resources, less training data, and faster training and inference speed across speech, image, and video tasks. - The model supports sound and speech understanding and generation, handles complex long speech inputs, and exhibits enhanced omni-comprehension capabilities. | ['Multimodal', 'Visual Question Answering', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition'] | [Link](https://github.com/dvlab-research/Lyra) | N/A |
| [RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios](https://arxiv.org/abs/2412.08972) | Xiaobao Wu, Sitao Cheng, Liangming Pan, Wenyue Hua, Ruiwen Zhou | - RULEARENA, a benchmark designed to evaluate LLMs' ability to follow complex real-world rules in reasoning across three domains: airline baggage fees, NBA transactions, and tax regulations. - It assesses proficiency in handling intricate instructions requiring long-context understanding, logical reasoning, and accurate mathematical computation. - Two key distinctions from existing benchmarks: extends beyond first-order logic and grounds tasks in authentic scenarios. - Findings reveal LLM limitations: difficulty applying rules, inaccurate math, and overall poor performance. - Highlights challenges in rule-guided reasoning for LLMs in real-world applications. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/skyriver-2000/RuleArena) | N/A |
| [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/abs/2412.09569) | Lilach Eden, Roy Bar-Haim, Yotam Perlitz, Odellia Boni, Ariel Gera | - This paper introduces JuStRank, a novel benchmark designed to assess the efficacy of Large Language Models (LLMs) in performing system-level ranking of other LLMs. - The benchmark employs a dataset of responses generated by various LLMs to a set of instructions and evaluates the judges' ability to rank the systems based on the quality of their responses, using correlation with a human-generated ranking as the primary metric. -  The study evaluates 48 state-of-the-art judges, encompassing both general-purpose LLMs and specialized reward models, and reveals that several smaller reward models can perform comparably to much larger LLMs in this ranking task. - The authors also investigate the influence of various judge realizations (e.g., absolute numeric scores, Likert-scale ratings, comparative judgments) on ranking accuracy and observe a significant impact, with absolute scores generally outperforming comparative judgments. -  The analysis identifies two key emergent properties of system-level judges: decisiveness, characterized by the tendency to amplify differences between strong and weak systems, and system-specific bias. | ['Natural Language Processing'] | N/A | N/A |
| [OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation](https://arxiv.org/abs/2412.09585) | Jianwei Yang, Jianfeng Gao, Humphrey Shi, Zhengyuan Yang, Jitesh Jain | - OLA-VLM introduces a novel approach to enhance visual representations within Multimodal Large Language Models (MLLMs) by distilling knowledge from specialized teacher vision encoders into the LLM's intermediate layers during pretraining. - This method involves a coupled optimization of predictive visual embedding and next text-token prediction, incorporating embedding losses at specific LLM layers to align with target visual features from encoders trained on tasks like segmentation, depth estimation, and image generation. - The approach also utilizes specialized tokens enriched with target-specific information, creating an implicit visual chain of thought within the LLM's input sequence. - OLA-VLM demonstrates improved performance over single and multi-encoder baselines on various benchmarks, including up to an 8.7% boost on the Depth task in CV-Bench, showcasing its superior visual understanding capabilities. - The embedding optimization strategy is hypothesized to yield better projector initialization for the instruction fine-tuning stage, leading to efficient and accurate visual processing within the MLLM while using only a single vision encoder during inference. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Depth Estimation', 'Image Segmentation'] | [Link](https://github.com/SHI-Labs/OLA-VLM) | N/A |
| [The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective](https://arxiv.org/abs/2412.09460) | David Samuel, Freddy Wetjen, Lemei Zhang, Vladislav Mikhailov, Javier de la Rosa | - This research investigates the impact of copyrighted material on Norwegian Large Language Models (LLMs) by training models using various datasets, including copyrighted and non-copyrighted materials. - The study uses Mistral 7B v0.1 architecture as base and creates different versions by pre-training from scratch and also warm-starting and fine-tuning with copyrighted materials and instructions. - Evaluations are conducted using a new benchmarking suite with 28 NLP tasks designed for Norwegian, demonstrating that copyrighted material leads to performance improvements across diverse tasks, particularly specialized ones. - The paper finds that warm-starting with other languages reduces the impact of adding Norwegian copyrighted data. - The findings highlight ethical and legal considerations regarding the use of copyrighted materials in LLM development and provide empirical evidence for policy discussions on author compensation and copyright in the digital age. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/mistralai/Mistral-7B-v0), [Link](https://huggingface.co/datasets/mimir-project/mimir-bias), [Link](https://huggingface.co/datasets/ltg/nortruthfulqa_mc), [Link](https://huggingface.co/datasets/ltg/nortruthfulqa_gen), [Link](https://huggingface.co/datasets/ltg/noropenbookqa), [Link](https://huggingface.co/datasets/ltg/nrk), [Link](https://huggingface.co/datasets/ltg/norcommonsensega), [Link](https://huggingface.co/datasets/mimir-project/noridiom), [Link](https://huggingface.co/datasets/SamiaT/NorSumm) |
| [Word Sense Linking: Disambiguating Outside the Sandbox](https://arxiv.org/abs/2412.09370) | Roberto Navigli, Alberte Fernández-Castro, Luigi Procopio, Edoardo Barba, Andrei Stefan Bejgu | - This paper introduces Word Sense Linking (WSL), a new task that aims to bridge the gap between Word Sense Disambiguation (WSD) research and its practical application in downstream tasks. - WSL requires systems to identify and disambiguate all spans in a given text using only a reference sense inventory, without relying on pre-identified spans or candidate senses. - A novel retriever-reader architecture is proposed for WSL, which inverts the traditional concept detection and candidate generation steps of WSD to overcome limitations in handling unannotated spans. - The model outperforms state-of-the-art WSD systems adapted for the WSL setting by a significant margin, demonstrating its robustness and efficiency. - A new WSL benchmark dataset, built by expanding the existing WSD evaluation datasets, facilitates comprehensive evaluation of both precision and recall. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Babelscape/WSL) | N/A |
| [SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](https://arxiv.org/abs/2412.05552) | Mohit Bansal, Chongyang Zhao, Zun Wang, Yicong Hong, Gengze Zhou | - This paper proposes State-Adaptive Mixture of Experts (SAME), a novel model for versatile language-guided visual navigation that can interpret and execute instructions with varying levels of granularity. - SAME utilizes a mixture of expert networks specialized in different navigation skills, such as exploration and instruction-following, routed based on the agent's current state (attended language and visual observation). - The model effectively addresses the challenge of conflicting learning objectives in multi-task training by selectively activating experts based on the input state, promoting shared knowledge learning while maintaining task-specific capabilities. - Experimental results on seven navigation tasks including R2R, RxR-EN, REVERIE, OBJECTNAV, CVDN, SOON, and R2R-CE, demonstrate that SAME outperforms or achieves comparable performance to task-specific agents, showing the efficacy of the state-adaptive expert routing mechanism. - Further analysis reveals that applying MoE on visual queries in the cross-attention layer of the navigation policy yields superior results compared to applying it on feed-forward networks, highlighting the importance of cross-modal attention in action selection. | ['Computer Vision', 'Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/GengzeZhou/SAME) | N/A |
| [Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages](https://arxiv.org/abs/2412.09025) | Srinivasan Umesh, rumourscape | - This paper introduces Shiksha, a dataset and model for translating technical content into Indian languages. - The dataset comprises 2.8 million high-quality, parallel translation pairs across 8 Indian languages, extracted from NPTEL lecture transcriptions. - A 3.3B parameter NLLB model is fine-tuned using LoRA on this dataset, demonstrating improvements in both in-domain and out-of-domain translation tasks. - Evaluation on a held-out test set and the Flores+ benchmark shows improved performance over baseline NLLB and comparable results to IndicTrans2. - The models are integrated into a tool called Translingua, which aids human annotators in translating NPTEL lectures. | ['Translation', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/SPRINGLab) |


## Papers for 2024-12-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations](https://arxiv.org/abs/2412.08580) | MAJIARUI, SYZhang0805, yeezlee, mengcy, hyllbd | - This paper introduces LAION-SG, a large-scale dataset with scene graph annotations for training complex image-text models. - LAION-SG is an enhancement of LAION-Aesthetics V2 (6.5+) with high-quality scene graph annotations by GPT-4, featuring multiple objects, detailed attributes, and relationships. - A new foundation model, SDXL-SG, based on Stable Diffusion XL, incorporates scene graph information through a graph neural network to improve complex scene generation. - Both quantitative and qualitative results show that models trained on LAION-SG significantly outperform those trained on existing datasets like COCO-Stuff and Visual Genome. - A new benchmark, CompSG-Bench, has been established to evaluate models on complex image generation, setting a new standard. | ['Text-to-Image', 'Graph Machine Learning', 'Multimodal'] | [Link](https://github.com/mengcye/LAION-SG) | N/A |
| [POINTS1.5: Building a Vision-Language Model towards Real World Applications](https://arxiv.org/abs/2412.08443) | Xiao Zhou, Le Tian, yangyu1, kavio, YuanLiuuuuuu | - POINTS1.5 is a new vision-language model based on the LLaVA architecture, which uses a pre-trained vision encoder, a randomly initialized projector, and a pre-trained large language model. - It incorporates a NaViT-style vision encoder that supports dynamic high resolution, eliminating the need to split images into tiles and improving performance on text-intensive tasks. - The model adds bilingual support (Chinese and English) and uses a refined chat template for pre-training, improving performance over its predecessor, POINTS1.0. - A rigorous filtering method is applied to visual instruction tuning datasets to remove samples with grammatical errors and questions answerable without images, further improving the quality of the training data. - POINTS1.5-7B achieves top ranking on the OpenCompass leaderboard among models under 10B parameters, outperforming models several times larger. | ['Multimodal', 'Image-Text-to-Text'] | [Link](https://github.com/WePOINTS/WePOINTS) | [Link](https://huggingface.co/WePOINTS/POINTS-1-5-Qwen-2-5-7B-Chat) |
| [StreamChat: Chatting with Streaming Video](https://arxiv.org/abs/2412.08646) | Shiyi Lan, hsli-cuhk, LucasFang, Zhiding, jjjjh | - This paper introduces StreamChat, a novel approach for enhancing Large Multimodal Models (LMMs) to interact with streaming video content by dynamically updating the visual context at each decoding step using a cross-attention based architecture and visual feedforward network (V-FFN). - A parallel 3D-ROPE mechanism is used to better encode temporal information, and a dense instruction-tuning dataset based on existing dense caption datasets is created to train the model. - StreamChat outperforms state-of-the-art video LMMs in streaming interaction scenarios, demonstrating its superior ability to handle dynamic video content, even outperforming LLaVA-Video-72B with a smaller 7B model. - StreamChat also achieves competitive performance on established image and video benchmarks. -  The model effectively captures video dynamics and adjusts responses accordingly by incorporating the latest video information at each decoding step for more temporally aligned responses. | ['Multimodal', 'Video-Text-to-Text'] | N/A | N/A |
| [StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements](https://arxiv.org/abs/2412.08503) | Chi Zhang, Hao Wang, Beier Zhu, Xue Song, Mingkun Lei | - StyleStudio, a novel text-driven style transfer model, addresses challenges like overfitting to reference styles, limited stylistic control, and misalignment with text content by employing three strategies. - It introduces cross-modal Adaptive Instance Normalization (AdaIN) to better integrate style and text features, Style-based Classifier-Free Guidance (SCFG) for selective style control, and a teacher model during early generation to stabilize layouts and reduce artifacts.  - Evaluations show significant improvements in style transfer quality, alignment with text prompts, and layout stability across different styles.  - The model achieves higher text alignment scores compared to existing methods and performs favorably in user studies assessing text alignment and style similarity. - The approach is versatile and can be integrated into various style transfer frameworks without requiring fine-tuning. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation](https://arxiv.org/abs/2412.07147) | Lijie Wen, Shaolin Zhu, liboaccn | - Introduced MIT-10M, a large-scale parallel corpus for multilingual image translation, containing over 10 million image-text pairs derived from real-world data and spanning 14 languages, 28 categories, and three difficulty levels. - The dataset underwent extensive cleaning and multilingual translation validation, including OCR annotation, NSFW detection, and sensitive content filtering. - Experiments demonstrated MIT-10M's superior performance in evaluating models on challenging real-world image translation tasks, particularly multi-line text in complex images.  - Fine-tuning Qwen2-VL with MIT-10M resulted in significant improvements, tripling performance compared to the baseline. - The dataset promotes the development of more robust and adaptable multilingual image translation models. | ['Multimodal', 'Image-to-Text', 'Translation'] | N/A | [Link](https://huggingface.co/datasets/liboaccn/MIT-10M) |


## Papers for 2024-12-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Evaluating and Aligning CodeLLMs on Human Preference](https://arxiv.org/abs/2412.05210) | JustinLin610, huybery, misakamage, instro, jx-yang | - This paper introduces CodeArena, a new human-curated benchmark for evaluating code large language models (LLMs) based on human preference, addressing the gap between code correctness and user satisfaction. - CodeArena contains 397 diverse samples across 40 coding categories and 44 programming languages, focusing on real-world coding scenarios. - In addition, the authors create SynCode-Instruct, a large-scale (20B token) synthetic instruction dataset generated by scaling instructions from web sources and generating corresponding code snippets with unit tests where applicable. Fine-tuning Qwen2.5-Coder using SynCode-Instruct leads to state-of-the-art performance for open-source code LLMs.  - Evaluations of 40+ LLMs on CodeArena show significant differences compared to execution-based benchmarks and a large performance gap between open-source and closed-source LLMs. - This work also shows that fine-tuning with larger synthetic instruction data sets improves code generation and benchmark performance in code LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://codearenaeval.github.io/) | N/A |
| [DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation](https://arxiv.org/abs/2412.07589) | Chao Tang, LXT, zengyh1900, JingboWang, jianzongwu | - DiffSensei, a novel framework for generating customized manga panels, bridges Multi-Modal Large Language Models (MLLMs) with diffusion-based image generators, enabling dynamic multi-character control and narrative consistency. - The framework uses an MLLM as a text-compatible identity adapter, allowing character features to adjust based on panel captions and ensuring layout control through masked cross-attention injection and dialog embedding. - A new large-scale dataset, MangaZero, containing 43,264 manga pages and 427,147 annotated panels, supports the training and evaluation of models on customized manga generation. - Experimental results show DiffSensei outperforms existing story visualization models in character consistency, layout controllability, and text adherence, marking a significant advancement in manga generation by enabling text-adaptable character customization. - A human preference study further validates DiffSensei’s superior performance, highlighting its capability to create coherent and expressive manga panels. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [STIV: Scalable Text and Image Conditioned Video Generation](https://arxiv.org/abs/2412.07730) | jefflai, JesseAllardice, tsujuifu, wenzehu, Jiasenlu | - STIV (Scalable Text and Image Conditioned Video Generation) is a new text- and image-conditioned video generation model based on a Diffusion Transformer (DiT) architecture, integrating image conditions via frame replacement and text conditions through a joint image-text conditional classifier-free guidance. - This design enables STIV to handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks concurrently, and it's adaptable to various applications like video prediction and frame interpolation. - The 8.7B parameter STIV model achieves state-of-the-art performance on VBench benchmarks, scoring 83.1 on T2V and 90.1 on TI2V at 512x512 resolution, outperforming existing models like CogVideoX-5B, Pika, Kling, and Gen-3. - A progressive training approach involving text-to-image and text-to-video stages, alongside techniques like QK-norm, sandwich-norm, and MaskDiT, enhances stability and efficiency during model scaling. - The model utilizes flow matching as a training objective and incorporates rotary positional embeddings, micro-conditions, and a novel joint image-text classifier-free guidance strategy to improve performance and address motion-related issues. | ['Text-to-Video', 'Image-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Perception Tokens Enhance Visual Reasoning in Multimodal Language Models](https://arxiv.org/abs/2412.03548) | Dongping Chen, Ethan Shen, Cheng-Yu Hsieh, Zelun Luo, Mahtab Bigverdi | - This research introduces Perception Tokens, intrinsic image representations designed to improve visual reasoning abilities in Multimodal Language Models (MLMs). - Perception tokens, such as depth maps and bounding boxes, are incorporated as intermediate reasoning steps in the MLM's chain-of-thought process using a novel training method called AURORA. - AURORA leverages a VQVAE to transform visual representations into tokens and trains the MLM in a multi-task framework with a curriculum learning strategy. - The proposed LLaVA-AURORA model demonstrates significant improvement over fine-tuning baselines on benchmarks including BLINK, CVBench, and SEED-Bench for depth estimation and object counting tasks. - Experimental results exhibit the effectiveness of Perception Tokens, resulting in a 6.4% boost for relative depth estimation on BLINK and up to 11.3% improvement on object counting across datasets. | ['Multimodal', 'Computer Vision', 'Depth Estimation', 'Object Detection'] | N/A | N/A |
| [Granite Guardian](https://arxiv.org/abs/2412.07724) | Tejaswini Pedapati, Subhajit Chaudhury, Manish Nagireddy, Inkit Padhi, Giandomenico | - This paper introduces Granite Guardian, a suite of safeguard models (2B and 8B parameter sizes) designed for comprehensive risk detection in Large Language Models, addressing prompt and response risks related to social biases, security (jailbreaking), and RAG-specific issues like context relevance, groundedness, and answer relevance. - The models are trained on a combined dataset of human-annotated data from diverse sources and synthetic data generated to specifically cover adversarial attacks and RAG hallucinations.  - Evaluation on standard benchmarks like the OpenAI Moderation Evaluation Dataset, HarmBench, and ToxicChat show that Granite Guardian outperforms existing open-source models on key metrics like AUC, F1, and recall. - On RAG hallucination benchmarks (groundedness), Granite Guardian achieves an average AUC of 0.854 on the TRUE dataset and performs competitively with dedicated models explicitly trained for groundedness detection. - The models are released open-source to encourage community adoption and promote responsible development of safer LLM applications. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/ibm-granite/granite-guardian) | [Link](https://huggingface.co/ibm-granite/granite-guardian-hap-38m) |
| [ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance](https://arxiv.org/abs/2412.06673) | Jianhua Han, Runhui Huang, Junwei Yang, Guansong Lu, Chunwei Wang | - ILLUME, a unified multimodal large language model (MLLM), seamlessly integrates understanding and generation capabilities using a next-token prediction approach. - It incorporates a semantic vision tokenizer and a progressive multi-stage training to enhance data efficiency, requiring only 15M image-text pairs for pretraining. - ILLUME introduces a self-enhancing multimodal alignment scheme where the model assesses consistency between generated images and text descriptions for synergistic improvement. - Experiments show ILLUME competing with state-of-the-art unified and specialized MLLMs across visual understanding, generation, and editing. - It outperforms previous best models on several benchmarks by significant margins, like a 25% improvement on MMMU and 14% on SEED. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation](https://arxiv.org/abs/2412.07759) | Menghan Xia, Sida Peng, Xintao Wang, Xian Liu, lemonaddie | - 3DTrajMaster is a novel approach for manipulating multi-entity 3D motions in video generation using entity-specific 6DoF pose sequences as input, leveraging a plug-and-play 3D-motion grounded object injector. - The object injector fuses entity descriptions and trajectories into latent embeddings, which are then combined and fed into a gated self-attention layer for motion fusion, preserving the video diffusion prior and generalizing to diverse entities and trajectories. - A new 360°-Motion Dataset is constructed using UE rendering, correlating 3D human and animal assets with GPT-generated trajectories and capturing motion with 12 cameras, addressing the lack of suitable training data. - A domain adaptor and annealed sampling strategy mitigate video quality degradation during training and inference, respectively. - Experimental results demonstrate state-of-the-art accuracy and generalization for controlling multi-entity 3D motions, outperforming existing methods in pose accuracy and handling complex scenarios like 3D occlusions. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation](https://arxiv.org/abs/2412.07334) | Kazuhiro Fukui, Erica K. Shimomoto, Lincon S. Souza, Pedro H. V. Valois | - This paper introduces the Frame Representation Hypothesis (FRH), a new framework for interpreting and controlling Large Language Models (LLMs) by modeling multi-token words as frames, which are ordered sequences of independent vectors. - FRH extends the Linear Representation Hypothesis (LRH) which was limited to single-token words, and makes it applicable to multi-token words and thus any textual data. - The paper proposes Concept Frames, which are centroids of a set of word frames that share a common concept, and shows that over 99% of words composed of several tokens are composed of linearly independent token vectors. - A Top-k Concept-Guided Decoding method is introduced for steering text generation using chosen concepts, which helps to expose and potentially remediate biases present in LLMs. - Experiments on Llama 3.1, Gemma 2, and Phi 3 demonstrate FRH's ability to expose biases and steer text generation, showing its applicability to enhance the transparency and controllability of LLMs. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/phvv-me/frame-representation-hypothesis.git) | N/A |
| [Fully Open Source Moxin-7B Technical Report](https://arxiv.org/abs/2412.06845) | Sung-En Chang, Yixin Shen, Zhenglun Kong, Xuan Shen, Pu Zhao | - Moxin-7B is a fully open-source large language model (LLM) based on the Mistral architecture, extended to 36 blocks from 32, and trained on over 2 trillion tokens. - It incorporates grouped-query attention (GQA), sliding window attention (SWA), and a rolling buffer cache for efficient long-context handling (up to 32K tokens). - The model was trained in three phases: initial pre-training with 2k and 4k context lengths and a final capability enhancement phase using curated data from Hugging Face and evaluation benchmarks. - Evaluation on standard benchmarks like ARC, HellaSwag, MMLU, Winogrande, and PIQA demonstrates superior zero-shot performance against other 7B models and competitive results in few-shot settings. - The chat model variant, Moxin-7B-chat, outperforms baselines on MTBench, showcasing strong alignment capabilities. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/moxin-org/Moxin-LLM) | [Link](https://huggingface.co/moxin-org/moxin-llm-7b), [Link](https://huggingface.co/moxin-org/moxin-chat-7b) |
| [Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation](https://arxiv.org/abs/2412.07338) | Felice Dell'Orletta, Marco Avvenuti, Amaury Trujillo, Alessio Miaschi, Lorenzo Cima | - This paper proposes and evaluates strategies for generating contextualized and personalized counterspeech using a LLaMA2-13B model. - The model is instructed to generate counterspeech by leveraging contextual information such as community, conversation details, and user history. - The study finds that contextualized counterspeech significantly outperforms generic counterspeech in adequacy and persuasiveness. -  A mixed-design crowdsourcing experiment revealed a poor correlation between quantitative evaluation metrics and human evaluations.  - This highlights the importance of human evaluation in assessing nuanced aspects of generated content like artificiality. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/dfurman/Llama-2-13B-Instruct-v0.2) |
| [Chimera: Improving Generalist Model with Domain-Specific Experts](https://arxiv.org/abs/2412.05983) | Renrui Zhang, Renqiu Xia, Hongbin Zhou, Mingsheng Li, Tianshuo Peng | - Chimera, a new multimodal pipeline, enhances Large Multi-modal Models (LMMs) with domain-specific experts to improve performance on specialized tasks like multimodal reasoning and visual content extraction. - The model integrates multiple expert encoders into a single LMM using a progressive training strategy and a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism to address optimization imbalances. - Chimera achieves state-of-the-art performance on MathVista and MathVerse, outperforming comparable-scale LMMs and specialized expert models. - The model also excels in visual structural extraction tasks for charts, tables, and documents, achieving near-specialist-level results on benchmarks like ChartQA-SE, Table-SE, and Doc-SE. - The authors plan to open-source Chimera and the training datasets to facilitate future research on LMMs. | ['Multimodal', 'Question Answering', 'Table Question Answering'] | N/A | N/A |


## Papers for 2024-12-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559) | Keming Lu, Beichen Zhang, Zhenru Zhang, RunjiLin, chujiezheng | - This paper introduces ProcessBench, a new benchmark for evaluating the ability of language models to identify erroneous steps in mathematical reasoning. - ProcessBench consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems, with each test case containing a step-by-step solution annotated with error locations by human experts. - Through extensive evaluation on ProcessBench, the authors found that existing Process Reward Models (PRMs) typically fail to generalize to more challenging math problems and underperform compared to critic models (prompted general language models). - The best open-source model, QwQ-32B-Preview, demonstrates competitive critique capability with the proprietary model GPT-40, but still lags behind the reasoning-specialized o1-mini. - This work aims to foster future research in reasoning process assessment and pave the way toward scalable oversight of language models. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/QwenLM/ProcessBench) | N/A |
| [Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models](https://arxiv.org/abs/2412.05939) | Wanxiang Che, Libo Qin, Yuxi Xie, Tianhao Niu, LooperXX | - This paper introduces MMGIC, a multimodal dataset featuring multi-grained concept annotations for Multimodal Large Language Models (MLLMs), including coarse-grained image captions, fine-grained object labels and regions, and label descriptions. - The authors propose a general MLLM framework and structured template to integrate these multi-grained annotations, facilitating vision-language alignment across different granularities. - Experiments demonstrate that MMGIC enhances MLLM performance in comprehension and generation tasks compared to training solely on image captions. - MMGIC and image-caption data complement each other; a curriculum learning strategy of pre-training on image captions and then MMGIC yields the best results, with gains of 3.95% and 2.34% on POPE and SEED-Bench, respectively.  - The study explores various data recipes for multi-grained annotations and their impact on MLLM performance, demonstrating their complementary nature. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/LooperXX/MMGiC) | N/A |
| [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769) | Zhiting Hu, Xian Li, DiJia Su, Sainbayar Sukhbaatar, Shibo Hao | - This paper introduces COCONUT (Chain of Continuous Thought), a novel paradigm for training large language models (LLMs) to reason in a continuous latent space, rather than the traditional language space used in chain-of-thought (CoT) prompting. - COCONUT utilizes the last hidden state of the LLM as a continuous representation of the reasoning state ("continuous thought") and feeds it directly back to the LLM as the next input embedding. - This latent reasoning approach allows the model to encode multiple potential next reasoning steps, enabling a breadth-first search (BFS) behavior and improved performance on logical reasoning tasks requiring backtracking compared to traditional CoT. - Experimental results on GSM8k, ProntoQA, and a newly proposed ProsQA dataset demonstrate the effectiveness of COCONUT, particularly in scenarios involving substantial planning and search. - The findings suggest that latent reasoning can be more efficient and adaptable for complex reasoning, offering insights into future research on LLM reasoning and problem-solving. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation](https://arxiv.org/abs/2412.04432) | Ying Shan, Yixiao Ge, Yizhuo Li, Yuying Ge | - This paper introduces Divot, a Diffusion-Powered Video Tokenizer, which uses a diffusion process for self-supervised video representation learning. - Divot is composed of a pre-trained Vision Transformer (ViT) encoder, a Spatial-Temporal transformer, and a Perceiver Resampler to get video representations. - It leverages a video diffusion model to predict the noise added to the VAE latents of video frames, conditioned on Divot’s features. - The authors also introduce Divot-LLM, integrating Divot with a pre-trained LLM, which achieves competitive performance in video comprehension and zero-shot video generation benchmarks. - Divot-LLM also excels in video storytelling, generating interleaved narratives and corresponding videos. | ['Text-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/TencentARC/Divot) | N/A |
| [Robust Multi-bit Text Watermark with LLM-based Paraphrasers](https://arxiv.org/abs/2412.03123) | Hang Li, Yang Liu, Yuanshun Yao, Jinghan Jia, xiaojunxu | - This paper introduces a novel method for embedding imperceptible multi-bit watermarks into text using LLM-based paraphrasers. - The method uses a pair of fine-tuned LLMs, one as the encoder to embed the watermark and another as the decoder to extract it. The encoder injects the watermark by alternatively paraphrasing sentences based on a binary code, while the decoder classifies each sentence to extract the embedded bits. - The method achieves high detection accuracy (over 99.99% AUC) with small LLMs (1.1B parameters) while maintaining the semantic similarity between the original and watermarked texts. - It also demonstrates robustness against word substitutions and sentence paraphrasing perturbations and generalizes well to out-of-distribution data. - The watermark's stealthiness is evaluated through both human and LLM-based analysis, showing it is difficult to distinguish between watermarked and non-watermarked text. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/xiaojunxu/multi-bit-text-watermark) | N/A |


## Papers for 2024-12-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271) | Yangzhou Liu, Yue Cao, Zhe Chen, qishisuren, Weiyun1025 | - This paper introduces InternVL 2.5, a series of advanced multimodal large language models (MLLMs) built upon InternVL 2.0, enhancing training and testing strategies and data quality. - InternVL 2.5 maintains the core "ViT-MLP-LLM" architecture, integrating an incrementally pre-trained InternViT-6B or InternViT-300M vision encoder with various large language models (LLMs) like InternLM 2.5 and Qwen 2.5. - The models demonstrate competitive performance, rivaling leading commercial models like GPT-40 and Claude-3.5-Sonnet, and achieving state-of-the-art results on benchmarks like MMMU and MathVista. - Key findings include the reduced dependency on training data with larger vision encoders, the impact of improved data quality, and the benefits of test-time scaling, especially with Chain-of-Thought (CoT) reasoning. - InternVL 2.5 is released open-source, aiming to push the boundaries of open-source multimodal models and facilitate further research in multimodal AI. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Video-Text-to-Text'] | [Link](https://github.com/OpenGVLab/InternVL) | [Link](//huggingface.co/OpenGVLab/InternVL2_5-78B), [Link](https://huggingface.co/spaces/OpenGVLab/InternVL) |
| [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/abs/2412.05237) | Yuelin Bai, Tuney Zheng, Jarvis Guo, yuexiang96, luodian | The paper introduces MAmmoTH-VL, a multimodal large language model (MLLM) trained on a newly created dataset containing 12 million instruction-response pairs.  The dataset was constructed using a cost-effective method employing open-source models.  MAmmoTH-VL-8B, based on the LLaVA-OneVision architecture, outperforms existing open-source models on various benchmarks, particularly those involving intricate reasoning. The model shows state-of-the-art performance on MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Ablation studies reveal the effectiveness of key components such as rewriting and self-filtering. | ['Multimodal'] | [Link](https://mammoth-vl.github.io) | N/A |
| [EXAONE 3.5: Series of Large Language Models for Real-world Use Cases](https://arxiv.org/abs/2412.04862) | Kyunghoon Bae, Soyoung An, LG AI Research, lhg912, Sunkyoung | The paper introduces EXAONE 3.5, a series of instruction-tuned language models available in three sizes (32B, 7.8B, and 2.4B).  The models demonstrate strong instruction-following capabilities and achieve high performance in various benchmarks, particularly in real-world scenarios and long-context understanding.  EXAONE 3.5 models outperform many similar-sized models on benchmark datasets. The models are open for research purposes and can be downloaded from HuggingFace. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/LGAI-EXAONE) |
| [Moto: Latent Motion Token as the Bridging Language for Robot Manipulation](https://arxiv.org/abs/2412.04445) | Mingyu Ding, Yixiao Ge, Yizhuo Li, Yuying Ge, Yi Chen | - Moto is a new robotics model that leverages latent motion tokens as a bridging language for autoregressive pre-training on video data and robot manipulation. - Moto-GPT is pre-trained through next motion token prediction, learning motion-related prior knowledge from videos and transferring this knowledge to downstream tasks via fine-tuning. - Moto-GPT is a transformer-based architecture (similar to GPT) with a motion tokenizer that encodes motion between frames and action query tokens for robot control prediction during fine-tuning. - Experimental results demonstrate that Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks compared to various baseline models. - Moto-GPT performs particularly well in low-resource scenarios, highlighting its effective transfer of learned knowledge from video data to downstream tasks. | ['Robotics', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://chenyi99.github.io/moto/) | N/A |
| [APOLLO: SGD-like Memory, AdamW-level Performance](https://arxiv.org/abs/2412.05270) | Sem Park, Xi Liu, Wenyan Cong, Hanqing Zhu, Kyriection | - APOLLO, a new memory-efficient optimizer for Large Language Models (LLMs), is proposed, offering SGD-level memory cost while maintaining or exceeding AdamW's performance. - It leverages structured learning rate updates (channel-wise or tensor-wise) and approximates them in a low-rank auxiliary space using random projections, eliminating the need for costly SVD operations. - APOLLO-Mini, an extremely memory-efficient variant, utilizes tensor-wise scaling with a rank-1 auxiliary space, achieving similar performance to AdamW with drastically reduced memory usage. - Experimental results on various LLaMA model sizes (60M to 7B) show APOLLO consistently outperforms AdamW and other memory-efficient methods in pre-training, even achieving a 2.8 reduction in validation perplexity with significantly lower memory overhead. - APOLLO also offers practical system-level advantages including enhanced throughput (3x on LLaMA 7B compared to AdamW) and improved model scalability, enabling LLaMA-13B pre-training with naive DDP on a single A100-80GB GPU and LLaMA-7B training on a single GPU with less than 12GB memory when combined with quantization. | ['Natural Language Processing'] | N/A | N/A |
| [GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration](https://arxiv.org/abs/2412.04440) | Yu Wang, Xuefei Ning, Yukun Huang, fjxmlzn, NinaKarine | - GENMAC, an iterative multi-agent framework, is proposed for compositional text-to-video generation. - It uses a three-stage collaborative workflow: DESIGN, GENERATION, and REDESIGN, with an iterative loop between GENERATION and REDESIGN for refinement. - The REDESIGN stage uses four sequentially executed MLLM-based agents: verification, suggestion, correction, and output structuring. - A self-routing mechanism adaptively selects the appropriate correction agent from a collection of specialized agents. - Experiments on T2V-CompBench show state-of-the-art performance across seven compositional aspects, significantly outperforming 17 existing methods, with notable improvement in generative numeracy. | ['Text-to-Video', 'Multimodal'] | N/A | [Link](https://karine-h.github.io/GenMAC/) |
| [DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling](https://arxiv.org/abs/2412.04905) | Haiyang Yu, Nan Xu, Kun Chen, Xinghua Zhang, iiiiwis | - This paper introduces Dialogue Element Modeling (DEMO), a new research task focusing on two core competencies: Element Awareness and Dialogue Agent Interaction. - Element Awareness involves reverse-engineering dialogue elements like goal, persona, and scene, while Dialogue Agent Interaction focuses on goal-directed multi-turn dialogue modeling. - A novel benchmark, DEMO, is proposed to facilitate comprehensive dialogue modeling and assessment in both English and Chinese, covering various dialogue elements and tasks. - A DEMO agent, trained using imitation learning and expert experience, demonstrates superior performance in both in-domain and out-of-domain tasks, exceeding several existing LLMs. - Experimental results show that current LLMs have room for improvement in dialogue element modeling, especially in feature perception tasks. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/MozerWang/DEMO) | N/A |


## Papers for 2024-12-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://arxiv.org/abs/2412.04454) | tianbaoxiexxx, ludunjie, ZeonLap, kugwzk, ranpox | - AGUVIS, a unified pure vision-based framework, is introduced for building generalizable GUI agents that operate with vision-based observations and a plugin-enabled action system, enhancing cross-platform adaptability. - A two-stage training process is employed: first for GUI grounding, followed by planning and reasoning. - The model leverages vision-based grounding to improve generalization and reduce inference costs while employing a standardized action space with a plugin system to facilitate consistent learning. - Through experiments, AGUVIS surpasses previous state-of-the-art methods on benchmarks like ScreenSpot, Multimodal-Mind2Web, and AndroidControl, achieving the first fully autonomous pure vision GUI agent capable of performing tasks independently. - This model demonstrates its efficiency by considerably reducing USD costs and input tokens compared to GPT-40 on Mind2Web-Live. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679) | Seongyun Lee, Vijay Viswanathan, Xiang Yue, Juyoung Suk, seungone | - This paper introduces AGORABENCH, a benchmark for evaluating the effectiveness of large language models (LLMs) as synthetic data generators for training other LMs. - The benchmark uses standardized settings and a new metric, Performance Gap Recovered (PGR), to compare the quality of synthetic data generated by different LLMs across various tasks and data generation methods. - The study finds that an LLM's ability to generate high-quality training data does not necessarily correlate with its problem-solving abilities, but rather with intrinsic properties of the data such as response quality, perplexity, and instruction difficulty. - Strategic choices like output format and cost-conscious model selection can significantly impact the effectiveness of data generation, with generating larger datasets from cheaper models sometimes outperforming smaller datasets from more expensive models. - The code and data for AGORABENCH are publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/neulab/data-agora) | N/A |
| [Densing Law of LLMs](https://arxiv.org/abs/2412.04315) | Xu Han, Guoyang Zeng, Weilin Zhao, Jie Cai, xcjthu | - This paper introduces "capacity density" to evaluate the training quality of Large Language Models (LLMs) across different scales, considering both effectiveness and efficiency. - Capacity density is calculated as the ratio of a model's effective parameter size (the size a reference model would need to achieve equivalent performance) to its actual parameter size. - The paper proposes a two-step process to predict downstream task performance: 1) estimate the relationship between parameter size and language modeling loss and 2) estimate the relationship between loss and downstream task performance using a sigmoid function. - An empirical law, the "Densing Law," is revealed, showing that the maximum capacity density of open-source base LLMs exhibits exponential growth, doubling approximately every three months. - This trend suggests that future LLM development should prioritize improving capacity density rather than solely increasing parameter size, enabling optimal performance with minimal computational overhead. | ['Natural Language Processing'] | N/A | N/A |
| [Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion](https://arxiv.org/abs/2412.04424) | Dianqi Li, Haiping Wu, Jianwei Yang, Jiuhai Chen, zhoutianyi | - Florence-VL, a new family of Multimodal Large Language Models (MLLMs), leverages the generative vision foundation model Florence-2 as its visual encoder, enabling it to capture diverse visual features at different levels of detail and under various prompts. - A novel Depth-Breadth Fusion (DBFusion) mechanism concatenates visual features from different layers (depth) and under multiple prompts (breadth), providing a rich visual representation to the language model. - This model is trained with a two-stage process: end-to-end pretraining on a large image captioning dataset followed by fine-tuning on a diverse set of instruction-tuning datasets. - Quantitative analysis and visualization demonstrate improved vision-language alignment compared to models using CLIP or SigLIP encoders.  - Florence-VL achieves state-of-the-art results across 25 multimodal and vision-centric benchmarks, including VQA, OCR, Chart understanding, and knowledge-based reasoning tasks, outperforming other advanced MLLMs like Cambrian. | ['Multimodal', 'Visual Question Answering', 'Image Feature Extraction'] | [Link](https://github.com/JiuhaiChen/Florence-VL) | N/A |
| [Towards Universal Soccer Video Understanding](https://arxiv.org/abs/2412.01820) | Yanfeng Wang, Ya Zhang, Hao Jiang, haoningwu, Homie0609 | - This paper introduces MatchVision, a novel visual-language foundation model designed for comprehensive soccer video understanding. - The model leverages a spatiotemporal attention mechanism inspired by TimeSformer, which allows it to effectively capture dynamic information within soccer videos. - MatchVision is trained on SoccerReplay-1988, a new dataset containing 1988 full soccer matches with rich annotations, alongside existing datasets like SoccerNet, that is significantly larger and more diverse. - The paper benchmarks MatchVision on various downstream tasks such as event classification, commentary generation, and foul recognition. - Experimental results demonstrate state-of-the-art performance across multiple benchmarks, highlighting the model's effectiveness and the value of the new dataset. | ['Computer Vision', 'Video Classification', 'Text Generation', 'Multimodal'] | [Link](https://jyrao.github.io/UniSoccer/) | N/A |
| [Personalized Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2412.02142) | Zhehao Zhang, Yu Xia, Hanjia Lyu, Junda Wu, Franck-Dernoncourt | - This paper presents a comprehensive survey of personalized multimodal large language models (MLLMs), examining their architectures, training methods, and applications. - The authors propose a taxonomy for categorizing techniques used to personalize MLLMs, focusing on instruction, alignment, generation, and fine-tuning methods. - The survey also summarizes applications of personalized MLLMs, including text generation, image generation, recommendation, and retrieval tasks. - They also present an overview of commonly used datasets and evaluation metrics for personalized MLLMs. - Finally, the authors highlight key open challenges in the field, including benchmarking, evaluation metrics, diverse modalities, modality fusion, and theoretical foundations. | ['Multimodal', 'Text Generation', 'Image-to-Text', 'Image-to-Image', 'Text2Text Generation', 'Summarization'] | N/A | N/A |
| [Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation](https://arxiv.org/abs/2412.03304) | Jian Gang Ngui, David I. Adelani, Clémentine Fourrier, Angelika Romanou, Shivalika Singh | - This paper introduces Global-MMLU, a 42-language multilingual multi-domain question-answering dataset designed to address cultural and linguistic biases in MMLU. - It includes culturally sensitive (CS) and culturally agnostic (CA) subsets, allowing for more nuanced evaluations. - The creation involved professional and community annotators for translation and post-editing, expanding language coverage and improving translation quality. - The paper also quantifies the impact of cultural biases, with analysis revealing that 28% of MMLU questions require culturally specific knowledge and a disproportionate focus on Western cultures. - State-of-the-art model evaluations on Global-MMLU highlight the impact of these biases on performance rankings, advocating for reporting performance on CS and CA subsets separately to provide a more comprehensive understanding of model capabilities across cultures. | ['Question Answering', 'Translation', 'Natural Language Processing'] | N/A | [Link](https://hf.co/datasets/CohereForAI/Global-MMLU) |
| [Monet: Mixture of Monosemantic Experts for Transformers](https://arxiv.org/abs/2412.04139) | Jaewoo Kang, Kee-Eung Kim, Young Jin Ahn, affjljoo3581 |  - This paper introduces MONET, a novel Mixture-of-Experts (MoE) architecture designed to enhance the mechanistic interpretability of large language models (LLMs) by addressing the issue of polysemanticity. - MONET incorporates sparse dictionary learning directly into end-to-end MoE pretraining, enabling the scaling of the expert count to 262,144 per layer while maintaining parameter efficiency. - The model's performance is evaluated across various benchmarks, showing competitive results with dense LLMs while offering superior knowledge manipulation capabilities. - Through qualitative and quantitative analyses, MONET demonstrates mutual exclusivity of knowledge across experts, enabling robust knowledge manipulation without performance degradation. - The code and pretrained checkpoints for MONET are publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/dmis-lab/Monet) | N/A |
| [OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows](https://arxiv.org/abs/2412.01169) | Yusuke Kato, Zichun Liao, Akash Gokul, Konstantinos Kallidromitis, Shufan Li | - OmniFlow is a novel generative model designed for any-to-any generation tasks, using a modular architecture inspired by Stable Diffusion 3's MMDiT. - It extends the rectified flow (RF) framework to handle multiple modalities (text, image, audio) jointly and introduces a novel guidance mechanism for flexible control over modality interaction in generated outputs. - OmniFlow outperforms previous any-to-any models on various tasks, achieving competitive performance with state-of-the-art specialist models in text-to-image and text-to-audio generation. - Its modular design allows individual component pretraining and merging with pretrained single-task models, reducing training resource requirements compared to training from scratch. - Evaluations show significant improvements over existing any-to-any models in image quality, text-image alignment, and CLIP scores, particularly on the GenEval benchmark. | ['Any-to-Any', 'Multimodal', 'Text-to-Image', 'Text-to-Audio'] | [Link](https://github.com/jacklishufan/OmniFlows) | N/A |
| [Discriminative Fine-tuning of LVLMs](https://arxiv.org/abs/2412.04378) | Ioannis Maniadis Metaxas, Anestis Zaganidis, Alexandros Xenos, Adrian Bulat, Yassine Ouali | - This paper introduces VladVA, a novel training approach for discriminative fine-tuning of Large Vision-Language Models (LVLMs). - VladVA converts a generative LVLM into a discriminative one by employing both contrastive and next-token prediction losses on image-text pairs with varying lengths and granularities. - The approach uses a parameter-efficient adaptation method that involves soft prompting and LoRA, thereby allowing for effective training on smaller datasets with limited compute. - On standard image-text retrieval benchmarks, VladVA shows significant improvement, achieving gains from +4.7% to +7.0% in absolute terms over similarly sized state-of-the-art CLIP-like models. - Additionally, the model demonstrates notable gains in compositionality tasks, showing improved language understanding over the standard two-tower image-text models. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Zero-Shot Image Classification'] | N/A | N/A |
| [KV Shifting Attention Enhances Language Modeling](https://arxiv.org/abs/2411.19574) | Weipeng Chen, Bingning Wang, Wei Cheng, xumingyu16 | - This paper introduces KV shifting attention, a novel attention mechanism designed to improve the efficiency of induction heads in large language models (LLMs). - KV shifting attention decouples keys and values in the attention mechanism, reducing the depth and width requirements for induction heads, enabling single-layer transformers to perform induction tasks effectively. - Theoretical analysis and empirical validation demonstrate that KV Shifting attention achieves comparable or superior performance to conventional multi-layer transformers in language modeling tasks. - The authors apply KV shifting attention to large language pre-training models with up to 19B parameters and show improved performance and faster convergence compared to baseline models using standard attention mechanisms. - KV shifting attention introduces a bias towards learning induction, which is beneficial for language modeling across diverse model scales. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/xumingyu16/Baseline_2.9B), [Link](https://huggingface.co/xumingyu16/KV_shifting_2.9B) |


## Papers for 2024-12-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation](https://arxiv.org/abs/2412.03069) | sweetrabor, gaozong, xuwang, liqingzju, leo1117 | - Introduces TokenFlow, a novel unified image tokenizer with a dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining alignment via shared index mapping, bridging the gap between multimodal understanding and generation. - Demonstrates state-of-the-art autoregressive image generation with a GenEval score of 0.55 at 256x256 resolution and strong reconstruction performance (FID 0.63 at 384x384), surpassing methods like EMU3 and LlamaGen with fewer sampling steps.  - Achieves a new state-of-the-art in multimodal understanding, surpassing LLaVA-1.5 13B by 7.2% on average by leveraging the Qwen-2.5-14B language model.  - Shows that discrete visual input can outperform continuous visual baselines for the first time on understanding tasks.  - Maintains high codebook utilization (95%+) even with large codebooks (over 130K), exceeding prior approaches in capacity and efficiency. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](https://arxiv.org/abs/2412.00493) | asdfg80, slvjul, zd11024 | - This paper introduces Video-3D LLM, a novel generalist model for 3D scene understanding. - The model leverages a Video LLM framework, processing video frames augmented with corresponding 3D spatial coordinates obtained from depth images. - It enhances 3D scene understanding by creating position-aware video representations through the integration of 3D position encodings derived from spatial coordinates. - A maximum coverage sampling technique optimizes the balance between computational cost and performance. - The model achieves state-of-the-art performance on benchmarks like ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D, outperforming LLaVA-3D while using only 26% of its 3D data. | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/LaVi-Lab/Video-3D-LLM) | N/A |
| [VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models](https://arxiv.org/abs/2411.19103) | SunYoung Park, Daeyoung Kim, kimyoungjune, hojunssss | - This paper introduces VARCO-VISION-14B, a bilingual (Korean-English) vision-language model based on Qwen-2.5-14B-Instruct as its language model and SigLIP as its vision encoder, trained using a four-stage process involving feature alignment, supervised fine-tuning, and preference optimization. - The model outperforms similarly sized open-source models on Korean multimodal benchmarks and achieves comparable performance to larger proprietary models, demonstrating strong bilingual capabilities. - Five Korean evaluation datasets are released alongside the model, including four closed-set (K-MMBench, K-SEED, K-MMStar, K-DTCBench) and one open-set (K-LLaVA-W) benchmarks, translated and validated from established English benchmarks to assess bilingual proficiency and document, table and chart understanding. - VARCO-VISION exhibits proficient grounding, referring, and OCR capabilities in both languages. - The authors aim to promote open research in Korean VLMs with this release and encourage further development of bilingual multimodal models. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Document Question Answering'] | N/A | [Link](https://huggingface.co/NCSOFT/VARCO-VISION-14B), [Link](https://huggingface.co/datasets/NCSOFT/K-MMBench), [Link](https://huggingface.co/datasets/NCSOFT/K-SEED), [Link](https://huggingface.co/datasets/NCSOFT/K-MMStar), [Link](https://huggingface.co/datasets/NCSOFT/K-DTCBench), [Link](https://huggingface.co/datasets/NCSOFT/K-LLAVA-W) |
| [Mimir: Improving Video Diffusion Models for Precise Text Understanding](https://arxiv.org/abs/2412.03085) | Dandan Zheng, Kecheng Zheng, Yutong Feng, Shuai Tan, BiaoGong | - Mimir is a novel text-to-video generation framework that integrates large language models (LLMs) within a diffusion model for enhanced text comprehension. - It employs a "token fuser" to combine features from both text encoders (like T5) and decoder-only LLMs (like Phi-3.5), addressing the distribution gap between these models. - This design allows Mimir to leverage existing video priors in diffusion models while capitalizing on the enhanced reasoning and precise understanding of LLMs. - Quantitative and qualitative evaluations on VBench demonstrate Mimir's superior performance, particularly in handling multiple objects, spatial relationships, and short, descriptive prompts. - A user study further confirms Mimir's improved capabilities in instruction following, physics simulation, and overall visual quality compared to existing state-of-the-art models. | ['Text-to-Video', 'Multimodal'] | N/A | [Link](https://lucaria-academy.github.io/Mimir/) |
| [Weighted-Reward Preference Optimization for Implicit Model Fusion](https://arxiv.org/abs/2412.03187) | Xiaojun Quan, Tianyuan Shi, Longguang Zhong, Fanqi Wan, Ziyi Yang | - This paper introduces Weighted-Reward Preference Optimization (WRPO), a novel implicit model fusion method for enhancing the capabilities of a Large Language Model (LLM) by leveraging preference optimization between source LLMs and a target LLM. - WRPO eliminates the need for vocabulary alignment and matrix fusion, enabling efficient scaling to accommodate diverse LLMs and mitigating distributional deviations through a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. - Experiments conducted on MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrated that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. - Using LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a 46.2% win rate against GPT-4-0314 on Arena-Hard, showcasing significant performance improvements. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SLIT-AI/WRPO) | N/A |


## Papers for 2024-12-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability](https://arxiv.org/abs/2411.19943) | zptu, Thu-redrobot, SihengLi, Chufan, Jiahao004 | - This paper introduces cDPO, a novel token-level contrastive estimation and preference optimization framework designed to enhance the reasoning capabilities of Large Language Models (LLMs). - cDPO identifies "critical tokens" within incorrect reasoning trajectories by comparing the generation likelihood from positive and negative models fine-tuned on correct and incorrect reasoning trajectories, respectively. - It then leverages these contrastive likelihoods as token-level rewards during preference optimization, thereby guiding the model to avoid generating critical tokens that lead to erroneous outcomes. - Experimental results on GSM8K and MATH500 benchmarks demonstrate that cDPO significantly outperforms existing example-level and step-level baseline strategies (p < 0.005) across various LLMs, including Llama-3 (8B and 70B) and DeepSeek-math (7B), achieving average accuracies of 77.2% and 33.4% respectively. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | N/A | N/A |
| [Free Process Rewards without Process Labels](https://arxiv.org/abs/2412.01981) | iseesaw, stingning, ganqu, wendili, lievan | - This paper introduces implicit Process Reward Models (PRMs), which can be derived from Outcome Reward Models (ORMs) trained on response-level labels without needing expensive step-level annotations. - By parameterizing the outcome reward as the log-likelihood ratio of policy and reference language models, a PRM can be automatically learned during ORM training, significantly reducing the data collection and training costs. - Experiments on MATH demonstrate that the implicit PRM outperforms a strong MCTS-based baseline, Math-Shepherd, with less than 1/38 of the training data and achieves state-of-the-art performance compared to open-source reward models. - Further analysis shows scaling data and using majority voting improves performance, but incorporating step labels during training provides no gains. - The paper suggests that the reference model can even be omitted for models pre-trained with preference learning without harming the performance, increasing the inference efficiency. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/lifan-yuan/ImplicitPRM) | N/A |
| [LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences](https://arxiv.org/abs/2412.01292) | Sunxy111, Xiaomabufei, senfu, PeihaoChen, Hoyard | - LSceneLLM, an adaptive framework for enhancing large 3D scene understanding, is introduced, which addresses the challenges of accurately locating task-relevant visual information within high-density point clouds. - It employs a scene magnifier module with a dense token selector and an adaptive self-attention mechanism to dynamically identify task-relevant areas, guided by the LLM's visual preferences, and extract and fuse detailed information from these regions. - A new cross-room understanding benchmark, XR-Scene, featuring XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption tasks, is also presented for comprehensive evaluation of large 3D scene understanding. - Experimental results demonstrate LSceneLLM's state-of-the-art performance on various 3D tasks and benchmarks, including both indoor and outdoor large-scene understanding, as well as existing single-room scene understanding benchmarks. - Integrating the scene magnifier module with existing 3D-VLMs leads to significant performance improvements. | ['Computer Vision', 'Visual Question Answering', 'Multimodal', 'Robotics'] | N/A | N/A |
| [OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation](https://arxiv.org/abs/2412.02592) | zichenwen, ouyanglinke, binwang, qintong21, Carkham | - This paper introduces OHRBench, a new benchmark designed to evaluate the cascading impact of Optical Character Recognition (OCR) on Retrieval-Augmented Generation (RAG) systems. - OHRBench includes a diverse dataset of PDF documents from six real-world applications, along with questions based on multimodal elements, as well as a set of perturbed structured data designed to explore the fine-grained effect of two identified primary types of OCR noise on RAG systems: Semantic Noise and Formatting Noise. - Through comprehensive evaluation with existing OCR solutions, results demonstrate that none of the extracted structured data from these solutions are competent for constructing high-quality knowledge bases for RAG, as they all suffer performance losses of at least 7.5%. - Further analysis reveals that all retrievers and large language models are significantly affected by Semantic Noise, especially in tables and formulas, while Formatting Noise affects specific retrievers and large language models differently. - Finally, through experiments with Vision-Language Models in the generation stage, results show that combining image and OCR text as input can improve performance significantly, demonstrating potential for further research on integrating VLMs in RAG. | ['Document Question Answering', 'Multimodal'] | [Link](https://github.com/opendatalab/OHR-Bench) | N/A |
| [MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation](https://arxiv.org/abs/2411.19067) | Dongyoon Han, Song Park, Seungho Lee, Minhyun Lee, bhheo | - This paper introduces MaskRIS, a novel training framework for Referring Image Segmentation (RIS) that leverages image and text masking alongside Distortion-aware Contextual Learning (DCL). - MaskRIS addresses the limitations of conventional data augmentation techniques in RIS by mitigating semantic conflicts and enhancing data diversity. - This framework consists of a primary path that processes original inputs for training stability, and a secondary path that processes masked inputs to enhance model robustness. - MaskRIS achieves state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg, demonstrating significant improvements in oIoU scores compared to existing methods. - For example, MaskRIS improves oIoU by 1.82%, 1.33%, and 2.25% on RefCOCO validation, testA, and testB, respectively, over CARIS. | ['Multimodal', 'Image Segmentation', 'Computer Vision'] | [Link](https://github.com/naver-ai/maskris) | N/A |
| [A dynamic parallel method for performance optimization on hybrid CPUs](https://arxiv.org/abs/2411.19542) | Liu Yucheng, Luo Yu, Haihao | - This paper introduces a dynamic parallel method for optimizing Large Language Model (LLM) inference performance on hybrid CPUs, which addresses the issue of imbalanced hardware capabilities among different cores. - The method dynamically balances the workload for each core before parallel processing begins, leading to significant performance improvements. - It integrates this new parallel method into Neural Speed, an optimized x86 assembly code framework based on llama.cpp. - The results demonstrate over 90% average memory bandwidth utilization on two hybrid Intel CPUs during 4-bit LLM inference, a 20%-30% improvement over the original OpenMP method in Neural Speed, and up to a 3.7x speedup compared to llama.cpp. - This dynamic approach adapts to varying system conditions and maximizes CPU performance by dynamically adjusting kernel workload distribution based on real-time performance ratios. | ['Natural Language Processing'] | N/A | N/A |
| [VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval](https://arxiv.org/abs/2412.01558) | Nabeel Mohammed, Md Rizwan Parvez, shafin5, dpaul06 | - VideoLights is a novel framework for joint Video Highlight Detection (HD) and Moment Retrieval (MR) using a Bi-Directional Cross-Modal Fusion (Bi-CMF) Network within a transformer architecture. - The model incorporates a Feature Refinement and Alignment (FRA) Module to refine visual features and align them with textual features at local and global levels, and a Unidirectional Joint-Task Feedback Mechanism (Uni-JFM) to enhance task correlation. - VideoLights leverages features from Large Vision-Language Models (LVLMs) like BLIP-2, CLIP, and SlowFast, and employs intelligent model pre-training with synthetic data generated by LVLMs.  - Adaptive hard positive/negative loss functions are utilized for adaptive error penalization and improved learning. - The model achieves state-of-the-art performance on QVHighlights, TVSum, and Charades-STA benchmarks, outperforming existing methods by significant margins and improving MR metrics such as R@0.5 by up to 6.81% and HD metrics such as mAP by up to 6.9%. | ['Multimodal', 'Video-Text-to-Text', 'Video Classification'] | [Link](https://github.com/dpaul06/VideoLights) | N/A |


## Papers for 2024-12-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models](https://arxiv.org/abs/2412.01824) | lindahua, TheYJ, yuhangzang, tongwu2020, Zery | - X-Prompt, an auto-regressive large vision-language model, is introduced for in-context image generation across various tasks. - It uses a novel design to compress features from in-context examples, enabling longer context sequences and better generalization to unseen tasks. - A unified training objective for text and image prediction allows the model to leverage task awareness from context. - Experimental results show competitive performance on several image generation tasks, including text-to-image generation, and improved in-context learning capabilities on novel tasks compared to baselines like OmniGen. - The method also exhibits strong capabilities in the image editing task by using Retrieval-Augmented Image Editing (RAIE), where a relevant image example is retrieved as in-context information to enhance editing performance. | ['Text-to-Image', 'Image-to-Image', 'Image Segmentation', 'Depth Estimation', 'Multimodal'] | [Link](https://github.com/SunzeY/X-Prompt) | N/A |
| [GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation](https://arxiv.org/abs/2411.18499) | LiruiZhao, yefly, xuzhaopan, xiaopengpeng, lyuukuu | - Introduces GATE OpenING (OpenING), a benchmark for evaluating open-ended interleaved image-text generation, comprised of 5,400 human-annotated instances across 56 real-world tasks and 23 meta-topics. - Presents IntJudge, a novel judging model trained with a Reference-Augmented Generation (RAG) approach and an Interleaved Arena for data annotation, achieving 82.42% agreement with human judgments, outperforming GPT-4 by 11.34%. - Demonstrates through experiments on OpenING that integrated pipelines for interleaved generation outperform end-to-end models, highlighting the potential of two-stage generators with unified architectures. - Reveals that generating high-quality, coherent interleaved content remains challenging for existing models, while GPT-generated text often surpasses human quality, and human-annotated images are preferred over generated ones. - Provides a comprehensive leaderboard and analysis of various interleaved generation methods, offering insights for future model development and benchmark design. | ['Multimodal', 'Image-to-Text', 'Text-to-Image'] | N/A | N/A |
| [o1-Coder: an o1 Replication for Coding](https://arxiv.org/abs/2412.00154) | Jinlin Xiao, Jiangming Shu, Yuqi Yang, Shangxi Wu, Yuxiang Zhang | - This paper introduces O1-CODER, a framework attempting to replicate OpenAI's O1 model, focusing on coding tasks and enhancing System-2 thinking through Reinforcement Learning (RL) and Monte Carlo Tree Search (MCTS). - The framework incorporates a Test Case Generator (TCG) for automated code evaluation, MCTS for generating reasoning data, and iterative fine-tuning of a policy model, initially producing pseudocode and subsequently full code. - O1-CODER uses pseudocode-based prompting and behavioral actions, addressing the challenges of self-play RL in code generation, including evaluation and process reward design. - While initial experiments on the MBPP benchmark show a slight decrease in overall pass rate with pseudocode, a significant improvement in the Average Sampling Pass Rate suggests enhanced reasoning capabilities when the generated pseudocode is correct. - The paper further discusses the broader implications of moving beyond human-recorded data, the potential of self-play+RL in complex problem solving, and the challenges in applying O1-like models to real-world applications requiring dynamic environment interaction. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/ADaM-BJTU/O1-CODER) | N/A |
| [VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models](https://arxiv.org/abs/2412.01822) | Yueh-Hua Wu, Yong Man Ro, Yu-Chiang Frank Wang, Ryo Hachiuma, BK-Lee | - Introduces VLSI, a new Vision Language Model (VLM) family (2B and 7B parameter sizes) that uses a novel natural language-based distillation process called "Verbalized Layers-to-Interactions" to transfer knowledge from large to small VLMs. - Employs layer-wise distillation with intermediate "verbalizers" to project features into natural language, which allows smaller VLMs to better align with the reasoning processes of larger VLMs, unlike traditional methods that focus solely on final-layer imitation. - Achieves notable performance improvements over GPT-4V (11.0% for 2B and 17.4% for 7B model sizes) on various vision-language benchmarks without increasing model size, module merging, or architectural modifications. - Validated across ten diverse benchmarks, demonstrating state-of-the-art performance and improved efficiency, especially for deployment on resource-constrained devices. - Offers an easily implementable and adaptable approach across different model architectures, showing significant gains with both Qwen2-VL and LLaVA-OV backbones. | ['Multimodal'] | N/A | N/A |
| [SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters](https://arxiv.org/abs/2412.00174) | Huaizhong Zhang, Zhengyu Lin, Weiye Xiao, Jianping Jiang, caizhongang | - SOLAMI is a novel end-to-end social Vision-Language-Action (VLA) model for generating multimodal responses (speech and motion) in interactions with 3D autonomous characters. - The model uses separate tokenizers for speech and motion, converting them into discrete tokens that are fed into a decoder-only LLM backbone (AnyGPT-base, based on LLaMA2-7B). - The model is trained in three stages: tokenizer training, multi-task pre-training for modality alignment (motion-text and speech-text), and instruction tuning on a synthetic multimodal social interaction dataset called SynMSI. - Quantitative results on SynMSI show that SOLAMI outperforms baseline methods (LLM+Speech, AnyGPT fine-tuned, and DLP) in terms of motion quality and inference latency, generating more natural and coherent responses. - A user study conducted with a VR interface further validates SOLAMI's superior performance, demonstrating enhanced user experience across metrics such as motion coherence, motion interaction, speech consistency, and overall experience. | ['Multimodal', 'Text-to-Speech', 'Text-to-Video', 'Text-to-3D', 'Robotics'] | [Link](https://solami-ai.github.io/) | N/A |
| [Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation](https://arxiv.org/abs/2412.01316) | Yuan Zhou, Qiuyue Wang, Yuxuan Cai, hyang0511, Cakeyan | - Presto, a novel video diffusion model, generates 15-second videos with long-range coherence and rich content using a Segmented Cross-Attention (SCA) strategy. - SCA divides hidden states into temporal segments, allowing each to cross-attend to a corresponding sub-caption, enhancing coherence without additional parameters. - The LongTake-HD dataset, comprising 261k content-rich videos with progressive sub-captions, facilitates high-quality long video generation. - Presto achieves 78.5% on VBench Semantic Score and 100% on Dynamic Degree, outperforming state-of-the-art methods in content richness and coherence. - A user study confirms Presto's superiority in scenario diversity, coherence, and text-video alignment compared to open-source and commercial alternatives. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input](https://arxiv.org/abs/2412.01250) | Alessandro Farinelli, Alberto Castellini, Gianni Franchi, e-zorzi, ftaioli | - Introduces Collaborative Instance Navigation (CoIN), a new task for embodied agents involving interactive dialogue with humans to locate target objects in unknown environments. - Presents AIUTA, a training-free method leveraging Vision-Language Models (VLMs) and Large Language Models (LLMs) to facilitate agent self-dialogue, reducing reliance on full initial descriptions. - Includes a novel Normalized-Entropy based technique to estimate and mitigate VLM uncertainty during object description generation. - Introduces CoIN-Bench, a new benchmark with real and simulated human evaluations for CoIN, and demonstrates AIUTA’s state-of-the-art performance on zero-shot instance navigation, handling user input flexibility. - Proposes IDKVQA, a dedicated dataset for evaluating VLM uncertainty estimation and shows the superiority of their approach. | ['Robotics', 'Computer Vision', 'Question Answering'] | N/A | [Link](https://intelligolabs.github.io/COIN) |
| [VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939) | Jing Shao, Xuanjing Huang, LLLeo612, Max9803, Foreshhh | - This paper introduces VLSBench, a new multimodal visual leakless safety benchmark with 2.4k image-text pairs designed to address the Visual Safety Information Leakage (VSIL) problem in existing multimodal safety benchmarks. - VSIL occurs when sensitive image content is revealed in the text query, allowing models to bypass visual processing and make safety decisions based on text alone.  - VLSBench pairs images with neutral text queries, forcing models to rely on visual understanding for safety assessments.  - Experimental results show that VLSBench is challenging for both open-source and closed-source Multimodal Large Language Models (MLLMs), with even the best performing model only achieving a 49.78% safety rate.  - The study also found that multimodal alignment methods outperform textual alignment on VLSBench, highlighting the importance of multimodal reasoning for visual safety in the absence of VSIL. | ['Multimodal'] | N/A | N/A |
| [INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge](https://arxiv.org/abs/2411.19799) | atcbosselut, jjzha, jebish7, shayekh, angelika | - This paper introduces INCLUDE, a multilingual benchmark dataset designed to evaluate the regional knowledge understanding of large language models (LLMs). - INCLUDE consists of 197,243 multiple-choice questions across 44 languages and 15 scripts, collected from various sources, including academic exams, professional certifications, and regional licenses. - The benchmark is designed to address the lack of high-quality evaluation resources in languages other than English and to capture cultural nuances associated with each language. - Experimental results demonstrate that current LLMs achieve high variance in performance between different languages and often struggle with questions requiring regional knowledge. - Analysis suggests that performance limitations stem from model's grasp of specialized regional knowledge for different languages. | ['Question Answering', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/CohereForAI/include-base-44) |
| [VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information](https://arxiv.org/abs/2412.00947) | Rui Zhang, Ranran Haoran Zhang, Sarkar Snigdha Sarathi Das, Yusen Zhang, ryokamoi | - This paper introduces VisOnlyQA, a new dataset designed to evaluate the visual perception capabilities of Large Vision Language Models (LVLMs) on questions related to geometric and numerical information in scientific figures. - VisOnlyQA includes 1,200 multiple-choice questions across 12 tasks and four categories of figures, along with 70k synthetic training instances. - Experiments with 20 LVLMs, including GPT-40 and Gemini 1.5 Pro, reveal poor performance on VisOnlyQA compared to near-perfect human performance. - Fine-tuning on synthetic data shows potential but limited improvement, suggesting both training data and model architecture need improvement. - The authors observed that stronger language models enhanced the visual perception of LVLMs, even though the dataset focuses solely on visual perception. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/psunlpgroup/VisOnlyQA) | N/A |
| [VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation](https://arxiv.org/abs/2412.00927) | Wenhu Chen, Cong Wei, Jie Min, hyang0511, wren93 | - VISTA, a novel Video SpatioTemporal Augmentation framework, synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets by spatially and temporally combining videos. - It leverages a large language model, Gemini 1.5-Pro, to generate question-answer pairs related to the newly synthesized videos. - VISTA-400K, a 400,000-sample video instruction-following dataset based on this approach, improves the performance of various video Large Multimodal Models (LMMs) by an average of 3.3% across four long-video understanding benchmarks. - Introduction of HRVideoBench, the first high-resolution video understanding benchmark, on which VISTA-finetuned models show a 6.5% performance gain. - Ablation studies demonstrate that disabling proposed augmentations reduces model performance, highlighting the quality and importance of the generated data. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering', 'Computer Vision', 'Video Classification'] | N/A | [Link](https://tiger-ai-lab.github.io/VISTA/) |
| [PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos](https://arxiv.org/abs/2412.01800) | Hangyu Guo, Haoze Zhao, Haoran Tang, Meng Cao, zhangysk | - PhysGame, a benchmark designed to evaluate the ability of Video Large Language Models (Video LLMs) to identify and interpret physical commonsense violations in gameplay videos. - Constructed using 880 gameplay videos annotated with multiple-choice questions focused on uncovering glitches that defy physical commonsense understanding, categorized across four primary physical domains: mechanics, kinematics, optics, and material properties, subdivided into twelve distinct categories. -  Analysis shows open-source Video LLMs underperforming compared to proprietary counterparts, leading to the creation of PhysInstruct and PhysDPO, two datasets containing over 174k training examples in total.  - The proposed physical knowledge-enhanced Video LLM, PhysVLM, trained on the introduced datasets, reaches state-of-the-art performance on PhysGame, outperforming existing open-source and commercial models.  - PhysVLM demonstrates strong generalizability, achieving high scores on standard video understanding benchmarks such as Video-MME and VCG. | ['Video-Text-to-Text', 'Multimodal', 'Question Answering'] | [Link](https://github.com/PhysGame/PhysGame) | N/A |
| [FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait](https://arxiv.org/abs/2412.01064) | Gyoungsu Chae, Dongchan Min, Taekyung Ki | - FLOAT is a novel audio-driven talking portrait video generation model based on flow matching in a learned motion latent space, enabling efficient design of temporally consistent motion. - It introduces a transformer-based vector field predictor with frame-wise conditioning, and supports emotion enhancement driven by speech-driven emotion labels. - FLOAT outperforms existing state-of-the-art audio-driven talking portrait methods on HDTF and RAVDESS datasets in terms of visual quality, motion fidelity, and efficiency, achieving FID scores of 21.10 and 31.68, respectively. - The use of flow matching allows for faster and higher quality sampling compared to diffusion-based methods, while the motion latent space ensures temporal consistency and expressiveness.  - Ablation studies confirm the benefits of the proposed FMT architecture and the use of speech-driven emotional labels | ['Text-to-Video', 'Computer Vision', 'Audio'] | N/A | N/A |
| [A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477) | Jingren Zhou, Bolin Ding, Yaliang Li, Xuchen Pan, yanxi-chen | - This paper proposes a two-stage algorithm for enhancing the test-time compute of Large Language Models (LLMs), aiming to boost their success probability on challenging tasks. - The algorithm first generates multiple candidate solutions and then selects the best one through a knockout tournament, where pairs of solutions are compared multiple times. - Theoretical analysis proves that the failure probability of this algorithm decreases exponentially with increased compute, given the assumptions that the LLM has a non-zero probability of generating a correct solution and can distinguish between correct and incorrect solutions better than random chance. - Empirical results on the MMLU-Pro benchmark validate these assumptions and demonstrate performance improvement with increased test-time compute, especially for reasoning-focused questions. - The paper discusses limitations and future research directions, including potential for handling complex tasks via decomposition and exploring more efficient algorithms with provable scaling laws. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning](https://arxiv.org/abs/2412.01408) | Noel Crespi, Reza Farahbaksh, callmesan | - This paper proposes a few-shot cross-lingual audio abuse detection method using Model-Agnostic Meta-Learning (MAML) with pre-trained audio representations in low-resource settings. - The model leverages Whisper and Wav2Vec and evaluates two feature normalization strategies: Temporal Mean and L2 normalization. - Experiments are conducted on the ADIMA dataset, comprising abusive audio clips in 10 Indian languages. - The best-performing model is Whisper with L2-Norm normalization, achieving accuracy scores ranging from 78.98% to 85.22% in the 100-shot setting. - A feature visualization study shows that language similarity can enhance cross-lingual abuse detection, especially in low-resource settings. | ['Audio', 'Audio Classification', 'Natural Language Processing', 'Zero-Shot Classification'] | [Link](https://github.com/callmesanfornow/fsl-audio-abuse.git) | N/A |


## Papers for 2024-12-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [On Domain-Specific Post-Training for Multimodal Large Language Models](https://arxiv.org/abs/2411.19930) | Xintong Zhang, doubling, edward2021, buaahsh, daixuancheng | - This paper introduces a novel approach for domain-specific post-training of Multimodal Large Language Models (MLLMs), focusing on data synthesis, training pipelines, and task evaluation. - A visual instruction synthesizer is developed using open-source models to extract diverse visual instruction tasks from domain-specific image-caption pairs, outperforming manual rules, GPT-4, and GPT-4V in enhancing MLLM performance on specialized domains such as biomedicine and food. - A single-stage training pipeline, combining synthetic tasks and image-caption pairs, is proposed to improve task diversity and mitigate catastrophic forgetting compared to traditional two-stage training. - Experiments on various MLLMs (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B) across biomedicine and food domains show consistent improvement on diverse domain-specific tasks using the resulting AdaMLLM (Adapted Multimodal Large Language Model). - All implementations will be open-sourced to support further research. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/AdaptLLM) |
| [Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS](https://arxiv.org/abs/2411.18478) | Zengqi Wen, Feihu Che, Shuai Zhang, fmk345, Jinyang23 | - This paper introduces HiAR-ICL, a novel automated reasoning paradigm that enhances in-context learning (ICL) by shifting the focus from specific examples to abstract thinking patterns, termed "thought cards." - HiAR-ICL uses Monte Carlo Tree Search (MCTS) to construct these thought cards from a small seed dataset and employs a cognitive complexity framework to dynamically match problems with appropriate thought cards during inference. - Five atomic reasoning actions, including System Analysis, One-Step Thought, Chain-of-Thought, Divide and Conquer, and Self-Reflection and Refinement, are defined as fundamental building blocks for these patterns. - The approach outperforms state-of-the-art methods on the MATH benchmark with Qwen2.5-7B-Instruct (79.6% accuracy), surpassing GPT-40 (76.6%) and Claude 3.5 (71.1%). - HiAR-ICL reduces time complexity compared to existing tree search methods by leveraging pre-computed reasoning patterns. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm), [Link](huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data) |
| [DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding](https://arxiv.org/abs/2411.19527) | Mingu Kang, Minseo Kim, Jisoo Kim, junwann, whwjdqls99 | - DisCoRD, a novel method for human motion generation, decodes discrete motion tokens into continuous motion using rectified flow, combining the naturalness of continuous representations with the faithfulness of discrete methods. - It employs an iterative refinement process in continuous space, capturing fine-grained dynamics and ensuring smoother motion, and uses discrete tokens as conditions in raw motion space to reduce noise. - A new metric, symmetric Jerk Percentage Error (sJPE), is introduced to evaluate both under-reconstruction and frame-wise noise in motion. - Extensive evaluations across text-to-motion, co-speech gesture, and music-to-dance generation demonstrate state-of-the-art performance, achieving an FID of 0.032 on HumanML3D and 0.169 on KIT-ML. - DisCoRD is adaptable to any discrete-based motion generation framework and improves naturalness without sacrificing faithfulness. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [Puzzle: Distillation-Based NAS for Inference-Optimized LLMs](https://arxiv.org/abs/2411.19146) | nav4, nailon-nvidia, talor-abr, tomer-nv, abercovich | - Puzzle, a novel framework, leverages decomposed Neural Architecture Search (NAS) and Blockwise Local Distillation (BLD) with Mixed-Integer Programming to create inference-optimized Large Language Models (LLMs) tailored to specific hardware. - This framework optimizes models by creating heterogeneous architectures with varying block configurations, reducing redundant computations while preserving performance. - The resulting model, Nemotron-51B derived from Llama-3.1-70B-Instruct, achieves up to 2.17x inference speedup on a single NVIDIA H100 GPU while retaining 98.4% of the parent model's accuracy. - Demonstrating unprecedented efficiency, Nemotron-51B's training required only 45B tokens compared to over 15T for its parent, setting a new benchmark for throughput and memory efficiency. - This work also introduces a derivative of Llama-3.1-8B-Instruct further demonstrating Puzzle's capacity to create highly efficient models across various hardware and parameter scales. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing](https://arxiv.org/abs/2411.19460) | Hyunjun Kim, dwightro, arkimjh, lakelee | - Introduces Video-Ma$^2$mba, a novel model for long-form video understanding that replaces the attention mechanism in Large Multimodal Models (LMMs) with State Space Models (SSMs), achieving linear scaling in time and memory. - Employs Multi-Axis Gradient Checkpointing (MA-GC) to enhance memory efficiency by retaining only essential activations across multiple computational axes. - Processes long video sequences, equivalent to over two hours of continuous video at 1 FPS, on a single GPU by handling the full sequence without frame sampling. - Improves accuracy and relevance of responses in long video understanding tasks by capturing detailed temporal dynamics. - Demonstrates substantial advantages over existing frameworks on benchmarks like Video-MME and LongVideoBench, showcasing its efficiency in handling lengthy video content and responding effectively to complex queries. | ['Video-Text-to-Text', 'Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification](https://arxiv.org/abs/2411.19638) | nljubesi, TajaKuzman | - This paper introduces a novel teacher-student framework utilizing Large Language Models (LLMs) for multilingual news topic classification without manual annotation. - A GPT model serves as the teacher, automatically annotating news articles in Slovenian, Croatian, Greek, and Catalan with IPTC Media Topic labels to create a training dataset. - Smaller BERT-like student models, specifically XLM-ROBERTa, are then fine-tuned on this dataset, achieving comparable performance to the teacher model while being more computationally efficient.  - The study demonstrates that student models achieve high performance with limited training data and exhibit strong zero-shot cross-lingual capabilities.  -  The best performing model, a multilingual IPTC news topic classifier, is publicly released. | ['Natural Language Processing', 'Text Classification', 'Zero-Shot Classification'] | [Link](https://github.com/TajaKuzman/IPTC-Media-Topic-Classification) | [Link](https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier) |


## Papers for 2024-11-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/abs/2411.18203) | Jingdi Lei, jwu323, ZonglinY, Duke-de-Artois, qq8933 | - This paper introduces Critic-V, a novel framework designed to enhance the reasoning capabilities of Vision-Language Models (VLMs) by incorporating a critic model that provides feedback during the reasoning process. - Critic-V features a Reasoner-Critic architecture where the Reasoner generates reasoning paths, and the Critic offers natural language critiques for refinement, inspired by the Actor-Critic paradigm and leveraging in-context reinforcement learning. - The Critic model is trained using Direct Preference Optimization (DPO) on a new dataset with critiques ranked by a Rule-based Reward (RBR) function to enhance its critic capabilities.  - Evaluation results show that Critic-V significantly improves the performance of existing VLMs, like Qwen2-VL-7B and DeepSeek-VL-7B, on 5 out of 8 benchmarks, surpassing even GPT-4V on several datasets and particularly improving mathematical reasoning tasks. - Critic-V integrates a dynamic text-based policy for the Reasoner and uses constructive feedback from the preference-optimized Critic to create a reliable and context-sensitive multimodal reasoning process. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting](https://arxiv.org/abs/2411.17176) | Hangwei Qian, Weijia Wu, Zhuohang Dang, Changliang Xia, ChengyouJia | - This paper introduces ChatGen, a novel framework for automating the text-to-image generation process from freestyle chat inputs. - It proposes a multi-stage evolution strategy (ChatGen-Evo) that equips language models with essential skills for prompt crafting, model selection, and argument configuration. - This approach outperforms baseline methods on a new benchmark dataset, ChatGenBench, which features diverse freestyle chat inputs paired with desired image outputs and generation parameters. - ChatGen-Evo with 2B parameters achieves comparable performance to a larger 8B parameter supervised baseline model. - The multi-stage evolution strategy results in high-quality images aligning with the diverse requirements from freestyle chat instructions. | ['Text-to-Image', 'Multimodal'] | [Link](https://chengyou-jia.github.io/ChatGen-Home) | N/A |
| [Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models](https://arxiv.org/abs/2411.17041) | Jong Chul Ye, Bryan S Kim, kjm981995 | - Free$^2$Guide is a novel gradient-free framework designed to enhance text-video alignment in diffusion-based generative models by leveraging Large Vision-Language Models (LVLMs). - The framework uses path integral control to approximate guidance during video generation, eliminating the need for gradients from reward functions and accommodating non-differentiable reward models like LVLMs. - Free$^2$Guide improves text alignment by processing multiple video frames and incorporates temporal understanding into the reward mechanism, allowing the use of powerful black-box vision-language model APIs. - It allows the ensembling of multiple reward models like LVLMs and image-based models to synergistically guide video generation, enhancing both text alignment and overall video quality. - Experiments demonstrate that using LVLMs in Free$^2$Guide significantly improved text alignment in video generation when compared to baseline models in various metrics, and exhibits improved general video quality across a range of attributes. | ['Text-to-Video', 'Multimodal'] | [Link](https://kjm981995.github.io/free2guide/) | N/A |
| [Morph: A Motion-free Physics Optimization Framework for Human Motion Generation](https://arxiv.org/abs/2411.14951) | Hao Liu, Xin Zhao, Ruibing Hou, Mingshuang Luo, Zhuo Li | - Morph is a novel motion-free physics optimization framework for generating realistic human motion from text or music, comprising a Motion Generator (MG) and a Motion Physics Refinement (MPR) module. - The MG synthesizes motion data, while the MPR, trained on this synthetic data, uses a motion imitator within a physics simulator to refine the generated motion, enforcing physical constraints and aligning its distribution with a discriminator using reinforcement learning. - This physics-refined data then fine-tunes the MG to enhance its realism. - Experiments on HumanML3D and AIST++ datasets show Morph improves physical plausibility metrics (e.g. Penetration, floating) drastically, while achieving competitive generation quality (FID, R-Precision) compared to state-of-the-art methods, across different generator architectures (diffusion, autoregressive, masked modeling). - This framework addresses the challenge of physically implausible artifacts in generated motion by leveraging synthetic data, making it a cost-effective and versatile solution. | ['Text-to-Video', 'Text-to-3D', 'Multimodal'] | N/A | N/A |
| [LongKey: Keyphrase Extraction for Long Documents](https://arxiv.org/abs/2411.17863) | Jean Paul Barddal, Cinthia Obladen de Almendra Freitas, Jeovane Honorio Alves, RaduState | - LongKey, a novel framework for keyphrase extraction from lengthy documents, leverages an encoder-based language model, specifically Longformer, to capture extended text intricacies, supporting up to 96K tokens. - It employs a max-pooling embedder to consolidate context across the document, refining keyphrase candidate representation. - Validated on LDKP datasets and six diverse unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based methods, achieving an F1@5 of 39.55% on LDKP3K and 41.81% on LDKP10K. - A component analysis confirmed the significant contribution of the keyphrase embedding pooler in enhancing performance. - While robust on long documents, LongKey's performance on short-context datasets suggests further development for broader applicability. | ['Natural Language Processing', 'Feature Extraction'] | [Link](https://github.com/jeohalves/longkey) | N/A |


## Papers for 2024-11-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment](https://arxiv.org/abs/2411.17188) | ranjaykrishna, Tim666, lzy8465, Dipsy0830, shuaishuaicdp | - This paper introduces ISG (Interleaved Scene Graph), a new evaluation framework for assessing the quality of interleaved text and image generation. - ISG uses a scene graph structure to capture relationships between text and image blocks, enabling multi-level evaluation (holistic, structural, block-level, and image-specific). - Alongside ISG, a new benchmark dataset ISG-BENCH containing 1,150 samples across 8 categories and 21 subcategories is introduced to assess model performance on complex language-vision dependencies. - Experimental results reveal that current unified vision-language models perform sub-optimally at generating interleaved content; compositional models that split language and image generation perform better but also have room to improve. - A new compositional baseline agent ISG-AGENT, based on a “plan-execute-refine” pipeline achieves performance improvement compared to other evaluated methods. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://interleave-eval.github.io) | N/A |
| [CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models](https://arxiv.org/abs/2411.18613) | Ruiqi Gao, holynski, atrevithick, doinkda, rundi | - CAT4D is a novel method for generating dynamic 3D scenes from monocular video using a multi-view video diffusion model. - The model is trained on a diverse combination of datasets and uses a novel sampling approach to transform a single video into a multi-view video, enabling robust 4D reconstruction. - CAT4D demonstrates competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, outperforming existing methods. - The model's creative capabilities are highlighted through its ability to generate 4D scenes from both real and generated videos. - The method is applicable to various tasks, including novel view synthesis, dynamic scene reconstruction, and sparse view reconstruction. | ['Text-to-Video', 'Image-to-Video', 'Video Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/google-research/cat4d) | N/A |
| [Large Language Model-Brained GUI Agents: A Survey](https://arxiv.org/abs/2411.18279) | Gezelligheid520, liqul, bowenli, shilhe, vyokky | - This paper presents a comprehensive survey of Large Language Model (LLM)-brained Graphical User Interface (GUI) agents, exploring their evolution, components, techniques, and applications. - LLM-brained GUI agents represent a new frontier in human-computer interaction, allowing users to interact with and control software applications using natural language. - The survey covers key aspects such as agent frameworks, data collection strategies, model optimization methods, evaluation metrics, and real-world use cases. - The paper also identifies key research gaps and challenges in the field, including privacy concerns, latency limitations, and the need for improved human-agent interaction. - It proposes future directions for research and development, focusing on enhancing agent capabilities, ensuring safety and reliability, and addressing ethical considerations. | ['Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation](https://arxiv.org/abs/2411.17945) | Sankalp Sinha, mzafzal, saali14, alootikki, SadilKhan | - This paper introduces MARVEL-40M+, a large-scale dataset with over 40 million text annotations for 8.9 million 3D assets, aimed at improving text-to-3D generation. - MARVEL, a multi-stage annotation pipeline, leverages open-source pretrained multi-view VLMs and LLMs to produce hierarchical descriptions ranging from detailed to concise tags. -  A two-stage text-to-3D framework, MARVEL-FX3D, is presented, which fine-tunes Stable Diffusion with MARVEL-40M+ annotations and utilizes a pretrained image-to-3D network. - Evaluations indicate that MARVEL-40M+ outperforms existing datasets in annotation quality and diversity, and MARVEL-FX3D achieves state-of-the-art results in text-to-3D generation. - Experimental results show that MARVEL-FX3D generates textured 3D meshes from text within 15 seconds, achieving superior prompt fidelity and overall preference compared to existing methods. | ['Text-to-3D', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing](https://arxiv.org/abs/2411.16781) | Shiguang Shan, Hong Chang, Heylon, flow2023, LiyiGang | - UniPose is a novel multimodal framework that unifies human pose comprehension, generation, and editing tasks.  It utilizes a pose tokenizer to convert 3D poses into discrete tokens for seamless integration with LLMs. - The model architecture incorporates a mixture of visual encoders (CLIP and a pose-specific visual encoder) to enhance fine-grained pose perception. - UniPose demonstrates superior performance across various pose-relevant tasks compared to existing methods, including pose comprehension, generation, and editing, as shown by experimental results in tables 2, 3, 4, and 5. - The model exhibits zero-shot generalization capabilities, enabling it to adapt to unseen tasks and enhance pose estimation, as shown in Figure 5. - UniPose addresses the limitations of existing methods by providing a unified multimodal framework that seamlessly handles multiple modalities, enabling finer-grained pose perception and more complex pose editing. | ['Multimodal'] | N/A | N/A |
| [Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding](https://arxiv.org/abs/2411.18462) | Xingyu Chen, Tian Liang, Jiahao Xu, Ziyin Zhang, zptu | - This paper introduces SVIP, a self-verification length policy for speculative decoding systems that dynamically determines the length of draft sequences based on the entropy of each draft token distribution. - SVIP achieves up to 20% walltime speedup on SpecBench and 60% speedup on MT-Bench for long-form generation. - The proposed method is training-free and compatible with any existing speculative decoding methods that generate draft tokens autoregressively. - Experimental results demonstrate consistent wall-time improvements on GliDe & CaPE and EAGLE-2. - The core idea is to adaptively adjust the draft length based on token difficulty, unlike traditional methods using a fixed draft length. | ['Text Generation'] | [Link](https://github.com/Geralt-Targaryen/SVIP) | N/A |
| [ChatRex: Taming Multimodal LLM for Joint Perception and Understanding](https://arxiv.org/abs/2411.18363) | Yihao Chen, Yuda Xiong, Yuqin Yang, Gen luo, Qing Jiang | - ChatRex, a novel multimodal large language model (MLLM), is introduced, featuring a decoupled architecture to address the conflict between perception and understanding tasks, commonly observed in existing MLLMs. - The model utilizes a Universal Proposal Network (UPN), a DETR-based model trained with granularity-based prompt learning, to provide robust object proposals for the LLM's retrieval-based detection process, eliminating coordinate prediction issues. - A new dataset, Rexverse-2M, containing two million image-region-text annotation triplets with varying granularities, was created with a fully automated data engine to train the model on joint perception and understanding tasks. - ChatRex demonstrates strong performance on object detection benchmarks like COCO (48.5 mAP) and LVIS (43.1 mAP), comparable to dedicated object detectors, and outperforms other MLLMs, particularly in multi-object scenes. - The model maintains competitive results on general multimodal benchmarks, showcasing robust understanding and dialogue capabilities, enhanced by the integration of perception abilities. | ['Multimodal', 'Object Detection', 'Visual Question Answering'] | [Link](https://github.com/IDEA-Research/ChatRex) | N/A |
| [Training and Evaluating Language Models with Template-based Data Generation](https://arxiv.org/abs/2411.18104) | yifAI | - This paper introduces Template-based Data Generation (TDG), a novel method for generating large-scale, high-quality mathematical datasets using GPT-4 to automatically generate meta-templates. - The TDG method leverages parameterized templates and a reject-sampling-based verification process to ensure data quality and scalability. - A dataset called TemplateGSM, consisting of over 7 million synthetically generated grade school math problems with verified solutions, is created using TDG. - Experiments show that TemplateGSM significantly improves the performance of LLMs in mathematical reasoning tasks. - The authors release both the TemplateGSM dataset and the TDG code to facilitate further research and development. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/iiis-ai/TemplateMath) | [Link](https://huggingface.co/datasets/math-ai/TemplateGSM) |


## Papers for 2024-11-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ShowUI: One Vision-Language-Action Model for GUI Visual Agent](https://arxiv.org/abs/2411.17465) | Shiwei Wu, Zhengyuan Yang, Difei Gao, Linjie Li, Kevin Qinghong Lin | - ShowUI, a novel 2B parameter vision-language-action (VLA) model, is designed to enhance Graphical User Interface (GUI) automation by processing visual screenshots, textual instructions, and generating appropriate actions. - The model employs UI-Guided Visual Token Selection, a technique that leverages connected components within UI screenshots to represent redundant visual areas, thus reducing computational overhead by 33% and improving training speed by 1.4x. - ShowUI implements Interleaved Vision-Language-Action Streaming to effectively manage interactions across different modalities and handle navigation history within multi-step GUI tasks. - Trained on a small, high-quality GUI instruction-following dataset consisting of only 256K samples, ShowUI achieves 75.1% accuracy in zero-shot screenshot grounding, outperforming larger models trained on larger datasets. - ShowUI also demonstrates strong navigation capabilities across diverse environments, including web, mobile, and online platforms. | ['Multimodal'] | [Link](https://github.com/showlab/ShowUI) | N/A |
| [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116) | Boris Ginsburg, Fei Jia, Shantanu Acharya | - Star Attention, a two-phase block-sparse approximation method, is introduced to improve the efficiency of Large Language Model (LLM) inference over long sequences. - The method involves an initial context encoding phase with blockwise local attention, followed by a query encoding and token generation phase using global attention via a distributed softmax algorithm. - This approach allows context length to scale linearly with the number of hosts, reducing memory requirements and inference time. - Evaluation on Llama3.1-8B and Llama3.1-70B across long-context benchmarks shows up to 11x faster inference while maintaining 95-100% accuracy compared to baseline methods. - Star Attention is compatible with existing LLM optimization techniques like Flash Attention and KV cache compression for potential further speed improvements. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/NVIDIA/Star-Attention) | N/A |
| [Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration](https://arxiv.org/abs/2411.17686) | Honggang Chen, Donglin Wang, Pengxiang Ding, Xuyang Liu, Yuhang Han | - This paper introduces a novel "filter-correlate-compress" paradigm for training-free token reduction in Multimodal Large Language Models (MLLMs), decomposing the process into three distinct stages for improved interpretability and flexibility. - A suite of methods called FiCoCo is proposed, implementing this paradigm with variants for different MLLM inference phases. - FiCoCo leverages intermediate computation products to minimize FLOPs and achieves up to 82.4% FLOPs reduction with minimal performance impact. - Experimental results on 10 multimodal benchmarks show FiCoCo outperforms state-of-the-art training-free token reduction methods. - Notably, FiCoCo achieves comparable performance to LLaVA-1.5-7B with just 17.6% of the computational cost and 67.6% of GPU memory in practical applications. | ['Multimodal', 'Visual Question Answering'] | [Link](https://ficoco-accelerate.github.io/) | N/A |
| [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/abs/2411.15296) | Xinyu Fang, Bo Li, Shukang Yin, Chaoyou Fu, yifanzhang114 | - This paper presents a comprehensive survey of evaluation methods for Multimodal Large Language Models (MLLMs), categorizing existing benchmarks based on the capabilities they assess (foundational, behavioral, and application-oriented). - The survey discusses the benchmark construction process, including data collection, annotation, and potential challenges like data contamination and benchmark diversity. - It also covers evaluation methods (human, LLM/MLLM, and script-based) and introduces common metrics and available toolkits for streamlined evaluation. - The paper highlights current limitations of MLLMs, such as struggling with fine-grained perception, complex chart understanding, and long-context reasoning. - Finally, it proposes future directions for benchmark development, including creating well-defined capability taxonomies, focusing on real-world applications, and evaluating more diverse modalities beyond vision and language. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text', 'Image-to-Text'] | [Link](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks) | N/A |
| [SketchAgent: Language-Driven Sequential Sketch Generation](https://arxiv.org/abs/2411.17673) | Judith E Fan, Alex Zhao, Kristine Zheng, Tamar Rott Shaham, Yael Vinker | - SketchAgent is a novel language-driven sequential sketch generation method that leverages pre-trained multimodal Large Language Models (LLMs) and requires no training or fine-tuning. - The core innovation lies in its intuitive sketching language, introduced to the model via in-context examples, enabling the LLM to "draw" using string-based actions processed into vector graphics and then rendered onto a pixel canvas. - By drawing stroke-by-stroke, coupled with an LLM's sequential nature and rich prior knowledge, SketchAgent captures the dynamic and evolving process of human sketching. - Evaluations demonstrate SketchAgent's ability to generate diverse sketches, engage in dialogue-driven drawing, collaborate with humans in real-time, and edit sketches via chat, effectively capturing the sequential and dynamic aspects of human sketching. - This approach marks a departure from optimization-based methods, which lack temporal structure, and paves the way for intuitive, interactive artificial sketching systems. | ['Text-to-Image', 'Multimodal'] | N/A | [Link](https://sketch-agent.csail.mit.edu/) |
| [SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE](https://arxiv.org/abs/2411.16856) | XIngang Pan, Tengfei Wang, Shangchen Zhou, Yushi Lan, Yongwei Chen | - SAR3D is a novel framework for fast 3D object generation and comprehensive understanding leveraging a multi-scale 3D vector-quantized variational autoencoder (VQVAE) and autoregressive modeling. - The multi-scale 3D VQVAE tokenizes 3D objects into hierarchical levels of tokens, enabling next-scale prediction training, which significantly reduces generation time, achieving speeds as fast as 0.82 seconds on an A6000 GPU. - For 3D generation, SAR3D employs an autoregressive model that predicts the next scale of the latent triplane representation, conditioned on single image or text prompts. - SAR3D-LLM, an extension of SAR3D for understanding, aligns the latent space of the 3D VQVAE with a large language model, enabling detailed 3D captioning and simultaneous generation and captioning. - Experimental results demonstrate that SAR3D surpasses existing 3D generation methods in both speed and quality, and enables detailed captioning of generated and encoded 3D objects. | ['Text-to-3D', 'Image-to-3D', 'Multimodal', 'Computer Vision'] | [Link](https://cyw-3d.github.io/projects/SAR3D/) | N/A |
| [VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models](https://arxiv.org/abs/2411.17451) | Yifan Song, Xuqing Yang, Zhihui Xie, Yuancheng Wei, Lei Li | - Introduces VL-RewardBench, a benchmark designed to evaluate Vision-Language Generative Reward Models (VL-GenRMs) and address limitations of existing evaluation methods that rely on biased AI-generated preferences or simplistic queries. - VL-RewardBench consists of 1,250 examples across three domains: general multimodal queries, visual hallucination detection, and complex reasoning tasks, curated through AI-assisted annotation with human verification. - Evaluation of 16 leading VL-GenRMs reveals that even top models like GPT-40 achieve only moderate accuracy on VL-RewardBench while open-source models struggle, highlighting the benchmark's challenging nature. - Analysis shows VL-GenRMs struggle more with basic visual perception than complex reasoning and that test-time scaling benefits vary by model capacity. - Demonstrates that critic training of VL-GenRMs for judgment substantially improves their evaluation abilities and that benchmark performance strongly correlates with downstream Best-of-N sampling effectiveness in MMMU-Pro. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis](https://arxiv.org/abs/2411.16173) | Yong Man Ro, Hosu Lee, Hyunjun Kim, Junho Kim | - SALOVA, a novel video-LLM framework, enhances long-form video comprehension through a targeted retrieval process using a new dataset and architectural design. - The SceneWalk dataset, with 87.8K densely captioned long videos, enables capturing scene continuity and rich descriptive context for improved long-form video understanding. - SALOVA integrates a dynamic routing mechanism and spatio-temporal projector to retrieve relevant video segments efficiently, addressing the limitations of current video-LMMs in context length and memory overhead. - A FocusFast approach analyzes selected segments in detail while maintaining overall context awareness for enhanced video interpretation. - Experiments demonstrate SALOVA's superior performance in processing complex long videos, reducing information loss, and maintaining contextual integrity, outperforming existing video-LMMs on benchmarks like Video-MME and LongVideoBench, especially for medium to long videos. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens](https://arxiv.org/abs/2411.17691) | Haitao Mi, Zhisong Zhang, Thomas Hartvigsen, Tao Ge, Xu Ouyang | - This paper reveals that low-bit quantization tends to favor undertrained large language models (LLMs). - The authors study over 1500 quantized LLM checkpoints and derive scaling laws for quantization-induced degradation (QiD) concerning training tokens, model size, and bit width. - They propose using QiD to measure LLM training levels and predict quantization performance for models trained with 100 trillion tokens, finding that low-bit quantization might not be desirable for such extensively trained models. - Based on the scaling laws derived, the authors predict the number of training tokens needed to reach different training levels based on the magnitude of QiD for different LLM sizes. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/Xu-C) |
| [MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts](https://arxiv.org/abs/2411.14721) | Jingdi Le, Wei Liu, Yunqing Liu, Jiatong Li, qq8933 | - MolReFlect, a teacher-student framework, is introduced to perform fine-grained molecule-caption alignments in the molecule-caption translation task. - A larger teacher LLM extracts important phrases from molecule SMILES or captions, aligning them with corresponding characteristics or sub-structures in a zero-shot manner.  - In-Context Selective Reflection refines these alignments using similar examples and perplexity-based selection by a smaller student LLM. - Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT) enhances the student LLM's learning by reformatting context examples into a thought chain, incorporating fine-grained alignments and reasoning.  - MolReFlect achieves state-of-the-art performance on the ChEBI-20 dataset, outperforming baselines like ICMA and BioT5 in both Mol2Cap and Cap2Mol tasks without extra modalities or complex structures. | ['Natural Language Processing', 'Text2Text Generation', 'Translation'] | N/A | N/A |


## Papers for 2024-11-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Knowledge Transfer Across Modalities with Natural Language Supervision](https://arxiv.org/abs/2411.15611) | Marco Grangetto, Emanuele Aiello, luca-molinaro, carloalbertobarbano | - This paper introduces Knowledge Transfer, a method for teaching pre-trained visual models new concepts using only textual descriptions. - The method leverages cross-modal interactions, hypothesizing that pre-trained visual encoders possess sufficient low-level features (shape, appearance, color) to represent unknown high-level concepts when provided with a textual description. - Explicit Knowledge Transfer involves synthesizing images via model inversion based on the textual description and then fine-tuning the visual encoder using an image-text matching loss. - Experiments demonstrate successful introduction of novel concepts into CLIP and ViLT models, with improved zero-shot performance on various tasks, including classification, segmentation, and image-text retrieval. - The method also shows potential for out-of-domain generalization, such as introducing medical concepts into models trained on natural images. | ['Multimodal', 'Zero-Shot Classification', 'Zero-Shot Image Classification', 'Image Classification', 'Image Segmentation', 'Image-to-Text'] | N/A | N/A |
| [From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge](https://arxiv.org/abs/2411.16594) | Chengshuai Zhao, Alimohammad Beigi, Liangjie Huang, Bohan Jiang, Dawei Li | - This paper surveys the emerging field of "LLM-as-a-judge," where Large Language Models (LLMs) are used for scoring, ranking, or selecting content in various AI tasks. - The paper introduces a taxonomy categorizing LLM-as-a-judge approaches based on what to judge (e.g., helpfulness, harmlessness), how to judge (tuning, prompting), and where to judge (evaluation, alignment, retrieval, reasoning). - The study compiles existing benchmarks for LLM-as-a-judge evaluation, covering aspects like general performance, bias detection, and domain-specific tasks. - The authors highlight challenges such as bias, dynamic judgment, and the need for self-judging capabilities. - Finally, they suggest promising future directions like incorporating Retrieval Augmented Generation (RAG), creating bias-aware datasets, and developing human-LLM co-judgement systems. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge) | N/A |
| [MH-MoE:Multi-Head Mixture-of-Experts](https://arxiv.org/abs/2411.16205) | Furu Wei, Shuming Ma, Xun Wu, Shaohan Huang | - This paper introduces a novel implementation of Multi-Head Mixture-of-Experts (MH-MoE), enhancing the multi-head mechanism by enabling it to attend to information from various representation spaces within different experts. - The key modifications include adding a "heads" dimension to the token dimension and incorporating two linear projection layers at the beginning and end of the MoE layer. - The new implementation maintains FLOPs and parameter parity with sparse Mixture of Experts models, which improves efficiency. - Experimental results on language models show that this new implementation leads to quality improvements over both vanilla MoE and fine-grained MoE models. - Additionally, the paper demonstrates MH-MoE's compatibility with 1-bit Large Language Models (LLMs), like BitNet. | ['Natural Language Processing'] | N/A | N/A |
| [DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation](https://arxiv.org/abs/2411.16657) | Mohit Bansal, Jaehong Yoon, Han Lin, Jialu Li, Zun Wang | - DREAMRUNNER is a novel story-to-video generation model that uses a hierarchical system with a large language model (LLM) to create detailed scripts and plans for multi-scene, character-driven videos. - It employs retrieval-augmented test-time adaptation with motion-oriented videos and reference images to learn motion and subject priors, enabling fine-grained video generation with enhanced motion quality and seamless transitions. - DREAMRUNNER introduces SR3AI, a spatial-temporal region-based 3D attention and prior injection module, to bind objects and motions precisely while controlling frame-level semantics. - Evaluated on a new dataset, DreamStorySet, and T2V-CompBench, DREAMRUNNER outperforms existing methods in character consistency, text alignment, and smooth transitions, demonstrating strong performance in generating multi-character interactions and compositional video generation. - Notably, DREAMRUNNER shows a 13.1% relative improvement in character consistency, an 8.56% relative gain in text-following ability, and a 27.2% relative improvement in transition smoothness compared to previous state-of-the-art methods. | ['Text-to-Video', 'Multimodal'] | [Link](https://dreamrunner-story2video.github.io/) | N/A |
| [O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?](https://arxiv.org/abs/2411.16489) | Yuxiang Zheng, Yixiu Liu, Xuefeng Li, Haoyang Zou, Zhen Huang | - This paper investigates replicating OpenAI's O1 model capabilities, particularly its long-thought chain reasoning abilities using knowledge distillation from O1's API and supervised fine-tuning. - The authors demonstrate that a 72B parameter base model, fine-tuned on a distilled dataset of tens of thousands of samples, outperforms the O1-preview model on the American Invitational Mathematics Examination (AIME). -  The study also explores the generalization capabilities of the distilled model on diverse tasks including hallucination, safety, and open-domain QA, demonstrating strong generalization and reduced susceptibility to sycophancy despite training solely on mathematical problem-solving data. - The authors emphasize the potential risks of over-reliance on distillation techniques, highlighting concerns about transparency, innovation stagnation, and the cultivation of first-principles thinking in AI research.  - A Technical Transparency Index (TTI) framework is proposed for evaluating and comparing O1 replication efforts based on transparency and reproducibility, advocating for a shift in research culture toward more transparent and innovative approaches. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/GAIR-NLP/01-Journey) | N/A |
| [GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI](https://arxiv.org/abs/2411.14522) | Zhe Chen, Bin Fu, Wei Li, Yanzhou Su, foreverbeliever | - This paper introduces GMAI-VL, a large vision-language model for general medical AI, and GMAI-VL-5.5M, a comprehensive multimodal medical dataset. - GMAI-VL-5.5M is constructed from 219 specialized medical datasets, covering 13 medical imaging modalities and 18 specialties, totaling 5.5M image-text pairs. - GMAI-VL employs a three-stage training strategy: shallow alignment (projector training), deep alignment (vision encoder and projector training), and instruction tuning. - Experimental results indicate that GMAI-VL achieves state-of-the-art performance across various multimodal medical tasks, including visual question answering and medical image diagnosis, outperforming existing models on benchmarks such as VQA-RAD, OmniMedVQA, and Health & Medicine track of MMMU. - The authors claim that the comprehensive task coverage, diverse modalities, and high-quality of GMAI-VL-5.5M contribute to GMAI-VL's superior performance. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/uni-medical/GMAI-VL) | N/A |
| [VisualLens: Personalization through Visual History](https://arxiv.org/abs/2411.16034) | Zhaojiang Lin, Yi Lu, Kai Sun, Deqing Fu, Wang Bill Zhu | - VisualLens is a novel approach for personalized recommendations that leverages a user's task-agnostic visual history, such as personal photos, instead of relying on traditional task-specific interaction logs or textual data. - The method addresses challenges like diversity and noise in visual histories by retrieving relevant images, extracting visual embeddings, captions, and aspect words, and using an iterative refinement process to improve aspect extraction. - VisualLens employs a grid-based approach to process multiple images simultaneously, reducing computational overhead, and uses a joint training strategy to enhance aspect word generation and candidate prediction. - Experimental results on two newly created benchmarks, Google Review-V and Yelp-V, demonstrate significant improvements over baselines and state-of-the-art methods, including a 1.6-4.6% improvement on Hit@3 over GPT-40. - The model uses an 8x8 grid for images along with their related captions and aspect words, which are combined with a d=2048 vector input to make predictions. | ['Multimodal', 'Image-to-Text', 'Question Answering'] | N/A | N/A |
| [One Diffusion to Generate Them All](https://arxiv.org/abs/2411.16318) | Aniruddha Kembhavi, Christopher Clark, Sangho Lee, Tuan Pham, Duong H. Le | - OneDiffusion, a unified, large-scale diffusion model, supports bidirectional image synthesis and understanding across diverse tasks, including text-to-image generation, image editing using various modalities (depth, pose, layout, semantic maps), image restoration (deblurring, upscaling), and multi-view generation. - The model employs a straightforward approach, treating all tasks as frame sequences with varying noise scales during training, eliminating the need for specialized architectures. - Using a novel "One-Gen" dataset, integrating diverse sources and synthetic data, enables scalable joint training across various tasks and resolutions. - Experimental results demonstrate OneDiffusion achieves competitive or state-of-the-art performance on text-to-image generation, multi-view generation, depth estimation, and ID customization, showcasing its strong generalization capabilities. - OneDiffusion supports novel conditioning setups, including text-to-multi-view and image-to-multi-view generation. | ['Text-to-Image', 'Image-to-Image', 'Multimodal', 'Image-to-3D', 'Computer Vision', 'Depth Estimation', 'Keypoint Detection'] | [Link](https://github.com/lehduong/OneDiffusion) | N/A |
| [Cautious Optimizers: Improving Training with One Line of Code](https://arxiv.org/abs/2411.16085) | Qiang Liu, Bo Liu, Lizhang Chen, Kaizhao Liang | - This paper introduces Cautious Optimizer, a one-line code modification for momentum-based optimizers like AdamW and Lion. - The modification involves masking updates where the proposed update direction and current gradient are misaligned, leading to faster and more stable training. - Theoretical analysis shows that this change preserves convergence guarantees and speeds up loss reduction without breaking the Hamiltonian function or Lyapunov analysis. - Empirical results demonstrate significant speed-ups of up to 1.47x on tasks such as LLaMA and MAE pretraining. - The cautious variants achieve improved sample efficiency with virtually no computational overhead, outperforming standard optimizers across different model sizes and datasets. | ['Natural Language Processing', 'Computer Vision'] | [Link](https://github.com/kyleliang919/C-Optim) | N/A |
| [The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz](https://arxiv.org/abs/2411.14486) | Forrest McKee, David Noever | - This research paper introduces a novel evaluation framework, the "impossible test," using a dataset of 675 fundamentally unsolvable problems to assess large language models' (LLMs) ability to acknowledge uncertainty. - Twelve state-of-the-art LLMs were evaluated on their propensity to admit "I don't know" rather than generate incorrect responses, with the best models achieving 62-68% accuracy in acknowledging uncertainty. - An inverse relationship was observed between problem difficulty and model accuracy, with GPT-4 demonstrating better uncertainty recognition on harder problems (35.8%) compared to simpler ones (20%). - The study reveals that models may be more prone to speculate when problems seem solvable and highlights variations in uncertainty acknowledgment across problem categories. - The impossible test contributes to Artificial General Intelligence (AGI) assessment by emphasizing uncertainty recognition as crucial for evaluating machine intelligence and prompting new directions for model training. | ['Question Answering'] | [Link](https://github.com/reveondivad/certify) | N/A |
| [Predicting Emergent Capabilities by Finetuning](https://arxiv.org/abs/2411.16035) | Sergey Levine, Dan Klein, Eric Wallace, sea-snell | - This paper proposes a novel method for predicting the emergence of capabilities in large language models (LLMs) by leveraging task-specific finetuning. - The key insight is that finetuning shifts the point of emergence to smaller models, and the amount of data controls the magnitude of this shift.  - They introduce an "emergence law", a parametric function fitted to finetuning data to model this shift, extrapolating it to predict few-shot emergence. -  Evaluating on standard NLP benchmarks (MMLU, GSM8K, CommonsenseQA, and CoLA), they demonstrate accurate emergence prediction up to 4x the FLOPs in advance using only small pre-emergence models. - Case studies on data quality assessment (OpenLLaMA v1 vs. v2) and complex capability prediction (LLaMA 2 on APPS coding) showcase potential real-world applications. | ['Natural Language Processing'] | N/A | N/A |
| [From CISC to RISC: language-model guided assembly transpilation](https://arxiv.org/abs/2411.16341) | Abdulrahman Mahmoud, Rania Hossam, Chaimaa Abi, Ahmed Heakl | - This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM and RISC-V assembly. - The approach uses a fine-tuned DeepSeekCoder-1.3B model with an extended tokenizer to handle assembly code specifics and a longer context window to capture complex semantic relationships. - Evaluated on real-world applications and compared against Apple's Rosetta 2, CRT achieves 79.25% translation accuracy from x86 to ARMv5 and 88.68% from x86 to RISC-V, along with 1.73x speedup, 1.47x better energy efficiency, and 2.41x better memory efficiency compared to Rosetta 2. - Analysis of the transpiled code reveals that the model maintains functional correctness despite syntactic variations, demonstrating the feasibility of LLM-based binary translation. - Further investigation revealed specific error patterns related to register allocation, memory addressing, and numerical token handling, indicating potential areas for future improvements. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://huggingface.co/blog/lorinma/yi-coder) |


## Papers for 2024-11-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [TÜLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/abs/2411.15124) | Hamish Ivison, Shengyi Huang, Valentina Pyatkin, Jacob Morrison, Nathan Lambert | - TÜLU 3 is a family of open-source, state-of-the-art language models fine-tuned from Llama 3.1 using a novel four-stage post-training recipe. - The recipe includes supervised fine-tuning, preference tuning using Direct Preference Optimization, and a novel Reinforcement Learning with Verifiable Rewards (RLVR) stage. - RLVR trains models against ground truth rewards for specific skills with verifiable answers (e.g., math, precise instruction following).  - TÜLU 3 models outperform existing open-weight models and some closed models, like GPT 3.5 Turbo and GPT 40 mini, in terms of average performance across benchmarks. - The release includes model weights, training code, evaluation suite, and datasets related to various core skills. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/allenai/open-instruct), [Link](https://github.com/allenai/olmes) | [Link](https://hf.co/allenai/Llama-3.1-Tulu-3-70B), [Link](https://hf.co/allenai/Llama-3.1-Tulu-3-8B), [Link](https://hf.co/collections/allenai/tulu-3-datasets-673b8df14442393f7213f372) |
| [A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection](https://arxiv.org/abs/2411.12946) | Shaun Khoo, shingurding, gabrielchua | - This paper introduces a flexible, data-free methodology for developing guardrails for Large Language Models (LLMs) to prevent off-topic misuse. - The methodology involves using an LLM to generate a synthetic dataset of on-topic and off-topic prompts based on a qualitative problem analysis, then fine-tuning embedding (jina-embeddings-v2-small-en) or cross-encoder (stsb-roberta-base) models on this data to create off-topic classifiers. - The resulting guardrails outperform baseline methods, including cosine similarity, KNN, and zero-shot classification using LLMs, demonstrating higher precision in identifying off-topic prompts. - By framing the detection task as assessing relevance between system and user prompts, the guardrail effectively generalizes to other misuse categories such as jailbreak and harmful prompts. - The synthetic dataset and trained models are open-sourced to facilitate research and development in LLM safety. | ['Natural Language Processing', 'Text Classification'] | N/A | [Link](https://huggingface.co/datasets/gabrielchua/off-topic), [Link](https://huggingface.co/collections/govtech/off-topic-guardrail-673838a62e4c661f248e81a4) |
| [Large Multi-modal Models Can Interpret Features in Large Multi-modal Models](https://arxiv.org/abs/2411.14982) | Ziwei Liu, Bo Li, Yifei Shen, Kaichen Zhang | - This paper introduces a framework to interpret the internal representations of Large Multimodal Models (LMMs), focusing on understanding their open-semantic features. - It employs Sparse Autoencoders (SAE) to disentangle complex representations into simpler, human-understandable features. - An automatic interpretation pipeline leverages larger LMMs to provide zero-shot explanations of these features, offering insights into model behavior. - The framework demonstrates the ability to steer LMM behavior by manipulating specific features, enabling targeted interventions and analysis of model decisions. - The authors analyze LLaVA-NeXT-8B using LLaVA-OV-72B and demonstrate effective feature interpretation and behavior steering, contributing to a deeper understanding of LMMs' strengths and weaknesses. | ['Multimodal', 'Image Feature Extraction'] | N/A | N/A |
| [VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection](https://arxiv.org/abs/2411.14794) | Xiu Su, Le Zhuo, Hairong Shi, Wei Huang, Songhao Han | - VideoEspresso, a large-scale video question-answering (VideoQA) dataset, is introduced, focusing on fine-grained video reasoning and featuring VideoQA pairs that retain spatial and temporal context. - An automatic pipeline is used to construct the dataset, employing semantic key-frame extraction and GPT-40 for generating QA pairs and enriching them with Chain-of-Thought annotations. - A Hybrid LVLMs Collaboration framework is proposed, incorporating a Frame Selector and a two-stage reasoning LVLM to address cost-effectiveness and accuracy in video reasoning. - Evaluation on a 14-task benchmark against popular LVLMs shows superior performance, highlighting improved video reasoning capabilities compared to baseline methods. - Both objective and subjective evaluations, incorporating metrics such as logic, factuality, accuracy, and conciseness, showcase the dataset and model efficacy. | ['Visual Question Answering', 'Multimodal'] | [Link](https://github.com/hshjerry/VideoEspresso) | N/A |
| [One to rule them all: natural language to bind communication, perception and action](https://arxiv.org/abs/2411.15033) | Giuseppe Boccignone, Dimitri Ognibene, colo286 | - This research presents a novel architecture for robot action planning which integrates Large Language Models (LLMs), perception, and action planning using a modified ReAct framework. - The core component, the Planner Module, translates natural language commands into executable actions and uses LLMs in a modified ReAct framework to dynamically adjust plans based on real-time feedback and environmental perception.  - The system leverages scene graphs for semantic mapping and context understanding while utilizing an execution control mechanism and a failure management system to enable robust error handling.  - Preliminary testing on RoBee, a humanoid robot, in a simulated kitchen and bedroom environment demonstrates the efficacy of the architecture. - It is able to handle simple, and moderately complex requests with good success rates, with future work directed to improve performance on complex, and open-ended requests and refining real-world integration. | ['Robotics', 'Natural Language Processing'] | N/A | N/A |


## Papers for 2024-11-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442) | Yangzhou Liu, Yue Cao, Wenhai Wang, Zhe Chen, Weiyun Wang | - This paper introduces Mixed Preference Optimization (MPO), a method for enhancing the multimodal reasoning capabilities of Large Language Models (LLMs), by combining supervised fine-tuning loss with preference and quality losses. - A new dataset, MMPR, a large-scale multimodal reasoning preference dataset is created using an automated preference data construction pipeline. - The InternVL2-8B-MPO model, trained using MPO, achieves state-of-the-art performance on MathVision (25.7% accuracy) among open-source models and a score of 67.0% on MathVista, outperforming the baseline InternVL2-8B by 8.7 points. - MPO also leads to improved performance on hallucination benchmarks like POPE and CRPE, and complex VQA benchmarks like MM-Vet and LLaVA-bench, comparable to the much larger InternVL2-76B model. - The paper includes ablation studies demonstrating the impact of different optimization algorithms, data scale, and hyperparameters on performance. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Text Generation', 'Image-to-Text'] | N/A | N/A |
| [Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/abs/2411.14405) | Tianqi Shi, Hao Wang, Bo Zeng, Huifeng Yin, Yu Zhao | - Marco-01 is a large language model fine-tuned for enhanced reasoning abilities, focusing on both disciplines with standard answers (math, physics, coding) and open-ended problem-solving. - The model leverages Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies. - Marco-01 achieved accuracy improvements of +6.17% on MGSM (English) and +5.60% on MGSM (Chinese) datasets, demonstrating enhanced reasoning capabilities.  - In translation tasks, Marco-01 excels at understanding colloquial nuances, translating slang expressions accurately, and surpassing tools like Google Translate. - It employs different granularities for MCTS actions ('steps' and 'mini-steps') to navigate diverse problem complexities and incorporates reflection mechanisms for self-correction and error detection in its reasoning process. | ['Question Answering', 'Natural Language Processing', 'Translation', 'Text2Text Generation'] | N/A | N/A |
| [OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://arxiv.org/abs/2411.14199) | Amanpreet Singh, Weijia Shi, Rulin Shao, jacquelinehe, akariasai | - Introduced OpenScholar, a retrieval-augmented language model (LM) designed to synthesize information from scientific literature. - OpenScholar leverages a specialized datastore of 45 million open-access papers, retrievers trained on scientific text, and an iterative self-feedback generation process. - Developed ScholarQABench, a multi-domain benchmark with nearly 3,000 expert-written queries and answers across computer science, physics, neuroscience, and biomedicine, to evaluate OpenScholar and other LMs. - Demonstrated that OpenScholar outperforms existing systems, including GPT-4, on ScholarQABench, particularly in citation accuracy, with human evaluation showing preference for OpenScholar responses over expert-written ones in over half of cases. - Open-sourced the code, models, datastore, data, and a public demo for OpenScholar and ScholarQABench. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/AkariAsai/OpenScholar), [Link](https://github.com/AkariAsai/ScholarBench) | [Link](https://huggingface.co/OpenScholar/openscholar-v1) |
| [Multimodal Autoregressive Pre-training of Large Vision Encoders](https://arxiv.org/abs/2411.14402) | Michal Klein, Philipp Dufter, Xiujun Li, Mustafa Shukor, efini | - AIMv2, a family of generalist vision encoders, is introduced, utilizing a novel multimodal autoregressive pre-training method with image and text inputs. - The model architecture consists of a vision transformer encoder with prefix attention, followed by a causal multimodal decoder that autoregressively generates image patches and text tokens. - AIMv2 demonstrates strong performance across various vision and multimodal tasks, including image recognition, object detection, grounding, and multimodal understanding benchmarks. - Notably, AIMv2 outperforms state-of-the-art contrastive models like CLIP and DINOv2 in several tasks, exhibiting strong scaling properties with increasing data and parameters. - The model achieves 89.5% accuracy on ImageNet-1k with a frozen trunk after high-resolution finetuning. | ['Multimodal', 'Computer Vision', 'Image Classification', 'Object Detection', 'Image Feature Extraction'] | [Link](https://github.com/apple/ml-aim) | N/A |
| [Ultra-Sparse Memory Network](https://arxiv.org/abs/2411.12364) | Defa Zhu, Qiyang Min, Taoer, xyzed, FetchFortune | - This paper introduces UltraMem, a novel architecture incorporating large-scale, ultra-sparse memory layers to enhance the performance of transformer models. - UltraMem builds upon the Product Key Memory (PKM) concept and introduces several improvements including Tucker Decomposed Query-Key Retrieval, Implicit Value Expansion, and Multi-Core Scoring. - The paper claims that UltraMem reduces inference latency while maintaining model performance. - Experiments show UltraMem achieves up to 6x speedup compared to Mixture of Experts (MoE) models at the same scale and with a given computation budget. - It also exhibits favorable scaling properties, outperforming traditional and MoE models on various benchmarks including MMLU, TriviaQA, and BBH. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Hymba: A Hybrid-head Architecture for Small Language Models](https://arxiv.org/abs/2411.13676) | Zijia Chen, Wonmin Byeon, Shizhe Diao, Yonggan Fu, Xin Dong | - Hymba, a family of small language models (LLMs), introduces a hybrid-head parallel architecture integrating transformer attention and state space models (SSMs) for improved efficiency and performance. - This architecture allows parallel processing, leveraging attention heads for high-resolution recall and SSM heads for efficient context summarization within the same layer. - Hymba incorporates learnable meta tokens, prepended to prompts to store critical information and optimize attention allocation, along with cross-layer key-value sharing and partial sliding window attention for a compact cache. - Hymba-1.5B-Base surpasses all sub-2B public models and even outperforms Llama-3.2-3B by 1.32% in average accuracy on commonsense reasoning tasks, with an 11.67× cache size reduction and 3.49× throughput improvement. - The instruction-tuned model, Hymba-1.5B-Instruct, achieves state-of-the-art results on various downstream tasks, including GSM8K, GPQA, and the Berkeley function-calling leaderboard. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/NVIDIA/Hymba-1.5B-Base), [Link](https://huggingface.co/NVIDIA/Hymba-1.5B-Instruct) |
| [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2411.14432) | Winston Hu, Jingkang Yang, Hai-Long Sun, Zuyan, THUdyh | - Insight-V is a novel system designed to enhance the visual reasoning capabilities of Multimodal Large Language Models (MLLMs) by generating structured reasoning data and employing a multi-agent training approach. - The system utilizes a two-step data generation pipeline with a progressive strategy to create diverse reasoning paths and a multi-granularity assessment method to ensure data quality, followed by training a multi-agent MLLM system with a reasoning agent and a summary agent to decompose the problem-solving process. - The reasoning agent produces detailed reasoning steps, while the summary agent assesses and selectively utilizes the reasoning to answer the question, with iterative DPO used to refine reasoning quality. - Evaluation on seven visual reasoning benchmarks demonstrates significant performance gains, with an average improvement of 7.0% for LLaVA-NeXT and 2.9% for a stronger base MLLM, showcasing the effectiveness and generalizability of Insight-V. - Insight-V’s data generation pipeline and multi-agent system enables improvements to MLLM reasoning capabilities without needing expensive human labor. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/dongyh20/Insight-V) | N/A |
| [UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages](https://arxiv.org/abs/2411.14343) | Tae-Sun Chung, Akhil Kedia, Bethel Melesse Tessema | - UnifiedCrawl, a method to create large monolingual datasets for low-resource languages by efficiently filtering and extracting data from the Common Crawl corpus using minimal compute resources. - Demonstrates that fine-tuning multilingual LLMs with this data using efficient adapter methods like QLoRA significantly improves performance on low-resource languages while minimizing VRAM usage. - Shows large improvements in language modeling perplexity and few-shot prompting scores on downstream question-answering tasks. - Provides an affordable approach to improve LLMs for low-resource languages using consumer hardware. - The extracted Amharic dataset, UnifiedCrawl-Amharic, is significantly larger than existing Amharic datasets and leads to a 24% improvement in F1 score and 77% improvement in EM score on the Amharic question answering dataset, AmQA, when fine-tuning a 4.5B parameter XGLM model using QLoRA. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/bethelmelesse/unifiedcrawl) | N/A |
| [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://arxiv.org/abs/2411.14257) | Neel Nanda, Senthooran Rajamanoharan, Oscar Obeso, Javier Ferrando | - This paper investigates the mechanisms behind hallucinations in large language models (LLMs), focusing on entity recognition as a key factor. - Using sparse autoencoders (SAEs) as an interpretability tool, the researchers discovered directions in the representation space that detect whether a model recognizes an entity, indicating a form of self-knowledge about its capabilities. - These directions are causally relevant, capable of steering the model to refuse answering questions about known entities or hallucinate about unknown ones.  - The study demonstrates that chat fine-tuning repurposes this existing mechanism from the base model, and that unknown entity recognition directions disrupt the factual recall mechanism by suppressing attention of attribute extraction heads. - Additionally, the researchers identify SAE latents seemingly representing uncertainty, which are predictive of incorrect answers. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/jbloom/Gemma-2b-IT-Residual-Stream-SAEs), [Link](https://huggingface.co/datasets/HuggingFaceFW/fineweb) |
| [Patience Is The Key to Large Language Model Reasoning](https://arxiv.org/abs/2411.13082) | Yijiong Yu | - This paper proposes a method for improving large language model (LLM) reasoning ability by encouraging a more patient reasoning style, without requiring new knowledge or skills. - The method involves training LLMs to favor thorough reasoning processes by using preference optimization, where detailed reasoning is treated as positive examples and simple answers as negative examples. - The model is fine-tuned using a lightweight dataset of mathematical problems and their solutions, which are further refined into more detailed steps. - Experimental results demonstrate a performance increase of up to 6.7% on the GSM8k benchmark, surpassing several previous works despite using less data and a simpler method. - Although the method increases inference time, the trade-off is considered worthwhile for enhanced accuracy in complex problem-solving. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2024-11-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2411.10958) | Jun Zhu, Jia Wei, Pengle Zhang, Haofeng Huang, jt-zhang | - SageAttention2 is a new attention mechanism that uses 4-bit matrix multiplication and additional precision-enhancing techniques to accelerate attention computation while maintaining precision. - It quantizes matrices Q and K to INT4 in warp-level granularity, and matrices P and V to FP8, along with smoothing techniques for Q and V to enhance accuracy. - An adaptive mixed-precision method employs 8-bit attention for problematic layers/timesteps and 4-bit attention for others, further improving accuracy. - On an RTX 4090, SageAttention2 achieves a peak performance of 485 TOPS, surpassing FlashAttention2 and xformers by approximately 3.1x and 5.4x, respectively. - Comprehensive experiments across diverse models, including large language, image generation, and video generation models, demonstrate negligible end-to-end metric loss with SageAttention2. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Image-to-Video', 'Text-to-Video', 'Image Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation](https://arxiv.org/abs/2411.13281) | Mohan Kankanhalli, Jing Ma, Dongxu Li, teowu, Ziyang | - Introduces VideoAutoArena, an automated arena-style benchmark for evaluating Large Multimodal Models (LMMs) in video analysis using simulated user interactions and open-ended questions. - Employs user simulation with role-playing by LMM agents to generate diverse, realistic questions based on video content and user personas with varying degrees of relevance to the video. - Implements a fault-driven evolution strategy to progressively increase question complexity, challenging models to address increasingly difficult video analysis scenarios. - Leverages an automated judging system based on the LMM-as-a-Judge paradigm, comparing responses and ranking models using a modified ELO rating system, showing strong alignment with human judgment (87.29%). - Introduces an auxiliary benchmark, VideoAutoBench, streamlining LMM evaluation by comparing model responses against human-selected winners from a subset of VideoAutoArena battles, demonstrating consistent ranking results between the arena and the bench. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text'] | N/A | [Link](https://videoautoarena.github.io/) |
| [When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training](https://arxiv.org/abs/2411.13476) | Cunxiao Du, Tongyao Zhu, Chao Du, Qian Liu, haonan3 | - This paper introduces AnchorAttention, a novel attention mechanism designed to improve long-context training of Large Language Models (LLMs) by addressing the numerical instability of Rotary Position Embedding (RoPE) when using BFloat16 precision. - AnchorAttention mitigates the issue by treating the first token in the input sequence as a shared anchor with a consistent position ID, visible to all documents within the context window, but masking out attention across different documents. - This reduces attention computations and the accumulation of numerical errors while ensuring the model learns a full spectrum of rotational angles in RoPE.  - Experiments on the RULER benchmark and real-world long-context datasets like LongBench show that AnchorAttention outperforms standard full attention and intra-document attention, boosting performance and reducing training time by over 50%. - AnchorAttention maintains performance on medium and short context benchmarks such as MMLU and HellaSwag while excelling in long context settings. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/haonan3/AnchorContext) | N/A |
| [ORID: Organ-Regional Information Driven Framework for Radiology Report Generation](https://arxiv.org/abs/2411.13025) | Dongnan Liu, Ziyong Feng, Xiang An, Tiancheng Gu, Kaichengalex | - This paper introduces ORID, an Organ-Regional Information Driven framework, for generating radiology reports from medical images.  - ORID leverages a fine-tuned multi-modal large language model (LLaVA-Med-RRG) to generate organ-specific diagnostic descriptions. - It employs an organ-based cross-modal fusion module and an organ importance coefficient analysis module to integrate image and text features effectively and to reduce the influence of unrelated organ regions, respectively.  - The framework is evaluated on IU-Xray and MIMIC-CXR datasets, achieving state-of-the-art performance in NLG metrics on both datasets.  - ORID also demonstrates superior clinical efficacy on MIMIC-CXR compared to other models. | ['Image-to-Text', 'Multimodal'] | N/A | N/A |


## Papers for 2024-11-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [RedPajama: an Open Dataset for Training Large Language Models](https://arxiv.org/abs/2411.12372) | Shane Adams, Yonatan Oren, Quentin Anthony, Daniel Fu, Maurice Weber | - The paper releases RedPajama-V1, an open reproduction of the LLaMA training dataset, and RedPajama-V2, a new web-based dataset comprising over 100 trillion tokens with quality signals for filtering. - RedPajama-V2 dataset emphasizes transparency by documenting its creation process, offering data at scale, and includes artifacts and quality signals for filtering and creating higher quality datasets. - This dataset has been instrumental in training production-ready large language models, such as Snowflake Arctic, Salesforce's XGen, and AI2's OLMo. - Ablation studies are conducted using decoder-only transformer models with up to 1.6B parameters, demonstrating how quality signals enhance dataset curation. - The study emphasizes the potential of RedPajama in building more transparent, high-performing large language models. | ['Natural Language Processing'] | [Link](github.com/togethercomputer/RedPajama-Data) | [Link](huggingface.co/datasets/togethercomputer/RedPajama-Data-1T), [Link](huggingface.co/datasets/togethercomputer/RedPajama-Data-V2) |
| [Building Trust: Foundations of Security, Safety and Transparency in AI](https://arxiv.org/abs/2411.12275) | Huamin Chen, Mark Bestavros, Emily Fox, Garth Mollett, huzaifas-sidhpurwala | - This paper explores the security and safety implications of publicly available AI models, particularly large language models (LLMs). - It reviews current security and safety scenarios, highlighting challenges like issue tracking, remediation, and the lack of established lifecycle and ownership processes for AI models. - The paper proposes comprehensive strategies to enhance security and safety for both model developers and end-users. - It discusses the distinction between AI security (protecting systems from threats) and AI safety (preventing harm from the system's operation), emphasizing the need for a holistic approach to AI risk management. - The paper also suggests adapting existing vulnerability disclosure processes for AI security flaws and proposes the establishment of a central body for tracking safety hazards. | ['Natural Language Processing'] | N/A | N/A |
| [Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages](https://arxiv.org/abs/2411.12240) | D. J. Bora, tamang0000 | - This research paper evaluates the performance of tokenizers used by 12 Large Language Models (LLMs) across all 22 official languages of India. - The study uses Normalized Sequence Length (NSL) as the key metric for evaluation and finds that the SUTRA tokenizer outperforms other models, including Indic-specific models, in 14 out of 22 languages. - Notable findings include SUTRA's superior performance with Indic languages, GPT-40's improvement over GPT-4 in processing Indian languages, and the comparatively limited performance of Project Indus. - This suggests that Project Indus' better performance on some Indian language is tied to the training on common scripts (Devanagari) between the languages and not to the linguistic understanding. - The study highlights the importance of developing targeted tokenization strategies for multilingual and Indic-centric LLMs. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |


## Papers for 2024-11-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices](https://arxiv.org/abs/2411.10640) | wolf1110, AJZhou, liuyangbian, yina0, lucky-lance | - BlueLM-V-3B is a 3 billion parameter multimodal large language model designed for mobile devices, featuring a 2.7B parameter language model and a 400M parameter vision encoder (SigLIP). - It introduces a relaxed aspect ratio matching method for dynamic image resolution, reducing the number of image tokens processed by the vision encoder without sacrificing model performance. - System optimizations include batched image encoding, pipeline parallelism for image processing, token downsampling, chunked computing of input tokens, and mixed-precision quantization. - BlueLM-V-3B achieves state-of-the-art performance on the OpenCompass benchmark, with an average score of 66.1, surpassing larger models like MiniCPM-V-2.6 and InternVL2-8B. - On a MediaTek Dimensity 9300 processor, it uses 2.2GB of memory, encodes 768x1536 images in 2.1 seconds, and generates text at 24.4 tokens/second. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Generative World Explorer](https://arxiv.org/abs/2411.11844) | Daniel Khashabi, Alan Yuille, Tianmin Shu, jienengchen, TaiMingLu | - The paper introduces Generative World Explorer (Genex), a novel egocentric video generation model that allows embodied agents to "mentally" explore 3D environments (e.g., urban scenes) by generating imagined future observations, and then use these observations to update the agent's beliefs about the world, which enable the agent to make more informed decisions. - The architecture is based on a video diffusion model that takes an initial egocentric view (represented as a panoramic image), the intended movement direction as action input, and then streams a video of future egocentric observations.  - A spherical-consistent learning objective is introduced to ensure that generated pixels are continuous in the spherical space and improve the consistency and coherence of generated videos during long imaginative exploration. - The experimental evaluation, with a new synthetic urban scene dataset (Genex-DB) and new Embodied QA benchmark (Genex-EQA), shows that Genex generates high-quality and consistent observations during imaginative exploration and improves an existing LLM agent’s decision-making process. - The authors extend Genex to multi-agent scenarios, where an agent infers the perspectives of other agents to make decisions based on a more complete understanding of the situation. | ['Text-to-Video', 'Computer Vision', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/Beckschen/genex) | [Link](https://generative-world-explorer.github.io/) |
| [Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering](https://arxiv.org/abs/2411.11504) | Ben He, Boxi Cao, Xinyu Lu, Yanjiang Liu, Xinyan Guan | This paper introduces verifier engineering, a novel post-training paradigm for foundation models.  It leverages automated verifiers to perform verification tasks and deliver feedback to the models, enhancing their capabilities.  The framework systematically categorizes this process into three stages: search, verify, and feedback.  The authors provide a comprehensive overview of state-of-the-art research in each stage. This approach is presented as a fundamental pathway toward achieving Artificial General Intelligence. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/icip-cas/Verifier-Engineering) | N/A |
| [SlimLM: An Efficient Small Language Model for On-Device Document Assistance](https://arxiv.org/abs/2411.09944) | Viet Dac Lai, Seunghyun Yoon, Phat T. Nguyen, Thang M. Pham, Franck-Dernoncourt | - SlimLM, a series of small language models (SLMs) optimized for document assistance tasks on mobile devices like smartphones, is introduced. - The models range from 125M to 1B parameters and are pre-trained on SlimPajama-627B and fine-tuned on DocAssist, a new dataset constructed for summarization, question answering, and question suggestion. - SlimLM models demonstrate comparable or superior performance to existing SLMs of similar sizes on a Samsung Galaxy S24, efficiently handling contexts up to 800 tokens. - An Android application demonstrates SlimLM's real-world applicability. - The models offer a balance between performance, efficiency, and privacy for on-device document processing, potentially reducing reliance on server-based APIs. | ['Natural Language Processing', 'Document Question Answering', 'Question Answering', 'Summarization'] | N/A | N/A |
| [Top-$nσ$: Not All Logits Are You Need](https://arxiv.org/abs/2411.07641) | Liusheng Huang, Hongli Xu, Jianchun Liu, tomorrowdawn | - This paper introduces top- σ, a novel sampling method for large language models (LLMs) that operates directly on pre-softmax logits by leveraging a statistical threshold. - The method distinguishes between a Gaussian-distributed noisy region and a distinct informative region in the logits, enabling efficient token filtering without complex probability manipulations. - Unlike existing methods, top- σ maintains a stable sampling space regardless of temperature scaling, making it suitable for test-time scaling techniques. - Experimental results across four reasoning-focused datasets demonstrate that top- σ outperforms existing sampling approaches and greedy decoding, while maintaining consistent performance at high temperatures. - The theoretical analysis of top-no provides further insights into its behavior and temperature invariance property. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts](https://arxiv.org/abs/2411.10669) | Nanyi Fei, Hongpeng Lin, Guoxing Yang, Yanqi Dai, Jinqiang Long | - This paper introduces Awaker2.5-VL, a Mixture of Experts (MoE) architecture designed for Multimodal Large Language Models (MLLMs) to address the "multi-task conflict" issue, where mixing data from various tasks leads to performance degradation. - Awaker2.5-VL utilizes multiple sparsely activated expert models, each specializing in a specific task, along with a global expert for general capabilities, and a gating network to control expert activation. - The model employs Low-Rank Adaptation (LoRA) for each expert to reduce training and inference costs and incorporates a simplified routing strategy for enhanced training stability. - The paper uses Qwen2-VL-7B-Instruct as its base model and evaluates performance on benchmarks such as MME-Realworld, MME-Realworld-CN, and MMBench. - Awaker2.5-VL achieves state-of-the-art results on the MME-Realworld-CN benchmark and shows significant improvements over the base model on other benchmarks, including a 5-point improvement in overall score on MME-Realworld-CN. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Object Detection'] | [Link](https://github.com/MetabrainAGI/Awaker) | N/A |
| [LLäMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171) | Andreas Hotho, Julia Wunderle, Jan Pfister | - This paper introduces LLäMmlein, two new German-only decoder-only LLMs (120M and 1B parameters), trained from scratch on a filtered and preprocessed version of the RedPajama dataset. - A new German tokenizer with a 32,000 token vocabulary was created and models were evaluated on the SuperGLEBer and lm-evaluation-harness-de benchmarks, showing competitive performance against similarly sized models and even some larger models. - The 1B model matches the performance of much larger models, like the German finetuned Llama 8B on the SuperGLEBer benchmark and its instruction-tuned version outperforms all other 1B models on TruthfulQA by at least 6%. - Intermediate checkpoints were analyzed to track the learning dynamics, revealing varying rates of improvement across different tasks. - All artifacts, including the models, code, and data, will be open-sourced to promote transparency and future research within the German NLP community. | ['Natural Language Processing', 'Text Generation', 'Question Answering', 'Token Classification', 'Sentence Similarity'] | N/A | [Link](https://huggingface.co/cis-lmu/bavarian_to_english), [Link](https://huggingface.co/LSX-UniWue/Guanako), [Link](https://huggingface.co/FreedomIntelligence/alpaca-gpt4-deutsch), [Link](https://huggingface.co/FreedomIntelligence/evol-instruct-deutsch), [Link](https://huggingface.co/FreedomIntelligence/sharegpt-deutsch), [Link](https://huggingface.co/DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1) |
| [Adaptive Decoding via Latent Preference Optimization](https://arxiv.org/abs/2411.09661) | Jason Weston, Asli Celikyilmaz, Ping Yu, Ilia Kulikov, Shehzaad Dhuliawala | - This paper introduces Adaptive Decoding, a method for dynamically adjusting the sampling temperature during language model generation, leading to improved performance on tasks requiring varying levels of creativity and factual accuracy. - The core component is the Adaptive Decoder module, a small neural network added to the LLM's architecture, predicting the optimal temperature at either the token or sequence level. - Latent Preference Optimization (LPO), a novel training approach based on Direct Preference Optimization, trains the Adaptive Decoder by sampling multiple responses with varying temperatures, scoring them with a reward model, and learning to prefer temperatures associated with higher-ranked responses. - Experiments on a combined dataset (UltraMathStories) of math, creative writing, and general instructions show that Adaptive Decoding outperforms fixed temperature baselines. - Additionally, the method demonstrates success in constrained creative writing, where it learns to use low temperatures for constrained parts and higher temperatures for creative parts of the text, outperforming greedy and high-temperature baselines. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2024-11-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440) | LiYuan, sunlichao137, Yibing, Pengjin, Xkev | - This paper introduces LLaVA-01, a Vision Language Model (VLM) designed for improved multi-stage reasoning in visual question answering. - LLaVA-01 uses a structured approach with four stages: summarization, visual interpretation, logical reasoning, and conclusion generation, unlike traditional chain-of-thought prompting. - A new dataset, LLaVA-01-100k, was created with structured reasoning annotations from various visual question answering sources, used to fine-tune the Llama-3.2-11B-Vision-Instruct model. - A novel stage-level beam search method enables efficient inference-time scaling. - LLaVA-01 surpasses larger and closed-source models like Gemini-1.5-pro and GPT-40-mini on multiple multimodal reasoning benchmarks. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use](https://arxiv.org/abs/2411.10323) | Mingyu Ouyang, AnalMom, QuStar, SiyuanH | - This paper presents a preliminary case study on Claude 3.5 Computer Use, a new large language model (LLM) designed for GUI automation. - The study evaluates the model's abilities in planning, action execution, and critic feedback across various desktop environments, including web search, office productivity software, workflow applications, and video games. - The researchers propose a novel framework, Computer Use Out-of-the-Box (OOTB), which offers a cross-platform solution for easy implementation and benchmarking of GUI automation models. - The study identifies some limitations in Claude 3.5's handling of tasks, including scrolling-based navigation and text selection precision, as well as occasional inaccuracies in final outcome evaluations. - The findings suggest the need for future research to focus on improved selection capabilities, more accurate validation feedback, and enhanced adaptability to real-world application complexities. | ['Multimodal'] | [Link](https://github.com/showlab/computer_use_ootb) | N/A |


## Papers for 2024-11-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models](https://arxiv.org/abs/2411.09595) | Jun Zhu, Hang Su, Yikai Wang, Jonathan Lorraine, Zhengyi Wang | - LLaMA-Mesh is a novel approach that unifies 3D mesh generation with Large Language Models (LLMs) by representing meshes as plain text, allowing seamless integration without tokenizer or vocabulary modifications. - The model leverages the OBJ file format, treating vertex coordinates and face definitions as text sequences, enabling LLMs to process 3D data directly. - A supervised fine-tuning (SFT) dataset with text-3D pairs and interleaved dialogues is used to train a pretrained LLaMA model, enabling it to generate 3D meshes from text prompts, understand 3D meshes, and maintain conversational abilities. - LLaMA-Mesh achieves mesh generation quality comparable to specialized 3D generation models while preserving the LLMs' language capabilities, as demonstrated by qualitative and quantitative results. - The work represents a significant step towards integrating multi-modal content generation within a unified language model. | ['Text-to-3D', 'Multimodal', 'Natural Language Processing'] | N/A | N/A |
| [Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/abs/2411.09009) | Philipp Krähenbühl, Vladlen Koltun, Alexander Hertzberg, Brody Huval, erikwijmans | - This paper introduces Cut Cross-Entropy (CCE), a novel method to compute cross-entropy loss and its gradients without materializing the full logits matrix in memory. - CCE performs matrix multiplications and log-sum-exp operations within flash memory, reducing the memory footprint for loss calculation from gigabytes to megabytes. - It leverages softmax sparsity to further improve throughput by skipping negligible gradient computations. - Experiments on large language models like Gemma 2 and Llama 3 demonstrate significant memory reduction and increased batch size without impacting training speed or convergence. - The proposed method facilitates training larger language models with extended vocabulary sizes under memory constraints. | ['Natural Language Processing'] | [Link](https://github.com/apple/ml-cross-entropy) | N/A |
| [ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?](https://arxiv.org/abs/2411.06469) | Zhongwei Wan, Che Liu, Shan Chen, Jian Yu, canyuchen | - ClinicalBench, a new benchmark, was introduced to evaluate the performance of LLMs and traditional machine learning models on clinical prediction tasks. - The benchmark includes three common clinical prediction tasks (Length-of-Stay, Mortality, and Readmission), two real-world clinical databases (MIMIC-III and MIMIC-IV), 22 LLMs with varying sizes, and 11 traditional ML models. - Through empirical studies, traditional machine learning models outperformed both general-purpose and medical LLMs across all tasks and datasets, even with varying model sizes, prompting, or fine-tuning strategies. - This suggests a potential deficiency in the clinical reasoning and decision-making capabilities of current LLMs. - The benchmark and code are publicly available to facilitate further research in this domain. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3), [Link](https://huggingface.co/google/gemma-2-9b-it), [Link](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2-7B-Instruct), [Link](https://huggingface.co/01-ai/Yi-1.5-6B-Chat), [Link](https://huggingface.co/01-ai/Yi-1.5-9B-Chat), [Link](https://huggingface.co/01-ai/Yi-1.5-34B-Chat), [Link](https://huggingface.co/lmsys/vicuna-7b-v1.5), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), [Link](https://huggingface.co/internlm/internlm2_5-7b-chat), [Link](https://huggingface.co/openbmb/MiniCPM3-4B), [Link](https://huggingface.co/epfl-llm/meditron-7b), [Link](https://huggingface.co/epfl-llm/meditron-70b), [Link](https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20), [Link](https://huggingface.co/BioMistral/BioMistral-7B), [Link](https://huggingface.co/m42-health/Llama3-Med42-8B), [Link](https://huggingface.co/m42-health/Llama3-Med42-70B), [Link](https://huggingface.co/PharMolix/BioMedGPT-LM-7B), [Link](https://huggingface.co/internistai/base-7b-v0.2), [Link](https://huggingface.co/docs/transformers/en/index) |
| [Hermes: A Large Language Model Framework on the Journey to Autonomous Networks](https://arxiv.org/abs/2411.06490) | Merouane Debbah, Antonio De Domenico, Ali Maatouk, Fadhel Ayed, nicopi | - Hermes is a novel chain-of-agent LLM framework designed to automate the creation of Network Digital Twins (NDTs) using blueprints, enhancing the path towards autonomous network management. - Unlike existing NDT approaches that require distinct architectures for each use case, Hermes uses LLMs to generate step-by-step logical blocks (blueprints) for NDT construction, improving flexibility and scalability. - The framework consists of a Designer agent for creating and refining the blueprint based on network data and policies, and a Coder agent for translating the blueprint into executable Python code. - Experimental results across four autonomous network tasks demonstrated that Hermes with GPT-40 as LLM consistently outperforms chain-of-thought prompting and direct code generation, achieving success rates up to 80%. - While open-source LLMs show limited performance independently, integrating them with a library of expert-designed models significantly improves their effectiveness in NDT blueprint design, highlighting the potential for broader application. | ['Natural Language Processing'] | N/A | N/A |


## Papers for 2024-11-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Large Language Models Can Self-Improve in Long-context Reasoning](https://arxiv.org/abs/2411.08147) | Mo Yu, Lemao Liu, Zesen Cheng, Cheng Yang, Siheng99 | - This paper introduces SEALONG, a self-improving method for Large Language Models (LLMs) to enhance long-context reasoning. - SEALONG samples multiple reasoning trajectories from the LLM, scores them using Minimum Bayes Risk (MBR), and fine-tunes using either supervised learning or preference optimization. - Experiments show improvement on LLMs like Llama-3.1-8B-Instruct by 4.2 points. - SEALONG outperforms existing methods reliant on human annotations or expert models by leveraging the LLM's own generated outputs. - The results demonstrate substantial potential for LLMs to self-improve in long-context reasoning and suggests promise for further research. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/SihengLi99/SEALONG) | N/A |
| [Direct Preference Optimization Using Sparse Feature-Level Constraints](https://arxiv.org/abs/2411.07618) | Hanqi Yan, Minjun Zhu, Hongbo Zhang, Chak Tou Leong, Qingyu Yin | - This paper introduces Feature-level constrained Preference Optimization (FPO), a novel method for aligning Large Language Models (LLMs) with human preferences. - FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment, simplifying the alignment process while ensuring stability. - FPO achieves an above 5% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines on benchmark datasets such as AlpacaEval-2 and Arena-Hard. - By constraining the shifts of sparse features during training, FPO achieves results that meet or exceed the effectiveness of sequential KL divergence with lower computational cost (17.6% reduction compared to TDPO2). - FPO combines the efficiency of SimPO with the constraint quality of sequential KL, making it a promising solution for efficient and controllable LLM alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/abs/2411.08868) | Benoît Sagot, Éric de la Clergerie, Rian Touchent, Francis Kulumba, Wissam Antoun | - This paper introduces two new versions of the French language model CamemBERT: CamemBERTav2 and CamemBERTv2. - CamemBERTav2 is based on the DeBERTaV3 architecture and uses a Replaced Token Detection (RTD) training objective, while CamemBERTv2 uses the RoBERTa architecture with a Masked Language Modeling (MLM) objective. - Both models are trained on a significantly larger and more up-to-date dataset than their predecessors, with an updated tokenizer to better handle the nuances of modern French text. - Evaluations on various NLP tasks, including question answering, named entity recognition, and text classification, show that both models significantly outperform previous versions, particularly CamemBERTav2 which yielded the best performance in most cases. - All models and checkpoints are publicly available on Hugging Face. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification'] | N/A | [Link](https://huggingface.co/almanach?search_models=camembert+v2) |
| [Can sparse autoencoders be used to decompose and interpret steering vectors?](https://arxiv.org/abs/2411.08790) | Adam Mahdi, Yushi Yang, Harry Mayne | - This paper investigates why applying sparse autoencoders (SAEs) directly to steering vectors results in inaccurate decompositions, hindering interpretability. - Two primary reasons are identified: steering vectors fall outside the training distribution of SAEs, and SAEs cannot accommodate meaningful negative projections present in steering vectors. - The study uses the corrigibility steering vector as a case study, demonstrating that the SAE decomposition is largely driven by the encoder bias, overshadowing the steering vector's contribution. - Scaling the steering vector's L2-norm does not resolve the out-of-distribution issue, as default components present in model activations are absent in steering vectors due to the subtraction process involved in their creation. -  The restriction of SAEs to non-negative reconstruction coefficients leads to misinterpretations, as it overlooks negative projections in feature directions, and these negative projections can also cause spurious positive activations in other features due to negative cosine similarity between features. | ['Natural Language Processing'] | [Link](https://github.com/HarryMayne/SV_interpretability) | [Link](https://huggingface.co/google/gemma-scope-2b-pt-res/tree/main/layer_14/width_16k/average_10_173) |


## Papers for 2024-11-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) | Chengyue Wu, Wen Liu, Xiaokang Chen, Xingchao Liu, Yiyang Ma | - JanusFlow is a unified multimodal model that integrates autoregressive language models with rectified flow for both image understanding and generation tasks. - The model uses a minimalist architecture with a lightweight encoder and decoder to adapt the LLM for rectified flow operations, and employs two key strategies: decoupling understanding and generation encoders and aligning their representations during training. - On text-to-image generation, JanusFlow surpasses existing models on benchmarks like MJHQ FID-30k, GenEval, and DPG-Bench. - It also achieves state-of-the-art performance in multimodal comprehension tasks on benchmarks like MMBench, SeedBench, and GQA, exceeding specialized models. - These results are achieved with a compact 1.3B parameter LLM, showing the potential for efficient and versatile vision-language models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | [Link](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) |
| [Stronger Models are NOT Stronger Teachers for Instruction Tuning](https://arxiv.org/abs/2411.07133) | Radha Poovendran, Luyao Niu, Fengqing Jiang, Zhangchen Xu, yuchenlin | - This paper challenges the assumption that larger language models (LLMs) are always better teachers for instruction tuning of smaller LLMs, a phenomenon termed the "Larger Models' Paradox." - Through experiments across five base models and twenty response generators, they demonstrate that larger models within a model family don't always lead to better instruction-following performance in smaller models after fine-tuning. - Existing metrics for data selection, such as quality, difficulty, and response length, fail to predict response generator effectiveness because they don't account for teacher-student model compatibility. - They propose a new metric, Compatibility-Adjusted Reward (CAR), that considers both the reward of generated responses and their compatibility with the base model (measured by loss on the base model), which outperforms baseline metrics in predicting response generator effectiveness. - Open-source models like Gemma-2-9b-it and Qwen2.5-72B-Instruct were found to be highly effective as response generators, outperforming even closed-source models like GPT-4 in some cases. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-100K-Generator-Zoo) |


## Papers for 2024-11-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models](https://arxiv.org/abs/2411.07140) | Hui Huang, Yingshui Tan, Jiaheng Liu, Shilong Li, Yancheng He | - This paper introduces Chinese SimpleQA, a benchmark designed to evaluate the factuality of Large Language Models (LLMs) in answering short questions in Chinese. - The dataset consists of 3000 high-quality question-answer pairs across six diverse topics, with a focus on static answers that do not change over time. - The questions and answers in Chinese SimpleQA are short, simplifying evaluation with existing LLMs, and the grading process employs the OpenAI API. - Initial findings indicate that Chinese SimpleQA is challenging for existing LLMs, with only a few achieving passing scores. - The research demonstrates the importance of model size, calibration, and the effectiveness of Retrieval-Augmented Generation (RAG) strategies in improving LLM performance in Chinese factuality. | ['Question Answering'] | N/A | N/A |
| [IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization](https://arxiv.org/abs/2411.06208) | Yongbin Li, Fei Huang, Cheng Fu, Haiyang Yu, Xinghua Zhang | - This paper introduces IOPO (Input-Output Preference Optimization), a new alignment method for Large Language Models (LLMs) that aims to improve complex instruction following. - Unlike existing methods like DPO that focus on output preference, IOPO considers both input and output preferences, enabling LLMs to better understand fine-grained constraints within complex instructions. - A new benchmark called TRACE, comprising 120K training and 1K evaluation instances with varying constraint complexities, is also introduced to facilitate training and evaluation of complex instruction following abilities. - Experiments on TRACE, IFEval, and CFBench demonstrate IOPO's effectiveness, showing improvements of 2.18% and 3.13% over DPO on in-domain and out-of-domain datasets, respectively. - Ablation studies confirm that both input and output preference modeling are crucial for instruction following, especially in complex scenarios. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework](https://arxiv.org/abs/2411.06176) | Maojia Song, Chaoqun Liu, Hou Pong Chan, Liying Cheng, Yew Ken Chia | - Introduces M-LongDoc, a benchmark dataset with 851 samples for evaluating multimodal long document understanding. - Proposes a retrieval-aware tuning approach to improve model performance in document question answering. - Presents an automated evaluation framework leveraging multiple judge models to assess the correctness of open-ended solutions. - Demonstrates that the retrieval-aware tuning approach achieves a 4.6% relative improvement in answer correctness compared to baseline open-source models. - Finds that existing models struggle with figure and table-based questions, highlighting a multimodal bias. | ['Multimodal', 'Document Question Answering'] | N/A | N/A |
| [Counterfactual Generation from Language Models](https://arxiv.org/abs/2411.07180) | Ryan Cotterell, Anej Svete, vesteinn, Shauli | - This paper proposes a framework for generating counterfactual text from language models (LMs) by treating them as Generalized Structural Equation Models (GSEMs) and using the Gumbel-max trick. - This approach allows for modeling the joint distribution of original and counterfactual strings, enabling investigation of cause-and-effect relationships within LMs. - An algorithm based on hindsight Gumbel sampling infers the noise variables that produced an observed string, facilitating the generation of its counterfactual. - Experiments on GPT2-XL and LLaMA3-8b, with interventions like knowledge editing and linear steering, reveal unintended side effects, showing that these techniques are not as surgical as intended. - This work highlights the need for more precise intervention methods in LMs to minimize unintended changes in generated text. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/shauli-ravfogel/lm-counterfactuals) | [Link](https://huggingface.co/intfloat/e5-base-v2), [Link](https://huggingface.co/jujipotle/honest_llama3_8B_instruct), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |
| [Game-theoretic LLM: Agent Workflow for Negotiation Games](https://arxiv.org/abs/2411.05990) | Julie Chen, Alfonso Amayuelas, Lingyao Li, Ollie Liu, Wenyue Hua | - This paper introduces game-theory-inspired workflows to enhance the rationality of Large Language Models (LLMs) in strategic decision-making, particularly within negotiation games. - The authors evaluate several state-of-the-art LLMs across various complete and incomplete information games and find that LLMs often deviate from rational strategies, especially in complex games. - They design distinct workflows incorporating principles like dominant strategy search, backward induction, and Bayesian belief updating to guide LLMs' reasoning and improve decision-making. - Experimental results demonstrate that these workflows significantly improve LLM performance in identifying optimal strategies, achieving near-optimal allocations, and reducing susceptibility to exploitation during negotiations. -  The paper further explores the meta-strategic considerations of workflow adoption, highlighting a novel research direction in analyzing the rationality of using such workflows in different scenarios. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/Wenyueh/game_theory) | N/A |
| [Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models](https://arxiv.org/abs/2411.06272) | Yiyan Qi, Zhouchi Lin, Huanyi Su, Junxi Liu, Xiaojun Wu | - This paper introduces Golden Touchstone, a bilingual (English and Chinese) benchmark for evaluating Financial Large Language Models (FinLLMs). - The benchmark covers eight core financial NLP tasks, including sentiment analysis, question answering, and summarization, with 22 datasets in total.  - The authors also release Touchstone-GPT, a FinLLM trained using continuous pre-training and financial instruction tuning on a 100B token dataset and 300k instruction-response pairs. - Evaluation results on Golden Touchstone show that while existing LLMs and FinLLMs perform reasonably well on some tasks like sentiment analysis, they struggle with more complex tasks like relation extraction and stock prediction. - Touchstone-GPT shows competitive performance compared to the other models. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Summarization'] | [Link](https://github.com/IDEA-FinAI/Golden-Touchstone) | N/A |
| [Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction](https://arxiv.org/abs/2411.06424) | Adam Mahdi, Harry Mayne, Filip Sondej, Yushi Yang | - This paper investigates the internal mechanisms of Direct Preference Optimization (DPO) for toxicity reduction in language models, challenging the existing explanation that it primarily works by dampening the most toxic neurons. - By ablating toxic neurons and applying activation patching, the study finds that only 31.8% of toxicity reduction stems from dampened toxic neurons. - Instead, DPO reduces toxicity through a more complex process involving multiple neuron groups, accumulating effects by both reducing writing in the toxic direction and actively promoting anti-toxicity in the residual stream. - DPO's adjustments to neuron activations are noisy, with many neurons actually increasing toxicity, suggesting a balancing process between opposing neuron effects to achieve overall toxicity reduction. - The research provides a more nuanced understanding of DPO's mechanism, suggesting potential for targeted interventions to replicate its effects and improve safety in language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Yushi-Y/dpo-toxic-neurons) | N/A |
| [KMM: Key Frame Mask Mamba for Extended Motion Generation](https://arxiv.org/abs/2411.06481) | Feng Chen, Qi Chen, Akide Liu, Zeyu Zhang, Ha0Tang | - This paper introduces Key Frame Mask Mamba (KMM), a novel architecture for extended motion generation that addresses the challenges of memory decay and poor text-motion alignment in previous methods by strategically masking key frames based on local density and minimum distances between high density motion embeddings within the latent space. - Using a customized contrastive learning strategy to learn dynamic text embeddings, the approach improves motion-text alignment by dynamically learning text encodings and enabling the generation of more accurate and aligned motion sequences. - On the BABEL dataset, KMM achieves state-of-the-art performance with a reduction of more than 57% in FID and 70% in parameters compared to existing methods. - A newly introduced benchmark, BABEL-D, focuses on evaluating text-motion alignment for directional instructions, and demonstrates KMM’s superior performance. - User studies further confirmed the efficacy of the proposed KMM model through qualitative and quantitative analysis of the generated motions across a diversity of prompts. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |


## Papers for 2024-11-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Balancing Pipeline Parallelism with Vocabulary Parallelism](https://arxiv.org/abs/2411.05288) | Min Lin, Penghui Qi, Man Tsung Yeung, ufotalent | - This paper introduces Vocabulary Parallelism (VP), a novel technique designed to address computational and memory imbalances stemming from vocabulary layers in pipeline parallelism, a common strategy for training large language models. - VP partitions vocabulary layers across all pipeline devices and integrates the computation as passes within the pipeline schedule, similar to the handling of transformer layers.  The approach involves algorithms to reduce communication barriers within these vocabulary passes, thereby minimizing activation memory overhead. - When combined with memory-balanced pipeline schedules like V-Half, the proposed method achieves near-perfect balance in both memory and computation. - Experimental results show improvements in throughput ranging from 5% to 51% compared to existing methods, particularly under large vocabulary scenarios, with significant reductions in peak memory usage. - Notably, the benefits extend to various vocabulary and model sizes, demonstrating the robustness and generalizability of the approach. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/sail-sg/VocabularyParallelism) | N/A |
| [DELIFT: Data Efficient Language model Instruction Fine Tuning](https://arxiv.org/abs/2411.04425) | Marina Danilevksy, Lucian Popa, Krishna Killamsetty, ishikaa | - DELIFT (Data Efficient Language Model Instruction Fine-Tuning) is a novel algorithm that optimizes data selection across three key stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning. - It utilizes a pairwise utility metric to quantify the value of a data sample in improving the model's responses to other samples and leverages submodular functions for optimal subset selection. - DELIFT reduces fine-tuning data size by up to 70% without compromising performance, leading to significant computational savings.  - It outperforms current data selection techniques by up to 26% across diverse tasks and model scales. - Experiments show minimal performance drops compared to using full datasets and even surpasses full dataset performance in niche tasks such as query rewriting. | ['Natural Language Processing', 'Question Answering'] | [Link](https://anonymous.4open.science/r/optimizing-data-selection-0CD0) | N/A |
| [Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study](https://arxiv.org/abs/2411.02462) | Jingyue Li, andstor | - This paper conducts an empirical study on Parameter-Efficient Fine-Tuning (PEFT) methods for Large Language Models (LLMs) in generating unit tests for code. - It evaluates full fine-tuning, LoRA, (IA)³, and prompt tuning across various open-source LLMs (CodeGen, Code Llama, StarCoder) ranging from 350 million to 16 billion parameters and use well-established unit-test datasets for evaluation. - The findings indicate that LoRA generally performs best, matching or exceeding full fine-tuning's performance in several cases while using fewer parameters, whereas prompt tuning is the most resource-efficient but has more variable performance. - They evaluate using codebleu to evaluate the similarity between the generated tests and the reference tests. -  The results also show that both full fine-tuning and PEFT methods are mostly resistant to catastrophic forgetting, sometimes even improving the code generation capabilities. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/andstor/methods2test_small), [Link](https://huggingface.co/andstor/peft-unit-test-generation-experiments) |
| [LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation](https://arxiv.org/abs/2411.04997) | Yuqing Yang, Xufang Luo, Aoqi Wu, Weiquan Huang, Yif29 | - LLM2CLIP is a novel approach that integrates Large Language Models (LLMs) into Contrastive Language-Image Pre-training (CLIP) to enhance visual representation learning. - It addresses the limitations of LLMs' output features by applying a caption contrastive fine-tuning strategy, which increases their discriminability and enables them to act as a more powerful teacher for CLIP's visual encoder. - This approach improves CLIP's ability to handle longer and more complex captions and incorporate richer knowledge from LLMs, without significant computational overhead. - LLM2CLIP demonstrates significant improvements across various cross-modal tasks, exceeding previous state-of-the-art models like EVA02 by a substantial margin on text and image retrieval benchmarks, and even enabling cross-lingual capabilities for models trained solely on English data. - It also shows promising results in multimodal training with models like LLaVA 1.5, consistently outperforming the original CLIP across several benchmarks. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | N/A | [Link](https://aka.ms/llm2clip) |
| [Improving the detection of technical debt in Java source code with an enriched dataset](https://arxiv.org/abs/2411.05457) | Rick Kazman, Davide Di Ruscio, Phuong T. Nguyen, Anh M. T. Bui, Nam Le Hai | - This paper introduces TESORO, a new dataset for detecting technical debt (TD) in Java source code, which includes both comments and the corresponding source code. - A pipeline is proposed for enriching technical debt data by extracting Self-Admitted Technical Debts (SATD) comments and their associated source code units. - The study demonstrates that incorporating source code context enhances the performance of state-of-the-art SATD detection models, and an ensemble approach combining predictions from different code context lengths yields even better results. - The paper investigates the accuracy of different pre-trained language models (PLMs) in detecting TD solely from source code, revealing the superior performance of CodeBERT and its variant GraphCodeBERT, and highlighting the potential of LLMs in this task. - The curated TESORO dataset is expected to catalyze future research in the domain of TD detection and facilitate the identification of other software artifacts such as code smells. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/NamCyan/tesoro) | N/A |


## Papers for 2024-11-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/abs/2411.04905) | Jiaran Hao, Jason Klein Liu, Tianhao Cheng, Siming Huang, Zenithwang | - OpenCoder, a top-tier code large language model (LLM) designed for code generation, reasoning, and agent systems, is introduced, boasting performance comparable to leading models while offering full transparency through the release of its training data, processing pipeline, and protocols. - OpenCoder's key ingredients for success include code-optimized heuristic rules for data cleaning and deduplication, incorporation of code-related text corpora, and utilization of high-quality synthetic data in annealing and fine-tuning. - The model architecture for OpenCoder is available in 1.5B and 8B parameter sizes, leveraging SwiGLU activation, RoPE positional embedding, and a vocabulary size of 96,640, with variations in layers, attention heads, and context window size between the two. - OpenCoder's training involves a two-stage instruction-tuning process: the first focuses on theoretical computer science question-answer pairs, while the second refines practical coding skills using high-quality code from GitHub. - Evaluation on benchmarks like HumanEval, MBPP, BigCodeBench, LiveCodeBench, and MultiPL-E reveals OpenCoder surpasses all previous fully open models and other open-access models at the 6B+ parameter scale. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/yuxiang630/hqcode) |
| [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965) | Furu Wei, Shuming Ma, Hongyu Wang | - BitNet a4.8 introduces 4-bit activations and a hybrid quantization and sparsification strategy for 1-bit Large Language Models (LLMs), aiming to reduce inference costs while maintaining performance comparable to the 1.58-bit BitNet b1.58 model. - The model employs 4-bit activations for inputs to attention and feed-forward network layers, and sparsifies intermediate states with 8-bit quantization to mitigate quantization errors caused by outlier channels.  - It also incorporates a two-stage training approach (from 8-bit to 4-bit activations) for efficiency.  - Experimental results show BitNet a4.8 achieves comparable performance to BitNet b1.58 with equivalent training costs but faster inference due to enabling INT4/FP4 kernels and supporting 3-bit KV cache. - Additionally, BitNet a4.8 activates only 55% of the parameters, further enhancing efficiency for large-scale LLM deployment and inference. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996) | Ning Dong, Srinivasan Iyer, Liang Luo, Lili Yu, WxWx | - This paper introduces Mixture-of-Transformers (MoT), a sparse multimodal transformer architecture designed to reduce the computational costs of pretraining large multimodal models. - MoT decouples the non-embedding parameters of the model by modality, including feed-forward networks, attention matrices, and layer normalization, enabling modality-specific processing with global self-attention over the full input sequence. - In the Chameleon 7B setting (autoregressive text and image generation), MoT matches the dense baseline performance using only 55.8% of the FLOPs.  With speech added, MoT reaches comparable speech performance using 37.2% of the FLOPs.  - In the Transfusion setting, which uses multi-objective training with autoregressive text and diffusion-based image generation, a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics; a 7B MoT matches the image performance of the dense baseline with one-third of the FLOPs.  - System profiling shows MoT achieves dense baseline image quality in 47.2% of the wall-clock time and text quality in 75.6% of the wall-clock time. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Text Generation', 'Image-to-Text'] | N/A | N/A |
| [TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation](https://arxiv.org/abs/2411.04709) | Yi Yang, Wenhao Wang | - This paper introduces TIP-I2V, a large-scale dataset of over 1.7 million text and image prompts specifically designed for image-to-video generation, accompanied by generated videos from five state-of-the-art models. - The dataset is sourced from Pika Discord channels and includes additional information like UUIDs, timestamps, embeddings, subjects, and NSFW scores. - Analysis reveals that TIP-I2V's prompts, which focus on animating existing image content, differ semantically from those in text-to-video and text-to-image datasets. - This dataset enables research into user preferences, improved model evaluation, misinformation detection, and source image tracing. - Initial benchmarks using TIP-I2V indicate that even early commercial image-to-video models can outperform open-source alternatives in key areas, emphasizing the importance of real-world user data. | ['Image-to-Video', 'Multimodal', 'Computer Vision'] | [Link](https://tip-i2v.github.io) | N/A |
| [Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model](https://arxiv.org/abs/2411.04496) | Ho-Jin Choi, Kyeongjin Oh, Junyoung Youn, Dokyong Lee, Young-Jun Lee | - This paper introduces THANOS, a family of large language models (LLMs) designed to improve the quality of responses generated by conversational agents by infusing them with "skill-of-mind." - Skill-of-mind is a process that involves considering social context, interpreting dialogue situations, planning an appropriate skill strategy, and selecting the most effective conversational skill for a given response.  - The authors also created MULTIFACETED SKILL-OF-MIND, a dataset of 100k conversations annotated with explanations and conversational skills, to train THANOS.  - Experimental results indicate that THANOS effectively predicts conversational skills and enhances response quality in various scenarios, promoting prosocial behavior in human evaluations.  - This improvement is demonstrated by incorporating the generated skill-of-mind as input for LLM-based conversational agents, leading to better response quality. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/passing2961/Thanos) | N/A |
| [DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation](https://arxiv.org/abs/2411.04999) | Chris Paxton, Soumith Chintala, Mohit Warke, Zhanqiu Guo, Peiqi Liu | - DynaMem, a novel dynamic spatio-semantic memory architecture for open-world mobile manipulation, is introduced, enabling robots to adapt to changing environments. - DynaMem maintains and updates a voxelized pointcloud of the environment, incorporating object additions and removals, and supports object localization queries using both Vision-Language Model features and Multimodal Large Language Model question answering. - In real-world experiments on Stretch SE3 robots across various dynamic scenes, DynaMem achieves a 70% pick-and-drop success rate for non-stationary objects, more than double the performance of static systems. - A new dynamic benchmark, DynaBench, is introduced to evaluate dynamic spatio-semantic memory algorithms in 9 changing environments, and ablation studies demonstrate the effectiveness of key design choices. - DynaMem handles object permanence, going beyond existing systems that often return incorrect matches when the queried object is absent. | ['Robotics', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/abs/2411.05000) | Samuel Albanie, Kai Han, Jonathan Roberts | - This paper introduces a new set of retrieval experiments to evaluate the long-context capabilities of Large Language Models (LLMs), called needle threading tasks. - These tasks involve following threads of linked information across different parts of the context and retrieving the final value, including single and multiple needle retrieval, conditional needle retrieval, threading and multi-threading, and branched threading variations. - The study evaluates 17 LLMs on these tasks using synthetically generated key-value pairs of UUIDs and finds that increased context length negatively impacts performance but concurrent threading is largely unaffected by concurrent queries. - It suggests the LLMs' "effective" context limit is shorter than stated due to performance degradation at longer context lengths.   - The paper introduces a task-specific and model-agnostic effective context limit metric and publicly releases the code and experimental data. | ['Question Answering'] | N/A | N/A |
| [RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval](https://arxiv.org/abs/2411.04752) | Subhankar Maity, Aniket Deroy | - This paper introduces RetrieveGPT, a novel approach for enhancing information retrieval from code-mixed conversations, particularly focusing on Roman transliterated Bengali mixed with English. - The approach uses GPT-3.5 Turbo with carefully designed prompts to evaluate document relevance to a given query, considering the sequential nature of conversations. - A mathematical model integrates GPT-3.5 Turbo's output, accounting for sequential dependencies among documents to determine relevance. - The model treats relevance detection as a problem of finding the optimal relevance chain across a sequence of documents. - Experiments on a Facebook dataset with Query Relevance files (QRels) demonstrate the effectiveness of the approach in extracting information from complex, code-mixed conversations. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos](https://arxiv.org/abs/2411.04923) | Eric Xing, Jiale Cao, Wenqi Zhu, Hanan Gani, Shehan Munasinghe | - VideoGLaMM, a large multimodal model designed for pixel-level visual grounding in videos, is introduced. - The model architecture comprises a Large Language Model (LLM), dual vision encoders (for spatial and temporal features), and a spatio-temporal decoder connected via tunable Vision-to-Language (V→L) and Language-to-Vision (L→V) adapters.  - VideoGLaMM is trained on a new dataset of grounded conversation video question-answer triplets which include segmentation masks generated using a semi-automatic annotation pipeline.  - The new dataset includes 38k video-QA triplets, 83k objects and 671k masks.  - Experimental results demonstrate state-of-the-art performance on grounded conversation generation, visual grounding, and referring video segmentation tasks, outperforming existing approaches. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Mask Generation'] | N/A | [Link](https://mbzuai-oryx.github.io/VideoGLaMM) |


## Papers for 2024-11-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level](https://arxiv.org/abs/2411.03562) | Albert Thomas, Giuseppe Paolo, James Doran, Alexandre Maraval, Antoine Grosnit | - Introduced Agent K v1.0, an end-to-end autonomous data science agent capable of automating the entire data science lifecycle, including task setup, solution generation, and submission to Kaggle competitions. - Employs a structured reasoning framework with a memory module for experience-based learning, enabling adaptation without retraining or backpropagation. - Achieved a 92.5% success rate in automating tasks across multiple modalities (tabular, computer vision, NLP, multimodal). - Ranked in the top 38% when compared against almost 6000 human competitors on Kaggle and reached a performance equivalent to Kaggle Grandmaster, winning 6 gold, 3 silver, and 7 bronze medals across diverse challenges. - Proposed a novel evaluation methodology and competitive benchmark using real-world Kaggle competitions to rigorously assess agent capabilities. | ['Natural Language Processing', 'Tabular', 'Computer Vision', 'Multimodal', 'Image Classification', 'Tabular Classification', 'Tabular Regression', 'Time Series Forecasting'] | N/A | N/A |
| [Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination](https://arxiv.org/abs/2411.03823) | Benyou Wang, Lichao Sun, Shunian Chen, Sicheng Lai, Dingjie Song | - This paper introduces MM-Detect, a framework for detecting data contamination in Multimodal Large Language Models (MLLMs). - MM-Detect employs two methods: Option Order Sensitivity Test for multiple-choice questions and Slot Guessing for Perturbation Captions for caption-based questions.  - Experiments on eleven MLLMs and five VQA datasets reveal varying degrees of contamination across models, impacting performance.  - The study also finds that data leakage can originate from both the pre-training phase of the base LLMs and the multimodal fine-tuning phase.  -  The results indicate that MM-Detect can identify contamination and demonstrate that training set leakage leads to performance inflation, creating unfair comparisons. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](https://github.com/MLLM-Data-Contamination/MM-Detect) | N/A |


## Papers for 2024-11-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems](https://arxiv.org/abs/2411.02959) | Weipeng Chen, Mang Wang, Wen Wang, Zhicheng Dou, Jiejun Tan | - HtmlRAG is a novel Retrieval-Augmented Generation (RAG) system that utilizes HTML, instead of plain text, as the format for retrieved knowledge, aiming to preserve structural and semantic information often lost during HTML-to-text conversion. - HtmlRAG incorporates HTML cleaning, compression, and a two-step pruning process, involving an embedding model for coarse-grained pruning and a generative model for fine-grained pruning, to address challenges of long input sequences and noisy content. - This approach outperforms existing RAG systems, utilizing text-based and Markdown-based post-retrieval processes on six QA datasets, including ASQA, HotpotQA, NQ, TriviaQA, MuSiQue, and ELI5 datasets.  - Specifically, it improves exact match by up to 4.5% on the NQ Dataset and 8.7% on the MuSiQue dataset when using Llama 3.1 70B instruct model. - The results demonstrate the effectiveness of utilizing HTML for knowledge modeling in RAG systems, particularly with powerful LLMs capable of handling complex HTML structures. | ['Question Answering'] | [Link](https://github.com/plageon/HtmlRAG) | N/A |
| [LLaMo: Large Language Model-based Molecular Graph Assistant](https://arxiv.org/abs/2411.00871) | Hyunwoo J. Kim, Dohwan Ko, Minseong Bae, Jinyoung Park | - LLaMo, a Large Language Model-based Molecular graph assistant, integrates a molecular graph encoder and a large language model for instruction-following response generation in the molecular domain. - LLaMo uses a multi-level graph projector to abstract representations of each GNN layer and motif representations, bridging the graph encoder and language model. - Machine-generated molecular graph instruction data, created through a multi-turn conversation format from molecular descriptions and IUPAC names, are used for instruction tuning. - Experimental results demonstrate LLaMo's superior performance in molecular description generation, property prediction, and IUPAC name prediction, outperforming LLM-based models like GPT-4. - Ablation studies validate the contribution of the multi-level graph projector and the instruction tuning process. | ['Graph Machine Learning', 'Multimodal', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/mlvlab/LLaMo) | N/A |
| [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | Shenzhi Wang, Yizeng Han, Bingyi Kang, Yulin Wang, Yang Yue | - DeeR-VLA dynamically adjusts the size of activated Multimodal Large Language Models (MLLMs) based on situation complexity, improving computational efficiency for robotic tasks. - DeeR leverages a multi-exit MLLM architecture allowing early termination of processing once sufficient model capacity is reached for a given input, avoiding redundant computation. - The framework includes algorithms to set early-exit criteria based on predefined computational budgets (average/peak FLOPs, GPU memory), enabling adaptability to resource constraints. - A tailored training method integrates temporal information within the multi-exit architecture to ensure reasonable action predictions. - Evaluation on the CALVIN benchmark shows 5.2-6.5x reduction in LLM computational costs and 2-6x reduction in LLM GPU memory usage without compromising task performance. | ['Robotics', 'Multimodal'] | [Link](https://github.com/yueyang130/DeeR-VLA) | N/A |
| [Sample-Efficient Alignment for LLMs](https://arxiv.org/abs/2411.01493) | Min Lin, Wee Sun Lee, Chao Du, Changyu Chen, Zichen Liu | - This paper introduces SEA (Sample-Efficient Alignment), a Thompson sampling-based algorithm, for aligning Large Language Models (LLMs) with human preferences efficiently, addressing the bottleneck of extensive human feedback requirements in current alignment methods. - The approach frames LLM alignment as a contextual dueling bandit problem and emphasizes two key properties for sample efficiency: online interaction and active exploration. - SEA leverages an epistemic reward model (deep ensemble of reward models) for posterior sampling, policy-guided search for efficient response selection, and mixed preference learning (combining online user feedback and synthetic feedback from the reward model) to update the LLM policy online. - Experimental results across various model scales (1B, 2.8B, 6.9B parameters) and direct preference optimization methods (DPO, IPO, SLiC) show SEA achieves higher win rates against reference responses and significantly better sample efficiency compared to existing baselines, including passive online learning and other active exploration methods.  - The authors release `oat`, an open-source, distributed learning system designed for online LLM alignment research, aiming to facilitate further studies and fair comparisons in the field. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/sail-sg/oat) | [Link](https://huggingface.co/docs/trl/main/en/online_dpo_trainer), [Link](https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B) |
| [Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge](https://arxiv.org/abs/2411.02657) | Lashaw Salta, Chinmay Agrawal, Catalina Villouta, Andrew Langdon, ksoman | - Zebra-Llama, a context-aware large language model specializing in Ehlers-Danlos Syndrome (EDS) information, was developed using a novel context-aware fine-tuning methodology. - The model leverages Retrieval-Augmented Generation (RAG) and is trained on a diverse dataset comprising medical literature, patient forums, and clinical resources, structured as question-context-answer triplets. - Evaluation on real-world questions from EDS patients and clinicians demonstrated Zebra-Llama's superior performance compared to the base Llama model across thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%), and citation reliability (70.6% vs. 52.3%). - A custom RAG API and Jupyter Notebook demo are also released. - The model and code are open-sourced to democratize expert-level knowledge in rare disease management. | ['Question Answering', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/karthiksoman/zebra-llama) | [Link](https://huggingface.co/zebraLLAMA/zebra-Llama-v0.2) |
| [Controlling Language and Diffusion Models by Transporting Activations](https://arxiv.org/abs/2410.23054) | Nicholas Apostoloff, Luca Zappella, Michal Klein, Arno Blaas, Pau Rodriguez | - This paper introduces Activation Transport (ACT), a framework to steer activations in generative models (GMs) using optimal transport theory. - ACT generalizes existing activation steering methods by applying univariate maps to activations while preserving target distributions, improving controllability and robustness.  - Linear-ACT, an inference-time intervention based on ACT, matches or outperforms other methods in toxicity mitigation, concept induction, and truthfulness in LLMs. - ACT effectively controls text-to-image diffusion models for fine-grained style control and concept negation.  - The authors adapt ITI (Li et al., 2024) for text-to-image and find that ACT with a strength parameter of 1 consistently achieves strong conditioning across tasks and models. | ['Natural Language Processing', 'Text-to-Image', 'Text Generation'] | N/A | N/A |


## Papers for 2024-11-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models](https://arxiv.org/abs/2411.00836) | Bin Hu, Junyu Zhang, Xingang Guo, Chengke Zou, Ray2333 | - DYNAMATH, a dynamic visual benchmark, is introduced to evaluate the robustness of Vision Language Models (VLMs) in mathematical reasoning. - The benchmark consists of 501 seed questions represented as Python programs, enabling automatic generation of diverse concrete questions with variations in visual and textual content. - An evaluation of 14 state-of-the-art VLMs on 5,010 generated questions revealed a significant gap between average-case and worst-case accuracy, indicating current VLMs' lack of robustness in handling question variations. - The analysis also found high repetition consistency in many models, suggesting that incorrect answers on certain variants are due to consistent errors rather than inherent randomness. - DYNAMATH provides insights to guide development of more robust VLMs and the paper suggests using adversarial training or reinforcement learning from human feedback with fine-grained process rewards as potential improvement strategies. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265) | Jiaqi Zhu, Xingwu Sun, Ruobing-Xie, Mimosa77, YanfengChen | - Tencent introduces Hunyuan-Large, a 389 billion parameter (52 billion activated) open-source Mixture-of-Experts (MoE) model based on the Transformer architecture and capable of handling up to 256K tokens. - The model outperforms LLama3.1-70B on various benchmarks, including language understanding, generation, logical reasoning, mathematics, coding, and long-context tasks, and exhibits performance comparable to the much larger LLama3.1-405B model. - Key innovations include using large-scale synthetic data, a mixed expert routing strategy combining shared and specialized experts with recycle routing for discarded tokens, KV cache compression by grouped-query attention and cross-layer attention, and an expert-specific learning rate scaling strategy. - The model is pre-trained on 7 trillion tokens, including 1.5 trillion synthetic tokens, followed by post-training stages involving supervised fine-tuning and reinforcement learning from human feedback using direct preference optimization. - Both pre-trained and post-trained versions of Hunyuan-Large are released to the open-source community. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Tencent/Tencent-Hunyuan-Large) | [Link](https://huggingface.co/tencent/Tencent-Hunyuan-Large) |
| [Survey of Cultural Awareness in Language Models: Text and Beyond](https://arxiv.org/abs/2411.00860) | Junho Myung, Arnav Arora, Junyeong Park, jinjh0123, sidicity | - This paper surveys efforts to incorporate cultural awareness into text-based and multimodal large language models (LLMs). - It defines cultural awareness in LLMs based on definitions from psychology and anthropology and examines methodologies for creating cross-cultural datasets and benchmarks, strategies for cultural inclusion in downstream tasks, and benchmarks for evaluating cultural awareness in LLMs. - The survey also discusses ethical implications of cultural alignment, the role of Human-Computer Interaction, and cultural alignment's role in social science research. - The paper identifies research gaps in current literature and provides suggestions for future research in areas such as cross-cultural LLMs and automatic context detection. - It organizes and compares efforts in incorporating culture into NLP and spans several modalities like image, video, audio and text. | ['Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models](https://arxiv.org/abs/2411.00918) | Quang Pham, Van Nguyen, Luong Tran, doantienthongbku, DavidNguyen | - This paper introduces LibMoE, a comprehensive and modular framework designed to streamline the research, training, and evaluation of Mixture of Experts (MoE) algorithms in Large Language Models (LLMs). - LibMoE facilitates easier access to MoE research for a wider range of researchers by standardizing the training and evaluation process, and by reducing the computational cost via sparse upcycling from pre-trained LLMs. - The authors benchmark five state-of-the-art MoE algorithms with three model configurations across eleven datasets under a zero-shot setting. - Results show that all MoE algorithms achieve roughly similar performance when averaged across a variety of tasks. - Further analysis suggests the potential benefits of early stopping and the importance of balanced expert utilization in MoE models. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/abs/2411.02335) | Chaojun Xiao, Yingfa Chen, Chenyang Song, Yuqi Luo, SillyXu | - This paper investigates activation sparsity in Large Language Models (LLMs), proposing a new metric called PPL-p% sparsity. - PPL-p% sparsity is performance-aware, versatile across activation functions, and precisely identifies weakly contributing neurons, improving upon existing metrics like CETT. - Through extensive experiments, the research reveals scaling laws relating activation sparsity to training data, activation function, width-depth ratio, and parameter scale. - ReLU activation is found to be superior to SiLU due to greater sparsity and comparable performance, with deeper models exhibiting higher sparsity below a certain bottleneck. - Notably, the limit of activation sparsity shows weak correlation with parameter scale, suggesting that activation patterns in LLMs are scale-insensitive. | ['Natural Language Processing'] | [Link](https://github.com/thunlp/SparsingLaw) | N/A |
| [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747) | Ryan A. Rossi, Seunghyun Yoon, Viet Dac Lai, Dang Nguyen, Franck-Dernoncourt | - DynaSaur is a novel LLM agent framework that dynamically creates and composes actions, represented as Python functions, enabling the agent to operate beyond a predefined action set. - At each step, the agent generates Python code to perform an action, accumulating these generated actions for reuse in future steps, enhancing flexibility and efficiency. - This framework outperforms existing methods on the GAIA benchmark, demonstrating its effectiveness in complex, long-horizon tasks. - Notably, DynaSaur allows the agent to recover from scenarios where the predefined action set is insufficient or existing actions fail due to unforeseen circumstances. - The dynamic action creation and accumulation capabilities enable the LLM agent to interact with various tools and systems, enhancing its ability to solve a diverse range of tasks. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/adobe-research/dynasaur) | N/A |
| [Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models](https://arxiv.org/abs/2411.00743) | Virginia Smith, Mona Diab, Aashiq Muhamed | - Introduces Specialized Sparse Autoencoders (SSAEs), which are designed to capture rare or infrequent features (tail concepts) within specific domains of foundation models. - Employs dense retrieval and TracIn reranking as effective methods for selecting training data relevant to the target domain, enabling targeted feature extraction without needing to scale to billions of features. - Utilizes Tilted Empirical Risk Minimization (TERM) as a training objective, demonstrating its effectiveness in enhancing tail concept representation in SSAEs compared to standard Empirical Risk Minimization (ERM). - Demonstrates through experiments on the Bias in Bios dataset that SSAEs improve interpretability by capturing rare features and significantly increase worst-group classification accuracy (12.5%) when used to remove spurious gender information. - Evaluation on downstream perplexity and Lo sparsity metrics shows SSAEs effectively capture domain-specific tail concepts, outperforming standard SAEs trained on general-purpose data. | ['Natural Language Processing', 'Feature Extraction'] | N/A | N/A |
| [Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks](https://arxiv.org/abs/2411.01192) | Muhammad Abdul-Mageed, Fakhraddin Alwajih, Abdellah El Mekki, El Moatez Billah Nagoudi, Gagan Bhatia | - This paper introduces Swan, a family of dialect-aware, Arabic-centric, cross-lingual, and cross-cultural embedding models. - Swan includes Swan-Small, based on ARBERTv2, and Swan-Large, based on the pretrained Arabic large language model ArMistral. - A new comprehensive benchmark suite, ArabicMTEB, is proposed to evaluate the models, covering eight tasks and 94 datasets, including cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance. - Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks while maintaining monetary efficiency. - Swan-Small also shows strong performance, consistently surpassing Multilingual-E5-base on most Arabic tasks. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | N/A | N/A |


## Papers for 2024-11-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OS-ATLAS: A Foundation Action Model for Generalist GUI Agents](https://arxiv.org/abs/2410.23218) | Fangzhi Xu, Zhenyu Wu, Zhiyong Wu, heroding77, QiushiSun | - This paper introduces OS-Atlas, a large action model designed for generalist GUI agents, focusing on GUI grounding and out-of-distribution (OOD) generalization. - It leverages a novel multi-platform data synthesis toolkit, enabling the creation of a 13 million GUI element dataset spanning Windows, macOS, Linux, Android, and web interfaces. - OS-Atlas employs a two-stage training process: GUI grounding pre-training on the large dataset and action fine-tuning on existing agent datasets with a unified action space to mitigate conflicts. - Evaluations across six benchmarks and three platforms (mobile, desktop, web) show significant performance improvements over state-of-the-art models. - OS-Atlas-Base, the pre-trained model, serves as an open-source alternative to commercial VLMs for building GUI agents, achieving comparable performance in some settings. | ['Multimodal'] | N/A | N/A |
| [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/abs/2411.00412) | Leon Bergen, Duncan Watson-Parris, Yadi Cao, yuqirose, Bohan22 | - This paper introduces a novel two-stage training method called Adapting While Learning (AWL) to enhance Large Language Models (LLMs) for solving scientific problems by incorporating tool usage and direct reasoning. - AWL consists of World Knowledge Distillation (WKD) which fine-tunes LLMs to internalize domain knowledge from solutions generated using tools and Tool Usage Adaptation (TUA) which trains LLMs to choose between direct answering and tool usage based on problem complexity. - Evaluation across six scientific benchmarks demonstrate average improvements of 28.18% in answer accuracy and 13.89% in tool usage precision compared to baselines and state-of-the-art models such as GPT-4 and Claude-3.5. - The model surpasses existing approaches on custom datasets that include complex and specialized scientific questions not commonly seen during pre-training. - It also showcases improved robustness in noisy data scenarios and adaptability to open-ended questions through integration with preference learning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027) | Yijia Shao, Branislav Kveton, Ryan A. Rossi, Zhehao Zhang, Franck-Dernoncourt | - This survey paper provides a comprehensive overview of personalized Large Language Models (LLMs), unifying research on personalized text generation and downstream task personalization. - It introduces a taxonomy for personalized LLM usage, formalizing foundations, and analyzing personalization granularity (user-level, persona-level, global preference). - The paper surveys techniques for personalization, including retrieval-augmented generation, prompting, representation learning, and reinforcement learning from human feedback (RLHF). - It also covers evaluation metrics and datasets for personalized LLMs, along with various applications like AI assistants, recommendation systems, and search engines. - Finally, it discusses open problems such as benchmarks, cold-start issues, bias, privacy, and multimodality in personalized LLMs. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation](https://arxiv.org/abs/2410.21157) | Shukai Liu, Jian Yang, Congnan Liu, Ken Deng, Jiaheng Liu | - This paper introduces M2RC-EVAL, a massively multilingual repository-level code completion benchmark encompassing 18 programming languages. - It offers two types of fine-grained annotations: bucket-level (based on abstract syntax tree depth) and semantic-level (categorizing completion scenarios). - The authors also present M2RC-INSTRUCT, a multilingual instruction dataset designed to improve repository-level code completion abilities in LLMs. - Experimental results show that incorporating cross-file context and fine-tuning on M2RC-INSTRUCT significantly enhances performance across various languages. - Code Llama with fine-tuning outperforms non-finetuned StarCoder after fine-tuning.  | ['Natural Language Processing', 'Text2Text Generation', 'Text Generation'] | [Link](https://github.com/M2RC-Eval-Team/M2RC-Eval) | N/A |
| [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030) | Pierre-François Marteau, Nicolas Béchet, Danrun Cao | - This paper introduces WikiNER-fr-gold, a manually revised gold-standard version of the French portion of the WikiNER corpus, aimed at improving the quality of Named Entity Recognition (NER) resources. - The corpus consists of 20% of the original WikiNER-fr (26,818 sentences, ~700k tokens), randomly sampled and corrected for inconsistencies in annotation stemming from the semi-supervised nature of the original WikiNER. - The correction process focused on standardizing entity boundaries and categories, resolving ambiguous hyperlinks, and addressing inconsistencies in the application of annotation guidelines. - The authors analyzed the errors in the silver-standard WikiNER-fr, categorized them, and described the correction strategies employed. - The paper also discusses future work, including a broader assessment of entity categorization and potential automation of the correction process for the remaining WikiNER data. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |
| [GRS-QA -- Graph Reasoning-Structured Question Answering Dataset](https://arxiv.org/abs/2411.00369) | Jincen Shuai, Devasha Trivedi, Anish Pahilajani, Franck-Dernoncourt, namyongp | - This paper introduces GRS-QA, a new multi-hop question answering dataset that includes explicit reasoning structures in the form of graphs for enhanced reasoning analysis of LLMs. - Unlike existing datasets that lack clear reasoning pathways, GRS-QA captures intricate structures by constructing reasoning graphs, where nodes denote textual contexts and edges signify logical flow. - GRS-QA offers benefits such as providing transparent reasoning steps for answer derivation, allowing fine-grained evaluation of LLM reasoning capabilities across diverse structures. - It also includes negative reasoning graphs, created by perturbing the structure of positive graphs, to isolate the impact of reasoning structures compared to content on question answering performance. - The authors benchmark state-of-the-art models on GRS-QA from retrieval, direct question answering, and retrieval-augmented generation perspectives, revealing that LLM performance degrades with increasing reasoning complexity and highlighting the importance of GRS-QA in pushing the limits of current QA models. | ['Question Answering', 'Natural Language Processing', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-11-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743) | Tianyi Zhou, Yanhong Li, MingLiiii | - This research paper investigates the layer-wise gradient patterns in Large Language Models (LLMs) during instruction-tuning with different reasoning approaches (fast vs. slow thinking) and response types. - It uses spectral analysis, specifically Singular Value Decomposition (SVD) and nuclear norm, to characterize gradient behaviors across LLM layers for tasks involving math, commonsense reasoning, and knowledge learning. - Slow thinking, using detailed Chain-of-Thought (CoT), results in more stable and uniform gradient norms across layers compared to fast thinking, suggesting improved learning stability. - The gradients associated with slow thinking effectively differentiate correct from irrelevant responses in reasoning tasks, while in knowledge learning tasks, gradient norms are sensitive to knowledge popularity but not correctness.  - The study also finds that instruction-tuned LLMs do not show significant advantages over pre-trained LLMs in identifying incorrect reasoning and have different gradient patterns for fast thinking responses, suggesting challenges in aligning with the instruction-tuning data. | ['Natural Language Processing'] | [Link](https://github.com/MingLiiii/Layer_Gradient) | N/A |
| [A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents](https://arxiv.org/abs/2410.22476) | Pawan Goyal, Gajula Sai Chaitanya, Abhilash Nandy, Sombit Bose, Ankan Mullick | - This paper introduces MLMCID, a pointer network-based architecture for joint extraction and detection of multi-label multi-class intents in task-oriented dialogue systems. - The MLMCID model uses an encoder-decoder framework with a pointer network and LSTM-based sequence generator to identify multiple intent spans within a sentence, along with their corresponding coarse- and fine-grained intent labels. - A new multilingual multi-label intent dataset (MLMCID-dataset) is also created from existing benchmark datasets.  - The model outperforms baseline approaches, including large language models (LLMs) like Llama2 and GPT, on various MLMCID datasets in terms of accuracy and F1-score. -  The approach is also effective in few-shot settings and demonstrates the importance of multi-intent modeling for real-world conversational AI. | ['Natural Language Processing', 'Text Classification', 'Question Answering'] | [Link](https://github.com/ankan2/multi-intent-pointer-network) | N/A |
| [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175) | Lei Hou, Bin Xu, Xiaozhi Wang, Hao Peng, Yunjia Qi | - This paper introduces constraint back-translation, a novel data generation technique for improving complex instruction following in Large Language Models (LLMs). - The technique involves taking existing instruction-response pairs and using an LLM to generate constraints that are already implicitly satisfied by the response.  - This method is used to create CRAB, a high-quality complex instruction-response dataset. - The method improves the performance of LLMs on complex instruction-following tasks, as measured by IFEval and FollowBench benchmarks. - It also serves as a useful auxiliary training objective during post-training by enhancing the model's understanding of complex constraints. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Language Models can Self-Lengthen to Generate Long Texts](https://arxiv.org/abs/2410.23933) | Dayiheng Liu, An Yang, Bowen Yu, Tianyi Tang, Shanghaoran Quan | - This paper introduces Self-Lengthen, an iterative training framework to improve the long text generation capabilities of Large Language Models (LLMs). - Self-Lengthen employs two roles: a Generator to produce initial responses and an Extender to lengthen these responses iteratively. - This method leverages the intrinsic knowledge of LLMs without needing additional data or proprietary models, addressing the training gap in current LLMs for long text generation. - Experimental results on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long text generation using open-source LLMs like Qwen2 and LLaMA3. - Notably, Self-Lengthen enhances the output length while preserving the quality, boosting output from 1,000 words to 8,000 in Qwen2.5. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/QwenLM/Self-Lengthen) | N/A |
| [BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays](https://arxiv.org/abs/2410.21969) | Xinxing Xu, Sicong Leng, Yanyu Xu, Tan Li Hui Faith, youngzhou12 | - BenchX, a unified benchmark framework, is proposed for evaluating Medical Vision-Language Pretraining (MedVLP) methods on chest X-rays, enabling head-to-head comparison and systematic analysis. - BenchX comprises three components: comprehensive datasets covering nine datasets and four medical tasks, benchmark suites to standardize data preprocessing and experimental setups, and unified fine-tuning protocols that accommodate heterogeneous MedVLP methods. - Baselines for nine state-of-the-art MedVLP methods are established using BenchX, revealing that some early methods can outperform recent ones with proper training strategies. - In particular, MGCA and MRM consistently demonstrate strong performance in most cases.  - MedCLIP-ViT also delivers good performance on multi-label image classification tasks. | ['Computer Vision', 'Image Classification', 'Image Segmentation', 'Image-to-Text', 'Multimodal'] | [Link](https://github.com/yangzhou12/BenchX) | N/A |
| [BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments](https://arxiv.org/abs/2410.23918) | Yunhua Zhou, Dong Zhang, Bo Wang, Pengyu Wang, Xinghao Wang | BitStack is a novel, training-free weight compression approach for large language models (LLMs) that enables megabyte-level trade-offs between memory usage and model performance.  It achieves this through iterative weight decomposition and considers parameter significance, resulting in approximately 1-bit per parameter residual blocks.  These blocks are sorted and stacked for dynamic loading based on memory availability.  Extensive experiments show BitStack matches or surpasses existing quantization baselines across various tasks, particularly at extreme compression ratios. | ['Natural Language Processing'] | [Link](https://github.com/xinghaow99/BitStack) | N/A |
| [Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks](https://arxiv.org/abs/2410.24032) | Qingwei Lin, Jue Zhang, Zhiyang Zhang, Xiaoting Qin, Yingzhe Peng | The paper introduces CARE, a collaborative chatbot system for personalized exploratory tasks that combines a multi-agent LLM framework with a structured UI.  CARE addresses limitations of existing LLM chatbots by extracting both explicit and implicit user needs through iterative query refinement and dynamic solution generation. A within-subject user study with 22 participants showed CARE was consistently preferred over a baseline LLM chatbot, reducing cognitive load and inspiring creativity.  CARE transforms LLM-based systems from passive information retrievers into proactive partners in personalized problem-solving and exploration. The study also revealed CARE's impact on facilitating better user experiences in complex tasks, by improving solution comprehensiveness and personalization. | ['Natural Language Processing'] | [Link](null) | [Link](null) |


## Papers for 2024-10-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation](https://arxiv.org/abs/2410.23090) | Hongjin Qian, Ziliang Zhao, Kelong Mao, dongguanting, ariya2357 | - This paper introduces CORAL, a large-scale benchmark designed to evaluate Retrieval-Augmented Generation (RAG) systems in multi-turn conversational settings. - CORAL is derived from Wikipedia, containing 8,000 information-seeking conversations covering various topics with citation labels. - It includes three tasks: passage retrieval, response generation, and citation labeling and proposes a framework to standardize different RAG methods. - Evaluations show that fine-tuned open-source LLMs outperform commercial closed-source LLMs in retrieval, and that input length reduction maintains response quality and improves citation accuracy. - The benchmark addresses challenges in multi-turn conversational RAG, such as redundant information and topic shifts, paving the way for evaluating and improving multi-turn conversational RAG systems. | ['Question Answering'] | [Link](https://github.com/Ariya12138/CORAL) | N/A |
| [Stealing User Prompts from Mixture of Experts](https://arxiv.org/abs/2410.22884) | Nicholas Carlini, Jamie Hayes, Ilia Shumailov, Itay Yona | - This paper introduces a novel attack, called MoE Tiebreak Leakage, which exploits a vulnerability in Mixture-of-Experts (MoE) models to extract user prompts. - The attack leverages the Expert-Choice-Routing (ECR) strategy, manipulating the order of inputs within a batch to cause predictable token dropping, thereby revealing information about a victim's prompt. - The authors successfully demonstrate the attack on a two-layer Mixtral model, extracting almost all secret messages (996/1000) across varying lengths (1-11 tokens), and achieving 99.9% success in recovering individual tokens (4833/4838). - The attack's complexity is O(VM²) for the number of queries to the target model and O(2DNVM²) for queries to a local model copy, where V is vocabulary size, M is prompt length, D is the number of layers, and N is the number of experts. - The paper discusses potential mitigations, including preserving in-batch data independence and introducing stochasticity into the model's routing or capacity parameters. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels](https://arxiv.org/abs/2410.20050) | Xiao Zhou, Xiangxu Zhang, Lei Li, zl101 | - This paper introduces SL-HyDE (Self-Learning Hypothetical Document Embeddings), a novel approach for zero-shot medical information retrieval that eliminates the need for labeled data. - SL-HyDE leverages LLMs to generate hypothetical documents from user queries, and utilizes a retriever to find relevant documents based on these hypothetical documents. - It employs a self-learning mechanism to enhance both LLM document generation and retriever performance without relying on labeled medical data. - A new benchmark for Chinese Medical Information Retrieval (CMIRB) is introduced, consisting of five tasks and ten datasets derived from real-world scenarios. - Experimental results on CMIRB show SL-HyDE surpasses HYDE by 4.9% in NDCG@10 and demonstrates performance gains across various LLM and retriever combinations. | ['Question Answering'] | [Link](https://github.com/CMIRB-benchmark/CMIRB) | N/A |
| [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://arxiv.org/abs/2410.23168) | Jan Eric Lenssen, Yongqin Xian, Muhammad Ferjad Naeem, Yue Fan, Haiyang Wang | - TokenFormer, a novel fully attention-driven neural network architecture, is introduced, which treats model parameters as tokens, enhancing flexibility in token-parameter computations. - By utilizing a cross-attention mechanism between input tokens and learnable parameter tokens, TokenFormer allows for scaling model parameters without altering input or output dimensions, enabling progressive scaling by adding new key-value parameter pairs. - This approach facilitates efficient scaling by reusing pre-trained models, thereby significantly reducing training costs compared to training large transformer models from scratch. - Experimental results demonstrate that TokenFormer achieves comparable perplexity to Transformers trained from scratch on language modeling tasks while substantially reducing the training budget, and maintains similar performance in visual modeling and zero-shot classification tasks. - TokenFormer offers controllable computational costs for long-context modeling, preserves learned distributions during scaling, and shows potential for integration into Mixture-of-Experts frameworks and parameter-efficient tuning strategies. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Zero-Shot Classification'] | [Link](https://github.com/Haiyang-W/TokenFormer) | N/A |


## Papers for 2024-10-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CLEAR: Character Unlearning in Textual and Visual Modalities](https://arxiv.org/abs/2410.18057) | Denis Bobkov, Boris Mikheev, Alexey Zhavoronkin, Dmitrii Korzh, therem | - Introduces CLEAR, a multimodal benchmark for evaluating machine unlearning (MU) in textual and visual modalities, focusing on removing information about specific individuals. - The benchmark includes a synthetic dataset of 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs, along with real-world face and visual question answering datasets for evaluating model retention. - Evaluates 10 existing MU methods adapted for multimodal unlearning, revealing that current state-of-the-art algorithms struggle in multimodal settings. - Demonstrates that simple L1 regularization on LoRA adapter weights during unlearning significantly mitigates catastrophic forgetting, improving the preservation of model performance on retained data. - Makes the dataset publicly available to encourage further research in the field. | ['Multimodal', 'Computer Vision', 'Natural Language Processing', 'Image-to-Text', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/therem/CLEAR) |
| [SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization](https://arxiv.org/abs/2410.21411) | Chuang Gan, Donglai Wei, Jiawei Zhou, zmeng0116, EthanTaylor | - SocialGPT is a novel modular framework that leverages Vision Foundation Models (VFMs) and Large Language Models (LLMs) for social relation reasoning. - It employs VFMs to translate image content into a textual "social story" and utilizes LLMs for reasoning based on the generated story and provided bounding boxes. - This framework incorporates systematic design principles to enhance the collaboration between VFMs and LLMs, including comprehensive and domain-specific visual information extraction and a structured reasoning prompt named SocialPrompt. - SocialGPT achieves competitive zero-shot performance on PIPA and PISC datasets, outperforming previous state-of-the-art supervised methods on PIPA by 1.4%. - The framework also introduces Greedy Segment Prompt Optimization (GSPO) for automatic prompt tuning, resulting in significant performance improvements across various LLMs. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Classification'] | [Link](https://github.com/Mengzibin/SocialGPT) | N/A |
| [OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization](https://arxiv.org/abs/2410.19609) | Hongming Zhang, Wenhao Yu, Kaixin Ma, Wenlin Yao, Hongliang He | - This paper introduces OpenWebVoyager, an open-source framework for building multimodal web agents that can explore real-world websites, receive feedback, and iteratively optimize their performance. - The agent architecture adapts the Idefics2-8b-instruct model, processing observations consisting of webpage screenshots and accessibility trees. -  OpenWebVoyager combines imitation learning from a GPT-40 powered web agent with an exploration-feedback-optimization cycle, where GPT-40 evaluates the agent's trajectory success. - Across multiple iterations on the WebVoyager and Mind2Web datasets, the agent shows improvement in task success rate, starting from 19.9% to 25.8% on the WebVoyager test set and 6.3% to 19.6% on the Mind2Web cross-task test set. - The results indicate that the iterative real-world exploration and optimization method is an effective way to improve the agent's real-world performance. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/MinorJerry/OpenWebVoyager) | N/A |
| [Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning](https://arxiv.org/abs/2410.22304) | Paul Mineiro, ydeng9 | - This paper introduces Flow-DPO, a novel approach to improve Large Language Model (LLM) mathematical reasoning by generating high-quality reasoning traces through online multi-agent learning flows. - The method employs an incremental output production flow composed of multiple LLMs that iteratively communicate to construct solutions, similar to a multi-agent system. - The flow is trained using online Direct Preference Optimization (DPO) with rollouts, generating DPO pairs for each training example during answer chunk generation and updating the models in real-time. - Experimental results on MetaMath, GSM8K, and MATH datasets demonstrate that Flow-DPO generates higher-quality reasoning traces compared to direct model inference, leading to improved performance in mathematical reasoning tasks after supervised fine-tuning. - This improvement is particularly significant for the Llama-3-8B-instruct model, achieving a 20% improvement in validation accuracy on mathematical reasoning tasks during training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465) | Ningxin Zheng, Size Zheng, Wenlei Bao, Li-Wen Chang, preminstrel | - SHADOWKV is a novel LLM inference system designed for enhanced throughput in long-context scenarios by storing a low-rank representation of the key cache on the GPU and offloading the value cache to the CPU. - It employs a precise KV selection strategy during decoding, utilizing landmarks and static outliers to minimize the sparse KV cache budget while maintaining accuracy. - Evaluations on benchmarks like RULER, LongBench, and Needle in a Haystack with various LLMs (Llama, GLM, Yi, Phi, Qwen) show that SHADOWKV can handle contexts up to 1M tokens. - It achieves up to a 6x increase in batch size and a 3.04x boost in throughput compared to full attention on an A100 GPU. - SHADOWKV's performance even surpasses the theoretical throughput of infinite batch size with full attention, assuming infinite GPU memory. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/bytedance/ShadowKV) | N/A |


## Papers for 2024-10-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation](https://arxiv.org/abs/2410.18565) | Remek, adgw, djstrong, lflis, chrisociepa | - This paper introduces Bielik 7B v0.1, a 7-billion parameter generative text model based on the Mistral 7B v0.1 architecture and trained on a curated Polish corpora. - The model utilizes techniques such as Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, and incorporates architectural features like Sliding Window Attention and SwiGLU activation function for enhanced performance. - To evaluate the model, new benchmark frameworks, the Open PL LLM Leaderboard and Polish MT-Bench, were created for assessing NLP tasks and conversational abilities. - Bielik 7B v0.1 showed a significant improvement of 9 percentage points in the RAG Reader task compared to Mistral-7B-v0.1. - In subjective conversational evaluations, Bielik outperformed models with higher average scores on the Open PL LLM Leaderboard benchmarks. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/spaces/speakleash/mt-bench-pl), [Link](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), [Link](https://huggingface.co/datasets/teknium/OpenHermes-2.5), [Link](https://huggingface.co/spaces/speakleash/open_pl_llm_leaderboard) |
| [AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant](https://arxiv.org/abs/2410.18603) | Fangzhi Xu, Qiushi Sun, Zhuohang Dang, Minnan Luo, Chengyou Jia | - AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents, has been introduced for automating diverse computer tasks. - It leverages a novel MetaAgent with an AgentToken strategy, enabling efficient management of diverse agents by representing each agent as a learnable token embedding and predicting the appropriate token(s) for task execution. - AgentStore allows for seamless third-party agent integration, enabling adaptability to evolving operating systems. - Evaluation on OSWorld and a mobile environment demonstrate its ability to improve performance in automating computer tasks, achieving a success rate of 23.85% on OSWorld—more than double the previous best (11.21%). - AgentStore's ability to integrate agents and specialize them for specific tasks while maintaining general capabilities demonstrates significant improvement over single generalist or specialized agents in handling complex tasks within real-world OS environments. | ['Multimodal'] | N/A | N/A |
| [GPT-4o System Card](https://arxiv.org/abs/2410.21276) | Adam Perelman, Adam P. Goucher, Adam Lerer, Aaron Hurst, OpenAI |  - OpenAI's GPT-40 is an "omni" autoregressive model that accepts and generates combinations of text, audio, image, and video, trained end-to-end across these modalities. - GPT-40 matches GPT-4 Turbo's performance on English text and code, surpasses it in non-English languages, and demonstrates significant improvement on vision and audio understanding. - The model's training data includes publicly available data, code and math data, multimodal data (images, audio, and video), and proprietary data from partnerships, with a cutoff date of October 2023. - Prior to deployment, OpenAI performed risk assessments and mitigations with methods including safety classifiers, content filtering, and preference alignment to reduce harms such as information hazards, bias, and policy violations. - Deployment preparation encompassed a four-phased external red teaming process with over 100 participants to evaluate risks and test mitigations across multiple modalities and potential harms such as disallowed content and misinformation. | ['Multimodal', 'Any-to-Any', 'Audio', 'Automatic Speech Recognition', 'Text-to-Speech', 'Text-to-Audio', 'Computer Vision', 'Image-to-Text', 'Image Classification', 'Object Detection', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169) | Zhengren Wang, Junyuan Zhang, Bin Wang, Victor Shea-Jay Huang, Qintong Zhang |  - This survey paper provides a comprehensive overview of document parsing, consolidating recent advancements in modular pipeline systems and end-to-end models driven by large vision-language models (VLMs) and covering key methodologies, challenges, and future research directions. - The paper discusses core document parsing components, including layout detection, content extraction (text, tables, mathematical expressions), and multimodal data integration, examining algorithms for each stage. - It addresses the challenges faced by modular document parsing systems and VLMs in handling complex layouts, integrating modules, and recognizing high-density text. - The survey consolidates widely used datasets and evaluation metrics for document parsing tasks, providing valuable resources for researchers and practitioners. - Finally, the paper emphasizes the importance of developing larger, more diverse datasets and outlines future research directions in the field, such as handling complex layouts and improving OCR for densely packed text. | ['Natural Language Processing', 'Document Question Answering', 'Computer Vision', 'Object Detection'] | N/A | N/A |
| [LongReward: Improving Long-context Large Language Models with AI Feedback](https://arxiv.org/abs/2410.21252) | Zhenyu Hou, Shulin Cao, Xin Lv, Zhongni Hou, Jiajie Zhang | - LongReward, a novel method to improve long-context large language models (LLMs) using AI feedback, is introduced. - It uses an off-the-shelf LLM to assign rewards to model responses based on four dimensions: helpfulness, logicality, faithfulness, and completeness. - When combined with the reinforcement learning algorithm Direct Preference Optimization (DPO), LongReward significantly boosts the performance of long-context SFT models, outperforming baseline methods. - Experiments show improvements on long-context question answering and summarization and a positive impact on short instruction following. - LongReward enhances model capabilities by mitigating common issues like hallucinations and ineffective context utilization in long-context scenarios. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/THUDM/LongReward) | N/A |
| [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011) | Samyadeep Basu, Yu Xia, Ryan Aponte, Xuan Shen, Chien Van Nguyen | - This paper presents a comprehensive survey of Small Language Models (SLMs), focusing on architectures, training techniques, and model compression methods. - The authors introduce a novel taxonomy to categorize SLM optimization methods, considering techniques used in pre-processing, training, post-processing, and the constraints being optimized (e.g., inference compute, training time). - The survey covers lightweight architectures, efficient self-attention approximations, neural architecture search for model building, efficient pre-training and fine-tuning strategies, and model compression techniques like pruning, quantization, and knowledge distillation. - Additionally, it summarizes benchmark datasets and evaluation metrics commonly used for assessing SLM performance and lists various real-world applications enabled by SLMs, categorized by constraints like real-time interaction, content generation, edge inference, and privacy. - Lastly, the paper highlights important open challenges and future research directions for SLMs, such as hallucination, bias, inference-time energy efficiency, and data privacy. | ['Natural Language Processing'] | N/A | N/A |
| [COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training](https://arxiv.org/abs/2410.19313) | Kurt Keutzer, Yao Lu, Ligeng Zhu, Han Cai, Haocheng Xi | - COAT is a novel FP8 training framework designed to reduce memory footprint and increase training speed for large models by compressing both optimizer states and activations. - It introduces Dynamic Range Expansion, aligning optimizer state distributions with FP8's range, thereby minimizing quantization error. - For activations, COAT proposes Mixed-Granularity Activation Quantization, using fine-grained quantization for non-linear layers and per-tensor quantization for linear layers. - COAT achieves nearly lossless performance while decreasing memory by 1.54x and increasing training speed by 1.43x on Llama 7B, 13B, and 30B models compared to BF16. - COAT facilitates training larger models on fewer GPUs by enabling full-parameter training of 7B models on a single GPU and supports doubling the micro-batch size for distributed training. | ['Natural Language Processing', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/NVlabs/COAT) | N/A |
| [Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines](https://arxiv.org/abs/2410.21220) | Xiangyu Yue, Xiaohan Ding, Yiyuan Zhang, Zhixin Zhang | - Vision Search Assistant, a novel framework to address the limitation of traditional methods in understanding unfamiliar visual content.  - The framework facilitates collaboration between VLMs and web agents, leveraging VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. - By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system.  - It involves Visual Content Formulation to represent visual content with correlated formulations, Web Knowledge Search with Chain of Search algorithm to obtain comprehensive web knowledge, and Collaborative Generation to generate the final answer.  - Extensive experiments on open-set and closed-set QA benchmarks demonstrate that Vision Search Assistant significantly outperforms other models. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/cnzzx/VSA) | [Link](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard), [Link](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b) |
| [Fast Best-of-N Decoding via Speculative Rejection](https://arxiv.org/abs/2410.20290) | Jiahao Qiu, Huitao Yang, Ruiqi Zhang, Momin Haider, Hanshi Sun | - This paper introduces Speculative Rejection, a novel inference-time alignment algorithm designed to improve the efficiency of Best-of-N decoding for large language models (LLMs). - The key idea is to dynamically reduce the batch size during generation by halting the generation of unpromising responses early, based on partial reward scores. - The algorithm starts with a large batch size, effectively simulating Best-of-N with large N and leverages a reward model to rank partial utterances and terminate low-scoring ones. - The results on the AlpacaFarm dataset demonstrate that Speculative Rejection can achieve higher rewards with similar latency while requiring significantly fewer GPUs (16-32 times less compute power) compared to standard Best-of-N.  - The method is also shown to be effective in maximizing the probability of generated utterances. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Zanette-Labs/SpeculativeRejection) | N/A |
| [Language Models And A Second Opinion Use Case: The Pocket Professional](https://arxiv.org/abs/2410.20636) | David Noever |  - This research assesses Large Language Models (LLMs) as second opinion tools in complex medical and legal scenarios. - Evaluated LLM performance on 183 medical cases from Medscape and 21 Supreme Court legal cases, comparing responses with crowd-sourced physician opinions and documented legal votes respectively. - Found high accuracy in straightforward medical cases (>81%) but reduced performance (43%) in complex scenarios, suggesting LLMs may be valuable for generating differential diagnoses rather than as primary diagnostic tools. - Developed novel benchmarks for others to assess the reliability of responses by both LLMs and human practitioners, revealing high contestation among human experts. - Suggests that using LLMs as specialized agents for second opinions in medicine, especially in challenging cases, might be more appropriate than current approaches that emphasize automation of routine tasks. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/reveondivad/certify) | N/A |


## Papers for 2024-10-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting](https://arxiv.org/abs/2410.17856) | Xiaojian Ma, Zhancun Mu, Zihao Wang, kevinLian, phython96 | ROCKET-1 is a novel, low-level policy that leverages visual-temporal context prompting, a communication protocol using object segmentation masks and interaction types from past and present observations to guide policy-environment interactions. ROCKET-1 uses a causal transformer architecture that processes observations and object segmentations concatenated into a 4-channel image along with interaction types as conditions. Experiments in Minecraft demonstrate that agents using this approach achieve higher success rates on complex tasks, outperforming methods based on language, future image, or latent code prompting. A backward trajectory relabeling method efficiently generates segmentation annotations, enabling automatic dataset creation for training ROCKET-1. The approach allows for spatial understanding in embodied decision-making, leading to agents accomplishing previously unattainable tasks like “place oak door on diamond block” with a 91% success rate and others requiring long-horizon planning such as obtaining obsidian with a 70% success rate. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [Continuous Speech Synthesis using per-token Latent Diffusion](https://arxiv.org/abs/2410.16048) | Hagai Aronowitz, Slava Shechtman, Arnon Turetzky, Avihu, NimrodShabtay1986 |  - This paper introduces SALAD, a per-token latent diffusion model for zero-shot text-to-speech that operates on continuous representations, inspired by the per-token diffusion head for image generation. - It extends the image generation method to handle variable-length outputs, uses semantic tokens for context and stopping conditions, and doesn't require text-audio alignment. - Three SALAD variants are proposed: T2A (Text2Acoustic), S2A-AR (Semantic2Acoustic Autoregressive), and S2A-NAR (Semantic2Acoustic Non-Autoregressive), along with corresponding discrete baseline models for comparison. -  Evaluations on speech quality, intelligibility, and speaker similarity show that SALAD's T2A model achieves the highest intelligibility score. -  It also maintains speech quality and speaker similarity comparable to ground-truth audio based on subjective listening tests. | ['Text-to-Speech', 'Audio'] | N/A | N/A |
| [Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data](https://arxiv.org/abs/2410.18558) | Jialing Zhang, Shuhao Gu, ZacLiu, bowen92, ldwang | {- Introduced Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through quality filtering and deduplication.  - Proposed a synthetic instruction generation method using open-source VLMs, detailed image annotations, and diverse question generation to improve data quality and scale.  - Trained Aquila-VL-2B, a 2-billion parameter VLM based on the LLaVA-OneVision architecture, using Infinity-MM and synthetic data.  - Aquila-VL-2B achieved state-of-the-art performance for models of similar scale on various visual benchmarks, including MMBench, MMStar, and MathVista.  - Demonstrated that scaling instruction data and generating synthetic data can significantly improve the performance of open-source multimodal models.} | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main/scripts/train) | N/A |
| [Teach Multimodal LLMs to Comprehend Electrocardiographic Images](https://arxiv.org/abs/2410.19008) | Ping Zhang, Xiang Yue, Yuelin Bai, Ruoqi Liu | - This paper introduces PULSE, a new Multimodal Large Language Model (MLLM) tailored for electrocardiogram (ECG) image comprehension.  - It also presents ECGInstruct, a new instruction tuning dataset of over one million ECG image-text samples featuring realistic image synthesis and a diverse range of ECG-related tasks.   - A new evaluation benchmark, ECGBench, covering four key ECG image interpretation tasks across nine different datasets is also constructed.  - PULSE achieves state-of-the-art results, significantly outperforming proprietary MLLMs such as GPT-40 by 15-30% accuracy on out-of-domain datasets. - Ablation studies highlight the importance of diverse data sources and incorporating instruction tasks for ECG image comprehension. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image-to-Text', 'Computer Vision'] | N/A | [Link](https://aimedlab.github.io/PULSE/) |
| [MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark](https://arxiv.org/abs/2410.19168) | Ramaneswaran Selvakumar, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, S Sakshi | - MMAU, a massive multi-task audio understanding and reasoning benchmark, is introduced to evaluate expert-level reasoning and knowledge retrieval abilities in Large Audio-Language Models (LALMs).  - It consists of 10,000 expertly annotated audio-question-response pairs across speech, sound, and music domains, covering 27 distinct tasks, including 16 for reasoning and 11 for information extraction.  - Evaluations of 18 open-source and proprietary LALMs reveal that even the best-performing model only achieves 53% accuracy on MMAU, with human performance at 82%, highlighting significant room for improvement. - Models performed best on sound-based tasks but struggled the most with music. Cascaded models employing audio captioning followed by an LLM achieved the best performance, suggesting the potential for independent advancements in audio perception and text-based reasoning. - A detailed error analysis highlights perceptual errors as the most common mistake, emphasizing the need for better audio processing capabilities in current models. | ['Audio', 'Multimodal'] | N/A | N/A |
| [Counting Ability of Large Language Models and Impact of Tokenization](https://arxiv.org/abs/2410.19730) | Chenyu You, Juntai Cao, Wyattz23 | This paper investigates the impact of tokenization on the counting abilities of Large Language Models (LLMs), demonstrating that tokenization choices significantly influence model performance on counting tasks. - The study adopts a model-agnostic approach, manipulating input string formats to control tokenization in both open and closed-source LLMs. - It is observed that byte-pair encoding (BPE), commonly used in LLMs, can severely degrade counting accuracy due to a mismatch between the unit being counted (letters) and the unit processed (tokens). - The research reveals that Chain-of-Thought (CoT) prompting significantly improves counting abilities by enabling iterative inductive reasoning in the text space, partially overcoming the inherent limitations of Transformer models in sequential computations. - Through extensive experiments, the study finds that clear item-separated tokenization, as opposed to letter-grouped tokenization, enhances counting accuracy. Furthermore, the experiments showed that lower-frequency characters are easier to count compared to higher-frequency ones. | ['Natural Language Processing'] | N/A | N/A |
| [Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning](https://arxiv.org/abs/2410.19290) | Yang Zhang, Tommi Jaakkola, code-terminator, yujianll | - PREREQ-TUNE, a novel fine-tuning strategy designed to mitigate LLM hallucinations, is introduced. - PREREQ-TUNE incorporates a two-stage process: a prerequisite learning stage where a knowledge LoRA is trained to acquire necessary knowledge, followed by a supervised fine-tuning (SFT) stage where a skill LoRA focuses solely on learning task-specific skills.  The prerequisite learning stage enhances factuality by equipping the LLM with the required knowledge for subsequent fine-tuning, thereby reducing reliance on generating incorrect information. - The method also utilizes fictitious synthetic data for multi-version training, further improving the grounding of LLM outputs to internal knowledge.  This decoupling of knowledge and skill learning allows for more robust factual generation and control. - Experiments on long-form generation (biography and medical QA) and short QA tasks demonstrate PREREQ-TUNE's superior performance compared to baselines, including those utilizing reinforcement learning and direct preference optimization. - Analysis confirms the effectiveness of PREREQ-TUNE's disentanglement mechanism, even when trained solely on fictitious data, opening possibilities for new retrieval augmented generation (RAG) paradigms and knowledge-controlled text generation. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/UCSB-NLP-Chang/Prereq_tune.git) | N/A |
| [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133) | Valentina Pyatkin, Sachin Kumar, Yanai Elazar, Yizhong Wang, ljvmiranda921 |  - This paper introduces a routing framework for preference learning that dynamically allocates instances to either human or LM annotators, creating a hybrid approach to data annotation. - The framework employs a performance prediction model (PPM) to estimate the performance of reward models trained on different mixes of human and LM annotations and uses this to strategically select an optimal combination. - Results on the MULTIPREF dataset and others show that the proposed hybrid preference approach significantly outperforms using either human or LM preferences exclusively, as well as random combinations, across several benchmarks. - Analysis of the framework highlights that instances with moderate semantic similarity, safety concerns, or intent complexity tend to benefit the most from human annotation. - The authors release the code, data, and annotation platform used to promote further research in efficient and effective preference data collection. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/allenai/hybrid-preferences) | [Link](https://hf.co/datasets/allenai/multipref) |
| [Reflection-Bench: probing AI intelligence with reflection](https://arxiv.org/abs/2410.16270) | Yan Teng, Shuqi Kong, Haiquan Zhao, Yixu Wang, LingyuLi | This paper introduces Reflection-Bench, a new benchmark designed to evaluate the reflection capabilities of Large Language Models (LLMs). - Reflection is defined as the ability of an intelligent system to adapt its beliefs or behaviors in response to unexpected outcomes, encompassing core cognitive functions such as perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. - The benchmark comprises seven tasks adapted from cognitive science paradigms, including the oddball paradigm, n-back task, probabilistic reversal learning task, Wisconsin card sorting test, weather prediction task, double-choice Iowa gambling task, and meta-bandit task. - An evaluation of 13 prominent LLMs reveals that current models still fall short of human-level reflection abilities, particularly lacking meta-reflection capabilities.  - The authors argue that reflection is a crucial aspect of intelligence and propose Reflection-Bench as a valuable tool for evaluating and furthering the development of more sophisticated AI systems. | ['Natural Language Processing'] | [Link](https://github.com/YabYum/ReflectionBench) | N/A |


## Papers for 2024-10-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss](https://arxiv.org/abs/2410.17243) | Kehan Li, Hang Zhang, LidongBing, Zhiqiang007, ClownRat | - Inf-CL, a novel tile-based contrastive loss implementation, is introduced to address the GPU memory limitations in scaling batch sizes for contrastive learning.  - By partitioning the log-sum-exp (LSE) calculation into smaller tiles and iteratively accumulating the LSE term, Inf-CL avoids full materialization of the similarity matrix, significantly reducing memory overhead and enabling training with near-infinite batch sizes. - A multi-level tiling strategy further enhances practical efficiency by distributing computations across multiple GPUs with ring-based communication and within each GPU across CUDA cores with fused kernels. - Experimental results show that Inf-CL achieves unprecedented batch sizes (e.g., 12M for CLIP-ViT-L/14 on 32 A800 80GB GPUs) without sacrificing accuracy. - Compared to state-of-the-art memory-efficient solutions, Inf-CL demonstrates a two-order-of-magnitude reduction in memory while maintaining comparable speed. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification'] | [Link](https://github.com/DAMO-NLP-SG/Inf-CLIP) | N/A |
| [LOGO -- Long cOntext aliGnment via efficient preference Optimization](https://arxiv.org/abs/2410.18533) | Min Zhang, Qiaoming Zhu, Zechen Sun, douvleplus, ZetangForward | This paper introduces LOGO (Long cOntext aliGnment via efficient preference Optimization), a novel training strategy to enhance the generation ability of Long-Context Models (LCMs) and address issues like hallucinations and instruction unfollowing. - LOGO employs reference-free preference optimization, guiding the model to distinguish between preferred and dis-preferred outputs, and a data construction pipeline leveraging open-source models. - It incorporates a position synthesis method, enabling training with a substantial 0.3B dataset on a single 8xA800 GPU within 16 hours. - Experimental results on LongBench show that LOGO significantly improves LCM performance, outperforming existing methods and approaching top closed-source models like GPT-4. - LOGO effectively scales context window size for short-context models and maintains performance on short-context tasks like MMLU, indicating its adaptability and minimal alignment tax. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ZetangForward/LCM_Stack.git) | [Link](https://huggingface.co/datasets/namespace-Pt/long-llm-data) |
| [Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch](https://arxiv.org/abs/2410.18693) | Qiaoming Zhu, Xiaobo Liang, douvleplus, XinyuShi, dyyyyyyyy | - ScaleQuest, a novel data synthesis method to generate large-scale question-answer pairs by leveraging "small-sized" open-source LLMs. - The method uses a two-stage question-tuning process of Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to improve question quality. - A filtering process for language clarity, solvability, and appropriate difficulty is used along with reward-based filtering for high-quality responses. - Experiments on a dataset of 1 million math problem-solution pairs show improvements of 29.2% to 46.4% on MATH benchmark across mainstream open-source models, outperforming existing datasets and models like GPT-4-Turbo and Claude 3.5. - The data synthesis method also proves to be cost-effective, with 10x reduced cost as compared to GPT-40. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/yyDing1/ScaleQuest) | N/A |
| [Can Knowledge Editing Really Correct Hallucinations?](https://arxiv.org/abs/2410.16251) | kaishu666, apayani, XiongxiaoXu, canyuchen, BaixHuang |  - This paper introduces HalluEditBench, a benchmark designed to evaluate the effectiveness of knowledge editing methods in correcting hallucinations generated by Large Language Models (LLMs). - The benchmark includes a new dataset of over 6,000 verified hallucinations across 9 domains and 26 topics, collected from Llama2-7B, Llama3-8B, and Mistral-v0.3-7B. -  HalluEditBench assesses knowledge editing methods across five dimensions: Efficacy, Generalization, Portability, Locality, and Robustness, offering a more comprehensive evaluation compared to existing datasets. - The evaluation reveals that existing methods struggle with generalization, portability, and robustness, despite showing high performance on standard knowledge editing datasets. For instance, while FT-M and MEMIT achieve near-perfect scores on existing datasets, their efficacy in correcting real-world hallucinations is significantly lower. -  ICE and GRACE show superior performance in correcting hallucinations but have limitations in other aspects, particularly robustness, suggesting that the efficacy of current knowledge editing techniques is highly dependent on domains and LLMs and requires further research. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Unbounded: A Generative Infinite Game of Character Life Simulation](https://arxiv.org/abs/2410.18975) | flavoredquark, mohitbansal, davejacobs, NealWadhwa, yzli |  - UNBOUNDED, a generative infinite game transcending finite, hard-coded video game systems by integrating generative AI models. - It simulates character life in open-ended virtual worlds inspired by sandbox and digital pet games, incorporating unconstrained storytelling of tabletop RPGs. - It uses a specialized, distilled LLM for dynamic game mechanics, narrative, character interactions and IP-Adapter for consistent character visuals across environments. - Evaluations showed improvement in character simulation, instruction following, narrative coherence, and visual consistency compared to traditional related approaches, as well as real-time interactivity (refreshing every second). - It also features a novel regional image prompt adapter that allows consistent and flexible visual generation of character in various environments. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Distill Visual Chart Reasoning Ability from LLMs to MLLMs](https://arxiv.org/abs/2410.18798) | zifeishan, cnxup, zh2001, WooooDyy, hewei2001 |  - This paper introduces Code-as-Intermediary Translation (CIT), a method to improve visual reasoning in Multimodal Large Language Models (MLLMs) by using code to translate visual charts into text, which is then used by LLMs to generate and answer complex questions about the charts. - The authors construct REACHQA, a dataset with 3k reasoning-intensive charts and 20k question-answer pairs, using CIT and leveraging LLMs for data synthesis. - Experiments demonstrate that fine-tuning MLLMs on REACHQA enhances their performance on chart-related benchmarks, improving LLaVA-Next-Llama3-8B by over 30% on average and notably transferring abilities to general mathematical reasoning tasks like MathVista. - The study also suggests that expert rationales distilled from stronger LLMs significantly impact reasoning abilities, and the balance between recognition- and reasoning-oriented data influences model performance. - This work provides valuable insights into improving and evaluating multimodal reasoning in LLMs through the innovative use of code as an intermediary representation. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/hewei2001/ReachQA) | N/A |
| [Why Does the Effective Context Length of LLMs Fall Short?](https://arxiv.org/abs/2410.18745) | Shansan Gong, Lei Li, Ming Zhong, Jun Zhang, Chenxin An | - This paper introduces ShifTed Rotray position embeddING (STRING), a training-free method to improve the effective context length of Large Language Models (LLMs). - STRING addresses the issue of left-skewed position frequency distribution in LLMs by shifting well-trained position indices to overwrite less effective ones during inference. - This allows LLMs to better capture distant information within their existing training lengths, improving long-range dependency modeling. - Experimental results show STRING boosts the performance of LLMs like Llama 3.1 70B and Qwen-2 72B by a significant margin on benchmarks like RULER and InfiniteBench, achieving state-of-the-art results for open-source LLMs. - Notably, Llama 3.1 70B with STRING outperforms commercial models like GPT-4-128K and surpasses Claude 2 and Kimi-chat. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/HKUNLP/STRING) | N/A |
| [Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs](https://arxiv.org/abs/2410.18451) | Jujie He, Rui Yan, Jiacai Liu, zengliangcs, chrisliu298 | - The paper introduces Skywork-Reward, a collection of data-centric methods for enhancing reward modeling in LLMs, along with a new dataset called Skywork-Reward, consisting of 80K curated preference pairs from public sources.  - Skywork-Reward data collection focuses on important domains for RLHF optimization, such as math and code, using a smaller, higher-quality data composition compared to larger datasets like Preference 700K.  - The paper details data selection and filtering strategies designed to prioritize pairs that effectively improve model performance, focusing on maximizing the margin between preferred and rejected responses during training.  - This work also explores various loss functions and finds that the vanilla Bradley-Terry loss consistently outperforms other options.  - As of October 2024, the resulting Skywork-Reward model series holds the top position on the RewardBench leaderboard, demonstrating the effectiveness of their approach. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/collections/Skywork/skywork-reward-model-66d7fbdebae0e60d00a6b60d), [Link](https://huggingface.co/collections/Skywork/skywork-reward-data-collection-66d7fda6a5098dc77035336d) |
| [MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms](https://arxiv.org/abs/2410.18977) | Lei Zhang, Shunlin Lu, Xuan Ju, Wenxun Dai, Ling-Hao Chen | MotionCLR is an attention-based motion diffusion model for interactive human motion generation and editing.  The model architecture is U-Net-like and consists of CLR blocks containing convolutional, self-attention, cross-attention, and feed-forward network layers. MotionCLR supports training-free motion editing including motion (de-)emphasizing, in-place motion replacement, style transfer and example-based motion generation by manipulating self- and cross-attention activations. Experimental results on the HumanML3D dataset demonstrate comparable generation performance to state-of-the-art methods, along with improved explainability and editing control. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Should We Really Edit Language Models? On the Evaluation of Edited Language Models](https://arxiv.org/abs/2410.18785) | Zeyu Li, Peijie Dong, Zhenheng Tang, Qi Li, Dominic789654 |  - This paper evaluates the impact of different model editing methods on the general abilities of Large Language Models (LLMs). - The study finds that existing editing methods lead to inevitable performance deterioration on general benchmarks, especially when the number of edits increases. - The research also reveals that instruction-tuned models are more robust to editing and that larger models are more resistant compared to smaller models. - Additionally, the study finds that editing can significantly weaken the safety of LLMs, even for safety-aligned models. - The results suggest that current editing methods are only suitable for small-scale knowledge updates, motivating further research on more practical and reliable editing methods. | ['Natural Language Processing'] | [Link](https://github.com/lqinfdim/EditingEvaluation) | N/A |
| [ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning](https://arxiv.org/abs/2410.17779) | Han Hu, Yong Luo, Li Shen, Jianyuan Guo, Zhiwei840 | - ADEM-VL is an efficient vision-language (VL) tuning framework based on pre-trained large language models (LLMs) that uses a parameter-free cross-attention mechanism for multimodal fusion. - This approach reduces trainable parameters and improves training and inference speed by embedding visual features into the language space and utilizing multiscale visual feature generation. - An adaptive fusion scheme dynamically discards less relevant visual information based on attention scores, allowing the model to concentrate on more pertinent visual features. - The model outperforms existing methods on ScienceQA by 0.77% with average accuracy and demonstrates comparable performance on image captioning tasks. - The framework suggests more efficient VL model development by utilizing intermediate-layer fusion. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Image Feature Extraction'] | [Link](https://github.com/Hao840/ADEM-VL) | N/A |
| [CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models](https://arxiv.org/abs/2410.18505) | Xiaofeng Shi, Hanyu Zhao, Chengwei Wu, Bo-Wen Zhang, ldwang |   - This paper introduces CCI3.0-HQ, a 500GB high-quality subset of the Chinese Corpora Internet 3.0 (CCI3.0) designed for pre-training large language models (LLMs). - The dataset was created using a novel two-stage hybrid filtering approach: 1. Fundamental Processing(safety filtering, text extraction, deduplication, and initial quality assessment) 2. High-Quality Processing (employs Qwen2-72B-Instruct to identify high-quality samples and train a smaller 0.5B classifier to filter the dataset). - A 0.5B parameter model trained from scratch on CCI3.0-HQ using 100B tokens achieved superior performance on 10 benchmarks compared to CCI3.0, SkyPile, and WanjuanV1 in zero-shot settings.  - The introduced quality classifier (CCI3-HQ) also outperforms existing classifiers like FineWeb-edu, IndustryCorpus2, and ChineseWebText in terms of F1 score. - The dataset and the classifier are open-sourced to benefit the community in developing high-quality Chinese LLMs. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/CCI3-HQ) | [Link](https://huggingface.co/datasets/BAAI/CCI3-HQ), [Link](https://huggingface.co/datasets/BAAI/CCI3-Data), [Link](https://huggingface.co/BAAI/CCI3-HQ-Classifier) |
| [CAMEL-Bench: A Comprehensive Arabic LMM Benchmark](https://arxiv.org/abs/2410.18976) | Ines Riahi, Ali Alharthi, Omkar Thawakar, Sara Ghaboura, ahmedheakl | CAMEL-Bench is a comprehensive Arabic LMM benchmark comprising eight diverse domains and 38 sub-domains, including multi-image understanding, complex visual perception, and video understanding. - It contains around 29,036 questions filtered from a larger pool of samples, and quality is manually verified by native speakers. - Evaluations of both closed-source, including GPT-4 series, and open-source LMMs were conducted. - GPT-4o achieved an overall score of 62%, revealing a need for improvement, especially among the best open-source models. - Closed-source models generally outperformed open-source models in most tests. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [WAFFLE: Multi-Modal Model for Automated Front-End Development](https://arxiv.org/abs/2410.18362) | Lin Tan, Shangshu Qian, jiang719, shanchao | - WAFFLE is a new fine-tuning strategy for Multi-modal Large Language Models (MLLMs) designed to automate front-end development by generating HTML code from UI design images. - It incorporates a structure-aware attention mechanism, enabling MLLMs to better understand HTML structure and a contrastive fine-tuning approach to align the visual understanding of UI designs with the generated HTML code. - The evaluation on WebSight-Test shows improvements of up to +9.00 percentage points in HTML Match, +0.0982 in CW-SSIM, +32.99 in CLIP, and +27.12 percentage points in LLEM. - Similar improvements are observed on Design2Code, another benchmark, demonstrating WAFFLE's effectiveness in bridging the gap between visual UI designs and text-based HTML/CSS code. - WAFFLE, as a fine-tuning method, is model-agnostic and therefore applicable to any MLLMs. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/lt-asset/Waffle) | N/A |
| [Language Models are Symbolic Learners in Arithmetic](https://arxiv.org/abs/2410.15580) | Hanjie Chen, Ruidi Chang, Roy Xie, Zhiqi Li, Chunyuan Deng | This paper investigates how Large Language Models (LLMs) learn arithmetic, specifically focusing on whether they leverage partial products during calculations and how they approach the task symbolically. - It finds that LLMs struggle to leverage partial products to solve multiplications and suggests that improvements in recognizing them arise from their symbol-learning process, not actual partial product calculation. - By decomposing arithmetic tasks into subgroups based on token-level analysis, the paper finds that LLMs treat a collection of different arithmetic operations similarly when subgroup complexity is fixed. - Through position-level accuracy analysis, it's observed that LLM learning follows a U-shaped curve, initially and finally performing well on easy patterns but struggling with harder patterns in between. - Overall, the study concludes that LLMs do not perform true calculations during arithmetic tasks. Rather, they act as symbolic learners by selecting subgroups based on complexity, which provides a novel framework for understanding these models' approach to arithmetic reasoning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Taipan: Efficient and Expressive State Space Language Models with Selective Attention](https://arxiv.org/abs/2410.18572) | Hanieh Deilamsalehy, Ruiyi Zhang, Thang M. Pham, Huy Huu Nguyen, chiennv |  - Taipan, a hybrid architecture for efficient long-context language modeling, combines the efficiency of Mamba-2 with Selective Attention Layers (SALs) to enhance long-range dependency handling. - SALs strategically select tokens requiring long-range interactions, refine their features, and augment them with attention, balancing efficiency and expressiveness. - Taipan scales to billions of parameters and demonstrates superior performance on various tasks, including zero-shot language modeling and memory-intensive tasks like in-context retrieval. - It achieves linear memory scaling, making it applicable for contexts up to 1 million tokens, and significantly outperforms Transformers and other SSM-based models on long sequences. - The ablation study emphasizes the importance of the attention budget and the absence of positional embeddings for efficient and enhanced generalization. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus) |
| [Value Residual Learning For Alleviating Attention Concentration In Transformers](https://arxiv.org/abs/2410.17897) | Zhenzhong Lan, Zhiyun Jiang, Tianyi Wu, Zcchill | • This paper introduces two novel Transformer variants: ResFormer and SVFormer. • ResFormer incorporates a residual connection from the first layer’s value embeddings to subsequent layers’ value embeddings to mitigate attention concentration, which is defined as a model’s attention increasingly focuses on fewer tokens as the network depth increases. • SVFormer shares the first layer’s value embeddings across all layers, reducing KV cache by approximately 50%. • Experimental results on a 20B SlimPajama dataset show ResFormer outperforms vanilla Transformer, DenseFormer, and NeuTRENO in training and downstream tasks. • SVFormer is shown to train faster than vanilla Transformer and perform better than GQA and CLA when sequence length is longer. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Zcchill/Value-Residual-Learning) | [Link](https://huggingface.co/datasets/cerebras/SlimPajama-627B) |
| [Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits](https://arxiv.org/abs/2410.18234) | Roland Memisevic, Arash Behboodi, Hassan Dbouk, Ashish Khisti, mamaj92 | This paper introduces a canonical architecture for multi-draft speculative sampling, where multiple draft models independently generate proposal token sequences. - It demonstrates that the optimal draft selection scheme can be achieved through a two-step process: importance sampling to select an intermediate token and single-draft speculative sampling on the selected token. - For two identical draft models, an analytical expression for optimal acceptance probability is derived, along with a necessary and sufficient condition for achieving an acceptance probability of 1. - A new token selection scheme based on weighted importance sampling is proposed, along with heuristic approaches to reduce computational complexity. - Experimental results on OPT models across various tasks show consistent improvements in block efficiency and token rates compared to baseline methods, especially when using non-identical draft distributions. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |


## Papers for 2024-10-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models](https://arxiv.org/abs/2410.17637) | conghui, KennyUTC, yhcao, yuhangzang, ziyuliu | **- MIA-DPO: a novel Multi-Image Augmented Direct Preference Optimization (DPO) framework, designed to enhance the multi-image understanding of Large Vision-Language Models (LVLMs).** **- MIA-DPO addresses the scarcity of diverse multi-image training data and high annotation costs by augmenting existing single-image data with noisy or unrelated images arranged in grid collages or pic-in-pic formats, reducing the need for manual annotation of multi-image data.** **- This framework leverages an attention-aware selection mechanism that filters out rejected responses by analyzing the attention value distribution across multiple images, allowing for automated, cost-effective, and scalable DPO data construction without relying on manual annotations or expensive APIs.** **- Experimental results demonstrate that MIA-DPO consistently outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on InternLM-XC2.5.** **- MIA-DPO maintains competitive performance on single-image tasks while boosting the performance on multi-image tasks, demonstrating its robustness across various architectures and its ability to handle both single and multiple images effectively.** | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Liuziyu77/MIA-DPO) | N/A |
| [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891) | Jiacheng Ye, Yizhe Zhang, kiaia, shivamag99, Sansa |  - This paper introduces a novel approach to scaling Diffusion Language Models (DLMs) by adapting pre-trained autoregressive (AR) language models like GPT2 and LLaMA. - The adaptation method bridges the gap between AR and DLM objectives through attention mask annealing to remove causal masking bias and inheriting the shift operation from AR models. - The resulting models, DiffuGPT and DiffuLLaMA (up to 7B parameters), are trained on less than 200B tokens and evaluated on various benchmarks, demonstrating competitive performance with their AR counterparts and state-of-the-art results among existing DLMs. - DiffuLLaMA showcases promising in-context learning and infilling abilities. - The models and training code are released to facilitate further DLM research.   | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/HKUNLP/DiffuLLaMA) | N/A |
| [Lightweight Neural App Control](https://arxiv.org/abs/2410.17883) | Jianye Hao, ShaoKun-HW, Fahren24, gpap, semitable |  - This paper introduces Lightweight Multi-modal App Control (LiMAC), a novel mobile phone control architecture designed for efficient interaction and control across various Android apps.  - LiMAC combines a small Action Transformer (AcT) with a fine-tuned vision-language model (VLM) to process textual goals and past mobile observations (screenshots, UI trees) and generate precise actions.  -  AcT predicts action types (click, scroll, input text) and executes straightforward interactions, while the VLM handles complex text generation tasks (composing messages, search queries).   - Experimental results on two mobile control datasets show LiMAC significantly outperforms fine-tuned open-source VLMs (Florence2, Qwen2-VL) and prompt engineering baselines using GPT-40, increasing overall action accuracy by up to 19% and 42% respectively.   - LiMAC also executes tasks 30 times faster than GPT-40 methods, making it more suitable for real-time mobile applications. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [MedINST: Meta Dataset of Biomedical Instructions](https://arxiv.org/abs/2410.13458) | Zirui Song, Yu Yin, Zihan Zhang, Meng Fang, Wenhan Han | • This paper introduces MEDINST, a large biomedical instruction meta-dataset comprising 133 tasks and over 7 million training examples spanning 12 distinct categories. • The authors curate MEDINST32, a benchmark derived from MEDINST consisting of 32 tasks with varying difficulty to evaluate large language models' (LLMs) generalization abilities in the biomedical domain.  • Several LLMs are fine-tuned on MEDINST and show improved generalization performance across various biomedical tasks, as evaluated on MEDINST32.  • The study finds that instruction fine-tuning is more effective than further pre-training on domain-specific data for adapting general LLMs to the biomedical domain. • Experimental results on MEDINST32 reveal that the models often struggle with generalization to new tasks when only fine-tuned on smaller datasets or in limited task formats, highlighting the value of large, comprehensive datasets. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification', 'Summarization', 'Translation'] | [Link](https://github.com/aialt/MedINST) | N/A |
| [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/abs/2410.15522) | Drishti Sharma, Rishabh Maheshwary, Lester James V. Miranda, shayekh, srishti-hf1110 | • This paper introduces M-REWARDBENCH, a multilingual benchmark for evaluating reward models (RMs) across 23 languages and six tasks. • M-REWARDBENCH consists of 2.87k preference instances covering chat, safety, reasoning, and translation capabilities. • Evaluation results show a significant performance gap between English and non-English languages, with RMs exhibiting higher performance in English and variations across different languages. • The analysis indicates that translation quality positively impacts RM performance, with better translations leading to improved accuracy. • The authors also explore the sensitivity of different RM types to translation quality and analyze the performance variations across different language families and scripts. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://hf.co/datasets/C4AI-Community/multilingual-reward-bench) |
| [TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts](https://arxiv.org/abs/2410.18071) | Tianhua Li, Yuxuan Xie, kpzhang, wqshao126 | TP-Eval is a new evaluation framework for Multimodal Large Language Models (MLLMs) that addresses the issue of prompt sensitivity, where minor prompt variations can lead to significant performance fluctuations, resulting in underestimation or bias in evaluation. - It introduces a prompt customization method using an automatic prompt optimizer, tailored for MLLMs, to generate optimal prompts for each model, tapping their full potential. - This optimizer leverages a scorer, composed of the target MLLM and an answer analyzer, to iteratively refine prompts based on accuracy, semantic similarity to the original prompt, and introspection from incorrect responses. - Experiments on MMT-Bench and MMMU datasets demonstrate that TP-Eval effectively reduces underestimation and bias, revealing models' true capabilities and facilitating fairer comparisons. - TP-Eval also shows promising results in zero-shot settings using in-context learning, enabling prompt optimization even with limited data. | ['Multimodal'] | N/A | N/A |


## Papers for 2024-10-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/abs/2410.17247) | lindahua, jiaqiwang-rex, conghui, yhcao, yuhangzang |  - PyramidDrop is a novel visual redundancy reduction strategy for Large Vision-Language Models (LVLMs) designed to accelerate training and inference. - It partitions the LVLM into stages and progressively drops image tokens at each stage's end based on a lightweight similarity calculation with the instruction's last token.  - This pyramid-like token reduction leverages the observation that token redundancy increases in deeper LVLM layers. - Experiments on LLaVA-NeXT-7B show 40% training time and 55% inference FLOPs reduction without performance loss on 15 vision-language tasks.  - PyramidDrop also allows training with doubled resolution using only 70% of the original training time and serves as a plug-and-play inference acceleration strategy outperforming existing methods. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/Cooperx521/PyramidDrop) | N/A |
| [Aligning Large Language Models via Self-Steering Optimization](https://arxiv.org/abs/2410.17131) | Jingren, xphan, luyaojie, keminglu, sanmusunrise |   - This paper introduces Self-Steering Optimization (SSO), an algorithm designed for automated alignment of large language models (LLMs), eliminating the need for manual annotation.  - SSO autonomously generates preference signals by prompting the policy model with contrastive principles and optimizing based on three objectives: steering the model towards chosen responses, maintaining on-policy behavior, and ensuring a consistent quality gap between responses. -  Experiments conducted on Qwen2 and Llama3.1 demonstrate SSO's ability to generate accurate and learnable signals, leading to significant performance improvements across various benchmarks without manual annotation or external models. - SSO enhanced the training of reward models using data generated during the alignment process, further highlighting its effectiveness. - This work contributes a scalable approach to preference optimization for more efficient and effective automated alignment. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/icip-cas/SSO) | N/A |
| [JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://arxiv.org/abs/2410.17250) | Yuki Imajuku, gneubig, ku21fan, AtsuMiyai, shtapm | JMMMU is a new large-scale Japanese benchmark dataset designed to evaluate Large Multimodal Models (LMMs) focusing on Japanese cultural understanding. - It comprises two subsets: a Culture-Agnostic (CA) subset, containing translations of culture-independent components from the MMMU benchmark, and a Culture-Specific (CS) subset with newly crafted questions related to Japanese culture. - The benchmark is significantly larger than existing culture-aware Japanese benchmarks, totaling 1,320 questions with 1,118 images across a diverse range of 28 subjects. - An evaluation of 15 open-source and 3 proprietary LMMs reveals up to 58.6% overall accuracy, indicating significant room for improvement in utilizing the Japanese context. - The results indicate that many LMMs perform worse on questions in Japanese compared to their English counterparts and highlight the importance of culture-specific evaluation. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/cyberagent/llava-calm2-siglip), [Link](https://huggingface.co/datasets/SakanaAI/JA-Multi-Image-VQA), [Link](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500), [Link](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild) |
| [EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search](https://arxiv.org/abs/2410.14649) | dalistarh, ekurtic, SpiridonSunRotator, OliverSieberling | • EvoPress, a new evolutionary search approach for dynamic compression of large language models (LLMs), is introduced, offering provable convergence and low sample and iteration complexity. • EvoPress challenges the assumption of error monotonicity in LLM compression, demonstrating instances where lower per-layer error sums do not translate to better overall performance. • This method improves upon existing layer dropping, unstructured sparsity, and quantization techniques, setting new state-of-the-art results. • It achieves significant improvements, particularly at higher compression ratios, across various LLM families. • EvoPress converges efficiently, often within hours on a single GPU, even for large models, and a lightweight version is available for faster processing. | ['Natural Language Processing', 'Text Generation', 'Feature Extraction'] | [Link](https://github.com/IST-DASLab/EvoPress) | N/A |
| [MiniPLM: Knowledge Distillation for Pre-Training Language Models](https://arxiv.org/abs/2410.17215) | Minlie Huang, Jie Zhou, Hao Zhou, fandong, t1101675 |  - MINIPLM is a new Knowledge Distillation (KD) framework for pre-training Language Models (LMs) that refines the training data distribution using a teacher LM's knowledge. - It addresses the efficiency, flexibility, and effectiveness challenges of existing KD methods during pre-training through offline teacher inference, corpus-based operation, and a Difference Sampling technique that leverages the discrepancies between large and small LMs. - Experiments across various student LM sizes show that MINIPLM improves performance on 9 downstream tasks, language modeling capabilities, and reduces pre-training computation by 2.2 times compared to Vanilla KD which achieves similar performance but with more compute. - MINIPLM also supports KD across model families with different tokenizations, unlike existing online KD methods. - Further analysis suggests that MINIPLM improves pre-training data utilization, reducing the data demand by 2.4 times. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/MiniPLM) | N/A |
| [Mitigating Object Hallucination via Concentric Causal Attention](https://arxiv.org/abs/2410.15926) | Shijian Lu, Ivan Laptev, Yiheng Li, xing0047 | - This paper introduces Concentric Causal Attention (CCA), a novel position alignment strategy for Large Vision-Language Models (LVLMs) designed to mitigate object hallucination, a phenomenon where LVLMs generate text responses misaligned with image content.  - CCA addresses the limitations of Rotary Position Encoding (ROPE), commonly used in LVLMs, where long-term decay in attention can lead to hallucination.  - The method reorganizes visual tokens in a concentric manner, reducing the relative distance between visual and instruction tokens and improving spatial locality. It also introduces a modified causal attention mask to support the 2-D structure of image data.  - Experimental results on benchmarks like POPE, CHAIR, and MME demonstrate that CCA surpasses existing debiasing methods, improving accuracy and reducing hallucination.  - CCA also enhances the overall perception capability of LVLMs in multiple-choice visual question answering tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/xing0047/cca-llava.git) | N/A |
| [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930) | Thomas Hartvigsen, Jonathan Kropko, Zack Gottesman, Bryan R. Christ |  - MathNeuro is introduced; a method for isolating math-specific parameters in LLMs using forward passes, building upon existing work by calculating parameter importance with weights and activations, but with the key innovation of removing parameters also important for general language tasks measured on non-math datasets. - Pruning MathNeuro-identified parameters eliminates a LLM's math reasoning ability, while the impact on other tasks is similar to pruning random parameters. - Scaling up MathNeuro-identified parameters by a small constant (1.1 for smaller models and 1.01 for larger models) improves performance on GSM8K by 4-17% without affecting non-math performance. - MathNeuro remains effective with a single sample for parameter identification, demonstrating its data efficiency. - Math-specific parameters are distributed across the model's decoder blocks, suggesting math reasoning is not localized. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/bryanchrist/MathNeuro) | N/A |


## Papers for 2024-10-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution](https://arxiv.org/abs/2410.16256) | Hongwei Liu, Maosong Cao, zsytony, KennyUTC, acylam | - CompassJudger-1 is introduced as the first open-source all-in-one judge LLM. - It supports unitary scoring, two-model comparisons, formatted evaluations, critique generation and diverse tasks. - A new benchmark called JudgerBench is created to evaluate judge models. It includes realistic human annotation from the LLM arena and GPT annotations on subjective benchmarks. - Training data for CompassJudger-1 includes several sources, like pair-wise data, critiques and reward data. - Several data filtering and sampling strategies are developed for training. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/CompassJudger) | N/A |
| [PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://arxiv.org/abs/2410.13861) | hsli-cuhk, daijifeng, zengxingyu, gogoduan, LucasFang |   - PUMA, a unified multimodal large language model (MLLM), is introduced, featuring multi-granular visual feature processing for diverse visual tasks.  - The model uses a three-part architecture: a multi-granular image encoder (CLIP), a set of dedicated diffusion-based decoders, and an autoregressive MLLM.  - PUMA is trained in two stages: Multimodal pretraining on large datasets (Laion-2B, Laion-Aesthetics, GRIT, The Pile, OCR-VQA-200K, LLaVAR) followed by task-specific instruction tuning.  - The evaluation shows that PUMA excels in diverse text-to-image generation, image editing, conditional image generation, and understanding, outperforming existing unified MLLMs.  - The multi-granular approach balances diversity and controllability by processing features at multiple levels from coarse-grained abstractions to fine-grained details. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image', 'Visual Question Answering'] | [Link](https://github.com/rongyaofang/PUMA) | N/A |
| [Baichuan Alignment Technical Report](https://arxiv.org/abs/2410.14940) | dongguosheng, YijieZhou, TJU-Tianpengli, zilchshen, lin5547 | • This report introduces Baichuan Alignment, a comprehensive suite of techniques used to align the Baichuan series of large language models (LLMs), including optimization methods, data strategies, and evaluation processes. • Baichuan Alignment consists of three phases: Prompt Augmentation System (PAS) which transforms user queries into actionable instructions, Supervised Fine-Tuning (SFT) which trains LLMs for dialogue and complex tasks, and Preference Alignment which aligns LLMs with human preferences. • The alignment process employs several optimizations such as sample packing and multi-layer gradient checkpointing to increase training efficiency and model merging to improve performance across domains. • Evaluations of Qwen2-Nova-72B and Llama3-PBM-Nova-70B, instruct versions of open-source models optimized with Baichuan Alignment, show significant performance improvements across various benchmarks, outperforming official instruct versions and competing with leading LLMs. • Baichuan-Instruct, an internal model, demonstrates 17% to 28% user experience improvement in core capabilities, highlighting the effectiveness of the proposed alignment techniques. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B) |
| [AutoTrain: No-code training for state-of-the-art models](https://arxiv.org/abs/2410.15735) | abhishek | AutoTrain (AutoTrain Advanced) is an open-source, no-code tool/library for training and fine-tuning machine learning models on a variety of tasks and modalities. - It supports various tasks, including large language model (LLM) fine-tuning, text classification/regression, token classification, sequence-to-sequence tasks, fine-tuning of sentence transformers, visual language model (VLM) fine-tuning, image classification/regression, and tabular data classification/regression. - AutoTrain simplifies the training process by providing a user-friendly interface and automating tasks such as dataset processing, hyperparameter tuning, and model validation. - It offers flexibility by supporting local and cloud-based training, multiple data formats (zip, CSV, JSONL), and various model architectures compatible with Hugging Face Transformers. - AutoTrain is designed for both novice and experienced users, enabling them to build and deploy high-performing models easily. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Object Detection', 'Text Classification', 'Token Classification', 'Tabular Classification', 'Tabular Regression', 'Text2Text Generation', 'Text Generation', 'Multimodal'] | [Link](https://github.com/huggingface/autotrain-advanced) | N/A |
| [RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style](https://arxiv.org/abs/2410.16184) | Rui Min, Yantao Liu, juanli, Nuomei, TranSirius | This paper introduces RM-BENCH, a novel benchmark designed to evaluate reward models for language models.  RM-BENCH focuses on assessing reward models' sensitivity to subtle content differences and resistance to style biases, unlike existing benchmarks.  Extensive experiments show RM-BENCH strongly correlates with policy model performance, making it a reliable tool for selecting effective reward models. Results indicate that current state-of-the-art reward models perform poorly when faced with style bias, showcasing areas for improvement in future model development. The benchmark includes datasets across various domains, including chat, code, math, and safety, with style-controlled variations. | ['Natural Language Processing', 'Text Classification', 'Reinforcement Learning'] | [Link](https://github.com/THU-KEG/RM-Bench) | N/A |
| [Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://arxiv.org/abs/2410.16153) | Nyandwi, seungone, akariasai, yueqis, yuexiang96 | This paper introduces PANGEA, a fully open multilingual multimodal large language model (LLM) trained on a diverse 6M instruction dataset spanning 39 languages.  PANGEA significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts, showing comparable performance to state-of-the-art (SOTA) models in English while substantially exceeding them in multilingual scenarios.  The model's architecture is based on LLaVA-Next, using Qwen2-7B-Instruct as the language model backbone.  PANGEA, along with its associated data and code, is fully open-sourced to promote equitable and accessible access to robust multilingual MLLMs. | ['Multimodal'] | [Link](https://neulab.github.io/Pangea/) | [Link](https://huggingface.co/datasets/cmarkea/table-vqa), [Link](https://huggingface.co/datasets/deepvk/GQA-ru), [Link](https://huggingface.co/datasets/cmarkea/doc-vqa), [Link](https://huggingface.co/datasets/cmarkea/table-vqa), [Link](https://huggingface.co/datasets/BUAADreamer/Chinese-LLaVA-Med-7B), [Link](https://huggingface.co/datasets/LinkSoul-AI/Chinese-LLaVA), [Link](https://huggingface.co/datasets/Toshi456/LLaVA-Japanese-Instruct), [Link](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT) |
| [Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception](https://arxiv.org/abs/2410.12788) | Zhiyuan Ji, jimi888, siminniu, MoCun, Robot2050 |  - This paper introduces Meta-Chunking, a novel text segmentation technique that leverages Large Language Models (LLMs) to divide documents into logically coherent chunks at a granularity between sentences and paragraphs. - Two strategies are proposed: Margin Sampling Chunking, which performs binary classification on consecutive sentences based on probability differences, and Perplexity Chunking, which analyzes perplexity distribution to identify chunk boundaries. - A dynamic merging strategy is also introduced to balance fine-grained and coarse-grained chunking, adjusting chunk sizes based on user-specified length requirements. - Experimental results across eleven datasets and four benchmarks demonstrate that Meta-Chunking significantly improves single-hop and multi-hop question answering performance in Retrieval-Augmented Generation (RAG) systems. - On the 2WikiMultihopQA dataset, for example, Meta-Chunking outperforms similarity chunking by 1.32 in F1 score while requiring only 45.8% of the processing time. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/IAAR-Shanghai/Meta-Chunking) | N/A |
| [Pre-training Distillation for Large Language Models: A Design Space Exploration](https://arxiv.org/abs/2410.16215) | Xin Lv, juanli, NeoZ123, bys0318, Wesleythu | - This paper explores pre-training distillation (PD), a method for transferring knowledge from a larger teacher LLM to a smaller student LLM during the pre-training phase. - The study investigates four key aspects of PD: logits processing, loss selection, scaling law (model and data size), and the use of offline vs. online logits. - A preliminary experiment using GLM-4-9B as the teacher and a 1.9B parameter student model shows a 1.6% average improvement across various datasets with PD. - Further experiments reveal that larger student LLMs benefit more from PD, while larger teacher LLMs don't always guarantee better results. - The best results are achieved by combining Kullback-Leibler divergence loss with language modeling loss using a Warmup-Stable-Decay scheduling strategy. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation](https://arxiv.org/abs/2410.15748) | Ping Wei, opotle, yegong, shuailu, EurekaWu123 |  - Alchemy, a novel framework, synthesizes formal theorems through symbolic mutations to address the data scarcity challenge in Neural Theorem Proving (NTP). - For each candidate theorem, Alchemy identifies invocable theorems from Mathlib and performs mutations by replacing terms with equivalent forms or antecedents. - This method significantly increases the number of theorems in Mathlib from 110k to 6M. - Continual pretraining and supervised finetuning on this augmented dataset for Large Language Models leads to a 5% absolute performance improvement on the Leandojo benchmark and a 2.5% gain on the miniF2F benchmark. - The analysis of synthetic data composition and training paradigms offers valuable insights for developing strong theorem provers. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation](https://arxiv.org/abs/2410.14745) | Wei Ju, Xiao Luo, Shockzipper, XtremSup, luojunyu | - SemiEvol is a semi-supervised fine-tuning framework designed to improve large language model (LLM) performance in scenarios with limited labeled data and abundant unlabeled data. - It employs a bi-level knowledge propagation strategy, transferring knowledge from labeled data to unlabeled data through both model adaptation and context enhancement.  - For unlabeled data utilization, it involves collaborative learning among multiple LLMs with diverse configurations and adaptive data selection. - Experimental results on various datasets, including MMLU, MMLU-Pro, ARC, FPB, USMLE, PubMedQA, and ConvFinQA, demonstrate significant performance improvements compared to SFT and self-evolution methods. - SemiEvol effectively utilizes both labeled and unlabeled data, enabling LLMs to adapt to specific scenarios more economically. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/luo-junyu/SemiEvol) | [Link](https://huggingface.co/Solshine/reflection-llama-3.1-8B), [Link](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B) |
| [Zero-shot Model-based Reinforcement Learning using Large Language Models](https://arxiv.org/abs/2410.11711) | GPaolo, albert9000, Xssama, ambroiseodt, abenechehab |  - This paper introduces Disentangled In-Context Learning (DICL), a novel approach for applying in-context learning (ICL) with large language models (LLMs) to reinforcement learning (RL) environments with continuous state spaces. - DICL addresses the challenges of incorporating action information and handling state-action dimension interdependence by projecting the state-action vector into a latent space using Principal Component Analysis (PCA) where features are linearly uncorrelated, then applying ICL. - The paper demonstrates the effectiveness of DICL in two RL applications: model-based policy evaluation and data-augmented off-policy RL, showing improved sample efficiency in both cases. - A theoretical analysis provides a novel return bound for the policy evaluation algorithm resulting from multi-branch rollouts with the LLM-based dynamics model.  - Additionally, the paper provides empirical evidence suggesting that LLMs offer well-calibrated uncertainty estimations, a desirable property for model-based RL algorithms. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/abenechehab/dicl) | N/A |
| [Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement](https://arxiv.org/abs/2410.15633) | Yunshui Li, Gang Chen, Haozhe Zhao, Shuzheng Si, kaikai1 |  - This paper introduces GATEAU, a novel framework for selecting influential samples to improve long-context alignment in large language models (LLMs).  - GATEAU leverages Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM) to identify high-quality samples exhibiting strong long-range dependencies.  - HMG compares perplexity scores from homologous models with different context windows to assess response generation difficulty, while CAM evaluates whether the model focuses on important input segments.  - Experimental results show that LLMs trained on samples selected by GATEAU outperform those trained on the full dataset and various baselines across multiple benchmarks, including LongBench, LongBench-Chat, and MT-Bench.  - Ablation studies demonstrate that both HMG and CAM contribute significantly to GATEAU's effectiveness. | ['Natural Language Processing', 'Text Generation', 'Summarization', 'Question Answering'] | N/A | N/A |
| [CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy](https://arxiv.org/abs/2410.13218) | Travis Labrum, wangwilliamyang, xz97, Xianjun, billmianz | This paper introduces CBT-BENCH, a new benchmark for evaluating large language models' (LLMs) ability to assist cognitive behavioral therapy (CBT).  CBT-BENCH includes three levels of tasks: basic CBT knowledge acquisition, cognitive model understanding, and therapeutic response generation.  The benchmark uses three new datasets (CBT-QA, CBT-CD, CBT-PC, CBT-FC and CBT-DP) to evaluate the LLMs across these tasks.  The results show that while LLMs perform well on knowledge-based tasks, they struggle with complex tasks that require deep understanding of patient cognitive structures and effective response generation.  This suggests the need for further research into how LLMs can be improved for real-world CBT applications. | ['Natural Language Processing', 'Text Classification', 'Question Answering', 'Text Generation'] | [Link](https://github.com/mianzhang/CBT-Bench) | N/A |
| [Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs](https://arxiv.org/abs/2410.13394) | anoopk, prajdabre, dipsivenkatesh, safikhan, sumanthd | This paper introduces CIA Suite, a framework for cross-lingual auto-evaluation of multilingual LLMs.  The framework includes a novel test set (RECON) with human annotations across six languages and a cross-lingual evaluator LLM (HERCULE). HERCULE leverages English reference answers to evaluate responses in other languages, addressing the scarcity of reference answers in low-resource scenarios. Experiments show HERCULE aligns more closely with human judgments than existing models, exhibiting effectiveness in zero-shot settings.  The CIA suite is publicly available to encourage further research. | ['Natural Language Processing', 'Text Generation', 'Text Classification', 'Zero-Shot Classification'] | [Link](https://github.com/CIA) | [Link](huggingface.co/CIA-Suite) |
| [DM-Codec: Distilling Multimodal Representations for Speech Tokenization](https://arxiv.org/abs/2410.15017) | A K M Mahbubur Rahman, Md Fahim, amanchadha, tasnim, mubtasim | DM-Codec is a novel speech tokenizer that leverages a neural codec architecture with Residual Vector Quantization (RVQ) and incorporates two novel distillation approaches: LM-guided and combined LM and SM-guided distillation. - The LM-guided approach distills contextual representations from a Language Model (LM) and integrates them with acoustic representations, while the combined approach incorporates semantic representations from a Speech Model (SM) along with contextual and acoustic representations.  - DM-Codec adopts a streamlined encoder-decoder framework enhanced by a multi-discriminator setup comprising Multi-Scale, Multi-Period, and Multi-Scale Short-Time Fourier Transform discriminators. - Experimental results on the LibriSpeech benchmark demonstrate that DM-Codec significantly outperforms state-of-the-art models, reducing Word Error Rate (WER) by up to 13.46%, Word Information Lost (WIL) by 9.82%, and improving speech quality and intelligibility.  - The combined distillation approach results in a WER of 4.05 and a WIL of 6.61, surpassing existing speech tokenization models. | ['Audio', 'Automatic Speech Recognition', 'Multimodal'] | [Link](https://github.com/mubtasimahasan/DM-Codec) | N/A |


## Papers for 2024-10-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation](https://arxiv.org/abs/2410.13232) | jihoonkim25, Gwanwoo, ktio, kimnamssya, hyungjoochae | • This paper introduces World-Model-Augmented (WMA) web agents, which leverage world models to simulate the outcomes of actions for enhanced decision-making in web navigation. • WMA agents address the limitations of Large Language Models (LLMs) in long-horizon web navigation tasks by incorporating a world model that predicts the effects of actions, enabling the agent to foresee potential outcomes. • The authors propose a transition-focused observation abstraction method to overcome training challenges, where the world model is trained to generate natural language descriptions of state differences between time steps, rather than predicting the entire next observation. • The WMA agent employs a value function to estimate rewards for simulated next observations, guiding the policy model to select optimal actions. • Experimental results on WebArena and Mind2Web show that WMA agents improve policy selection, achieve state-of-the-art performance on Mind2Web, and demonstrate superior cost and time efficiency compared to tree-search-based agents. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/kyle8581/WMA-Agents) | N/A |
| [UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models](https://arxiv.org/abs/2410.14059) | Yilin Guo, Yan Hu, wittenberg, amstrongzyf, TobyYang7 | - UCFE, a User-Centric Financial Expertise benchmark, is introduced to evaluate LLMs' ability to handle complex, real-world financial tasks using dynamic, task-specific interactions in a hybrid approach combining human and LLM evaluations. - Based on a user study with 804 participants, a dataset is created that incorporates various user intents and interactions across different user groups, serving as a foundation for benchmarking 12 LLMs using LLM-as-Judge methodology. - Results demonstrate a strong correlation (Pearson coefficient 0.78) between benchmark scores and human preferences, validating the UCFE dataset and evaluation method. - Mid-sized LLMs (7B-14B parameters), fine-tuned on financial texts, achieve a balance between performance and resource efficiency. - The user-centric design highlights the necessity of aligning AI systems with diverse user requirements in finance, setting the stage for enhanced, reliable AI-driven solutions. | ['Natural Language Processing'] | [Link](https://github.com/TobyYang7/UCFE-Benchmark) | N/A |
| [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669) | Daniel Jiang, Wenxuan Peng, Zhiqiu Lin, Nyandwi, BaiqiL | • NaturalBench is a new benchmark designed for evaluating vision-language models (VLMs) on natural adversarial samples. • These are image-question pairs derived from real-world images and questions, which are easily answered by humans but pose a challenge for current VLMs. • The authors use a semi-automated approach to curate the benchmark, making use of CLIP and ChatGPT to source and filter questions from image caption datasets. • The benchmark comprises 10,000 human-verified question-answer samples, categorized by visual reasoning skill. • Evaluation results of 53 state-of-the-art VLMs demonstrate a significant performance gap compared to humans, suggesting the benchmark's efficacy in revealing areas for improvement. | ['Visual Question Answering', 'Multimodal'] | [Link](https://linzhiqiu.github.io/papers/naturalbench) | N/A |
| [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](https://arxiv.org/abs/2410.13276) | Hayden Kwok-Hay So, Dayou Du, Shijie, CharyZeng, Retromonic |  - This paper introduces SeerAttention, a novel attention mechanism designed to improve the efficiency and scalability of Large Language Models (LLMs), especially those with long context windows, by learning intrinsic sparse attention rather than using predefined patterns. - SeerAttention augments conventional attention with a learnable gate, called Attention Gate (AttnGate), to dynamically select important blocks in an attention map and treat the rest as sparse. - It employs a customized FlashAttention kernel to extract the block-level ground truth of attention maps for efficient training of the gating network, minimizing overhead. - Evaluations show SeerAttention outperforms existing sparse attention methods in post-training and achieves near-lossless accuracy with high sparsity (up to 90%) during fine-tuning for long context extension using YaRN. -  With a block-sparse pattern, the attention kernel achieves up to a 5.67x speedup over the FlashAttention-2 dense baseline on a single A100 GPU. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/SeerAttention) | N/A |
| [Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts](https://arxiv.org/abs/2410.14677) | Yury Chekhovich, Anastasia Voznyuk, German Gritsai, andriygav |  - This paper presents a systematic review of datasets used in competitions and research papers dedicated to AI-generated content detection and proposes methods for evaluating the quality of such datasets. - The authors argue that the high performance of current detectors on benchmark datasets may be due to the poor quality of the evaluation datasets rather than the true effectiveness of the detectors. - The authors investigate different metrics, such as detecting low-quality generations with the use of metrics based on topological time series, detecting suspicious activation maps, and detecting sensibility to perturbations, such as text modification and sentence shuffling - The paper emphasizes the need for robust and qualitative methods to evaluate generated data to be secure against bias and low generalization ability of future models and provide a more comprehensive understanding of the dynamics between human and machine text. - The paper suggests that the use of high-quality generated data can be used for two purposes: enhancing the training of detection models and refining the training datasets themselves. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828) | Mengdi Wang, Huazheng Wang, Yue Wu, yokey, huiyuan23 |  - This paper identifies a common pitfall in margin-based language model alignment methods used in Reinforcement Learning from Human Feedback (RLHF): the under-specification of ideal behavior on preferred and dispreferred responses.  - This issue leads to two problems as the margin increases: an increase in unsafe responses, and a decrease in preferred, ideal responses.  - The underlying cause is identified as the *gradient entanglement* effect, in which margin-based losses couple the preferred and dispreferred probabilities, thus often preventing ideal changes.  - This effect is characterized by an inner product condition involving the gradients of preferred and dispreferred log-probabilities.  - The theoretical analysis is empirically validated, and suggests potential mitigation through pairwise normalized gradient descent and sparsity regularized token masking. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/HumainLab/Understand_MarginPO) | N/A |
| [DPLM-2: A Multimodal Diffusion Protein Language Model](https://arxiv.org/abs/2410.13782) | Shujian Huang, Dongyu Xue, Fei Ye, Zaixiang Zheng, Xinyou Wang | • DPLM-2 is a multimodal protein foundation model based on a discrete diffusion probabilistic framework that models both protein sequences and structures.  • DPLM-2 employs a lookup-free quantizer (LFQ) to convert 3D coordinates to discrete tokens, facilitating structure learning within the language model.  • It uses an efficient warm-up strategy, leveraging pre-trained sequence-based DPLM and evolutionary data to enhance structural modeling.  • DPLM-2 demonstrates competitive performance in co-generation of structure and sequence, achieving high designability and outperforming ESM3-Open and Multiflow in structure-sequence compatibility.  • DPLM-2 also shows strong results in various conditional generation tasks like folding, inverse folding, and motif scaffolding and structure-aware representations for predictive tasks. | ['Multimodal', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media](https://arxiv.org/abs/2410.12791) | Mette Thunø, Rebecca M. M. Hicke, Ross Deans Kristensen-McLachlan, kardosdrur | This paper introduces KeyNMF, a novel approach to topic modeling that leverages contextual embeddings and Non-negative Matrix Factorization (NMF). - KeyNMF extracts keywords from documents using contextual embeddings and then applies NMF to these embeddings to generate topics.  - It is evaluated on Chinese news data and demonstrates competitive performance compared to other contextual topic models, especially in terms of external coherence. - KeyNMF is integrated with existing methods for analyzing information dynamics to study Chinese diaspora media's coverage of the 2024 European parliamentary elections.  -  The pipeline identifies trends in novelty and resonance signals that correlate with key political events, demonstrating its effectiveness in capturing information dynamics. - The researchers find that KeyNMF enables nuanced analysis of information flow and agenda-setting within Chinese diaspora media during the election period. | ['Natural Language Processing', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) |


## Papers for 2024-10-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures](https://arxiv.org/abs/2410.13754) | kcz358, fuzhao, Junhao233, dghosal, jinjieni |  - MixEval-X is a benchmark for evaluating multimodal models across various input-output modalities, including image, video, audio, text, and actions. - The benchmark covers eight input-output modality combinations and uses a mixture of existing datasets and real-world web data to construct evaluations.  - MixEval-X employs a novel multi-modal benchmark mixture and adaptation-rectification pipeline to optimize evaluation tasks by aligning them with real-world task distributions and mitigating biases.  - Meta-evaluations demonstrate that MixEval-X effectively aligns benchmark samples with real-world distributions, with model rankings correlating strongly (up to 0.98) with crowd-sourced real-world evaluations.  - The benchmark offers comprehensive leaderboards to rerank existing models and organizations across modalities. | ['Multimodal', 'Any-to-Any', 'Image-to-Text', 'Video-Text-to-Text', 'Text-to-Image', 'Text-to-Video', 'Text-to-Audio'] | [Link](https://mixeval-x.github.io/) | N/A |
| [Harnessing Webpage UIs for Text-Rich Visual Understanding](https://arxiv.org/abs/2410.13824) | Yuxiao Qu, Yifan Song, yuexiang96, oottyy, jeepliu |  - This paper introduces MultiUI, a 7.3 million sample dataset synthesized from 1 million web page UIs using LLMs, for training multimodal models in text-rich visual understanding. - MultiUI covers nine diverse tasks across three categories (visual understanding and reasoning, text recognition, and grounding), enhancing model perception, comprehension, grounding, and reasoning capabilities.  - Models trained on MultiUI demonstrate significant improvement, up to 48% on VisualWebBench and 19.1% on Mind2Web, outperforming larger models like LLaVA 1.6 34B and GPT-4V in GUI tasks.  - MultiUI also generalizes well to non-web UI tasks like document understanding, OCR, and chart interpretation, showing strong cross-domain generalization. - This highlights the value of structured web UI data for advancing text-rich visual understanding in MLLMs.  | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | [Link](https://neulab.github.io/MultiUI/) |
| [MobA: A Two-Level Agent System for Efficient Mobile Task Automation](https://arxiv.org/abs/2410.13757) | Yixuan Jiang, Kunyao Lan, Yansi Li, Hao Tang, JamesZhutheThird | - MobA, a novel two-level agent architecture designed to enhance the abilities of mobile phone assistants, using Multimodal Large Language Models (MLLMs). -  Composed of a higher-level Global Agent for tasks such as command interpretation and task planning, and a lower-level Local Agent to select and execute actions based on current screen information and historical data. -  A double reflection mechanism allowing the system to correct errors quickly and avoid sub-optimal operations, as well as an integrated memory module to track actions and optimize execution. -  Evaluation performed on the Mobbench dataset containing 50 mobile tasks across 10 applications of varying difficulty, outperforming other mobile agents, achieving the highest milestone score of 66.2%. | ['Multimodal', 'Computer Vision', 'Natural Language Processing'] | N/A | N/A |
| [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) | zdaxie, zizhpan, XCLiu, CNMaxwell, WuChengyue | - Janus is an autoregressive multimodal model that decouples visual encoding pathways for understanding and generation tasks using a shared transformer architecture.  - For understanding, it uses a SigLIP encoder for high-level semantic information, while for generation, it utilizes a VQ tokenizer focusing on fine-grained visual details.  - This approach addresses the conflicting representational needs of the two tasks, enabling both strong performance and model flexibility.  - Experimental results demonstrate that Janus outperforms other unified models of comparable size and matches or exceeds task-specific models on benchmarks like MMBench, SEED-Bench, POPE, MSCOCO, and GenEval.  - The model's performance and flexibility make it a potential candidate for the next generation of unified multimodal models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/deepseek-ai/Janus) | N/A |
| [MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://arxiv.org/abs/2410.13085) | Weijia Shi, Tianze Wang, Haoran Li, Kangyu Zhu, richardxp888 | MMed-RAG is a new multimodal retrieval-augmented generation (RAG) system designed to improve the factuality of Medical Large Vision-Language Models (Med-LVLMs). - It incorporates a domain-aware retrieval mechanism, adaptive context selection, and RAG-based preference fine-tuning to address misalignment issues and enhance alignment with ground truth. - The model achieves an average improvement of 43.8% in factual accuracy across five medical datasets and two tasks (medical VQA and report generation) compared to the original Med-LVLM.  - It outperforms other decoding-based and RAG-based approaches on medical VQA and report generation tasks. - MMed-RAG demonstrates strong generalizability, achieving consistent improvements across various medical image modalities (radiology, ophthalmology, and pathology). - Through ablation studies, the contribution of each proposed component is validated, demonstrating its effectiveness in enhancing the factuality and performance of Med-LVLMs in different medical domains. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/richard-peng-xia/MMed-RAG) | N/A |
| [A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models](https://arxiv.org/abs/2410.13841) | Keming Lu, Hongyu Lin, Bowen Yu, Le Yu, TangQiaoYu | - This paper introduces a unified perspective on delta parameter editing in post-trained large-scale models, formulating editing operations based on Riemann sum approximation of the loss difference. - This analysis categorizes existing methods into three performance classes: competitive (e.g., DARE, DELLA-Merging), decreased (e.g., BitDelta, Twin-Merging, TIES-Merging), and improved (e.g., EXPO), explaining their impact on model performance through the lens of Riemann sum approximation. - Extensive experiments on visual and language models (ViT, LLaMA 3, Qwen 2, Mistral) support the theoretical findings. - The paper further proposes extensions to existing techniques like DARE and BitDelta, generalizing their formats and improving applicability. - For example, introducing a factor *k* to DARE handles dropped parameters more effectively and expanding BitDelta to use multiple bits improves performance beyond the original post-trained model. | ['Natural Language Processing', 'Computer Vision'] | N/A | N/A |
| [PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment](https://arxiv.org/abs/2410.13785) | Ke Xu, Jiaheng Liu, Shawn Wang, Zekun Moore Wang, kangz | PopAlign is a framework for aligning large language models (LLMs) by diversifying contrasting patterns across prompt, model, and pipeline levels. - It integrates six distinct contrasting strategies: Prefix Contrast, Demon Contrast, Elicitive Contrast, NParam Contrast, Leaderboard Contrast, and Refine Contrast. - These strategies synthesize preference-contrastive data without requiring additional feedback labeling. - Experimental results demonstrate that PopAlign significantly outperforms existing methods on various alignment tasks and leaderboards. - Notably, PopAlign achieves higher scores than strong baselines trained on original labels, indicating its effectiveness in preference modeling and comprehensive alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [MoH: Multi-Head Attention as Mixture-of-Head Attention](https://arxiv.org/abs/2410.11842) | Shuicheng Yan, Li Yuan, Bo Zhu, Chat-UniVi | - Mixture-of-Head attention (MoH) is proposed, which integrates multi-head attention with a Mixture-of-Experts (MoE) mechanism by treating attention heads as experts. - MoH employs a router to select the top-k heads for each token, improving inference efficiency, and uses a weighted sum of outputs rather than standard summation, potentially enhancing performance. - Shared heads in MoH retain constant activation, capturing general knowledge. - Evaluations on ViT, DiT, and LLMs show MoH outperforms multi-head attention using only 50%~90% of heads. - Pre-trained models like LLaMA3-8B can be continue-tuned into MoH models, with MoH-LLaMA3-8B showing improved accuracy with fewer heads. | ['Computer Vision', 'Image Classification', 'Unconditional Image Generation', 'Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/SkyworkAI/MoH) | N/A |
| [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852) | Anne Wu, Gloria Geng, Yiwei Chen, Mustafa Omer Gul, Zizhao Chen |  - This paper introduces RESPECT, a novel method for improving large language models (LLMs) through retrospective learning from implicit feedback signals in multi-turn interactions. - RESPECT leverages user responses such as rephrased requests, expressions of frustration, or task pivots as implicit feedback signals, eliminating the need for explicit annotations or feedback solicitation.  - The method involves decoding feedback from past interactions by prompting the LLM to analyze interaction contexts and follow-up utterances. -  This decoded feedback is then used to re-train the LLM, resulting in continual improvement over multiple rounds of interaction and training. -  In a new multimodal interaction scenario called MULTIREF, where humans instruct an LLM to solve an abstract reasoning task, RESPECT demonstrates significant improvement, boosting task completion rate from 31% to 82% without external annotations. | ['Multimodal', 'Natural Language Processing', 'Reinforcement Learning'] | [Link](https://lil-lab.github.io/respect) | N/A |
| [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426) | Kang Zhao, Han Bao, Haoli Bai, Yuxuan Sun, lianlio |  - FLATQUANT, a novel post-training quantization approach, enhances the flatness of Large Language Model (LLM) weights and activations through fast and learnable affine transformations, improving quantization accuracy and reducing error propagation. - FLATQUANT employs a lightweight, block-wise training strategy over calibration data and utilizes Kronecker decomposition for efficient affine transformations, minimizing memory and computational demands. - A single kernel fusing affine transformations and quantization reduces transformation overhead, resulting in inference speedups of up to 2.3x for prefill and 1.7x for decoding compared to the FP16 baseline.  - FLATQUANT achieves state-of-the-art quantization results, including less than 1% accuracy drop for W4A4 quantization on LLaMA-3-70B, outperforming SpinQuant by 7.5%.  - The method's effectiveness is shown on various LLMs (LLaMA-2/3, 7B to 70B parameters) across tasks like language modeling and question answering, demonstrating superior accuracy and inference latency compared to other state-of-the-art techniques. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ruikangliu/FlatQuant) | N/A |
| [MedMobile: A mobile-sized language model with expert-level clinical capabilities](https://arxiv.org/abs/2410.09019) | Eric Karl Oermann, Daniel Alexander Alber, Anton Alaykin, Jaden Stryker, KrithikV | - MedMobile, a fine-tuned 3.8B parameter phi-3-mini language model, demonstrates expert-level clinical reasoning capabilities, achieving a 75.7% accuracy on MedQA (USMLE), surpassing the passing score for physicians and outperforming previous state-of-the-art sub-5B parameter models by over 20%. - MedMobile leverages chain-of-thought prompting, ensemble methods, and supervised fine-tuning, with the latter contributing an 8.4% improvement in accuracy.  - Unlike larger models, techniques such as k-shot prompting and retrieval-augmented generation did not enhance MedMobile's performance, possibly due to context window limitations, leaving potential avenues for future research.  - This model holds promise for low-resource medical settings and democratizes access to advanced language models beyond large technology companies.  - The model can be expanded to vision-language tasks by utilizing Phi-3-vision architecture. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/nyuolab/MedMobile) | [Link](https://huggingface.co/KrithikV/MedMobile) |
| [Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation](https://arxiv.org/abs/2410.13198) | Jian Xue, Peidong Wang, Michael Levit, Mohammad Sadegh Rasooli, Sreyan Ghosh |  - This paper introduces DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach to improve the performance of generative error correction (GEC) models for automatic speech recognition (ASR) systems.  - DARAG addresses limitations of traditional GEC models by augmenting training data with synthetic examples generated by prompting large language models (LLMs) and text-to-speech (TTS) models, simulating realistic ASR errors.  - It also incorporates retrieval augmentation, extracting named entities from the training data and retrieving similar entities during correction to handle novel or unknown named entities more effectively. - Experimental results on various in-domain and out-of-domain settings show that DARAG consistently outperforms baseline methods, with relative word error rate (WER) improvements of 8%-30% in in-domain and 10%-33% in out-of-domain scenarios. -  DARAG improves named entity correction and shows the benefit of using synthetic data in low-resource domain adaptation setting as well. | ['Automatic Speech Recognition', 'Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2410.13618) | Chengwei Sun, Ran Ran, Yujia Wu, Jiwei Wei, Shiym | • LoLDU is a novel Parameter-Efficient Fine-Tuning (PEFT) method that leverages Lower-Diag-Upper (LDU) decomposition to reduce the number of trainable parameters during fine-tuning. • LoLDU initializes low-rank matrices with orthogonal properties using LDU decomposition, focusing on optimizing a diagonal matrix for scaling transformations and dynamic adjustment of a scaling factor to align updates with the target matrix. • LoLDU achieves comparable performance to full fine-tuning and other PEFT methods while drastically reducing trainable parameters, sometimes down to 0.00025% of the original model. • Experimental results across various tasks, including instruction following, natural language understanding, image classification, and image generation, with models ranging from 86 million to 7 billion parameters (LLaMA2, RoBERTa, ViT, and Stable Diffusion) demonstrate LoLDU's effectiveness. • LoLDU excels in preserving pre-trained knowledge and enhancing generalization through the use of orthogonal lower and upper triangular matrices, outperforming LoRA on certain tasks while using significantly fewer parameters. | ['Computer Vision', 'Image Classification', 'Text-to-Image', 'Natural Language Processing', 'Text Generation', 'Text Classification'] | [Link](https://github.com/SKDDJ/LoLDU) | N/A |
| [BenTo: Benchmark Task Reduction with In-Context Transferability](https://arxiv.org/abs/2410.13804) | Lichao Sun, Ming Li, Hongyu Zhao, zhoutianyi | BENTO: Benchmark Task Reduction with In-Context Transferability - This paper introduces a novel benchmark reduction method called BENTO (Benchmark Task Reduction) designed to reduce the evaluation cost of Large Language Models (LLMs).  - BENTO leverages In-Context Transferability (ICT), a training-free approach to estimate the transferability between different tasks using in-context learning.  - By analyzing the ICT matrix and applying spectral clustering, BENTO identifies representative tasks that capture the overall benchmark's essence.  - The paper shows that BENTO can reduce the number of tasks in popular LLM benchmarks like MMLU and FLAN by up to 95% while maintaining evaluation accuracy within a 4% margin of the full benchmark.  - This method is significantly more efficient than existing benchmark reduction techniques as it doesn't rely on computationally expensive fine-tuning or extensive training data. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/tianyi-lab/bento) | N/A |
| [AERO: Softmax-Only LLMs for Efficient Private Inference](https://arxiv.org/abs/2410.13060) | Brandon Reagen, Nandan Kumar Jha |  - AERO, a four-step architectural optimization framework, refines existing large language models (LLMs) for efficient private inference (PI) by removing nonlinearities and reducing FLOPs. - AERO systematically removes nonlinearities such as LayerNorm and GELU, proposes using ReLU in LayerNorm-free models, and designs a Softmax-only architecture tailored for PI. - A novel entropy regularization technique mitigates entropic overload, improving the performance of the Softmax-only model. - AERO achieves up to a 4.23x reduction in communication overhead and a 1.94x speedup in latency compared to the baseline. - Experiments were conducted on GPT-2 and Pythia-70M models, trained from scratch on CodeParrot and Languini datasets, demonstrating improvements across various context sizes and model depths. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant](https://arxiv.org/abs/2410.13360) | Xiangyu Yue, Yu-Feng Li, Changsheng Li, Jiaming Han, Hoar012 |  - This paper introduces Retrieval Augmented Personalization (RAP), a framework for personalizing Multimodal Large Language Models (MLLMs) by integrating user-specific visual concepts without requiring further training. - RAP employs a key-value database to store user-provided concept information (image, name, description), retrieves relevant information using a multimodal retriever based on user input (image and/or text), and feeds both the query and retrieved information to the MLLM for personalized response generation. - A dedicated dataset is created using a pipeline that leverages Gemini to automatically generate personalized captions, descriptions, and question-answer pairs associated with user-provided visual concepts.  - Experimental results show that RAP-MLLMs, trained on this dataset using LLaVA and Phi-3V backbones, achieve superior performance in personalized image captioning and visual question answering compared to finetuning and other personalization methods, while also performing well on standard multimodal benchmarks like MMMU and InfoSeek. - RAP offers real-time concept editing and addition by updating the external database, providing flexibility and eliminating retraining needs, though performance depends on the robustness of the multimodal retriever. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Hoar012/RAP-MLLM) | N/A |
| [MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization](https://arxiv.org/abs/2410.12957) | Shengpeng Ji, Ziang Zhang, Xize Cheng, Siqi Zheng, Ruiqi Li | MuVi is a novel video-to-music generation framework that focuses on semantic alignment and rhythmic synchronization. - MuVi employs a non-autoregressive encoder-decoder architecture, using a pre-trained visual encoder and a flow-matching-based music generator. A visual adaptor connects the two modules and performs efficient compression of high-frame-rate visual features. - A contrastive music-visual pre-training scheme is introduced, utilizing negative samples from temporal shifts and random replacements to enhance rhythmic synchronization. - Experimental results demonstrate MuVi's superior performance over existing methods, achieving improvements in audio quality and temporal synchronization in generated music. | ['Text-to-Audio', 'Multimodal'] | N/A | N/A |
| [Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems](https://arxiv.org/abs/2410.13334) | Isack Lee, hbseong |  - This paper introduces PCJailbreak, a method to analyze how intentional biases in Large Language Models (LLMs), implemented for safety alignment, can be exploited to generate harmful content.  - The method involves using LLM-generated keywords representing contrasting demographic groups in prompts containing harmful requests to assess the model's susceptibility to jailbreak attacks.  - Experiments on various LLMs, including GPT models and open-source alternatives, revealed that intentional biases lead to significant differences in jailbreak success rates between marginalized and privileged groups.  - The paper also proposes PCDefense, a mitigation strategy that uses prompts to adjust biases without the need for additional inference or models, unlike Guard Models.  - The authors advocate for responsible development and deployment of LLMs, emphasizing careful consideration of safety measures to avoid unintended vulnerabilities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation](https://arxiv.org/abs/2410.13293) | Tim Oates, pdx97 |  - This paper introduces SBI-RAG, a Schema-Based Instruction Retrieval-Augmented Generation framework, for enhancing math word problem solving using a Large Language Model (LLM).  - SBI-RAG uses a schema classifier (trained on DistilBERT) to predict the problem's schema, which guides prompt creation for context retrieval using RAG and generates step-by-step solutions using Ollama Llama 3.1.  - The authors evaluate SBI-RAG on GSM8K, comparing it with GPT-4 and GPT-3.5 Turbo, using a "reasoning score" to assess solution quality.  - Results suggest SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially improving student learning.  - The approach incorporates a schema classifier, structured prompt generation, schema-relevant RAG, and a new evaluation metric. | ['Question Answering', 'Text2Text Generation', 'Natural Language Processing'] | N/A | N/A |
| [$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models](https://arxiv.org/abs/2410.13859) | Xiaoshuai Sun, Yiyi Zhou, Jiayi Ji, Gen Luo, YaxinLuo |  - This paper introduces $\gamma$-MoD, a novel mixture-of-depth (MoD) adaptation strategy for enhancing the computational efficiency of existing Multimodal Large Language Models (MLLMs). - $\gamma$-MoD employs a new metric called Rank of Attention Maps (ARank) to identify and replace redundant MLLM layers with MoD layers, dynamically allocating computational resources based on token relevance. - Two key designs, shared vision-language router and masked routing learning, are incorporated to maximize sparsity while preserving performance. - The shared router applies routing to the entire multimodal sequence for better optimization, and masked routing learning prevents critical tokens from being skipped during training. - Experiments on nine benchmarks show that $\gamma$-MoD notably reduces training and inference time while maintaining competitive performance compared to existing dense and sparse MLLMs. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment](https://arxiv.org/abs/2410.09347) | Jun Zhu, Peize Sun, Hang Su, ChenDRAG | - This paper introduces Condition Contrastive Alignment (CCA), a fine-tuning technique for autoregressive (AR) visual generation models to improve sample quality without relying on guided sampling methods like Classifier-Free Guidance (CFG). - CCA fine-tunes pre-trained models by contrasting positive and negative image-condition pairs, directly optimizing the model to achieve the desired target distribution, similar to alignment techniques used in language models. - Experimental results on LlamaGen and VAR models demonstrate significant improvement in guidance-free FID and IS scores after just one epoch of fine-tuning with CCA, achieving performance comparable to CFG while reducing sampling costs. - CCA offers a controllable trade-off between image diversity and fidelity similar to CFG by adjusting a training hyperparameter (λ), further confirming their theoretical connection in targeting the same sampling distribution. - Combining CCA with CFG can lead to further performance gains, showcasing its potential as a complementary technique for enhancing visual generation. | ['Text-to-Image', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/thu-ml/CCA) | N/A |
| [Can MLLMs Understand the Deep Implication Behind Chinese Images?](https://arxiv.org/abs/2410.13854) | Xinrun Du, Yuelin Bai, Xi Feng, zhangysk, MING-ZCH |  - This research introduces CII-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) ability to understand the implications behind Chinese images, including those deeply rooted in Chinese traditional culture. - CII-Bench includes 698 images across diverse domains and visual content types, paired with 800 multiple-choice questions to assess comprehension and reasoning abilities. - Experimental findings reveal a notable performance gap between MLLMs and humans, with models achieving a maximum accuracy of 64.4% compared to human accuracy averaging 78.2%. - A custom evaluation metric is designed using GPT-4 to better evaluate Chinese traditional painting comprehension, revealing model limitations in grasping complex cultural nuances. - Models benefit from image emotion hints in prompts, indicating ongoing struggles with emotional understanding crucial for accurate interpretation. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/MING_X/CII-Bench) | [Link](https://cii-bench.github.io/) |
| [Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key](https://arxiv.org/abs/2410.10210) | Yunlin Mao, Jintao Huang, Daoze, wangxingjun778, Yingda |  - This research introduces a technique for enhancing the long-form output generation capabilities of Large Language Models (LLMs) through minimal tuning with high-quality data. - By curating a smaller, higher-quality dataset from the existing LongWriter-6k dataset, and combining it with a small fraction of alignment data, this method demonstrates comparable performance improvements to more compute-intensive training approaches. - Notably, the new dataset requires just 3.74% of the original training data, improving tuning efficiency by effectively addressing issues with data quality such as mismatched output lengths and missing instructions in the original data. - Evaluations based on length-following score (SL) and writing quality score (SQ) show improvements across various models, including the Qwen and GLM families. - This approach provides an efficient method for enhancing long-form output generation while preserving model coherence and alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://www.modelscope.com/models/swift/MS-LongWriter-GLM4-9B-Chat), [Link](https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2-7B-Instruct), [Link](https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2.5-7B-instruct), [Link](https://www.modelscope.com/datasets/ZhipuAI/LongWriter-6k), [Link](https://huggingface.co/datasets/THUDM/LongWriter-6k), [Link](https://huggingface.co/THUDM/LongWriter-glm4-9b), [Link](https://github.com/modelscope/evalscope/tree/main/evalscope/third_party/longbench_write), [Link](https://www.modelscope.com/datasets/swift/longwriter-6k-filtered), [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-Chinese), [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-English), [Link](https://huggingface.co/THUDM/glm-4-9b) |
| [TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration](https://arxiv.org/abs/2410.12183) | Yali Wang, Yu Qiao, Kunchang Li, Shaobin Zhuang, markywg | TransAgent is a novel framework that transfers knowledge from heterogeneous vision, language, and multimodal agents to enhance the generalization of Vision-Language (V-L) foundation models like CLIP. - It leverages 11 different pre-trained agents covering various tasks and modalities, including visual recognition, dense prediction, chatbot, text encoding, multimodal generation, and captioning. - The knowledge transfer is achieved through a unified distillation framework, where a Mixture-of-Agents (MoA) gating mechanism adaptively integrates knowledge from different agents. - TransAgent achieves state-of-the-art performance on 11 visual recognition datasets, outperforming CoOp by approximately 10% on average and 20% on EuroSAT under the same low-shot setting. - All pre-trained agent models can be unloaded after distillation, resulting in efficient deployment with no need for model ensembles in the inference phase. | ['Zero-Shot Image Classification', 'Image Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/markywg/transagent) | N/A |


## Papers for 2024-10-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks](https://arxiv.org/abs/2410.12381) | Xiao Li, Guancheng Lin, Huiyu Bai, Linquan Wu, zfj1998 | - HumanEval-V is introduced; a novel benchmark designed to evaluate the visual understanding and reasoning abilities of Large Multimodal Models (LMMs) through Python code generation tasks.  - The benchmark comprises 108 coding tasks adapted from platforms like CodeForces and Stack Overflow, each requiring LMMs to integrate visual and textual information to generate functional code.  - Evaluation results for 19 state-of-the-art LMMs reveal that even leading proprietary models struggle, with GPT-4o achieving 13% pass@1, highlighting limitations in visual reasoning and coding abilities.  -  Ablation studies indicate current LMMs have limitations in vision reasoning and coding capabilities, showing significant performance improvement when image descriptions are provided. - Further analysis reveals that open-weight LMMs suffer deteriorated coding performance after vision-encoder integration, suggesting areas for future LMM research. | ['Multimodal', 'Computer Vision', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/HumanEval-V/HumanEval-V-Benchmark) | N/A |
| [VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI](https://arxiv.org/abs/2410.11623) | Sicheng Zhou, Yangyang Yu, Kechen Fang, yetian, SijieCheng |  - VidEgoThink is introduced; a benchmark designed to assess egocentric video understanding capabilities for embodied AI, focusing on bridging the gap between Multimodal Large Language Models (MLLMs) and low-level control. - It incorporates four tasks: video question answering, hierarchical planning, visual grounding, and reward modeling.  - Leverages GPT-4 to generate data automatically, which is filtered by human annotators.  This pipeline is based on the Ego4D dataset. - Experimental evaluation of various MLLMs, including GPT-4, open-source image and video-based models, reveals poor performance across all tasks, particularly in sequence and order understanding.  - Findings indicate a need for significant advancements in foundational models for first-person Embodied AI applications. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text', 'Robotics'] | N/A | N/A |
| [The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio](https://arxiv.org/abs/2410.12787) | Hang Zhang, Yang Zhou, Yun Xing, Sicong Leng, ClownRat | - This paper investigates hallucinations in Large Multimodal Models (LMMs) across language, visual, and audio modalities. - Two key contributors to hallucinations are identified: overreliance on unimodal priors and spurious inter-modality correlations. - The Curse of Multi-Modalities (CMM) benchmark is introduced, which provides a detailed analysis of these underlying issues. - CMM converts hallucination evaluation into a binary classification task with object-level and event-level probing across 1200 samples with 2400 probing questions. - Experimental results reveal key vulnerabilities, including imbalances in modality integration and biases from training data. | ['Multimodal'] | [Link](github.com/DAMO-NLP-SG/CMM) | [Link](cmm-damovl.site) |
| [Revealing the Barriers of Language Agents in Planning](https://arxiv.org/abs/2410.12409) | Kai Zhang, Siyu Yuan, jiangjiechen, kexunz, hsaest |  - This paper investigates the limitations of current large language models (LLMs) in planning tasks using feature attribution analysis.  - It identifies two key weaknesses: a limited understanding of constraints and the diminishing influence of questions as the planning horizon expands. - The study explores episodic and parametric memory updating strategies, finding that while they improve constraint and question utilization, they do not fully resolve the core issues. - The episodic memory updating reiterates constraints, making them easier for agents to recognize, but agents primarily understand it on a global level. -  Parametric memory updating enhances the impact of questions, yet agents still lose focus on them as the horizon increases; both strategies resemble shortcut learning and are insufficient for high-level planning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Exploring Model Kinship for Merging Large Language Models](https://arxiv.org/abs/2410.12613) | Huajun Chen, Shumin Deng, Ningyu Zhang, Yunzhi Yao, Yedi Hu |  - This paper introduces "model kinship", a metric to assess the similarity between Large Language Models (LLMs), drawing an analogy to biological kinship, for enhanced model merging. - It is shown empirically that model kinship correlates with performance gains after merging, which helps guide the selection of candidate models for merging and escape local optima. - A novel merging strategy, "Top-k Greedy Merging with Model Kinship", is proposed, demonstrating improved performance on benchmark datasets by mitigating performance degradation and avoiding local optima during model evolution. - The analysis of model evolution through iterative merging reveals two distinct stages: a learning stage with rapid performance improvement and a saturation stage where improvements plateau, with the latter attributed to weight space convergence and high kinship values. - Model kinship is further suggested as a criterion for early stopping in the merging process, which improves efficiency without compromising performance gains.  | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/ModelKinship) | N/A |
| [Large Language Model Evaluation via Matrix Nuclear-Norm](https://arxiv.org/abs/2410.10672) | Yi Chang, Yahan Li, WhiteCatY, xiatingyu | • This paper introduces Matrix Nuclear-Norm, a novel metric for evaluating the information compression and redundancy reduction capabilities of Large Language Models (LLMs). • The metric leverages the nuclear norm and its L1,2-norm approximation to quantify the data compression proficiency of LLMs. • Matrix Nuclear-Norm addresses the computational limitations of existing metrics like Matrix Entropy by reducing the time complexity from O(n³) to O(n²), eliminating the need for Singular Value Decomposition (SVD). • Experimental results on various LLMs, including Cerebras-GPT and Pythia, demonstrate that Matrix Nuclear-Norm effectively captures compression capabilities with values decreasing as model size increases. • Evaluations on benchmark datasets like AlpacaEval and Chatbot Arena confirm that the proposed metric reliably assesses and ranks model performance, achieving a balance between accuracy and computational efficiency. | ['Natural Language Processing'] | [Link](https://github.com/MLGroupJLU/MatrixNuclearNorm) | N/A |
| [ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs](https://arxiv.org/abs/2410.12405) | Dahua Lin, Xinyu Fang, KennyUTC, zsytony, JingmingZ | • ProSA, a framework designed to evaluate and understand prompt sensitivity in LLMs, is introduced, incorporating a novel sensitivity metric, PromptSensiScore (PSS), and leveraging decoding confidence. • PSS quantifies the average discrepancy in LLM responses when given different semantic variants of the same instruction. • The study, spanning multiple tasks and models, reveals that prompt sensitivity varies across datasets and models, with larger models generally exhibiting better robustness, and few-shot examples, especially for larger models, mitigate sensitivity. • Subjective evaluations highlight increased sensitivity in complex reasoning tasks compared to straightforward ones, with higher model confidence correlating with increased prompt robustness. • Prompt sensitivity is linked to decoding confidence, where greater confidence corresponds to higher robustness against prompt variations. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/ProSA) | N/A |
| [ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression](https://arxiv.org/abs/2410.08584) | Wenqi Shao, Jing Liu, Feng Chen, Yefei He, kpzhang996 | - ZipVL is an efficient inference framework for Large Vision-Language Models (LVLMs) that addresses computational and memory bottlenecks through dynamic token sparsification and KV cache compression. - It employs a layer-wise adaptive ratio assignment for important tokens based on attention score distribution, optimizing both prefill and decoding phases. - The prefill phase is accelerated by performing attention only on important tokens, seamlessly integrating with existing attention implementations. - Mixed-precision quantization is applied to the KV cache, using higher bit-width for important tokens and lower bit-width for others, reducing memory usage without significant performance loss. - Experiments show ZipVL accelerates prefill by 2.6x and reduces GPU memory by 50% with minimal accuracy reduction on Video-MME, outperforming fixed-ratio methods like FastV. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Video-Text-to-Text'] | N/A | N/A |
| [Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL](https://arxiv.org/abs/2410.12491) | Sonali Parbhoo, Arjun Jagota, Jared Joselowitz, skrishna | • This paper introduces a novel approach to interpreting Large Language Models (LLMs) by applying Inverse Reinforcement Learning (IRL) to recover their implicit reward functions, focusing on toxicity-aligned LLMs. • Experiments conducted on toxicity-aligned LLMs of varying sizes extracted reward models that achieved up to 80.40% accuracy in predicting human preferences. • The analysis reveals insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the Reinforcement Learning from Human Feedback (RLHF) process. • The study demonstrates that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. • The paper proposes that this work provides a new perspective for understanding and improving LLM alignment, with implications for responsible development. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |
| [WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation](https://arxiv.org/abs/2410.12722) | Juan Carlos Climent Pardo, Yingya Li, Siena Placino, João Matos, shanchen |  - WorldMedQA-V is a new multilingual and multimodal dataset designed to evaluate the performance of multimodal language models (VLMs) on medical question answering tasks. - The dataset consists of 568 multiple-choice questions with images from real medical exams in Brazil, Israel, Japan, and Spain. - Evaluations of several popular open and closed-source VLMs reveal that GPT4o achieved the best performance, generally exceeding passing thresholds across countries and both local languages and English translations. - Including the associated image with the medical question generally improves the model performance, particularly for models with lower baseline accuracies. - The results also highlight persistent language disparities, where models showed relatively lower performance on Hebrew, potentially due to underrepresentation in pre-training datasets. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/WorldMedQA/V) | [Link](https://huggingface.co/datasets/WorldMedQA/V) |
| [OMCAT: Omni Context Aware Transformer](https://arxiv.org/abs/2410.12109) | Andrew Tao, Rafael Valle, Matthieu Le, Karan Sapra, goarushi27 |  - The paper introduces OMCAT (Omni Context Aware Transformer), a novel multimodal large language model designed for enhanced temporal understanding and cross-modal alignment in audio-visual contexts.  - OMCAT leverages ROTE (Rotary Time Embeddings), a modification of RoPE (Rotary Position Embeddings), to encode absolute and relative temporal information, improving performance on time-anchored tasks. -  A new dataset, OCTAV (Omni Context and Temporal Audio Video), is also introduced, focusing on event transitions within videos and their correlation with audio cues, facilitating training for fine-grained temporal reasoning.  -  OMCAT undergoes a three-stage training process: feature alignment, instruction tuning, and OCTAV-specific training, achieving state-of-the-art results on Audio-Visual Question Answering (AVQA) and temporal video grounding benchmarks, surpassing existing models on the OCTAV dataset by a significant margin.  - The paper's contributions include a new model and dataset, demonstrating significant advancements in multimodal LLMs' capacity for fine-grained temporal and cross-modal understanding. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Tracking Universal Features Through Fine-Tuning and Model Merging](https://arxiv.org/abs/2410.12391) | Desmond Elliott, nilq |  - This paper investigates the evolution of features in one-layer Transformer language models during fine-tuning and merging. - The study uses sparse autoencoders to extract and track features across models trained on different domains (English text, Python, Lua, TinyStories). - Findings reveal that few features persist across models, but those that do are often interpretable, relating to code-related elements like punctuation and formatting. - Case studies highlight a persistent variable assignment feature and a disappearing Python exception-handling feature. - The paper contributes to understanding feature dynamics in transfer learning scenarios. | ['Natural Language Processing', 'Feature Extraction'] | N/A | N/A |
| [DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities](https://arxiv.org/abs/2410.07722) | Jeff Dalton, Iain Mackie, Sean MacAvaney, Shubham Chatterjee, Thong Nguyen | - DyVo, a novel dynamic vocabulary model, is introduced to enhance Learned Sparse Retrieval (LSR) by incorporating Wikipedia entities into the vocabulary. - The model utilizes a Dynamic Vocabulary (DyVo) head which leverages existing entity embeddings and an entity retrieval component to generate entity weights. - These weights are merged with word piece weights and used for efficient indexing and retrieval using an inverted index. - Experiments on three entity-rich document ranking datasets show DyVo consistently outperforms state-of-the-art baselines, demonstrating significant improvements over traditional LSR models by incorporating entities. - A few-shot generative entity retrieval approach using LLMs like Mixtral and GPT-4 is introduced, generating highly relevant entity candidates leading to superior performance compared to using linked entities or entities found by human annotators. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/thongnt99/DyVo) | N/A |


## Papers for 2024-10-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation](https://arxiv.org/abs/2410.11779) | Haoming Xu, Bozhong Tian, Xiang Chen, Chenxi Wang, Ningyu | This paper introduces DeCo, a novel dynamic correction decoding method for Multimodal Large Language Models (MLLMs) designed to mitigate hallucinations by leveraging information from preceding layers. - DeCo dynamically selects an appropriate preceding layer ("anchor layer") based on the probabilities of candidate tokens, and integrates its knowledge into the final layer to adjust the output logits, thereby correcting potential hallucinations. - The method is training-free and model-agnostic, compatible with various decoding strategies (greedy search, nucleus sampling, beam search) and applicable to different MLLMs. - Experimental results on benchmarks like CHAIR and POPE demonstrate that DeCo significantly reduces hallucination rates compared to baselines and existing methods like OPERA and VCD, with an average suppression rate of 10.8% on image captioning datasets. - Empirical analysis suggests that MLLMs can recognize visual objects in earlier layers, but this recognition is suppressed in later layers due to language model priors, leading to hallucinations. DeCo addresses this by correcting final-layer logits using more accurate information from preceding layers. - Further analysis shows DeCo is also effective in reducing snowballing hallucinations, where an initial hallucination leads to a cascade of errors. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/zjunlp/DeCo) | N/A |
| [MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models](https://arxiv.org/abs/2410.11710) | Xiaoshuai Song, Jiaheng Liu, Zekun Wang, Yanan Wu, Pei Wang |  - This paper introduces MTU-Bench, a multi-granularity tool-use benchmark designed to evaluate large language models' (LLMs) ability to interact with external tools.  - MTU-Bench consists of two main components: MTU-Instruct, a diverse instruction tuning dataset for training LLMs on tool usage, and MTU-Eval, a comprehensive evaluation framework featuring fine-grained metrics that assess various tool-use scenarios without relying on GPT-based evaluation.  - The authors propose a novel automated data synthesis pipeline based on existing task-oriented dialogue datasets to create MTU-Bench.  - The benchmark covers five tool usage scenes: single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks.  - Experiments demonstrate that fine-tuning LLaMA on MTU-Bench yields a robust model, MTU-LLaMA, with improved performance in various tool-use scenarios, outperforming the baseline model and demonstrating the efficacy of the MTU-Instruct dataset. | ['Natural Language Processing'] | [Link](https://github.com/MTU-Bench-Team/MTU-Bench.git) | N/A |
| [SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI](https://arxiv.org/abs/2410.11096) | Wenbo Guo, Yuheng Tang, Zhun Wang, Yuzhou Nie, yuyangy |  - This paper introduces SECCODEPLT, a unified evaluation platform designed to assess the security risks of code generation AI models concerning insecure coding and cyberattack helpfulness.  - For insecure coding, a two-stage data creation pipeline is employed, combining expert-crafted seed examples with LLM-based mutation and dynamic testing to ensure benchmark quality and scalability, covering 27 critical Python CWEs compared to existing benchmarks' 8.  - For cyberattack helpfulness, a real-world attack environment with dynamic metrics is designed to evaluate models' capabilities across different attack stages based on MITRE ATT&CK.  - Experimental results indicate SECCODEPLT outperforms CYBERSECEVAL in benchmark quality and reveals higher risks in SOTA models, including GPT-40 and Claude’s capability to generate end-to-end attacks, also uncovering risks in the code agent Cursor where it failed on code injection, access control and data leakage prevention CWEs.  - Additionally, providing security policy reminders significantly improves model performance in secure coding by 30%. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/Virtue-AI-HUB/SecCodePLT) |
| [LVD-2M: A Long-take Video Dataset with Temporally Dense Captions](https://arxiv.org/abs/2410.10816) | Zhijie Lin, Daquan Zhou, Yuqing Wang, XihuiLiu, YuuTennYi |  - This paper introduces LVD-2M, a large-scale dataset of 2 million long-take videos with temporally dense captions, designed to address the limitations of existing datasets for training long video generation models. - The dataset creation involved an automatic pipeline with low-level filtering (scene cut detection, optical flow) and semantic-level filtering (video LLMs) to select high-quality videos, and a hierarchical captioning approach combining LLaVA and Claude3-Haiku to generate detailed descriptions of video content over time. - Human evaluations show LVD-2M surpasses other datasets in long-take consistency, dynamic degree, and caption quality. - Fine-tuning experiments with both diffusion-based and LM-based video generation models demonstrate that LVD-2M improves their ability to generate longer, more dynamic videos with smoother transitions and camera motions. - The authors argue that LVD-2M will significantly benefit future research in long video generation. | ['Text-to-Video', 'Video-Text-to-Text', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/SilentView/LVD-2M) | N/A |
| [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786) | Zheyu Shen, Guoheng Sun, Shwai He, charleslipku | -"What Matters in Transformers? Not All Attention is Needed" introduces a method called "Joint Layer Drop" to efficiently prune redundant attention and MLP layers in Transformer-based language models. -The method identifies these layers using a similarity-based metric, removing those with minimal transformation between input and output, and prioritizes dropping attention layers due to their observed higher redundancy. -Experiments demonstrate that removing a substantial portion of attention layers (e.g., 50% in Llama-2-70B) leads to minimal performance degradation while significantly improving inference speed (48.4% speedup with a 2.4% performance drop). -The redundancy in attention layers is found to be consistent throughout training, suggesting it's an inherent property, and Joint Layer Drop allows for even more aggressive pruning by targeting both attention and MLP layers for increased efficiency. -This work reveals that not all attention layers are equally important in transformers, leading to potential improvements in future network architecture designs and training techniques. | ['Natural Language Processing', 'Feature Extraction'] | [Link](https://github.com/Shwai-He/LLM-Drop) | [Link](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k) |


## Papers for 2024-10-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2410.10139) | WendellZwh, wangzhaoyang, StarThomas1002, Lillianwei, richardxp888 | • MMIE is a large-scale benchmark designed to evaluate the interleaved multimodal comprehension and generation capabilities of Large Vision-Language Models (LVLMs). • The benchmark comprises 20K meticulously curated multimodal queries across diverse fields, supporting both interleaved inputs and outputs in multiple-choice and open-ended formats. • An automated evaluation metric is proposed based on a fine-tuned InternVL-2-4B scoring model, which demonstrates strong alignment with human evaluation and mitigates potential biases.  • Experimental results reveal that even state-of-the-art LVLMs and the combination of advanced LLMs with text-to-image models face significant challenges in MMIE, with most achieving moderate performance, indicating substantial room for improvement.  • Error analysis categorizes key challenges into temporal understanding (cross-modality coherence, generation adaptability) and reasoning (multimodal information comprehension, complex reasoning) skills. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://mmie-bench.github.io/) |
| [LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models](https://arxiv.org/abs/2410.09732) | Junan Zhang, Zilong Huang, beccabai, bczhou, Yejy53 | - LOKI, a new benchmark designed to evaluate large multimodal models (LMMs) on synthetic data detection across various modalities (video, image, 3D, text, and audio), has been introduced. - The benchmark includes 18K questions across 26 subcategories, with multi-level annotations including coarse-grained and multiple-choice questions, and fine-grained anomaly selection and explanation tasks. - An evaluation of 22 open-source and 6 closed-source LMMs on LOKI has revealed their potential as synthetic data detectors while also showing limitations such as model biases, a lack of expert domain knowledge, and unbalanced multimodal capabilities. - While LMMs exhibited moderate capabilities with some levels of explainability and generalization, they still lag behind human performance in synthetic data detection tasks. - Chain-of-thought prompting improved the performance of most LMMs, but not GPT-4, suggesting that GPT-4 already exhibits strong reasoning capabilities for this task. | ['Multimodal', 'Computer Vision'] | N/A | N/A |
| [Toward General Instruction-Following Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.09584) | Zhicheng Dou, Runqi Qiao, Yutao Zhu, Xiaoshuai Song, Guanting Dong |   - This paper introduces VIF-RAG, an automated, scalable, and verifiable data synthesis pipeline designed to improve instruction-following alignment in Retrieval-Augmented Generation (RAG) systems.  - VIF-RAG begins with a small set of manually crafted atomic instructions and uses a combination of rule-based composition, supervised rewriting, and code-based verification to generate a large-scale dataset (VIF-RAG-QA) of instruction-following data for RAG.  - It also presents FollowRAG, a new benchmark for evaluating complex instruction-following capabilities in RAG, composed of 2.8K samples covering 22 categories of general instruction constraints and 4 knowledge-intensive QA datasets.  - In experiments, VIF-RAG significantly boosts performance across various LLMs and datasets, demonstrating a remarkable 44% improvement over the Llama3-base model in instruction-following within RAG scenarios.  - The results further indicate that VIF-RAG not only enhances IF capability but also maintains stability in RAG performance across different model sizes and datasets, offering promise for real-world applications. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://arxiv.org/abs/2410.10563) | wenhu, yuexiang96, DongfuJiang, yuanshengni, shermansiu |   - MEGA-BENCH is a multimodal evaluation benchmark comprising over 500 real-world tasks designed to assess the diverse capabilities of contemporary vision-language models.  - The benchmark employs a taxonomy of multimodal tasks and incorporates diverse output formats, moving beyond standard multiple-choice questions to include numbers, phrases, code, LaTeX, and coordinates.  -  A range of over 40 unique evaluation metrics, including rule-based and LLM-assisted options, is used to accommodate these diverse formats.  -  In evaluations, MEGA-BENCH demonstrated GPT-4's superior performance over other flagship models, and Qwen2-VL's leading performance among open-source models.  -  The benchmark facilitates fine-grained capability analysis by offering a breakdown of model performance across various dimensions such as input/output format and required skill.  | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | N/A |
| [LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content](https://arxiv.org/abs/2410.10783) | M. Jehanzeb Mirza, Sivan Doveh, Felipe Maia Polo, Nimrod Shabtay, wlin21at |  - LiveXiv is a novel, fully automated, multimodal live benchmark focusing on scientific domains, designed to address test set contamination and provide an updated evaluation of Large Multi-modal Models (LMMs). - It uses scientific papers from arXiv to generate Visual Question Answering (VQA) and Table Question Answering (TQA) pairs automatically, avoiding human bias and ensuring scalability. - An efficient evaluation pipeline based on Item Response Theory (IRT) allows for performance estimation on new benchmark versions by reevaluating only a small subset of models, significantly reducing computational costs. - The benchmark has been evaluated with 17 prominent open and proprietary LMMs, demonstrating its challenging nature and exposing model capabilities on less-contaminated data. - It provides the first version of the dataset including VQA and TQA pairs, alongside an efficient evaluation methodology and benchmark results, along with its limitations. | ['Visual Question Answering', 'Table Question Answering', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/IBM/LiveXiv) |
| [TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models](https://arxiv.org/abs/2410.10818) | Jianrui Zhang, Reuben Tan, Mu Cai, fengyao1909, BochengZou |  - TemporalBench, a novel video understanding benchmark, is introduced to evaluate the fine-grained temporal understanding abilities of multimodal video models. - The benchmark consists of ~10K video question-answer pairs derived from ~2K human-annotated captions with rich activity details, focusing on long-range dependencies and event progression. -  State-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, significantly lower than human performance (67.9%). - A critical pitfall in multi-choice QA is identified where LLMs can detect subtle changes in negative captions and find a "centralized" description as a cue for prediction. - Multiple Binary Accuracy (MBA) is proposed to correct such bias by decomposing multi-choice QA into multiple binary QAs. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://TemporalBench.github.io/) | N/A |
| [Tree of Problems: Improving structured problem solving with compositionality](https://arxiv.org/abs/2410.06634) | Rachel Bawden, Benoît Sagot, Armel Zebaze | - This research paper proposes Tree of Problems (ToP), a novel prompting approach for enhancing the problem-solving abilities of Large Language Models (LLMs). - ToP decomposes complex problems into a tree structure of simpler, analogous subproblems, leveraging compositionality for efficient problem-solving, and drawing inspiration from techniques like divide-and-conquer. - Empirical results demonstrate that ToP outperforms existing methods like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of Thoughts (GoT) on structured tasks. - Furthermore, ToP excels in out-of-distribution generalization scenarios. - The authors provide evidence of superior performance across various LLMs, including GPT-3.5, on difficult benchmark tasks such as Last Letter Concatenation and Navigate from BIG-Bench Hard. | ['Natural Language Processing'] | [Link](https://github.com/ArmelRandy/tree-of-problems) | N/A |
| [LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813) | Kai-Wei Chang, Yuwei Zhang, Wenhao Yu, Hongwei Wang, xiaowu0162 | - LongMemEval, a comprehensive benchmark designed to evaluate the long-term memory capabilities of chat assistants.  - It focuses on five core abilities: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. - The benchmark consists of 500 meticulously curated questions embedded within freely scalable user-assistant chat histories. - A unified framework is presented that breaks down long-term memory design into four design choices across indexing, retrieval, and reading stages.  - Several memory designs, including session decomposition, fact-augmented key expansion, and time-aware query expansion, are proposed and shown to greatly improve both memory recall and downstream question answering. | ['Question Answering'] | [Link](https://github.com/xiaowu0162/LongMemEval) | N/A |


## Papers for 2024-10-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Baichuan-Omni Technical Report](https://arxiv.org/abs/2410.08565) | kenshinn, dbv, dongguosheng, TJU-Tianpengli, lin5547 |  - Baichuan-Omni is a 7B Multimodal Large Language Model (MLLM) capable of processing image, video, audio, and text modalities concurrently. - The model architecture involves separate encoders for each modality, projectors to map these encodings into the language model's embedding space, and a shared decoder. - The training process consists of two phases: multimodal alignment pre-training and multitask fine-tuning, using a diverse dataset of open-source, synthetic, and internally annotated data. - Evaluation across various benchmarks demonstrates that Baichuan-Omni outperforms existing open-source omni-modal models like VITA and achieves competitive results compared to closed-source models like GPT-4, particularly excelling in Chinese benchmarks and audio tasks. - Real-time interaction is facilitated by predicting audio input boundaries while concurrently processing visual data, enhancing dynamic attention calculation and streaming capabilities. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Video-Text-to-Text', 'Any-to-Any', 'Audio', 'Automatic Speech Recognition', 'Text Generation'] | [Link](https://github.com/westlake-baichuan-mllm/bc-omni) | N/A |
| [SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights](https://arxiv.org/abs/2410.09008) | Joseph E. Gonzalez, Minkai Xu, Tianjun Zhang, Zhaochen Yu, Ling Yang | SuperCorrect is a novel two-stage framework that leverages a large teacher model to supervise and correct the reasoning and reflection processes of a smaller student model, thereby improving mathematical reasoning and self-correction abilities.  The first stage uses hierarchical thought templates extracted from the teacher model to guide the student in generating more fine-grained reasoning thoughts.  The second stage employs cross-model collaborative direct preference optimization (DPO) to refine the student model's self-correction capabilities by following the teacher's correction traces during training.  Experimental results show that SuperCorrect achieves state-of-the-art performance among 7B models on MATH and GSM8K benchmarks, outperforming DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3%, respectively and surpasses models that are larger, such as Llama3-70B. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/YangLing0818/SuperCorrect-llm) | N/A |
| [Mechanistic Permutability: Match Features Across Layers](https://arxiv.org/abs/2410.07656) | Ian Maksimov, kefirski, elephantmipt |  - This paper introduces SAE Match, a data-free method for aligning Sparse Autoencoder (SAE) features across different layers of a neural network, addressing the challenge of understanding feature evolution and polysematicity in large language models (LLMs). - The method involves matching features by minimizing the mean squared error (MSE) between the "folded" parameters of SAEs, a technique that integrates activation thresholds into encoder and decoder weights to account for differences in feature scales. - Experiments on the Gemma 2 language model demonstrate improved feature matching quality compared to methods without parameter folding and provide insights into feature persistence and transformation across layers. - The approach also shows potential for approximating hidden states across layers, effectively skipping intermediate layers with minimal performance loss, especially in later layers where features are more monosemantic.  -  Evaluation using external LLMs and matching scores shows that feature similarity gradually declines over several layers but remains significant for approximately five layers, while initial layers appear to exhibit higher polysematicity, making feature matching more challenging in these layers. | ['Natural Language Processing', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/datasets/loubnabnl/github-small-near-dedup) |
| [Mentor-KD: Making Small Language Models Better Multi-step Reasoners](https://arxiv.org/abs/2410.09037) | SKyii, monocrat23, nokomon |  - Mentor-KD is a novel reasoning distillation framework that improves the multi-step reasoning capabilities of small language models (LLMs) by addressing the limitations of insufficient distillation sets from large LLM teachers. - It introduces a mentor model, an intermediate-sized task-specific model, to augment the distillation sets by generating additional chain-of-thought (CoT) rationales and soft labels for the student model. - Through extensive experiments, Mentor-KD has shown to improve student performance and outperform existing reasoning distillation baselines on complex reasoning tasks, including commonsense, arithmetic, logical, and symbolic reasoning. - Notably, the student models trained with Mentor-KD sometimes even surpassed the performance of the LLM teacher (GPT-3.5) on certain tasks. - The framework also proved effective in low-resource scenarios, offering performance improvements even with limited distillation sets, which showcases its cost-efficiency. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/2hojae/mentor-kd) | N/A |
| [DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models](https://arxiv.org/abs/2410.07331) | Yiming Huang, lx865712528, bjEdward, FangyuLei, Jianwen2003 |  - Introduces DA-Code, a benchmark designed to evaluate Large Language Models (LLMs) on agent-based data science tasks.  - DA-Code features challenging tasks requiring advanced coding skills, diverse real-world data sources, and complex data science programming languages (Python, SQL, Bash).  - A controllable and executable environment simulating real-world scenarios is provided, along with a meticulously designed evaluation suite and a DA-Agent baseline.  - Experimental results show that even state-of-the-art LLMs achieve only 30.5% accuracy on DA-Code, indicating significant room for improvement in LLM-agent capabilities.  - The benchmark and baseline are released to facilitate research in this area. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://da-code-bench.github.io) | N/A |


## Papers for 2024-10-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code](https://arxiv.org/abs/2410.08196) | juntingpan, shiwk20, Houxing, scikkk, AJZhou |  - MathCoder2, a new family of models, enhances mathematical reasoning in Large Language Models (LLMs) through continued pretraining on a 19.2B token dataset named MathCode-Pile, which pairs mathematical code with corresponding natural language reasoning steps. - The MathCode-Pile dataset was constructed by filtering and combining various math-related data sources, including web data, synthetic data, code using math packages, textbooks, and model-translated mathematical code. - A novel method was introduced to extract reasoning steps (conditions, LaTeX expressions, and results) from text using Llama 3.1-70B Instruct, subsequently translated into executable Python snippets. - MathCoder2-Llama-3-8B, a model from the MathCoder2 family, achieves 4-shot accuracies of 38.4% on MATH and 69.9% on GSM8K, improving upon the baseline by 3.1% and 4.1% respectively. - The complete data processing and training code, along with the dataset, is open-sourced for transparency and reproducibility. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/mathllm/MathCoder2) | N/A |
| [PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs](https://arxiv.org/abs/2410.05265) | Yi Bin, Jiahao Wang, Yi Liu, wqshao126, ChenMnZ | • PrefixQuant is a novel quantization technique for Large Language Models (LLMs) that leverages the observation that outlier tokens often appear at predictable locations or have low semantic value.  • The technique involves offline identification and prefixing of these outlier tokens in the key-value cache to prevent their generation during inference, enabling the use of per-tensor static quantization.  • This method enables static quantization to outperform the more computationally expensive per-token dynamic quantization.  • The authors demonstrate PrefixQuant's efficacy on Llama-2, Llama-3, and other LLMs, achieving perplexity improvements and accuracy gains over existing methods like QuaRot while also improving inference speed.  • For example, in a W4A4KV4 quantized Llama-3-8B model, PrefixQuant attains a 7.43 WikiText2 perplexity and 71.08% average accuracy on five common sense reasoning tasks, surpassing QuaRot by 0.98 perplexity and 5.98 accuracy points. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ChenMnZ/PrefixQuant) | N/A |
| [MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents](https://arxiv.org/abs/2410.03450) | Zongqing Lu, Xinru Xu, tellarin, yuejunpengpku | - MART (MLLM As ReTriever) is a new approach for multimodal retrieval in embodied agents, using interactive learning to fine-tune an MLLM retriever to assess trajectory effectiveness. - It leverages interaction data and preference learning to prioritize trajectories that are most beneficial for unseen tasks, addressing limitations of current retrieval methods that focus on surface-level similarities. - It introduces Trajectory Abstraction, a mechanism using MLLMs' summarization capabilities to condense trajectories while preserving key information, improving comprehension and efficiency in long-horizon tasks. - Experimental results across various environments show that MART significantly improves task success rates in unseen scenes compared to baselines, often exceeding 10% improvement. - MART offers a new paradigm for multimodal retrieval, adapting general-purpose MLLMs as retrievers for embodied agents to consider the task-specific relevance of retrieved information. | ['Multimodal', 'Robotics', 'Reinforcement Learning'] | N/A | N/A |
| [DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models](https://arxiv.org/abs/2410.08207) | akashsri, FelixXu, quandao10, ligongh, AristHe | - DICE (Discrete Inversion for Controllable Editing) is introduced as the first method to enable precise inversion for discrete diffusion models such as VQ-Diffusion, Paella, and masked generative models such as RoBERTa. - DICE enhances the editability of these models by recording noise sequences or masking patterns in the reverse sampling process, allowing for accurate reconstruction and controlled editing without reliance on predefined masks or attention manipulations. - The method's effectiveness has been demonstrated in image and text modalities.  For image editing, the experimental results on PIE-Bench using Paella show that the proposed method achieves lower structure distance while preserving background as well as competitive CLIP similarity compared to baselines including DDIM inversion with Stable Diffusion v1.4 and masked inpainting. - For text editing, using RoBERTa as the language model, DICE shows the ability to adjust a sentence’s sentiment without altering its original structure, outperforming masked generation by a large margin based on structure preservation and sentiment correctness evaluation using ChatGPT-4. - A novel text-editing dataset, Sentiment Editing, focusing on controlled sentiment adjustments in sentences while preserving their structure and theme, is presented | ['Text-to-Image', 'Image-to-Image', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Benchmarking Agentic Workflow Generation](https://arxiv.org/abs/2410.07869) | Ningyu, xiaoyuehanbin, consultantQ, Runnaning, GoooDte | WORFBENCH, a unified workflow generation benchmark featuring diverse scenarios and complex graph workflow structures, is introduced to evaluate Large Language Model (LLM) agents' ability to decompose problems into executable workflows. - WORFEVAL, a systematic evaluation protocol employing subsequence and subgraph matching algorithms, is presented to rigorously assess workflow generation capabilities. - Evaluations across different LLMs reveal performance gaps between sequence and graph planning, with GPT-4 showing a 15% gap. - Two open-source models are trained and evaluated, demonstrating improved but limited generalization on held-out tasks. - Generated workflows enhance downstream tasks by serving as Chain-of-Thought augmentation and prior knowledge, enabling superior performance with reduced inference time through parallel and shortened planning steps. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/zjunlp/WorFBench) | N/A |
| [Agent S: An Open Agentic Framework that Uses Computers Like a Human](https://arxiv.org/abs/2410.08164) | Shuyu Gan, Saaket Agashe, xw-eric, jc-y42, Jiuzhouh |  - Agent S is introduced as an open agentic framework designed for autonomous interaction with computers through a GUI, aiming to automate complex multi-step tasks. - The framework utilizes experience-augmented hierarchical planning, learning from both external web knowledge searches and internal experience retrieval to plan and execute subtasks efficiently. - It employs an Agent-Computer Interface (ACI) that improves grounding by using vision-augmented accessibility tree observations and restricts the agent's action space to enhance safety and control. - Evaluation on the OSWorld benchmark demonstrates a significant performance improvement, achieving a 9.37% higher success rate than the baseline and establishing a new state-of-the-art, with consistent improvement across five categories of computer tasks. - Further evaluation on the WindowsAgentArena benchmark reveals the framework's broad generalizability to different operating systems with an improvement from 13.3% to 18.2% on an equivalent setup without explicit adaption. | ['Multimodal'] | [Link](https://github.com/simular-ai/Agent-S) | N/A |
| [Intriguing Properties of Large Language and Vision Models](https://arxiv.org/abs/2410.04751) | Ho-Jin Choi, yechan99, mkmiracle, kobiso, passing2961 | • This paper investigates the intriguing properties of Large Language and Vision Models (LLVMs), focusing on how they perceive and process images.  • The study evaluates the performance of LLaVA-series models across 10 diverse benchmarks, including visual question answering, OCR and mathematical reasoning tasks, revealing that LLVMs process images globally despite using localized visual tokens. • The experiments show that LLVMs can solve math problems even with missing numerical details from the image, and the lower layers of the model are crucial for visual understanding while higher layers focus on text interpretation. • The research highlights LLVMs' struggle to preserve initial visual understanding capabilities after alignment and visual instruction tuning.  • It suggests that future work should focus on developing interactive evaluation benchmarks and new model architectures to improve cross-modal alignment and visual perception. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/passing2961/IP-LLVM) | N/A |
| [Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning](https://arxiv.org/abs/2410.06508) | Ye Tian, haitaominlp, Pluie1503, freesunshine0316, russwang | - ALPHALLM-CPL, a novel pairwise training framework, enhances the reasoning capabilities of Large Language Models (LLMs) through Monte Carlo Tree Search (MCTS) behavior distillation. - It leverages stepwise trajectory pairs from child nodes in the search tree, providing step-level information for effective distillation. - Curriculum preference learning dynamically adjusts the training sequence, prioritizing critical learning steps and mitigating overfitting. - Experiments on mathematical reasoning tasks (GSM8K and MATH) show substantial improvements over existing MCTS distillation methods. - ALPHALLM-CPL boosts LLaMA2-7B's accuracy on GSM8K by 150%, Mistral-7B by 48.8%, and LLaMA3-8B by 17.4% on MATH, demonstrating its effectiveness in LLM self-improvement. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality](https://arxiv.org/abs/2410.05210) | Junmo Kim, In So Kweon, Dong-Jin Kim, Jae Won Cho, ytaek-oh | - FSC-CLIP, a novel fine-tuning framework for Vision-Language Models (VLMs), enhances compositional reasoning without sacrificing performance in zero-shot multi-modal tasks. - It integrates Local Hard Negative (LHN) Loss, which uses dense alignments between image patches and text tokens to compute loss, and Selective Calibrated Regularization (SCR) to regulate hard negative supervision. - Extensive evaluations on 11 compositionality benchmarks and 21 zero-shot classification tasks show that FSC-CLIP achieves comparable compositionality to state-of-the-art methods while better preserving multi-modal capabilities and exceeding pre-trained CLIP's zero-shot classification score by +0.5 points when fine-tuned on 100k LAION-COCO samples, a substantial improvement compared to a drop of -4.9 observed in existing methods. - Additionally, FSC-CLIP demonstrates superior retrieval capabilities, particularly in counterfactual scenarios, showcasing a more nuanced understanding of compositional concepts, as evidenced by qualitative examples on COCO-Counterfactuals. - FSC-CLIP addresses the trade-off between compositionality and multi-modal task performance, common in existing fine-tuning approaches that use global hard negative losses and often lead to degraded performance in tasks like zero-shot classification and retrieval. | ['Multimodal', 'Computer Vision', 'Image-to-Text', 'Zero-Shot Classification'] | [Link](https://github.com/ytaek-oh/fsc-clip) | N/A |
| [SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe](https://arxiv.org/abs/2410.05248) | Sanqiang Zhao, Marzyeh Ghassemi, wzhouad, szhang42, YuxinXiao | - SFTMix is a novel Mixup-based recipe for Large Language Model (LLM) instruction tuning that aims to improve performance without relying on curated datasets.  - SFTMix leverages training dynamics to identify and split the training dataset into confident and unconfident subsets based on the model's perplexity. - A Mixup-based regularization is then applied, interpolating examples between these subsets to mitigate overfitting on confident examples and propagate supervision to unconfident ones. - SFTMix significantly outperforms next-token prediction (NTP) across various instruction-following tasks and healthcare-related benchmarks using different LLMs and dataset sizes.  - Ablation studies confirm the method's robustness and design choices, demonstrating its potential across NLP applications. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models](https://arxiv.org/abs/2410.06154) | aquila147, mdorkenw, paulgavrikov, sivand, kevinmzy | • GLOV is a novel method that utilizes LLMs as implicit optimizers for Vision-Language Models (VLMs), enhancing performance on downstream tasks by optimizing natural language prompts.  • It uses a meta-prompt to guide iterative prompt generation, incorporating ranked in-context examples based on a few-shot training set and explicit guidance in the embedding space using offset vectors.  • This guidance steers the LLM towards positive solutions, improving recognition performance by up to 15% and 57.5% (3.8% and 21.6% average) on dual-encoder and encoder-decoder VLMs.  •  Comprehensive evaluation on 16 diverse datasets using CLIP and LLaVa demonstrates GLOV's ability to consistently improve performance. • The method was shown to be effective even for challenging fine-grained recognition tasks using encoder-decoder models without requiring gradient-based learning. | ['Zero-Shot Image Classification', 'Image Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/jmiemirza/GLOV) | N/A |
| [Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System](https://arxiv.org/abs/2410.08115) | Cheng Yang, Chen Qian, Jiarui Yuan, zibuyu9, weizechen |  - OPTIMA, a novel framework designed to optimize Large Language Model (LLM)-based Multi-Agent Systems (MAS) by enhancing both communication efficiency and task effectiveness through LLM training.  - Employs an iterative "generate, rank, select, and train" paradigm and utilizes a reward function that balances task performance, token efficiency, and communication interpretability.  - Integrates Monte Carlo Tree Search (MCTS)-inspired techniques for DPO data generation, to explore diverse interaction paths during conversations.  - Evaluated on various multi-agent tasks, including information-asymmetric question answering and complex reasoning, OPTIMA consistently outperforms single-agent and vanilla LLM-based MAS baselines, showing significant improvements in token usage and task performance (up to 2.8x performance gain with <10% tokens).  - The efficiency gains also contribute to improved inference-time scaling laws, enhancing the overall capabilities of LLM systems. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Emergent properties with repeated examples](https://arxiv.org/abs/2410.07041) | François Charton, Knykny | This study explores the impact of training example repetition on transformer performance using generated datasets for three mathematical tasks: greatest common divisor (GCD), modular multiplication, and matrix eigenvalues. - For a fixed number of training steps, models trained on smaller datasets with repeated examples outperform models trained on larger datasets with single-use examples. - This "repetition helps" phenomenon sometimes leads to the emergence of properties learned only by models trained on smaller, repeated datasets.  - A "two-set training" approach, where a small random subset of examples is repeated more often alongside normal sampling on the rest of the training set, further improves learning speed and performance.   - The findings suggest that repetition's benefits can outweigh those of data diversity, challenging the common practice of minimizing example reuse. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations](https://arxiv.org/abs/2410.08049) | xyyue, DingXiaoH, Yiyuan |   - UniRepLKNet, a large-kernel Convolutional Neural Network (ConvNet) architecture, is proposed, challenging the dominance of Vision Transformers (ViTs) in multimodal tasks by demonstrating comparable performance with faster inference and reduced complexity. - The architecture employs a few strategically placed large kernels to efficiently capture global context, supplemented by small kernels for detailed spatial feature extraction, achieving a balance between receptive field coverage and computational efficiency. - Design principles for large-kernel ConvNets are introduced, including guidelines for kernel size selection based on task and layer depth, efficient implementation of large kernels using depth-wise convolutions, the vital role of identity shortcuts, and the use of dilated small kernels for re-parameterizing large kernels. - Experiments across diverse modalities like images, audio, video, point clouds, and time series demonstrate UniRepLKNet's superior performance. It achieves state-of-the-art results on ImageNet classification, ADE20K semantic segmentation, and a global weather forecasting task, surpassing both existing large-kernel ConvNets and recent transformer-based models. - When scaled to 1.4B parameters and pretrained on a massive dataset of 10B image-text pairs, UniRepLKNet exhibits exceptional zero-shot image recognition capabilities and competitive performance on large vision-language model benchmarks, showcasing its scalability and potential for broader applications in multimodal learning. | ['Computer Vision', 'Image Classification', 'Object Detection', 'Image Segmentation', 'Zero-Shot Image Classification', 'Audio Classification', 'Time Series Forecasting', 'Multimodal'] | [Link](https://github.com/AILab-CVC/UniRepLKNet) | N/A |


## Papers for 2024-10-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/abs/2410.05254) | Roi Reichart, Samuel Joseph Amouyal, Omer Madmon, ireinman, EilamSha |   - This paper introduces GLEE, a unified framework and benchmark for evaluating Large Language Models (LLMs) in language-based economic games like bargaining, negotiation, and persuasion.   - It parameterizes the space of these games, defines consistent evaluation metrics (self-gain, efficiency, and fairness), and provides an open-source framework for interaction simulation.   - A dataset of 7.15M LLM decisions across various game configurations and an additional human vs. LLM dataset are collected using four different LLMs.   - The framework facilitates controlled experiments across numerous game configurations and LLMs, enabling robust evaluation.   - Demonstrates the framework's utility in evaluating and comparing LLMs to human players and in quantifying the impact of economic environment parameters. | ['Natural Language Processing'] | [Link](https://github.com/eilamshapira/GLEE) | N/A |
| [Personalized Visual Instruction Tuning](https://arxiv.org/abs/2410.07113) | Jipeng Zhang, Tianyang Han, research4pan, Sterzhang, renjiepi |  PVIT (Personalized Visual Instruction Tuning) is a new training paradigm designed to enable Multimodal Large Language Models (MLLMs) to engage in personalized conversations by identifying target individuals within an image. - The framework leverages in-context learning, utilizing a multimodal prefix of <personal image, personal introduction> and personalized wrapper tokens to eliminate ambiguity. - PVIT involves an automatic framework to create training data in three stages: visual concept curation, dual-level textual information extraction and fusion, and dataset generation using LLM reasoning. - A benchmark named P-Bench, with various question types, is introduced to evaluate the personalized capabilities of MLLMs.  - Experimental results on P-Bench demonstrate that current MLLMs have limited ability for personalized conversations. P-LLaVA trained with PVIT significantly improves performance on both answerable and unanswerable question types across all input complexities, achieving an average accuracy of 96.69% for answerable questions and 99.72% for unanswerable questions on the multiple choice questions in P-Bench. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/sterzhang/PVIT) | [Link](https://huggingface.co/datasets/Sterzhang/PVIT-3M) |
| [Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate](https://arxiv.org/abs/2410.07167) | Pan Zhang, Xiaoyi Dong, lindahua, yuhangzang, shikiw | - This paper introduces the Modality Integration Rate (MIR), a new metric for evaluating the cross-modal alignment quality during the pre-training phase of Large Vision-Language Models (LVLMs). - MIR quantifies the domain divergence between vision and language features across all layers of the LLM, thus, correlates strongly with the model's post-SFT multi-modal performance and exhibits convergence behavior during pre-training, offering insights for training optimization. - Furthermore, it is robust to variations in input type and training/evaluation datasets, and generalizes across different pre-training recipes, strategies, and module designs. - A lightweight and learnable calibration module called MoCa is proposed, improving alignment between visual and textual tokens and leading to performance gains when integrated into both pre-training and SFT stages. - Experiments show that MoCa yields a 1.5% average performance increase for LLaVA-1.5 and a 0.9% increase for Mini-Gemini. | ['Multimodal'] | [Link](https://github.com/shikiw/Modality-Integration-Rate) | N/A |
| [Pixtral 12B](https://arxiv.org/abs/2410.07073) | saurabhgarg, devendrachaplot, EmmaBH, Simontwice, pragra |  - Pixtral 12B is a 12-billion parameter multimodal language model trained to understand both images and text. - It utilizes a novel vision encoder trained from scratch, allowing it to process images at native resolution, and a multimodal decoder based on Mistral Nemo 12B. - Pixtral 12B outperforms open models of similar size on multimodal benchmarks, such as Llama 3.2 11B and Qwen-2-VL 7B and even surpasses larger models like Llama 3.2 90B on certain tasks.  - It also achieves strong performance on text-only tasks, demonstrating its capability as a general purpose language model.  - The authors introduce MM-MT-Bench, an open-source benchmark to evaluate vision-language models in practical multi-turn scenarios. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/mistralai/mistral-inference), [Link](https://github.com/mistralai/mistral-evals) | [Link](https://huggingface.co/datasets/mistralai/MM-MT-Bench) |
| [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/abs/2410.05993) | JunnanLi, guoyinwang, sirius-ctrl, teowu, dxli1 | **Summary of Aria: An Open Multimodal Native Mixture-of-Experts Model:** - ARIA is an open-source, multimodal native, mixture-of-experts (MoE) model with 24.9B parameters, trained from scratch and designed for comprehensive understanding across diverse modalities. - With a visual encoder lightweight of only 438M parameters, ARIA's MoE decoder has 3.9B and 3.5B activated parameters per visual and text token, respectively, enabling efficient parameter utilization and leading to faster training and inference. It outperforms Pixtral-12B and Llama3.2-11B and is competitive with top proprietary models on various multimodal tasks. - Trained in a 4-stage pipeline, the model progressively develops capabilities in language understanding, multimodal understanding, long context (64k tokens), and instruction following. This pipeline design ensures that each stage enhances the model's capabilities while preserving the already acquired skills from the previous stages. - ARIA's training data includes 6.4T language tokens and 400B multimodal tokens, with a rigorous curation process employing a combination of rule-based and model-based filtering to maintain data quality. - Qualitative results showcases ARIA is able to integrate information across multiple modalities in complex reasoning tasks involving chart, table, text, and images understanding and show advanced coding, debugging, math, paper reading, video understanding abilities. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Text2Text Generation', 'Video-Text-to-Text', 'Question Answering'] | N/A | N/A |
| [MM-Ego: Towards Building Egocentric Multimodal LLMs](https://arxiv.org/abs/2410.07177) | HaoxuanYou, FrozzZen, edaxberger, haotiz, leoye | **Key Points:** - Introduces MM-Ego, a multimodal large language model (MLLM) designed for egocentric video understanding, featuring a novel "Memory Pointer Prompting" mechanism. This mechanism incorporates a global glimpse step, which extracts compressed visual embeddings from the entire video to gain an overarching understanding, and a fallback step, which uses higher-resolution key visual embeddings identified in the global glimpse stage to respond to questions. - Creates a 7M egocentric QA dataset, generated automatically from human-annotated video narrations from the Ego4D dataset, that ranges from 30 seconds to one hour, representing the largest egocentric QA dataset currently available. - Introduces EgoMemoria, a benchmark to evaluate egocentric video understanding capabilities with 7,026 multiple-choice questions across 629 videos ranging from 30 seconds to one hour in length, alongside a debiased metric to mitigate language bias. - In experiments, MM-Ego outperforms prior state-of-the-art models on the EgoMemoria benchmark and demonstrates competitive results on general video benchmarks like EgoSchema and Video-MME. - The Memory Pointer Prompting and data augmentation strategies show improvements even after the removal of language-biased questions, demonstrating their efficacy for the targeted task. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation](https://arxiv.org/abs/2410.07170) | Marc Peter Deisenroth, Benedikt Alkin, thomasschmied, sirluk, paischer101 |  - This paper introduces Explained Variance Adaptation (EVA), a novel data-driven initialization method for Low-Rank Adaptation (LoRA) used in fine-tuning large foundation models. - EVA computes the Singular Value Decomposition (SVD) on mini-batches of activation vectors derived from downstream data to initialize LoRA weights, maximizing explained variance and enabling adaptive rank allocation across model layers. - Experiments conducted on diverse tasks, including language generation, understanding, image classification, and reinforcement learning, demonstrate EVA's superior performance to existing initialization and rank adaptation techniques. - EVA achieves faster convergence than competitor models across multiple tasks, such as achieving higher average scores on commonsense reasoning with LLMs and even exceeding full fine-tuning performance when combined with DORA on reinforcement learning tasks. - Ablation studies confirm that both the directional components and scale obtained from SVD contribute to EVA's enhanced performance. | ['Natural Language Processing', 'Text Generation', 'Image Classification', 'Reinforcement Learning', 'Robotics'] | [Link](https://github.com/ml-jku/EVA), [Link](https://github.com/BenediktAlkin/vtab1k-pytorch), [Link](https://github.com/sirluk/peft/blob/main/examples/eva_finetuning/eva_finetuning.py) | N/A |
| [Self-Boosting Large Language Models with Synthetic Preference Data](https://arxiv.org/abs/2410.06961) | Zhifang Sui, Li Dong, thegenerality, THU-CHUNXIA, Rsy24 | SynPO, a novel self-boosting paradigm, leverages synthetic preference data for Large Language Model (LLM) alignment, eliminating the need for extensive human preference data. It employs an iterative mechanism where a self-prompt generator creates diverse prompts, and a response improver refines model responses.  After four SynPO iterations, LLMs like Llama2-8B and Mistral-7B demonstrated significant improvements, achieving over 22.1% win rate improvements on benchmarks like AlpacaEval 2.0 and ArenaHard. Moreover, SynPO boosts the general LLM performance, as evidenced by a 3.2 to 5.0 average score increase on the Open LLM leaderboard.  SynPO's self-boosting mechanism dynamically guides LLMs to refine their own outputs, effectively integrating generative rewards for preference learning. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Falcon Mamba: The First Competitive Attention-free 7B Language Model](https://arxiv.org/abs/2410.05355) | Ilyas Chahed, Dhia Eddine Rhaiem, ybelkada, yellowvm, JingweiZuo | - Falcon Mamba 7B is a new large language model based on the Mamba architecture, making it attention-free, trained on 5.8 trillion tokens. - It outperforms other open-source 7B models like Mistral 7B and Llama 3.1 8B, as well as larger models such as Falcon2 11B in benchmarks like the Open LLM Leaderboard. - Falcon Mamba 7B has faster inference speeds and lower memory usage, especially beneficial for long sequence generation due to the Mamba architecture's linear memory scaling. - The model uses an AdamW optimizer with a warmup-stable-decay learning rate schedule and is trained on a dataset mixture of web data, curated content, code, and math data. - Falcon Mamba 7B is available with a permissive license on Hugging Face, supporting functionalities such as inference, quantization, and fine-tuning. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/tiiuae/falcon-mamba-7b), [Link](https://huggingface.co/tiiuae/falcon-mamba-7b-pre-decay) |
| [Temporal Reasoning Transfer from Text to Video](https://arxiv.org/abs/2410.06166) | Chancy, PY007, yaolily, lyx97, tobiaslee | - T3 (Textual Temporal reasoning Transfer) is introduced, a method that enhances Video Large Language Models' (Video LLMs) temporal reasoning by transferring knowledge from the text domain.  - T3 creates diverse temporal reasoning tasks in text format from existing image-text datasets, addressing the lack of video samples with complex temporal scenarios.  - Without using any video data, T3 improves LongVA-7B's performance significantly, achieving a 5.3 absolute accuracy gain on TempCompass, exceeding ShareGPT4Video-8B (trained on 28,000 video samples). - The enhanced LongVA-7B achieves competitive performance on video benchmarks, e.g. 49.7 accuracy on Video-MME's Temporal Reasoning task, outperforming InternVL-Chat-V1.5-20B and VILA1.5-40B.  -  Analysis reveals a strong correlation between textual and video temporal task performance (e.g., Pearson r=0.89 on TempCompass), validating the efficacy of T3. | ['Multimodal', 'Video-Text-to-Text', 'Video Classification'] | N/A | N/A |
| [TRACE: Temporal Grounding Video LLM via Causal Event Modeling](https://arxiv.org/abs/2410.05643) | Xiaoying Tang, Mingda Li, Jingyu Liu, qingbinliu, Yongxin-Guo |  - TRACE, a novel task-interleaved video Large Language Model (LLM), is introduced for Video Temporal Grounding (VTG). It addresses the limitations of current video LLMs that rely solely on natural language generation, which lack the clear structure and information presented in videos.  - TRACE models videos as sequences of events, each with timestamps, salient scores, and captions, and leverages causal event modeling framework to represent the inherent structure of videos. - The TRACE architecture uses an interleaved sequence of task tokens for visual frames, timestamps, salient scores, and text, and employs separate encoders and decoding heads for each task. - The model also incorporates an adaptive head-switching mechanism for improved generation and achieves superior performance on various VTG tasks and datasets, outperforming current video LLMs. - TRACE improves zero-shot performance by 3.1% and 4.9% on Youcook2 (CIDEr and F1 Score), by 6.5% and 3.7% on Charades-STA (Recall with IOU=0.5 and IOU=0.7 respectively), and by 10.3% and 9.2% on QVHighlights (mAP and HIT@1). | ['Video-Text-to-Text', 'Multimodal', 'Question Answering'] | [Link](https://github.com/gyxxyg/TRACE) | N/A |
| [Data Selection via Optimal Control for Language Models](https://arxiv.org/abs/2410.07064) | Li Dong, thegenerality, Rsy24, howang, t1101675 | This paper introduces PMP-based Data Selection (PDS), a framework for selecting high-quality pre-training data for language models (LMs). PDS formulates data selection as an Optimal Control problem and leverages Pontryagin's Maximum Principle (PMP) to derive necessary conditions for optimal data selection. Experiments show that PDS accelerates LM pre-training by 2x and improves performance across various model sizes and downstream tasks, even extrapolating to 400B models trained on 15T tokens. PDS also enhances data utilization in data-constrained settings, reducing pre-training data demand by 1.8 times. This method offers a principled, theory-driven approach to data selection compared to existing heuristics, leading to more efficient and effective LM training. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/microsoft/LMOps/tree/main/data_selection) | N/A |
| [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/abs/2410.07002) | Shijin Wang, Rui Li, Qi Liu, Eviloder, TechxGenus |  - This paper introduces CursorCore, a new framework for AI-assisted programming that integrates various information sources such as coding history, current code, and user instructions for enhanced automation.  - It also presents a new benchmark called APEval (Assist Programming Eval) to evaluate models on this task and a data generation pipeline, Programming-Instruct, to create synthetic training data from diverse sources.  - This pipeline generated 219K samples to fine-tune the CursorCore models.  - The CursorCore models reportedly outperforms other models of comparable size on the APEval benchmark.  - This framework unifies applications like inline chat and automated editing, contributing to the advancement of coding assistants. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/TechxGenus/CursorCore) | N/A |
| [Response Tuning: Aligning Large Language Models without Instruction](https://arxiv.org/abs/2410.02465) | Hyounghun Kim, seokhyun |  - Response Tuning (RT) is proposed, a novel fine-tuning method that omits the instruction-conditioning step of instruction tuning, instead focusing exclusively on the supervision of response space. - RT models, trained solely on responses, exhibit helpfulness and open-ended instruction following capabilities comparable to instruction-tuned models, demonstrating the potential of response space supervision in alignment. - Refining the structural attributes of training responses leads to significant improvements in user preference for RT models, while incorporating contextual refusals into the training data allows RT models to implicitly evaluate and reject unsafe queries.  - These findings emphasize the importance of controlling response distribution in safety alignment and suggest that large language models inherently acquire many capabilities during pre-training. - In-context learning with response demonstrations only yields effective instruction-following and refusal behaviors, further strengthening the argument for the power of response supervision and highlighting the inherent potential of pretrained large language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/seokhyunan/response-tuning) | N/A |
| [ING-VP: MLLMs cannot Play Easy Vision-based Games Yet](https://arxiv.org/abs/2410.06555) | Haoran Zhang, zhangysk, CheeryLJH, EZ-hwh, Rosiness |  - This research introduces ING-VP, a novel interactive game-based vision planning benchmark designed to evaluate the spatial imagination and multi-step reasoning capabilities of Multimodal Large Language Models (MLLMs). - ING-VP comprises six distinct games with varying complexity, offering 300 levels and six unique configurations per level, leading to over 60,000 interaction rounds for a single model. - The benchmark incorporates image-text and text-only input modalities, single and multi-step reasoning settings, and conditions with and without interaction history, facilitating a comprehensive evaluation of MLLM performance. - Initial evaluations using ING-VP demonstrate that current state-of-the-art MLLMs struggle with these seemingly simple game tasks. The highest performing model, Claude-3.5 Sonnet, only achieves an average accuracy of 3.37%, significantly below human performance. - This underscores the need for further research and development to enhance MLLMs' capacity for complex spatial reasoning and planning, a crucial aspect of achieving robust artificial general intelligence. | ['Multimodal'] | [Link](https://github.com/Thisisus7/ING-VP.git) | N/A |
| [Mixed-Session Conversation with Egocentric Memory](https://arxiv.org/abs/2410.02503) | Taeyoung Kim, khh3323, jihyoung | • The paper introduces Mixed-Session Conversation, a new dialogue paradigm where a main speaker interacts with different partners across multiple sessions, promoting deeper layered interactions and complex dynamics.  • MISC, a new dataset comprising 8.5K episodes with 6 sessions and 4 speakers per episode is presented, implementing Mixed-Session Conversation and managing memories across sessions and partners from the main speaker's perspective.   • EMMA (Egocentric Memory Enhanced Mixed-session Conversation Agent), a novel dialogue model trained on MISC, facilitates seamless conversation continuity using Egocentric Memory, and allows retention of all conversational contexts across sessions and partners.   • Human evaluations validate that dialogues in MISC demonstrate seamless conversational flow even with changing partners, with EMMA exhibiting high humanness, engagingness, and memorability.  • EMMA's use of Egocentric memory retains high memorability without contradiction by connecting instances within and across sessions and tagging memory to each utterance. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://mixed-session.github.io/) |
| [FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance](https://arxiv.org/abs/2410.05791) | C. Karen Liu, Elizabeth Schumann, Haochen Shi, Pei Xu, rcwang | -   This paper introduces FürElise, a large-scale dataset of 3D hand motions and audio from 15 pianists playing 153 classical music pieces, captured using a markerless multi-view video setup and refined with MIDI data from a Disklavier piano.   - A new model is proposed to synthesize physically plausible piano playing motions from sheet music, combining a diffusion model for initial motion generation, a music-based motion retrieval method for enhancing accuracy, and reinforcement learning for physics-based bimanual control.  - The diffusion model, trained on FürElise, generates kinematic hand trajectories conditioned on sheet music, providing high-level guidance and fingering information.  - Motion retrieval augments the diffusion model's output by retrieving similar motions from FürElise based on musical similarity, improving the precision of key presses.  - The reinforcement learning policy learns to control simulated hands interacting with a piano keyboard, optimizing a combination of imitation and goal-based rewards to achieve realistic and musically accurate performance. | ['Computer Vision', 'Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs](https://arxiv.org/abs/2410.05295) | Edward Suh, huansun, someshjha, peiranli0930, ShletonLiu-N | - AutoDAN-Turbo, a novel black-box jailbreak method for Large Language Models (LLMs), automatically discovers and combines diverse jailbreak strategies using a lifelong learning approach. - This method leverages three core modules: an Attack Generation and Exploration Module, a Strategy Library Construction Module, and a Jailbreak Strategy Retrieval Module, allowing for continuous strategy discovery, evolution, and integration of human-designed strategies. - Evaluation on Harmbench and StrongREJECT benchmarks shows that AutoDAN-Turbo significantly outperforms existing methods, achieving a 74.3% higher average attack success rate and a 92.3% higher StrongREJECT score than the runner-up. - Notably, it demonstrates exceptional effectiveness on GPT-4-1106-turbo, reaching an 88.5% attack success rate, which further increases to 93.4% with the integration of human-designed strategies. - The learned strategy library exhibits strong transferability across different target models and datasets, demonstrating its robustness and adaptability in various attack scenarios. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SaFoLab-WISC/AutoDAN-Turbo) | N/A |
| [Multimodal Situational Safety](https://arxiv.org/abs/2410.06172) | xw-eric, dawnsong, acompalas, Xuandong, LCZZZZ |  - This paper introduces the novel problem of Multimodal Situational Safety, which focuses on evaluating a multimodal model's ability to consider safety aspects based on visual context. - A new benchmark called MSSBench is created to evaluate the situational safety performance of current Multimodal Large Language Models (MLLMs). - The benchmark comprises 1820 language query-image pairs across two scenarios: chat and embodied assistants, where half the images depict safe situations and the other half unsafe. - An evaluation framework analyzes key safety aspects, including explicit safety reasoning, visual understanding, and situational safety reasoning. - Results show current MLLMs struggle with recognizing unsafe situations, especially open-source models which frequently ignore safety clues.  | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](mssbench.github.io) | N/A |
| [T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design](https://arxiv.org/abs/2410.05677) | wangwilliamyang, wenhu, rpiramuthu, xfgao, jiachenli-ucsb | • T2V-Turbo-v2, a novel text-to-video (T2V) generation model, enhances post-training through incorporating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance into the consistency distillation process.  • It eliminates the target network from T2V-Turbo for improved memory efficiency and enables full model training, rather than just LORA. • It leverages motion guidance from training videos to formulate an energy function that augments the ODE solver, improving motion quality.  • Evaluated on VBench, T2V-Turbo-v2 achieves state-of-the-art performance with a Total Score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.  • Ablation studies confirm the benefits of curating specialized datasets, utilizing diverse reward models and employing motion guidance. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | [Link](https://huggingface.co/spaces/Vchitect/VBench_Leaderboard) |
| [Collective Critics for Creative Story Generation](https://arxiv.org/abs/2410.02428) | Hyounghun Kim, minwook | CRITICS is a novel framework for long-form story generation that integrates a collaborative critique mechanism to enhance story creativity and expressiveness. - It consists of two stages: CRPLAN for refining story plans and CRTEXT for enhancing story expressiveness. - Multiple LLM critics and a leader collaborate to refine story plans and enhance story texts based on criteria for creativity. - Human evaluation shows that CRITICS significantly improves story creativity and reader engagement while maintaining coherence. - It supports interactive writing, where humans can participate as any player within the framework. | ['Text Generation'] | [Link](https://github.com/EMNLP-2024-CritiCS/Collective-Critics-for-Creative-Story-Generation) | N/A |
| [Diversity-Rewarded CFG Distillation](https://arxiv.org/abs/2410.06084) | alexrame, Sper42, bachem, ferretj, aagostinelli86 |  - This paper introduces diversity-rewarded CFG distillation, a novel finetuning strategy to enhance the quality-diversity trade-off in generative models, specifically for text-to-music generation. - It combines distillation and reinforcement learning (RL) to optimize two complementary objectives; a novel CFG distillation objective and an RL with diversity reward objective. - By interpolating between the weights of two models(quality-focused and diversity-focused model), the strategy controls the quality-diversity trade-off at deployment time, further boosting performance. - Experiments on MusicLM using human evaluation validate that the model generates more diverse music samples while maintaining high quality. - The finetuned-then-merged model outperforms CFG augmentation in terms of Pareto-optimal quality and diversity, generating high-quality samples with improved diversity. | ['Text-to-Audio', 'Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [TinyEmo: Scaling down Emotional Reasoning via Metric Projection](https://arxiv.org/abs/2410.07062) | ggcristian |  - TinyEmo, a family of small Multimodal Large Language Models (MM-LLMs), is introduced for enhanced emotional reasoning and classification, integrating a synthetic emotional instruction dataset, a Metric Projector for classification, and a conditional reasoning approach. - The architecture includes a vision encoder (CLIP ViT-L/14), two projectors for classification and reasoning respectively and different LLM backbones (OpenELM, TinyLlama, Phi-2) ranging from 0.7B to 3.21B parameters. The Metric Projector is trained separately with metric learning, detaching classification from the LLM to improve efficiency and performance. - TinyEmo-700M outperforms larger state-of-the-art models like EmoVIT (7.91B parameters) with only 700M parameters on emotion classification and achieves a Zero-Shot accuracy of 57.62% when trained with data augmentation, outperforming EmoVIT's 55.57%. - A Conditional Reasoning approach, where the predicted emotion label from the Metric Projector is inserted into the prompt, leads to more accurate reasoning compared to the standard approach. - A semi-automated framework is proposed which uses the Metric Projector for interpretability and bias detection by analyzing neuron activations and embedding space robustness, showing the potential for mitigating bias and improving understanding of model behavior. | ['Multimodal', 'Image Classification', 'Visual Question Answering', 'Text Generation', 'Zero-Shot Classification', 'Zero-Shot Image Classification'] | [Link](https://github.com/ggcr/TinyEmo) | N/A |
| [F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](https://arxiv.org/abs/2410.06885) | Zhikang Niu, kaiyu-hf, ChunHuiWangFN, D-Keqi, SWivid | • F5-TTS is a fully non-autoregressive text-to-speech model based on flow matching with Diffusion Transformer (DiT) and ConvNeXt V2. • It simplifies the pipeline by removing the need for a duration model, text encoder, phoneme alignment, and semantically infused codec, using padded character sequences as input. • The model employs a novel Sway Sampling strategy during inference, improving performance and allowing for faster inference with fewer function evaluations. • Evaluation on LibriSpeech-PC, Seed-TTS test-en, and test-zh demonstrates that F5-TTS achieves state-of-the-art zero-shot performance with a real-time factor (RTF) of 0.15, outperforming existing methods in terms of both speed and quality. • Ablation studies highlight the robustness of F5-TTS, especially in handling challenging scenarios where the alignment between text and speech is crucial. | ['Text-to-Speech', 'Audio'] | [Link](https://github.com/SWivid/F5-TTS) | N/A |
| [MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders](https://arxiv.org/abs/2410.06845) | Chi Han, Qingyun Wang, May Fung, jindongwang, Cheng228 | - MentalArena is a novel self-play training framework for LLMs to improve their ability to diagnose and treat mental health disorders by generating personalized training data. - It consists of three modules: Symptom Encoder simulates realistic mental health patients, Symptom Decoder mitigates intent bias in patient-therapist dialogues, and Model Optimizer fine-tunes the LLM on the generated data. - The Symptom Encoder uses cognitive models and behavior principles of patients to produce realistic symptom descriptions. - The framework significantly outperformed several state-of-the-art and mental-health-specific LLMs, including GPT-4, on six benchmark datasets, demonstrating improvement over base models by 20.7% for GPT-3.5-turbo and 6.6% for Llama-3-8b. - Further analysis revealed a strong correlation between model performance and perplexity of the training data, and that maintaining data diversity above a certain threshold during training contributes to improved model performance. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/Scarelette/MentalArena/tree/main) | N/A |
| [Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning](https://arxiv.org/abs/2410.04223) | Jie Chen, Wojciech Matusik, Michael Sun, Gang Liu, mjiang89 | This research paper presents Llamole, a multimodal large language model (MLLM) for controllable and synthesizable molecular generation and retrosynthetic planning. Llamole integrates a base LLM with a graph diffusion transformer, graph neural networks, and A* search, allowing for the seamless generation of text, molecules, and reactions. Benchmarks on 14 LLMs of various sizes reveal the limitations of existing models in controllable molecular design and synthetic planning. Llamole shows significant improvement, increasing success rates from 5.5% to 35% and enhancing controllability by up to 80.9% across various metrics. | ['Multimodal', 'Graph Machine Learning', 'Text Generation'] | N/A | N/A |
| [Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach](https://arxiv.org/abs/2410.06949) | Minlie Huang, Yuan Yuan, Yuxuan Chen, XUANMINGZHANG |  - Seeker, a multi-agent framework leverages LLMs to enhance exception handling in code by addressing three key issues: insensitive detection of fragile code, inaccurate capture of exception types, and distorted handling solutions. - Seeker employs five agents—Scanner, Detector, Predator, Ranker, and Handler—inspired by expert developer strategies. -  A Common Exception Enumeration (CEE) document, built from trusted external experience and exception practices, is used to improve retrieval and handling. - A deep retrieval-augmented generation (Deep-RAG) algorithm is proposed to handle complex inheritance relationships between exception types. - Experimental results show that Seeker outperforms baselines on various metrics including code quality, coverage, accuracy, and edit similarity. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/XMZhangAI/Seeker) | N/A |
| [Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA](https://arxiv.org/abs/2410.06524) | Jordan Boyd-Graber, Hal Daumé III, zhoutianyi, mgor |  - This paper introduces CAIMIRA, a novel framework based on Item Response Theory (IRT) for evaluating and comparing the question-answering abilities of humans and AI systems. - CAIMIRA uses question text to infer characteristics, enabling generalization to new questions without needing prior responses and allowing for analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions. - The study finds that humans outperform AI in knowledge-grounded abductive and conceptual reasoning, while LLMs like GPT-4-TURBO excel at targeted information retrieval and fact-based reasoning. - The authors suggest future QA tasks focus on challenging higher-order reasoning, scientific thinking, nuanced linguistic interpretation, and cross-contextual knowledge application. - The implementation can be found at https://github.com/maharshi95/neural-irt | ['Question Answering'] | [Link](https://github.com/maharshi95/neural-irt) | [Link](mgor/protobowl-11-13) |
| [Does Spatial Cognition Emerge in Frontier Models?](https://arxiv.org/abs/2410.06468) | vkoltun, philkra, erikwijmans, sramakrishnan |  - The paper introduces SPACE, a benchmark for evaluating spatial cognition in large language models (LLMs) and large multimodal models.   - SPACE evaluates large-scale mapping abilities and smaller-scale reasoning about object shapes and layouts.   - The benchmark includes tasks from cognitive science, instantiated in parallel via text and images.   - Results indicate that current frontier models fall short of animal spatial intelligence, performing near chance level on several classic tests.   - The authors suggest that spatial cognition is a crucial form of intelligence, and its emergence in models is worthy of further investigation. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Zero-Shot Image Classification', 'Zero-Shot Object Detection', 'Computer Vision', 'Image Classification', 'Image Segmentation', 'Video Classification', 'Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |


## Papers for 2024-10-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LongGenBench: Long-context Generation Benchmark](https://arxiv.org/abs/2410.04199) | Peijie Dong, wenxinsiju, xuminghui, Dominic789654 | - LongGenBench, a synthetic benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs), focusing on consistency and logical flow. - It redesigns question formats, requiring LLMs to provide single, cohesive long-context answers encompassing multiple questions within a single query. - Evaluation on LongGenBench reveals performance degradation across both API-accessed and open-source LLMs in long-context scenarios, ranging from 1.2% to 47.1%. - Different LLM series show varying degradation trends, with Gemini-1.5-FLASH exhibiting minimal degradation among API-accessed models, and QWEN2 series showing minimal degradation among open-source models. - Model size influences performance decline, with larger models within a series generally demonstrating less degradation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization](https://arxiv.org/abs/2410.04717) | Francois Charton, Justin Wang, shizhuo2 | - This paper investigates the impact of instruction diversity on the generalization ability of Large Language Models (LLMs), focusing solely on instruction-following capabilities and isolating them from reasoning and knowledge retrieval. - Through controlled string rewriting experiments inspired by the Turing-complete Markov algorithm and mathematical deduction tasks, the study demonstrates that generalization to unseen instructions emerges only when training data is sufficiently diverse across semantic domains. - Findings reveal that diversifying data within limited domains does not guarantee robust generalization, while cross-domain diversification significantly enhances adaptability to new instructions. - The research further shows that increasing the diversity of training data can lead to performance improvements in real-world scenarios, including code generation and reasoning tasks with both specialized and generalist models.  - The results underscore the importance of strategic data diversification over simply increasing data size, offering guidelines for improving instruction-tuning datasets and enhancing model performance across various domains. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://arxiv.org/abs/2410.05193) | lifengshang, YuxinJiang, Tiezheng, yufeiwang201217a, DonJoey | • REVISEVAL, a novel evaluation paradigm, leverages the revision capabilities of Large Language Models (LLMs) to generate response-adapted references for evaluating text generation quality.  • It revises the generated response based on the given instruction and evaluation rubric, then uses the revised text as a reference for subsequent evaluation by either LLM-as-a-Judge or classic text evaluation metrics. • REVISEVAL outperforms reference-free and reference-based evaluation methods across various NLG and instruction-following tasks using both open-source and proprietary LLMs.  • Response-adapted references enhance the performance of classic metrics, sometimes even rivaling LLM-as-a-Judge.  • REVISEVAL effectively reduces bias in evaluation, such as verbosity and positional biases, and its effectiveness is linked to the relevance of the generated references. | ['Natural Language Processing'] | N/A | N/A |
| [MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions](https://arxiv.org/abs/2410.02743) | Yu Sun, Shuohuan Wang, Huang Fang, Haoran Sun, Yekun Chai |  - MA-RLHF, a new Reinforcement Learning from Human Feedback (RLHF) framework, is introduced to improve large language model alignment with human preferences.  It leverages "macro actions" which are sequences of tokens or higher-level language constructs.  - This approach reduces the temporal distance between actions and rewards, addressing the credit assignment problem in token-level RLHF, and facilitates faster and more accurate credit assignment.  - The model achieves substantial performance improvements across various tasks, including up to a 30% gain in summarization, an 18% gain in dialogue, and an 8% gain in question answering, while demonstrating a 1.7x-2x faster convergence compared to standard RLHF.  - MA-RLHF's robustness is highlighted through experiments conducted with different model sizes (2B to 27B) on various tasks, such as text summarization with the TL;DR dataset and dialogue generation with the HH-RLHF dataset.  - Further analysis explores termination strategies for macro actions, demonstrating the effectiveness of n-gram and parsing-based approaches in improving model performance. | ['Reinforcement Learning', 'Natural Language Processing', 'Summarization', 'Text2Text Generation', 'Question Answering', 'Text Generation'] | [Link](https://github.com/ernie-research/MA-RLHF) | [Link](https://huggingface.co/datasets/Dahoas/full-hh-rlhf) |
| [Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models](https://arxiv.org/abs/2410.03290) | Yufan Zhou, Shizhe Diao, Yu Cheng, Zhiyang Xu, WHB139426 | **-** This paper introduces Grounded-VideoLLM, a novel Video Large Language Model (Video-LLM) designed for fine-grained temporal grounding in videos.  **-** Grounded-VideoLLM uses a two-stream architecture, encoding spatial information from keyframes and temporal dynamics from multiple frames using a video encoder, to create a temporally-aware video representation. **-**  It introduces discrete temporal tokens into the LLM's vocabulary for representing timestamps efficiently, avoiding tokenization of numerical text and integrating time representations directly into the LLM.   **-** A multi-stage training approach is employed, progressing from video-caption alignment to temporal token alignment and finally multi-task instruction tuning on datasets incorporating temporal grounding tasks. **-** Experimental results demonstrate that Grounded-VideoLLM achieves state-of-the-art performance on various fine-grained temporal grounding tasks including Temporal Sentence Grounding, Dense Video Captioning and Grounded VideoQA, as well as general video understanding benchmarks. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/WHB139426/Grounded-Video-LLM) | N/A |
| [Hyper-multi-step: The Truth Behind Difficult Long-context Tasks](https://arxiv.org/abs/2410.04422) | yuyijiong |  - This paper investigates the underlying reasons why Long Context Language Models (LCLMs) struggle with complex tasks, despite their ability to handle extensive text.  - Through experiments with synthetic datasets, the study identifies "multi-matching retrieval" (retrieving multiple items simultaneously) and "logic-based retrieval" (using logic within retrieval criteria) as the core challenges, and further defines them as "hyper-multi-step" problems. - "Hyper-multi-step" implies that these seemingly simple tasks actually comprise a large number of indivisible sub-steps, which increases with context length and exceeds the processing capacity of current LCLMs.  - The paper provides empirical evidence through linear probing of hidden states and analysis of attention weights, demonstrating that these problems are more akin to complex arithmetic tasks, rather than traditional retrieval, and are therefore not adequately addressed by existing techniques such as Retrieval-Augmented Generation (RAG) or Chain-of-Thought (CoT) prompting.  - The study concludes that simply increasing the context window size of LCLMs may not suffice; instead, future research should focus on addressing the numerous steps involved and explore alternative solutions, such as using external tools. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |


## Papers for 2024-10-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Differential Transformer](https://arxiv.org/abs/2410.05258) | Li Dong, thegenerality, sunyt32, yuqxia, ytz20 | • This paper introduces the Differential Transformer (DIFF Transformer), a novel architecture for large language models (LLMs) designed to improve attention to relevant context and mitigate noise. • The core innovation is the differential attention mechanism, which calculates attention scores as the difference between two separate softmax attention maps, thus canceling noise and promoting sparse attention patterns. • Experimental results on language modeling demonstrate that DIFF Transformer outperforms standard Transformer models in various scaling settings, requiring only about 65% of the model size or training tokens to achieve comparable performance. • The model also exhibits advantages in downstream tasks such as long-context modeling, key information retrieval, hallucination mitigation, and in-context learning. • Additionally, DIFF Transformer demonstrates increased robustness to order permutation in in-context learning and a reduction in activation outliers, which presents opportunities for model quantization. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | N/A | N/A |
| [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707) | Roi Reichart, Zorik Gekhman, belinkov, tokeron, hadasor |   - This paper investigates the internal representations of large language models (LLMs) and their connection to the phenomenon of hallucinations. - The research finds that truthfulness information is highly localized within exact answer tokens, leading to improved error detection when probing these specific tokens. - The study demonstrates that while error detection is enhanced by focusing on these tokens, probing classifiers trained on one dataset often fail to generalize effectively to others, indicating that truthfulness mechanisms are skill-specific. - The authors further categorize LLM errors based on repeated sampling, showing that error types are predictable from internal representations. - Finally, they highlight a discrepancy between LLM internal encoding and external behavior, revealing that models may internally identify the correct answer but consistently generate an incorrect one, suggesting the potential for harnessing this existing knowledge to reduce errors. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/technion-cs-nlp/LLMsKnow) | N/A |
| [FAN: Fourier Analysis Networks](https://arxiv.org/abs/2410.02675) | Yongding Tao, Ge Li, Jingjingxu, zkcpku, dongyh | - This paper introduces the Fourier Analysis Network (FAN), a novel neural network architecture designed to effectively model and reason about periodic phenomena by incorporating Fourier Series into its structure and computational process. - FAN aims to address the limitations of existing neural networks, such as MLPs and Transformers, which often struggle to generalize periodic functions beyond the training data domain. - The architecture consists of stacking FAN layers where each layer outputs a concatenation of cosine, sine transformations, and an activation function applied to a linear transformation of the input. - Experimental results demonstrate FAN's superior performance compared to MLP, KAN, and Transformer on various tasks, including symbolic formula representation, time series forecasting, and language modeling tasks. - By seamlessly replacing MLP layers with FAN layers, models achieve improved generalization while reducing parameters and FLOPs. | ['Time Series Forecasting', 'Natural Language Processing'] | [Link](https://github.com/YihongDong/FAN) | N/A |
| [Presto! Distilling Steps and Layers for Accelerating Music Generation](https://arxiv.org/abs/2410.05167) | Jonah Casebeer, Ge Zhu, Njb, tberg12, ZacharyNovack |  - Presto! is a new dual-faceted distillation approach for accelerating score-based diffusion transformers by reducing sampling steps and the cost per step. - Presto includes score-based distribution-matching distillation for continuous-time diffusion (EDM) using a GAN, improved conditional layer distillation with better-preserved hidden-state variance, and combined layer-step distillation. - For step distillation, Presto-S achieves best-in-class performance among step distillation techniques and matches the original model quality with 4-step inference. - When combined with the novel layer distillation Presto-L, which independently outperforms SOTA layer dropping and base diffusion sampling, the resulting Presto-LS approach accelerates the model by 10-18x, generating 32-second mono audio in 230ms and stereo audio in 435ms on an A100 40GB GPU, outperforming Stable Audio Open by 15x. | ['Audio', 'Text-to-Audio'] | [Link](https://presto-music.github.io/web/) | N/A |
| [Named Clinical Entity Recognition Benchmark](https://arxiv.org/abs/2410.05046) | Clément Christophe, Tathagata Raha, Muhammad Umar Salman, Marco AF Pimentel, Wadood M Abdul | - This paper introduces a Named Clinical Entity Recognition (NER) benchmark designed for evaluating language models in healthcare. - This benchmark encompasses a curated selection of publicly accessible medical datasets with standardized entities adhering to the Observational Medical Outcomes Partnership (OMOP) Common Data Model. - The leaderboard accommodates various language model architectures, including encoder, decoder, and GLiNER models, and employs standardized evaluation metrics, predominantly the F1-score, to ensure consistent performance comparisons. - Initial findings from the leaderboard indicate superior performance by GLiNER-based models over decoder-only architectures, commonly used in Large Language Models (LLMs). - The choice of evaluation strategy, token-based or span-based, has been found to influence model ranking. | ['Natural Language Processing', 'Token Classification'] | [Link](https://github.com/WadoodAbdul/clinical_ner_benchmark) | [Link](https://huggingface.co/m42-health/clinical_ner_leaderboard), [Link](https://huggingface.co/spaces/m42-health/clinical_ner_leaderboard) |
| [TLDR: Token-Level Detective Reward Model for Large Vision Language Models](https://arxiv.org/abs/2410.04734) | Rui Wang, Tong Xiao, tbpangolin, pzzhang, deqing |  - This paper introduces TLDR, a novel token-level reward model designed to improve the performance and interpretability of large vision-language models (VLMs). - The TLDR model assigns rewards to individual tokens rather than entire sequences, enabling finer-grained feedback and more precise identification of errors, like hallucinations. - A perturbation-based method is used to generate synthetic hard negatives for training TLDR, enhancing its robustness. - Experiments demonstrate that TLDR significantly improves VLM performance in various tasks and reduces human annotation time by approximately threefold. - The study shows that the proposed model speeds up human annotation by 3 times in acquiring high-quality vision-language data. | ['Multimodal', 'Image-Text-to-Text', 'Reinforcement Learning'] | N/A | N/A |
| [UniMuMo: Unified Text, Music and Motion Generation](https://arxiv.org/abs/2410.04534) | Yutong Zhang, Kun Su, Han Yang, auspicious3000, Jiaben |   - UniMuMo is a unified multimodal model that uses a transformer-based encoder-decoder architecture to generate music, motion, and text from any combination of the three modalities as input. - The model bridges the modalities through a unified encoder-decoder architecture after converting inputs to a token-based representation and addresses the lack of time-synchronized data by aligning unpaired music and motion data based on rhythmic patterns and using existing large-scale datasets of single modalities.  - It utilizes a music codebook to encode motion and introduces a music-motion parallel generation scheme. - This design unifies all music and motion generation tasks into a single transformer decoder architecture with one training task of music-motion joint generation and can be efficiently achieved by fine-tuning existing pre-trained single-modality models. - Extensive evaluations shows that UniMuMo achieves competitive results across all unidirectional generation benchmarks including text-to-music, music-to-motion, motion-to-music, music captioning and motion captioning. | ['Multimodal', 'Text-to-Audio', 'Text-to-Video', 'Audio-to-Audio', 'Audio-to-Audio', 'Video-Text-to-Text'] | [Link](https://hanyangclarence.github.io/unimumo_demo/) | N/A |
| [LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning](https://arxiv.org/abs/2410.02884) | Tong Che, Jingdi Lei, schrodingers-tiger, jwu323, qq8933 | LLaMA-Berry is a new framework for enhancing the mathematical reasoning ability of Large Language Models (LLMs) by combining Monte Carlo Tree Search (MCTS) with iterative Self-Refine and a pairwise reward model. - The framework uses Self-Refine applied to MCTS (SR-MCTS) to optimize the reasoning path by leveraging the self-critic and rewriting capabilities of LLMs. - A Pairwise Preference Reward Model (PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is used to evaluate different reasoning paths globally. - An Enhanced Borda Count (EBC) method synthesizes pairwise preferences between solutions into a global ranking score to identify better answers. - Experimental results on benchmarks like GSM8K, MATH, AIME24, AMC23, and GPQA Diamond demonstrate that LLaMA-Berry significantly improves the performance of LLaMA-3.1-8B, achieving results competitive with GPT-4 Turbo without additional training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs](https://arxiv.org/abs/2410.04698) | cxiong, lunshi, hendrydong, yuhuixu, demolei | **- MATHHAY: An automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs.** **- Unlike previous benchmarks, MATHHAY requires both information retrieval and complex mathematical reasoning, focusing on real-world scenarios within a specified time period.** **- Includes questions of varying difficulty levels across different input lengths (32K, 64K, 128K) and utilizes a combination of rule-based exact matching and LLM-based judgment for evaluation.** **- Experimental results reveal that even top-performing LLMs like Gemini struggle with long contexts in mathematical reasoning, indicating room for improvement.** **- Open-source models significantly underperform compared to closed-source models.** | ['Question Answering'] | N/A | N/A |
| [TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles](https://arxiv.org/abs/2410.05262) | siminniu, fan2goa1, WinfredShi, Ki-Seki, Duguce |  - TurtleBench is a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) using real user guesses from an online Turtle Soup Puzzle game. - This dynamic approach creates a bilingual dataset (Chinese and English) with 1532 annotated user guesses, which are then used to test the reasoning abilities of the LLMs.  - The benchmark emphasizes reasoning ability and minimizes reliance on memorization and background knowledge.  - Nine advanced LLMs, including open and closed-source models, were tested on TurtleBench.  - The results show that Claude-3.5-Sonnet and GPT-4 performed best but that OpenAI's o1 series models performed sub-optimally. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/mazzzystar/TurtleBench) | N/A |
| [Grounding Language in Multi-Perspective Referential Communication](https://arxiv.org/abs/2410.03959) | alsuhr, mao1207, ZinengTang | This paper introduces a new task and dataset for evaluating referring expression generation and comprehension in multi-agent embodied environments. The dataset, comprising 2,970 human-written referring expressions, requires agents to consider each other's perspective when generating and understanding references to objects.  The authors find that model performance lags behind that of human agents in both generation and comprehension tasks.  A speaker model fine-tuned using communicative success significantly improves performance, surpassing even a strong proprietary model (GPT-40). The contributions include a novel platform for generating 3D scenes, a new dataset, and analysis of language strategies in embodied referential communication. | ['Multimodal'] | [Link](https://github.com/zinengtang/MulAgentRef) | N/A |


## Papers for 2024-10-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Addition is All You Need for Energy-efficient Language Models](https://arxiv.org/abs/2410.00907) | Wei Sun, luohy | - The paper proposes a novel linear-complexity multiplication (L-Mul) algorithm to approximate floating-point multiplication with integer addition, aiming to reduce energy consumption in large language models (LLMs). - L-Mul replaces expensive floating-point multiplications with less energy-intensive integer additions and introduces an offset to maintain accuracy. - The authors claim L-Mul achieves higher precision and requires less computation compared to 8-bit floating-point multiplications and 80% energy reduction for dot products. - Experiments on various LLMs and tasks (MMLU, BBH, GSM8k, visual question answering) showed that L-Mul in attention layers maintained or even slightly improved performance compared to standard multiplication and outperformed float8 with training free setting. - Fine-tuning models with all multiplications replaced by 3-bit L-Mul achieved comparable results to models using float8_e4m3 accumulation, showcasing its potential for efficient LLM training and deployment. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [NL-Eye: Abductive NLI for Images](https://arxiv.org/abs/2410.02613) | Zorik Gekhman, yonatanbitton, nitay, tokeron, MorVentura |  - NL-EYE, a benchmark designed to evaluate the visual abductive reasoning skills of Visual Language Models (VLMs), is introduced. - NL-EYE tasks models with evaluating the plausibility of hypothesis images given a premise image, requiring explanations for their choices and consisting of 350 image triplets across six reasoning categories: physical, functional, logical, emotional, cultural, and social. - Results show that while humans perform well, VLMs struggle, often failing to surpass random baselines in plausibility prediction. - Even with correct predictions, VLM explanations are frequently unhelpful, indicating weaknesses in visual interpretation and accurate representation generation for reasoning. - Further analysis suggests that VLMs face challenges with temporal reasoning, absolute judgments, and non-correlational tasks, particularly emotional reasoning. | ['Multimodal', 'Computer Vision'] | N/A | N/A |
| [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703) | Yossi Matias, Matan Kalman, yanivle | -"Selective Attention" is introduced; a parameter-free adjustment to the standard attention mechanism in Transformers, enabling a token to deem another as no longer relevant for future tokens and masking it, improving language modelling performance across various model sizes and context lengths. -It allows for reduction in the attention context buffer size without quality loss, resulting in significant memory and compute savings during inference, achieving up to 16X, 25X, and 47X memory reduction for context sizes of 512, 1024, and 2048 respectively with a 100M parameter model trained on C4. -Selective attention transformers often outperform standard transformers with ~2X more parameters and heads in their attention module. -Visualizations show selective attention exhibiting dynamic context pruning behavior; masking previous assignments to the same variable in variable assignment, masking ambiguous inputs until ambiguity resolution, and retaining only necessary elements in tasks like Parity and Copy. -Evaluation on C4 dataset shows consistent perplexity improvements across different model sizes and context lengths; further improvements via explicit loss to encourage masking, and HellaSwag benchmark reveals consistent accuracy gains across various model sizes using selective attention. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise](https://arxiv.org/abs/2410.03017) | Susanna Loeb, ddemszky, carlycodes, Analu, rose-e-wang |  - This paper introduces Tutor CoPilot, a Human-AI system designed to enhance real-time tutoring in K-12 education by providing expert-like guidance to tutors as they interact with students.  - Tutor CoPilot leverages the Bridge method, which captures expert decision-making patterns and adapts Large Language Models (LLMs) to generate contextually relevant suggestions for tutors during live sessions.  - A randomized controlled trial involving 900 tutors and 1,800 K-12 students demonstrates that Tutor CoPilot significantly improves student learning outcomes, particularly for students with lower-rated tutors.  - Analysis of over 550,000 chat messages reveals that tutors using Tutor CoPilot are more likely to employ high-quality pedagogical strategies that foster student understanding and less likely to simply provide answers.  - Tutor CoPilot offers a scalable and cost-effective solution ( $20 per tutor annually) for enhancing tutoring quality, especially in under-served communities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760) | David Bau, Samuel Marks, sfeucht, RohitGandikota | - This research introduces Erasure of Language Memory (ELM), a novel method for removing specific concepts from large language models (LLMs) while preserving fluency and general knowledge. - ELM employs a multi-objective fine-tuning approach with targeted low-rank updates (LoRA). - The method optimizes for erasure of the target concept, retention of unrelated information, and generation fluency when prompted with the erased concept. - Experiments on biosecurity, cybersecurity, and literary domains demonstrate ELM’s efficacy in achieving near-random performance on erased topics while maintaining high scores on general knowledge benchmarks and generating more fluent text than baseline methods. - ELM also exhibits robustness against adversarial attacks, further highlighting its potential for safe and controlled LLM editing. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/rohitgandikota/erasing-llm) | [Link](https://huggingface.co/cais/Zephyr_RMU) |
| [CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction](https://arxiv.org/abs/2410.01273) | wpiioos, Unmanned-YuBeen, lastdefiance20, PurpleSand, MilkClouds |  - CANVAS, a novel framework for intuitive human-robot interaction, is introduced for commonsense-aware navigation. It combines visual and linguistic instructions to generate robot actions, leveraging pre-trained vision-language models (VLMs) to achieve this. - A new dataset called COMMAND, containing 48 hours of driving data over 219 kilometers with human-annotated instructions and navigation outcomes across office, street and orchard simulated environments, was collected to train and test the model. - Experimental results show that CANVAS consistently outperforms the rule-based ROS NavStack in all environments, especially in challenging scenarios like uneven terrain or misleading instructions, with higher success and lower collision rates. - CANVAS achieves successful Sim2Real transfer with a 69% success rate in a real-world office setting, demonstrating its robustness beyond simulated data. - Ablation study confirms that using pre-trained VLM weights improves performance considerably, indicating the usefulness of existing knowledge for navigation tasks. | ['Robotics', 'Multimodal'] | N/A | N/A |


## Papers for 2024-10-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](https://arxiv.org/abs/2410.02740) | Chen Chen, Vasileios Saveris, haotiz, Hong-You, jefflai | This paper investigates the role of large-scale image-caption data in pre-training multimodal foundation models, particularly focusing on the interplay between synthetic captions and original AltText. - It proposes a controllable and scalable captioning pipeline capable of generating diverse caption formats (short, descriptive, dense, AltText-fused). - Experiments across CLIP, multimodal LLMs, and diffusion models reveal that a hybrid approach, combining synthetic captions and AltText, often outperforms using synthetic captions alone.  - Different model types exhibit preferences for specific caption formats: shorter captions for CLIP, descriptive for multimodal LLMs and diffusion models. - Combining AltText with synthetic captions enhances performance, likely due to improved image-text alignment from synthetic captions and increased data diversity from AltText. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification'] | N/A | N/A |
| [Video Instruction Tuning With Synthetic Data](https://arxiv.org/abs/2410.02713) | Wei Li, Chunyuan24, liuziwei7, kimingng, ZhangYuanhan |  - This paper introduces LLaVA-Video, a large multimodal model for video understanding, and LLaVA-Video-178K, a synthetic dataset created for video instruction following. - LLaVA-Video-178K consists of 178,510 videos with 1.3 million instruction samples including detailed captions generated with a recurrent, multi-level approach, along with open-ended and multiple-choice question answering generated using GPT-4. - The model leverages a SlowFast video representation technique to optimize the balance between frame count and limited GPU memory, enabling processing of three times more frames than traditional methods. - LLaVA-Video achieves state-of-the-art results on various video benchmarks, outperforming existing open-source models and demonstrating the effectiveness of the proposed synthetic dataset and training approach. - The dataset, codebase, model checkpoints, and a visual chat demo are publicly released to foster development of general-purpose visual assistants. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/datasets/lmms-lab/VideoDetailCaption) |
| [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712) | Chunyuan24, henghuang, thughost, russwang, txiong23 | **-** LLaVA-Critic is the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess the performance of other multimodal models across various tasks.  **-** It leverages a new high-quality critic instruction-following dataset incorporating diverse evaluation criteria and scenarios, including pointwise scoring and pairwise ranking.  **-** The model shows strong performance as an LMM-as-a-Judge, generating evaluation scores and rankings comparable to commercial GPT models.  **-** In preference learning, LLaVA-Critic generates effective reward signals for iterative Direct Preference Optimization (DPO), surpassing rewards from human feedback as seen in LLaVA-RLHF.  **-** LLaVA-Critic is open-sourced, including its data, code, checkpoints, and demo. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [Contrastive Localized Language-Image Pre-Training](https://arxiv.org/abs/2410.02746) | Marcin Eichner, Xinze Wang, haotiz, jefflai, Hong-You | - CLOC is a new pre-training framework for vision encoders with enhanced localization capabilities. - It augments the CLIP loss with a region-text contrastive loss and a lightweight prompter module that extracts region embeddings from the image embedding given spatial hints. - A visually-enriched and spatially-localized captioning pipeline is designed to generate region-text pseudo-labels at scale, resulting in a two-billion image-text dataset with fine-grained region-text annotations. - CLOC consistently outperforms CLIP on 31 evaluation tasks, including standard image-text tasks, newly constructed region-text tasks, and downstream evaluations with MLLMs, particularly on referring and grounding tasks. - The enhanced localization capabilities of CLOC enable it to be a drop-in replacement of CLIP to enhance MLLMs. | ['Multimodal', 'Image Classification', 'Image Feature Extraction', 'Visual Question Answering', 'Zero-Shot Image Classification'] | N/A | [Link](https://huggingface.co/datasets/zzliang/GRIT) |
| [Large Language Models as Markov Chains](https://arxiv.org/abs/2410.02724) | Abdelhakim Benechehab, Oussama Zekri, ievred, NBoulle, ambroiseodt |  - This paper draws an equivalence between large language models (LLMs) and Markov chains, offering a new theoretical framework to analyze LLM inference.  - By representing LLMs with vocabulary size *T* and context window *K* as Markov chains on a state space of size O(*T*<sup>*K*</sup>), the authors derive findings on stationary distribution, convergence speed, and temperature influence.  - The paper derives generalization bounds for pre-training and in-context learning under minimal assumptions, using concentration inequalities for dependent random variables and leveraging insights from the Markov chain equivalence.  - The theoretical analysis predicts in-context scaling laws that are experimentally validated on recent LLMs (2023-2024), showing that LLMs outperform minimax optimal frequentist Markov chain learning.  - Experimental results on various Markov chains and dynamical systems further support the theoretical findings and demonstrate the practical implications of the proposed framework. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling](https://arxiv.org/abs/2409.19291) | Yu Cheng, Jihai Zhang, Spico, Xiaoye08 | -  This paper introduces Diversified Multiplet Upcycling (DMU), a novel method for enhancing the Contrastive Language-Image Pre-training (CLIP) model by integrating it with a Mixture of Experts (MoE) architecture. DMU fine-tunes multiple CLIP models from a pre-trained checkpoint using Multistage Contrastive Learning (MCL) to capture diverse feature distributions. These fine-tuned models, sharing parameters except for the Feed-Forward Network, are then used to initialize a CLIP-MoE. The approach significantly improves CLIP's performance on various zero-shot tasks, including retrieval and image classification, as well as in downstream Multimodal Large Language Model (MLLM) benchmarks when serving as a vision encoder. Notably, CLIP-MoE surpasses the base OpenAI CLIP model by approximately 20% on retrieval tasks and exhibits minimal additional training overhead, using only 2% of the computational resources required to train a CLIP from scratch. This method provides a model-agnostic and computationally efficient way to scale CLIP and enhance its ability to capture rich, fine-grained information for improved performance in various multimodal applications. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/OpenSparseLLMS/CLIP-MOE) | N/A |
| [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367) | Jun Zhu, Pengle Zhang, Jia wei, Jintao Zhang, surfingtomchen | - SageAttention, a novel post-training quantization method designed to accelerate attention in Transformer models by quantizing tensors to 8-bit integers. - It overcomes the challenges of accuracy degradation in existing methods by smoothing the K matrix to mitigate outlier effects and employing a low-precision FP16 accumulator for the PV matrix multiplication. - It integrates effective kernel fusion with ROPE and an online softmax inspired by FlashAttention. - Comprehensive experiments demonstrate a 2.1x speed improvement over FlashAttention2 and 2.7x over xFormers on an RTX 4090. - It maintains comparable end-to-end metrics across diverse applications, including language, image, and video generation models. | ['Text-to-Image', 'Text-to-Video', 'Text2Text Generation', 'Image Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?](https://arxiv.org/abs/2410.02115) | Jianye Hou, Baibei Ji, Juntao Li, Keyan Zhou, ZetangForward | • L-CiteEval, a new multi-task benchmark for evaluating long-context understanding with citations in large language models (LLMs) is introduced. • The benchmark comprises 11 diverse tasks with context lengths ranging from 8K to 48K tokens and employs automatic evaluation metrics for reproducibility. • Evaluation of 11 LLMs reveals that open-source models lag significantly behind closed-source counterparts in citation accuracy, suggesting reliance on inherent knowledge rather than provided context. • Retrieval-Augmented Generation (RAG) improves faithfulness in open-source LLMs but slightly diminishes generation quality. • A strong correlation is observed between LLMs' attention mechanisms and citation generation process, offering insight into LLM evaluation and development. | ['Question Answering', 'Summarization', 'Natural Language Processing'] | [Link](https://github.com/ZetangForward/L-CITEEVAL.git) | N/A |
| [Training Language Models on Synthetic Edit Sequences Improves Code Synthesis](https://arxiv.org/abs/2410.02749) | Rob Fergus, lerrel, upiter | - LintSeq, a synthetic data generation algorithm, refactors existing code into edit sequences to improve code synthesis in large language models (LLMs). - LLMs trained on this data produce more diverse programs, resulting in better inference-time scaling for benchmark pass rate. - Tiny (150M parameter) edit sequence LMs achieve state-of-the-art performance for their model class, matching or outperforming models twice their size. - Repeated sampling from smaller edit sequence finetuned LLMs achieves HumanEval coverage competitive with GPT-4 at similar cumulative inference cost to single samples from large open-source LLMs. - Ablating linter guidance from LintSeq degrades downstream performance. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Feature Extraction'] | [Link](https://github.com/upiterbarg/lintseq) | N/A |
| [Distilling an End-to-End Voice Assistant Without Instruction Training Data](https://arxiv.org/abs/2410.02678) | Michael Ryan, Ella Li, zyanzhe, missblanchett, WillHeld | **Summary of "Distilling an End-to-End Voice Assistant Without Instruction Training Data"**  - This paper introduces DiVA, a new speech large language model (LLM) trained through knowledge distillation from a text-based LLM, eliminating the need for explicit instruction-following data. DiVA utilizes a novel cross-modal context distillation method, which uses a frozen text-based LLM to guide the audio model's training by matching the output distribution from text transcripts of the audio. The audio input is processed using Whisper for feature extraction and a Q-Former initialized from Whisper's decoder to achieve audio-text feature alignment. - DiVA generalizes well to various spoken language tasks such as Spoken Question Answering, Classification (emotion, humor, and sarcasm detection), and Translation, using only ASR data for training. - In evaluation benchmarks, DiVA outperforms other open-access Speech and Audio LLMs on question answering by a significant margin despite using substantially less compute for training. - DiVA excels in following text-based instructions provided through prompts and user's speech, addressing the "forgetting" issue observed in other models trained using supervised fine-tuning.  - In a user study, DiVA received a 72% preference rate compared to Qwen 2 Audio, demonstrating its effectiveness in real-world scenarios despite some limitations like inheriting the base LLM's bias. | ['Multimodal', 'Audio', 'Automatic Speech Recognition', 'Question Answering', 'Translation'] | N/A | N/A |
| [Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos](https://arxiv.org/abs/2410.02763) | Jianrui Zhang, yjlee0222, mucai |  - This paper introduces Vinoground, a novel temporal counterfactual benchmark for evaluating Large Multimodal Models (LMMs) on dense temporal reasoning in short videos. - Vinoground contains 1000 short video and caption pairs with captions containing the same words but in different orders to create temporal counterfactuals. - The benchmark evaluates an LMM’s ability to distinguish temporal differences between actions and object transformations (e.g., "water turning into ice” vs. “ice turning into water”). - Experimental results show that even state-of-the-art LMMs struggle with temporal reasoning, with the best model (GPT-40) achieving only 54% accuracy on text score and much worse on other metrics, while human performance is around 90%. - All open-source models and CLIP-based models perform much worse, suggesting that existing methods struggle at fully understanding video temporality. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://vinoground.github.io) | N/A |
| [Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data](https://arxiv.org/abs/2410.02056) | manocha, ctnzr, rafaelvalle, ZhifengKong, SreyanG-NVIDIA | Synthio is a novel approach to augment small-scale audio classification datasets using synthetic data generated from text-to-audio (T2A) diffusion models, aligning the generated data with the target dataset's acoustic characteristics through preference optimization. - It addresses the challenge of creating diverse synthetic augmentations by introducing MixCap, a technique that leverages Large Language Models (LLMs) to generate and refine meaningful audio captions used for prompting the T2A model. - Synthio's evaluation across ten datasets and four limited-data settings demonstrates consistent outperformance of existing baselines, improving classification accuracy by 0.1% to 39% using a T2A model trained solely on weakly-captioned AudioSet. - Ablation studies show the vital role of preference optimization and MixCap in achieving optimal results. - Additional analysis demonstrates effectiveness of Synthio in enhancing captioning tasks and addressing long-tail categories. | ['Audio', 'Audio Classification', 'Text-to-Audio'] | [Link](https://github.com/Sreyan88/Synthio) | N/A |


## Papers for 2024-10-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging](https://arxiv.org/abs/2410.01215) | Xiaodong Gu, Chengcheng Wan, Songsong Wang, YerbaPage |  - MGDebugger, a hierarchical code debugger, is introduced to improve the pass rate of LLM-generated code by addressing bugs at multiple levels of granularity.   - MGDebugger decomposes code into subfunctions, debugs them iteratively in a bottom-up manner, and uses an LLM-simulated Python executor to track variable states for precise error identification.   - Experiments show that MGDebugger significantly outperforms existing debugging systems, achieving an 18.9% accuracy improvement over seed generations in HumanEval and a 97.6% repair success rate in HumanEval-Fix.  - Ablation studies confirm the effectiveness of hierarchical debugging, and further analysis highlights the robustness of MGDebugger across diverse bug types, code lengths, and debugging attempts.  - MGDebugger leverages pretrained LLMs for debugging, eliminating task-specific retraining for a lightweight and scalable solution. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/YerbaPage/MGDebugger) | N/A |
| [Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis](https://arxiv.org/abs/2409.20059) | nunonmg, PierreColombo, CelineH, emmanuelmalherbe, hgissbkh | This paper conducts an empirical analysis of preference-based alignment techniques for enhancing large language model (LLM)-based translation, focusing on Contrastive Preference Optimization (CPO). - CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data regarding alignment metrics, like xCOMET-QE. - Preference-based alignment is highly sensitive to the choice of candidate translation systems used for generating preference data, affecting both the alignment metric and downstream metric consistency. - Aligning a model using its own translations achieves performance comparable to employing multiple external systems, ensuring better metric consistency.  - The paper also finds that preference-based lexical alignment using the gold reference as the preferred translation performs poorly.  - Optimizing preference data in a mono-system setting, specifically setting the quality of the chosen and rejected translations, allows the model to match the performance of multi-system settings. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://huggingface.co/collections/artefactory/translation-alignment-analysis) |
| [LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/abs/2410.01744) | Zhihan Zhang, Tianqing Fang, Mengzhao Jia, kaixinm, wyu1 | - LEOPARD, a Multimodal Large Language Model (MLLM), specializes in handling text-rich, multi-image tasks, addressing the limitations of existing MLLMs in this area by focusing on high-quality instruction tuning data and image resolution. - A new dataset, LEOPARD-INSTRUCT, comprising 925K samples, including 739K designed for text-rich, multi-image scenarios, is introduced to train the model. The dataset focuses on real-world domains like multi-page documents, multi-charts, and webpage snapshots. - An adaptive, high-resolution, multi-image encoding module dynamically optimizes the visual sequence length based on image dimensions using pixel shuffling for compression, enabling processing of multiple high-resolution images without information loss. - Experiments conducted on 13 benchmarks demonstrate LEOPARD's superior performance in text-rich multi-image benchmarks with a +9.61 point improvement over other open-source MLLMs. - The model remains competitive on single image and general-domain tasks, highlighting the benefits of training on high-quality, tailored multi-image datasets | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | [Link](https://github.com/Jill0001/Leopard) | N/A |
| [Not All LLM Reasoners Are Created Equal](https://arxiv.org/abs/2410.01748) | Aaron Courville, Daniel Toyama, Alessandro Sordoni, agarwl, arianhosseini |  - This paper investigates Large Language Models' (LLMs) reasoning abilities on grade-school math (GSM) problems, specifically focusing on compositional GSM problems, where the answer to the first question is a variable in the second question. - The study reveals a significant reasoning gap in most LLMs, indicated by a performance difference between solving compositional question pairs and solving each question independently. - This gap is more pronounced in smaller, more cost-efficient, and math-specialized models, suggesting potential limitations in reasoning abilities. -  Instruction-tuning, code generation, and finetuning have varying effects across LLMs, while finetuning can lead to overfitting. - Large reasoning gaps stem from distraction from additional context and poor second-hop reasoning, rather than dataset leakage, impacting performance despite high scores on standard GSM benchmarks. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [HelpSteer2-Preference: Complementing Ratings with Preferences](https://arxiv.org/abs/2410.01257) | okuchaiev, gshennvm, trias702, odelalleau, alexwb |   - This paper introduces HelpSteer2-Preference, a novel dataset of preference annotations designed to complement the existing ratings in the HelpSteer2 dataset, enabling a head-to-head comparison of Bradley-Terry and Regression style reward models. - The authors propose a novel approach combining Bradley-Terry and Regression reward modeling, leading to a Llama 3.1 70B Instruct model that achieved a state-of-the-art 94.1 score on RewardBench as of October 1, 2024. - The preference annotations are accompanied by human-written justifications, enhancing data interpretability and providing insights into annotator decision-making. - The research demonstrates that data format (regression vs. preference) is less critical than the model's ability to capture annotation information, with preference magnitude being key for Bradley-Terry models.  - The combined reward model effectively aligns language models to follow instructions using online Reinforcement Learning from Human Feedback (RLHF), particularly with the REINFORCE algorithm. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/nvidia/HelpSteer2), [Link](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) |
| [RATIONALYST: Pre-training Process-Supervision for Improving Reasoning](https://arxiv.org/abs/2410.01044) | Guoxuan Wang, danyaljj, ChuyuLiu, ylu610, Dongwei | - RATIONALYST, a model pre-trained on implicit rationales extracted from unlabeled text and existing reasoning datasets, is introduced for process-supervision of reasoning. - RATIONALYST leverages these implicit rationales during inference to guide the reasoning process of large language models, enhancing both interpretability and performance. - It consistently generalizes across various reasoning tasks, demonstrating an average 3.9% accuracy improvement on 7 representative reasoning benchmarks when fine-tuned from LLaMa-3-8B. - RATIONALYST outperforms both stronger general-purpose verifiers like GPT-4 and similarly sized models trained on matching datasets, showcasing the efficacy of its process supervision approach. - An ablation study shows that rationales from web-scale data enhance performance, while implicit supervision proves more robust than explicit supervision due to tolerance for imperfect rationales. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JHU-CLSP/Rationalyst) | N/A |
| [Quantifying Generalization Complexity for Large Language Models](https://arxiv.org/abs/2410.01769) | maxtiktok, Nrain, zhuokai, Xulianghuang, luohy | This paper introduces SCYLLA, a dynamic evaluation framework designed to measure the generalization ability of Large Language Models (LLMs) and disentangle it from memorization. - SCYLLA evaluates LLMs across 20 tasks and 5 complexity levels, generating in-distribution and out-of-distribution data to assess generalization. - The study reveals a "generalization valley," where the performance gap between in-distribution and out-of-distribution data is non-monotonic with task complexity. - The peak of this valley, the "critical complexity," represents the upper bound of an LLM's generalization and shifts to higher complexity levels with increasing model size. - The benchmark results covering 28 LLMs show that closed-source models generally exhibit stronger generalization abilities and higher critical complexity than their open-sourced counterparts. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zhentingqi/scylla) | N/A |
| [E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding](https://arxiv.org/abs/2409.18111) | Ying Shan, Yang Wu, Zhongang Qi, Zongyang Ma, Ye Liu | -"E.T. Bench", a large-scale benchmark designed for open-ended, event-level video understanding. - The benchmark comprises 7.3K samples across 12 tasks, spanning 8 domains and featuring 7K videos totaling 251.4 hours. -A novel Video-LLM called "E.T. Chat" is introduced, which excels in event-level understanding by treating timestamp prediction as an embedding matching problem. - A dedicated instruction-tuning dataset, "E.T. Instruct 164K", tailored for multi-event, time-sensitive videos is created. - State-of-the-art models on existing video question answering benchmarks struggle with this new benchmark indicating that current methods struggle with fine-grained time-sensitive video understanding. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling](https://arxiv.org/abs/2410.01440) | Jiazhong Yu, Cao Sheng, Fei Li, feifeiobama, ljh0104 |  - This paper introduces equilibrium sequence modeling, a novel method for training large language models (LLMs) to perform long-horizon robotic planning by iteratively refining plans based on environmental feedback through a self-refinement process. - The approach formulates self-refinement as a fixed-point problem, allowing for end-to-end supervised training without needing external verifiers or reward models, simplifying training compared to reinforcement learning methods. - A nested equilibrium sequence modeling procedure enables efficient closed-loop planning, leveraging feedback from the environment (or a world model) and accelerating plan refinement by reusing previously computed equilibrium solutions. - Evaluations on VirtualHome-Env benchmark demonstrate state-of-the-art performance in most metrics, especially when incorporating environmental feedback, and show advantageous scaling of performance with increased inference computation. - Ablation studies highlight the effectiveness of equilibrium sequence modeling, reuse of previous solutions, and dynamic computation allocation in improving plan quality and computational efficiency. | ['Robotics', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/Singularity0104/equilibrium-planner) | N/A |
| [Selective Aggregation for Low-Rank Adaptation in Federated Learning](https://arxiv.org/abs/2410.01463) | Huijie Fan, Liangqiong-QU, yanranw1, stevezs, gpx333 |  - This research paper introduces FedSA-LoRA, a new method for federated learning that selectively aggregates learned A and B matrices from LoRA. - It asserts that A matrices learn general knowledge while B matrices capture client-specific information, leading to only sharing A matrices for aggregation. - Experimental validation across language understanding and generation tasks on benchmarks like GLUE and GSM8K demonstrates FedSA-LoRA outperforms other methods.  - The authors extend this approach to other LoRA variants (rsLoRA and VeRA), creating FedSA-rsLoRA and FedSA-VeRA, and show consistent improvements. - The findings provide insights into LoRA in federated settings and a general framework for using future LoRA adaptations. | ['Natural Language Processing', 'Text Classification', 'Text Generation', 'Question Answering'] | N/A | N/A |


## Papers for 2024-10-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Law of the Weakest Link: Cross Capabilities of Large Language Models](https://arxiv.org/abs/2409.19951) | xwhan, ruihou16, xwwang, astonzhang, MingZhong |  - This research paper explores the intersection of multiple abilities, termed "cross capabilities," in Large Language Models (LLMs), which are essential for real-world tasks but often overlooked in current evaluations that focus on individual capabilities. - It introduces CROSSEVAL, a benchmark with 1,400 human-annotated prompts and 8,400 human ratings, designed to evaluate both individual and cross capabilities, revealing that current LLMs underperform in cross-capability tasks. - The study finds that LLM cross-capability performance adheres to the "Law of the Weakest Link," being significantly limited by the weakest individual capability, regardless of improvements in other areas. - The results highlight that tool use is a major challenge for LLMs and suggest that prioritizing the enhancement of weaker capabilities is more crucial for improving overall performance than focusing on already strong ones. -  The work emphasizes the importance of shifting focus towards cross-capability evaluation and development to improve LLM effectiveness in complex, real-world scenarios rather than just on individual capabilities. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/facebookresearch/llm-cross-capabilities) | N/A |
| [TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices](https://arxiv.org/abs/2410.00531) | Hongfang Yu, Mohsen Guizani, Jiaoshen, LIKirin | TPI-LLM is a tensor parallel inference system designed for serving 70B-scale LLMs efficiently on low-resource edge devices. - It addresses memory limitations by introducing a sliding window memory scheduler that dynamically manages layer weights during inference, overlapping disk I/O with computation and communication. - TPI-LLM prioritizes tensor parallelism over pipeline parallelism for single-user scenarios on edge devices and implements a star-based allreduce algorithm to minimize link latency. - Experimental results show significant reductions in time-to-first-token, token latency, and peak memory footprint compared to benchmarks like Transformers, Accelerate, and Galaxy. - TPI-LLM successfully runs Llama 2-70B with a peak memory footprint of 3.1GB across 8 low-resource devices, enabling larger models to run on edge devices while preserving user privacy. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Lizonghang/TPI-LLM) | N/A |
| [Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect](https://arxiv.org/abs/2409.17912) | imomayiz, amr-mohamed, khoubrane-yousef, habdine, guokan-shang | Atlas-Chat introduces the first Large Language Models (LLMs) for Moroccan Arabic, a low-resource dialectal Arabic (DA) variant also known as Darija. - A new instruction dataset, Darija-SFT-Mixture, was created by combining existing and new manually and synthetically created Darija resources, as well as translated English instructions. - Atlas-Chat-9B and 2B models, fine-tuned on this dataset, outperform existing LLMs, including Arabic-specific and state-of-the-art models like LLaMa, Jais, and AceGPT, achieving a 13% improvement over a 13B model on a new Darija benchmark. - A new evaluation suite, including DarijaMMLU, DarijaHellaSwag, and DarijaBench, was developed for comprehensive LLM assessment in Darija, focusing on discriminative and generative tasks.  - An experimental analysis was conducted on fine-tuning strategies and base model choices, finding that instruction-tuned Gemma 2 models with LoRA performed optimally. | ['Natural Language Processing', 'Translation', 'Summarization'] | N/A | [Link](https://hf.co/MBZUAI-Paris/Atlas-Chat-9B), [Link](https://hf.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture) |
| [ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer](https://arxiv.org/abs/2410.00086) | Jingren, chenweix7, chaojiemao, jingfengzhang, jiangzeyinzi |  - ACE, a unified framework based on a Diffusion Transformer, supports a wide range of visual generation and editing tasks through natural language instructions, including text-guided generation, low-level visual analysis, controllable generation, semantic editing, element editing, repainting, layer editing, and reference generation. - ACE introduces the Long-context Condition Unit (LCU) to incorporate historical information from previous generation rounds, enabling multi-turn and long-context generation. - A meticulous data collection workflow is established to construct a 0.7 billion-scale dataset covering various generation and editing tasks. - Evaluation on benchmarks such as MagicBrush and a user study on a manually curated benchmark demonstrates ACE’s superior performance in various visual generation tasks. - ACE can be easily integrated into a multimodal chat system to streamline image creation and editing, avoiding cumbersome pipelines typically employed in visual agents. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | [Link](https://huggingface.co/runwayml/stable-diffusion-v1-5), [Link](https://huggingface.co/runwayml/stable-diffusion-inpainting) |
| [Visual Context Window Extension: A New Perspective for Long Video Understanding](https://arxiv.org/abs/2409.20018) | Zhenzhong Chen, hcwei | This research paper proposes a novel approach to enhance long video understanding by extending the visual context window of Large Multimodal Models (LMMs). - It redefines the context window in LMMs as two distinct windows: visual and language, addressing the discrepancies between these modalities. - The study introduces a method to extend positional embeddings within the visual context window, enabling LMMs to handle lengthy videos without retraining on large video-text datasets. - A progressive pooling strategy is implemented to reduce memory consumption by selectively adjusting the spatial resolution of frame embeddings. - Experimental results on benchmarks like MLVU, VideoMME, and LongVideoBench demonstrate consistent performance improvements with increasing video frames, outperforming models like GPT-40 and achieving memory savings of approximately 45%. | ['Multimodal', 'Video-Text-to-Text'] | N/A | N/A |


## Papers for 2024-10-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566) | nm-w, pdufter, zhegan27, fly6464, haotiz |  - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. - MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training. - It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks. - MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI). -  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [DiaSynth -- Synthetic Dialogue Generation Framework](https://arxiv.org/abs/2409.19020) | Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl |  - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning. - It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues. - Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks. - The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data. - DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [Hyper-Connections](https://arxiv.org/abs/2409.19606) | banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder | This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse. - Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters. - Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections. - Hyper-connections are also effective in vision tasks. - Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections. - The authors anticipate this method's broad applicability across various AI problems. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Image-to-Text', 'Unconditional Image Generation', 'Text2Text Generation'] | N/A | N/A |
| [Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models](https://arxiv.org/abs/2409.18943) | yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li | - RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs). - Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation. - RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs. - Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments.  - RULER maintains overall performance on various other benchmarks without affecting non-length based generation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Geaming2002/Ruler) | N/A |
| [Cottention: Linear Transformers With Cosine Attention](https://arxiv.org/abs/2409.18747) | Eric C. Larson, TrevorDohm, gmongaras | This study introduces "Cottention," a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/gmongaras/Cottention_Transformer) | N/A |
| [Can Models Learn Skill Composition from Examples?](https://arxiv.org/abs/2409.19808) | Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu | This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4. - Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills. - Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill. - The study shows that incorporating skill-rich synthetic text improves compositional capabilities. - Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency. - Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code](https://arxiv.org/abs/2409.19715) | Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae | COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym) |
| [IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding](https://arxiv.org/abs/2409.19627) | Jianzong Wang, Jing Xiao, zhangxulong, Pechola | - IDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization. - It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating. - A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability. - IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods. - Experimental results show its ability to withstand various attacks while maintaining good imperceptibility. | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/PecholaL/IDEAW) | N/A |


## Papers for 2024-09-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692) | Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang |  - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video. - It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning. - Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines. - It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning. - MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs. | ['Multimodal', 'Any-to-Any', 'Text-to-Image', 'Image-to-Text', 'Text-to-Speech', 'Automatic Speech Recognition', 'Video-Text-to-Text'] | N/A | N/A |
| [VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2409.17066) | Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92 |  - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization. - VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization. - The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization. - They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks. - VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/VPTQ) | N/A |
| [Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult](https://arxiv.org/abs/2409.17545) | fetong | This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs). - MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning. - Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models. - On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B. - MIPO simplifies hyperparameter tuning by using only a single parameter, β, exhibiting robustness across different model architectures and datasets within a specific range. - MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback) |
| [MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making](https://arxiv.org/abs/2409.16686) | Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu | - The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales. - MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights. - Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment. - The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios. - MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents. | ['Robotics', 'Question Answering'] | N/A | N/A |


## Papers for 2024-09-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference. - Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end. - Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models. - MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks. - The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | - LLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings. - This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks. - LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets. - The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks. - Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling. | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-to-Text', 'Image-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | - EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities. - It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control. - The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data. - EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models. - A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Text-to-Audio', 'Audio-to-Audio', 'Visual Question Answering', 'Image-to-Text'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming |  - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs.  - It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference.  - This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV.  - Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilter’s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O.  - Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | [Link](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose.  - Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data).  - Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models.  - Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain.  - A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/john-hewitt/implicit-ins) | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era. - It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations. - The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs. - It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs. -  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security. | ['Natural Language Processing', 'Summarization'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations. - Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets. - It can achieve even further reduction of vector count by 66% with less than 3% performance degradation. - This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models. - The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors. - The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/colbert-ir/colbertv2.0) |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar |  - This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG). - It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness. - Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time. - The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs. - While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.  - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.  -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.  -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.  -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Text-to-Speech', 'Automatic Speech Recognition', 'Any-to-Any'] | N/A | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Image-to-3D'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence. | ['Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt | This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that "response tuning," which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that "single-task finetuning,"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang | • This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field. • The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA. • The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses. • The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach. • The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs). | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


## Papers for 2024-09-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This research introduces MaskLLM, a novel learnable pruning method that generates semi-structured (N:M) sparsity in Large Language Models (LLMs) for enhanced inference efficiency. - MaskLLM distinguishes itself from previous methods by directly learning the distribution of N:M sparsity patterns using Gumbel Softmax sampling, enabling end-to-end training on large datasets and addressing limitations of hand-crafted importance criteria. -  Empirical evaluations on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, demonstrate that MaskLLM surpasses state-of-the-art techniques, achieving a perplexity of 6.72 on LLaMA2-7B compared to SparseGPT's 10.42. - The research underscores the efficacy of learning sparsity patterns directly from data, leading to more accurate and efficient compression of LLMs without compromising performance. - The adaptability of MaskLLM to downstream tasks and its ability to achieve lossless compression in certain scenarios highlight its potential for practical applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research proposes LLaVA-3D, a novel framework for building 3D-aware Large Multimodal Models (LMMs) by adapting the existing 2D LLaVA model.  LLaVA-3D introduces the concept of "3D Patches," which inject 3D positional embeddings into 2D image features, enhancing the model's spatial understanding without complex 3D processing pipelines.  Evaluations demonstrate LLaVA-3D's state-of-the-art performance on various 3D tasks, including question answering, dense captioning, and visual grounding, surpassing existing 3D LMMs while maintaining comparable 2D image understanding capabilities to its 2D counterpart. The research highlights the advantages of leveraging pre-trained 2D LMMs for 3D scene understanding and the benefits of integrating 3D spatial information into 2D visual features. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image-to-Text', 'Text-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal large language model capable of perceiving and generating images, text, and speech with emotional expressiveness. EMOVA utilizes a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for end-to-end speech processing.  The model achieves state-of-the-art performance on both vision-language and speech benchmarks, outperforming models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks and surpassing the speech LLM Mini-Omni in ASR tasks. EMOVA also enables emotional spoken dialogue by explicitly predicting speech style labels (emotions and pitches) and leveraging a lightweight style module for controllable speech synthesis. This is achieved through a novel text-centric multimodal alignment approach, which leverages publicly available bimodal data and eliminates the reliance on scarce trimodal data. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach to reduce the computational cost and latency of processing long context inputs with Large Language Models (LLMs). GemFilter leverages the ability of early LLM layers to identify relevant tokens and compresses the input sequence by a factor of 1000x for subsequent processing by the full model. Empirical evaluations show that GemFilter achieves a 2.4x speedup and 30% reduction in GPU memory consumption compared to state-of-the-art methods, while maintaining comparable performance on benchmarks like LongBench and outperforming them on the Needle in a Haystack task. GemFilter is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting the selected input sequence. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This research paper investigates implicit instruction tuning, demonstrating that instruction following can emerge without explicit instruction-response training.  - The authors show that training solely on responses (response tuning) and on narrow-domain data (single-task finetuning) leads to broad instruction-following abilities in language models.  - For instance, response-tuned models achieve a 43% win rate against explicitly instruction-tuned models in head-to-head evaluations.  - Furthermore, they introduce a simple rule-based language model that, when combined with a pretrained model, exhibits instruction-following behavior.  - These findings highlight that adaptation methods not explicitly designed for instruction following can implicitly induce such capabilities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This research paper introduces a novel technique named "TOKEN POOLING" for enhancing the efficiency of multi-vector retrieval models, especially focusing on ColBERT, without significantly affecting performance.  The method uses clustering techniques to group together similar token representations and then applies mean pooling to create a single, representative vector, effectively reducing the overall storage footprint. Experiments show that this approach reduces the required vector count by 50% without compromising accuracy, and a 66% reduction still yields strong performance. The paper also demonstrates that TOKEN POOLING can be effectively combined with existing quantization methods, leading to even more significant compression rates while maintaining reasonable retrieval performance.  | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper presents a comprehensive review of the emerging field of Conversation Analysis (CA), a process designed to extract critical information from conversational data and leverage it for system optimization and decision-making.  - The paper systematically defines CA as a four-step procedure: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, and discusses the challenges and trends within each step. - The paper further argues that while previous CA efforts focused on atomic tasks with limited business impact, the rise of Large Language Models (LLMs) enables deeper, more insightful analysis and strategic decision-making from conversations.  - The authors compile and categorize existing benchmark datasets for CA but highlight a significant gap in comprehensive benchmarks containing fine-grained conversation elements and long-context modeling capabilities.  - The paper concludes by outlining future research directions, including the development of LLM-based conversation simulators, fine-grained CA benchmarks, long-context conversation modeling, in-depth attribution analysis, and advanced goal-directed optimization and evaluation methods. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging Knowledge Graphs (KGs) and graph-based architectures. The authors demonstrate the effectiveness of their framework by applying it to the SoccerNet dataset, a large dataset of soccer videos. Their findings show that Structured-GraphRAG significantly improves query processing efficiency, reduces response times, and enhances accuracy compared to traditional RAG methods. The structured nature of KGs reduces hallucinations in LLMs, making the responses more consistent and reliable. The authors highlight that their framework can be applied to a broad range of applications due to its flexible design. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |


