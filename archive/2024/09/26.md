

## Papers for 2024-09-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models](https://arxiv.org/abs/2409.17146) | SMSD75, jamepark3922, yyupenn, sharpen, mattdeitke | This paper introduces Molmo, a family of open-weight and open-data multimodal models for image captioning and visual question answering. The authors curate a novel dataset, PixMo, which includes detailed image descriptions from human annotators and diverse question-answer pairs. Molmo achieves state-of-the-art performance on 11 academic benchmarks, matching or surpassing proprietary models like GPT-4V and exceeding Gemini 1.5 Pro and Flash on certain tasks. Human evaluation further supports Molmo's efficacy, demonstrating its competitive performance against both open and closed models. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale](https://arxiv.org/abs/2409.17115) | Qian Liu, Pengfei, lockon, SinclairWang, koalazf99 | • This paper introduces Programming Every Example (PROX), a framework that utilizes small language models to refine pre-training corpora at scale by generating and executing data processing programs for each example. • Experimental results demonstrate that PROX-curated data consistently improves downstream task performance by over 2% on average and surpasses existing data selection methods. • PROX shows efficacy across different model sizes and pre-training corpora, achieving comparable performance to models trained on 20x larger datasets, especially in domain-specific continual pre-training. • Further analysis indicates PROX leads to more efficient pre-training, requiring less computing power for comparable performance, highlighting its potential for reducing training costs and enabling wider access to LLM development. • The authors propose future directions such as incorporating more refining operations and expanding PROX to other domains like code and multilingual data. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/GAIR-NLP/ProX) | [Link](https://huggingface.co/gair-prox) |
| [Boosting Healthcare LLMs Through Retrieved Context](https://arxiv.org/abs/2409.15127) | Ashwin Kumar Gururajan, dariog, JordiBayarri |  - This paper presents a context retrieval system designed to enhance the accuracy and reliability of Large Language Models (LLMs) for medical question answering, specifically focusing on multiple-choice question answering (MCQA).  -  The authors demonstrate that their optimized context retrieval system significantly boosts the performance of various open-source LLMs on four benchmark medical MCQA datasets, achieving results comparable to, and even surpassing, much larger private models like Google's MedPalm-2 and OpenAI's GPT-4.  -  The paper investigates the impact of different components within the context retrieval framework, such as choice shuffling, number of ensembles, embedding models, databases, and reranking models, leading to recommendations for optimal configuration based on empirical findings.  - Acknowledging the limitations of MCQA in real-world clinical settings, the authors extend their approach to develop OpenMedprompt, a novel framework specifically designed for open-ended medical question answering.  - Two OpenMedprompt strategies, Ensemble Refining (OM-ER) and Self-Reflection (OM-SR), are introduced and evaluated, showing promising results in improving open-ended answer generation accuracy compared to baseline approaches.  | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [NoTeeline: Supporting Real-Time Notetaking from Keypoints with Large Language Models](https://arxiv.org/abs/2409.16493) | Jeffrey P. Bigham, Xiaodi Alice Tang, David Chuan-en Lin, Abdus Samee, oaishi | This paper introduces NoTeeline, a novel system for real-time notetaking from videos that leverages large language models (LLMs). NoTeeline allows users to jot down concise "micronotes" and expands them into full-fledged notes reflecting the user's writing style using contextual information from video transcripts and prior user notes. The user study demonstrated that NoTeeline significantly reduced notetaking time and text length compared to a baseline note-taking application, while also increasing the number of notes taken and reducing disruptions during video watching. Furthermore, NoTeeline achieved high factual consistency (93.2%) in generated notes and received positive feedback from users for its ability to capture essential information, maintain writing style, and facilitate efficient note organization and review. | ['Multimodal', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale](https://arxiv.org/abs/2409.16299) | Nghi D. Q. Bui, Phong X. Nguyen, Huy Nhat Phan |  - This paper introduces HyperAgent, a novel generalist multi-agent system designed to address a wide spectrum of software engineering (SE) tasks across different programming languages.   - HyperAgent achieves state-of-the-art performance across various SE tasks, including a 25.01% success rate on SWE-Bench-Lite and 31.40% on SWE-Bench-Verified for GitHub issue resolution, surpassing existing methods.   - It also demonstrates superior performance in code generation at repository scale (RepoExec) and fault localization and program repair (Defects4J), often outperforming specialized systems.   - HyperAgent comprises four specialized agents – Planner, Navigator, Code Editor, and Executor – mimicking human developers' workflows to manage the full lifecycle of SE tasks.   - The system's adaptability, efficiency, and use of open-source models position it as a practical solution for various SE tasks. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Other'] | N/A | N/A |
