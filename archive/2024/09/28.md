

## Papers for 2024-09-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.  - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.  -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.  -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.  -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Text-to-Speech', 'Automatic Speech Recognition', 'Any-to-Any'] | N/A | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Image-to-3D'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming | This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence. | ['Text Generation'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | N/A |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt | This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that "response tuning," which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that "single-task finetuning,"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang | • This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field. • The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA. • The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses. • The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach. • The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs). | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar | This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |
