

## Papers for 2024-09-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](https://arxiv.org/abs/2409.17481) | wxcTest, gheinrich, srvm, yinhongxu, Vinnnf |  - MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference. - Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end. - Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models. - MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks. - The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/NVlabs/MaskLLM) | N/A |
| [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125) | Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu | - LLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings. - This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks. - LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets. - The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks. - Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling. | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-to-Text', 'Image-to-3D'] | N/A | N/A |
| [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) | vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998 | - EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities. - It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control. - The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data. - EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models. - A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue. | ['Multimodal', 'Text-to-Speech', 'Automatic Speech Recognition', 'Text-to-Audio', 'Audio-to-Audio', 'Visual Question Answering', 'Image-to-Text'] | N/A | N/A |
| [Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction](https://arxiv.org/abs/2409.17422) | Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming |  - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs.  - It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference.  - This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV.  - Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilter’s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O.  - Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/SalesforceAIResearch/GemFilter) | [Link](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) |
| [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254) | Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt |  - This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose.  - Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data).  - Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models.  - Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain.  - A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/john-hewitt/implicit-ins) | N/A |
| [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195) | Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang |  - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era. - It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations. - The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs. - It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs. -  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security. | ['Natural Language Processing', 'Summarization'] | N/A | N/A |
| [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683) | Griffin Adams, Benjamin Clavié, NohTow | This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations. - Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets. - It can achieve even further reduction of vector count by 66% with less than 3% performance degradation. - This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models. - The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors. - The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/colbert-ir/colbertv2.0) |
| [Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study](https://arxiv.org/abs/2409.17580) | Pål Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar |  - This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG). - It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness. - Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time. - The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs. - While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications. | ['Question Answering', 'Graph Machine Learning'] | N/A | N/A |
