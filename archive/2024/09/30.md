

## Papers for 2024-09-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692) | Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang |  - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video. - It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning. - Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines. - It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning. - MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs. | ['Multimodal', 'Any-to-Any', 'Text-to-Image', 'Image-to-Text', 'Text-to-Speech', 'Automatic Speech Recognition', 'Video-Text-to-Text'] | N/A | N/A |
| [VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2409.17066) | Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92 |  - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization. - VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization. - The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization. - They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks. - VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/VPTQ) | N/A |
| [Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult](https://arxiv.org/abs/2409.17545) | fetong | This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs). - MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning. - Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models. - On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B. - MIPO simplifies hyperparameter tuning by using only a single parameter, Î², exhibiting robustness across different model architectures and datasets within a specific range. - MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback) |
| [MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making](https://arxiv.org/abs/2409.16686) | Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu | - The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales. - MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights. - Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment. - The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios. - MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents. | ['Robotics', 'Question Answering'] | N/A | N/A |
