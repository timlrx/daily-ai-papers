

## Papers for 2024-10-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566) | nm-w, pdufter, zhegan27, fly6464, haotiz |  - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. - MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training. - It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks. - MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI). -  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [DiaSynth -- Synthetic Dialogue Generation Framework](https://arxiv.org/abs/2409.19020) | Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl |  - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning. - It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues. - Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks. - The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data. - DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [Hyper-Connections](https://arxiv.org/abs/2409.19606) | banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder | This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse. - Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters. - Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections. - Hyper-connections are also effective in vision tasks. - Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections. - The authors anticipate this method's broad applicability across various AI problems. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Image-to-Text', 'Unconditional Image Generation', 'Text2Text Generation'] | N/A | N/A |
| [Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models](https://arxiv.org/abs/2409.18943) | yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li | - RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs). - Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation. - RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs. - Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments.  - RULER maintains overall performance on various other benchmarks without affecting non-length based generation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Geaming2002/Ruler) | N/A |
| [Cottention: Linear Transformers With Cosine Attention](https://arxiv.org/abs/2409.18747) | Eric C. Larson, TrevorDohm, gmongaras | This study introduces "Cottention," a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/gmongaras/Cottention_Transformer) | N/A |
| [Can Models Learn Skill Composition from Examples?](https://arxiv.org/abs/2409.19808) | Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu | This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4. - Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills. - Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill. - The study shows that incorporating skill-rich synthetic text improves compositional capabilities. - Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency. - Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code](https://arxiv.org/abs/2409.19715) | Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae | COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym) |
| [IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding](https://arxiv.org/abs/2409.19627) | Jianzong Wang, Jing Xiao, zhangxulong, Pechola | - IDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization. - It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating. - A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability. - IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods. - Experimental results show its ability to withstand various attacks while maintaining good imperceptibility. | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/PecholaL/IDEAW) | N/A |
