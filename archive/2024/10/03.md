

## Papers for 2024-10-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging](https://arxiv.org/abs/2410.01215) | Xiaodong Gu, Chengcheng Wan, Songsong Wang, YerbaPage |  - MGDebugger, a hierarchical code debugger, is introduced to improve the pass rate of LLM-generated code by addressing bugs at multiple levels of granularity.   - MGDebugger decomposes code into subfunctions, debugs them iteratively in a bottom-up manner, and uses an LLM-simulated Python executor to track variable states for precise error identification.   - Experiments show that MGDebugger significantly outperforms existing debugging systems, achieving an 18.9% accuracy improvement over seed generations in HumanEval and a 97.6% repair success rate in HumanEval-Fix.  - Ablation studies confirm the effectiveness of hierarchical debugging, and further analysis highlights the robustness of MGDebugger across diverse bug types, code lengths, and debugging attempts.  - MGDebugger leverages pretrained LLMs for debugging, eliminating task-specific retraining for a lightweight and scalable solution. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/YerbaPage/MGDebugger) | N/A |
| [Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis](https://arxiv.org/abs/2409.20059) | nunonmg, PierreColombo, CelineH, emmanuelmalherbe, hgissbkh | This paper conducts an empirical analysis of preference-based alignment techniques for enhancing large language model (LLM)-based translation, focusing on Contrastive Preference Optimization (CPO). - CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data regarding alignment metrics, like xCOMET-QE. - Preference-based alignment is highly sensitive to the choice of candidate translation systems used for generating preference data, affecting both the alignment metric and downstream metric consistency. - Aligning a model using its own translations achieves performance comparable to employing multiple external systems, ensuring better metric consistency.  - The paper also finds that preference-based lexical alignment using the gold reference as the preferred translation performs poorly.  - Optimizing preference data in a mono-system setting, specifically setting the quality of the chosen and rejected translations, allows the model to match the performance of multi-system settings. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://huggingface.co/collections/artefactory/translation-alignment-analysis) |
| [LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/abs/2410.01744) | Zhihan Zhang, Tianqing Fang, Mengzhao Jia, kaixinm, wyu1 | - LEOPARD, a Multimodal Large Language Model (MLLM), specializes in handling text-rich, multi-image tasks, addressing the limitations of existing MLLMs in this area by focusing on high-quality instruction tuning data and image resolution. - A new dataset, LEOPARD-INSTRUCT, comprising 925K samples, including 739K designed for text-rich, multi-image scenarios, is introduced to train the model. The dataset focuses on real-world domains like multi-page documents, multi-charts, and webpage snapshots. - An adaptive, high-resolution, multi-image encoding module dynamically optimizes the visual sequence length based on image dimensions using pixel shuffling for compression, enabling processing of multiple high-resolution images without information loss. - Experiments conducted on 13 benchmarks demonstrate LEOPARD's superior performance in text-rich multi-image benchmarks with a +9.61 point improvement over other open-source MLLMs. - The model remains competitive on single image and general-domain tasks, highlighting the benefits of training on high-quality, tailored multi-image datasets | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | [Link](https://github.com/Jill0001/Leopard) | N/A |
| [Not All LLM Reasoners Are Created Equal](https://arxiv.org/abs/2410.01748) | Aaron Courville, Daniel Toyama, Alessandro Sordoni, agarwl, arianhosseini |  - This paper investigates Large Language Models' (LLMs) reasoning abilities on grade-school math (GSM) problems, specifically focusing on compositional GSM problems, where the answer to the first question is a variable in the second question. - The study reveals a significant reasoning gap in most LLMs, indicated by a performance difference between solving compositional question pairs and solving each question independently. - This gap is more pronounced in smaller, more cost-efficient, and math-specialized models, suggesting potential limitations in reasoning abilities. -  Instruction-tuning, code generation, and finetuning have varying effects across LLMs, while finetuning can lead to overfitting. - Large reasoning gaps stem from distraction from additional context and poor second-hop reasoning, rather than dataset leakage, impacting performance despite high scores on standard GSM benchmarks. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [HelpSteer2-Preference: Complementing Ratings with Preferences](https://arxiv.org/abs/2410.01257) | okuchaiev, gshennvm, trias702, odelalleau, alexwb |   - This paper introduces HelpSteer2-Preference, a novel dataset of preference annotations designed to complement the existing ratings in the HelpSteer2 dataset, enabling a head-to-head comparison of Bradley-Terry and Regression style reward models. - The authors propose a novel approach combining Bradley-Terry and Regression reward modeling, leading to a Llama 3.1 70B Instruct model that achieved a state-of-the-art 94.1 score on RewardBench as of October 1, 2024. - The preference annotations are accompanied by human-written justifications, enhancing data interpretability and providing insights into annotator decision-making. - The research demonstrates that data format (regression vs. preference) is less critical than the model's ability to capture annotation information, with preference magnitude being key for Bradley-Terry models.  - The combined reward model effectively aligns language models to follow instructions using online Reinforcement Learning from Human Feedback (RLHF), particularly with the REINFORCE algorithm. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/nvidia/HelpSteer2), [Link](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) |
| [RATIONALYST: Pre-training Process-Supervision for Improving Reasoning](https://arxiv.org/abs/2410.01044) | Guoxuan Wang, danyaljj, ChuyuLiu, ylu610, Dongwei | - RATIONALYST, a model pre-trained on implicit rationales extracted from unlabeled text and existing reasoning datasets, is introduced for process-supervision of reasoning. - RATIONALYST leverages these implicit rationales during inference to guide the reasoning process of large language models, enhancing both interpretability and performance. - It consistently generalizes across various reasoning tasks, demonstrating an average 3.9% accuracy improvement on 7 representative reasoning benchmarks when fine-tuned from LLaMa-3-8B. - RATIONALYST outperforms both stronger general-purpose verifiers like GPT-4 and similarly sized models trained on matching datasets, showcasing the efficacy of its process supervision approach. - An ablation study shows that rationales from web-scale data enhance performance, while implicit supervision proves more robust than explicit supervision due to tolerance for imperfect rationales. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JHU-CLSP/Rationalyst) | N/A |
| [Quantifying Generalization Complexity for Large Language Models](https://arxiv.org/abs/2410.01769) | maxtiktok, Nrain, zhuokai, Xulianghuang, luohy | This paper introduces SCYLLA, a dynamic evaluation framework designed to measure the generalization ability of Large Language Models (LLMs) and disentangle it from memorization. - SCYLLA evaluates LLMs across 20 tasks and 5 complexity levels, generating in-distribution and out-of-distribution data to assess generalization. - The study reveals a "generalization valley," where the performance gap between in-distribution and out-of-distribution data is non-monotonic with task complexity. - The peak of this valley, the "critical complexity," represents the upper bound of an LLM's generalization and shifts to higher complexity levels with increasing model size. - The benchmark results covering 28 LLMs show that closed-source models generally exhibit stronger generalization abilities and higher critical complexity than their open-sourced counterparts. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zhentingqi/scylla) | N/A |
| [E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding](https://arxiv.org/abs/2409.18111) | Ying Shan, Yang Wu, Zhongang Qi, Zongyang Ma, Ye Liu | -"E.T. Bench", a large-scale benchmark designed for open-ended, event-level video understanding. - The benchmark comprises 7.3K samples across 12 tasks, spanning 8 domains and featuring 7K videos totaling 251.4 hours. -A novel Video-LLM called "E.T. Chat" is introduced, which excels in event-level understanding by treating timestamp prediction as an embedding matching problem. - A dedicated instruction-tuning dataset, "E.T. Instruct 164K", tailored for multi-event, time-sensitive videos is created. - State-of-the-art models on existing video question answering benchmarks struggle with this new benchmark indicating that current methods struggle with fine-grained time-sensitive video understanding. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling](https://arxiv.org/abs/2410.01440) | Jiazhong Yu, Cao Sheng, Fei Li, feifeiobama, ljh0104 |  - This paper introduces equilibrium sequence modeling, a novel method for training large language models (LLMs) to perform long-horizon robotic planning by iteratively refining plans based on environmental feedback through a self-refinement process. - The approach formulates self-refinement as a fixed-point problem, allowing for end-to-end supervised training without needing external verifiers or reward models, simplifying training compared to reinforcement learning methods. - A nested equilibrium sequence modeling procedure enables efficient closed-loop planning, leveraging feedback from the environment (or a world model) and accelerating plan refinement by reusing previously computed equilibrium solutions. - Evaluations on VirtualHome-Env benchmark demonstrate state-of-the-art performance in most metrics, especially when incorporating environmental feedback, and show advantageous scaling of performance with increased inference computation. - Ablation studies highlight the effectiveness of equilibrium sequence modeling, reuse of previous solutions, and dynamic computation allocation in improving plan quality and computational efficiency. | ['Robotics', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/Singularity0104/equilibrium-planner) | N/A |
| [Selective Aggregation for Low-Rank Adaptation in Federated Learning](https://arxiv.org/abs/2410.01463) | Huijie Fan, Liangqiong-QU, yanranw1, stevezs, gpx333 |  - This research paper introduces FedSA-LoRA, a new method for federated learning that selectively aggregates learned A and B matrices from LoRA. - It asserts that A matrices learn general knowledge while B matrices capture client-specific information, leading to only sharing A matrices for aggregation. - Experimental validation across language understanding and generation tasks on benchmarks like GLUE and GSM8K demonstrates FedSA-LoRA outperforms other methods.  - The authors extend this approach to other LoRA variants (rsLoRA and VeRA), creating FedSA-rsLoRA and FedSA-VeRA, and show consistent improvements. - The findings provide insights into LoRA in federated settings and a general framework for using future LoRA adaptations. | ['Natural Language Processing', 'Text Classification', 'Text Generation', 'Question Answering'] | N/A | N/A |
