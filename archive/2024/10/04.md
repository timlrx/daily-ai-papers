

## Papers for 2024-10-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](https://arxiv.org/abs/2410.02740) | Chen Chen, Vasileios Saveris, haotiz, Hong-You, jefflai | This paper investigates the role of large-scale image-caption data in pre-training multimodal foundation models, particularly focusing on the interplay between synthetic captions and original AltText. - It proposes a controllable and scalable captioning pipeline capable of generating diverse caption formats (short, descriptive, dense, AltText-fused). - Experiments across CLIP, multimodal LLMs, and diffusion models reveal that a hybrid approach, combining synthetic captions and AltText, often outperforms using synthetic captions alone.  - Different model types exhibit preferences for specific caption formats: shorter captions for CLIP, descriptive for multimodal LLMs and diffusion models. - Combining AltText with synthetic captions enhances performance, likely due to improved image-text alignment from synthetic captions and increased data diversity from AltText. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification'] | N/A | N/A |
| [Video Instruction Tuning With Synthetic Data](https://arxiv.org/abs/2410.02713) | Wei Li, Chunyuan24, liuziwei7, kimingng, ZhangYuanhan |  - This paper introduces LLaVA-Video, a large multimodal model for video understanding, and LLaVA-Video-178K, a synthetic dataset created for video instruction following. - LLaVA-Video-178K consists of 178,510 videos with 1.3 million instruction samples including detailed captions generated with a recurrent, multi-level approach, along with open-ended and multiple-choice question answering generated using GPT-4. - The model leverages a SlowFast video representation technique to optimize the balance between frame count and limited GPU memory, enabling processing of three times more frames than traditional methods. - LLaVA-Video achieves state-of-the-art results on various video benchmarks, outperforming existing open-source models and demonstrating the effectiveness of the proposed synthetic dataset and training approach. - The dataset, codebase, model checkpoints, and a visual chat demo are publicly released to foster development of general-purpose visual assistants. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/datasets/lmms-lab/VideoDetailCaption) |
| [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712) | Chunyuan24, henghuang, thughost, russwang, txiong23 | **-** LLaVA-Critic is the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess the performance of other multimodal models across various tasks.  **-** It leverages a new high-quality critic instruction-following dataset incorporating diverse evaluation criteria and scenarios, including pointwise scoring and pairwise ranking.  **-** The model shows strong performance as an LMM-as-a-Judge, generating evaluation scores and rankings comparable to commercial GPT models.  **-** In preference learning, LLaVA-Critic generates effective reward signals for iterative Direct Preference Optimization (DPO), surpassing rewards from human feedback as seen in LLaVA-RLHF.  **-** LLaVA-Critic is open-sourced, including its data, code, checkpoints, and demo. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [Contrastive Localized Language-Image Pre-Training](https://arxiv.org/abs/2410.02746) | Marcin Eichner, Xinze Wang, haotiz, jefflai, Hong-You | - CLOC is a new pre-training framework for vision encoders with enhanced localization capabilities. - It augments the CLIP loss with a region-text contrastive loss and a lightweight prompter module that extracts region embeddings from the image embedding given spatial hints. - A visually-enriched and spatially-localized captioning pipeline is designed to generate region-text pseudo-labels at scale, resulting in a two-billion image-text dataset with fine-grained region-text annotations. - CLOC consistently outperforms CLIP on 31 evaluation tasks, including standard image-text tasks, newly constructed region-text tasks, and downstream evaluations with MLLMs, particularly on referring and grounding tasks. - The enhanced localization capabilities of CLOC enable it to be a drop-in replacement of CLIP to enhance MLLMs. | ['Multimodal', 'Image Classification', 'Image Feature Extraction', 'Visual Question Answering', 'Zero-Shot Image Classification'] | N/A | [Link](https://huggingface.co/datasets/zzliang/GRIT) |
| [Large Language Models as Markov Chains](https://arxiv.org/abs/2410.02724) | Abdelhakim Benechehab, Oussama Zekri, ievred, NBoulle, ambroiseodt |  - This paper draws an equivalence between large language models (LLMs) and Markov chains, offering a new theoretical framework to analyze LLM inference.  - By representing LLMs with vocabulary size *T* and context window *K* as Markov chains on a state space of size O(*T*<sup>*K*</sup>), the authors derive findings on stationary distribution, convergence speed, and temperature influence.  - The paper derives generalization bounds for pre-training and in-context learning under minimal assumptions, using concentration inequalities for dependent random variables and leveraging insights from the Markov chain equivalence.  - The theoretical analysis predicts in-context scaling laws that are experimentally validated on recent LLMs (2023-2024), showing that LLMs outperform minimax optimal frequentist Markov chain learning.  - Experimental results on various Markov chains and dynamical systems further support the theoretical findings and demonstrate the practical implications of the proposed framework. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling](https://arxiv.org/abs/2409.19291) | Yu Cheng, Jihai Zhang, Spico, Xiaoye08 | -  This paper introduces Diversified Multiplet Upcycling (DMU), a novel method for enhancing the Contrastive Language-Image Pre-training (CLIP) model by integrating it with a Mixture of Experts (MoE) architecture. DMU fine-tunes multiple CLIP models from a pre-trained checkpoint using Multistage Contrastive Learning (MCL) to capture diverse feature distributions. These fine-tuned models, sharing parameters except for the Feed-Forward Network, are then used to initialize a CLIP-MoE. The approach significantly improves CLIP's performance on various zero-shot tasks, including retrieval and image classification, as well as in downstream Multimodal Large Language Model (MLLM) benchmarks when serving as a vision encoder. Notably, CLIP-MoE surpasses the base OpenAI CLIP model by approximately 20% on retrieval tasks and exhibits minimal additional training overhead, using only 2% of the computational resources required to train a CLIP from scratch. This method provides a model-agnostic and computationally efficient way to scale CLIP and enhance its ability to capture rich, fine-grained information for improved performance in various multimodal applications. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/OpenSparseLLMS/CLIP-MOE) | N/A |
| [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367) | Jun Zhu, Pengle Zhang, Jia wei, Jintao Zhang, surfingtomchen | - SageAttention, a novel post-training quantization method designed to accelerate attention in Transformer models by quantizing tensors to 8-bit integers. - It overcomes the challenges of accuracy degradation in existing methods by smoothing the K matrix to mitigate outlier effects and employing a low-precision FP16 accumulator for the PV matrix multiplication. - It integrates effective kernel fusion with ROPE and an online softmax inspired by FlashAttention. - Comprehensive experiments demonstrate a 2.1x speed improvement over FlashAttention2 and 2.7x over xFormers on an RTX 4090. - It maintains comparable end-to-end metrics across diverse applications, including language, image, and video generation models. | ['Text-to-Image', 'Text-to-Video', 'Text2Text Generation', 'Image Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?](https://arxiv.org/abs/2410.02115) | Jianye Hou, Baibei Ji, Juntao Li, Keyan Zhou, ZetangForward | • L-CiteEval, a new multi-task benchmark for evaluating long-context understanding with citations in large language models (LLMs) is introduced. • The benchmark comprises 11 diverse tasks with context lengths ranging from 8K to 48K tokens and employs automatic evaluation metrics for reproducibility. • Evaluation of 11 LLMs reveals that open-source models lag significantly behind closed-source counterparts in citation accuracy, suggesting reliance on inherent knowledge rather than provided context. • Retrieval-Augmented Generation (RAG) improves faithfulness in open-source LLMs but slightly diminishes generation quality. • A strong correlation is observed between LLMs' attention mechanisms and citation generation process, offering insight into LLM evaluation and development. | ['Question Answering', 'Summarization', 'Natural Language Processing'] | [Link](https://github.com/ZetangForward/L-CITEEVAL.git) | N/A |
| [Training Language Models on Synthetic Edit Sequences Improves Code Synthesis](https://arxiv.org/abs/2410.02749) | Rob Fergus, lerrel, upiter | - LintSeq, a synthetic data generation algorithm, refactors existing code into edit sequences to improve code synthesis in large language models (LLMs). - LLMs trained on this data produce more diverse programs, resulting in better inference-time scaling for benchmark pass rate. - Tiny (150M parameter) edit sequence LMs achieve state-of-the-art performance for their model class, matching or outperforming models twice their size. - Repeated sampling from smaller edit sequence finetuned LLMs achieves HumanEval coverage competitive with GPT-4 at similar cumulative inference cost to single samples from large open-source LLMs. - Ablating linter guidance from LintSeq degrades downstream performance. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Feature Extraction'] | [Link](https://github.com/upiterbarg/lintseq) | N/A |
| [Distilling an End-to-End Voice Assistant Without Instruction Training Data](https://arxiv.org/abs/2410.02678) | Michael Ryan, Ella Li, zyanzhe, missblanchett, WillHeld | **Summary of "Distilling an End-to-End Voice Assistant Without Instruction Training Data"**  - This paper introduces DiVA, a new speech large language model (LLM) trained through knowledge distillation from a text-based LLM, eliminating the need for explicit instruction-following data. DiVA utilizes a novel cross-modal context distillation method, which uses a frozen text-based LLM to guide the audio model's training by matching the output distribution from text transcripts of the audio. The audio input is processed using Whisper for feature extraction and a Q-Former initialized from Whisper's decoder to achieve audio-text feature alignment. - DiVA generalizes well to various spoken language tasks such as Spoken Question Answering, Classification (emotion, humor, and sarcasm detection), and Translation, using only ASR data for training. - In evaluation benchmarks, DiVA outperforms other open-access Speech and Audio LLMs on question answering by a significant margin despite using substantially less compute for training. - DiVA excels in following text-based instructions provided through prompts and user's speech, addressing the "forgetting" issue observed in other models trained using supervised fine-tuning.  - In a user study, DiVA received a 72% preference rate compared to Qwen 2 Audio, demonstrating its effectiveness in real-world scenarios despite some limitations like inheriting the base LLM's bias. | ['Multimodal', 'Audio', 'Automatic Speech Recognition', 'Question Answering', 'Translation'] | N/A | N/A |
| [Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos](https://arxiv.org/abs/2410.02763) | Jianrui Zhang, yjlee0222, mucai |  - This paper introduces Vinoground, a novel temporal counterfactual benchmark for evaluating Large Multimodal Models (LMMs) on dense temporal reasoning in short videos. - Vinoground contains 1000 short video and caption pairs with captions containing the same words but in different orders to create temporal counterfactuals. - The benchmark evaluates an LMM’s ability to distinguish temporal differences between actions and object transformations (e.g., "water turning into ice” vs. “ice turning into water”). - Experimental results show that even state-of-the-art LMMs struggle with temporal reasoning, with the best model (GPT-40) achieving only 54% accuracy on text score and much worse on other metrics, while human performance is around 90%. - All open-source models and CLIP-based models perform much worse, suggesting that existing methods struggle at fully understanding video temporality. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://vinoground.github.io) | N/A |
| [Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data](https://arxiv.org/abs/2410.02056) | manocha, ctnzr, rafaelvalle, ZhifengKong, SreyanG-NVIDIA | Synthio is a novel approach to augment small-scale audio classification datasets using synthetic data generated from text-to-audio (T2A) diffusion models, aligning the generated data with the target dataset's acoustic characteristics through preference optimization. - It addresses the challenge of creating diverse synthetic augmentations by introducing MixCap, a technique that leverages Large Language Models (LLMs) to generate and refine meaningful audio captions used for prompting the T2A model. - Synthio's evaluation across ten datasets and four limited-data settings demonstrates consistent outperformance of existing baselines, improving classification accuracy by 0.1% to 39% using a T2A model trained solely on weakly-captioned AudioSet. - Ablation studies show the vital role of preference optimization and MixCap in achieving optimal results. - Additional analysis demonstrates effectiveness of Synthio in enhancing captioning tasks and addressing long-tail categories. | ['Audio', 'Audio Classification', 'Text-to-Audio'] | [Link](https://github.com/Sreyan88/Synthio) | N/A |
