

## Papers for 2024-10-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Addition is All You Need for Energy-efficient Language Models](https://arxiv.org/abs/2410.00907) | Wei Sun, luohy | - The paper proposes a novel linear-complexity multiplication (L-Mul) algorithm to approximate floating-point multiplication with integer addition, aiming to reduce energy consumption in large language models (LLMs). - L-Mul replaces expensive floating-point multiplications with less energy-intensive integer additions and introduces an offset to maintain accuracy. - The authors claim L-Mul achieves higher precision and requires less computation compared to 8-bit floating-point multiplications and 80% energy reduction for dot products. - Experiments on various LLMs and tasks (MMLU, BBH, GSM8k, visual question answering) showed that L-Mul in attention layers maintained or even slightly improved performance compared to standard multiplication and outperformed float8 with training free setting. - Fine-tuning models with all multiplications replaced by 3-bit L-Mul achieved comparable results to models using float8_e4m3 accumulation, showcasing its potential for efficient LLM training and deployment. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [NL-Eye: Abductive NLI for Images](https://arxiv.org/abs/2410.02613) | Zorik Gekhman, yonatanbitton, nitay, tokeron, MorVentura |  - NL-EYE, a benchmark designed to evaluate the visual abductive reasoning skills of Visual Language Models (VLMs), is introduced. - NL-EYE tasks models with evaluating the plausibility of hypothesis images given a premise image, requiring explanations for their choices and consisting of 350 image triplets across six reasoning categories: physical, functional, logical, emotional, cultural, and social. - Results show that while humans perform well, VLMs struggle, often failing to surpass random baselines in plausibility prediction. - Even with correct predictions, VLM explanations are frequently unhelpful, indicating weaknesses in visual interpretation and accurate representation generation for reasoning. - Further analysis suggests that VLMs face challenges with temporal reasoning, absolute judgments, and non-correlational tasks, particularly emotional reasoning. | ['Multimodal', 'Computer Vision'] | N/A | N/A |
| [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703) | Yossi Matias, Matan Kalman, yanivle | -"Selective Attention" is introduced; a parameter-free adjustment to the standard attention mechanism in Transformers, enabling a token to deem another as no longer relevant for future tokens and masking it, improving language modelling performance across various model sizes and context lengths. -It allows for reduction in the attention context buffer size without quality loss, resulting in significant memory and compute savings during inference, achieving up to 16X, 25X, and 47X memory reduction for context sizes of 512, 1024, and 2048 respectively with a 100M parameter model trained on C4. -Selective attention transformers often outperform standard transformers with ~2X more parameters and heads in their attention module. -Visualizations show selective attention exhibiting dynamic context pruning behavior; masking previous assignments to the same variable in variable assignment, masking ambiguous inputs until ambiguity resolution, and retaining only necessary elements in tasks like Parity and Copy. -Evaluation on C4 dataset shows consistent perplexity improvements across different model sizes and context lengths; further improvements via explicit loss to encourage masking, and HellaSwag benchmark reveals consistent accuracy gains across various model sizes using selective attention. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise](https://arxiv.org/abs/2410.03017) | Susanna Loeb, ddemszky, carlycodes, Analu, rose-e-wang |  - This paper introduces Tutor CoPilot, a Human-AI system designed to enhance real-time tutoring in K-12 education by providing expert-like guidance to tutors as they interact with students.  - Tutor CoPilot leverages the Bridge method, which captures expert decision-making patterns and adapts Large Language Models (LLMs) to generate contextually relevant suggestions for tutors during live sessions.  - A randomized controlled trial involving 900 tutors and 1,800 K-12 students demonstrates that Tutor CoPilot significantly improves student learning outcomes, particularly for students with lower-rated tutors.  - Analysis of over 550,000 chat messages reveals that tutors using Tutor CoPilot are more likely to employ high-quality pedagogical strategies that foster student understanding and less likely to simply provide answers.  - Tutor CoPilot offers a scalable and cost-effective solution ( $20 per tutor annually) for enhancing tutoring quality, especially in under-served communities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760) | David Bau, Samuel Marks, sfeucht, RohitGandikota | - This research introduces Erasure of Language Memory (ELM), a novel method for removing specific concepts from large language models (LLMs) while preserving fluency and general knowledge. - ELM employs a multi-objective fine-tuning approach with targeted low-rank updates (LoRA). - The method optimizes for erasure of the target concept, retention of unrelated information, and generation fluency when prompted with the erased concept. - Experiments on biosecurity, cybersecurity, and literary domains demonstrate ELMâ€™s efficacy in achieving near-random performance on erased topics while maintaining high scores on general knowledge benchmarks and generating more fluent text than baseline methods. - ELM also exhibits robustness against adversarial attacks, further highlighting its potential for safe and controlled LLM editing. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/rohitgandikota/erasing-llm) | [Link](https://huggingface.co/cais/Zephyr_RMU) |
| [CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction](https://arxiv.org/abs/2410.01273) | wpiioos, Unmanned-YuBeen, lastdefiance20, PurpleSand, MilkClouds |  - CANVAS, a novel framework for intuitive human-robot interaction, is introduced for commonsense-aware navigation. It combines visual and linguistic instructions to generate robot actions, leveraging pre-trained vision-language models (VLMs) to achieve this. - A new dataset called COMMAND, containing 48 hours of driving data over 219 kilometers with human-annotated instructions and navigation outcomes across office, street and orchard simulated environments, was collected to train and test the model. - Experimental results show that CANVAS consistently outperforms the rule-based ROS NavStack in all environments, especially in challenging scenarios like uneven terrain or misleading instructions, with higher success and lower collision rates. - CANVAS achieves successful Sim2Real transfer with a 69% success rate in a real-world office setting, demonstrating its robustness beyond simulated data. - Ablation study confirms that using pre-trained VLM weights improves performance considerably, indicating the usefulness of existing knowledge for navigation tasks. | ['Robotics', 'Multimodal'] | N/A | N/A |
