

## Papers for 2024-10-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LongGenBench: Long-context Generation Benchmark](https://arxiv.org/abs/2410.04199) | Peijie Dong, wenxinsiju, xuminghui, Dominic789654 | - LongGenBench, a synthetic benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs), focusing on consistency and logical flow. - It redesigns question formats, requiring LLMs to provide single, cohesive long-context answers encompassing multiple questions within a single query. - Evaluation on LongGenBench reveals performance degradation across both API-accessed and open-source LLMs in long-context scenarios, ranging from 1.2% to 47.1%. - Different LLM series show varying degradation trends, with Gemini-1.5-FLASH exhibiting minimal degradation among API-accessed models, and QWEN2 series showing minimal degradation among open-source models. - Model size influences performance decline, with larger models within a series generally demonstrating less degradation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization](https://arxiv.org/abs/2410.04717) | Francois Charton, Justin Wang, shizhuo2 | - This paper investigates the impact of instruction diversity on the generalization ability of Large Language Models (LLMs), focusing solely on instruction-following capabilities and isolating them from reasoning and knowledge retrieval. - Through controlled string rewriting experiments inspired by the Turing-complete Markov algorithm and mathematical deduction tasks, the study demonstrates that generalization to unseen instructions emerges only when training data is sufficiently diverse across semantic domains. - Findings reveal that diversifying data within limited domains does not guarantee robust generalization, while cross-domain diversification significantly enhances adaptability to new instructions. - The research further shows that increasing the diversity of training data can lead to performance improvements in real-world scenarios, including code generation and reasoning tasks with both specialized and generalist models.  - The results underscore the importance of strategic data diversification over simply increasing data size, offering guidelines for improving instruction-tuning datasets and enhancing model performance across various domains. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://arxiv.org/abs/2410.05193) | lifengshang, YuxinJiang, Tiezheng, yufeiwang201217a, DonJoey | • REVISEVAL, a novel evaluation paradigm, leverages the revision capabilities of Large Language Models (LLMs) to generate response-adapted references for evaluating text generation quality.  • It revises the generated response based on the given instruction and evaluation rubric, then uses the revised text as a reference for subsequent evaluation by either LLM-as-a-Judge or classic text evaluation metrics. • REVISEVAL outperforms reference-free and reference-based evaluation methods across various NLG and instruction-following tasks using both open-source and proprietary LLMs.  • Response-adapted references enhance the performance of classic metrics, sometimes even rivaling LLM-as-a-Judge.  • REVISEVAL effectively reduces bias in evaluation, such as verbosity and positional biases, and its effectiveness is linked to the relevance of the generated references. | ['Natural Language Processing'] | N/A | N/A |
| [MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions](https://arxiv.org/abs/2410.02743) | Yu Sun, Shuohuan Wang, Huang Fang, Haoran Sun, Yekun Chai |  - MA-RLHF, a new Reinforcement Learning from Human Feedback (RLHF) framework, is introduced to improve large language model alignment with human preferences.  It leverages "macro actions" which are sequences of tokens or higher-level language constructs.  - This approach reduces the temporal distance between actions and rewards, addressing the credit assignment problem in token-level RLHF, and facilitates faster and more accurate credit assignment.  - The model achieves substantial performance improvements across various tasks, including up to a 30% gain in summarization, an 18% gain in dialogue, and an 8% gain in question answering, while demonstrating a 1.7x-2x faster convergence compared to standard RLHF.  - MA-RLHF's robustness is highlighted through experiments conducted with different model sizes (2B to 27B) on various tasks, such as text summarization with the TL;DR dataset and dialogue generation with the HH-RLHF dataset.  - Further analysis explores termination strategies for macro actions, demonstrating the effectiveness of n-gram and parsing-based approaches in improving model performance. | ['Reinforcement Learning', 'Natural Language Processing', 'Summarization', 'Text2Text Generation', 'Question Answering', 'Text Generation'] | [Link](https://github.com/ernie-research/MA-RLHF) | [Link](https://huggingface.co/datasets/Dahoas/full-hh-rlhf) |
| [Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models](https://arxiv.org/abs/2410.03290) | Yufan Zhou, Shizhe Diao, Yu Cheng, Zhiyang Xu, WHB139426 | **-** This paper introduces Grounded-VideoLLM, a novel Video Large Language Model (Video-LLM) designed for fine-grained temporal grounding in videos.  **-** Grounded-VideoLLM uses a two-stream architecture, encoding spatial information from keyframes and temporal dynamics from multiple frames using a video encoder, to create a temporally-aware video representation. **-**  It introduces discrete temporal tokens into the LLM's vocabulary for representing timestamps efficiently, avoiding tokenization of numerical text and integrating time representations directly into the LLM.   **-** A multi-stage training approach is employed, progressing from video-caption alignment to temporal token alignment and finally multi-task instruction tuning on datasets incorporating temporal grounding tasks. **-** Experimental results demonstrate that Grounded-VideoLLM achieves state-of-the-art performance on various fine-grained temporal grounding tasks including Temporal Sentence Grounding, Dense Video Captioning and Grounded VideoQA, as well as general video understanding benchmarks. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/WHB139426/Grounded-Video-LLM) | N/A |
| [Hyper-multi-step: The Truth Behind Difficult Long-context Tasks](https://arxiv.org/abs/2410.04422) | yuyijiong |  - This paper investigates the underlying reasons why Long Context Language Models (LCLMs) struggle with complex tasks, despite their ability to handle extensive text.  - Through experiments with synthetic datasets, the study identifies "multi-matching retrieval" (retrieving multiple items simultaneously) and "logic-based retrieval" (using logic within retrieval criteria) as the core challenges, and further defines them as "hyper-multi-step" problems. - "Hyper-multi-step" implies that these seemingly simple tasks actually comprise a large number of indivisible sub-steps, which increases with context length and exceeds the processing capacity of current LCLMs.  - The paper provides empirical evidence through linear probing of hidden states and analysis of attention weights, demonstrating that these problems are more akin to complex arithmetic tasks, rather than traditional retrieval, and are therefore not adequately addressed by existing techniques such as Retrieval-Augmented Generation (RAG) or Chain-of-Thought (CoT) prompting.  - The study concludes that simply increasing the context window size of LCLMs may not suffice; instead, future research should focus on addressing the numerous steps involved and explore alternative solutions, such as using external tools. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
