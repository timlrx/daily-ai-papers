

## Papers for 2024-10-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/abs/2410.05254) | Roi Reichart, Samuel Joseph Amouyal, Omer Madmon, ireinman, EilamSha |   - This paper introduces GLEE, a unified framework and benchmark for evaluating Large Language Models (LLMs) in language-based economic games like bargaining, negotiation, and persuasion.   - It parameterizes the space of these games, defines consistent evaluation metrics (self-gain, efficiency, and fairness), and provides an open-source framework for interaction simulation.   - A dataset of 7.15M LLM decisions across various game configurations and an additional human vs. LLM dataset are collected using four different LLMs.   - The framework facilitates controlled experiments across numerous game configurations and LLMs, enabling robust evaluation.   - Demonstrates the framework's utility in evaluating and comparing LLMs to human players and in quantifying the impact of economic environment parameters. | ['Natural Language Processing'] | [Link](https://github.com/eilamshapira/GLEE) | N/A |
| [Personalized Visual Instruction Tuning](https://arxiv.org/abs/2410.07113) | Jipeng Zhang, Tianyang Han, research4pan, Sterzhang, renjiepi |  PVIT (Personalized Visual Instruction Tuning) is a new training paradigm designed to enable Multimodal Large Language Models (MLLMs) to engage in personalized conversations by identifying target individuals within an image. - The framework leverages in-context learning, utilizing a multimodal prefix of <personal image, personal introduction> and personalized wrapper tokens to eliminate ambiguity. - PVIT involves an automatic framework to create training data in three stages: visual concept curation, dual-level textual information extraction and fusion, and dataset generation using LLM reasoning. - A benchmark named P-Bench, with various question types, is introduced to evaluate the personalized capabilities of MLLMs.  - Experimental results on P-Bench demonstrate that current MLLMs have limited ability for personalized conversations. P-LLaVA trained with PVIT significantly improves performance on both answerable and unanswerable question types across all input complexities, achieving an average accuracy of 96.69% for answerable questions and 99.72% for unanswerable questions on the multiple choice questions in P-Bench. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/sterzhang/PVIT) | [Link](https://huggingface.co/datasets/Sterzhang/PVIT-3M) |
| [Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate](https://arxiv.org/abs/2410.07167) | Pan Zhang, Xiaoyi Dong, lindahua, yuhangzang, shikiw | - This paper introduces the Modality Integration Rate (MIR), a new metric for evaluating the cross-modal alignment quality during the pre-training phase of Large Vision-Language Models (LVLMs). - MIR quantifies the domain divergence between vision and language features across all layers of the LLM, thus, correlates strongly with the model's post-SFT multi-modal performance and exhibits convergence behavior during pre-training, offering insights for training optimization. - Furthermore, it is robust to variations in input type and training/evaluation datasets, and generalizes across different pre-training recipes, strategies, and module designs. - A lightweight and learnable calibration module called MoCa is proposed, improving alignment between visual and textual tokens and leading to performance gains when integrated into both pre-training and SFT stages. - Experiments show that MoCa yields a 1.5% average performance increase for LLaVA-1.5 and a 0.9% increase for Mini-Gemini. | ['Multimodal'] | [Link](https://github.com/shikiw/Modality-Integration-Rate) | N/A |
| [Pixtral 12B](https://arxiv.org/abs/2410.07073) | saurabhgarg, devendrachaplot, EmmaBH, Simontwice, pragra |  - Pixtral 12B is a 12-billion parameter multimodal language model trained to understand both images and text. - It utilizes a novel vision encoder trained from scratch, allowing it to process images at native resolution, and a multimodal decoder based on Mistral Nemo 12B. - Pixtral 12B outperforms open models of similar size on multimodal benchmarks, such as Llama 3.2 11B and Qwen-2-VL 7B and even surpasses larger models like Llama 3.2 90B on certain tasks.  - It also achieves strong performance on text-only tasks, demonstrating its capability as a general purpose language model.  - The authors introduce MM-MT-Bench, an open-source benchmark to evaluate vision-language models in practical multi-turn scenarios. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/mistralai/mistral-inference), [Link](https://github.com/mistralai/mistral-evals) | [Link](https://huggingface.co/datasets/mistralai/MM-MT-Bench) |
| [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/abs/2410.05993) | JunnanLi, guoyinwang, sirius-ctrl, teowu, dxli1 | **Summary of Aria: An Open Multimodal Native Mixture-of-Experts Model:** - ARIA is an open-source, multimodal native, mixture-of-experts (MoE) model with 24.9B parameters, trained from scratch and designed for comprehensive understanding across diverse modalities. - With a visual encoder lightweight of only 438M parameters, ARIA's MoE decoder has 3.9B and 3.5B activated parameters per visual and text token, respectively, enabling efficient parameter utilization and leading to faster training and inference. It outperforms Pixtral-12B and Llama3.2-11B and is competitive with top proprietary models on various multimodal tasks. - Trained in a 4-stage pipeline, the model progressively develops capabilities in language understanding, multimodal understanding, long context (64k tokens), and instruction following. This pipeline design ensures that each stage enhances the model's capabilities while preserving the already acquired skills from the previous stages. - ARIA's training data includes 6.4T language tokens and 400B multimodal tokens, with a rigorous curation process employing a combination of rule-based and model-based filtering to maintain data quality. - Qualitative results showcases ARIA is able to integrate information across multiple modalities in complex reasoning tasks involving chart, table, text, and images understanding and show advanced coding, debugging, math, paper reading, video understanding abilities. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Text2Text Generation', 'Video-Text-to-Text', 'Question Answering'] | N/A | N/A |
| [MM-Ego: Towards Building Egocentric Multimodal LLMs](https://arxiv.org/abs/2410.07177) | HaoxuanYou, FrozzZen, edaxberger, haotiz, leoye | **Key Points:** - Introduces MM-Ego, a multimodal large language model (MLLM) designed for egocentric video understanding, featuring a novel "Memory Pointer Prompting" mechanism. This mechanism incorporates a global glimpse step, which extracts compressed visual embeddings from the entire video to gain an overarching understanding, and a fallback step, which uses higher-resolution key visual embeddings identified in the global glimpse stage to respond to questions. - Creates a 7M egocentric QA dataset, generated automatically from human-annotated video narrations from the Ego4D dataset, that ranges from 30 seconds to one hour, representing the largest egocentric QA dataset currently available. - Introduces EgoMemoria, a benchmark to evaluate egocentric video understanding capabilities with 7,026 multiple-choice questions across 629 videos ranging from 30 seconds to one hour in length, alongside a debiased metric to mitigate language bias. - In experiments, MM-Ego outperforms prior state-of-the-art models on the EgoMemoria benchmark and demonstrates competitive results on general video benchmarks like EgoSchema and Video-MME. - The Memory Pointer Prompting and data augmentation strategies show improvements even after the removal of language-biased questions, demonstrating their efficacy for the targeted task. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation](https://arxiv.org/abs/2410.07170) | Marc Peter Deisenroth, Benedikt Alkin, thomasschmied, sirluk, paischer101 |  - This paper introduces Explained Variance Adaptation (EVA), a novel data-driven initialization method for Low-Rank Adaptation (LoRA) used in fine-tuning large foundation models. - EVA computes the Singular Value Decomposition (SVD) on mini-batches of activation vectors derived from downstream data to initialize LoRA weights, maximizing explained variance and enabling adaptive rank allocation across model layers. - Experiments conducted on diverse tasks, including language generation, understanding, image classification, and reinforcement learning, demonstrate EVA's superior performance to existing initialization and rank adaptation techniques. - EVA achieves faster convergence than competitor models across multiple tasks, such as achieving higher average scores on commonsense reasoning with LLMs and even exceeding full fine-tuning performance when combined with DORA on reinforcement learning tasks. - Ablation studies confirm that both the directional components and scale obtained from SVD contribute to EVA's enhanced performance. | ['Natural Language Processing', 'Text Generation', 'Image Classification', 'Reinforcement Learning', 'Robotics'] | [Link](https://github.com/ml-jku/EVA), [Link](https://github.com/BenediktAlkin/vtab1k-pytorch), [Link](https://github.com/sirluk/peft/blob/main/examples/eva_finetuning/eva_finetuning.py) | N/A |
| [Self-Boosting Large Language Models with Synthetic Preference Data](https://arxiv.org/abs/2410.06961) | Zhifang Sui, Li Dong, thegenerality, THU-CHUNXIA, Rsy24 | SynPO, a novel self-boosting paradigm, leverages synthetic preference data for Large Language Model (LLM) alignment, eliminating the need for extensive human preference data. It employs an iterative mechanism where a self-prompt generator creates diverse prompts, and a response improver refines model responses.  After four SynPO iterations, LLMs like Llama2-8B and Mistral-7B demonstrated significant improvements, achieving over 22.1% win rate improvements on benchmarks like AlpacaEval 2.0 and ArenaHard. Moreover, SynPO boosts the general LLM performance, as evidenced by a 3.2 to 5.0 average score increase on the Open LLM leaderboard.  SynPO's self-boosting mechanism dynamically guides LLMs to refine their own outputs, effectively integrating generative rewards for preference learning. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Falcon Mamba: The First Competitive Attention-free 7B Language Model](https://arxiv.org/abs/2410.05355) | Ilyas Chahed, Dhia Eddine Rhaiem, ybelkada, yellowvm, JingweiZuo | - Falcon Mamba 7B is a new large language model based on the Mamba architecture, making it attention-free, trained on 5.8 trillion tokens. - It outperforms other open-source 7B models like Mistral 7B and Llama 3.1 8B, as well as larger models such as Falcon2 11B in benchmarks like the Open LLM Leaderboard. - Falcon Mamba 7B has faster inference speeds and lower memory usage, especially beneficial for long sequence generation due to the Mamba architecture's linear memory scaling. - The model uses an AdamW optimizer with a warmup-stable-decay learning rate schedule and is trained on a dataset mixture of web data, curated content, code, and math data. - Falcon Mamba 7B is available with a permissive license on Hugging Face, supporting functionalities such as inference, quantization, and fine-tuning. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/tiiuae/falcon-mamba-7b), [Link](https://huggingface.co/tiiuae/falcon-mamba-7b-pre-decay) |
| [Temporal Reasoning Transfer from Text to Video](https://arxiv.org/abs/2410.06166) | Chancy, PY007, yaolily, lyx97, tobiaslee | - T3 (Textual Temporal reasoning Transfer) is introduced, a method that enhances Video Large Language Models' (Video LLMs) temporal reasoning by transferring knowledge from the text domain.  - T3 creates diverse temporal reasoning tasks in text format from existing image-text datasets, addressing the lack of video samples with complex temporal scenarios.  - Without using any video data, T3 improves LongVA-7B's performance significantly, achieving a 5.3 absolute accuracy gain on TempCompass, exceeding ShareGPT4Video-8B (trained on 28,000 video samples). - The enhanced LongVA-7B achieves competitive performance on video benchmarks, e.g. 49.7 accuracy on Video-MME's Temporal Reasoning task, outperforming InternVL-Chat-V1.5-20B and VILA1.5-40B.  -  Analysis reveals a strong correlation between textual and video temporal task performance (e.g., Pearson r=0.89 on TempCompass), validating the efficacy of T3. | ['Multimodal', 'Video-Text-to-Text', 'Video Classification'] | N/A | N/A |
| [TRACE: Temporal Grounding Video LLM via Causal Event Modeling](https://arxiv.org/abs/2410.05643) | Xiaoying Tang, Mingda Li, Jingyu Liu, qingbinliu, Yongxin-Guo |  - TRACE, a novel task-interleaved video Large Language Model (LLM), is introduced for Video Temporal Grounding (VTG). It addresses the limitations of current video LLMs that rely solely on natural language generation, which lack the clear structure and information presented in videos.  - TRACE models videos as sequences of events, each with timestamps, salient scores, and captions, and leverages causal event modeling framework to represent the inherent structure of videos. - The TRACE architecture uses an interleaved sequence of task tokens for visual frames, timestamps, salient scores, and text, and employs separate encoders and decoding heads for each task. - The model also incorporates an adaptive head-switching mechanism for improved generation and achieves superior performance on various VTG tasks and datasets, outperforming current video LLMs. - TRACE improves zero-shot performance by 3.1% and 4.9% on Youcook2 (CIDEr and F1 Score), by 6.5% and 3.7% on Charades-STA (Recall with IOU=0.5 and IOU=0.7 respectively), and by 10.3% and 9.2% on QVHighlights (mAP and HIT@1). | ['Video-Text-to-Text', 'Multimodal', 'Question Answering'] | [Link](https://github.com/gyxxyg/TRACE) | N/A |
| [Data Selection via Optimal Control for Language Models](https://arxiv.org/abs/2410.07064) | Li Dong, thegenerality, Rsy24, howang, t1101675 | This paper introduces PMP-based Data Selection (PDS), a framework for selecting high-quality pre-training data for language models (LMs). PDS formulates data selection as an Optimal Control problem and leverages Pontryagin's Maximum Principle (PMP) to derive necessary conditions for optimal data selection. Experiments show that PDS accelerates LM pre-training by 2x and improves performance across various model sizes and downstream tasks, even extrapolating to 400B models trained on 15T tokens. PDS also enhances data utilization in data-constrained settings, reducing pre-training data demand by 1.8 times. This method offers a principled, theory-driven approach to data selection compared to existing heuristics, leading to more efficient and effective LM training. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/microsoft/LMOps/tree/main/data_selection) | N/A |
| [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/abs/2410.07002) | Shijin Wang, Rui Li, Qi Liu, Eviloder, TechxGenus |  - This paper introduces CursorCore, a new framework for AI-assisted programming that integrates various information sources such as coding history, current code, and user instructions for enhanced automation.  - It also presents a new benchmark called APEval (Assist Programming Eval) to evaluate models on this task and a data generation pipeline, Programming-Instruct, to create synthetic training data from diverse sources.  - This pipeline generated 219K samples to fine-tune the CursorCore models.  - The CursorCore models reportedly outperforms other models of comparable size on the APEval benchmark.  - This framework unifies applications like inline chat and automated editing, contributing to the advancement of coding assistants. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/TechxGenus/CursorCore) | N/A |
| [Response Tuning: Aligning Large Language Models without Instruction](https://arxiv.org/abs/2410.02465) | Hyounghun Kim, seokhyun |  - Response Tuning (RT) is proposed, a novel fine-tuning method that omits the instruction-conditioning step of instruction tuning, instead focusing exclusively on the supervision of response space. - RT models, trained solely on responses, exhibit helpfulness and open-ended instruction following capabilities comparable to instruction-tuned models, demonstrating the potential of response space supervision in alignment. - Refining the structural attributes of training responses leads to significant improvements in user preference for RT models, while incorporating contextual refusals into the training data allows RT models to implicitly evaluate and reject unsafe queries.  - These findings emphasize the importance of controlling response distribution in safety alignment and suggest that large language models inherently acquire many capabilities during pre-training. - In-context learning with response demonstrations only yields effective instruction-following and refusal behaviors, further strengthening the argument for the power of response supervision and highlighting the inherent potential of pretrained large language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/seokhyunan/response-tuning) | N/A |
| [ING-VP: MLLMs cannot Play Easy Vision-based Games Yet](https://arxiv.org/abs/2410.06555) | Haoran Zhang, zhangysk, CheeryLJH, EZ-hwh, Rosiness |  - This research introduces ING-VP, a novel interactive game-based vision planning benchmark designed to evaluate the spatial imagination and multi-step reasoning capabilities of Multimodal Large Language Models (MLLMs). - ING-VP comprises six distinct games with varying complexity, offering 300 levels and six unique configurations per level, leading to over 60,000 interaction rounds for a single model. - The benchmark incorporates image-text and text-only input modalities, single and multi-step reasoning settings, and conditions with and without interaction history, facilitating a comprehensive evaluation of MLLM performance. - Initial evaluations using ING-VP demonstrate that current state-of-the-art MLLMs struggle with these seemingly simple game tasks. The highest performing model, Claude-3.5 Sonnet, only achieves an average accuracy of 3.37%, significantly below human performance. - This underscores the need for further research and development to enhance MLLMs' capacity for complex spatial reasoning and planning, a crucial aspect of achieving robust artificial general intelligence. | ['Multimodal'] | [Link](https://github.com/Thisisus7/ING-VP.git) | N/A |
| [Mixed-Session Conversation with Egocentric Memory](https://arxiv.org/abs/2410.02503) | Taeyoung Kim, khh3323, jihyoung | • The paper introduces Mixed-Session Conversation, a new dialogue paradigm where a main speaker interacts with different partners across multiple sessions, promoting deeper layered interactions and complex dynamics.  • MISC, a new dataset comprising 8.5K episodes with 6 sessions and 4 speakers per episode is presented, implementing Mixed-Session Conversation and managing memories across sessions and partners from the main speaker's perspective.   • EMMA (Egocentric Memory Enhanced Mixed-session Conversation Agent), a novel dialogue model trained on MISC, facilitates seamless conversation continuity using Egocentric Memory, and allows retention of all conversational contexts across sessions and partners.   • Human evaluations validate that dialogues in MISC demonstrate seamless conversational flow even with changing partners, with EMMA exhibiting high humanness, engagingness, and memorability.  • EMMA's use of Egocentric memory retains high memorability without contradiction by connecting instances within and across sessions and tagging memory to each utterance. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://mixed-session.github.io/) |
| [FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance](https://arxiv.org/abs/2410.05791) | C. Karen Liu, Elizabeth Schumann, Haochen Shi, Pei Xu, rcwang | -   This paper introduces FürElise, a large-scale dataset of 3D hand motions and audio from 15 pianists playing 153 classical music pieces, captured using a markerless multi-view video setup and refined with MIDI data from a Disklavier piano.   - A new model is proposed to synthesize physically plausible piano playing motions from sheet music, combining a diffusion model for initial motion generation, a music-based motion retrieval method for enhancing accuracy, and reinforcement learning for physics-based bimanual control.  - The diffusion model, trained on FürElise, generates kinematic hand trajectories conditioned on sheet music, providing high-level guidance and fingering information.  - Motion retrieval augments the diffusion model's output by retrieving similar motions from FürElise based on musical similarity, improving the precision of key presses.  - The reinforcement learning policy learns to control simulated hands interacting with a piano keyboard, optimizing a combination of imitation and goal-based rewards to achieve realistic and musically accurate performance. | ['Computer Vision', 'Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs](https://arxiv.org/abs/2410.05295) | Edward Suh, huansun, someshjha, peiranli0930, ShletonLiu-N | - AutoDAN-Turbo, a novel black-box jailbreak method for Large Language Models (LLMs), automatically discovers and combines diverse jailbreak strategies using a lifelong learning approach. - This method leverages three core modules: an Attack Generation and Exploration Module, a Strategy Library Construction Module, and a Jailbreak Strategy Retrieval Module, allowing for continuous strategy discovery, evolution, and integration of human-designed strategies. - Evaluation on Harmbench and StrongREJECT benchmarks shows that AutoDAN-Turbo significantly outperforms existing methods, achieving a 74.3% higher average attack success rate and a 92.3% higher StrongREJECT score than the runner-up. - Notably, it demonstrates exceptional effectiveness on GPT-4-1106-turbo, reaching an 88.5% attack success rate, which further increases to 93.4% with the integration of human-designed strategies. - The learned strategy library exhibits strong transferability across different target models and datasets, demonstrating its robustness and adaptability in various attack scenarios. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SaFoLab-WISC/AutoDAN-Turbo) | N/A |
| [Multimodal Situational Safety](https://arxiv.org/abs/2410.06172) | xw-eric, dawnsong, acompalas, Xuandong, LCZZZZ |  - This paper introduces the novel problem of Multimodal Situational Safety, which focuses on evaluating a multimodal model's ability to consider safety aspects based on visual context. - A new benchmark called MSSBench is created to evaluate the situational safety performance of current Multimodal Large Language Models (MLLMs). - The benchmark comprises 1820 language query-image pairs across two scenarios: chat and embodied assistants, where half the images depict safe situations and the other half unsafe. - An evaluation framework analyzes key safety aspects, including explicit safety reasoning, visual understanding, and situational safety reasoning. - Results show current MLLMs struggle with recognizing unsafe situations, especially open-source models which frequently ignore safety clues.  | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](mssbench.github.io) | N/A |
| [T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design](https://arxiv.org/abs/2410.05677) | wangwilliamyang, wenhu, rpiramuthu, xfgao, jiachenli-ucsb | • T2V-Turbo-v2, a novel text-to-video (T2V) generation model, enhances post-training through incorporating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance into the consistency distillation process.  • It eliminates the target network from T2V-Turbo for improved memory efficiency and enables full model training, rather than just LORA. • It leverages motion guidance from training videos to formulate an energy function that augments the ODE solver, improving motion quality.  • Evaluated on VBench, T2V-Turbo-v2 achieves state-of-the-art performance with a Total Score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.  • Ablation studies confirm the benefits of curating specialized datasets, utilizing diverse reward models and employing motion guidance. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | [Link](https://huggingface.co/spaces/Vchitect/VBench_Leaderboard) |
| [Collective Critics for Creative Story Generation](https://arxiv.org/abs/2410.02428) | Hyounghun Kim, minwook | CRITICS is a novel framework for long-form story generation that integrates a collaborative critique mechanism to enhance story creativity and expressiveness. - It consists of two stages: CRPLAN for refining story plans and CRTEXT for enhancing story expressiveness. - Multiple LLM critics and a leader collaborate to refine story plans and enhance story texts based on criteria for creativity. - Human evaluation shows that CRITICS significantly improves story creativity and reader engagement while maintaining coherence. - It supports interactive writing, where humans can participate as any player within the framework. | ['Text Generation'] | [Link](https://github.com/EMNLP-2024-CritiCS/Collective-Critics-for-Creative-Story-Generation) | N/A |
| [Diversity-Rewarded CFG Distillation](https://arxiv.org/abs/2410.06084) | alexrame, Sper42, bachem, ferretj, aagostinelli86 |  - This paper introduces diversity-rewarded CFG distillation, a novel finetuning strategy to enhance the quality-diversity trade-off in generative models, specifically for text-to-music generation. - It combines distillation and reinforcement learning (RL) to optimize two complementary objectives; a novel CFG distillation objective and an RL with diversity reward objective. - By interpolating between the weights of two models(quality-focused and diversity-focused model), the strategy controls the quality-diversity trade-off at deployment time, further boosting performance. - Experiments on MusicLM using human evaluation validate that the model generates more diverse music samples while maintaining high quality. - The finetuned-then-merged model outperforms CFG augmentation in terms of Pareto-optimal quality and diversity, generating high-quality samples with improved diversity. | ['Text-to-Audio', 'Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [TinyEmo: Scaling down Emotional Reasoning via Metric Projection](https://arxiv.org/abs/2410.07062) | ggcristian |  - TinyEmo, a family of small Multimodal Large Language Models (MM-LLMs), is introduced for enhanced emotional reasoning and classification, integrating a synthetic emotional instruction dataset, a Metric Projector for classification, and a conditional reasoning approach. - The architecture includes a vision encoder (CLIP ViT-L/14), two projectors for classification and reasoning respectively and different LLM backbones (OpenELM, TinyLlama, Phi-2) ranging from 0.7B to 3.21B parameters. The Metric Projector is trained separately with metric learning, detaching classification from the LLM to improve efficiency and performance. - TinyEmo-700M outperforms larger state-of-the-art models like EmoVIT (7.91B parameters) with only 700M parameters on emotion classification and achieves a Zero-Shot accuracy of 57.62% when trained with data augmentation, outperforming EmoVIT's 55.57%. - A Conditional Reasoning approach, where the predicted emotion label from the Metric Projector is inserted into the prompt, leads to more accurate reasoning compared to the standard approach. - A semi-automated framework is proposed which uses the Metric Projector for interpretability and bias detection by analyzing neuron activations and embedding space robustness, showing the potential for mitigating bias and improving understanding of model behavior. | ['Multimodal', 'Image Classification', 'Visual Question Answering', 'Text Generation', 'Zero-Shot Classification', 'Zero-Shot Image Classification'] | [Link](https://github.com/ggcr/TinyEmo) | N/A |
| [F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](https://arxiv.org/abs/2410.06885) | Zhikang Niu, kaiyu-hf, ChunHuiWangFN, D-Keqi, SWivid | • F5-TTS is a fully non-autoregressive text-to-speech model based on flow matching with Diffusion Transformer (DiT) and ConvNeXt V2. • It simplifies the pipeline by removing the need for a duration model, text encoder, phoneme alignment, and semantically infused codec, using padded character sequences as input. • The model employs a novel Sway Sampling strategy during inference, improving performance and allowing for faster inference with fewer function evaluations. • Evaluation on LibriSpeech-PC, Seed-TTS test-en, and test-zh demonstrates that F5-TTS achieves state-of-the-art zero-shot performance with a real-time factor (RTF) of 0.15, outperforming existing methods in terms of both speed and quality. • Ablation studies highlight the robustness of F5-TTS, especially in handling challenging scenarios where the alignment between text and speech is crucial. | ['Text-to-Speech', 'Audio'] | [Link](https://github.com/SWivid/F5-TTS) | N/A |
| [MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders](https://arxiv.org/abs/2410.06845) | Chi Han, Qingyun Wang, May Fung, jindongwang, Cheng228 | - MentalArena is a novel self-play training framework for LLMs to improve their ability to diagnose and treat mental health disorders by generating personalized training data. - It consists of three modules: Symptom Encoder simulates realistic mental health patients, Symptom Decoder mitigates intent bias in patient-therapist dialogues, and Model Optimizer fine-tunes the LLM on the generated data. - The Symptom Encoder uses cognitive models and behavior principles of patients to produce realistic symptom descriptions. - The framework significantly outperformed several state-of-the-art and mental-health-specific LLMs, including GPT-4, on six benchmark datasets, demonstrating improvement over base models by 20.7% for GPT-3.5-turbo and 6.6% for Llama-3-8b. - Further analysis revealed a strong correlation between model performance and perplexity of the training data, and that maintaining data diversity above a certain threshold during training contributes to improved model performance. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/Scarelette/MentalArena/tree/main) | N/A |
| [Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning](https://arxiv.org/abs/2410.04223) | Jie Chen, Wojciech Matusik, Michael Sun, Gang Liu, mjiang89 | This research paper presents Llamole, a multimodal large language model (MLLM) for controllable and synthesizable molecular generation and retrosynthetic planning. Llamole integrates a base LLM with a graph diffusion transformer, graph neural networks, and A* search, allowing for the seamless generation of text, molecules, and reactions. Benchmarks on 14 LLMs of various sizes reveal the limitations of existing models in controllable molecular design and synthetic planning. Llamole shows significant improvement, increasing success rates from 5.5% to 35% and enhancing controllability by up to 80.9% across various metrics. | ['Multimodal', 'Graph Machine Learning', 'Text Generation'] | N/A | N/A |
| [Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach](https://arxiv.org/abs/2410.06949) | Minlie Huang, Yuan Yuan, Yuxuan Chen, XUANMINGZHANG |  - Seeker, a multi-agent framework leverages LLMs to enhance exception handling in code by addressing three key issues: insensitive detection of fragile code, inaccurate capture of exception types, and distorted handling solutions. - Seeker employs five agents—Scanner, Detector, Predator, Ranker, and Handler—inspired by expert developer strategies. -  A Common Exception Enumeration (CEE) document, built from trusted external experience and exception practices, is used to improve retrieval and handling. - A deep retrieval-augmented generation (Deep-RAG) algorithm is proposed to handle complex inheritance relationships between exception types. - Experimental results show that Seeker outperforms baselines on various metrics including code quality, coverage, accuracy, and edit similarity. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/XMZhangAI/Seeker) | N/A |
| [Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA](https://arxiv.org/abs/2410.06524) | Jordan Boyd-Graber, Hal Daumé III, zhoutianyi, mgor |  - This paper introduces CAIMIRA, a novel framework based on Item Response Theory (IRT) for evaluating and comparing the question-answering abilities of humans and AI systems. - CAIMIRA uses question text to infer characteristics, enabling generalization to new questions without needing prior responses and allowing for analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions. - The study finds that humans outperform AI in knowledge-grounded abductive and conceptual reasoning, while LLMs like GPT-4-TURBO excel at targeted information retrieval and fact-based reasoning. - The authors suggest future QA tasks focus on challenging higher-order reasoning, scientific thinking, nuanced linguistic interpretation, and cross-contextual knowledge application. - The implementation can be found at https://github.com/maharshi95/neural-irt | ['Question Answering'] | [Link](https://github.com/maharshi95/neural-irt) | [Link](mgor/protobowl-11-13) |
| [Does Spatial Cognition Emerge in Frontier Models?](https://arxiv.org/abs/2410.06468) | vkoltun, philkra, erikwijmans, sramakrishnan |  - The paper introduces SPACE, a benchmark for evaluating spatial cognition in large language models (LLMs) and large multimodal models.   - SPACE evaluates large-scale mapping abilities and smaller-scale reasoning about object shapes and layouts.   - The benchmark includes tasks from cognitive science, instantiated in parallel via text and images.   - Results indicate that current frontier models fall short of animal spatial intelligence, performing near chance level on several classic tests.   - The authors suggest that spatial cognition is a crucial form of intelligence, and its emergence in models is worthy of further investigation. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Zero-Shot Image Classification', 'Zero-Shot Object Detection', 'Computer Vision', 'Image Classification', 'Image Segmentation', 'Video Classification', 'Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |
