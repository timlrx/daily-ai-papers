

## Papers for 2024-10-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2410.10139) | WendellZwh, wangzhaoyang, StarThomas1002, Lillianwei, richardxp888 | • MMIE is a large-scale benchmark designed to evaluate the interleaved multimodal comprehension and generation capabilities of Large Vision-Language Models (LVLMs). • The benchmark comprises 20K meticulously curated multimodal queries across diverse fields, supporting both interleaved inputs and outputs in multiple-choice and open-ended formats. • An automated evaluation metric is proposed based on a fine-tuned InternVL-2-4B scoring model, which demonstrates strong alignment with human evaluation and mitigates potential biases.  • Experimental results reveal that even state-of-the-art LVLMs and the combination of advanced LLMs with text-to-image models face significant challenges in MMIE, with most achieving moderate performance, indicating substantial room for improvement.  • Error analysis categorizes key challenges into temporal understanding (cross-modality coherence, generation adaptability) and reasoning (multimodal information comprehension, complex reasoning) skills. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://mmie-bench.github.io/) |
| [LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models](https://arxiv.org/abs/2410.09732) | Junan Zhang, Zilong Huang, beccabai, bczhou, Yejy53 | - LOKI, a new benchmark designed to evaluate large multimodal models (LMMs) on synthetic data detection across various modalities (video, image, 3D, text, and audio), has been introduced. - The benchmark includes 18K questions across 26 subcategories, with multi-level annotations including coarse-grained and multiple-choice questions, and fine-grained anomaly selection and explanation tasks. - An evaluation of 22 open-source and 6 closed-source LMMs on LOKI has revealed their potential as synthetic data detectors while also showing limitations such as model biases, a lack of expert domain knowledge, and unbalanced multimodal capabilities. - While LMMs exhibited moderate capabilities with some levels of explainability and generalization, they still lag behind human performance in synthetic data detection tasks. - Chain-of-thought prompting improved the performance of most LMMs, but not GPT-4, suggesting that GPT-4 already exhibits strong reasoning capabilities for this task. | ['Multimodal', 'Computer Vision'] | N/A | N/A |
| [Toward General Instruction-Following Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.09584) | Zhicheng Dou, Runqi Qiao, Yutao Zhu, Xiaoshuai Song, Guanting Dong |   - This paper introduces VIF-RAG, an automated, scalable, and verifiable data synthesis pipeline designed to improve instruction-following alignment in Retrieval-Augmented Generation (RAG) systems.  - VIF-RAG begins with a small set of manually crafted atomic instructions and uses a combination of rule-based composition, supervised rewriting, and code-based verification to generate a large-scale dataset (VIF-RAG-QA) of instruction-following data for RAG.  - It also presents FollowRAG, a new benchmark for evaluating complex instruction-following capabilities in RAG, composed of 2.8K samples covering 22 categories of general instruction constraints and 4 knowledge-intensive QA datasets.  - In experiments, VIF-RAG significantly boosts performance across various LLMs and datasets, demonstrating a remarkable 44% improvement over the Llama3-base model in instruction-following within RAG scenarios.  - The results further indicate that VIF-RAG not only enhances IF capability but also maintains stability in RAG performance across different model sizes and datasets, offering promise for real-world applications. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://arxiv.org/abs/2410.10563) | wenhu, yuexiang96, DongfuJiang, yuanshengni, shermansiu |   - MEGA-BENCH is a multimodal evaluation benchmark comprising over 500 real-world tasks designed to assess the diverse capabilities of contemporary vision-language models.  - The benchmark employs a taxonomy of multimodal tasks and incorporates diverse output formats, moving beyond standard multiple-choice questions to include numbers, phrases, code, LaTeX, and coordinates.  -  A range of over 40 unique evaluation metrics, including rule-based and LLM-assisted options, is used to accommodate these diverse formats.  -  In evaluations, MEGA-BENCH demonstrated GPT-4's superior performance over other flagship models, and Qwen2-VL's leading performance among open-source models.  -  The benchmark facilitates fine-grained capability analysis by offering a breakdown of model performance across various dimensions such as input/output format and required skill.  | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | N/A |
| [LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content](https://arxiv.org/abs/2410.10783) | M. Jehanzeb Mirza, Sivan Doveh, Felipe Maia Polo, Nimrod Shabtay, wlin21at |  - LiveXiv is a novel, fully automated, multimodal live benchmark focusing on scientific domains, designed to address test set contamination and provide an updated evaluation of Large Multi-modal Models (LMMs). - It uses scientific papers from arXiv to generate Visual Question Answering (VQA) and Table Question Answering (TQA) pairs automatically, avoiding human bias and ensuring scalability. - An efficient evaluation pipeline based on Item Response Theory (IRT) allows for performance estimation on new benchmark versions by reevaluating only a small subset of models, significantly reducing computational costs. - The benchmark has been evaluated with 17 prominent open and proprietary LMMs, demonstrating its challenging nature and exposing model capabilities on less-contaminated data. - It provides the first version of the dataset including VQA and TQA pairs, alongside an efficient evaluation methodology and benchmark results, along with its limitations. | ['Visual Question Answering', 'Table Question Answering', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/IBM/LiveXiv) |
| [TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models](https://arxiv.org/abs/2410.10818) | Jianrui Zhang, Reuben Tan, Mu Cai, fengyao1909, BochengZou |  - TemporalBench, a novel video understanding benchmark, is introduced to evaluate the fine-grained temporal understanding abilities of multimodal video models. - The benchmark consists of ~10K video question-answer pairs derived from ~2K human-annotated captions with rich activity details, focusing on long-range dependencies and event progression. -  State-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, significantly lower than human performance (67.9%). - A critical pitfall in multi-choice QA is identified where LLMs can detect subtle changes in negative captions and find a "centralized" description as a cue for prediction. - Multiple Binary Accuracy (MBA) is proposed to correct such bias by decomposing multi-choice QA into multiple binary QAs. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://TemporalBench.github.io/) | N/A |
| [Tree of Problems: Improving structured problem solving with compositionality](https://arxiv.org/abs/2410.06634) | Rachel Bawden, Benoît Sagot, Armel Zebaze | - This research paper proposes Tree of Problems (ToP), a novel prompting approach for enhancing the problem-solving abilities of Large Language Models (LLMs). - ToP decomposes complex problems into a tree structure of simpler, analogous subproblems, leveraging compositionality for efficient problem-solving, and drawing inspiration from techniques like divide-and-conquer. - Empirical results demonstrate that ToP outperforms existing methods like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of Thoughts (GoT) on structured tasks. - Furthermore, ToP excels in out-of-distribution generalization scenarios. - The authors provide evidence of superior performance across various LLMs, including GPT-3.5, on difficult benchmark tasks such as Last Letter Concatenation and Navigate from BIG-Bench Hard. | ['Natural Language Processing'] | [Link](https://github.com/ArmelRandy/tree-of-problems) | N/A |
| [LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813) | Kai-Wei Chang, Yuwei Zhang, Wenhao Yu, Hongwei Wang, xiaowu0162 | - LongMemEval, a comprehensive benchmark designed to evaluate the long-term memory capabilities of chat assistants.  - It focuses on five core abilities: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. - The benchmark consists of 500 meticulously curated questions embedded within freely scalable user-assistant chat histories. - A unified framework is presented that breaks down long-term memory design into four design choices across indexing, retrieval, and reading stages.  - Several memory designs, including session decomposition, fact-augmented key expansion, and time-aware query expansion, are proposed and shown to greatly improve both memory recall and downstream question answering. | ['Question Answering'] | [Link](https://github.com/xiaowu0162/LongMemEval) | N/A |
