

## Papers for 2024-10-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures](https://arxiv.org/abs/2410.13754) | kcz358, fuzhao, Junhao233, dghosal, jinjieni |  - MixEval-X is a benchmark for evaluating multimodal models across various input-output modalities, including image, video, audio, text, and actions. - The benchmark covers eight input-output modality combinations and uses a mixture of existing datasets and real-world web data to construct evaluations.  - MixEval-X employs a novel multi-modal benchmark mixture and adaptation-rectification pipeline to optimize evaluation tasks by aligning them with real-world task distributions and mitigating biases.  - Meta-evaluations demonstrate that MixEval-X effectively aligns benchmark samples with real-world distributions, with model rankings correlating strongly (up to 0.98) with crowd-sourced real-world evaluations.  - The benchmark offers comprehensive leaderboards to rerank existing models and organizations across modalities. | ['Multimodal', 'Any-to-Any', 'Image-to-Text', 'Video-Text-to-Text', 'Text-to-Image', 'Text-to-Video', 'Text-to-Audio'] | [Link](https://mixeval-x.github.io/) | N/A |
| [Harnessing Webpage UIs for Text-Rich Visual Understanding](https://arxiv.org/abs/2410.13824) | Yuxiao Qu, Yifan Song, yuexiang96, oottyy, jeepliu |  - This paper introduces MultiUI, a 7.3 million sample dataset synthesized from 1 million web page UIs using LLMs, for training multimodal models in text-rich visual understanding. - MultiUI covers nine diverse tasks across three categories (visual understanding and reasoning, text recognition, and grounding), enhancing model perception, comprehension, grounding, and reasoning capabilities.  - Models trained on MultiUI demonstrate significant improvement, up to 48% on VisualWebBench and 19.1% on Mind2Web, outperforming larger models like LLaVA 1.6 34B and GPT-4V in GUI tasks.  - MultiUI also generalizes well to non-web UI tasks like document understanding, OCR, and chart interpretation, showing strong cross-domain generalization. - This highlights the value of structured web UI data for advancing text-rich visual understanding in MLLMs.  | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | N/A | [Link](https://neulab.github.io/MultiUI/) |
| [MobA: A Two-Level Agent System for Efficient Mobile Task Automation](https://arxiv.org/abs/2410.13757) | Yixuan Jiang, Kunyao Lan, Yansi Li, Hao Tang, JamesZhutheThird | - MobA, a novel two-level agent architecture designed to enhance the abilities of mobile phone assistants, using Multimodal Large Language Models (MLLMs). -  Composed of a higher-level Global Agent for tasks such as command interpretation and task planning, and a lower-level Local Agent to select and execute actions based on current screen information and historical data. -  A double reflection mechanism allowing the system to correct errors quickly and avoid sub-optimal operations, as well as an integrated memory module to track actions and optimize execution. -  Evaluation performed on the Mobbench dataset containing 50 mobile tasks across 10 applications of varying difficulty, outperforming other mobile agents, achieving the highest milestone score of 66.2%. | ['Multimodal', 'Computer Vision', 'Natural Language Processing'] | N/A | N/A |
| [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) | zdaxie, zizhpan, XCLiu, CNMaxwell, WuChengyue | - Janus is an autoregressive multimodal model that decouples visual encoding pathways for understanding and generation tasks using a shared transformer architecture.  - For understanding, it uses a SigLIP encoder for high-level semantic information, while for generation, it utilizes a VQ tokenizer focusing on fine-grained visual details.  - This approach addresses the conflicting representational needs of the two tasks, enabling both strong performance and model flexibility.  - Experimental results demonstrate that Janus outperforms other unified models of comparable size and matches or exceeds task-specific models on benchmarks like MMBench, SEED-Bench, POPE, MSCOCO, and GenEval.  - The model's performance and flexibility make it a potential candidate for the next generation of unified multimodal models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/deepseek-ai/Janus) | N/A |
| [MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://arxiv.org/abs/2410.13085) | Weijia Shi, Tianze Wang, Haoran Li, Kangyu Zhu, richardxp888 | MMed-RAG is a new multimodal retrieval-augmented generation (RAG) system designed to improve the factuality of Medical Large Vision-Language Models (Med-LVLMs). - It incorporates a domain-aware retrieval mechanism, adaptive context selection, and RAG-based preference fine-tuning to address misalignment issues and enhance alignment with ground truth. - The model achieves an average improvement of 43.8% in factual accuracy across five medical datasets and two tasks (medical VQA and report generation) compared to the original Med-LVLM.  - It outperforms other decoding-based and RAG-based approaches on medical VQA and report generation tasks. - MMed-RAG demonstrates strong generalizability, achieving consistent improvements across various medical image modalities (radiology, ophthalmology, and pathology). - Through ablation studies, the contribution of each proposed component is validated, demonstrating its effectiveness in enhancing the factuality and performance of Med-LVLMs in different medical domains. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/richard-peng-xia/MMed-RAG) | N/A |
| [A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models](https://arxiv.org/abs/2410.13841) | Keming Lu, Hongyu Lin, Bowen Yu, Le Yu, TangQiaoYu | - This paper introduces a unified perspective on delta parameter editing in post-trained large-scale models, formulating editing operations based on Riemann sum approximation of the loss difference. - This analysis categorizes existing methods into three performance classes: competitive (e.g., DARE, DELLA-Merging), decreased (e.g., BitDelta, Twin-Merging, TIES-Merging), and improved (e.g., EXPO), explaining their impact on model performance through the lens of Riemann sum approximation. - Extensive experiments on visual and language models (ViT, LLaMA 3, Qwen 2, Mistral) support the theoretical findings. - The paper further proposes extensions to existing techniques like DARE and BitDelta, generalizing their formats and improving applicability. - For example, introducing a factor *k* to DARE handles dropped parameters more effectively and expanding BitDelta to use multiple bits improves performance beyond the original post-trained model. | ['Natural Language Processing', 'Computer Vision'] | N/A | N/A |
| [PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment](https://arxiv.org/abs/2410.13785) | Ke Xu, Jiaheng Liu, Shawn Wang, Zekun Moore Wang, kangz | PopAlign is a framework for aligning large language models (LLMs) by diversifying contrasting patterns across prompt, model, and pipeline levels. - It integrates six distinct contrasting strategies: Prefix Contrast, Demon Contrast, Elicitive Contrast, NParam Contrast, Leaderboard Contrast, and Refine Contrast. - These strategies synthesize preference-contrastive data without requiring additional feedback labeling. - Experimental results demonstrate that PopAlign significantly outperforms existing methods on various alignment tasks and leaderboards. - Notably, PopAlign achieves higher scores than strong baselines trained on original labels, indicating its effectiveness in preference modeling and comprehensive alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [MoH: Multi-Head Attention as Mixture-of-Head Attention](https://arxiv.org/abs/2410.11842) | Shuicheng Yan, Li Yuan, Bo Zhu, Chat-UniVi | - Mixture-of-Head attention (MoH) is proposed, which integrates multi-head attention with a Mixture-of-Experts (MoE) mechanism by treating attention heads as experts. - MoH employs a router to select the top-k heads for each token, improving inference efficiency, and uses a weighted sum of outputs rather than standard summation, potentially enhancing performance. - Shared heads in MoH retain constant activation, capturing general knowledge. - Evaluations on ViT, DiT, and LLMs show MoH outperforms multi-head attention using only 50%~90% of heads. - Pre-trained models like LLaMA3-8B can be continue-tuned into MoH models, with MoH-LLaMA3-8B showing improved accuracy with fewer heads. | ['Computer Vision', 'Image Classification', 'Unconditional Image Generation', 'Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/SkyworkAI/MoH) | N/A |
| [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852) | Anne Wu, Gloria Geng, Yiwei Chen, Mustafa Omer Gul, Zizhao Chen |  - This paper introduces RESPECT, a novel method for improving large language models (LLMs) through retrospective learning from implicit feedback signals in multi-turn interactions. - RESPECT leverages user responses such as rephrased requests, expressions of frustration, or task pivots as implicit feedback signals, eliminating the need for explicit annotations or feedback solicitation.  - The method involves decoding feedback from past interactions by prompting the LLM to analyze interaction contexts and follow-up utterances. -  This decoded feedback is then used to re-train the LLM, resulting in continual improvement over multiple rounds of interaction and training. -  In a new multimodal interaction scenario called MULTIREF, where humans instruct an LLM to solve an abstract reasoning task, RESPECT demonstrates significant improvement, boosting task completion rate from 31% to 82% without external annotations. | ['Multimodal', 'Natural Language Processing', 'Reinforcement Learning'] | [Link](https://lil-lab.github.io/respect) | N/A |
| [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426) | Kang Zhao, Han Bao, Haoli Bai, Yuxuan Sun, lianlio |  - FLATQUANT, a novel post-training quantization approach, enhances the flatness of Large Language Model (LLM) weights and activations through fast and learnable affine transformations, improving quantization accuracy and reducing error propagation. - FLATQUANT employs a lightweight, block-wise training strategy over calibration data and utilizes Kronecker decomposition for efficient affine transformations, minimizing memory and computational demands. - A single kernel fusing affine transformations and quantization reduces transformation overhead, resulting in inference speedups of up to 2.3x for prefill and 1.7x for decoding compared to the FP16 baseline.  - FLATQUANT achieves state-of-the-art quantization results, including less than 1% accuracy drop for W4A4 quantization on LLaMA-3-70B, outperforming SpinQuant by 7.5%.  - The method's effectiveness is shown on various LLMs (LLaMA-2/3, 7B to 70B parameters) across tasks like language modeling and question answering, demonstrating superior accuracy and inference latency compared to other state-of-the-art techniques. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ruikangliu/FlatQuant) | N/A |
| [MedMobile: A mobile-sized language model with expert-level clinical capabilities](https://arxiv.org/abs/2410.09019) | Eric Karl Oermann, Daniel Alexander Alber, Anton Alaykin, Jaden Stryker, KrithikV | - MedMobile, a fine-tuned 3.8B parameter phi-3-mini language model, demonstrates expert-level clinical reasoning capabilities, achieving a 75.7% accuracy on MedQA (USMLE), surpassing the passing score for physicians and outperforming previous state-of-the-art sub-5B parameter models by over 20%. - MedMobile leverages chain-of-thought prompting, ensemble methods, and supervised fine-tuning, with the latter contributing an 8.4% improvement in accuracy.  - Unlike larger models, techniques such as k-shot prompting and retrieval-augmented generation did not enhance MedMobile's performance, possibly due to context window limitations, leaving potential avenues for future research.  - This model holds promise for low-resource medical settings and democratizes access to advanced language models beyond large technology companies.  - The model can be expanded to vision-language tasks by utilizing Phi-3-vision architecture. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/nyuolab/MedMobile) | [Link](https://huggingface.co/KrithikV/MedMobile) |
| [Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation](https://arxiv.org/abs/2410.13198) | Jian Xue, Peidong Wang, Michael Levit, Mohammad Sadegh Rasooli, Sreyan Ghosh |  - This paper introduces DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach to improve the performance of generative error correction (GEC) models for automatic speech recognition (ASR) systems.  - DARAG addresses limitations of traditional GEC models by augmenting training data with synthetic examples generated by prompting large language models (LLMs) and text-to-speech (TTS) models, simulating realistic ASR errors.  - It also incorporates retrieval augmentation, extracting named entities from the training data and retrieving similar entities during correction to handle novel or unknown named entities more effectively. - Experimental results on various in-domain and out-of-domain settings show that DARAG consistently outperforms baseline methods, with relative word error rate (WER) improvements of 8%-30% in in-domain and 10%-33% in out-of-domain scenarios. -  DARAG improves named entity correction and shows the benefit of using synthetic data in low-resource domain adaptation setting as well. | ['Automatic Speech Recognition', 'Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2410.13618) | Chengwei Sun, Ran Ran, Yujia Wu, Jiwei Wei, Shiym | • LoLDU is a novel Parameter-Efficient Fine-Tuning (PEFT) method that leverages Lower-Diag-Upper (LDU) decomposition to reduce the number of trainable parameters during fine-tuning. • LoLDU initializes low-rank matrices with orthogonal properties using LDU decomposition, focusing on optimizing a diagonal matrix for scaling transformations and dynamic adjustment of a scaling factor to align updates with the target matrix. • LoLDU achieves comparable performance to full fine-tuning and other PEFT methods while drastically reducing trainable parameters, sometimes down to 0.00025% of the original model. • Experimental results across various tasks, including instruction following, natural language understanding, image classification, and image generation, with models ranging from 86 million to 7 billion parameters (LLaMA2, RoBERTa, ViT, and Stable Diffusion) demonstrate LoLDU's effectiveness. • LoLDU excels in preserving pre-trained knowledge and enhancing generalization through the use of orthogonal lower and upper triangular matrices, outperforming LoRA on certain tasks while using significantly fewer parameters. | ['Computer Vision', 'Image Classification', 'Text-to-Image', 'Natural Language Processing', 'Text Generation', 'Text Classification'] | [Link](https://github.com/SKDDJ/LoLDU) | N/A |
| [BenTo: Benchmark Task Reduction with In-Context Transferability](https://arxiv.org/abs/2410.13804) | Lichao Sun, Ming Li, Hongyu Zhao, zhoutianyi | BENTO: Benchmark Task Reduction with In-Context Transferability - This paper introduces a novel benchmark reduction method called BENTO (Benchmark Task Reduction) designed to reduce the evaluation cost of Large Language Models (LLMs).  - BENTO leverages In-Context Transferability (ICT), a training-free approach to estimate the transferability between different tasks using in-context learning.  - By analyzing the ICT matrix and applying spectral clustering, BENTO identifies representative tasks that capture the overall benchmark's essence.  - The paper shows that BENTO can reduce the number of tasks in popular LLM benchmarks like MMLU and FLAN by up to 95% while maintaining evaluation accuracy within a 4% margin of the full benchmark.  - This method is significantly more efficient than existing benchmark reduction techniques as it doesn't rely on computationally expensive fine-tuning or extensive training data. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/tianyi-lab/bento) | N/A |
| [AERO: Softmax-Only LLMs for Efficient Private Inference](https://arxiv.org/abs/2410.13060) | Brandon Reagen, Nandan Kumar Jha |  - AERO, a four-step architectural optimization framework, refines existing large language models (LLMs) for efficient private inference (PI) by removing nonlinearities and reducing FLOPs. - AERO systematically removes nonlinearities such as LayerNorm and GELU, proposes using ReLU in LayerNorm-free models, and designs a Softmax-only architecture tailored for PI. - A novel entropy regularization technique mitigates entropic overload, improving the performance of the Softmax-only model. - AERO achieves up to a 4.23x reduction in communication overhead and a 1.94x speedup in latency compared to the baseline. - Experiments were conducted on GPT-2 and Pythia-70M models, trained from scratch on CodeParrot and Languini datasets, demonstrating improvements across various context sizes and model depths. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant](https://arxiv.org/abs/2410.13360) | Xiangyu Yue, Yu-Feng Li, Changsheng Li, Jiaming Han, Hoar012 |  - This paper introduces Retrieval Augmented Personalization (RAP), a framework for personalizing Multimodal Large Language Models (MLLMs) by integrating user-specific visual concepts without requiring further training. - RAP employs a key-value database to store user-provided concept information (image, name, description), retrieves relevant information using a multimodal retriever based on user input (image and/or text), and feeds both the query and retrieved information to the MLLM for personalized response generation. - A dedicated dataset is created using a pipeline that leverages Gemini to automatically generate personalized captions, descriptions, and question-answer pairs associated with user-provided visual concepts.  - Experimental results show that RAP-MLLMs, trained on this dataset using LLaVA and Phi-3V backbones, achieve superior performance in personalized image captioning and visual question answering compared to finetuning and other personalization methods, while also performing well on standard multimodal benchmarks like MMMU and InfoSeek. - RAP offers real-time concept editing and addition by updating the external database, providing flexibility and eliminating retraining needs, though performance depends on the robustness of the multimodal retriever. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Hoar012/RAP-MLLM) | N/A |
| [MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization](https://arxiv.org/abs/2410.12957) | Shengpeng Ji, Ziang Zhang, Xize Cheng, Siqi Zheng, Ruiqi Li | MuVi is a novel video-to-music generation framework that focuses on semantic alignment and rhythmic synchronization. - MuVi employs a non-autoregressive encoder-decoder architecture, using a pre-trained visual encoder and a flow-matching-based music generator. A visual adaptor connects the two modules and performs efficient compression of high-frame-rate visual features. - A contrastive music-visual pre-training scheme is introduced, utilizing negative samples from temporal shifts and random replacements to enhance rhythmic synchronization. - Experimental results demonstrate MuVi's superior performance over existing methods, achieving improvements in audio quality and temporal synchronization in generated music. | ['Text-to-Audio', 'Multimodal'] | N/A | N/A |
| [Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems](https://arxiv.org/abs/2410.13334) | Isack Lee, hbseong |  - This paper introduces PCJailbreak, a method to analyze how intentional biases in Large Language Models (LLMs), implemented for safety alignment, can be exploited to generate harmful content.  - The method involves using LLM-generated keywords representing contrasting demographic groups in prompts containing harmful requests to assess the model's susceptibility to jailbreak attacks.  - Experiments on various LLMs, including GPT models and open-source alternatives, revealed that intentional biases lead to significant differences in jailbreak success rates between marginalized and privileged groups.  - The paper also proposes PCDefense, a mitigation strategy that uses prompts to adjust biases without the need for additional inference or models, unlike Guard Models.  - The authors advocate for responsible development and deployment of LLMs, emphasizing careful consideration of safety measures to avoid unintended vulnerabilities. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation](https://arxiv.org/abs/2410.13293) | Tim Oates, pdx97 |  - This paper introduces SBI-RAG, a Schema-Based Instruction Retrieval-Augmented Generation framework, for enhancing math word problem solving using a Large Language Model (LLM).  - SBI-RAG uses a schema classifier (trained on DistilBERT) to predict the problem's schema, which guides prompt creation for context retrieval using RAG and generates step-by-step solutions using Ollama Llama 3.1.  - The authors evaluate SBI-RAG on GSM8K, comparing it with GPT-4 and GPT-3.5 Turbo, using a "reasoning score" to assess solution quality.  - Results suggest SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially improving student learning.  - The approach incorporates a schema classifier, structured prompt generation, schema-relevant RAG, and a new evaluation metric. | ['Question Answering', 'Text2Text Generation', 'Natural Language Processing'] | N/A | N/A |
| [$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models](https://arxiv.org/abs/2410.13859) | Xiaoshuai Sun, Yiyi Zhou, Jiayi Ji, Gen Luo, YaxinLuo |  - This paper introduces $\gamma$-MoD, a novel mixture-of-depth (MoD) adaptation strategy for enhancing the computational efficiency of existing Multimodal Large Language Models (MLLMs). - $\gamma$-MoD employs a new metric called Rank of Attention Maps (ARank) to identify and replace redundant MLLM layers with MoD layers, dynamically allocating computational resources based on token relevance. - Two key designs, shared vision-language router and masked routing learning, are incorporated to maximize sparsity while preserving performance. - The shared router applies routing to the entire multimodal sequence for better optimization, and masked routing learning prevents critical tokens from being skipped during training. - Experiments on nine benchmarks show that $\gamma$-MoD notably reduces training and inference time while maintaining competitive performance compared to existing dense and sparse MLLMs. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment](https://arxiv.org/abs/2410.09347) | Jun Zhu, Peize Sun, Hang Su, ChenDRAG | - This paper introduces Condition Contrastive Alignment (CCA), a fine-tuning technique for autoregressive (AR) visual generation models to improve sample quality without relying on guided sampling methods like Classifier-Free Guidance (CFG). - CCA fine-tunes pre-trained models by contrasting positive and negative image-condition pairs, directly optimizing the model to achieve the desired target distribution, similar to alignment techniques used in language models. - Experimental results on LlamaGen and VAR models demonstrate significant improvement in guidance-free FID and IS scores after just one epoch of fine-tuning with CCA, achieving performance comparable to CFG while reducing sampling costs. - CCA offers a controllable trade-off between image diversity and fidelity similar to CFG by adjusting a training hyperparameter (λ), further confirming their theoretical connection in targeting the same sampling distribution. - Combining CCA with CFG can lead to further performance gains, showcasing its potential as a complementary technique for enhancing visual generation. | ['Text-to-Image', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/thu-ml/CCA) | N/A |
| [Can MLLMs Understand the Deep Implication Behind Chinese Images?](https://arxiv.org/abs/2410.13854) | Xinrun Du, Yuelin Bai, Xi Feng, zhangysk, MING-ZCH |  - This research introduces CII-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) ability to understand the implications behind Chinese images, including those deeply rooted in Chinese traditional culture. - CII-Bench includes 698 images across diverse domains and visual content types, paired with 800 multiple-choice questions to assess comprehension and reasoning abilities. - Experimental findings reveal a notable performance gap between MLLMs and humans, with models achieving a maximum accuracy of 64.4% compared to human accuracy averaging 78.2%. - A custom evaluation metric is designed using GPT-4 to better evaluate Chinese traditional painting comprehension, revealing model limitations in grasping complex cultural nuances. - Models benefit from image emotion hints in prompts, indicating ongoing struggles with emotional understanding crucial for accurate interpretation. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/MING_X/CII-Bench) | [Link](https://cii-bench.github.io/) |
| [Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key](https://arxiv.org/abs/2410.10210) | Yunlin Mao, Jintao Huang, Daoze, wangxingjun778, Yingda |  - This research introduces a technique for enhancing the long-form output generation capabilities of Large Language Models (LLMs) through minimal tuning with high-quality data. - By curating a smaller, higher-quality dataset from the existing LongWriter-6k dataset, and combining it with a small fraction of alignment data, this method demonstrates comparable performance improvements to more compute-intensive training approaches. - Notably, the new dataset requires just 3.74% of the original training data, improving tuning efficiency by effectively addressing issues with data quality such as mismatched output lengths and missing instructions in the original data. - Evaluations based on length-following score (SL) and writing quality score (SQ) show improvements across various models, including the Qwen and GLM families. - This approach provides an efficient method for enhancing long-form output generation while preserving model coherence and alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://www.modelscope.com/models/swift/MS-LongWriter-GLM4-9B-Chat), [Link](https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2-7B-Instruct), [Link](https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2.5-7B-instruct), [Link](https://www.modelscope.com/datasets/ZhipuAI/LongWriter-6k), [Link](https://huggingface.co/datasets/THUDM/LongWriter-6k), [Link](https://huggingface.co/THUDM/LongWriter-glm4-9b), [Link](https://github.com/modelscope/evalscope/tree/main/evalscope/third_party/longbench_write), [Link](https://www.modelscope.com/datasets/swift/longwriter-6k-filtered), [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-Chinese), [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-English), [Link](https://huggingface.co/THUDM/glm-4-9b) |
| [TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration](https://arxiv.org/abs/2410.12183) | Yali Wang, Yu Qiao, Kunchang Li, Shaobin Zhuang, markywg | TransAgent is a novel framework that transfers knowledge from heterogeneous vision, language, and multimodal agents to enhance the generalization of Vision-Language (V-L) foundation models like CLIP. - It leverages 11 different pre-trained agents covering various tasks and modalities, including visual recognition, dense prediction, chatbot, text encoding, multimodal generation, and captioning. - The knowledge transfer is achieved through a unified distillation framework, where a Mixture-of-Agents (MoA) gating mechanism adaptively integrates knowledge from different agents. - TransAgent achieves state-of-the-art performance on 11 visual recognition datasets, outperforming CoOp by approximately 10% on average and 20% on EuroSAT under the same low-shot setting. - All pre-trained agent models can be unloaded after distillation, resulting in efficient deployment with no need for model ensembles in the inference phase. | ['Zero-Shot Image Classification', 'Image Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/markywg/transagent) | N/A |
