

## Papers for 2024-10-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation](https://arxiv.org/abs/2410.13232) | jihoonkim25, Gwanwoo, ktio, kimnamssya, hyungjoochae | • This paper introduces World-Model-Augmented (WMA) web agents, which leverage world models to simulate the outcomes of actions for enhanced decision-making in web navigation. • WMA agents address the limitations of Large Language Models (LLMs) in long-horizon web navigation tasks by incorporating a world model that predicts the effects of actions, enabling the agent to foresee potential outcomes. • The authors propose a transition-focused observation abstraction method to overcome training challenges, where the world model is trained to generate natural language descriptions of state differences between time steps, rather than predicting the entire next observation. • The WMA agent employs a value function to estimate rewards for simulated next observations, guiding the policy model to select optimal actions. • Experimental results on WebArena and Mind2Web show that WMA agents improve policy selection, achieve state-of-the-art performance on Mind2Web, and demonstrate superior cost and time efficiency compared to tree-search-based agents. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/kyle8581/WMA-Agents) | N/A |
| [UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models](https://arxiv.org/abs/2410.14059) | Yilin Guo, Yan Hu, wittenberg, amstrongzyf, TobyYang7 | - UCFE, a User-Centric Financial Expertise benchmark, is introduced to evaluate LLMs' ability to handle complex, real-world financial tasks using dynamic, task-specific interactions in a hybrid approach combining human and LLM evaluations. - Based on a user study with 804 participants, a dataset is created that incorporates various user intents and interactions across different user groups, serving as a foundation for benchmarking 12 LLMs using LLM-as-Judge methodology. - Results demonstrate a strong correlation (Pearson coefficient 0.78) between benchmark scores and human preferences, validating the UCFE dataset and evaluation method. - Mid-sized LLMs (7B-14B parameters), fine-tuned on financial texts, achieve a balance between performance and resource efficiency. - The user-centric design highlights the necessity of aligning AI systems with diverse user requirements in finance, setting the stage for enhanced, reliable AI-driven solutions. | ['Natural Language Processing'] | [Link](https://github.com/TobyYang7/UCFE-Benchmark) | N/A |
| [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669) | Daniel Jiang, Wenxuan Peng, Zhiqiu Lin, Nyandwi, BaiqiL | • NaturalBench is a new benchmark designed for evaluating vision-language models (VLMs) on natural adversarial samples. • These are image-question pairs derived from real-world images and questions, which are easily answered by humans but pose a challenge for current VLMs. • The authors use a semi-automated approach to curate the benchmark, making use of CLIP and ChatGPT to source and filter questions from image caption datasets. • The benchmark comprises 10,000 human-verified question-answer samples, categorized by visual reasoning skill. • Evaluation results of 53 state-of-the-art VLMs demonstrate a significant performance gap compared to humans, suggesting the benchmark's efficacy in revealing areas for improvement. | ['Visual Question Answering', 'Multimodal'] | [Link](https://linzhiqiu.github.io/papers/naturalbench) | N/A |
| [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](https://arxiv.org/abs/2410.13276) | Hayden Kwok-Hay So, Dayou Du, Shijie, CharyZeng, Retromonic |  - This paper introduces SeerAttention, a novel attention mechanism designed to improve the efficiency and scalability of Large Language Models (LLMs), especially those with long context windows, by learning intrinsic sparse attention rather than using predefined patterns. - SeerAttention augments conventional attention with a learnable gate, called Attention Gate (AttnGate), to dynamically select important blocks in an attention map and treat the rest as sparse. - It employs a customized FlashAttention kernel to extract the block-level ground truth of attention maps for efficient training of the gating network, minimizing overhead. - Evaluations show SeerAttention outperforms existing sparse attention methods in post-training and achieves near-lossless accuracy with high sparsity (up to 90%) during fine-tuning for long context extension using YaRN. -  With a block-sparse pattern, the attention kernel achieves up to a 5.67x speedup over the FlashAttention-2 dense baseline on a single A100 GPU. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/microsoft/SeerAttention) | N/A |
| [Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts](https://arxiv.org/abs/2410.14677) | Yury Chekhovich, Anastasia Voznyuk, German Gritsai, andriygav |  - This paper presents a systematic review of datasets used in competitions and research papers dedicated to AI-generated content detection and proposes methods for evaluating the quality of such datasets. - The authors argue that the high performance of current detectors on benchmark datasets may be due to the poor quality of the evaluation datasets rather than the true effectiveness of the detectors. - The authors investigate different metrics, such as detecting low-quality generations with the use of metrics based on topological time series, detecting suspicious activation maps, and detecting sensibility to perturbations, such as text modification and sentence shuffling - The paper emphasizes the need for robust and qualitative methods to evaluate generated data to be secure against bias and low generalization ability of future models and provide a more comprehensive understanding of the dynamics between human and machine text. - The paper suggests that the use of high-quality generated data can be used for two purposes: enhancing the training of detection models and refining the training datasets themselves. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828) | Mengdi Wang, Huazheng Wang, Yue Wu, yokey, huiyuan23 |  - This paper identifies a common pitfall in margin-based language model alignment methods used in Reinforcement Learning from Human Feedback (RLHF): the under-specification of ideal behavior on preferred and dispreferred responses.  - This issue leads to two problems as the margin increases: an increase in unsafe responses, and a decrease in preferred, ideal responses.  - The underlying cause is identified as the *gradient entanglement* effect, in which margin-based losses couple the preferred and dispreferred probabilities, thus often preventing ideal changes.  - This effect is characterized by an inner product condition involving the gradients of preferred and dispreferred log-probabilities.  - The theoretical analysis is empirically validated, and suggests potential mitigation through pairwise normalized gradient descent and sparsity regularized token masking. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/HumainLab/Understand_MarginPO) | N/A |
| [DPLM-2: A Multimodal Diffusion Protein Language Model](https://arxiv.org/abs/2410.13782) | Shujian Huang, Dongyu Xue, Fei Ye, Zaixiang Zheng, Xinyou Wang | • DPLM-2 is a multimodal protein foundation model based on a discrete diffusion probabilistic framework that models both protein sequences and structures.  • DPLM-2 employs a lookup-free quantizer (LFQ) to convert 3D coordinates to discrete tokens, facilitating structure learning within the language model.  • It uses an efficient warm-up strategy, leveraging pre-trained sequence-based DPLM and evolutionary data to enhance structural modeling.  • DPLM-2 demonstrates competitive performance in co-generation of structure and sequence, achieving high designability and outperforming ESM3-Open and Multiflow in structure-sequence compatibility.  • DPLM-2 also shows strong results in various conditional generation tasks like folding, inverse folding, and motif scaffolding and structure-aware representations for predictive tasks. | ['Multimodal', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media](https://arxiv.org/abs/2410.12791) | Mette Thunø, Rebecca M. M. Hicke, Ross Deans Kristensen-McLachlan, kardosdrur | This paper introduces KeyNMF, a novel approach to topic modeling that leverages contextual embeddings and Non-negative Matrix Factorization (NMF). - KeyNMF extracts keywords from documents using contextual embeddings and then applies NMF to these embeddings to generate topics.  - It is evaluated on Chinese news data and demonstrates competitive performance compared to other contextual topic models, especially in terms of external coherence. - KeyNMF is integrated with existing methods for analyzing information dynamics to study Chinese diaspora media's coverage of the 2024 European parliamentary elections.  -  The pipeline identifies trends in novelty and resonance signals that correlate with key political events, demonstrating its effectiveness in capturing information dynamics. - The researchers find that KeyNMF enables nuanced analysis of information flow and agenda-setting within Chinese diaspora media during the election period. | ['Natural Language Processing', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) |
