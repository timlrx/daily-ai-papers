

## Papers for 2024-10-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution](https://arxiv.org/abs/2410.16256) | Hongwei Liu, Maosong Cao, zsytony, KennyUTC, acylam | - CompassJudger-1 is introduced as the first open-source all-in-one judge LLM. - It supports unitary scoring, two-model comparisons, formatted evaluations, critique generation and diverse tasks. - A new benchmark called JudgerBench is created to evaluate judge models. It includes realistic human annotation from the LLM arena and GPT annotations on subjective benchmarks. - Training data for CompassJudger-1 includes several sources, like pair-wise data, critiques and reward data. - Several data filtering and sampling strategies are developed for training. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/CompassJudger) | N/A |
| [PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://arxiv.org/abs/2410.13861) | hsli-cuhk, daijifeng, zengxingyu, gogoduan, LucasFang |   - PUMA, a unified multimodal large language model (MLLM), is introduced, featuring multi-granular visual feature processing for diverse visual tasks.  - The model uses a three-part architecture: a multi-granular image encoder (CLIP), a set of dedicated diffusion-based decoders, and an autoregressive MLLM.  - PUMA is trained in two stages: Multimodal pretraining on large datasets (Laion-2B, Laion-Aesthetics, GRIT, The Pile, OCR-VQA-200K, LLaVAR) followed by task-specific instruction tuning.  - The evaluation shows that PUMA excels in diverse text-to-image generation, image editing, conditional image generation, and understanding, outperforming existing unified MLLMs.  - The multi-granular approach balances diversity and controllability by processing features at multiple levels from coarse-grained abstractions to fine-grained details. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image', 'Visual Question Answering'] | [Link](https://github.com/rongyaofang/PUMA) | N/A |
| [Baichuan Alignment Technical Report](https://arxiv.org/abs/2410.14940) | dongguosheng, YijieZhou, TJU-Tianpengli, zilchshen, lin5547 | • This report introduces Baichuan Alignment, a comprehensive suite of techniques used to align the Baichuan series of large language models (LLMs), including optimization methods, data strategies, and evaluation processes. • Baichuan Alignment consists of three phases: Prompt Augmentation System (PAS) which transforms user queries into actionable instructions, Supervised Fine-Tuning (SFT) which trains LLMs for dialogue and complex tasks, and Preference Alignment which aligns LLMs with human preferences. • The alignment process employs several optimizations such as sample packing and multi-layer gradient checkpointing to increase training efficiency and model merging to improve performance across domains. • Evaluations of Qwen2-Nova-72B and Llama3-PBM-Nova-70B, instruct versions of open-source models optimized with Baichuan Alignment, show significant performance improvements across various benchmarks, outperforming official instruct versions and competing with leading LLMs. • Baichuan-Instruct, an internal model, demonstrates 17% to 28% user experience improvement in core capabilities, highlighting the effectiveness of the proposed alignment techniques. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B) |
| [AutoTrain: No-code training for state-of-the-art models](https://arxiv.org/abs/2410.15735) | abhishek | AutoTrain (AutoTrain Advanced) is an open-source, no-code tool/library for training and fine-tuning machine learning models on a variety of tasks and modalities. - It supports various tasks, including large language model (LLM) fine-tuning, text classification/regression, token classification, sequence-to-sequence tasks, fine-tuning of sentence transformers, visual language model (VLM) fine-tuning, image classification/regression, and tabular data classification/regression. - AutoTrain simplifies the training process by providing a user-friendly interface and automating tasks such as dataset processing, hyperparameter tuning, and model validation. - It offers flexibility by supporting local and cloud-based training, multiple data formats (zip, CSV, JSONL), and various model architectures compatible with Hugging Face Transformers. - AutoTrain is designed for both novice and experienced users, enabling them to build and deploy high-performing models easily. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Object Detection', 'Text Classification', 'Token Classification', 'Tabular Classification', 'Tabular Regression', 'Text2Text Generation', 'Text Generation', 'Multimodal'] | [Link](https://github.com/huggingface/autotrain-advanced) | N/A |
| [RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style](https://arxiv.org/abs/2410.16184) | Rui Min, Yantao Liu, juanli, Nuomei, TranSirius | This paper introduces RM-BENCH, a novel benchmark designed to evaluate reward models for language models.  RM-BENCH focuses on assessing reward models' sensitivity to subtle content differences and resistance to style biases, unlike existing benchmarks.  Extensive experiments show RM-BENCH strongly correlates with policy model performance, making it a reliable tool for selecting effective reward models. Results indicate that current state-of-the-art reward models perform poorly when faced with style bias, showcasing areas for improvement in future model development. The benchmark includes datasets across various domains, including chat, code, math, and safety, with style-controlled variations. | ['Natural Language Processing', 'Text Classification', 'Reinforcement Learning'] | [Link](https://github.com/THU-KEG/RM-Bench) | N/A |
| [Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://arxiv.org/abs/2410.16153) | Nyandwi, seungone, akariasai, yueqis, yuexiang96 | This paper introduces PANGEA, a fully open multilingual multimodal large language model (LLM) trained on a diverse 6M instruction dataset spanning 39 languages.  PANGEA significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts, showing comparable performance to state-of-the-art (SOTA) models in English while substantially exceeding them in multilingual scenarios.  The model's architecture is based on LLaVA-Next, using Qwen2-7B-Instruct as the language model backbone.  PANGEA, along with its associated data and code, is fully open-sourced to promote equitable and accessible access to robust multilingual MLLMs. | ['Multimodal'] | [Link](https://neulab.github.io/Pangea/) | [Link](https://huggingface.co/datasets/cmarkea/table-vqa), [Link](https://huggingface.co/datasets/deepvk/GQA-ru), [Link](https://huggingface.co/datasets/cmarkea/doc-vqa), [Link](https://huggingface.co/datasets/cmarkea/table-vqa), [Link](https://huggingface.co/datasets/BUAADreamer/Chinese-LLaVA-Med-7B), [Link](https://huggingface.co/datasets/LinkSoul-AI/Chinese-LLaVA), [Link](https://huggingface.co/datasets/Toshi456/LLaVA-Japanese-Instruct), [Link](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT) |
| [Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception](https://arxiv.org/abs/2410.12788) | Zhiyuan Ji, jimi888, siminniu, MoCun, Robot2050 |  - This paper introduces Meta-Chunking, a novel text segmentation technique that leverages Large Language Models (LLMs) to divide documents into logically coherent chunks at a granularity between sentences and paragraphs. - Two strategies are proposed: Margin Sampling Chunking, which performs binary classification on consecutive sentences based on probability differences, and Perplexity Chunking, which analyzes perplexity distribution to identify chunk boundaries. - A dynamic merging strategy is also introduced to balance fine-grained and coarse-grained chunking, adjusting chunk sizes based on user-specified length requirements. - Experimental results across eleven datasets and four benchmarks demonstrate that Meta-Chunking significantly improves single-hop and multi-hop question answering performance in Retrieval-Augmented Generation (RAG) systems. - On the 2WikiMultihopQA dataset, for example, Meta-Chunking outperforms similarity chunking by 1.32 in F1 score while requiring only 45.8% of the processing time. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/IAAR-Shanghai/Meta-Chunking) | N/A |
| [Pre-training Distillation for Large Language Models: A Design Space Exploration](https://arxiv.org/abs/2410.16215) | Xin Lv, juanli, NeoZ123, bys0318, Wesleythu | - This paper explores pre-training distillation (PD), a method for transferring knowledge from a larger teacher LLM to a smaller student LLM during the pre-training phase. - The study investigates four key aspects of PD: logits processing, loss selection, scaling law (model and data size), and the use of offline vs. online logits. - A preliminary experiment using GLM-4-9B as the teacher and a 1.9B parameter student model shows a 1.6% average improvement across various datasets with PD. - Further experiments reveal that larger student LLMs benefit more from PD, while larger teacher LLMs don't always guarantee better results. - The best results are achieved by combining Kullback-Leibler divergence loss with language modeling loss using a Warmup-Stable-Decay scheduling strategy. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation](https://arxiv.org/abs/2410.15748) | Ping Wei, opotle, yegong, shuailu, EurekaWu123 |  - Alchemy, a novel framework, synthesizes formal theorems through symbolic mutations to address the data scarcity challenge in Neural Theorem Proving (NTP). - For each candidate theorem, Alchemy identifies invocable theorems from Mathlib and performs mutations by replacing terms with equivalent forms or antecedents. - This method significantly increases the number of theorems in Mathlib from 110k to 6M. - Continual pretraining and supervised finetuning on this augmented dataset for Large Language Models leads to a 5% absolute performance improvement on the Leandojo benchmark and a 2.5% gain on the miniF2F benchmark. - The analysis of synthetic data composition and training paradigms offers valuable insights for developing strong theorem provers. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation](https://arxiv.org/abs/2410.14745) | Wei Ju, Xiao Luo, Shockzipper, XtremSup, luojunyu | - SemiEvol is a semi-supervised fine-tuning framework designed to improve large language model (LLM) performance in scenarios with limited labeled data and abundant unlabeled data. - It employs a bi-level knowledge propagation strategy, transferring knowledge from labeled data to unlabeled data through both model adaptation and context enhancement.  - For unlabeled data utilization, it involves collaborative learning among multiple LLMs with diverse configurations and adaptive data selection. - Experimental results on various datasets, including MMLU, MMLU-Pro, ARC, FPB, USMLE, PubMedQA, and ConvFinQA, demonstrate significant performance improvements compared to SFT and self-evolution methods. - SemiEvol effectively utilizes both labeled and unlabeled data, enabling LLMs to adapt to specific scenarios more economically. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/luo-junyu/SemiEvol) | [Link](https://huggingface.co/Solshine/reflection-llama-3.1-8B), [Link](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B) |
| [Zero-shot Model-based Reinforcement Learning using Large Language Models](https://arxiv.org/abs/2410.11711) | GPaolo, albert9000, Xssama, ambroiseodt, abenechehab |  - This paper introduces Disentangled In-Context Learning (DICL), a novel approach for applying in-context learning (ICL) with large language models (LLMs) to reinforcement learning (RL) environments with continuous state spaces. - DICL addresses the challenges of incorporating action information and handling state-action dimension interdependence by projecting the state-action vector into a latent space using Principal Component Analysis (PCA) where features are linearly uncorrelated, then applying ICL. - The paper demonstrates the effectiveness of DICL in two RL applications: model-based policy evaluation and data-augmented off-policy RL, showing improved sample efficiency in both cases. - A theoretical analysis provides a novel return bound for the policy evaluation algorithm resulting from multi-branch rollouts with the LLM-based dynamics model.  - Additionally, the paper provides empirical evidence suggesting that LLMs offer well-calibrated uncertainty estimations, a desirable property for model-based RL algorithms. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/abenechehab/dicl) | N/A |
| [Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement](https://arxiv.org/abs/2410.15633) | Yunshui Li, Gang Chen, Haozhe Zhao, Shuzheng Si, kaikai1 |  - This paper introduces GATEAU, a novel framework for selecting influential samples to improve long-context alignment in large language models (LLMs).  - GATEAU leverages Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM) to identify high-quality samples exhibiting strong long-range dependencies.  - HMG compares perplexity scores from homologous models with different context windows to assess response generation difficulty, while CAM evaluates whether the model focuses on important input segments.  - Experimental results show that LLMs trained on samples selected by GATEAU outperform those trained on the full dataset and various baselines across multiple benchmarks, including LongBench, LongBench-Chat, and MT-Bench.  - Ablation studies demonstrate that both HMG and CAM contribute significantly to GATEAU's effectiveness. | ['Natural Language Processing', 'Text Generation', 'Summarization', 'Question Answering'] | N/A | N/A |
| [CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy](https://arxiv.org/abs/2410.13218) | Travis Labrum, wangwilliamyang, xz97, Xianjun, billmianz | This paper introduces CBT-BENCH, a new benchmark for evaluating large language models' (LLMs) ability to assist cognitive behavioral therapy (CBT).  CBT-BENCH includes three levels of tasks: basic CBT knowledge acquisition, cognitive model understanding, and therapeutic response generation.  The benchmark uses three new datasets (CBT-QA, CBT-CD, CBT-PC, CBT-FC and CBT-DP) to evaluate the LLMs across these tasks.  The results show that while LLMs perform well on knowledge-based tasks, they struggle with complex tasks that require deep understanding of patient cognitive structures and effective response generation.  This suggests the need for further research into how LLMs can be improved for real-world CBT applications. | ['Natural Language Processing', 'Text Classification', 'Question Answering', 'Text Generation'] | [Link](https://github.com/mianzhang/CBT-Bench) | N/A |
| [Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs](https://arxiv.org/abs/2410.13394) | anoopk, prajdabre, dipsivenkatesh, safikhan, sumanthd | This paper introduces CIA Suite, a framework for cross-lingual auto-evaluation of multilingual LLMs.  The framework includes a novel test set (RECON) with human annotations across six languages and a cross-lingual evaluator LLM (HERCULE). HERCULE leverages English reference answers to evaluate responses in other languages, addressing the scarcity of reference answers in low-resource scenarios. Experiments show HERCULE aligns more closely with human judgments than existing models, exhibiting effectiveness in zero-shot settings.  The CIA suite is publicly available to encourage further research. | ['Natural Language Processing', 'Text Generation', 'Text Classification', 'Zero-Shot Classification'] | [Link](https://github.com/CIA) | [Link](huggingface.co/CIA-Suite) |
| [DM-Codec: Distilling Multimodal Representations for Speech Tokenization](https://arxiv.org/abs/2410.15017) | A K M Mahbubur Rahman, Md Fahim, amanchadha, tasnim, mubtasim | DM-Codec is a novel speech tokenizer that leverages a neural codec architecture with Residual Vector Quantization (RVQ) and incorporates two novel distillation approaches: LM-guided and combined LM and SM-guided distillation. - The LM-guided approach distills contextual representations from a Language Model (LM) and integrates them with acoustic representations, while the combined approach incorporates semantic representations from a Speech Model (SM) along with contextual and acoustic representations.  - DM-Codec adopts a streamlined encoder-decoder framework enhanced by a multi-discriminator setup comprising Multi-Scale, Multi-Period, and Multi-Scale Short-Time Fourier Transform discriminators. - Experimental results on the LibriSpeech benchmark demonstrate that DM-Codec significantly outperforms state-of-the-art models, reducing Word Error Rate (WER) by up to 13.46%, Word Information Lost (WIL) by 9.82%, and improving speech quality and intelligibility.  - The combined distillation approach results in a WER of 4.05 and a WIL of 6.61, surpassing existing speech tokenization models. | ['Audio', 'Automatic Speech Recognition', 'Multimodal'] | [Link](https://github.com/mubtasimahasan/DM-Codec) | N/A |
