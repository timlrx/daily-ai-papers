

## Papers for 2024-10-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/abs/2410.17247) | lindahua, jiaqiwang-rex, conghui, yhcao, yuhangzang |  - PyramidDrop is a novel visual redundancy reduction strategy for Large Vision-Language Models (LVLMs) designed to accelerate training and inference. - It partitions the LVLM into stages and progressively drops image tokens at each stage's end based on a lightweight similarity calculation with the instruction's last token.  - This pyramid-like token reduction leverages the observation that token redundancy increases in deeper LVLM layers. - Experiments on LLaVA-NeXT-7B show 40% training time and 55% inference FLOPs reduction without performance loss on 15 vision-language tasks.  - PyramidDrop also allows training with doubled resolution using only 70% of the original training time and serves as a plug-and-play inference acceleration strategy outperforming existing methods. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/Cooperx521/PyramidDrop) | N/A |
| [Aligning Large Language Models via Self-Steering Optimization](https://arxiv.org/abs/2410.17131) | Jingren, xphan, luyaojie, keminglu, sanmusunrise |   - This paper introduces Self-Steering Optimization (SSO), an algorithm designed for automated alignment of large language models (LLMs), eliminating the need for manual annotation.  - SSO autonomously generates preference signals by prompting the policy model with contrastive principles and optimizing based on three objectives: steering the model towards chosen responses, maintaining on-policy behavior, and ensuring a consistent quality gap between responses. -  Experiments conducted on Qwen2 and Llama3.1 demonstrate SSO's ability to generate accurate and learnable signals, leading to significant performance improvements across various benchmarks without manual annotation or external models. - SSO enhanced the training of reward models using data generated during the alignment process, further highlighting its effectiveness. - This work contributes a scalable approach to preference optimization for more efficient and effective automated alignment. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/icip-cas/SSO) | N/A |
| [JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://arxiv.org/abs/2410.17250) | Yuki Imajuku, gneubig, ku21fan, AtsuMiyai, shtapm | JMMMU is a new large-scale Japanese benchmark dataset designed to evaluate Large Multimodal Models (LMMs) focusing on Japanese cultural understanding. - It comprises two subsets: a Culture-Agnostic (CA) subset, containing translations of culture-independent components from the MMMU benchmark, and a Culture-Specific (CS) subset with newly crafted questions related to Japanese culture. - The benchmark is significantly larger than existing culture-aware Japanese benchmarks, totaling 1,320 questions with 1,118 images across a diverse range of 28 subjects. - An evaluation of 15 open-source and 3 proprietary LMMs reveals up to 58.6% overall accuracy, indicating significant room for improvement in utilizing the Japanese context. - The results indicate that many LMMs perform worse on questions in Japanese compared to their English counterparts and highlight the importance of culture-specific evaluation. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/cyberagent/llava-calm2-siglip), [Link](https://huggingface.co/datasets/SakanaAI/JA-Multi-Image-VQA), [Link](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500), [Link](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild) |
| [EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search](https://arxiv.org/abs/2410.14649) | dalistarh, ekurtic, SpiridonSunRotator, OliverSieberling | • EvoPress, a new evolutionary search approach for dynamic compression of large language models (LLMs), is introduced, offering provable convergence and low sample and iteration complexity. • EvoPress challenges the assumption of error monotonicity in LLM compression, demonstrating instances where lower per-layer error sums do not translate to better overall performance. • This method improves upon existing layer dropping, unstructured sparsity, and quantization techniques, setting new state-of-the-art results. • It achieves significant improvements, particularly at higher compression ratios, across various LLM families. • EvoPress converges efficiently, often within hours on a single GPU, even for large models, and a lightweight version is available for faster processing. | ['Natural Language Processing', 'Text Generation', 'Feature Extraction'] | [Link](https://github.com/IST-DASLab/EvoPress) | N/A |
| [MiniPLM: Knowledge Distillation for Pre-Training Language Models](https://arxiv.org/abs/2410.17215) | Minlie Huang, Jie Zhou, Hao Zhou, fandong, t1101675 |  - MINIPLM is a new Knowledge Distillation (KD) framework for pre-training Language Models (LMs) that refines the training data distribution using a teacher LM's knowledge. - It addresses the efficiency, flexibility, and effectiveness challenges of existing KD methods during pre-training through offline teacher inference, corpus-based operation, and a Difference Sampling technique that leverages the discrepancies between large and small LMs. - Experiments across various student LM sizes show that MINIPLM improves performance on 9 downstream tasks, language modeling capabilities, and reduces pre-training computation by 2.2 times compared to Vanilla KD which achieves similar performance but with more compute. - MINIPLM also supports KD across model families with different tokenizations, unlike existing online KD methods. - Further analysis suggests that MINIPLM improves pre-training data utilization, reducing the data demand by 2.4 times. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/MiniPLM) | N/A |
| [Mitigating Object Hallucination via Concentric Causal Attention](https://arxiv.org/abs/2410.15926) | Shijian Lu, Ivan Laptev, Yiheng Li, xing0047 | - This paper introduces Concentric Causal Attention (CCA), a novel position alignment strategy for Large Vision-Language Models (LVLMs) designed to mitigate object hallucination, a phenomenon where LVLMs generate text responses misaligned with image content.  - CCA addresses the limitations of Rotary Position Encoding (ROPE), commonly used in LVLMs, where long-term decay in attention can lead to hallucination.  - The method reorganizes visual tokens in a concentric manner, reducing the relative distance between visual and instruction tokens and improving spatial locality. It also introduces a modified causal attention mask to support the 2-D structure of image data.  - Experimental results on benchmarks like POPE, CHAIR, and MME demonstrate that CCA surpasses existing debiasing methods, improving accuracy and reducing hallucination.  - CCA also enhances the overall perception capability of LVLMs in multiple-choice visual question answering tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/xing0047/cca-llava.git) | N/A |
| [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930) | Thomas Hartvigsen, Jonathan Kropko, Zack Gottesman, Bryan R. Christ |  - MathNeuro is introduced; a method for isolating math-specific parameters in LLMs using forward passes, building upon existing work by calculating parameter importance with weights and activations, but with the key innovation of removing parameters also important for general language tasks measured on non-math datasets. - Pruning MathNeuro-identified parameters eliminates a LLM's math reasoning ability, while the impact on other tasks is similar to pruning random parameters. - Scaling up MathNeuro-identified parameters by a small constant (1.1 for smaller models and 1.01 for larger models) improves performance on GSM8K by 4-17% without affecting non-math performance. - MathNeuro remains effective with a single sample for parameter identification, demonstrating its data efficiency. - Math-specific parameters are distributed across the model's decoder blocks, suggesting math reasoning is not localized. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/bryanchrist/MathNeuro) | N/A |
