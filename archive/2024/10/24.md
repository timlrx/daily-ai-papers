

## Papers for 2024-10-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models](https://arxiv.org/abs/2410.17637) | conghui, KennyUTC, yhcao, yuhangzang, ziyuliu | **- MIA-DPO: a novel Multi-Image Augmented Direct Preference Optimization (DPO) framework, designed to enhance the multi-image understanding of Large Vision-Language Models (LVLMs).** **- MIA-DPO addresses the scarcity of diverse multi-image training data and high annotation costs by augmenting existing single-image data with noisy or unrelated images arranged in grid collages or pic-in-pic formats, reducing the need for manual annotation of multi-image data.** **- This framework leverages an attention-aware selection mechanism that filters out rejected responses by analyzing the attention value distribution across multiple images, allowing for automated, cost-effective, and scalable DPO data construction without relying on manual annotations or expensive APIs.** **- Experimental results demonstrate that MIA-DPO consistently outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on InternLM-XC2.5.** **- MIA-DPO maintains competitive performance on single-image tasks while boosting the performance on multi-image tasks, demonstrating its robustness across various architectures and its ability to handle both single and multiple images effectively.** | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Liuziyu77/MIA-DPO) | N/A |
| [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891) | Jiacheng Ye, Yizhe Zhang, kiaia, shivamag99, Sansa |  - This paper introduces a novel approach to scaling Diffusion Language Models (DLMs) by adapting pre-trained autoregressive (AR) language models like GPT2 and LLaMA. - The adaptation method bridges the gap between AR and DLM objectives through attention mask annealing to remove causal masking bias and inheriting the shift operation from AR models. - The resulting models, DiffuGPT and DiffuLLaMA (up to 7B parameters), are trained on less than 200B tokens and evaluated on various benchmarks, demonstrating competitive performance with their AR counterparts and state-of-the-art results among existing DLMs. - DiffuLLaMA showcases promising in-context learning and infilling abilities. - The models and training code are released to facilitate further DLM research.   | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/HKUNLP/DiffuLLaMA) | N/A |
| [Lightweight Neural App Control](https://arxiv.org/abs/2410.17883) | Jianye Hao, ShaoKun-HW, Fahren24, gpap, semitable |  - This paper introduces Lightweight Multi-modal App Control (LiMAC), a novel mobile phone control architecture designed for efficient interaction and control across various Android apps.  - LiMAC combines a small Action Transformer (AcT) with a fine-tuned vision-language model (VLM) to process textual goals and past mobile observations (screenshots, UI trees) and generate precise actions.  -  AcT predicts action types (click, scroll, input text) and executes straightforward interactions, while the VLM handles complex text generation tasks (composing messages, search queries).   - Experimental results on two mobile control datasets show LiMAC significantly outperforms fine-tuned open-source VLMs (Florence2, Qwen2-VL) and prompt engineering baselines using GPT-40, increasing overall action accuracy by up to 19% and 42% respectively.   - LiMAC also executes tasks 30 times faster than GPT-40 methods, making it more suitable for real-time mobile applications. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [MedINST: Meta Dataset of Biomedical Instructions](https://arxiv.org/abs/2410.13458) | Zirui Song, Yu Yin, Zihan Zhang, Meng Fang, Wenhan Han | • This paper introduces MEDINST, a large biomedical instruction meta-dataset comprising 133 tasks and over 7 million training examples spanning 12 distinct categories. • The authors curate MEDINST32, a benchmark derived from MEDINST consisting of 32 tasks with varying difficulty to evaluate large language models' (LLMs) generalization abilities in the biomedical domain.  • Several LLMs are fine-tuned on MEDINST and show improved generalization performance across various biomedical tasks, as evaluated on MEDINST32.  • The study finds that instruction fine-tuning is more effective than further pre-training on domain-specific data for adapting general LLMs to the biomedical domain. • Experimental results on MEDINST32 reveal that the models often struggle with generalization to new tasks when only fine-tuned on smaller datasets or in limited task formats, highlighting the value of large, comprehensive datasets. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification', 'Summarization', 'Translation'] | [Link](https://github.com/aialt/MedINST) | N/A |
| [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/abs/2410.15522) | Drishti Sharma, Rishabh Maheshwary, Lester James V. Miranda, shayekh, srishti-hf1110 | • This paper introduces M-REWARDBENCH, a multilingual benchmark for evaluating reward models (RMs) across 23 languages and six tasks. • M-REWARDBENCH consists of 2.87k preference instances covering chat, safety, reasoning, and translation capabilities. • Evaluation results show a significant performance gap between English and non-English languages, with RMs exhibiting higher performance in English and variations across different languages. • The analysis indicates that translation quality positively impacts RM performance, with better translations leading to improved accuracy. • The authors also explore the sensitivity of different RM types to translation quality and analyze the performance variations across different language families and scripts. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://hf.co/datasets/C4AI-Community/multilingual-reward-bench) |
| [TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts](https://arxiv.org/abs/2410.18071) | Tianhua Li, Yuxuan Xie, kpzhang, wqshao126 | TP-Eval is a new evaluation framework for Multimodal Large Language Models (MLLMs) that addresses the issue of prompt sensitivity, where minor prompt variations can lead to significant performance fluctuations, resulting in underestimation or bias in evaluation. - It introduces a prompt customization method using an automatic prompt optimizer, tailored for MLLMs, to generate optimal prompts for each model, tapping their full potential. - This optimizer leverages a scorer, composed of the target MLLM and an answer analyzer, to iteratively refine prompts based on accuracy, semantic similarity to the original prompt, and introspection from incorrect responses. - Experiments on MMT-Bench and MMMU datasets demonstrate that TP-Eval effectively reduces underestimation and bias, revealing models' true capabilities and facilitating fairer comparisons. - TP-Eval also shows promising results in zero-shot settings using in-context learning, enabling prompt optimization even with limited data. | ['Multimodal'] | N/A | N/A |
