

## Papers for 2024-10-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting](https://arxiv.org/abs/2410.17856) | Xiaojian Ma, Zhancun Mu, Zihao Wang, kevinLian, phython96 | ROCKET-1 is a novel, low-level policy that leverages visual-temporal context prompting, a communication protocol using object segmentation masks and interaction types from past and present observations to guide policy-environment interactions. ROCKET-1 uses a causal transformer architecture that processes observations and object segmentations concatenated into a 4-channel image along with interaction types as conditions. Experiments in Minecraft demonstrate that agents using this approach achieve higher success rates on complex tasks, outperforming methods based on language, future image, or latent code prompting. A backward trajectory relabeling method efficiently generates segmentation annotations, enabling automatic dataset creation for training ROCKET-1. The approach allows for spatial understanding in embodied decision-making, leading to agents accomplishing previously unattainable tasks like “place oak door on diamond block” with a 91% success rate and others requiring long-horizon planning such as obtaining obsidian with a 70% success rate. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [Continuous Speech Synthesis using per-token Latent Diffusion](https://arxiv.org/abs/2410.16048) | Hagai Aronowitz, Slava Shechtman, Arnon Turetzky, Avihu, NimrodShabtay1986 |  - This paper introduces SALAD, a per-token latent diffusion model for zero-shot text-to-speech that operates on continuous representations, inspired by the per-token diffusion head for image generation. - It extends the image generation method to handle variable-length outputs, uses semantic tokens for context and stopping conditions, and doesn't require text-audio alignment. - Three SALAD variants are proposed: T2A (Text2Acoustic), S2A-AR (Semantic2Acoustic Autoregressive), and S2A-NAR (Semantic2Acoustic Non-Autoregressive), along with corresponding discrete baseline models for comparison. -  Evaluations on speech quality, intelligibility, and speaker similarity show that SALAD's T2A model achieves the highest intelligibility score. -  It also maintains speech quality and speaker similarity comparable to ground-truth audio based on subjective listening tests. | ['Text-to-Speech', 'Audio'] | N/A | N/A |
| [Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data](https://arxiv.org/abs/2410.18558) | Jialing Zhang, Shuhao Gu, ZacLiu, bowen92, ldwang | {- Introduced Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through quality filtering and deduplication.  - Proposed a synthetic instruction generation method using open-source VLMs, detailed image annotations, and diverse question generation to improve data quality and scale.  - Trained Aquila-VL-2B, a 2-billion parameter VLM based on the LLaVA-OneVision architecture, using Infinity-MM and synthetic data.  - Aquila-VL-2B achieved state-of-the-art performance for models of similar scale on various visual benchmarks, including MMBench, MMStar, and MathVista.  - Demonstrated that scaling instruction data and generating synthetic data can significantly improve the performance of open-source multimodal models.} | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main/scripts/train) | N/A |
| [Teach Multimodal LLMs to Comprehend Electrocardiographic Images](https://arxiv.org/abs/2410.19008) | Ping Zhang, Xiang Yue, Yuelin Bai, Ruoqi Liu | - This paper introduces PULSE, a new Multimodal Large Language Model (MLLM) tailored for electrocardiogram (ECG) image comprehension.  - It also presents ECGInstruct, a new instruction tuning dataset of over one million ECG image-text samples featuring realistic image synthesis and a diverse range of ECG-related tasks.   - A new evaluation benchmark, ECGBench, covering four key ECG image interpretation tasks across nine different datasets is also constructed.  - PULSE achieves state-of-the-art results, significantly outperforming proprietary MLLMs such as GPT-40 by 15-30% accuracy on out-of-domain datasets. - Ablation studies highlight the importance of diverse data sources and incorporating instruction tasks for ECG image comprehension. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Image-to-Text', 'Computer Vision'] | N/A | [Link](https://aimedlab.github.io/PULSE/) |
| [MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark](https://arxiv.org/abs/2410.19168) | Ramaneswaran Selvakumar, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, S Sakshi | - MMAU, a massive multi-task audio understanding and reasoning benchmark, is introduced to evaluate expert-level reasoning and knowledge retrieval abilities in Large Audio-Language Models (LALMs).  - It consists of 10,000 expertly annotated audio-question-response pairs across speech, sound, and music domains, covering 27 distinct tasks, including 16 for reasoning and 11 for information extraction.  - Evaluations of 18 open-source and proprietary LALMs reveal that even the best-performing model only achieves 53% accuracy on MMAU, with human performance at 82%, highlighting significant room for improvement. - Models performed best on sound-based tasks but struggled the most with music. Cascaded models employing audio captioning followed by an LLM achieved the best performance, suggesting the potential for independent advancements in audio perception and text-based reasoning. - A detailed error analysis highlights perceptual errors as the most common mistake, emphasizing the need for better audio processing capabilities in current models. | ['Audio', 'Multimodal'] | N/A | N/A |
| [Counting Ability of Large Language Models and Impact of Tokenization](https://arxiv.org/abs/2410.19730) | Chenyu You, Juntai Cao, Wyattz23 | This paper investigates the impact of tokenization on the counting abilities of Large Language Models (LLMs), demonstrating that tokenization choices significantly influence model performance on counting tasks. - The study adopts a model-agnostic approach, manipulating input string formats to control tokenization in both open and closed-source LLMs. - It is observed that byte-pair encoding (BPE), commonly used in LLMs, can severely degrade counting accuracy due to a mismatch between the unit being counted (letters) and the unit processed (tokens). - The research reveals that Chain-of-Thought (CoT) prompting significantly improves counting abilities by enabling iterative inductive reasoning in the text space, partially overcoming the inherent limitations of Transformer models in sequential computations. - Through extensive experiments, the study finds that clear item-separated tokenization, as opposed to letter-grouped tokenization, enhances counting accuracy. Furthermore, the experiments showed that lower-frequency characters are easier to count compared to higher-frequency ones. | ['Natural Language Processing'] | N/A | N/A |
| [Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning](https://arxiv.org/abs/2410.19290) | Yang Zhang, Tommi Jaakkola, code-terminator, yujianll | - PREREQ-TUNE, a novel fine-tuning strategy designed to mitigate LLM hallucinations, is introduced. - PREREQ-TUNE incorporates a two-stage process: a prerequisite learning stage where a knowledge LoRA is trained to acquire necessary knowledge, followed by a supervised fine-tuning (SFT) stage where a skill LoRA focuses solely on learning task-specific skills.  The prerequisite learning stage enhances factuality by equipping the LLM with the required knowledge for subsequent fine-tuning, thereby reducing reliance on generating incorrect information. - The method also utilizes fictitious synthetic data for multi-version training, further improving the grounding of LLM outputs to internal knowledge.  This decoupling of knowledge and skill learning allows for more robust factual generation and control. - Experiments on long-form generation (biography and medical QA) and short QA tasks demonstrate PREREQ-TUNE's superior performance compared to baselines, including those utilizing reinforcement learning and direct preference optimization. - Analysis confirms the effectiveness of PREREQ-TUNE's disentanglement mechanism, even when trained solely on fictitious data, opening possibilities for new retrieval augmented generation (RAG) paradigms and knowledge-controlled text generation. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/UCSB-NLP-Chang/Prereq_tune.git) | N/A |
| [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133) | Valentina Pyatkin, Sachin Kumar, Yanai Elazar, Yizhong Wang, ljvmiranda921 |  - This paper introduces a routing framework for preference learning that dynamically allocates instances to either human or LM annotators, creating a hybrid approach to data annotation. - The framework employs a performance prediction model (PPM) to estimate the performance of reward models trained on different mixes of human and LM annotations and uses this to strategically select an optimal combination. - Results on the MULTIPREF dataset and others show that the proposed hybrid preference approach significantly outperforms using either human or LM preferences exclusively, as well as random combinations, across several benchmarks. - Analysis of the framework highlights that instances with moderate semantic similarity, safety concerns, or intent complexity tend to benefit the most from human annotation. - The authors release the code, data, and annotation platform used to promote further research in efficient and effective preference data collection. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/allenai/hybrid-preferences) | [Link](https://hf.co/datasets/allenai/multipref) |
| [Reflection-Bench: probing AI intelligence with reflection](https://arxiv.org/abs/2410.16270) | Yan Teng, Shuqi Kong, Haiquan Zhao, Yixu Wang, LingyuLi | This paper introduces Reflection-Bench, a new benchmark designed to evaluate the reflection capabilities of Large Language Models (LLMs). - Reflection is defined as the ability of an intelligent system to adapt its beliefs or behaviors in response to unexpected outcomes, encompassing core cognitive functions such as perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. - The benchmark comprises seven tasks adapted from cognitive science paradigms, including the oddball paradigm, n-back task, probabilistic reversal learning task, Wisconsin card sorting test, weather prediction task, double-choice Iowa gambling task, and meta-bandit task. - An evaluation of 13 prominent LLMs reveals that current models still fall short of human-level reflection abilities, particularly lacking meta-reflection capabilities.  - The authors argue that reflection is a crucial aspect of intelligence and propose Reflection-Bench as a valuable tool for evaluating and furthering the development of more sophisticated AI systems. | ['Natural Language Processing'] | [Link](https://github.com/YabYum/ReflectionBench) | N/A |
