

## Papers for 2024-10-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation](https://arxiv.org/abs/2410.18565) | Remek, adgw, djstrong, lflis, chrisociepa | - This paper introduces Bielik 7B v0.1, a 7-billion parameter generative text model based on the Mistral 7B v0.1 architecture and trained on a curated Polish corpora. - The model utilizes techniques such as Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, and incorporates architectural features like Sliding Window Attention and SwiGLU activation function for enhanced performance. - To evaluate the model, new benchmark frameworks, the Open PL LLM Leaderboard and Polish MT-Bench, were created for assessing NLP tasks and conversational abilities. - Bielik 7B v0.1 showed a significant improvement of 9 percentage points in the RAG Reader task compared to Mistral-7B-v0.1. - In subjective conversational evaluations, Bielik outperformed models with higher average scores on the Open PL LLM Leaderboard benchmarks. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/spaces/speakleash/mt-bench-pl), [Link](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), [Link](https://huggingface.co/datasets/teknium/OpenHermes-2.5), [Link](https://huggingface.co/spaces/speakleash/open_pl_llm_leaderboard) |
| [AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant](https://arxiv.org/abs/2410.18603) | Fangzhi Xu, Qiushi Sun, Zhuohang Dang, Minnan Luo, Chengyou Jia | - AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents, has been introduced for automating diverse computer tasks. - It leverages a novel MetaAgent with an AgentToken strategy, enabling efficient management of diverse agents by representing each agent as a learnable token embedding and predicting the appropriate token(s) for task execution. - AgentStore allows for seamless third-party agent integration, enabling adaptability to evolving operating systems. - Evaluation on OSWorld and a mobile environment demonstrate its ability to improve performance in automating computer tasks, achieving a success rate of 23.85% on OSWorldâ€”more than double the previous best (11.21%). - AgentStore's ability to integrate agents and specialize them for specific tasks while maintaining general capabilities demonstrates significant improvement over single generalist or specialized agents in handling complex tasks within real-world OS environments. | ['Multimodal'] | N/A | N/A |
| [GPT-4o System Card](https://arxiv.org/abs/2410.21276) | Adam Perelman, Adam P. Goucher, Adam Lerer, Aaron Hurst, OpenAI |  - OpenAI's GPT-40 is an "omni" autoregressive model that accepts and generates combinations of text, audio, image, and video, trained end-to-end across these modalities. - GPT-40 matches GPT-4 Turbo's performance on English text and code, surpasses it in non-English languages, and demonstrates significant improvement on vision and audio understanding. - The model's training data includes publicly available data, code and math data, multimodal data (images, audio, and video), and proprietary data from partnerships, with a cutoff date of October 2023. - Prior to deployment, OpenAI performed risk assessments and mitigations with methods including safety classifiers, content filtering, and preference alignment to reduce harms such as information hazards, bias, and policy violations. - Deployment preparation encompassed a four-phased external red teaming process with over 100 participants to evaluate risks and test mitigations across multiple modalities and potential harms such as disallowed content and misinformation. | ['Multimodal', 'Any-to-Any', 'Audio', 'Automatic Speech Recognition', 'Text-to-Speech', 'Text-to-Audio', 'Computer Vision', 'Image-to-Text', 'Image Classification', 'Object Detection', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169) | Zhengren Wang, Junyuan Zhang, Bin Wang, Victor Shea-Jay Huang, Qintong Zhang |  - This survey paper provides a comprehensive overview of document parsing, consolidating recent advancements in modular pipeline systems and end-to-end models driven by large vision-language models (VLMs) and covering key methodologies, challenges, and future research directions. - The paper discusses core document parsing components, including layout detection, content extraction (text, tables, mathematical expressions), and multimodal data integration, examining algorithms for each stage. - It addresses the challenges faced by modular document parsing systems and VLMs in handling complex layouts, integrating modules, and recognizing high-density text. - The survey consolidates widely used datasets and evaluation metrics for document parsing tasks, providing valuable resources for researchers and practitioners. - Finally, the paper emphasizes the importance of developing larger, more diverse datasets and outlines future research directions in the field, such as handling complex layouts and improving OCR for densely packed text. | ['Natural Language Processing', 'Document Question Answering', 'Computer Vision', 'Object Detection'] | N/A | N/A |
| [LongReward: Improving Long-context Large Language Models with AI Feedback](https://arxiv.org/abs/2410.21252) | Zhenyu Hou, Shulin Cao, Xin Lv, Zhongni Hou, Jiajie Zhang | - LongReward, a novel method to improve long-context large language models (LLMs) using AI feedback, is introduced. - It uses an off-the-shelf LLM to assign rewards to model responses based on four dimensions: helpfulness, logicality, faithfulness, and completeness. - When combined with the reinforcement learning algorithm Direct Preference Optimization (DPO), LongReward significantly boosts the performance of long-context SFT models, outperforming baseline methods. - Experiments show improvements on long-context question answering and summarization and a positive impact on short instruction following. - LongReward enhances model capabilities by mitigating common issues like hallucinations and ineffective context utilization in long-context scenarios. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/THUDM/LongReward) | N/A |
| [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011) | Samyadeep Basu, Yu Xia, Ryan Aponte, Xuan Shen, Chien Van Nguyen | - This paper presents a comprehensive survey of Small Language Models (SLMs), focusing on architectures, training techniques, and model compression methods. - The authors introduce a novel taxonomy to categorize SLM optimization methods, considering techniques used in pre-processing, training, post-processing, and the constraints being optimized (e.g., inference compute, training time). - The survey covers lightweight architectures, efficient self-attention approximations, neural architecture search for model building, efficient pre-training and fine-tuning strategies, and model compression techniques like pruning, quantization, and knowledge distillation. - Additionally, it summarizes benchmark datasets and evaluation metrics commonly used for assessing SLM performance and lists various real-world applications enabled by SLMs, categorized by constraints like real-time interaction, content generation, edge inference, and privacy. - Lastly, the paper highlights important open challenges and future research directions for SLMs, such as hallucination, bias, inference-time energy efficiency, and data privacy. | ['Natural Language Processing'] | N/A | N/A |
| [COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training](https://arxiv.org/abs/2410.19313) | Kurt Keutzer, Yao Lu, Ligeng Zhu, Han Cai, Haocheng Xi | - COAT is a novel FP8 training framework designed to reduce memory footprint and increase training speed for large models by compressing both optimizer states and activations. - It introduces Dynamic Range Expansion, aligning optimizer state distributions with FP8's range, thereby minimizing quantization error. - For activations, COAT proposes Mixed-Granularity Activation Quantization, using fine-grained quantization for non-linear layers and per-tensor quantization for linear layers. - COAT achieves nearly lossless performance while decreasing memory by 1.54x and increasing training speed by 1.43x on Llama 7B, 13B, and 30B models compared to BF16. - COAT facilitates training larger models on fewer GPUs by enabling full-parameter training of 7B models on a single GPU and supports doubling the micro-batch size for distributed training. | ['Natural Language Processing', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/NVlabs/COAT) | N/A |
| [Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines](https://arxiv.org/abs/2410.21220) | Xiangyu Yue, Xiaohan Ding, Yiyuan Zhang, Zhixin Zhang | - Vision Search Assistant, a novel framework to address the limitation of traditional methods in understanding unfamiliar visual content.  - The framework facilitates collaboration between VLMs and web agents, leveraging VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. - By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system.  - It involves Visual Content Formulation to represent visual content with correlated formulations, Web Knowledge Search with Chain of Search algorithm to obtain comprehensive web knowledge, and Collaborative Generation to generate the final answer.  - Extensive experiments on open-set and closed-set QA benchmarks demonstrate that Vision Search Assistant significantly outperforms other models. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/cnzzx/VSA) | [Link](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard), [Link](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b) |
| [Fast Best-of-N Decoding via Speculative Rejection](https://arxiv.org/abs/2410.20290) | Jiahao Qiu, Huitao Yang, Ruiqi Zhang, Momin Haider, Hanshi Sun | - This paper introduces Speculative Rejection, a novel inference-time alignment algorithm designed to improve the efficiency of Best-of-N decoding for large language models (LLMs). - The key idea is to dynamically reduce the batch size during generation by halting the generation of unpromising responses early, based on partial reward scores. - The algorithm starts with a large batch size, effectively simulating Best-of-N with large N and leverages a reward model to rank partial utterances and terminate low-scoring ones. - The results on the AlpacaFarm dataset demonstrate that Speculative Rejection can achieve higher rewards with similar latency while requiring significantly fewer GPUs (16-32 times less compute power) compared to standard Best-of-N.  - The method is also shown to be effective in maximizing the probability of generated utterances. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Zanette-Labs/SpeculativeRejection) | N/A |
| [Language Models And A Second Opinion Use Case: The Pocket Professional](https://arxiv.org/abs/2410.20636) | David Noever |  - This research assesses Large Language Models (LLMs) as second opinion tools in complex medical and legal scenarios. - Evaluated LLM performance on 183 medical cases from Medscape and 21 Supreme Court legal cases, comparing responses with crowd-sourced physician opinions and documented legal votes respectively. - Found high accuracy in straightforward medical cases (>81%) but reduced performance (43%) in complex scenarios, suggesting LLMs may be valuable for generating differential diagnoses rather than as primary diagnostic tools. - Developed novel benchmarks for others to assess the reliability of responses by both LLMs and human practitioners, revealing high contestation among human experts. - Suggests that using LLMs as specialized agents for second opinions in medicine, especially in challenging cases, might be more appropriate than current approaches that emphasize automation of routine tasks. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/reveondivad/certify) | N/A |
