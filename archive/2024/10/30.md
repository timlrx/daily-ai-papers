

## Papers for 2024-10-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CLEAR: Character Unlearning in Textual and Visual Modalities](https://arxiv.org/abs/2410.18057) | Denis Bobkov, Boris Mikheev, Alexey Zhavoronkin, Dmitrii Korzh, therem | - Introduces CLEAR, a multimodal benchmark for evaluating machine unlearning (MU) in textual and visual modalities, focusing on removing information about specific individuals. - The benchmark includes a synthetic dataset of 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs, along with real-world face and visual question answering datasets for evaluating model retention. - Evaluates 10 existing MU methods adapted for multimodal unlearning, revealing that current state-of-the-art algorithms struggle in multimodal settings. - Demonstrates that simple L1 regularization on LoRA adapter weights during unlearning significantly mitigates catastrophic forgetting, improving the preservation of model performance on retained data. - Makes the dataset publicly available to encourage further research in the field. | ['Multimodal', 'Computer Vision', 'Natural Language Processing', 'Image-to-Text', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/therem/CLEAR) |
| [SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization](https://arxiv.org/abs/2410.21411) | Chuang Gan, Donglai Wei, Jiawei Zhou, zmeng0116, EthanTaylor | - SocialGPT is a novel modular framework that leverages Vision Foundation Models (VFMs) and Large Language Models (LLMs) for social relation reasoning. - It employs VFMs to translate image content into a textual "social story" and utilizes LLMs for reasoning based on the generated story and provided bounding boxes. - This framework incorporates systematic design principles to enhance the collaboration between VFMs and LLMs, including comprehensive and domain-specific visual information extraction and a structured reasoning prompt named SocialPrompt. - SocialGPT achieves competitive zero-shot performance on PIPA and PISC datasets, outperforming previous state-of-the-art supervised methods on PIPA by 1.4%. - The framework also introduces Greedy Segment Prompt Optimization (GSPO) for automatic prompt tuning, resulting in significant performance improvements across various LLMs. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Classification'] | [Link](https://github.com/Mengzibin/SocialGPT) | N/A |
| [OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization](https://arxiv.org/abs/2410.19609) | Hongming Zhang, Wenhao Yu, Kaixin Ma, Wenlin Yao, Hongliang He | - This paper introduces OpenWebVoyager, an open-source framework for building multimodal web agents that can explore real-world websites, receive feedback, and iteratively optimize their performance. - The agent architecture adapts the Idefics2-8b-instruct model, processing observations consisting of webpage screenshots and accessibility trees. -  OpenWebVoyager combines imitation learning from a GPT-40 powered web agent with an exploration-feedback-optimization cycle, where GPT-40 evaluates the agent's trajectory success. - Across multiple iterations on the WebVoyager and Mind2Web datasets, the agent shows improvement in task success rate, starting from 19.9% to 25.8% on the WebVoyager test set and 6.3% to 19.6% on the Mind2Web cross-task test set. - The results indicate that the iterative real-world exploration and optimization method is an effective way to improve the agent's real-world performance. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/MinorJerry/OpenWebVoyager) | N/A |
| [Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning](https://arxiv.org/abs/2410.22304) | Paul Mineiro, ydeng9 | - This paper introduces Flow-DPO, a novel approach to improve Large Language Model (LLM) mathematical reasoning by generating high-quality reasoning traces through online multi-agent learning flows. - The method employs an incremental output production flow composed of multiple LLMs that iteratively communicate to construct solutions, similar to a multi-agent system. - The flow is trained using online Direct Preference Optimization (DPO) with rollouts, generating DPO pairs for each training example during answer chunk generation and updating the models in real-time. - Experimental results on MetaMath, GSM8K, and MATH datasets demonstrate that Flow-DPO generates higher-quality reasoning traces compared to direct model inference, leading to improved performance in mathematical reasoning tasks after supervised fine-tuning. - This improvement is particularly significant for the Llama-3-8B-instruct model, achieving a 20% improvement in validation accuracy on mathematical reasoning tasks during training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465) | Ningxin Zheng, Size Zheng, Wenlei Bao, Li-Wen Chang, preminstrel | - SHADOWKV is a novel LLM inference system designed for enhanced throughput in long-context scenarios by storing a low-rank representation of the key cache on the GPU and offloading the value cache to the CPU. - It employs a precise KV selection strategy during decoding, utilizing landmarks and static outliers to minimize the sparse KV cache budget while maintaining accuracy. - Evaluations on benchmarks like RULER, LongBench, and Needle in a Haystack with various LLMs (Llama, GLM, Yi, Phi, Qwen) show that SHADOWKV can handle contexts up to 1M tokens. - It achieves up to a 6x increase in batch size and a 3.04x boost in throughput compared to full attention on an A100 GPU. - SHADOWKV's performance even surpasses the theoretical throughput of infinite batch size with full attention, assuming infinite GPU memory. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/bytedance/ShadowKV) | N/A |
