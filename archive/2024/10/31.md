

## Papers for 2024-10-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation](https://arxiv.org/abs/2410.23090) | Hongjin Qian, Ziliang Zhao, Kelong Mao, dongguanting, ariya2357 | - This paper introduces CORAL, a large-scale benchmark designed to evaluate Retrieval-Augmented Generation (RAG) systems in multi-turn conversational settings. - CORAL is derived from Wikipedia, containing 8,000 information-seeking conversations covering various topics with citation labels. - It includes three tasks: passage retrieval, response generation, and citation labeling and proposes a framework to standardize different RAG methods. - Evaluations show that fine-tuned open-source LLMs outperform commercial closed-source LLMs in retrieval, and that input length reduction maintains response quality and improves citation accuracy. - The benchmark addresses challenges in multi-turn conversational RAG, such as redundant information and topic shifts, paving the way for evaluating and improving multi-turn conversational RAG systems. | ['Question Answering'] | [Link](https://github.com/Ariya12138/CORAL) | N/A |
| [Stealing User Prompts from Mixture of Experts](https://arxiv.org/abs/2410.22884) | Nicholas Carlini, Jamie Hayes, Ilia Shumailov, Itay Yona | - This paper introduces a novel attack, called MoE Tiebreak Leakage, which exploits a vulnerability in Mixture-of-Experts (MoE) models to extract user prompts. - The attack leverages the Expert-Choice-Routing (ECR) strategy, manipulating the order of inputs within a batch to cause predictable token dropping, thereby revealing information about a victim's prompt. - The authors successfully demonstrate the attack on a two-layer Mixtral model, extracting almost all secret messages (996/1000) across varying lengths (1-11 tokens), and achieving 99.9% success in recovering individual tokens (4833/4838). - The attack's complexity is O(VM²) for the number of queries to the target model and O(2DNVM²) for queries to a local model copy, where V is vocabulary size, M is prompt length, D is the number of layers, and N is the number of experts. - The paper discusses potential mitigations, including preserving in-batch data independence and introducing stochasticity into the model's routing or capacity parameters. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels](https://arxiv.org/abs/2410.20050) | Xiao Zhou, Xiangxu Zhang, Lei Li, zl101 | - This paper introduces SL-HyDE (Self-Learning Hypothetical Document Embeddings), a novel approach for zero-shot medical information retrieval that eliminates the need for labeled data. - SL-HyDE leverages LLMs to generate hypothetical documents from user queries, and utilizes a retriever to find relevant documents based on these hypothetical documents. - It employs a self-learning mechanism to enhance both LLM document generation and retriever performance without relying on labeled medical data. - A new benchmark for Chinese Medical Information Retrieval (CMIRB) is introduced, consisting of five tasks and ten datasets derived from real-world scenarios. - Experimental results on CMIRB show SL-HyDE surpasses HYDE by 4.9% in NDCG@10 and demonstrates performance gains across various LLM and retriever combinations. | ['Question Answering'] | [Link](https://github.com/CMIRB-benchmark/CMIRB) | N/A |
| [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://arxiv.org/abs/2410.23168) | Jan Eric Lenssen, Yongqin Xian, Muhammad Ferjad Naeem, Yue Fan, Haiyang Wang | - TokenFormer, a novel fully attention-driven neural network architecture, is introduced, which treats model parameters as tokens, enhancing flexibility in token-parameter computations. - By utilizing a cross-attention mechanism between input tokens and learnable parameter tokens, TokenFormer allows for scaling model parameters without altering input or output dimensions, enabling progressive scaling by adding new key-value parameter pairs. - This approach facilitates efficient scaling by reusing pre-trained models, thereby significantly reducing training costs compared to training large transformer models from scratch. - Experimental results demonstrate that TokenFormer achieves comparable perplexity to Transformers trained from scratch on language modeling tasks while substantially reducing the training budget, and maintains similar performance in visual modeling and zero-shot classification tasks. - TokenFormer offers controllable computational costs for long-context modeling, preserves learned distributions during scaling, and shows potential for integration into Mixture-of-Experts frameworks and parameter-efficient tuning strategies. | ['Natural Language Processing', 'Computer Vision', 'Image Classification', 'Text Generation', 'Zero-Shot Classification'] | [Link](https://github.com/Haiyang-W/TokenFormer) | N/A |
