

## Papers for 2024-11-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743) | Tianyi Zhou, Yanhong Li, MingLiiii | - This research paper investigates the layer-wise gradient patterns in Large Language Models (LLMs) during instruction-tuning with different reasoning approaches (fast vs. slow thinking) and response types. - It uses spectral analysis, specifically Singular Value Decomposition (SVD) and nuclear norm, to characterize gradient behaviors across LLM layers for tasks involving math, commonsense reasoning, and knowledge learning. - Slow thinking, using detailed Chain-of-Thought (CoT), results in more stable and uniform gradient norms across layers compared to fast thinking, suggesting improved learning stability. - The gradients associated with slow thinking effectively differentiate correct from irrelevant responses in reasoning tasks, while in knowledge learning tasks, gradient norms are sensitive to knowledge popularity but not correctness.  - The study also finds that instruction-tuned LLMs do not show significant advantages over pre-trained LLMs in identifying incorrect reasoning and have different gradient patterns for fast thinking responses, suggesting challenges in aligning with the instruction-tuning data. | ['Natural Language Processing'] | [Link](https://github.com/MingLiiii/Layer_Gradient) | N/A |
| [A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents](https://arxiv.org/abs/2410.22476) | Pawan Goyal, Gajula Sai Chaitanya, Abhilash Nandy, Sombit Bose, Ankan Mullick | - This paper introduces MLMCID, a pointer network-based architecture for joint extraction and detection of multi-label multi-class intents in task-oriented dialogue systems. - The MLMCID model uses an encoder-decoder framework with a pointer network and LSTM-based sequence generator to identify multiple intent spans within a sentence, along with their corresponding coarse- and fine-grained intent labels. - A new multilingual multi-label intent dataset (MLMCID-dataset) is also created from existing benchmark datasets.  - The model outperforms baseline approaches, including large language models (LLMs) like Llama2 and GPT, on various MLMCID datasets in terms of accuracy and F1-score. -  The approach is also effective in few-shot settings and demonstrates the importance of multi-intent modeling for real-world conversational AI. | ['Natural Language Processing', 'Text Classification', 'Question Answering'] | [Link](https://github.com/ankan2/multi-intent-pointer-network) | N/A |
| [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175) | Lei Hou, Bin Xu, Xiaozhi Wang, Hao Peng, Yunjia Qi | - This paper introduces constraint back-translation, a novel data generation technique for improving complex instruction following in Large Language Models (LLMs). - The technique involves taking existing instruction-response pairs and using an LLM to generate constraints that are already implicitly satisfied by the response.  - This method is used to create CRAB, a high-quality complex instruction-response dataset. - The method improves the performance of LLMs on complex instruction-following tasks, as measured by IFEval and FollowBench benchmarks. - It also serves as a useful auxiliary training objective during post-training by enhancing the model's understanding of complex constraints. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Language Models can Self-Lengthen to Generate Long Texts](https://arxiv.org/abs/2410.23933) | Dayiheng Liu, An Yang, Bowen Yu, Tianyi Tang, Shanghaoran Quan | - This paper introduces Self-Lengthen, an iterative training framework to improve the long text generation capabilities of Large Language Models (LLMs). - Self-Lengthen employs two roles: a Generator to produce initial responses and an Extender to lengthen these responses iteratively. - This method leverages the intrinsic knowledge of LLMs without needing additional data or proprietary models, addressing the training gap in current LLMs for long text generation. - Experimental results on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long text generation using open-source LLMs like Qwen2 and LLaMA3. - Notably, Self-Lengthen enhances the output length while preserving the quality, boosting output from 1,000 words to 8,000 in Qwen2.5. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/QwenLM/Self-Lengthen) | N/A |
| [BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays](https://arxiv.org/abs/2410.21969) | Xinxing Xu, Sicong Leng, Yanyu Xu, Tan Li Hui Faith, youngzhou12 | - BenchX, a unified benchmark framework, is proposed for evaluating Medical Vision-Language Pretraining (MedVLP) methods on chest X-rays, enabling head-to-head comparison and systematic analysis. - BenchX comprises three components: comprehensive datasets covering nine datasets and four medical tasks, benchmark suites to standardize data preprocessing and experimental setups, and unified fine-tuning protocols that accommodate heterogeneous MedVLP methods. - Baselines for nine state-of-the-art MedVLP methods are established using BenchX, revealing that some early methods can outperform recent ones with proper training strategies. - In particular, MGCA and MRM consistently demonstrate strong performance in most cases.  - MedCLIP-ViT also delivers good performance on multi-label image classification tasks. | ['Computer Vision', 'Image Classification', 'Image Segmentation', 'Image-to-Text', 'Multimodal'] | [Link](https://github.com/yangzhou12/BenchX) | N/A |
| [BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments](https://arxiv.org/abs/2410.23918) | Yunhua Zhou, Dong Zhang, Bo Wang, Pengyu Wang, Xinghao Wang | BitStack is a novel, training-free weight compression approach for large language models (LLMs) that enables megabyte-level trade-offs between memory usage and model performance.  It achieves this through iterative weight decomposition and considers parameter significance, resulting in approximately 1-bit per parameter residual blocks.  These blocks are sorted and stacked for dynamic loading based on memory availability.  Extensive experiments show BitStack matches or surpasses existing quantization baselines across various tasks, particularly at extreme compression ratios. | ['Natural Language Processing'] | [Link](https://github.com/xinghaow99/BitStack) | N/A |
| [Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks](https://arxiv.org/abs/2410.24032) | Qingwei Lin, Jue Zhang, Zhiyang Zhang, Xiaoting Qin, Yingzhe Peng | The paper introduces CARE, a collaborative chatbot system for personalized exploratory tasks that combines a multi-agent LLM framework with a structured UI.  CARE addresses limitations of existing LLM chatbots by extracting both explicit and implicit user needs through iterative query refinement and dynamic solution generation. A within-subject user study with 22 participants showed CARE was consistently preferred over a baseline LLM chatbot, reducing cognitive load and inspiring creativity.  CARE transforms LLM-based systems from passive information retrievers into proactive partners in personalized problem-solving and exploration. The study also revealed CARE's impact on facilitating better user experiences in complex tasks, by improving solution comprehensiveness and personalization. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
