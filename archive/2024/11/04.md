

## Papers for 2024-11-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OS-ATLAS: A Foundation Action Model for Generalist GUI Agents](https://arxiv.org/abs/2410.23218) | Fangzhi Xu, Zhenyu Wu, Zhiyong Wu, heroding77, QiushiSun | - This paper introduces OS-Atlas, a large action model designed for generalist GUI agents, focusing on GUI grounding and out-of-distribution (OOD) generalization. - It leverages a novel multi-platform data synthesis toolkit, enabling the creation of a 13 million GUI element dataset spanning Windows, macOS, Linux, Android, and web interfaces. - OS-Atlas employs a two-stage training process: GUI grounding pre-training on the large dataset and action fine-tuning on existing agent datasets with a unified action space to mitigate conflicts. - Evaluations across six benchmarks and three platforms (mobile, desktop, web) show significant performance improvements over state-of-the-art models. - OS-Atlas-Base, the pre-trained model, serves as an open-source alternative to commercial VLMs for building GUI agents, achieving comparable performance in some settings. | ['Multimodal'] | N/A | N/A |
| [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/abs/2411.00412) | Leon Bergen, Duncan Watson-Parris, Yadi Cao, yuqirose, Bohan22 | - This paper introduces a novel two-stage training method called Adapting While Learning (AWL) to enhance Large Language Models (LLMs) for solving scientific problems by incorporating tool usage and direct reasoning. - AWL consists of World Knowledge Distillation (WKD) which fine-tunes LLMs to internalize domain knowledge from solutions generated using tools and Tool Usage Adaptation (TUA) which trains LLMs to choose between direct answering and tool usage based on problem complexity. - Evaluation across six scientific benchmarks demonstrate average improvements of 28.18% in answer accuracy and 13.89% in tool usage precision compared to baselines and state-of-the-art models such as GPT-4 and Claude-3.5. - The model surpasses existing approaches on custom datasets that include complex and specialized scientific questions not commonly seen during pre-training. - It also showcases improved robustness in noisy data scenarios and adaptability to open-ended questions through integration with preference learning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027) | Yijia Shao, Branislav Kveton, Ryan A. Rossi, Zhehao Zhang, Franck-Dernoncourt | - This survey paper provides a comprehensive overview of personalized Large Language Models (LLMs), unifying research on personalized text generation and downstream task personalization. - It introduces a taxonomy for personalized LLM usage, formalizing foundations, and analyzing personalization granularity (user-level, persona-level, global preference). - The paper surveys techniques for personalization, including retrieval-augmented generation, prompting, representation learning, and reinforcement learning from human feedback (RLHF). - It also covers evaluation metrics and datasets for personalized LLMs, along with various applications like AI assistants, recommendation systems, and search engines. - Finally, it discusses open problems such as benchmarks, cold-start issues, bias, privacy, and multimodality in personalized LLMs. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation](https://arxiv.org/abs/2410.21157) | Shukai Liu, Jian Yang, Congnan Liu, Ken Deng, Jiaheng Liu | - This paper introduces M2RC-EVAL, a massively multilingual repository-level code completion benchmark encompassing 18 programming languages. - It offers two types of fine-grained annotations: bucket-level (based on abstract syntax tree depth) and semantic-level (categorizing completion scenarios). - The authors also present M2RC-INSTRUCT, a multilingual instruction dataset designed to improve repository-level code completion abilities in LLMs. - Experimental results show that incorporating cross-file context and fine-tuning on M2RC-INSTRUCT significantly enhances performance across various languages. - Code Llama with fine-tuning outperforms non-finetuned StarCoder after fine-tuning.  | ['Natural Language Processing', 'Text2Text Generation', 'Text Generation'] | [Link](https://github.com/M2RC-Eval-Team/M2RC-Eval) | N/A |
| [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030) | Pierre-François Marteau, Nicolas Béchet, Danrun Cao | - This paper introduces WikiNER-fr-gold, a manually revised gold-standard version of the French portion of the WikiNER corpus, aimed at improving the quality of Named Entity Recognition (NER) resources. - The corpus consists of 20% of the original WikiNER-fr (26,818 sentences, ~700k tokens), randomly sampled and corrected for inconsistencies in annotation stemming from the semi-supervised nature of the original WikiNER. - The correction process focused on standardizing entity boundaries and categories, resolving ambiguous hyperlinks, and addressing inconsistencies in the application of annotation guidelines. - The authors analyzed the errors in the silver-standard WikiNER-fr, categorized them, and described the correction strategies employed. - The paper also discusses future work, including a broader assessment of entity categorization and potential automation of the correction process for the remaining WikiNER data. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |
| [GRS-QA -- Graph Reasoning-Structured Question Answering Dataset](https://arxiv.org/abs/2411.00369) | Jincen Shuai, Devasha Trivedi, Anish Pahilajani, Franck-Dernoncourt, namyongp | - This paper introduces GRS-QA, a new multi-hop question answering dataset that includes explicit reasoning structures in the form of graphs for enhanced reasoning analysis of LLMs. - Unlike existing datasets that lack clear reasoning pathways, GRS-QA captures intricate structures by constructing reasoning graphs, where nodes denote textual contexts and edges signify logical flow. - GRS-QA offers benefits such as providing transparent reasoning steps for answer derivation, allowing fine-grained evaluation of LLM reasoning capabilities across diverse structures. - It also includes negative reasoning graphs, created by perturbing the structure of positive graphs, to isolate the impact of reasoning structures compared to content on question answering performance. - The authors benchmark state-of-the-art models on GRS-QA from retrieval, direct question answering, and retrieval-augmented generation perspectives, revealing that LLM performance degrades with increasing reasoning complexity and highlighting the importance of GRS-QA in pushing the limits of current QA models. | ['Question Answering', 'Natural Language Processing', 'Graph Machine Learning'] | N/A | N/A |
