

## Papers for 2024-11-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models](https://arxiv.org/abs/2411.00836) | Bin Hu, Junyu Zhang, Xingang Guo, Chengke Zou, Ray2333 | - DYNAMATH, a dynamic visual benchmark, is introduced to evaluate the robustness of Vision Language Models (VLMs) in mathematical reasoning. - The benchmark consists of 501 seed questions represented as Python programs, enabling automatic generation of diverse concrete questions with variations in visual and textual content. - An evaluation of 14 state-of-the-art VLMs on 5,010 generated questions revealed a significant gap between average-case and worst-case accuracy, indicating current VLMs' lack of robustness in handling question variations. - The analysis also found high repetition consistency in many models, suggesting that incorrect answers on certain variants are due to consistent errors rather than inherent randomness. - DYNAMATH provides insights to guide development of more robust VLMs and the paper suggests using adversarial training or reinforcement learning from human feedback with fine-grained process rewards as potential improvement strategies. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265) | Jiaqi Zhu, Xingwu Sun, Ruobing-Xie, Mimosa77, YanfengChen | - Tencent introduces Hunyuan-Large, a 389 billion parameter (52 billion activated) open-source Mixture-of-Experts (MoE) model based on the Transformer architecture and capable of handling up to 256K tokens. - The model outperforms LLama3.1-70B on various benchmarks, including language understanding, generation, logical reasoning, mathematics, coding, and long-context tasks, and exhibits performance comparable to the much larger LLama3.1-405B model. - Key innovations include using large-scale synthetic data, a mixed expert routing strategy combining shared and specialized experts with recycle routing for discarded tokens, KV cache compression by grouped-query attention and cross-layer attention, and an expert-specific learning rate scaling strategy. - The model is pre-trained on 7 trillion tokens, including 1.5 trillion synthetic tokens, followed by post-training stages involving supervised fine-tuning and reinforcement learning from human feedback using direct preference optimization. - Both pre-trained and post-trained versions of Hunyuan-Large are released to the open-source community. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Tencent/Tencent-Hunyuan-Large) | [Link](https://huggingface.co/tencent/Tencent-Hunyuan-Large) |
| [Survey of Cultural Awareness in Language Models: Text and Beyond](https://arxiv.org/abs/2411.00860) | Junho Myung, Arnav Arora, Junyeong Park, jinjh0123, sidicity | - This paper surveys efforts to incorporate cultural awareness into text-based and multimodal large language models (LLMs). - It defines cultural awareness in LLMs based on definitions from psychology and anthropology and examines methodologies for creating cross-cultural datasets and benchmarks, strategies for cultural inclusion in downstream tasks, and benchmarks for evaluating cultural awareness in LLMs. - The survey also discusses ethical implications of cultural alignment, the role of Human-Computer Interaction, and cultural alignment's role in social science research. - The paper identifies research gaps in current literature and provides suggestions for future research in areas such as cross-cultural LLMs and automatic context detection. - It organizes and compares efforts in incorporating culture into NLP and spans several modalities like image, video, audio and text. | ['Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models](https://arxiv.org/abs/2411.00918) | Quang Pham, Van Nguyen, Luong Tran, doantienthongbku, DavidNguyen | - This paper introduces LibMoE, a comprehensive and modular framework designed to streamline the research, training, and evaluation of Mixture of Experts (MoE) algorithms in Large Language Models (LLMs). - LibMoE facilitates easier access to MoE research for a wider range of researchers by standardizing the training and evaluation process, and by reducing the computational cost via sparse upcycling from pre-trained LLMs. - The authors benchmark five state-of-the-art MoE algorithms with three model configurations across eleven datasets under a zero-shot setting. - Results show that all MoE algorithms achieve roughly similar performance when averaged across a variety of tasks. - Further analysis suggests the potential benefits of early stopping and the importance of balanced expert utilization in MoE models. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/abs/2411.02335) | Chaojun Xiao, Yingfa Chen, Chenyang Song, Yuqi Luo, SillyXu | - This paper investigates activation sparsity in Large Language Models (LLMs), proposing a new metric called PPL-p% sparsity. - PPL-p% sparsity is performance-aware, versatile across activation functions, and precisely identifies weakly contributing neurons, improving upon existing metrics like CETT. - Through extensive experiments, the research reveals scaling laws relating activation sparsity to training data, activation function, width-depth ratio, and parameter scale. - ReLU activation is found to be superior to SiLU due to greater sparsity and comparable performance, with deeper models exhibiting higher sparsity below a certain bottleneck. - Notably, the limit of activation sparsity shows weak correlation with parameter scale, suggesting that activation patterns in LLMs are scale-insensitive. | ['Natural Language Processing'] | [Link](https://github.com/thunlp/SparsingLaw) | N/A |
| [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747) | Ryan A. Rossi, Seunghyun Yoon, Viet Dac Lai, Dang Nguyen, Franck-Dernoncourt | - DynaSaur is a novel LLM agent framework that dynamically creates and composes actions, represented as Python functions, enabling the agent to operate beyond a predefined action set. - At each step, the agent generates Python code to perform an action, accumulating these generated actions for reuse in future steps, enhancing flexibility and efficiency. - This framework outperforms existing methods on the GAIA benchmark, demonstrating its effectiveness in complex, long-horizon tasks. - Notably, DynaSaur allows the agent to recover from scenarios where the predefined action set is insufficient or existing actions fail due to unforeseen circumstances. - The dynamic action creation and accumulation capabilities enable the LLM agent to interact with various tools and systems, enhancing its ability to solve a diverse range of tasks. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/adobe-research/dynasaur) | N/A |
| [Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models](https://arxiv.org/abs/2411.00743) | Virginia Smith, Mona Diab, Aashiq Muhamed | - Introduces Specialized Sparse Autoencoders (SSAEs), which are designed to capture rare or infrequent features (tail concepts) within specific domains of foundation models. - Employs dense retrieval and TracIn reranking as effective methods for selecting training data relevant to the target domain, enabling targeted feature extraction without needing to scale to billions of features. - Utilizes Tilted Empirical Risk Minimization (TERM) as a training objective, demonstrating its effectiveness in enhancing tail concept representation in SSAEs compared to standard Empirical Risk Minimization (ERM). - Demonstrates through experiments on the Bias in Bios dataset that SSAEs improve interpretability by capturing rare features and significantly increase worst-group classification accuracy (12.5%) when used to remove spurious gender information. - Evaluation on downstream perplexity and Lo sparsity metrics shows SSAEs effectively capture domain-specific tail concepts, outperforming standard SAEs trained on general-purpose data. | ['Natural Language Processing', 'Feature Extraction'] | N/A | N/A |
| [Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks](https://arxiv.org/abs/2411.01192) | Muhammad Abdul-Mageed, Fakhraddin Alwajih, Abdellah El Mekki, El Moatez Billah Nagoudi, Gagan Bhatia | - This paper introduces Swan, a family of dialect-aware, Arabic-centric, cross-lingual, and cross-cultural embedding models. - Swan includes Swan-Small, based on ARBERTv2, and Swan-Large, based on the pretrained Arabic large language model ArMistral. - A new comprehensive benchmark suite, ArabicMTEB, is proposed to evaluate the models, covering eight tasks and 94 datasets, including cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance. - Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks while maintaining monetary efficiency. - Swan-Small also shows strong performance, consistently surpassing Multilingual-E5-base on most Arabic tasks. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | N/A | N/A |
