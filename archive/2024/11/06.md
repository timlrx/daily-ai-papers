

## Papers for 2024-11-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems](https://arxiv.org/abs/2411.02959) | Weipeng Chen, Mang Wang, Wen Wang, Zhicheng Dou, Jiejun Tan | - HtmlRAG is a novel Retrieval-Augmented Generation (RAG) system that utilizes HTML, instead of plain text, as the format for retrieved knowledge, aiming to preserve structural and semantic information often lost during HTML-to-text conversion. - HtmlRAG incorporates HTML cleaning, compression, and a two-step pruning process, involving an embedding model for coarse-grained pruning and a generative model for fine-grained pruning, to address challenges of long input sequences and noisy content. - This approach outperforms existing RAG systems, utilizing text-based and Markdown-based post-retrieval processes on six QA datasets, including ASQA, HotpotQA, NQ, TriviaQA, MuSiQue, and ELI5 datasets.  - Specifically, it improves exact match by up to 4.5% on the NQ Dataset and 8.7% on the MuSiQue dataset when using Llama 3.1 70B instruct model. - The results demonstrate the effectiveness of utilizing HTML for knowledge modeling in RAG systems, particularly with powerful LLMs capable of handling complex HTML structures. | ['Question Answering'] | [Link](https://github.com/plageon/HtmlRAG) | N/A |
| [LLaMo: Large Language Model-based Molecular Graph Assistant](https://arxiv.org/abs/2411.00871) | Hyunwoo J. Kim, Dohwan Ko, Minseong Bae, Jinyoung Park | - LLaMo, a Large Language Model-based Molecular graph assistant, integrates a molecular graph encoder and a large language model for instruction-following response generation in the molecular domain. - LLaMo uses a multi-level graph projector to abstract representations of each GNN layer and motif representations, bridging the graph encoder and language model. - Machine-generated molecular graph instruction data, created through a multi-turn conversation format from molecular descriptions and IUPAC names, are used for instruction tuning. - Experimental results demonstrate LLaMo's superior performance in molecular description generation, property prediction, and IUPAC name prediction, outperforming LLM-based models like GPT-4. - Ablation studies validate the contribution of the multi-level graph projector and the instruction tuning process. | ['Graph Machine Learning', 'Multimodal', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/mlvlab/LLaMo) | N/A |
| [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | Shenzhi Wang, Yizeng Han, Bingyi Kang, Yulin Wang, Yang Yue | - DeeR-VLA dynamically adjusts the size of activated Multimodal Large Language Models (MLLMs) based on situation complexity, improving computational efficiency for robotic tasks. - DeeR leverages a multi-exit MLLM architecture allowing early termination of processing once sufficient model capacity is reached for a given input, avoiding redundant computation. - The framework includes algorithms to set early-exit criteria based on predefined computational budgets (average/peak FLOPs, GPU memory), enabling adaptability to resource constraints. - A tailored training method integrates temporal information within the multi-exit architecture to ensure reasonable action predictions. - Evaluation on the CALVIN benchmark shows 5.2-6.5x reduction in LLM computational costs and 2-6x reduction in LLM GPU memory usage without compromising task performance. | ['Robotics', 'Multimodal'] | [Link](https://github.com/yueyang130/DeeR-VLA) | N/A |
| [Sample-Efficient Alignment for LLMs](https://arxiv.org/abs/2411.01493) | Min Lin, Wee Sun Lee, Chao Du, Changyu Chen, Zichen Liu | - This paper introduces SEA (Sample-Efficient Alignment), a Thompson sampling-based algorithm, for aligning Large Language Models (LLMs) with human preferences efficiently, addressing the bottleneck of extensive human feedback requirements in current alignment methods. - The approach frames LLM alignment as a contextual dueling bandit problem and emphasizes two key properties for sample efficiency: online interaction and active exploration. - SEA leverages an epistemic reward model (deep ensemble of reward models) for posterior sampling, policy-guided search for efficient response selection, and mixed preference learning (combining online user feedback and synthetic feedback from the reward model) to update the LLM policy online. - Experimental results across various model scales (1B, 2.8B, 6.9B parameters) and direct preference optimization methods (DPO, IPO, SLiC) show SEA achieves higher win rates against reference responses and significantly better sample efficiency compared to existing baselines, including passive online learning and other active exploration methods.  - The authors release `oat`, an open-source, distributed learning system designed for online LLM alignment research, aiming to facilitate further studies and fair comparisons in the field. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/sail-sg/oat) | [Link](https://huggingface.co/docs/trl/main/en/online_dpo_trainer), [Link](https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B) |
| [Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge](https://arxiv.org/abs/2411.02657) | Lashaw Salta, Chinmay Agrawal, Catalina Villouta, Andrew Langdon, ksoman | - Zebra-Llama, a context-aware large language model specializing in Ehlers-Danlos Syndrome (EDS) information, was developed using a novel context-aware fine-tuning methodology. - The model leverages Retrieval-Augmented Generation (RAG) and is trained on a diverse dataset comprising medical literature, patient forums, and clinical resources, structured as question-context-answer triplets. - Evaluation on real-world questions from EDS patients and clinicians demonstrated Zebra-Llama's superior performance compared to the base Llama model across thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%), and citation reliability (70.6% vs. 52.3%). - A custom RAG API and Jupyter Notebook demo are also released. - The model and code are open-sourced to democratize expert-level knowledge in rare disease management. | ['Question Answering', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/karthiksoman/zebra-llama) | [Link](https://huggingface.co/zebraLLAMA/zebra-Llama-v0.2) |
| [Controlling Language and Diffusion Models by Transporting Activations](https://arxiv.org/abs/2410.23054) | Nicholas Apostoloff, Luca Zappella, Michal Klein, Arno Blaas, Pau Rodriguez | - This paper introduces Activation Transport (ACT), a framework to steer activations in generative models (GMs) using optimal transport theory. - ACT generalizes existing activation steering methods by applying univariate maps to activations while preserving target distributions, improving controllability and robustness.  - Linear-ACT, an inference-time intervention based on ACT, matches or outperforms other methods in toxicity mitigation, concept induction, and truthfulness in LLMs. - ACT effectively controls text-to-image diffusion models for fine-grained style control and concept negation.  - The authors adapt ITI (Li et al., 2024) for text-to-image and find that ACT with a strength parameter of 1 consistently achieves strong conditioning across tasks and models. | ['Natural Language Processing', 'Text-to-Image', 'Text Generation'] | N/A | N/A |
