

## Papers for 2024-11-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/abs/2411.04905) | Jiaran Hao, Jason Klein Liu, Tianhao Cheng, Siming Huang, Zenithwang | - OpenCoder, a top-tier code large language model (LLM) designed for code generation, reasoning, and agent systems, is introduced, boasting performance comparable to leading models while offering full transparency through the release of its training data, processing pipeline, and protocols. - OpenCoder's key ingredients for success include code-optimized heuristic rules for data cleaning and deduplication, incorporation of code-related text corpora, and utilization of high-quality synthetic data in annealing and fine-tuning. - The model architecture for OpenCoder is available in 1.5B and 8B parameter sizes, leveraging SwiGLU activation, RoPE positional embedding, and a vocabulary size of 96,640, with variations in layers, attention heads, and context window size between the two. - OpenCoder's training involves a two-stage instruction-tuning process: the first focuses on theoretical computer science question-answer pairs, while the second refines practical coding skills using high-quality code from GitHub. - Evaluation on benchmarks like HumanEval, MBPP, BigCodeBench, LiveCodeBench, and MultiPL-E reveals OpenCoder surpasses all previous fully open models and other open-access models at the 6B+ parameter scale. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/yuxiang630/hqcode) |
| [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965) | Furu Wei, Shuming Ma, Hongyu Wang | - BitNet a4.8 introduces 4-bit activations and a hybrid quantization and sparsification strategy for 1-bit Large Language Models (LLMs), aiming to reduce inference costs while maintaining performance comparable to the 1.58-bit BitNet b1.58 model. - The model employs 4-bit activations for inputs to attention and feed-forward network layers, and sparsifies intermediate states with 8-bit quantization to mitigate quantization errors caused by outlier channels.  - It also incorporates a two-stage training approach (from 8-bit to 4-bit activations) for efficiency.  - Experimental results show BitNet a4.8 achieves comparable performance to BitNet b1.58 with equivalent training costs but faster inference due to enabling INT4/FP4 kernels and supporting 3-bit KV cache. - Additionally, BitNet a4.8 activates only 55% of the parameters, further enhancing efficiency for large-scale LLM deployment and inference. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996) | Ning Dong, Srinivasan Iyer, Liang Luo, Lili Yu, WxWx | - This paper introduces Mixture-of-Transformers (MoT), a sparse multimodal transformer architecture designed to reduce the computational costs of pretraining large multimodal models. - MoT decouples the non-embedding parameters of the model by modality, including feed-forward networks, attention matrices, and layer normalization, enabling modality-specific processing with global self-attention over the full input sequence. - In the Chameleon 7B setting (autoregressive text and image generation), MoT matches the dense baseline performance using only 55.8% of the FLOPs.  With speech added, MoT reaches comparable speech performance using 37.2% of the FLOPs.  - In the Transfusion setting, which uses multi-objective training with autoregressive text and diffusion-based image generation, a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics; a 7B MoT matches the image performance of the dense baseline with one-third of the FLOPs.  - System profiling shows MoT achieves dense baseline image quality in 47.2% of the wall-clock time and text quality in 75.6% of the wall-clock time. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Text Generation', 'Image-to-Text'] | N/A | N/A |
| [TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation](https://arxiv.org/abs/2411.04709) | Yi Yang, Wenhao Wang | - This paper introduces TIP-I2V, a large-scale dataset of over 1.7 million text and image prompts specifically designed for image-to-video generation, accompanied by generated videos from five state-of-the-art models. - The dataset is sourced from Pika Discord channels and includes additional information like UUIDs, timestamps, embeddings, subjects, and NSFW scores. - Analysis reveals that TIP-I2V's prompts, which focus on animating existing image content, differ semantically from those in text-to-video and text-to-image datasets. - This dataset enables research into user preferences, improved model evaluation, misinformation detection, and source image tracing. - Initial benchmarks using TIP-I2V indicate that even early commercial image-to-video models can outperform open-source alternatives in key areas, emphasizing the importance of real-world user data. | ['Image-to-Video', 'Multimodal', 'Computer Vision'] | [Link](https://tip-i2v.github.io) | N/A |
| [Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model](https://arxiv.org/abs/2411.04496) | Ho-Jin Choi, Kyeongjin Oh, Junyoung Youn, Dokyong Lee, Young-Jun Lee | - This paper introduces THANOS, a family of large language models (LLMs) designed to improve the quality of responses generated by conversational agents by infusing them with "skill-of-mind." - Skill-of-mind is a process that involves considering social context, interpreting dialogue situations, planning an appropriate skill strategy, and selecting the most effective conversational skill for a given response.  - The authors also created MULTIFACETED SKILL-OF-MIND, a dataset of 100k conversations annotated with explanations and conversational skills, to train THANOS.  - Experimental results indicate that THANOS effectively predicts conversational skills and enhances response quality in various scenarios, promoting prosocial behavior in human evaluations.  - This improvement is demonstrated by incorporating the generated skill-of-mind as input for LLM-based conversational agents, leading to better response quality. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/passing2961/Thanos) | N/A |
| [DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation](https://arxiv.org/abs/2411.04999) | Chris Paxton, Soumith Chintala, Mohit Warke, Zhanqiu Guo, Peiqi Liu | - DynaMem, a novel dynamic spatio-semantic memory architecture for open-world mobile manipulation, is introduced, enabling robots to adapt to changing environments. - DynaMem maintains and updates a voxelized pointcloud of the environment, incorporating object additions and removals, and supports object localization queries using both Vision-Language Model features and Multimodal Large Language Model question answering. - In real-world experiments on Stretch SE3 robots across various dynamic scenes, DynaMem achieves a 70% pick-and-drop success rate for non-stationary objects, more than double the performance of static systems. - A new dynamic benchmark, DynaBench, is introduced to evaluate dynamic spatio-semantic memory algorithms in 9 changing environments, and ablation studies demonstrate the effectiveness of key design choices. - DynaMem handles object permanence, going beyond existing systems that often return incorrect matches when the queried object is absent. | ['Robotics', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/abs/2411.05000) | Samuel Albanie, Kai Han, Jonathan Roberts | - This paper introduces a new set of retrieval experiments to evaluate the long-context capabilities of Large Language Models (LLMs), called needle threading tasks. - These tasks involve following threads of linked information across different parts of the context and retrieving the final value, including single and multiple needle retrieval, conditional needle retrieval, threading and multi-threading, and branched threading variations. - The study evaluates 17 LLMs on these tasks using synthetically generated key-value pairs of UUIDs and finds that increased context length negatively impacts performance but concurrent threading is largely unaffected by concurrent queries. - It suggests the LLMs' "effective" context limit is shorter than stated due to performance degradation at longer context lengths.   - The paper introduces a task-specific and model-agnostic effective context limit metric and publicly releases the code and experimental data. | ['Question Answering'] | N/A | N/A |
| [RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval](https://arxiv.org/abs/2411.04752) | Subhankar Maity, Aniket Deroy | - This paper introduces RetrieveGPT, a novel approach for enhancing information retrieval from code-mixed conversations, particularly focusing on Roman transliterated Bengali mixed with English. - The approach uses GPT-3.5 Turbo with carefully designed prompts to evaluate document relevance to a given query, considering the sequential nature of conversations. - A mathematical model integrates GPT-3.5 Turbo's output, accounting for sequential dependencies among documents to determine relevance. - The model treats relevance detection as a problem of finding the optimal relevance chain across a sequence of documents. - Experiments on a Facebook dataset with Query Relevance files (QRels) demonstrate the effectiveness of the approach in extracting information from complex, code-mixed conversations. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos](https://arxiv.org/abs/2411.04923) | Eric Xing, Jiale Cao, Wenqi Zhu, Hanan Gani, Shehan Munasinghe | - VideoGLaMM, a large multimodal model designed for pixel-level visual grounding in videos, is introduced. - The model architecture comprises a Large Language Model (LLM), dual vision encoders (for spatial and temporal features), and a spatio-temporal decoder connected via tunable Vision-to-Language (V→L) and Language-to-Vision (L→V) adapters.  - VideoGLaMM is trained on a new dataset of grounded conversation video question-answer triplets which include segmentation masks generated using a semi-automatic annotation pipeline.  - The new dataset includes 38k video-QA triplets, 83k objects and 671k masks.  - Experimental results demonstrate state-of-the-art performance on grounded conversation generation, visual grounding, and referring video segmentation tasks, outperforming existing approaches. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Mask Generation'] | N/A | [Link](https://mbzuai-oryx.github.io/VideoGLaMM) |
