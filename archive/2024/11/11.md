

## Papers for 2024-11-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Balancing Pipeline Parallelism with Vocabulary Parallelism](https://arxiv.org/abs/2411.05288) | Min Lin, Penghui Qi, Man Tsung Yeung, ufotalent | - This paper introduces Vocabulary Parallelism (VP), a novel technique designed to address computational and memory imbalances stemming from vocabulary layers in pipeline parallelism, a common strategy for training large language models. - VP partitions vocabulary layers across all pipeline devices and integrates the computation as passes within the pipeline schedule, similar to the handling of transformer layers.  The approach involves algorithms to reduce communication barriers within these vocabulary passes, thereby minimizing activation memory overhead. - When combined with memory-balanced pipeline schedules like V-Half, the proposed method achieves near-perfect balance in both memory and computation. - Experimental results show improvements in throughput ranging from 5% to 51% compared to existing methods, particularly under large vocabulary scenarios, with significant reductions in peak memory usage. - Notably, the benefits extend to various vocabulary and model sizes, demonstrating the robustness and generalizability of the approach. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/sail-sg/VocabularyParallelism) | N/A |
| [DELIFT: Data Efficient Language model Instruction Fine Tuning](https://arxiv.org/abs/2411.04425) | Marina Danilevksy, Lucian Popa, Krishna Killamsetty, ishikaa | - DELIFT (Data Efficient Language Model Instruction Fine-Tuning) is a novel algorithm that optimizes data selection across three key stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning. - It utilizes a pairwise utility metric to quantify the value of a data sample in improving the model's responses to other samples and leverages submodular functions for optimal subset selection. - DELIFT reduces fine-tuning data size by up to 70% without compromising performance, leading to significant computational savings.  - It outperforms current data selection techniques by up to 26% across diverse tasks and model scales. - Experiments show minimal performance drops compared to using full datasets and even surpasses full dataset performance in niche tasks such as query rewriting. | ['Natural Language Processing', 'Question Answering'] | [Link](https://anonymous.4open.science/r/optimizing-data-selection-0CD0) | N/A |
| [Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study](https://arxiv.org/abs/2411.02462) | Jingyue Li, andstor | - This paper conducts an empirical study on Parameter-Efficient Fine-Tuning (PEFT) methods for Large Language Models (LLMs) in generating unit tests for code. - It evaluates full fine-tuning, LoRA, (IA)Â³, and prompt tuning across various open-source LLMs (CodeGen, Code Llama, StarCoder) ranging from 350 million to 16 billion parameters and use well-established unit-test datasets for evaluation. - The findings indicate that LoRA generally performs best, matching or exceeding full fine-tuning's performance in several cases while using fewer parameters, whereas prompt tuning is the most resource-efficient but has more variable performance. - They evaluate using codebleu to evaluate the similarity between the generated tests and the reference tests. -  The results also show that both full fine-tuning and PEFT methods are mostly resistant to catastrophic forgetting, sometimes even improving the code generation capabilities. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/andstor/methods2test_small), [Link](https://huggingface.co/andstor/peft-unit-test-generation-experiments) |
| [LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation](https://arxiv.org/abs/2411.04997) | Yuqing Yang, Xufang Luo, Aoqi Wu, Weiquan Huang, Yif29 | - LLM2CLIP is a novel approach that integrates Large Language Models (LLMs) into Contrastive Language-Image Pre-training (CLIP) to enhance visual representation learning. - It addresses the limitations of LLMs' output features by applying a caption contrastive fine-tuning strategy, which increases their discriminability and enables them to act as a more powerful teacher for CLIP's visual encoder. - This approach improves CLIP's ability to handle longer and more complex captions and incorporate richer knowledge from LLMs, without significant computational overhead. - LLM2CLIP demonstrates significant improvements across various cross-modal tasks, exceeding previous state-of-the-art models like EVA02 by a substantial margin on text and image retrieval benchmarks, and even enabling cross-lingual capabilities for models trained solely on English data. - It also shows promising results in multimodal training with models like LLaVA 1.5, consistently outperforming the original CLIP across several benchmarks. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | N/A | [Link](https://aka.ms/llm2clip) |
| [Improving the detection of technical debt in Java source code with an enriched dataset](https://arxiv.org/abs/2411.05457) | Rick Kazman, Davide Di Ruscio, Phuong T. Nguyen, Anh M. T. Bui, Nam Le Hai | - This paper introduces TESORO, a new dataset for detecting technical debt (TD) in Java source code, which includes both comments and the corresponding source code. - A pipeline is proposed for enriching technical debt data by extracting Self-Admitted Technical Debts (SATD) comments and their associated source code units. - The study demonstrates that incorporating source code context enhances the performance of state-of-the-art SATD detection models, and an ensemble approach combining predictions from different code context lengths yields even better results. - The paper investigates the accuracy of different pre-trained language models (PLMs) in detecting TD solely from source code, revealing the superior performance of CodeBERT and its variant GraphCodeBERT, and highlighting the potential of LLMs in this task. - The curated TESORO dataset is expected to catalyze future research in the domain of TD detection and facilitate the identification of other software artifacts such as code smells. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/NamCyan/tesoro) | N/A |
