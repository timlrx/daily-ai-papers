

## Papers for 2024-11-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models](https://arxiv.org/abs/2411.07140) | Hui Huang, Yingshui Tan, Jiaheng Liu, Shilong Li, Yancheng He | - This paper introduces Chinese SimpleQA, a benchmark designed to evaluate the factuality of Large Language Models (LLMs) in answering short questions in Chinese. - The dataset consists of 3000 high-quality question-answer pairs across six diverse topics, with a focus on static answers that do not change over time. - The questions and answers in Chinese SimpleQA are short, simplifying evaluation with existing LLMs, and the grading process employs the OpenAI API. - Initial findings indicate that Chinese SimpleQA is challenging for existing LLMs, with only a few achieving passing scores. - The research demonstrates the importance of model size, calibration, and the effectiveness of Retrieval-Augmented Generation (RAG) strategies in improving LLM performance in Chinese factuality. | ['Question Answering'] | N/A | N/A |
| [IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization](https://arxiv.org/abs/2411.06208) | Yongbin Li, Fei Huang, Cheng Fu, Haiyang Yu, Xinghua Zhang | - This paper introduces IOPO (Input-Output Preference Optimization), a new alignment method for Large Language Models (LLMs) that aims to improve complex instruction following. - Unlike existing methods like DPO that focus on output preference, IOPO considers both input and output preferences, enabling LLMs to better understand fine-grained constraints within complex instructions. - A new benchmark called TRACE, comprising 120K training and 1K evaluation instances with varying constraint complexities, is also introduced to facilitate training and evaluation of complex instruction following abilities. - Experiments on TRACE, IFEval, and CFBench demonstrate IOPO's effectiveness, showing improvements of 2.18% and 3.13% over DPO on in-domain and out-of-domain datasets, respectively. - Ablation studies confirm that both input and output preference modeling are crucial for instruction following, especially in complex scenarios. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework](https://arxiv.org/abs/2411.06176) | Maojia Song, Chaoqun Liu, Hou Pong Chan, Liying Cheng, Yew Ken Chia | - Introduces M-LongDoc, a benchmark dataset with 851 samples for evaluating multimodal long document understanding. - Proposes a retrieval-aware tuning approach to improve model performance in document question answering. - Presents an automated evaluation framework leveraging multiple judge models to assess the correctness of open-ended solutions. - Demonstrates that the retrieval-aware tuning approach achieves a 4.6% relative improvement in answer correctness compared to baseline open-source models. - Finds that existing models struggle with figure and table-based questions, highlighting a multimodal bias. | ['Multimodal', 'Document Question Answering'] | N/A | N/A |
| [Counterfactual Generation from Language Models](https://arxiv.org/abs/2411.07180) | Ryan Cotterell, Anej Svete, vesteinn, Shauli | - This paper proposes a framework for generating counterfactual text from language models (LMs) by treating them as Generalized Structural Equation Models (GSEMs) and using the Gumbel-max trick. - This approach allows for modeling the joint distribution of original and counterfactual strings, enabling investigation of cause-and-effect relationships within LMs. - An algorithm based on hindsight Gumbel sampling infers the noise variables that produced an observed string, facilitating the generation of its counterfactual. - Experiments on GPT2-XL and LLaMA3-8b, with interventions like knowledge editing and linear steering, reveal unintended side effects, showing that these techniques are not as surgical as intended. - This work highlights the need for more precise intervention methods in LMs to minimize unintended changes in generated text. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/shauli-ravfogel/lm-counterfactuals) | [Link](https://huggingface.co/intfloat/e5-base-v2), [Link](https://huggingface.co/jujipotle/honest_llama3_8B_instruct), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |
| [Game-theoretic LLM: Agent Workflow for Negotiation Games](https://arxiv.org/abs/2411.05990) | Julie Chen, Alfonso Amayuelas, Lingyao Li, Ollie Liu, Wenyue Hua | - This paper introduces game-theory-inspired workflows to enhance the rationality of Large Language Models (LLMs) in strategic decision-making, particularly within negotiation games. - The authors evaluate several state-of-the-art LLMs across various complete and incomplete information games and find that LLMs often deviate from rational strategies, especially in complex games. - They design distinct workflows incorporating principles like dominant strategy search, backward induction, and Bayesian belief updating to guide LLMs' reasoning and improve decision-making. - Experimental results demonstrate that these workflows significantly improve LLM performance in identifying optimal strategies, achieving near-optimal allocations, and reducing susceptibility to exploitation during negotiations. -  The paper further explores the meta-strategic considerations of workflow adoption, highlighting a novel research direction in analyzing the rationality of using such workflows in different scenarios. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/Wenyueh/game_theory) | N/A |
| [Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models](https://arxiv.org/abs/2411.06272) | Yiyan Qi, Zhouchi Lin, Huanyi Su, Junxi Liu, Xiaojun Wu | - This paper introduces Golden Touchstone, a bilingual (English and Chinese) benchmark for evaluating Financial Large Language Models (FinLLMs). - The benchmark covers eight core financial NLP tasks, including sentiment analysis, question answering, and summarization, with 22 datasets in total.  - The authors also release Touchstone-GPT, a FinLLM trained using continuous pre-training and financial instruction tuning on a 100B token dataset and 300k instruction-response pairs. - Evaluation results on Golden Touchstone show that while existing LLMs and FinLLMs perform reasonably well on some tasks like sentiment analysis, they struggle with more complex tasks like relation extraction and stock prediction. - Touchstone-GPT shows competitive performance compared to the other models. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Summarization'] | [Link](https://github.com/IDEA-FinAI/Golden-Touchstone) | N/A |
| [Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction](https://arxiv.org/abs/2411.06424) | Adam Mahdi, Harry Mayne, Filip Sondej, Yushi Yang | - This paper investigates the internal mechanisms of Direct Preference Optimization (DPO) for toxicity reduction in language models, challenging the existing explanation that it primarily works by dampening the most toxic neurons. - By ablating toxic neurons and applying activation patching, the study finds that only 31.8% of toxicity reduction stems from dampened toxic neurons. - Instead, DPO reduces toxicity through a more complex process involving multiple neuron groups, accumulating effects by both reducing writing in the toxic direction and actively promoting anti-toxicity in the residual stream. - DPO's adjustments to neuron activations are noisy, with many neurons actually increasing toxicity, suggesting a balancing process between opposing neuron effects to achieve overall toxicity reduction. - The research provides a more nuanced understanding of DPO's mechanism, suggesting potential for targeted interventions to replicate its effects and improve safety in language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Yushi-Y/dpo-toxic-neurons) | N/A |
| [KMM: Key Frame Mask Mamba for Extended Motion Generation](https://arxiv.org/abs/2411.06481) | Feng Chen, Qi Chen, Akide Liu, Zeyu Zhang, Ha0Tang | - This paper introduces Key Frame Mask Mamba (KMM), a novel architecture for extended motion generation that addresses the challenges of memory decay and poor text-motion alignment in previous methods by strategically masking key frames based on local density and minimum distances between high density motion embeddings within the latent space. - Using a customized contrastive learning strategy to learn dynamic text embeddings, the approach improves motion-text alignment by dynamically learning text encodings and enabling the generation of more accurate and aligned motion sequences. - On the BABEL dataset, KMM achieves state-of-the-art performance with a reduction of more than 57% in FID and 70% in parameters compared to existing methods. - A newly introduced benchmark, BABEL-D, focuses on evaluating text-motion alignment for directional instructions, and demonstrates KMMâ€™s superior performance. - User studies further confirmed the efficacy of the proposed KMM model through qualitative and quantitative analysis of the generated motions across a diversity of prompts. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
