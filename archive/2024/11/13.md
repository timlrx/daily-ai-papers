

## Papers for 2024-11-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) | Chengyue Wu, Wen Liu, Xiaokang Chen, Xingchao Liu, Yiyang Ma | - JanusFlow is a unified multimodal model that integrates autoregressive language models with rectified flow for both image understanding and generation tasks. - The model uses a minimalist architecture with a lightweight encoder and decoder to adapt the LLM for rectified flow operations, and employs two key strategies: decoupling understanding and generation encoders and aligning their representations during training. - On text-to-image generation, JanusFlow surpasses existing models on benchmarks like MJHQ FID-30k, GenEval, and DPG-Bench. - It also achieves state-of-the-art performance in multimodal comprehension tasks on benchmarks like MMBench, SeedBench, and GQA, exceeding specialized models. - These results are achieved with a compact 1.3B parameter LLM, showing the potential for efficient and versatile vision-language models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | [Link](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) |
| [Stronger Models are NOT Stronger Teachers for Instruction Tuning](https://arxiv.org/abs/2411.07133) | Radha Poovendran, Luyao Niu, Fengqing Jiang, Zhangchen Xu, yuchenlin | - This paper challenges the assumption that larger language models (LLMs) are always better teachers for instruction tuning of smaller LLMs, a phenomenon termed the "Larger Models' Paradox." - Through experiments across five base models and twenty response generators, they demonstrate that larger models within a model family don't always lead to better instruction-following performance in smaller models after fine-tuning. - Existing metrics for data selection, such as quality, difficulty, and response length, fail to predict response generator effectiveness because they don't account for teacher-student model compatibility. - They propose a new metric, Compatibility-Adjusted Reward (CAR), that considers both the reward of generated responses and their compatibility with the base model (measured by loss on the base model), which outperforms baseline metrics in predicting response generator effectiveness. - Open-source models like Gemma-2-9b-it and Qwen2.5-72B-Instruct were found to be highly effective as response generators, outperforming even closed-source models like GPT-4 in some cases. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/Magpie-Align/Magpie-100K-Generator-Zoo) |
