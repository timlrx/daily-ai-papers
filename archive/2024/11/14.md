

## Papers for 2024-11-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Large Language Models Can Self-Improve in Long-context Reasoning](https://arxiv.org/abs/2411.08147) | Mo Yu, Lemao Liu, Zesen Cheng, Cheng Yang, Siheng99 | - This paper introduces SEALONG, a self-improving method for Large Language Models (LLMs) to enhance long-context reasoning. - SEALONG samples multiple reasoning trajectories from the LLM, scores them using Minimum Bayes Risk (MBR), and fine-tunes using either supervised learning or preference optimization. - Experiments show improvement on LLMs like Llama-3.1-8B-Instruct by 4.2 points. - SEALONG outperforms existing methods reliant on human annotations or expert models by leveraging the LLM's own generated outputs. - The results demonstrate substantial potential for LLMs to self-improve in long-context reasoning and suggests promise for further research. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/SihengLi99/SEALONG) | N/A |
| [Direct Preference Optimization Using Sparse Feature-Level Constraints](https://arxiv.org/abs/2411.07618) | Hanqi Yan, Minjun Zhu, Hongbo Zhang, Chak Tou Leong, Qingyu Yin | - This paper introduces Feature-level constrained Preference Optimization (FPO), a novel method for aligning Large Language Models (LLMs) with human preferences. - FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment, simplifying the alignment process while ensuring stability. - FPO achieves an above 5% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines on benchmark datasets such as AlpacaEval-2 and Arena-Hard. - By constraining the shifts of sparse features during training, FPO achieves results that meet or exceed the effectiveness of sequential KL divergence with lower computational cost (17.6% reduction compared to TDPO2). - FPO combines the efficiency of SimPO with the constraint quality of sequential KL, making it a promising solution for efficient and controllable LLM alignment. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/abs/2411.08868) | Benoît Sagot, Éric de la Clergerie, Rian Touchent, Francis Kulumba, Wissam Antoun | - This paper introduces two new versions of the French language model CamemBERT: CamemBERTav2 and CamemBERTv2. - CamemBERTav2 is based on the DeBERTaV3 architecture and uses a Replaced Token Detection (RTD) training objective, while CamemBERTv2 uses the RoBERTa architecture with a Masked Language Modeling (MLM) objective. - Both models are trained on a significantly larger and more up-to-date dataset than their predecessors, with an updated tokenizer to better handle the nuances of modern French text. - Evaluations on various NLP tasks, including question answering, named entity recognition, and text classification, show that both models significantly outperform previous versions, particularly CamemBERTav2 which yielded the best performance in most cases. - All models and checkpoints are publicly available on Hugging Face. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification'] | N/A | [Link](https://huggingface.co/almanach?search_models=camembert+v2) |
| [Can sparse autoencoders be used to decompose and interpret steering vectors?](https://arxiv.org/abs/2411.08790) | Adam Mahdi, Yushi Yang, Harry Mayne | - This paper investigates why applying sparse autoencoders (SAEs) directly to steering vectors results in inaccurate decompositions, hindering interpretability. - Two primary reasons are identified: steering vectors fall outside the training distribution of SAEs, and SAEs cannot accommodate meaningful negative projections present in steering vectors. - The study uses the corrigibility steering vector as a case study, demonstrating that the SAE decomposition is largely driven by the encoder bias, overshadowing the steering vector's contribution. - Scaling the steering vector's L2-norm does not resolve the out-of-distribution issue, as default components present in model activations are absent in steering vectors due to the subtraction process involved in their creation. -  The restriction of SAEs to non-negative reconstruction coefficients leads to misinterpretations, as it overlooks negative projections in feature directions, and these negative projections can also cause spurious positive activations in other features due to negative cosine similarity between features. | ['Natural Language Processing'] | [Link](https://github.com/HarryMayne/SV_interpretability) | [Link](https://huggingface.co/google/gemma-scope-2b-pt-res/tree/main/layer_14/width_16k/average_10_173) |
