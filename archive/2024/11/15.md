

## Papers for 2024-11-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models](https://arxiv.org/abs/2411.09595) | Jun Zhu, Hang Su, Yikai Wang, Jonathan Lorraine, Zhengyi Wang | - LLaMA-Mesh is a novel approach that unifies 3D mesh generation with Large Language Models (LLMs) by representing meshes as plain text, allowing seamless integration without tokenizer or vocabulary modifications. - The model leverages the OBJ file format, treating vertex coordinates and face definitions as text sequences, enabling LLMs to process 3D data directly. - A supervised fine-tuning (SFT) dataset with text-3D pairs and interleaved dialogues is used to train a pretrained LLaMA model, enabling it to generate 3D meshes from text prompts, understand 3D meshes, and maintain conversational abilities. - LLaMA-Mesh achieves mesh generation quality comparable to specialized 3D generation models while preserving the LLMs' language capabilities, as demonstrated by qualitative and quantitative results. - The work represents a significant step towards integrating multi-modal content generation within a unified language model. | ['Text-to-3D', 'Multimodal', 'Natural Language Processing'] | N/A | N/A |
| [Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/abs/2411.09009) | Philipp Krähenbühl, Vladlen Koltun, Alexander Hertzberg, Brody Huval, erikwijmans | - This paper introduces Cut Cross-Entropy (CCE), a novel method to compute cross-entropy loss and its gradients without materializing the full logits matrix in memory. - CCE performs matrix multiplications and log-sum-exp operations within flash memory, reducing the memory footprint for loss calculation from gigabytes to megabytes. - It leverages softmax sparsity to further improve throughput by skipping negligible gradient computations. - Experiments on large language models like Gemma 2 and Llama 3 demonstrate significant memory reduction and increased batch size without impacting training speed or convergence. - The proposed method facilitates training larger language models with extended vocabulary sizes under memory constraints. | ['Natural Language Processing'] | [Link](https://github.com/apple/ml-cross-entropy) | N/A |
| [ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?](https://arxiv.org/abs/2411.06469) | Zhongwei Wan, Che Liu, Shan Chen, Jian Yu, canyuchen | - ClinicalBench, a new benchmark, was introduced to evaluate the performance of LLMs and traditional machine learning models on clinical prediction tasks. - The benchmark includes three common clinical prediction tasks (Length-of-Stay, Mortality, and Readmission), two real-world clinical databases (MIMIC-III and MIMIC-IV), 22 LLMs with varying sizes, and 11 traditional ML models. - Through empirical studies, traditional machine learning models outperformed both general-purpose and medical LLMs across all tasks and datasets, even with varying model sizes, prompting, or fine-tuning strategies. - This suggests a potential deficiency in the clinical reasoning and decision-making capabilities of current LLMs. - The benchmark and code are publicly available to facilitate further research in this domain. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct), [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3), [Link](https://huggingface.co/google/gemma-2-9b-it), [Link](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2-7B-Instruct), [Link](https://huggingface.co/01-ai/Yi-1.5-6B-Chat), [Link](https://huggingface.co/01-ai/Yi-1.5-9B-Chat), [Link](https://huggingface.co/01-ai/Yi-1.5-34B-Chat), [Link](https://huggingface.co/lmsys/vicuna-7b-v1.5), [Link](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), [Link](https://huggingface.co/internlm/internlm2_5-7b-chat), [Link](https://huggingface.co/openbmb/MiniCPM3-4B), [Link](https://huggingface.co/epfl-llm/meditron-7b), [Link](https://huggingface.co/epfl-llm/meditron-70b), [Link](https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20), [Link](https://huggingface.co/BioMistral/BioMistral-7B), [Link](https://huggingface.co/m42-health/Llama3-Med42-8B), [Link](https://huggingface.co/m42-health/Llama3-Med42-70B), [Link](https://huggingface.co/PharMolix/BioMedGPT-LM-7B), [Link](https://huggingface.co/internistai/base-7b-v0.2), [Link](https://huggingface.co/docs/transformers/en/index) |
| [Hermes: A Large Language Model Framework on the Journey to Autonomous Networks](https://arxiv.org/abs/2411.06490) | Merouane Debbah, Antonio De Domenico, Ali Maatouk, Fadhel Ayed, nicopi | - Hermes is a novel chain-of-agent LLM framework designed to automate the creation of Network Digital Twins (NDTs) using blueprints, enhancing the path towards autonomous network management. - Unlike existing NDT approaches that require distinct architectures for each use case, Hermes uses LLMs to generate step-by-step logical blocks (blueprints) for NDT construction, improving flexibility and scalability. - The framework consists of a Designer agent for creating and refining the blueprint based on network data and policies, and a Coder agent for translating the blueprint into executable Python code. - Experimental results across four autonomous network tasks demonstrated that Hermes with GPT-40 as LLM consistently outperforms chain-of-thought prompting and direct code generation, achieving success rates up to 80%. - While open-source LLMs show limited performance independently, integrating them with a library of expert-designed models significantly improves their effectiveness in NDT blueprint design, highlighting the potential for broader application. | ['Natural Language Processing'] | N/A | N/A |
