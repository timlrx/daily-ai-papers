

## Papers for 2024-11-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices](https://arxiv.org/abs/2411.10640) | wolf1110, AJZhou, liuyangbian, yina0, lucky-lance | - BlueLM-V-3B is a 3 billion parameter multimodal large language model designed for mobile devices, featuring a 2.7B parameter language model and a 400M parameter vision encoder (SigLIP). - It introduces a relaxed aspect ratio matching method for dynamic image resolution, reducing the number of image tokens processed by the vision encoder without sacrificing model performance. - System optimizations include batched image encoding, pipeline parallelism for image processing, token downsampling, chunked computing of input tokens, and mixed-precision quantization. - BlueLM-V-3B achieves state-of-the-art performance on the OpenCompass benchmark, with an average score of 66.1, surpassing larger models like MiniCPM-V-2.6 and InternVL2-8B. - On a MediaTek Dimensity 9300 processor, it uses 2.2GB of memory, encodes 768x1536 images in 2.1 seconds, and generates text at 24.4 tokens/second. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Generative World Explorer](https://arxiv.org/abs/2411.11844) | Daniel Khashabi, Alan Yuille, Tianmin Shu, jienengchen, TaiMingLu | - The paper introduces Generative World Explorer (Genex), a novel egocentric video generation model that allows embodied agents to "mentally" explore 3D environments (e.g., urban scenes) by generating imagined future observations, and then use these observations to update the agent's beliefs about the world, which enable the agent to make more informed decisions. - The architecture is based on a video diffusion model that takes an initial egocentric view (represented as a panoramic image), the intended movement direction as action input, and then streams a video of future egocentric observations.  - A spherical-consistent learning objective is introduced to ensure that generated pixels are continuous in the spherical space and improve the consistency and coherence of generated videos during long imaginative exploration. - The experimental evaluation, with a new synthetic urban scene dataset (Genex-DB) and new Embodied QA benchmark (Genex-EQA), shows that Genex generates high-quality and consistent observations during imaginative exploration and improves an existing LLM agent’s decision-making process. - The authors extend Genex to multi-agent scenarios, where an agent infers the perspectives of other agents to make decisions based on a more complete understanding of the situation. | ['Text-to-Video', 'Computer Vision', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/Beckschen/genex) | [Link](https://generative-world-explorer.github.io/) |
| [Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering](https://arxiv.org/abs/2411.11504) | Ben He, Boxi Cao, Xinyu Lu, Yanjiang Liu, Xinyan Guan | This paper introduces verifier engineering, a novel post-training paradigm for foundation models.  It leverages automated verifiers to perform verification tasks and deliver feedback to the models, enhancing their capabilities.  The framework systematically categorizes this process into three stages: search, verify, and feedback.  The authors provide a comprehensive overview of state-of-the-art research in each stage. This approach is presented as a fundamental pathway toward achieving Artificial General Intelligence. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/icip-cas/Verifier-Engineering) | N/A |
| [SlimLM: An Efficient Small Language Model for On-Device Document Assistance](https://arxiv.org/abs/2411.09944) | Viet Dac Lai, Seunghyun Yoon, Phat T. Nguyen, Thang M. Pham, Franck-Dernoncourt | - SlimLM, a series of small language models (SLMs) optimized for document assistance tasks on mobile devices like smartphones, is introduced. - The models range from 125M to 1B parameters and are pre-trained on SlimPajama-627B and fine-tuned on DocAssist, a new dataset constructed for summarization, question answering, and question suggestion. - SlimLM models demonstrate comparable or superior performance to existing SLMs of similar sizes on a Samsung Galaxy S24, efficiently handling contexts up to 800 tokens. - An Android application demonstrates SlimLM's real-world applicability. - The models offer a balance between performance, efficiency, and privacy for on-device document processing, potentially reducing reliance on server-based APIs. | ['Natural Language Processing', 'Document Question Answering', 'Question Answering', 'Summarization'] | N/A | N/A |
| [Top-$nσ$: Not All Logits Are You Need](https://arxiv.org/abs/2411.07641) | Liusheng Huang, Hongli Xu, Jianchun Liu, tomorrowdawn | - This paper introduces top- σ, a novel sampling method for large language models (LLMs) that operates directly on pre-softmax logits by leveraging a statistical threshold. - The method distinguishes between a Gaussian-distributed noisy region and a distinct informative region in the logits, enabling efficient token filtering without complex probability manipulations. - Unlike existing methods, top- σ maintains a stable sampling space regardless of temperature scaling, making it suitable for test-time scaling techniques. - Experimental results across four reasoning-focused datasets demonstrate that top- σ outperforms existing sampling approaches and greedy decoding, while maintaining consistent performance at high temperatures. - The theoretical analysis of top-no provides further insights into its behavior and temperature invariance property. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts](https://arxiv.org/abs/2411.10669) | Nanyi Fei, Hongpeng Lin, Guoxing Yang, Yanqi Dai, Jinqiang Long | - This paper introduces Awaker2.5-VL, a Mixture of Experts (MoE) architecture designed for Multimodal Large Language Models (MLLMs) to address the "multi-task conflict" issue, where mixing data from various tasks leads to performance degradation. - Awaker2.5-VL utilizes multiple sparsely activated expert models, each specializing in a specific task, along with a global expert for general capabilities, and a gating network to control expert activation. - The model employs Low-Rank Adaptation (LoRA) for each expert to reduce training and inference costs and incorporates a simplified routing strategy for enhanced training stability. - The paper uses Qwen2-VL-7B-Instruct as its base model and evaluates performance on benchmarks such as MME-Realworld, MME-Realworld-CN, and MMBench. - Awaker2.5-VL achieves state-of-the-art results on the MME-Realworld-CN benchmark and shows significant improvements over the base model on other benchmarks, including a 5-point improvement in overall score on MME-Realworld-CN. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Object Detection'] | [Link](https://github.com/MetabrainAGI/Awaker) | N/A |
| [LLäMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171) | Andreas Hotho, Julia Wunderle, Jan Pfister | - This paper introduces LLäMmlein, two new German-only decoder-only LLMs (120M and 1B parameters), trained from scratch on a filtered and preprocessed version of the RedPajama dataset. - A new German tokenizer with a 32,000 token vocabulary was created and models were evaluated on the SuperGLEBer and lm-evaluation-harness-de benchmarks, showing competitive performance against similarly sized models and even some larger models. - The 1B model matches the performance of much larger models, like the German finetuned Llama 8B on the SuperGLEBer benchmark and its instruction-tuned version outperforms all other 1B models on TruthfulQA by at least 6%. - Intermediate checkpoints were analyzed to track the learning dynamics, revealing varying rates of improvement across different tasks. - All artifacts, including the models, code, and data, will be open-sourced to promote transparency and future research within the German NLP community. | ['Natural Language Processing', 'Text Generation', 'Question Answering', 'Token Classification', 'Sentence Similarity'] | N/A | [Link](https://huggingface.co/cis-lmu/bavarian_to_english), [Link](https://huggingface.co/LSX-UniWue/Guanako), [Link](https://huggingface.co/FreedomIntelligence/alpaca-gpt4-deutsch), [Link](https://huggingface.co/FreedomIntelligence/evol-instruct-deutsch), [Link](https://huggingface.co/FreedomIntelligence/sharegpt-deutsch), [Link](https://huggingface.co/DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1) |
| [Adaptive Decoding via Latent Preference Optimization](https://arxiv.org/abs/2411.09661) | Jason Weston, Asli Celikyilmaz, Ping Yu, Ilia Kulikov, Shehzaad Dhuliawala | - This paper introduces Adaptive Decoding, a method for dynamically adjusting the sampling temperature during language model generation, leading to improved performance on tasks requiring varying levels of creativity and factual accuracy. - The core component is the Adaptive Decoder module, a small neural network added to the LLM's architecture, predicting the optimal temperature at either the token or sequence level. - Latent Preference Optimization (LPO), a novel training approach based on Direct Preference Optimization, trains the Adaptive Decoder by sampling multiple responses with varying temperatures, scoring them with a reward model, and learning to prefer temperatures associated with higher-ranked responses. - Experiments on a combined dataset (UltraMathStories) of math, creative writing, and general instructions show that Adaptive Decoding outperforms fixed temperature baselines. - Additionally, the method demonstrates success in constrained creative writing, where it learns to use low temperatures for constrained parts and higher temperatures for creative parts of the text, outperforming greedy and high-temperature baselines. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
