

## Papers for 2024-11-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [RedPajama: an Open Dataset for Training Large Language Models](https://arxiv.org/abs/2411.12372) | Shane Adams, Yonatan Oren, Quentin Anthony, Daniel Fu, Maurice Weber | - The paper releases RedPajama-V1, an open reproduction of the LLaMA training dataset, and RedPajama-V2, a new web-based dataset comprising over 100 trillion tokens with quality signals for filtering. - RedPajama-V2 dataset emphasizes transparency by documenting its creation process, offering data at scale, and includes artifacts and quality signals for filtering and creating higher quality datasets. - This dataset has been instrumental in training production-ready large language models, such as Snowflake Arctic, Salesforce's XGen, and AI2's OLMo. - Ablation studies are conducted using decoder-only transformer models with up to 1.6B parameters, demonstrating how quality signals enhance dataset curation. - The study emphasizes the potential of RedPajama in building more transparent, high-performing large language models. | ['Natural Language Processing'] | [Link](github.com/togethercomputer/RedPajama-Data) | [Link](huggingface.co/datasets/togethercomputer/RedPajama-Data-1T), [Link](huggingface.co/datasets/togethercomputer/RedPajama-Data-V2) |
| [Building Trust: Foundations of Security, Safety and Transparency in AI](https://arxiv.org/abs/2411.12275) | Huamin Chen, Mark Bestavros, Emily Fox, Garth Mollett, huzaifas-sidhpurwala | - This paper explores the security and safety implications of publicly available AI models, particularly large language models (LLMs). - It reviews current security and safety scenarios, highlighting challenges like issue tracking, remediation, and the lack of established lifecycle and ownership processes for AI models. - The paper proposes comprehensive strategies to enhance security and safety for both model developers and end-users. - It discusses the distinction between AI security (protecting systems from threats) and AI safety (preventing harm from the system's operation), emphasizing the need for a holistic approach to AI risk management. - The paper also suggests adapting existing vulnerability disclosure processes for AI security flaws and proposes the establishment of a central body for tracking safety hazards. | ['Natural Language Processing'] | N/A | N/A |
| [Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages](https://arxiv.org/abs/2411.12240) | D. J. Bora, tamang0000 | - This research paper evaluates the performance of tokenizers used by 12 Large Language Models (LLMs) across all 22 official languages of India. - The study uses Normalized Sequence Length (NSL) as the key metric for evaluation and finds that the SUTRA tokenizer outperforms other models, including Indic-specific models, in 14 out of 22 languages. - Notable findings include SUTRA's superior performance with Indic languages, GPT-40's improvement over GPT-4 in processing Indian languages, and the comparatively limited performance of Project Indus. - This suggests that Project Indus' better performance on some Indian language is tied to the training on common scripts (Devanagari) between the languages and not to the linguistic understanding. - The study highlights the importance of developing targeted tokenization strategies for multilingual and Indic-centric LLMs. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |
