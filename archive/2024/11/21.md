

## Papers for 2024-11-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2411.10958) | Jun Zhu, Jia Wei, Pengle Zhang, Haofeng Huang, jt-zhang | - SageAttention2 is a new attention mechanism that uses 4-bit matrix multiplication and additional precision-enhancing techniques to accelerate attention computation while maintaining precision. - It quantizes matrices Q and K to INT4 in warp-level granularity, and matrices P and V to FP8, along with smoothing techniques for Q and V to enhance accuracy. - An adaptive mixed-precision method employs 8-bit attention for problematic layers/timesteps and 4-bit attention for others, further improving accuracy. - On an RTX 4090, SageAttention2 achieves a peak performance of 485 TOPS, surpassing FlashAttention2 and xformers by approximately 3.1x and 5.4x, respectively. - Comprehensive experiments across diverse models, including large language, image generation, and video generation models, demonstrate negligible end-to-end metric loss with SageAttention2. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Image-to-Video', 'Text-to-Video', 'Image Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation](https://arxiv.org/abs/2411.13281) | Mohan Kankanhalli, Jing Ma, Dongxu Li, teowu, Ziyang | - Introduces VideoAutoArena, an automated arena-style benchmark for evaluating Large Multimodal Models (LMMs) in video analysis using simulated user interactions and open-ended questions. - Employs user simulation with role-playing by LMM agents to generate diverse, realistic questions based on video content and user personas with varying degrees of relevance to the video. - Implements a fault-driven evolution strategy to progressively increase question complexity, challenging models to address increasingly difficult video analysis scenarios. - Leverages an automated judging system based on the LMM-as-a-Judge paradigm, comparing responses and ranking models using a modified ELO rating system, showing strong alignment with human judgment (87.29%). - Introduces an auxiliary benchmark, VideoAutoBench, streamlining LMM evaluation by comparing model responses against human-selected winners from a subset of VideoAutoArena battles, demonstrating consistent ranking results between the arena and the bench. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text'] | N/A | [Link](https://videoautoarena.github.io/) |
| [When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training](https://arxiv.org/abs/2411.13476) | Cunxiao Du, Tongyao Zhu, Chao Du, Qian Liu, haonan3 | - This paper introduces AnchorAttention, a novel attention mechanism designed to improve long-context training of Large Language Models (LLMs) by addressing the numerical instability of Rotary Position Embedding (RoPE) when using BFloat16 precision. - AnchorAttention mitigates the issue by treating the first token in the input sequence as a shared anchor with a consistent position ID, visible to all documents within the context window, but masking out attention across different documents. - This reduces attention computations and the accumulation of numerical errors while ensuring the model learns a full spectrum of rotational angles in RoPE.  - Experiments on the RULER benchmark and real-world long-context datasets like LongBench show that AnchorAttention outperforms standard full attention and intra-document attention, boosting performance and reducing training time by over 50%. - AnchorAttention maintains performance on medium and short context benchmarks such as MMLU and HellaSwag while excelling in long context settings. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/haonan3/AnchorContext) | N/A |
| [ORID: Organ-Regional Information Driven Framework for Radiology Report Generation](https://arxiv.org/abs/2411.13025) | Dongnan Liu, Ziyong Feng, Xiang An, Tiancheng Gu, Kaichengalex | - This paper introduces ORID, an Organ-Regional Information Driven framework, for generating radiology reports from medical images.  - ORID leverages a fine-tuned multi-modal large language model (LLaVA-Med-RRG) to generate organ-specific diagnostic descriptions. - It employs an organ-based cross-modal fusion module and an organ importance coefficient analysis module to integrate image and text features effectively and to reduce the influence of unrelated organ regions, respectively.  - The framework is evaluated on IU-Xray and MIMIC-CXR datasets, achieving state-of-the-art performance in NLG metrics on both datasets.  - ORID also demonstrates superior clinical efficacy on MIMIC-CXR compared to other models. | ['Image-to-Text', 'Multimodal'] | N/A | N/A |
