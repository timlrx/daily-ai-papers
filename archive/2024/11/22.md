

## Papers for 2024-11-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442) | Yangzhou Liu, Yue Cao, Wenhai Wang, Zhe Chen, Weiyun Wang | - This paper introduces Mixed Preference Optimization (MPO), a method for enhancing the multimodal reasoning capabilities of Large Language Models (LLMs), by combining supervised fine-tuning loss with preference and quality losses. - A new dataset, MMPR, a large-scale multimodal reasoning preference dataset is created using an automated preference data construction pipeline. - The InternVL2-8B-MPO model, trained using MPO, achieves state-of-the-art performance on MathVision (25.7% accuracy) among open-source models and a score of 67.0% on MathVista, outperforming the baseline InternVL2-8B by 8.7 points. - MPO also leads to improved performance on hallucination benchmarks like POPE and CRPE, and complex VQA benchmarks like MM-Vet and LLaVA-bench, comparable to the much larger InternVL2-76B model. - The paper includes ablation studies demonstrating the impact of different optimization algorithms, data scale, and hyperparameters on performance. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Text Generation', 'Image-to-Text'] | N/A | N/A |
| [Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/abs/2411.14405) | Tianqi Shi, Hao Wang, Bo Zeng, Huifeng Yin, Yu Zhao | - Marco-01 is a large language model fine-tuned for enhanced reasoning abilities, focusing on both disciplines with standard answers (math, physics, coding) and open-ended problem-solving. - The model leverages Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies. - Marco-01 achieved accuracy improvements of +6.17% on MGSM (English) and +5.60% on MGSM (Chinese) datasets, demonstrating enhanced reasoning capabilities.  - In translation tasks, Marco-01 excels at understanding colloquial nuances, translating slang expressions accurately, and surpassing tools like Google Translate. - It employs different granularities for MCTS actions ('steps' and 'mini-steps') to navigate diverse problem complexities and incorporates reflection mechanisms for self-correction and error detection in its reasoning process. | ['Question Answering', 'Natural Language Processing', 'Translation', 'Text2Text Generation'] | N/A | N/A |
| [OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://arxiv.org/abs/2411.14199) | Amanpreet Singh, Weijia Shi, Rulin Shao, jacquelinehe, akariasai | - Introduced OpenScholar, a retrieval-augmented language model (LM) designed to synthesize information from scientific literature. - OpenScholar leverages a specialized datastore of 45 million open-access papers, retrievers trained on scientific text, and an iterative self-feedback generation process. - Developed ScholarQABench, a multi-domain benchmark with nearly 3,000 expert-written queries and answers across computer science, physics, neuroscience, and biomedicine, to evaluate OpenScholar and other LMs. - Demonstrated that OpenScholar outperforms existing systems, including GPT-4, on ScholarQABench, particularly in citation accuracy, with human evaluation showing preference for OpenScholar responses over expert-written ones in over half of cases. - Open-sourced the code, models, datastore, data, and a public demo for OpenScholar and ScholarQABench. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/AkariAsai/OpenScholar), [Link](https://github.com/AkariAsai/ScholarBench) | [Link](https://huggingface.co/OpenScholar/openscholar-v1) |
| [Multimodal Autoregressive Pre-training of Large Vision Encoders](https://arxiv.org/abs/2411.14402) | Michal Klein, Philipp Dufter, Xiujun Li, Mustafa Shukor, efini | - AIMv2, a family of generalist vision encoders, is introduced, utilizing a novel multimodal autoregressive pre-training method with image and text inputs. - The model architecture consists of a vision transformer encoder with prefix attention, followed by a causal multimodal decoder that autoregressively generates image patches and text tokens. - AIMv2 demonstrates strong performance across various vision and multimodal tasks, including image recognition, object detection, grounding, and multimodal understanding benchmarks. - Notably, AIMv2 outperforms state-of-the-art contrastive models like CLIP and DINOv2 in several tasks, exhibiting strong scaling properties with increasing data and parameters. - The model achieves 89.5% accuracy on ImageNet-1k with a frozen trunk after high-resolution finetuning. | ['Multimodal', 'Computer Vision', 'Image Classification', 'Object Detection', 'Image Feature Extraction'] | [Link](https://github.com/apple/ml-aim) | N/A |
| [Ultra-Sparse Memory Network](https://arxiv.org/abs/2411.12364) | Defa Zhu, Qiyang Min, Taoer, xyzed, FetchFortune | - This paper introduces UltraMem, a novel architecture incorporating large-scale, ultra-sparse memory layers to enhance the performance of transformer models. - UltraMem builds upon the Product Key Memory (PKM) concept and introduces several improvements including Tucker Decomposed Query-Key Retrieval, Implicit Value Expansion, and Multi-Core Scoring. - The paper claims that UltraMem reduces inference latency while maintaining model performance. - Experiments show UltraMem achieves up to 6x speedup compared to Mixture of Experts (MoE) models at the same scale and with a given computation budget. - It also exhibits favorable scaling properties, outperforming traditional and MoE models on various benchmarks including MMLU, TriviaQA, and BBH. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Hymba: A Hybrid-head Architecture for Small Language Models](https://arxiv.org/abs/2411.13676) | Zijia Chen, Wonmin Byeon, Shizhe Diao, Yonggan Fu, Xin Dong | - Hymba, a family of small language models (LLMs), introduces a hybrid-head parallel architecture integrating transformer attention and state space models (SSMs) for improved efficiency and performance. - This architecture allows parallel processing, leveraging attention heads for high-resolution recall and SSM heads for efficient context summarization within the same layer. - Hymba incorporates learnable meta tokens, prepended to prompts to store critical information and optimize attention allocation, along with cross-layer key-value sharing and partial sliding window attention for a compact cache. - Hymba-1.5B-Base surpasses all sub-2B public models and even outperforms Llama-3.2-3B by 1.32% in average accuracy on commonsense reasoning tasks, with an 11.67× cache size reduction and 3.49× throughput improvement. - The instruction-tuned model, Hymba-1.5B-Instruct, achieves state-of-the-art results on various downstream tasks, including GSM8K, GPQA, and the Berkeley function-calling leaderboard. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/NVIDIA/Hymba-1.5B-Base), [Link](https://huggingface.co/NVIDIA/Hymba-1.5B-Instruct) |
| [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2411.14432) | Winston Hu, Jingkang Yang, Hai-Long Sun, Zuyan, THUdyh | - Insight-V is a novel system designed to enhance the visual reasoning capabilities of Multimodal Large Language Models (MLLMs) by generating structured reasoning data and employing a multi-agent training approach. - The system utilizes a two-step data generation pipeline with a progressive strategy to create diverse reasoning paths and a multi-granularity assessment method to ensure data quality, followed by training a multi-agent MLLM system with a reasoning agent and a summary agent to decompose the problem-solving process. - The reasoning agent produces detailed reasoning steps, while the summary agent assesses and selectively utilizes the reasoning to answer the question, with iterative DPO used to refine reasoning quality. - Evaluation on seven visual reasoning benchmarks demonstrates significant performance gains, with an average improvement of 7.0% for LLaVA-NeXT and 2.9% for a stronger base MLLM, showcasing the effectiveness and generalizability of Insight-V. - Insight-V’s data generation pipeline and multi-agent system enables improvements to MLLM reasoning capabilities without needing expensive human labor. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/dongyh20/Insight-V) | N/A |
| [UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages](https://arxiv.org/abs/2411.14343) | Tae-Sun Chung, Akhil Kedia, Bethel Melesse Tessema | - UnifiedCrawl, a method to create large monolingual datasets for low-resource languages by efficiently filtering and extracting data from the Common Crawl corpus using minimal compute resources. - Demonstrates that fine-tuning multilingual LLMs with this data using efficient adapter methods like QLoRA significantly improves performance on low-resource languages while minimizing VRAM usage. - Shows large improvements in language modeling perplexity and few-shot prompting scores on downstream question-answering tasks. - Provides an affordable approach to improve LLMs for low-resource languages using consumer hardware. - The extracted Amharic dataset, UnifiedCrawl-Amharic, is significantly larger than existing Amharic datasets and leads to a 24% improvement in F1 score and 77% improvement in EM score on the Amharic question answering dataset, AmQA, when fine-tuning a 4.5B parameter XGLM model using QLoRA. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/bethelmelesse/unifiedcrawl) | N/A |
| [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://arxiv.org/abs/2411.14257) | Neel Nanda, Senthooran Rajamanoharan, Oscar Obeso, Javier Ferrando | - This paper investigates the mechanisms behind hallucinations in large language models (LLMs), focusing on entity recognition as a key factor. - Using sparse autoencoders (SAEs) as an interpretability tool, the researchers discovered directions in the representation space that detect whether a model recognizes an entity, indicating a form of self-knowledge about its capabilities. - These directions are causally relevant, capable of steering the model to refuse answering questions about known entities or hallucinate about unknown ones.  - The study demonstrates that chat fine-tuning repurposes this existing mechanism from the base model, and that unknown entity recognition directions disrupt the factual recall mechanism by suppressing attention of attribute extraction heads. - Additionally, the researchers identify SAE latents seemingly representing uncertainty, which are predictive of incorrect answers. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/jbloom/Gemma-2b-IT-Residual-Stream-SAEs), [Link](https://huggingface.co/datasets/HuggingFaceFW/fineweb) |
| [Patience Is The Key to Large Language Model Reasoning](https://arxiv.org/abs/2411.13082) | Yijiong Yu | - This paper proposes a method for improving large language model (LLM) reasoning ability by encouraging a more patient reasoning style, without requiring new knowledge or skills. - The method involves training LLMs to favor thorough reasoning processes by using preference optimization, where detailed reasoning is treated as positive examples and simple answers as negative examples. - The model is fine-tuned using a lightweight dataset of mathematical problems and their solutions, which are further refined into more detailed steps. - Experimental results demonstrate a performance increase of up to 6.7% on the GSM8k benchmark, surpassing several previous works despite using less data and a simpler method. - Although the method increases inference time, the trade-off is considered worthwhile for enhanced accuracy in complex problem-solving. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
