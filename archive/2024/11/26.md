

## Papers for 2024-11-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Knowledge Transfer Across Modalities with Natural Language Supervision](https://arxiv.org/abs/2411.15611) | Marco Grangetto, Emanuele Aiello, luca-molinaro, carloalbertobarbano | - This paper introduces Knowledge Transfer, a method for teaching pre-trained visual models new concepts using only textual descriptions. - The method leverages cross-modal interactions, hypothesizing that pre-trained visual encoders possess sufficient low-level features (shape, appearance, color) to represent unknown high-level concepts when provided with a textual description. - Explicit Knowledge Transfer involves synthesizing images via model inversion based on the textual description and then fine-tuning the visual encoder using an image-text matching loss. - Experiments demonstrate successful introduction of novel concepts into CLIP and ViLT models, with improved zero-shot performance on various tasks, including classification, segmentation, and image-text retrieval. - The method also shows potential for out-of-domain generalization, such as introducing medical concepts into models trained on natural images. | ['Multimodal', 'Zero-Shot Classification', 'Zero-Shot Image Classification', 'Image Classification', 'Image Segmentation', 'Image-to-Text'] | N/A | N/A |
| [From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge](https://arxiv.org/abs/2411.16594) | Chengshuai Zhao, Alimohammad Beigi, Liangjie Huang, Bohan Jiang, Dawei Li | - This paper surveys the emerging field of "LLM-as-a-judge," where Large Language Models (LLMs) are used for scoring, ranking, or selecting content in various AI tasks. - The paper introduces a taxonomy categorizing LLM-as-a-judge approaches based on what to judge (e.g., helpfulness, harmlessness), how to judge (tuning, prompting), and where to judge (evaluation, alignment, retrieval, reasoning). - The study compiles existing benchmarks for LLM-as-a-judge evaluation, covering aspects like general performance, bias detection, and domain-specific tasks. - The authors highlight challenges such as bias, dynamic judgment, and the need for self-judging capabilities. - Finally, they suggest promising future directions like incorporating Retrieval Augmented Generation (RAG), creating bias-aware datasets, and developing human-LLM co-judgement systems. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge) | N/A |
| [MH-MoE:Multi-Head Mixture-of-Experts](https://arxiv.org/abs/2411.16205) | Furu Wei, Shuming Ma, Xun Wu, Shaohan Huang | - This paper introduces a novel implementation of Multi-Head Mixture-of-Experts (MH-MoE), enhancing the multi-head mechanism by enabling it to attend to information from various representation spaces within different experts. - The key modifications include adding a "heads" dimension to the token dimension and incorporating two linear projection layers at the beginning and end of the MoE layer. - The new implementation maintains FLOPs and parameter parity with sparse Mixture of Experts models, which improves efficiency. - Experimental results on language models show that this new implementation leads to quality improvements over both vanilla MoE and fine-grained MoE models. - Additionally, the paper demonstrates MH-MoE's compatibility with 1-bit Large Language Models (LLMs), like BitNet. | ['Natural Language Processing'] | N/A | N/A |
| [DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation](https://arxiv.org/abs/2411.16657) | Mohit Bansal, Jaehong Yoon, Han Lin, Jialu Li, Zun Wang | - DREAMRUNNER is a novel story-to-video generation model that uses a hierarchical system with a large language model (LLM) to create detailed scripts and plans for multi-scene, character-driven videos. - It employs retrieval-augmented test-time adaptation with motion-oriented videos and reference images to learn motion and subject priors, enabling fine-grained video generation with enhanced motion quality and seamless transitions. - DREAMRUNNER introduces SR3AI, a spatial-temporal region-based 3D attention and prior injection module, to bind objects and motions precisely while controlling frame-level semantics. - Evaluated on a new dataset, DreamStorySet, and T2V-CompBench, DREAMRUNNER outperforms existing methods in character consistency, text alignment, and smooth transitions, demonstrating strong performance in generating multi-character interactions and compositional video generation. - Notably, DREAMRUNNER shows a 13.1% relative improvement in character consistency, an 8.56% relative gain in text-following ability, and a 27.2% relative improvement in transition smoothness compared to previous state-of-the-art methods. | ['Text-to-Video', 'Multimodal'] | [Link](https://dreamrunner-story2video.github.io/) | N/A |
| [O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?](https://arxiv.org/abs/2411.16489) | Yuxiang Zheng, Yixiu Liu, Xuefeng Li, Haoyang Zou, Zhen Huang | - This paper investigates replicating OpenAI's O1 model capabilities, particularly its long-thought chain reasoning abilities using knowledge distillation from O1's API and supervised fine-tuning. - The authors demonstrate that a 72B parameter base model, fine-tuned on a distilled dataset of tens of thousands of samples, outperforms the O1-preview model on the American Invitational Mathematics Examination (AIME). -  The study also explores the generalization capabilities of the distilled model on diverse tasks including hallucination, safety, and open-domain QA, demonstrating strong generalization and reduced susceptibility to sycophancy despite training solely on mathematical problem-solving data. - The authors emphasize the potential risks of over-reliance on distillation techniques, highlighting concerns about transparency, innovation stagnation, and the cultivation of first-principles thinking in AI research.  - A Technical Transparency Index (TTI) framework is proposed for evaluating and comparing O1 replication efforts based on transparency and reproducibility, advocating for a shift in research culture toward more transparent and innovative approaches. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/GAIR-NLP/01-Journey) | N/A |
| [GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI](https://arxiv.org/abs/2411.14522) | Zhe Chen, Bin Fu, Wei Li, Yanzhou Su, foreverbeliever | - This paper introduces GMAI-VL, a large vision-language model for general medical AI, and GMAI-VL-5.5M, a comprehensive multimodal medical dataset. - GMAI-VL-5.5M is constructed from 219 specialized medical datasets, covering 13 medical imaging modalities and 18 specialties, totaling 5.5M image-text pairs. - GMAI-VL employs a three-stage training strategy: shallow alignment (projector training), deep alignment (vision encoder and projector training), and instruction tuning. - Experimental results indicate that GMAI-VL achieves state-of-the-art performance across various multimodal medical tasks, including visual question answering and medical image diagnosis, outperforming existing models on benchmarks such as VQA-RAD, OmniMedVQA, and Health & Medicine track of MMMU. - The authors claim that the comprehensive task coverage, diverse modalities, and high-quality of GMAI-VL-5.5M contribute to GMAI-VL's superior performance. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/uni-medical/GMAI-VL) | N/A |
| [VisualLens: Personalization through Visual History](https://arxiv.org/abs/2411.16034) | Zhaojiang Lin, Yi Lu, Kai Sun, Deqing Fu, Wang Bill Zhu | - VisualLens is a novel approach for personalized recommendations that leverages a user's task-agnostic visual history, such as personal photos, instead of relying on traditional task-specific interaction logs or textual data. - The method addresses challenges like diversity and noise in visual histories by retrieving relevant images, extracting visual embeddings, captions, and aspect words, and using an iterative refinement process to improve aspect extraction. - VisualLens employs a grid-based approach to process multiple images simultaneously, reducing computational overhead, and uses a joint training strategy to enhance aspect word generation and candidate prediction. - Experimental results on two newly created benchmarks, Google Review-V and Yelp-V, demonstrate significant improvements over baselines and state-of-the-art methods, including a 1.6-4.6% improvement on Hit@3 over GPT-40. - The model uses an 8x8 grid for images along with their related captions and aspect words, which are combined with a d=2048 vector input to make predictions. | ['Multimodal', 'Image-to-Text', 'Question Answering'] | N/A | N/A |
| [One Diffusion to Generate Them All](https://arxiv.org/abs/2411.16318) | Aniruddha Kembhavi, Christopher Clark, Sangho Lee, Tuan Pham, Duong H. Le | - OneDiffusion, a unified, large-scale diffusion model, supports bidirectional image synthesis and understanding across diverse tasks, including text-to-image generation, image editing using various modalities (depth, pose, layout, semantic maps), image restoration (deblurring, upscaling), and multi-view generation. - The model employs a straightforward approach, treating all tasks as frame sequences with varying noise scales during training, eliminating the need for specialized architectures. - Using a novel "One-Gen" dataset, integrating diverse sources and synthetic data, enables scalable joint training across various tasks and resolutions. - Experimental results demonstrate OneDiffusion achieves competitive or state-of-the-art performance on text-to-image generation, multi-view generation, depth estimation, and ID customization, showcasing its strong generalization capabilities. - OneDiffusion supports novel conditioning setups, including text-to-multi-view and image-to-multi-view generation. | ['Text-to-Image', 'Image-to-Image', 'Multimodal', 'Image-to-3D', 'Computer Vision', 'Depth Estimation', 'Keypoint Detection'] | [Link](https://github.com/lehduong/OneDiffusion) | N/A |
| [Cautious Optimizers: Improving Training with One Line of Code](https://arxiv.org/abs/2411.16085) | Qiang Liu, Bo Liu, Lizhang Chen, Kaizhao Liang | - This paper introduces Cautious Optimizer, a one-line code modification for momentum-based optimizers like AdamW and Lion. - The modification involves masking updates where the proposed update direction and current gradient are misaligned, leading to faster and more stable training. - Theoretical analysis shows that this change preserves convergence guarantees and speeds up loss reduction without breaking the Hamiltonian function or Lyapunov analysis. - Empirical results demonstrate significant speed-ups of up to 1.47x on tasks such as LLaMA and MAE pretraining. - The cautious variants achieve improved sample efficiency with virtually no computational overhead, outperforming standard optimizers across different model sizes and datasets. | ['Natural Language Processing', 'Computer Vision'] | [Link](https://github.com/kyleliang919/C-Optim) | N/A |
| [The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz](https://arxiv.org/abs/2411.14486) | Forrest McKee, David Noever | - This research paper introduces a novel evaluation framework, the "impossible test," using a dataset of 675 fundamentally unsolvable problems to assess large language models' (LLMs) ability to acknowledge uncertainty. - Twelve state-of-the-art LLMs were evaluated on their propensity to admit "I don't know" rather than generate incorrect responses, with the best models achieving 62-68% accuracy in acknowledging uncertainty. - An inverse relationship was observed between problem difficulty and model accuracy, with GPT-4 demonstrating better uncertainty recognition on harder problems (35.8%) compared to simpler ones (20%). - The study reveals that models may be more prone to speculate when problems seem solvable and highlights variations in uncertainty acknowledgment across problem categories. - The impossible test contributes to Artificial General Intelligence (AGI) assessment by emphasizing uncertainty recognition as crucial for evaluating machine intelligence and prompting new directions for model training. | ['Question Answering'] | [Link](https://github.com/reveondivad/certify) | N/A |
| [Predicting Emergent Capabilities by Finetuning](https://arxiv.org/abs/2411.16035) | Sergey Levine, Dan Klein, Eric Wallace, sea-snell | - This paper proposes a novel method for predicting the emergence of capabilities in large language models (LLMs) by leveraging task-specific finetuning. - The key insight is that finetuning shifts the point of emergence to smaller models, and the amount of data controls the magnitude of this shift.  - They introduce an "emergence law", a parametric function fitted to finetuning data to model this shift, extrapolating it to predict few-shot emergence. -  Evaluating on standard NLP benchmarks (MMLU, GSM8K, CommonsenseQA, and CoLA), they demonstrate accurate emergence prediction up to 4x the FLOPs in advance using only small pre-emergence models. - Case studies on data quality assessment (OpenLLaMA v1 vs. v2) and complex capability prediction (LLaMA 2 on APPS coding) showcase potential real-world applications. | ['Natural Language Processing'] | N/A | N/A |
| [From CISC to RISC: language-model guided assembly transpilation](https://arxiv.org/abs/2411.16341) | Abdulrahman Mahmoud, Rania Hossam, Chaimaa Abi, Ahmed Heakl | - This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM and RISC-V assembly. - The approach uses a fine-tuned DeepSeekCoder-1.3B model with an extended tokenizer to handle assembly code specifics and a longer context window to capture complex semantic relationships. - Evaluated on real-world applications and compared against Apple's Rosetta 2, CRT achieves 79.25% translation accuracy from x86 to ARMv5 and 88.68% from x86 to RISC-V, along with 1.73x speedup, 1.47x better energy efficiency, and 2.41x better memory efficiency compared to Rosetta 2. - Analysis of the transpiled code reveals that the model maintains functional correctness despite syntactic variations, demonstrating the feasibility of LLM-based binary translation. - Further investigation revealed specific error patterns related to register allocation, memory addressing, and numerical token handling, indicating potential areas for future improvements. | ['Natural Language Processing', 'Translation'] | N/A | [Link](https://huggingface.co/blog/lorinma/yi-coder) |
