

## Papers for 2024-11-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ShowUI: One Vision-Language-Action Model for GUI Visual Agent](https://arxiv.org/abs/2411.17465) | Shiwei Wu, Zhengyuan Yang, Difei Gao, Linjie Li, Kevin Qinghong Lin | - ShowUI, a novel 2B parameter vision-language-action (VLA) model, is designed to enhance Graphical User Interface (GUI) automation by processing visual screenshots, textual instructions, and generating appropriate actions. - The model employs UI-Guided Visual Token Selection, a technique that leverages connected components within UI screenshots to represent redundant visual areas, thus reducing computational overhead by 33% and improving training speed by 1.4x. - ShowUI implements Interleaved Vision-Language-Action Streaming to effectively manage interactions across different modalities and handle navigation history within multi-step GUI tasks. - Trained on a small, high-quality GUI instruction-following dataset consisting of only 256K samples, ShowUI achieves 75.1% accuracy in zero-shot screenshot grounding, outperforming larger models trained on larger datasets. - ShowUI also demonstrates strong navigation capabilities across diverse environments, including web, mobile, and online platforms. | ['Multimodal'] | [Link](https://github.com/showlab/ShowUI) | N/A |
| [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116) | Boris Ginsburg, Fei Jia, Shantanu Acharya | - Star Attention, a two-phase block-sparse approximation method, is introduced to improve the efficiency of Large Language Model (LLM) inference over long sequences. - The method involves an initial context encoding phase with blockwise local attention, followed by a query encoding and token generation phase using global attention via a distributed softmax algorithm. - This approach allows context length to scale linearly with the number of hosts, reducing memory requirements and inference time. - Evaluation on Llama3.1-8B and Llama3.1-70B across long-context benchmarks shows up to 11x faster inference while maintaining 95-100% accuracy compared to baseline methods. - Star Attention is compatible with existing LLM optimization techniques like Flash Attention and KV cache compression for potential further speed improvements. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/NVIDIA/Star-Attention) | N/A |
| [Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration](https://arxiv.org/abs/2411.17686) | Honggang Chen, Donglin Wang, Pengxiang Ding, Xuyang Liu, Yuhang Han | - This paper introduces a novel "filter-correlate-compress" paradigm for training-free token reduction in Multimodal Large Language Models (MLLMs), decomposing the process into three distinct stages for improved interpretability and flexibility. - A suite of methods called FiCoCo is proposed, implementing this paradigm with variants for different MLLM inference phases. - FiCoCo leverages intermediate computation products to minimize FLOPs and achieves up to 82.4% FLOPs reduction with minimal performance impact. - Experimental results on 10 multimodal benchmarks show FiCoCo outperforms state-of-the-art training-free token reduction methods. - Notably, FiCoCo achieves comparable performance to LLaVA-1.5-7B with just 17.6% of the computational cost and 67.6% of GPU memory in practical applications. | ['Multimodal', 'Visual Question Answering'] | [Link](https://ficoco-accelerate.github.io/) | N/A |
| [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/abs/2411.15296) | Xinyu Fang, Bo Li, Shukang Yin, Chaoyou Fu, yifanzhang114 | - This paper presents a comprehensive survey of evaluation methods for Multimodal Large Language Models (MLLMs), categorizing existing benchmarks based on the capabilities they assess (foundational, behavioral, and application-oriented). - The survey discusses the benchmark construction process, including data collection, annotation, and potential challenges like data contamination and benchmark diversity. - It also covers evaluation methods (human, LLM/MLLM, and script-based) and introduces common metrics and available toolkits for streamlined evaluation. - The paper highlights current limitations of MLLMs, such as struggling with fine-grained perception, complex chart understanding, and long-context reasoning. - Finally, it proposes future directions for benchmark development, including creating well-defined capability taxonomies, focusing on real-world applications, and evaluating more diverse modalities beyond vision and language. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text', 'Image-to-Text'] | [Link](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks) | N/A |
| [SketchAgent: Language-Driven Sequential Sketch Generation](https://arxiv.org/abs/2411.17673) | Judith E Fan, Alex Zhao, Kristine Zheng, Tamar Rott Shaham, Yael Vinker | - SketchAgent is a novel language-driven sequential sketch generation method that leverages pre-trained multimodal Large Language Models (LLMs) and requires no training or fine-tuning. - The core innovation lies in its intuitive sketching language, introduced to the model via in-context examples, enabling the LLM to "draw" using string-based actions processed into vector graphics and then rendered onto a pixel canvas. - By drawing stroke-by-stroke, coupled with an LLM's sequential nature and rich prior knowledge, SketchAgent captures the dynamic and evolving process of human sketching. - Evaluations demonstrate SketchAgent's ability to generate diverse sketches, engage in dialogue-driven drawing, collaborate with humans in real-time, and edit sketches via chat, effectively capturing the sequential and dynamic aspects of human sketching. - This approach marks a departure from optimization-based methods, which lack temporal structure, and paves the way for intuitive, interactive artificial sketching systems. | ['Text-to-Image', 'Multimodal'] | N/A | [Link](https://sketch-agent.csail.mit.edu/) |
| [SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE](https://arxiv.org/abs/2411.16856) | XIngang Pan, Tengfei Wang, Shangchen Zhou, Yushi Lan, Yongwei Chen | - SAR3D is a novel framework for fast 3D object generation and comprehensive understanding leveraging a multi-scale 3D vector-quantized variational autoencoder (VQVAE) and autoregressive modeling. - The multi-scale 3D VQVAE tokenizes 3D objects into hierarchical levels of tokens, enabling next-scale prediction training, which significantly reduces generation time, achieving speeds as fast as 0.82 seconds on an A6000 GPU. - For 3D generation, SAR3D employs an autoregressive model that predicts the next scale of the latent triplane representation, conditioned on single image or text prompts. - SAR3D-LLM, an extension of SAR3D for understanding, aligns the latent space of the 3D VQVAE with a large language model, enabling detailed 3D captioning and simultaneous generation and captioning. - Experimental results demonstrate that SAR3D surpasses existing 3D generation methods in both speed and quality, and enables detailed captioning of generated and encoded 3D objects. | ['Text-to-3D', 'Image-to-3D', 'Multimodal', 'Computer Vision'] | [Link](https://cyw-3d.github.io/projects/SAR3D/) | N/A |
| [VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models](https://arxiv.org/abs/2411.17451) | Yifan Song, Xuqing Yang, Zhihui Xie, Yuancheng Wei, Lei Li | - Introduces VL-RewardBench, a benchmark designed to evaluate Vision-Language Generative Reward Models (VL-GenRMs) and address limitations of existing evaluation methods that rely on biased AI-generated preferences or simplistic queries. - VL-RewardBench consists of 1,250 examples across three domains: general multimodal queries, visual hallucination detection, and complex reasoning tasks, curated through AI-assisted annotation with human verification. - Evaluation of 16 leading VL-GenRMs reveals that even top models like GPT-40 achieve only moderate accuracy on VL-RewardBench while open-source models struggle, highlighting the benchmark's challenging nature. - Analysis shows VL-GenRMs struggle more with basic visual perception than complex reasoning and that test-time scaling benefits vary by model capacity. - Demonstrates that critic training of VL-GenRMs for judgment substantially improves their evaluation abilities and that benchmark performance strongly correlates with downstream Best-of-N sampling effectiveness in MMMU-Pro. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis](https://arxiv.org/abs/2411.16173) | Yong Man Ro, Hosu Lee, Hyunjun Kim, Junho Kim | - SALOVA, a novel video-LLM framework, enhances long-form video comprehension through a targeted retrieval process using a new dataset and architectural design. - The SceneWalk dataset, with 87.8K densely captioned long videos, enables capturing scene continuity and rich descriptive context for improved long-form video understanding. - SALOVA integrates a dynamic routing mechanism and spatio-temporal projector to retrieve relevant video segments efficiently, addressing the limitations of current video-LMMs in context length and memory overhead. - A FocusFast approach analyzes selected segments in detail while maintaining overall context awareness for enhanced video interpretation. - Experiments demonstrate SALOVA's superior performance in processing complex long videos, reducing information loss, and maintaining contextual integrity, outperforming existing video-LMMs on benchmarks like Video-MME and LongVideoBench, especially for medium to long videos. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens](https://arxiv.org/abs/2411.17691) | Haitao Mi, Zhisong Zhang, Thomas Hartvigsen, Tao Ge, Xu Ouyang | - This paper reveals that low-bit quantization tends to favor undertrained large language models (LLMs). - The authors study over 1500 quantized LLM checkpoints and derive scaling laws for quantization-induced degradation (QiD) concerning training tokens, model size, and bit width. - They propose using QiD to measure LLM training levels and predict quantization performance for models trained with 100 trillion tokens, finding that low-bit quantization might not be desirable for such extensively trained models. - Based on the scaling laws derived, the authors predict the number of training tokens needed to reach different training levels based on the magnitude of QiD for different LLM sizes. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/Xu-C) |
| [MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts](https://arxiv.org/abs/2411.14721) | Jingdi Le, Wei Liu, Yunqing Liu, Jiatong Li, qq8933 | - MolReFlect, a teacher-student framework, is introduced to perform fine-grained molecule-caption alignments in the molecule-caption translation task. - A larger teacher LLM extracts important phrases from molecule SMILES or captions, aligning them with corresponding characteristics or sub-structures in a zero-shot manner.  - In-Context Selective Reflection refines these alignments using similar examples and perplexity-based selection by a smaller student LLM. - Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT) enhances the student LLM's learning by reformatting context examples into a thought chain, incorporating fine-grained alignments and reasoning.  - MolReFlect achieves state-of-the-art performance on the ChEBI-20 dataset, outperforming baselines like ICMA and BioT5 in both Mol2Cap and Cap2Mol tasks without extra modalities or complex structures. | ['Natural Language Processing', 'Text2Text Generation', 'Translation'] | N/A | N/A |
