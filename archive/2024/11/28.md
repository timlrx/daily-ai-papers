

## Papers for 2024-11-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment](https://arxiv.org/abs/2411.17188) | ranjaykrishna, Tim666, lzy8465, Dipsy0830, shuaishuaicdp | - This paper introduces ISG (Interleaved Scene Graph), a new evaluation framework for assessing the quality of interleaved text and image generation. - ISG uses a scene graph structure to capture relationships between text and image blocks, enabling multi-level evaluation (holistic, structural, block-level, and image-specific). - Alongside ISG, a new benchmark dataset ISG-BENCH containing 1,150 samples across 8 categories and 21 subcategories is introduced to assess model performance on complex language-vision dependencies. - Experimental results reveal that current unified vision-language models perform sub-optimally at generating interleaved content; compositional models that split language and image generation perform better but also have room to improve. - A new compositional baseline agent ISG-AGENT, based on a “plan-execute-refine” pipeline achieves performance improvement compared to other evaluated methods. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://interleave-eval.github.io) | N/A |
| [CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models](https://arxiv.org/abs/2411.18613) | Ruiqi Gao, holynski, atrevithick, doinkda, rundi | - CAT4D is a novel method for generating dynamic 3D scenes from monocular video using a multi-view video diffusion model. - The model is trained on a diverse combination of datasets and uses a novel sampling approach to transform a single video into a multi-view video, enabling robust 4D reconstruction. - CAT4D demonstrates competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, outperforming existing methods. - The model's creative capabilities are highlighted through its ability to generate 4D scenes from both real and generated videos. - The method is applicable to various tasks, including novel view synthesis, dynamic scene reconstruction, and sparse view reconstruction. | ['Text-to-Video', 'Image-to-Video', 'Video Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/google-research/cat4d) | N/A |
| [Large Language Model-Brained GUI Agents: A Survey](https://arxiv.org/abs/2411.18279) | Gezelligheid520, liqul, bowenli, shilhe, vyokky | - This paper presents a comprehensive survey of Large Language Model (LLM)-brained Graphical User Interface (GUI) agents, exploring their evolution, components, techniques, and applications. - LLM-brained GUI agents represent a new frontier in human-computer interaction, allowing users to interact with and control software applications using natural language. - The survey covers key aspects such as agent frameworks, data collection strategies, model optimization methods, evaluation metrics, and real-world use cases. - The paper also identifies key research gaps and challenges in the field, including privacy concerns, latency limitations, and the need for improved human-agent interaction. - It proposes future directions for research and development, focusing on enhancing agent capabilities, ensuring safety and reliability, and addressing ethical considerations. | ['Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation](https://arxiv.org/abs/2411.17945) | Sankalp Sinha, mzafzal, saali14, alootikki, SadilKhan | - This paper introduces MARVEL-40M+, a large-scale dataset with over 40 million text annotations for 8.9 million 3D assets, aimed at improving text-to-3D generation. - MARVEL, a multi-stage annotation pipeline, leverages open-source pretrained multi-view VLMs and LLMs to produce hierarchical descriptions ranging from detailed to concise tags. -  A two-stage text-to-3D framework, MARVEL-FX3D, is presented, which fine-tunes Stable Diffusion with MARVEL-40M+ annotations and utilizes a pretrained image-to-3D network. - Evaluations indicate that MARVEL-40M+ outperforms existing datasets in annotation quality and diversity, and MARVEL-FX3D achieves state-of-the-art results in text-to-3D generation. - Experimental results show that MARVEL-FX3D generates textured 3D meshes from text within 15 seconds, achieving superior prompt fidelity and overall preference compared to existing methods. | ['Text-to-3D', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing](https://arxiv.org/abs/2411.16781) | Shiguang Shan, Hong Chang, Heylon, flow2023, LiyiGang | - UniPose is a novel multimodal framework that unifies human pose comprehension, generation, and editing tasks.  It utilizes a pose tokenizer to convert 3D poses into discrete tokens for seamless integration with LLMs. - The model architecture incorporates a mixture of visual encoders (CLIP and a pose-specific visual encoder) to enhance fine-grained pose perception. - UniPose demonstrates superior performance across various pose-relevant tasks compared to existing methods, including pose comprehension, generation, and editing, as shown by experimental results in tables 2, 3, 4, and 5. - The model exhibits zero-shot generalization capabilities, enabling it to adapt to unseen tasks and enhance pose estimation, as shown in Figure 5. - UniPose addresses the limitations of existing methods by providing a unified multimodal framework that seamlessly handles multiple modalities, enabling finer-grained pose perception and more complex pose editing. | ['Multimodal'] | N/A | N/A |
| [Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding](https://arxiv.org/abs/2411.18462) | Xingyu Chen, Tian Liang, Jiahao Xu, Ziyin Zhang, zptu | - This paper introduces SVIP, a self-verification length policy for speculative decoding systems that dynamically determines the length of draft sequences based on the entropy of each draft token distribution. - SVIP achieves up to 20% walltime speedup on SpecBench and 60% speedup on MT-Bench for long-form generation. - The proposed method is training-free and compatible with any existing speculative decoding methods that generate draft tokens autoregressively. - Experimental results demonstrate consistent wall-time improvements on GliDe & CaPE and EAGLE-2. - The core idea is to adaptively adjust the draft length based on token difficulty, unlike traditional methods using a fixed draft length. | ['Text Generation'] | [Link](https://github.com/Geralt-Targaryen/SVIP) | N/A |
| [ChatRex: Taming Multimodal LLM for Joint Perception and Understanding](https://arxiv.org/abs/2411.18363) | Yihao Chen, Yuda Xiong, Yuqin Yang, Gen luo, Qing Jiang | - ChatRex, a novel multimodal large language model (MLLM), is introduced, featuring a decoupled architecture to address the conflict between perception and understanding tasks, commonly observed in existing MLLMs. - The model utilizes a Universal Proposal Network (UPN), a DETR-based model trained with granularity-based prompt learning, to provide robust object proposals for the LLM's retrieval-based detection process, eliminating coordinate prediction issues. - A new dataset, Rexverse-2M, containing two million image-region-text annotation triplets with varying granularities, was created with a fully automated data engine to train the model on joint perception and understanding tasks. - ChatRex demonstrates strong performance on object detection benchmarks like COCO (48.5 mAP) and LVIS (43.1 mAP), comparable to dedicated object detectors, and outperforms other MLLMs, particularly in multi-object scenes. - The model maintains competitive results on general multimodal benchmarks, showcasing robust understanding and dialogue capabilities, enhanced by the integration of perception abilities. | ['Multimodal', 'Object Detection', 'Visual Question Answering'] | [Link](https://github.com/IDEA-Research/ChatRex) | N/A |
| [Training and Evaluating Language Models with Template-based Data Generation](https://arxiv.org/abs/2411.18104) | yifAI | - This paper introduces Template-based Data Generation (TDG), a novel method for generating large-scale, high-quality mathematical datasets using GPT-4 to automatically generate meta-templates. - The TDG method leverages parameterized templates and a reject-sampling-based verification process to ensure data quality and scalability. - A dataset called TemplateGSM, consisting of over 7 million synthetically generated grade school math problems with verified solutions, is created using TDG. - Experiments show that TemplateGSM significantly improves the performance of LLMs in mathematical reasoning tasks. - The authors release both the TemplateGSM dataset and the TDG code to facilitate further research and development. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/iiis-ai/TemplateMath) | [Link](https://huggingface.co/datasets/math-ai/TemplateGSM) |
