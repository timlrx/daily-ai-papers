

## Papers for 2024-12-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [On Domain-Specific Post-Training for Multimodal Large Language Models](https://arxiv.org/abs/2411.19930) | Xintong Zhang, doubling, edward2021, buaahsh, daixuancheng | - This paper introduces a novel approach for domain-specific post-training of Multimodal Large Language Models (MLLMs), focusing on data synthesis, training pipelines, and task evaluation. - A visual instruction synthesizer is developed using open-source models to extract diverse visual instruction tasks from domain-specific image-caption pairs, outperforming manual rules, GPT-4, and GPT-4V in enhancing MLLM performance on specialized domains such as biomedicine and food. - A single-stage training pipeline, combining synthetic tasks and image-caption pairs, is proposed to improve task diversity and mitigate catastrophic forgetting compared to traditional two-stage training. - Experiments on various MLLMs (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B) across biomedicine and food domains show consistent improvement on diverse domain-specific tasks using the resulting AdaMLLM (Adapted Multimodal Large Language Model). - All implementations will be open-sourced to support further research. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | [Link](https://huggingface.co/AdaptLLM) |
| [Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS](https://arxiv.org/abs/2411.18478) | Zengqi Wen, Feihu Che, Shuai Zhang, fmk345, Jinyang23 | - This paper introduces HiAR-ICL, a novel automated reasoning paradigm that enhances in-context learning (ICL) by shifting the focus from specific examples to abstract thinking patterns, termed "thought cards." - HiAR-ICL uses Monte Carlo Tree Search (MCTS) to construct these thought cards from a small seed dataset and employs a cognitive complexity framework to dynamically match problems with appropriate thought cards during inference. - Five atomic reasoning actions, including System Analysis, One-Step Thought, Chain-of-Thought, Divide and Conquer, and Self-Reflection and Refinement, are defined as fundamental building blocks for these patterns. - The approach outperforms state-of-the-art methods on the MATH benchmark with Qwen2.5-7B-Instruct (79.6% accuracy), surpassing GPT-40 (76.6%) and Claude 3.5 (71.1%). - HiAR-ICL reduces time complexity compared to existing tree search methods by leveraging pre-computed reasoning patterns. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm), [Link](huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data) |
| [DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding](https://arxiv.org/abs/2411.19527) | Mingu Kang, Minseo Kim, Jisoo Kim, junwann, whwjdqls99 | - DisCoRD, a novel method for human motion generation, decodes discrete motion tokens into continuous motion using rectified flow, combining the naturalness of continuous representations with the faithfulness of discrete methods. - It employs an iterative refinement process in continuous space, capturing fine-grained dynamics and ensuring smoother motion, and uses discrete tokens as conditions in raw motion space to reduce noise. - A new metric, symmetric Jerk Percentage Error (sJPE), is introduced to evaluate both under-reconstruction and frame-wise noise in motion. - Extensive evaluations across text-to-motion, co-speech gesture, and music-to-dance generation demonstrate state-of-the-art performance, achieving an FID of 0.032 on HumanML3D and 0.169 on KIT-ML. - DisCoRD is adaptable to any discrete-based motion generation framework and improves naturalness without sacrificing faithfulness. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [Puzzle: Distillation-Based NAS for Inference-Optimized LLMs](https://arxiv.org/abs/2411.19146) | nav4, nailon-nvidia, talor-abr, tomer-nv, abercovich | - Puzzle, a novel framework, leverages decomposed Neural Architecture Search (NAS) and Blockwise Local Distillation (BLD) with Mixed-Integer Programming to create inference-optimized Large Language Models (LLMs) tailored to specific hardware. - This framework optimizes models by creating heterogeneous architectures with varying block configurations, reducing redundant computations while preserving performance. - The resulting model, Nemotron-51B derived from Llama-3.1-70B-Instruct, achieves up to 2.17x inference speedup on a single NVIDIA H100 GPU while retaining 98.4% of the parent model's accuracy. - Demonstrating unprecedented efficiency, Nemotron-51B's training required only 45B tokens compared to over 15T for its parent, setting a new benchmark for throughput and memory efficiency. - This work also introduces a derivative of Llama-3.1-8B-Instruct further demonstrating Puzzle's capacity to create highly efficient models across various hardware and parameter scales. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing](https://arxiv.org/abs/2411.19460) | Hyunjun Kim, dwightro, arkimjh, lakelee | - Introduces Video-Ma$^2$mba, a novel model for long-form video understanding that replaces the attention mechanism in Large Multimodal Models (LMMs) with State Space Models (SSMs), achieving linear scaling in time and memory. - Employs Multi-Axis Gradient Checkpointing (MA-GC) to enhance memory efficiency by retaining only essential activations across multiple computational axes. - Processes long video sequences, equivalent to over two hours of continuous video at 1 FPS, on a single GPU by handling the full sequence without frame sampling. - Improves accuracy and relevance of responses in long video understanding tasks by capturing detailed temporal dynamics. - Demonstrates substantial advantages over existing frameworks on benchmarks like Video-MME and LongVideoBench, showcasing its efficiency in handling lengthy video content and responding effectively to complex queries. | ['Video-Text-to-Text', 'Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification](https://arxiv.org/abs/2411.19638) | nljubesi, TajaKuzman | - This paper introduces a novel teacher-student framework utilizing Large Language Models (LLMs) for multilingual news topic classification without manual annotation. - A GPT model serves as the teacher, automatically annotating news articles in Slovenian, Croatian, Greek, and Catalan with IPTC Media Topic labels to create a training dataset. - Smaller BERT-like student models, specifically XLM-ROBERTa, are then fine-tuned on this dataset, achieving comparable performance to the teacher model while being more computationally efficient.  - The study demonstrates that student models achieve high performance with limited training data and exhibit strong zero-shot cross-lingual capabilities.  -  The best performing model, a multilingual IPTC news topic classifier, is publicly released. | ['Natural Language Processing', 'Text Classification', 'Zero-Shot Classification'] | [Link](https://github.com/TajaKuzman/IPTC-Media-Topic-Classification) | [Link](https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier) |
