

## Papers for 2024-12-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models](https://arxiv.org/abs/2412.01824) | lindahua, TheYJ, yuhangzang, tongwu2020, Zery | - X-Prompt, an auto-regressive large vision-language model, is introduced for in-context image generation across various tasks. - It uses a novel design to compress features from in-context examples, enabling longer context sequences and better generalization to unseen tasks. - A unified training objective for text and image prediction allows the model to leverage task awareness from context. - Experimental results show competitive performance on several image generation tasks, including text-to-image generation, and improved in-context learning capabilities on novel tasks compared to baselines like OmniGen. - The method also exhibits strong capabilities in the image editing task by using Retrieval-Augmented Image Editing (RAIE), where a relevant image example is retrieved as in-context information to enhance editing performance. | ['Text-to-Image', 'Image-to-Image', 'Image Segmentation', 'Depth Estimation', 'Multimodal'] | [Link](https://github.com/SunzeY/X-Prompt) | N/A |
| [GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation](https://arxiv.org/abs/2411.18499) | LiruiZhao, yefly, xuzhaopan, xiaopengpeng, lyuukuu | - Introduces GATE OpenING (OpenING), a benchmark for evaluating open-ended interleaved image-text generation, comprised of 5,400 human-annotated instances across 56 real-world tasks and 23 meta-topics. - Presents IntJudge, a novel judging model trained with a Reference-Augmented Generation (RAG) approach and an Interleaved Arena for data annotation, achieving 82.42% agreement with human judgments, outperforming GPT-4 by 11.34%. - Demonstrates through experiments on OpenING that integrated pipelines for interleaved generation outperform end-to-end models, highlighting the potential of two-stage generators with unified architectures. - Reveals that generating high-quality, coherent interleaved content remains challenging for existing models, while GPT-generated text often surpasses human quality, and human-annotated images are preferred over generated ones. - Provides a comprehensive leaderboard and analysis of various interleaved generation methods, offering insights for future model development and benchmark design. | ['Multimodal', 'Image-to-Text', 'Text-to-Image'] | N/A | N/A |
| [o1-Coder: an o1 Replication for Coding](https://arxiv.org/abs/2412.00154) | Jinlin Xiao, Jiangming Shu, Yuqi Yang, Shangxi Wu, Yuxiang Zhang | - This paper introduces O1-CODER, a framework attempting to replicate OpenAI's O1 model, focusing on coding tasks and enhancing System-2 thinking through Reinforcement Learning (RL) and Monte Carlo Tree Search (MCTS). - The framework incorporates a Test Case Generator (TCG) for automated code evaluation, MCTS for generating reasoning data, and iterative fine-tuning of a policy model, initially producing pseudocode and subsequently full code. - O1-CODER uses pseudocode-based prompting and behavioral actions, addressing the challenges of self-play RL in code generation, including evaluation and process reward design. - While initial experiments on the MBPP benchmark show a slight decrease in overall pass rate with pseudocode, a significant improvement in the Average Sampling Pass Rate suggests enhanced reasoning capabilities when the generated pseudocode is correct. - The paper further discusses the broader implications of moving beyond human-recorded data, the potential of self-play+RL in complex problem solving, and the challenges in applying O1-like models to real-world applications requiring dynamic environment interaction. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/ADaM-BJTU/O1-CODER) | N/A |
| [VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models](https://arxiv.org/abs/2412.01822) | Yueh-Hua Wu, Yong Man Ro, Yu-Chiang Frank Wang, Ryo Hachiuma, BK-Lee | - Introduces VLSI, a new Vision Language Model (VLM) family (2B and 7B parameter sizes) that uses a novel natural language-based distillation process called "Verbalized Layers-to-Interactions" to transfer knowledge from large to small VLMs. - Employs layer-wise distillation with intermediate "verbalizers" to project features into natural language, which allows smaller VLMs to better align with the reasoning processes of larger VLMs, unlike traditional methods that focus solely on final-layer imitation. - Achieves notable performance improvements over GPT-4V (11.0% for 2B and 17.4% for 7B model sizes) on various vision-language benchmarks without increasing model size, module merging, or architectural modifications. - Validated across ten diverse benchmarks, demonstrating state-of-the-art performance and improved efficiency, especially for deployment on resource-constrained devices. - Offers an easily implementable and adaptable approach across different model architectures, showing significant gains with both Qwen2-VL and LLaVA-OV backbones. | ['Multimodal'] | N/A | N/A |
| [SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters](https://arxiv.org/abs/2412.00174) | Huaizhong Zhang, Zhengyu Lin, Weiye Xiao, Jianping Jiang, caizhongang | - SOLAMI is a novel end-to-end social Vision-Language-Action (VLA) model for generating multimodal responses (speech and motion) in interactions with 3D autonomous characters. - The model uses separate tokenizers for speech and motion, converting them into discrete tokens that are fed into a decoder-only LLM backbone (AnyGPT-base, based on LLaMA2-7B). - The model is trained in three stages: tokenizer training, multi-task pre-training for modality alignment (motion-text and speech-text), and instruction tuning on a synthetic multimodal social interaction dataset called SynMSI. - Quantitative results on SynMSI show that SOLAMI outperforms baseline methods (LLM+Speech, AnyGPT fine-tuned, and DLP) in terms of motion quality and inference latency, generating more natural and coherent responses. - A user study conducted with a VR interface further validates SOLAMI's superior performance, demonstrating enhanced user experience across metrics such as motion coherence, motion interaction, speech consistency, and overall experience. | ['Multimodal', 'Text-to-Speech', 'Text-to-Video', 'Text-to-3D', 'Robotics'] | [Link](https://solami-ai.github.io/) | N/A |
| [Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation](https://arxiv.org/abs/2412.01316) | Yuan Zhou, Qiuyue Wang, Yuxuan Cai, hyang0511, Cakeyan | - Presto, a novel video diffusion model, generates 15-second videos with long-range coherence and rich content using a Segmented Cross-Attention (SCA) strategy. - SCA divides hidden states into temporal segments, allowing each to cross-attend to a corresponding sub-caption, enhancing coherence without additional parameters. - The LongTake-HD dataset, comprising 261k content-rich videos with progressive sub-captions, facilitates high-quality long video generation. - Presto achieves 78.5% on VBench Semantic Score and 100% on Dynamic Degree, outperforming state-of-the-art methods in content richness and coherence. - A user study confirms Presto's superiority in scenario diversity, coherence, and text-video alignment compared to open-source and commercial alternatives. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input](https://arxiv.org/abs/2412.01250) | Alessandro Farinelli, Alberto Castellini, Gianni Franchi, e-zorzi, ftaioli | - Introduces Collaborative Instance Navigation (CoIN), a new task for embodied agents involving interactive dialogue with humans to locate target objects in unknown environments. - Presents AIUTA, a training-free method leveraging Vision-Language Models (VLMs) and Large Language Models (LLMs) to facilitate agent self-dialogue, reducing reliance on full initial descriptions. - Includes a novel Normalized-Entropy based technique to estimate and mitigate VLM uncertainty during object description generation. - Introduces CoIN-Bench, a new benchmark with real and simulated human evaluations for CoIN, and demonstrates AIUTAâ€™s state-of-the-art performance on zero-shot instance navigation, handling user input flexibility. - Proposes IDKVQA, a dedicated dataset for evaluating VLM uncertainty estimation and shows the superiority of their approach. | ['Robotics', 'Computer Vision', 'Question Answering'] | N/A | [Link](https://intelligolabs.github.io/COIN) |
| [VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939) | Jing Shao, Xuanjing Huang, LLLeo612, Max9803, Foreshhh | - This paper introduces VLSBench, a new multimodal visual leakless safety benchmark with 2.4k image-text pairs designed to address the Visual Safety Information Leakage (VSIL) problem in existing multimodal safety benchmarks. - VSIL occurs when sensitive image content is revealed in the text query, allowing models to bypass visual processing and make safety decisions based on text alone.  - VLSBench pairs images with neutral text queries, forcing models to rely on visual understanding for safety assessments.  - Experimental results show that VLSBench is challenging for both open-source and closed-source Multimodal Large Language Models (MLLMs), with even the best performing model only achieving a 49.78% safety rate.  - The study also found that multimodal alignment methods outperform textual alignment on VLSBench, highlighting the importance of multimodal reasoning for visual safety in the absence of VSIL. | ['Multimodal'] | N/A | N/A |
| [INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge](https://arxiv.org/abs/2411.19799) | atcbosselut, jjzha, jebish7, shayekh, angelika | - This paper introduces INCLUDE, a multilingual benchmark dataset designed to evaluate the regional knowledge understanding of large language models (LLMs). - INCLUDE consists of 197,243 multiple-choice questions across 44 languages and 15 scripts, collected from various sources, including academic exams, professional certifications, and regional licenses. - The benchmark is designed to address the lack of high-quality evaluation resources in languages other than English and to capture cultural nuances associated with each language. - Experimental results demonstrate that current LLMs achieve high variance in performance between different languages and often struggle with questions requiring regional knowledge. - Analysis suggests that performance limitations stem from model's grasp of specialized regional knowledge for different languages. | ['Question Answering', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/CohereForAI/include-base-44) |
| [VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information](https://arxiv.org/abs/2412.00947) | Rui Zhang, Ranran Haoran Zhang, Sarkar Snigdha Sarathi Das, Yusen Zhang, ryokamoi | - This paper introduces VisOnlyQA, a new dataset designed to evaluate the visual perception capabilities of Large Vision Language Models (LVLMs) on questions related to geometric and numerical information in scientific figures. - VisOnlyQA includes 1,200 multiple-choice questions across 12 tasks and four categories of figures, along with 70k synthetic training instances. - Experiments with 20 LVLMs, including GPT-40 and Gemini 1.5 Pro, reveal poor performance on VisOnlyQA compared to near-perfect human performance. - Fine-tuning on synthetic data shows potential but limited improvement, suggesting both training data and model architecture need improvement. - The authors observed that stronger language models enhanced the visual perception of LVLMs, even though the dataset focuses solely on visual perception. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/psunlpgroup/VisOnlyQA) | N/A |
| [VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation](https://arxiv.org/abs/2412.00927) | Wenhu Chen, Cong Wei, Jie Min, hyang0511, wren93 | - VISTA, a novel Video SpatioTemporal Augmentation framework, synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets by spatially and temporally combining videos. - It leverages a large language model, Gemini 1.5-Pro, to generate question-answer pairs related to the newly synthesized videos. - VISTA-400K, a 400,000-sample video instruction-following dataset based on this approach, improves the performance of various video Large Multimodal Models (LMMs) by an average of 3.3% across four long-video understanding benchmarks. - Introduction of HRVideoBench, the first high-resolution video understanding benchmark, on which VISTA-finetuned models show a 6.5% performance gain. - Ablation studies demonstrate that disabling proposed augmentations reduces model performance, highlighting the quality and importance of the generated data. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering', 'Computer Vision', 'Video Classification'] | N/A | [Link](https://tiger-ai-lab.github.io/VISTA/) |
| [PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos](https://arxiv.org/abs/2412.01800) | Hangyu Guo, Haoze Zhao, Haoran Tang, Meng Cao, zhangysk | - PhysGame, a benchmark designed to evaluate the ability of Video Large Language Models (Video LLMs) to identify and interpret physical commonsense violations in gameplay videos. - Constructed using 880 gameplay videos annotated with multiple-choice questions focused on uncovering glitches that defy physical commonsense understanding, categorized across four primary physical domains: mechanics, kinematics, optics, and material properties, subdivided into twelve distinct categories. -  Analysis shows open-source Video LLMs underperforming compared to proprietary counterparts, leading to the creation of PhysInstruct and PhysDPO, two datasets containing over 174k training examples in total.  - The proposed physical knowledge-enhanced Video LLM, PhysVLM, trained on the introduced datasets, reaches state-of-the-art performance on PhysGame, outperforming existing open-source and commercial models.  - PhysVLM demonstrates strong generalizability, achieving high scores on standard video understanding benchmarks such as Video-MME and VCG. | ['Video-Text-to-Text', 'Multimodal', 'Question Answering'] | [Link](https://github.com/PhysGame/PhysGame) | N/A |
| [FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait](https://arxiv.org/abs/2412.01064) | Gyoungsu Chae, Dongchan Min, Taekyung Ki | - FLOAT is a novel audio-driven talking portrait video generation model based on flow matching in a learned motion latent space, enabling efficient design of temporally consistent motion. - It introduces a transformer-based vector field predictor with frame-wise conditioning, and supports emotion enhancement driven by speech-driven emotion labels. - FLOAT outperforms existing state-of-the-art audio-driven talking portrait methods on HDTF and RAVDESS datasets in terms of visual quality, motion fidelity, and efficiency, achieving FID scores of 21.10 and 31.68, respectively. - The use of flow matching allows for faster and higher quality sampling compared to diffusion-based methods, while the motion latent space ensures temporal consistency and expressiveness.  - Ablation studies confirm the benefits of the proposed FMT architecture and the use of speech-driven emotional labels | ['Text-to-Video', 'Computer Vision', 'Audio'] | N/A | N/A |
| [A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477) | Jingren Zhou, Bolin Ding, Yaliang Li, Xuchen Pan, yanxi-chen | - This paper proposes a two-stage algorithm for enhancing the test-time compute of Large Language Models (LLMs), aiming to boost their success probability on challenging tasks. - The algorithm first generates multiple candidate solutions and then selects the best one through a knockout tournament, where pairs of solutions are compared multiple times. - Theoretical analysis proves that the failure probability of this algorithm decreases exponentially with increased compute, given the assumptions that the LLM has a non-zero probability of generating a correct solution and can distinguish between correct and incorrect solutions better than random chance. - Empirical results on the MMLU-Pro benchmark validate these assumptions and demonstrate performance improvement with increased test-time compute, especially for reasoning-focused questions. - The paper discusses limitations and future research directions, including potential for handling complex tasks via decomposition and exploring more efficient algorithms with provable scaling laws. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning](https://arxiv.org/abs/2412.01408) | Noel Crespi, Reza Farahbaksh, callmesan | - This paper proposes a few-shot cross-lingual audio abuse detection method using Model-Agnostic Meta-Learning (MAML) with pre-trained audio representations in low-resource settings. - The model leverages Whisper and Wav2Vec and evaluates two feature normalization strategies: Temporal Mean and L2 normalization. - Experiments are conducted on the ADIMA dataset, comprising abusive audio clips in 10 Indian languages. - The best-performing model is Whisper with L2-Norm normalization, achieving accuracy scores ranging from 78.98% to 85.22% in the 100-shot setting. - A feature visualization study shows that language similarity can enhance cross-lingual abuse detection, especially in low-resource settings. | ['Audio', 'Audio Classification', 'Natural Language Processing', 'Zero-Shot Classification'] | [Link](https://github.com/callmesanfornow/fsl-audio-abuse.git) | N/A |
