

## Papers for 2024-12-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation](https://arxiv.org/abs/2412.03069) | sweetrabor, gaozong, xuwang, liqingzju, leo1117 | - Introduces TokenFlow, a novel unified image tokenizer with a dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining alignment via shared index mapping, bridging the gap between multimodal understanding and generation. - Demonstrates state-of-the-art autoregressive image generation with a GenEval score of 0.55 at 256x256 resolution and strong reconstruction performance (FID 0.63 at 384x384), surpassing methods like EMU3 and LlamaGen with fewer sampling steps.  - Achieves a new state-of-the-art in multimodal understanding, surpassing LLaVA-1.5 13B by 7.2% on average by leveraging the Qwen-2.5-14B language model.  - Shows that discrete visual input can outperform continuous visual baselines for the first time on understanding tasks.  - Maintains high codebook utilization (95%+) even with large codebooks (over 130K), exceeding prior approaches in capacity and efficiency. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](https://arxiv.org/abs/2412.00493) | asdfg80, slvjul, zd11024 | - This paper introduces Video-3D LLM, a novel generalist model for 3D scene understanding. - The model leverages a Video LLM framework, processing video frames augmented with corresponding 3D spatial coordinates obtained from depth images. - It enhances 3D scene understanding by creating position-aware video representations through the integration of 3D position encodings derived from spatial coordinates. - A maximum coverage sampling technique optimizes the balance between computational cost and performance. - The model achieves state-of-the-art performance on benchmarks like ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D, outperforming LLaVA-3D while using only 26% of its 3D data. | ['Multimodal', 'Computer Vision', 'Visual Question Answering', 'Image-to-Text'] | [Link](https://github.com/LaVi-Lab/Video-3D-LLM) | N/A |
| [VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models](https://arxiv.org/abs/2411.19103) | SunYoung Park, Daeyoung Kim, kimyoungjune, hojunssss | - This paper introduces VARCO-VISION-14B, a bilingual (Korean-English) vision-language model based on Qwen-2.5-14B-Instruct as its language model and SigLIP as its vision encoder, trained using a four-stage process involving feature alignment, supervised fine-tuning, and preference optimization. - The model outperforms similarly sized open-source models on Korean multimodal benchmarks and achieves comparable performance to larger proprietary models, demonstrating strong bilingual capabilities. - Five Korean evaluation datasets are released alongside the model, including four closed-set (K-MMBench, K-SEED, K-MMStar, K-DTCBench) and one open-set (K-LLaVA-W) benchmarks, translated and validated from established English benchmarks to assess bilingual proficiency and document, table and chart understanding. - VARCO-VISION exhibits proficient grounding, referring, and OCR capabilities in both languages. - The authors aim to promote open research in Korean VLMs with this release and encourage further development of bilingual multimodal models. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering', 'Document Question Answering'] | N/A | [Link](https://huggingface.co/NCSOFT/VARCO-VISION-14B), [Link](https://huggingface.co/datasets/NCSOFT/K-MMBench), [Link](https://huggingface.co/datasets/NCSOFT/K-SEED), [Link](https://huggingface.co/datasets/NCSOFT/K-MMStar), [Link](https://huggingface.co/datasets/NCSOFT/K-DTCBench), [Link](https://huggingface.co/datasets/NCSOFT/K-LLAVA-W) |
| [Mimir: Improving Video Diffusion Models for Precise Text Understanding](https://arxiv.org/abs/2412.03085) | Dandan Zheng, Kecheng Zheng, Yutong Feng, Shuai Tan, BiaoGong | - Mimir is a novel text-to-video generation framework that integrates large language models (LLMs) within a diffusion model for enhanced text comprehension. - It employs a "token fuser" to combine features from both text encoders (like T5) and decoder-only LLMs (like Phi-3.5), addressing the distribution gap between these models. - This design allows Mimir to leverage existing video priors in diffusion models while capitalizing on the enhanced reasoning and precise understanding of LLMs. - Quantitative and qualitative evaluations on VBench demonstrate Mimir's superior performance, particularly in handling multiple objects, spatial relationships, and short, descriptive prompts. - A user study further confirms Mimir's improved capabilities in instruction following, physics simulation, and overall visual quality compared to existing state-of-the-art models. | ['Text-to-Video', 'Multimodal'] | N/A | [Link](https://lucaria-academy.github.io/Mimir/) |
| [Weighted-Reward Preference Optimization for Implicit Model Fusion](https://arxiv.org/abs/2412.03187) | Xiaojun Quan, Tianyuan Shi, Longguang Zhong, Fanqi Wan, Ziyi Yang | - This paper introduces Weighted-Reward Preference Optimization (WRPO), a novel implicit model fusion method for enhancing the capabilities of a Large Language Model (LLM) by leveraging preference optimization between source LLMs and a target LLM. - WRPO eliminates the need for vocabulary alignment and matrix fusion, enabling efficient scaling to accommodate diverse LLMs and mitigating distributional deviations through a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. - Experiments conducted on MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrated that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. - Using LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a 46.2% win rate against GPT-4-0314 on Arena-Hard, showcasing significant performance improvements. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SLIT-AI/WRPO) | N/A |
