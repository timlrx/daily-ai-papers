

## Papers for 2024-12-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://arxiv.org/abs/2412.04454) | tianbaoxiexxx, ludunjie, ZeonLap, kugwzk, ranpox | - AGUVIS, a unified pure vision-based framework, is introduced for building generalizable GUI agents that operate with vision-based observations and a plugin-enabled action system, enhancing cross-platform adaptability. - A two-stage training process is employed: first for GUI grounding, followed by planning and reasoning. - The model leverages vision-based grounding to improve generalization and reduce inference costs while employing a standardized action space with a plugin system to facilitate consistent learning. - Through experiments, AGUVIS surpasses previous state-of-the-art methods on benchmarks like ScreenSpot, Multimodal-Mind2Web, and AndroidControl, achieving the first fully autonomous pure vision GUI agent capable of performing tasks independently. - This model demonstrates its efficiency by considerably reducing USD costs and input tokens compared to GPT-40 on Mind2Web-Live. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679) | Seongyun Lee, Vijay Viswanathan, Xiang Yue, Juyoung Suk, seungone | - This paper introduces AGORABENCH, a benchmark for evaluating the effectiveness of large language models (LLMs) as synthetic data generators for training other LMs. - The benchmark uses standardized settings and a new metric, Performance Gap Recovered (PGR), to compare the quality of synthetic data generated by different LLMs across various tasks and data generation methods. - The study finds that an LLM's ability to generate high-quality training data does not necessarily correlate with its problem-solving abilities, but rather with intrinsic properties of the data such as response quality, perplexity, and instruction difficulty. - Strategic choices like output format and cost-conscious model selection can significantly impact the effectiveness of data generation, with generating larger datasets from cheaper models sometimes outperforming smaller datasets from more expensive models. - The code and data for AGORABENCH are publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/neulab/data-agora) | N/A |
| [Densing Law of LLMs](https://arxiv.org/abs/2412.04315) | Xu Han, Guoyang Zeng, Weilin Zhao, Jie Cai, xcjthu | - This paper introduces "capacity density" to evaluate the training quality of Large Language Models (LLMs) across different scales, considering both effectiveness and efficiency. - Capacity density is calculated as the ratio of a model's effective parameter size (the size a reference model would need to achieve equivalent performance) to its actual parameter size. - The paper proposes a two-step process to predict downstream task performance: 1) estimate the relationship between parameter size and language modeling loss and 2) estimate the relationship between loss and downstream task performance using a sigmoid function. - An empirical law, the "Densing Law," is revealed, showing that the maximum capacity density of open-source base LLMs exhibits exponential growth, doubling approximately every three months. - This trend suggests that future LLM development should prioritize improving capacity density rather than solely increasing parameter size, enabling optimal performance with minimal computational overhead. | ['Natural Language Processing'] | N/A | N/A |
| [Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion](https://arxiv.org/abs/2412.04424) | Dianqi Li, Haiping Wu, Jianwei Yang, Jiuhai Chen, zhoutianyi | - Florence-VL, a new family of Multimodal Large Language Models (MLLMs), leverages the generative vision foundation model Florence-2 as its visual encoder, enabling it to capture diverse visual features at different levels of detail and under various prompts. - A novel Depth-Breadth Fusion (DBFusion) mechanism concatenates visual features from different layers (depth) and under multiple prompts (breadth), providing a rich visual representation to the language model. - This model is trained with a two-stage process: end-to-end pretraining on a large image captioning dataset followed by fine-tuning on a diverse set of instruction-tuning datasets. - Quantitative analysis and visualization demonstrate improved vision-language alignment compared to models using CLIP or SigLIP encoders.  - Florence-VL achieves state-of-the-art results across 25 multimodal and vision-centric benchmarks, including VQA, OCR, Chart understanding, and knowledge-based reasoning tasks, outperforming other advanced MLLMs like Cambrian. | ['Multimodal', 'Visual Question Answering', 'Image Feature Extraction'] | [Link](https://github.com/JiuhaiChen/Florence-VL) | N/A |
| [Towards Universal Soccer Video Understanding](https://arxiv.org/abs/2412.01820) | Yanfeng Wang, Ya Zhang, Hao Jiang, haoningwu, Homie0609 | - This paper introduces MatchVision, a novel visual-language foundation model designed for comprehensive soccer video understanding. - The model leverages a spatiotemporal attention mechanism inspired by TimeSformer, which allows it to effectively capture dynamic information within soccer videos. - MatchVision is trained on SoccerReplay-1988, a new dataset containing 1988 full soccer matches with rich annotations, alongside existing datasets like SoccerNet, that is significantly larger and more diverse. - The paper benchmarks MatchVision on various downstream tasks such as event classification, commentary generation, and foul recognition. - Experimental results demonstrate state-of-the-art performance across multiple benchmarks, highlighting the model's effectiveness and the value of the new dataset. | ['Computer Vision', 'Video Classification', 'Text Generation', 'Multimodal'] | [Link](https://jyrao.github.io/UniSoccer/) | N/A |
| [Personalized Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2412.02142) | Zhehao Zhang, Yu Xia, Hanjia Lyu, Junda Wu, Franck-Dernoncourt | - This paper presents a comprehensive survey of personalized multimodal large language models (MLLMs), examining their architectures, training methods, and applications. - The authors propose a taxonomy for categorizing techniques used to personalize MLLMs, focusing on instruction, alignment, generation, and fine-tuning methods. - The survey also summarizes applications of personalized MLLMs, including text generation, image generation, recommendation, and retrieval tasks. - They also present an overview of commonly used datasets and evaluation metrics for personalized MLLMs. - Finally, the authors highlight key open challenges in the field, including benchmarking, evaluation metrics, diverse modalities, modality fusion, and theoretical foundations. | ['Multimodal', 'Text Generation', 'Image-to-Text', 'Image-to-Image', 'Text2Text Generation', 'Summarization'] | N/A | N/A |
| [Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation](https://arxiv.org/abs/2412.03304) | Jian Gang Ngui, David I. Adelani, Cl√©mentine Fourrier, Angelika Romanou, Shivalika Singh | - This paper introduces Global-MMLU, a 42-language multilingual multi-domain question-answering dataset designed to address cultural and linguistic biases in MMLU. - It includes culturally sensitive (CS) and culturally agnostic (CA) subsets, allowing for more nuanced evaluations. - The creation involved professional and community annotators for translation and post-editing, expanding language coverage and improving translation quality. - The paper also quantifies the impact of cultural biases, with analysis revealing that 28% of MMLU questions require culturally specific knowledge and a disproportionate focus on Western cultures. - State-of-the-art model evaluations on Global-MMLU highlight the impact of these biases on performance rankings, advocating for reporting performance on CS and CA subsets separately to provide a more comprehensive understanding of model capabilities across cultures. | ['Question Answering', 'Translation', 'Natural Language Processing'] | N/A | [Link](https://hf.co/datasets/CohereForAI/Global-MMLU) |
| [Monet: Mixture of Monosemantic Experts for Transformers](https://arxiv.org/abs/2412.04139) | Jaewoo Kang, Kee-Eung Kim, Young Jin Ahn, affjljoo3581 |  - This paper introduces MONET, a novel Mixture-of-Experts (MoE) architecture designed to enhance the mechanistic interpretability of large language models (LLMs) by addressing the issue of polysemanticity. - MONET incorporates sparse dictionary learning directly into end-to-end MoE pretraining, enabling the scaling of the expert count to 262,144 per layer while maintaining parameter efficiency. - The model's performance is evaluated across various benchmarks, showing competitive results with dense LLMs while offering superior knowledge manipulation capabilities. - Through qualitative and quantitative analyses, MONET demonstrates mutual exclusivity of knowledge across experts, enabling robust knowledge manipulation without performance degradation. - The code and pretrained checkpoints for MONET are publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/dmis-lab/Monet) | N/A |
| [OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows](https://arxiv.org/abs/2412.01169) | Yusuke Kato, Zichun Liao, Akash Gokul, Konstantinos Kallidromitis, Shufan Li | - OmniFlow is a novel generative model designed for any-to-any generation tasks, using a modular architecture inspired by Stable Diffusion 3's MMDiT. - It extends the rectified flow (RF) framework to handle multiple modalities (text, image, audio) jointly and introduces a novel guidance mechanism for flexible control over modality interaction in generated outputs. - OmniFlow outperforms previous any-to-any models on various tasks, achieving competitive performance with state-of-the-art specialist models in text-to-image and text-to-audio generation. - Its modular design allows individual component pretraining and merging with pretrained single-task models, reducing training resource requirements compared to training from scratch. - Evaluations show significant improvements over existing any-to-any models in image quality, text-image alignment, and CLIP scores, particularly on the GenEval benchmark. | ['Any-to-Any', 'Multimodal', 'Text-to-Image', 'Text-to-Audio'] | [Link](https://github.com/jacklishufan/OmniFlows) | N/A |
| [Discriminative Fine-tuning of LVLMs](https://arxiv.org/abs/2412.04378) | Ioannis Maniadis Metaxas, Anestis Zaganidis, Alexandros Xenos, Adrian Bulat, Yassine Ouali | - This paper introduces VladVA, a novel training approach for discriminative fine-tuning of Large Vision-Language Models (LVLMs). - VladVA converts a generative LVLM into a discriminative one by employing both contrastive and next-token prediction losses on image-text pairs with varying lengths and granularities. - The approach uses a parameter-efficient adaptation method that involves soft prompting and LoRA, thereby allowing for effective training on smaller datasets with limited compute. - On standard image-text retrieval benchmarks, VladVA shows significant improvement, achieving gains from +4.7% to +7.0% in absolute terms over similarly sized state-of-the-art CLIP-like models. - Additionally, the model demonstrates notable gains in compositionality tasks, showing improved language understanding over the standard two-tower image-text models. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Zero-Shot Image Classification'] | N/A | N/A |
| [KV Shifting Attention Enhances Language Modeling](https://arxiv.org/abs/2411.19574) | Weipeng Chen, Bingning Wang, Wei Cheng, xumingyu16 | - This paper introduces KV shifting attention, a novel attention mechanism designed to improve the efficiency of induction heads in large language models (LLMs). - KV shifting attention decouples keys and values in the attention mechanism, reducing the depth and width requirements for induction heads, enabling single-layer transformers to perform induction tasks effectively. - Theoretical analysis and empirical validation demonstrate that KV Shifting attention achieves comparable or superior performance to conventional multi-layer transformers in language modeling tasks. - The authors apply KV shifting attention to large language pre-training models with up to 19B parameters and show improved performance and faster convergence compared to baseline models using standard attention mechanisms. - KV shifting attention introduces a bias towards learning induction, which is beneficial for language modeling across diverse model scales. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/xumingyu16/Baseline_2.9B), [Link](https://huggingface.co/xumingyu16/KV_shifting_2.9B) |
