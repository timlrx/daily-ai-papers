

## Papers for 2024-12-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271) | Yangzhou Liu, Yue Cao, Zhe Chen, qishisuren, Weiyun1025 | - This paper introduces InternVL 2.5, a series of advanced multimodal large language models (MLLMs) built upon InternVL 2.0, enhancing training and testing strategies and data quality. - InternVL 2.5 maintains the core "ViT-MLP-LLM" architecture, integrating an incrementally pre-trained InternViT-6B or InternViT-300M vision encoder with various large language models (LLMs) like InternLM 2.5 and Qwen 2.5. - The models demonstrate competitive performance, rivaling leading commercial models like GPT-40 and Claude-3.5-Sonnet, and achieving state-of-the-art results on benchmarks like MMMU and MathVista. - Key findings include the reduced dependency on training data with larger vision encoders, the impact of improved data quality, and the benefits of test-time scaling, especially with Chain-of-Thought (CoT) reasoning. - InternVL 2.5 is released open-source, aiming to push the boundaries of open-source multimodal models and facilitate further research in multimodal AI. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Video-Text-to-Text'] | [Link](https://github.com/OpenGVLab/InternVL) | [Link](//huggingface.co/OpenGVLab/InternVL2_5-78B), [Link](https://huggingface.co/spaces/OpenGVLab/InternVL) |
| [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/abs/2412.05237) | Yuelin Bai, Tuney Zheng, Jarvis Guo, yuexiang96, luodian | The paper introduces MAmmoTH-VL, a multimodal large language model (MLLM) trained on a newly created dataset containing 12 million instruction-response pairs.  The dataset was constructed using a cost-effective method employing open-source models.  MAmmoTH-VL-8B, based on the LLaVA-OneVision architecture, outperforms existing open-source models on various benchmarks, particularly those involving intricate reasoning. The model shows state-of-the-art performance on MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Ablation studies reveal the effectiveness of key components such as rewriting and self-filtering. | ['Multimodal'] | [Link](https://mammoth-vl.github.io) | N/A |
| [EXAONE 3.5: Series of Large Language Models for Real-world Use Cases](https://arxiv.org/abs/2412.04862) | Kyunghoon Bae, Soyoung An, LG AI Research, lhg912, Sunkyoung | The paper introduces EXAONE 3.5, a series of instruction-tuned language models available in three sizes (32B, 7.8B, and 2.4B).  The models demonstrate strong instruction-following capabilities and achieve high performance in various benchmarks, particularly in real-world scenarios and long-context understanding.  EXAONE 3.5 models outperform many similar-sized models on benchmark datasets. The models are open for research purposes and can be downloaded from HuggingFace. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/LGAI-EXAONE) |
| [Moto: Latent Motion Token as the Bridging Language for Robot Manipulation](https://arxiv.org/abs/2412.04445) | Mingyu Ding, Yixiao Ge, Yizhuo Li, Yuying Ge, Yi Chen | - Moto is a new robotics model that leverages latent motion tokens as a bridging language for autoregressive pre-training on video data and robot manipulation. - Moto-GPT is pre-trained through next motion token prediction, learning motion-related prior knowledge from videos and transferring this knowledge to downstream tasks via fine-tuning. - Moto-GPT is a transformer-based architecture (similar to GPT) with a motion tokenizer that encodes motion between frames and action query tokens for robot control prediction during fine-tuning. - Experimental results demonstrate that Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks compared to various baseline models. - Moto-GPT performs particularly well in low-resource scenarios, highlighting its effective transfer of learned knowledge from video data to downstream tasks. | ['Robotics', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://chenyi99.github.io/moto/) | N/A |
| [APOLLO: SGD-like Memory, AdamW-level Performance](https://arxiv.org/abs/2412.05270) | Sem Park, Xi Liu, Wenyan Cong, Hanqing Zhu, Kyriection | - APOLLO, a new memory-efficient optimizer for Large Language Models (LLMs), is proposed, offering SGD-level memory cost while maintaining or exceeding AdamW's performance. - It leverages structured learning rate updates (channel-wise or tensor-wise) and approximates them in a low-rank auxiliary space using random projections, eliminating the need for costly SVD operations. - APOLLO-Mini, an extremely memory-efficient variant, utilizes tensor-wise scaling with a rank-1 auxiliary space, achieving similar performance to AdamW with drastically reduced memory usage. - Experimental results on various LLaMA model sizes (60M to 7B) show APOLLO consistently outperforms AdamW and other memory-efficient methods in pre-training, even achieving a 2.8 reduction in validation perplexity with significantly lower memory overhead. - APOLLO also offers practical system-level advantages including enhanced throughput (3x on LLaMA 7B compared to AdamW) and improved model scalability, enabling LLaMA-13B pre-training with naive DDP on a single A100-80GB GPU and LLaMA-7B training on a single GPU with less than 12GB memory when combined with quantization. | ['Natural Language Processing'] | N/A | N/A |
| [GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration](https://arxiv.org/abs/2412.04440) | Yu Wang, Xuefei Ning, Yukun Huang, fjxmlzn, NinaKarine | - GENMAC, an iterative multi-agent framework, is proposed for compositional text-to-video generation. - It uses a three-stage collaborative workflow: DESIGN, GENERATION, and REDESIGN, with an iterative loop between GENERATION and REDESIGN for refinement. - The REDESIGN stage uses four sequentially executed MLLM-based agents: verification, suggestion, correction, and output structuring. - A self-routing mechanism adaptively selects the appropriate correction agent from a collection of specialized agents. - Experiments on T2V-CompBench show state-of-the-art performance across seven compositional aspects, significantly outperforming 17 existing methods, with notable improvement in generative numeracy. | ['Text-to-Video', 'Multimodal'] | N/A | [Link](https://karine-h.github.io/GenMAC/) |
| [DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling](https://arxiv.org/abs/2412.04905) | Haiyang Yu, Nan Xu, Kun Chen, Xinghua Zhang, iiiiwis | - This paper introduces Dialogue Element Modeling (DEMO), a new research task focusing on two core competencies: Element Awareness and Dialogue Agent Interaction. - Element Awareness involves reverse-engineering dialogue elements like goal, persona, and scene, while Dialogue Agent Interaction focuses on goal-directed multi-turn dialogue modeling. - A novel benchmark, DEMO, is proposed to facilitate comprehensive dialogue modeling and assessment in both English and Chinese, covering various dialogue elements and tasks. - A DEMO agent, trained using imitation learning and expert experience, demonstrates superior performance in both in-domain and out-of-domain tasks, exceeding several existing LLMs. - Experimental results show that current LLMs have room for improvement in dialogue element modeling, especially in feature perception tasks. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/MozerWang/DEMO) | N/A |
