

## Papers for 2024-12-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559) | Keming Lu, Beichen Zhang, Zhenru Zhang, RunjiLin, chujiezheng | - This paper introduces ProcessBench, a new benchmark for evaluating the ability of language models to identify erroneous steps in mathematical reasoning. - ProcessBench consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems, with each test case containing a step-by-step solution annotated with error locations by human experts. - Through extensive evaluation on ProcessBench, the authors found that existing Process Reward Models (PRMs) typically fail to generalize to more challenging math problems and underperform compared to critic models (prompted general language models). - The best open-source model, QwQ-32B-Preview, demonstrates competitive critique capability with the proprietary model GPT-40, but still lags behind the reasoning-specialized o1-mini. - This work aims to foster future research in reasoning process assessment and pave the way toward scalable oversight of language models. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/QwenLM/ProcessBench) | N/A |
| [Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models](https://arxiv.org/abs/2412.05939) | Wanxiang Che, Libo Qin, Yuxi Xie, Tianhao Niu, LooperXX | - This paper introduces MMGIC, a multimodal dataset featuring multi-grained concept annotations for Multimodal Large Language Models (MLLMs), including coarse-grained image captions, fine-grained object labels and regions, and label descriptions. - The authors propose a general MLLM framework and structured template to integrate these multi-grained annotations, facilitating vision-language alignment across different granularities. - Experiments demonstrate that MMGIC enhances MLLM performance in comprehension and generation tasks compared to training solely on image captions. - MMGIC and image-caption data complement each other; a curriculum learning strategy of pre-training on image captions and then MMGIC yields the best results, with gains of 3.95% and 2.34% on POPE and SEED-Bench, respectively.  - The study explores various data recipes for multi-grained annotations and their impact on MLLM performance, demonstrating their complementary nature. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/LooperXX/MMGiC) | N/A |
| [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769) | Zhiting Hu, Xian Li, DiJia Su, Sainbayar Sukhbaatar, Shibo Hao | - This paper introduces COCONUT (Chain of Continuous Thought), a novel paradigm for training large language models (LLMs) to reason in a continuous latent space, rather than the traditional language space used in chain-of-thought (CoT) prompting. - COCONUT utilizes the last hidden state of the LLM as a continuous representation of the reasoning state ("continuous thought") and feeds it directly back to the LLM as the next input embedding. - This latent reasoning approach allows the model to encode multiple potential next reasoning steps, enabling a breadth-first search (BFS) behavior and improved performance on logical reasoning tasks requiring backtracking compared to traditional CoT. - Experimental results on GSM8k, ProntoQA, and a newly proposed ProsQA dataset demonstrate the effectiveness of COCONUT, particularly in scenarios involving substantial planning and search. - The findings suggest that latent reasoning can be more efficient and adaptable for complex reasoning, offering insights into future research on LLM reasoning and problem-solving. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation](https://arxiv.org/abs/2412.04432) | Ying Shan, Yixiao Ge, Yizhuo Li, Yuying Ge | - This paper introduces Divot, a Diffusion-Powered Video Tokenizer, which uses a diffusion process for self-supervised video representation learning. - Divot is composed of a pre-trained Vision Transformer (ViT) encoder, a Spatial-Temporal transformer, and a Perceiver Resampler to get video representations. - It leverages a video diffusion model to predict the noise added to the VAE latents of video frames, conditioned on Divotâ€™s features. - The authors also introduce Divot-LLM, integrating Divot with a pre-trained LLM, which achieves competitive performance in video comprehension and zero-shot video generation benchmarks. - Divot-LLM also excels in video storytelling, generating interleaved narratives and corresponding videos. | ['Text-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/TencentARC/Divot) | N/A |
| [Robust Multi-bit Text Watermark with LLM-based Paraphrasers](https://arxiv.org/abs/2412.03123) | Hang Li, Yang Liu, Yuanshun Yao, Jinghan Jia, xiaojunxu | - This paper introduces a novel method for embedding imperceptible multi-bit watermarks into text using LLM-based paraphrasers. - The method uses a pair of fine-tuned LLMs, one as the encoder to embed the watermark and another as the decoder to extract it. The encoder injects the watermark by alternatively paraphrasing sentences based on a binary code, while the decoder classifies each sentence to extract the embedded bits. - The method achieves high detection accuracy (over 99.99% AUC) with small LLMs (1.1B parameters) while maintaining the semantic similarity between the original and watermarked texts. - It also demonstrates robustness against word substitutions and sentence paraphrasing perturbations and generalizes well to out-of-distribution data. - The watermark's stealthiness is evaluated through both human and LLM-based analysis, showing it is difficult to distinguish between watermarked and non-watermarked text. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/xiaojunxu/multi-bit-text-watermark) | N/A |
