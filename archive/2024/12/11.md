

## Papers for 2024-12-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Evaluating and Aligning CodeLLMs on Human Preference](https://arxiv.org/abs/2412.05210) | JustinLin610, huybery, misakamage, instro, jx-yang | - This paper introduces CodeArena, a new human-curated benchmark for evaluating code large language models (LLMs) based on human preference, addressing the gap between code correctness and user satisfaction. - CodeArena contains 397 diverse samples across 40 coding categories and 44 programming languages, focusing on real-world coding scenarios. - In addition, the authors create SynCode-Instruct, a large-scale (20B token) synthetic instruction dataset generated by scaling instructions from web sources and generating corresponding code snippets with unit tests where applicable. Fine-tuning Qwen2.5-Coder using SynCode-Instruct leads to state-of-the-art performance for open-source code LLMs.  - Evaluations of 40+ LLMs on CodeArena show significant differences compared to execution-based benchmarks and a large performance gap between open-source and closed-source LLMs. - This work also shows that fine-tuning with larger synthetic instruction data sets improves code generation and benchmark performance in code LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://codearenaeval.github.io/) | N/A |
| [DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation](https://arxiv.org/abs/2412.07589) | Chao Tang, LXT, zengyh1900, JingboWang, jianzongwu | - DiffSensei, a novel framework for generating customized manga panels, bridges Multi-Modal Large Language Models (MLLMs) with diffusion-based image generators, enabling dynamic multi-character control and narrative consistency. - The framework uses an MLLM as a text-compatible identity adapter, allowing character features to adjust based on panel captions and ensuring layout control through masked cross-attention injection and dialog embedding. - A new large-scale dataset, MangaZero, containing 43,264 manga pages and 427,147 annotated panels, supports the training and evaluation of models on customized manga generation. - Experimental results show DiffSensei outperforms existing story visualization models in character consistency, layout controllability, and text adherence, marking a significant advancement in manga generation by enabling text-adaptable character customization. - A human preference study further validates DiffSensei’s superior performance, highlighting its capability to create coherent and expressive manga panels. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [STIV: Scalable Text and Image Conditioned Video Generation](https://arxiv.org/abs/2412.07730) | jefflai, JesseAllardice, tsujuifu, wenzehu, Jiasenlu | - STIV (Scalable Text and Image Conditioned Video Generation) is a new text- and image-conditioned video generation model based on a Diffusion Transformer (DiT) architecture, integrating image conditions via frame replacement and text conditions through a joint image-text conditional classifier-free guidance. - This design enables STIV to handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks concurrently, and it's adaptable to various applications like video prediction and frame interpolation. - The 8.7B parameter STIV model achieves state-of-the-art performance on VBench benchmarks, scoring 83.1 on T2V and 90.1 on TI2V at 512x512 resolution, outperforming existing models like CogVideoX-5B, Pika, Kling, and Gen-3. - A progressive training approach involving text-to-image and text-to-video stages, alongside techniques like QK-norm, sandwich-norm, and MaskDiT, enhances stability and efficiency during model scaling. - The model utilizes flow matching as a training objective and incorporates rotary positional embeddings, micro-conditions, and a novel joint image-text classifier-free guidance strategy to improve performance and address motion-related issues. | ['Text-to-Video', 'Image-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Perception Tokens Enhance Visual Reasoning in Multimodal Language Models](https://arxiv.org/abs/2412.03548) | Dongping Chen, Ethan Shen, Cheng-Yu Hsieh, Zelun Luo, Mahtab Bigverdi | - This research introduces Perception Tokens, intrinsic image representations designed to improve visual reasoning abilities in Multimodal Language Models (MLMs). - Perception tokens, such as depth maps and bounding boxes, are incorporated as intermediate reasoning steps in the MLM's chain-of-thought process using a novel training method called AURORA. - AURORA leverages a VQVAE to transform visual representations into tokens and trains the MLM in a multi-task framework with a curriculum learning strategy. - The proposed LLaVA-AURORA model demonstrates significant improvement over fine-tuning baselines on benchmarks including BLINK, CVBench, and SEED-Bench for depth estimation and object counting tasks. - Experimental results exhibit the effectiveness of Perception Tokens, resulting in a 6.4% boost for relative depth estimation on BLINK and up to 11.3% improvement on object counting across datasets. | ['Multimodal', 'Computer Vision', 'Depth Estimation', 'Object Detection'] | N/A | N/A |
| [Granite Guardian](https://arxiv.org/abs/2412.07724) | Tejaswini Pedapati, Subhajit Chaudhury, Manish Nagireddy, Inkit Padhi, Giandomenico | - This paper introduces Granite Guardian, a suite of safeguard models (2B and 8B parameter sizes) designed for comprehensive risk detection in Large Language Models, addressing prompt and response risks related to social biases, security (jailbreaking), and RAG-specific issues like context relevance, groundedness, and answer relevance. - The models are trained on a combined dataset of human-annotated data from diverse sources and synthetic data generated to specifically cover adversarial attacks and RAG hallucinations.  - Evaluation on standard benchmarks like the OpenAI Moderation Evaluation Dataset, HarmBench, and ToxicChat show that Granite Guardian outperforms existing open-source models on key metrics like AUC, F1, and recall. - On RAG hallucination benchmarks (groundedness), Granite Guardian achieves an average AUC of 0.854 on the TRUE dataset and performs competitively with dedicated models explicitly trained for groundedness detection. - The models are released open-source to encourage community adoption and promote responsible development of safer LLM applications. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/ibm-granite/granite-guardian) | [Link](https://huggingface.co/ibm-granite/granite-guardian-hap-38m) |
| [ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance](https://arxiv.org/abs/2412.06673) | Jianhua Han, Runhui Huang, Junwei Yang, Guansong Lu, Chunwei Wang | - ILLUME, a unified multimodal large language model (MLLM), seamlessly integrates understanding and generation capabilities using a next-token prediction approach. - It incorporates a semantic vision tokenizer and a progressive multi-stage training to enhance data efficiency, requiring only 15M image-text pairs for pretraining. - ILLUME introduces a self-enhancing multimodal alignment scheme where the model assesses consistency between generated images and text descriptions for synergistic improvement. - Experiments show ILLUME competing with state-of-the-art unified and specialized MLLMs across visual understanding, generation, and editing. - It outperforms previous best models on several benchmarks by significant margins, like a 25% improvement on MMMU and 14% on SEED. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation](https://arxiv.org/abs/2412.07759) | Menghan Xia, Sida Peng, Xintao Wang, Xian Liu, lemonaddie | - 3DTrajMaster is a novel approach for manipulating multi-entity 3D motions in video generation using entity-specific 6DoF pose sequences as input, leveraging a plug-and-play 3D-motion grounded object injector. - The object injector fuses entity descriptions and trajectories into latent embeddings, which are then combined and fed into a gated self-attention layer for motion fusion, preserving the video diffusion prior and generalizing to diverse entities and trajectories. - A new 360°-Motion Dataset is constructed using UE rendering, correlating 3D human and animal assets with GPT-generated trajectories and capturing motion with 12 cameras, addressing the lack of suitable training data. - A domain adaptor and annealed sampling strategy mitigate video quality degradation during training and inference, respectively. - Experimental results demonstrate state-of-the-art accuracy and generalization for controlling multi-entity 3D motions, outperforming existing methods in pose accuracy and handling complex scenarios like 3D occlusions. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation](https://arxiv.org/abs/2412.07334) | Kazuhiro Fukui, Erica K. Shimomoto, Lincon S. Souza, Pedro H. V. Valois | - This paper introduces the Frame Representation Hypothesis (FRH), a new framework for interpreting and controlling Large Language Models (LLMs) by modeling multi-token words as frames, which are ordered sequences of independent vectors. - FRH extends the Linear Representation Hypothesis (LRH) which was limited to single-token words, and makes it applicable to multi-token words and thus any textual data. - The paper proposes Concept Frames, which are centroids of a set of word frames that share a common concept, and shows that over 99% of words composed of several tokens are composed of linearly independent token vectors. - A Top-k Concept-Guided Decoding method is introduced for steering text generation using chosen concepts, which helps to expose and potentially remediate biases present in LLMs. - Experiments on Llama 3.1, Gemma 2, and Phi 3 demonstrate FRH's ability to expose biases and steer text generation, showing its applicability to enhance the transparency and controllability of LLMs. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/phvv-me/frame-representation-hypothesis.git) | N/A |
| [Fully Open Source Moxin-7B Technical Report](https://arxiv.org/abs/2412.06845) | Sung-En Chang, Yixin Shen, Zhenglun Kong, Xuan Shen, Pu Zhao | - Moxin-7B is a fully open-source large language model (LLM) based on the Mistral architecture, extended to 36 blocks from 32, and trained on over 2 trillion tokens. - It incorporates grouped-query attention (GQA), sliding window attention (SWA), and a rolling buffer cache for efficient long-context handling (up to 32K tokens). - The model was trained in three phases: initial pre-training with 2k and 4k context lengths and a final capability enhancement phase using curated data from Hugging Face and evaluation benchmarks. - Evaluation on standard benchmarks like ARC, HellaSwag, MMLU, Winogrande, and PIQA demonstrates superior zero-shot performance against other 7B models and competitive results in few-shot settings. - The chat model variant, Moxin-7B-chat, outperforms baselines on MTBench, showcasing strong alignment capabilities. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/moxin-org/Moxin-LLM) | [Link](https://huggingface.co/moxin-org/moxin-llm-7b), [Link](https://huggingface.co/moxin-org/moxin-chat-7b) |
| [Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation](https://arxiv.org/abs/2412.07338) | Felice Dell'Orletta, Marco Avvenuti, Amaury Trujillo, Alessio Miaschi, Lorenzo Cima | - This paper proposes and evaluates strategies for generating contextualized and personalized counterspeech using a LLaMA2-13B model. - The model is instructed to generate counterspeech by leveraging contextual information such as community, conversation details, and user history. - The study finds that contextualized counterspeech significantly outperforms generic counterspeech in adequacy and persuasiveness. -  A mixed-design crowdsourcing experiment revealed a poor correlation between quantitative evaluation metrics and human evaluations.  - This highlights the importance of human evaluation in assessing nuanced aspects of generated content like artificiality. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/dfurman/Llama-2-13B-Instruct-v0.2) |
| [Chimera: Improving Generalist Model with Domain-Specific Experts](https://arxiv.org/abs/2412.05983) | Renrui Zhang, Renqiu Xia, Hongbin Zhou, Mingsheng Li, Tianshuo Peng | - Chimera, a new multimodal pipeline, enhances Large Multi-modal Models (LMMs) with domain-specific experts to improve performance on specialized tasks like multimodal reasoning and visual content extraction. - The model integrates multiple expert encoders into a single LMM using a progressive training strategy and a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism to address optimization imbalances. - Chimera achieves state-of-the-art performance on MathVista and MathVerse, outperforming comparable-scale LMMs and specialized expert models. - The model also excels in visual structural extraction tasks for charts, tables, and documents, achieving near-specialist-level results on benchmarks like ChartQA-SE, Table-SE, and Doc-SE. - The authors plan to open-source Chimera and the training datasets to facilitate future research on LMMs. | ['Multimodal', 'Question Answering', 'Table Question Answering'] | N/A | N/A |
