

## Papers for 2024-12-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations](https://arxiv.org/abs/2412.08580) | MAJIARUI, SYZhang0805, yeezlee, mengcy, hyllbd | - This paper introduces LAION-SG, a large-scale dataset with scene graph annotations for training complex image-text models. - LAION-SG is an enhancement of LAION-Aesthetics V2 (6.5+) with high-quality scene graph annotations by GPT-4, featuring multiple objects, detailed attributes, and relationships. - A new foundation model, SDXL-SG, based on Stable Diffusion XL, incorporates scene graph information through a graph neural network to improve complex scene generation. - Both quantitative and qualitative results show that models trained on LAION-SG significantly outperform those trained on existing datasets like COCO-Stuff and Visual Genome. - A new benchmark, CompSG-Bench, has been established to evaluate models on complex image generation, setting a new standard. | ['Text-to-Image', 'Graph Machine Learning', 'Multimodal'] | [Link](https://github.com/mengcye/LAION-SG) | N/A |
| [POINTS1.5: Building a Vision-Language Model towards Real World Applications](https://arxiv.org/abs/2412.08443) | Xiao Zhou, Le Tian, yangyu1, kavio, YuanLiuuuuuu | - POINTS1.5 is a new vision-language model based on the LLaVA architecture, which uses a pre-trained vision encoder, a randomly initialized projector, and a pre-trained large language model. - It incorporates a NaViT-style vision encoder that supports dynamic high resolution, eliminating the need to split images into tiles and improving performance on text-intensive tasks. - The model adds bilingual support (Chinese and English) and uses a refined chat template for pre-training, improving performance over its predecessor, POINTS1.0. - A rigorous filtering method is applied to visual instruction tuning datasets to remove samples with grammatical errors and questions answerable without images, further improving the quality of the training data. - POINTS1.5-7B achieves top ranking on the OpenCompass leaderboard among models under 10B parameters, outperforming models several times larger. | ['Multimodal', 'Image-Text-to-Text'] | [Link](https://github.com/WePOINTS/WePOINTS) | [Link](https://huggingface.co/WePOINTS/POINTS-1-5-Qwen-2-5-7B-Chat) |
| [StreamChat: Chatting with Streaming Video](https://arxiv.org/abs/2412.08646) | Shiyi Lan, hsli-cuhk, LucasFang, Zhiding, jjjjh | - This paper introduces StreamChat, a novel approach for enhancing Large Multimodal Models (LMMs) to interact with streaming video content by dynamically updating the visual context at each decoding step using a cross-attention based architecture and visual feedforward network (V-FFN). - A parallel 3D-ROPE mechanism is used to better encode temporal information, and a dense instruction-tuning dataset based on existing dense caption datasets is created to train the model. - StreamChat outperforms state-of-the-art video LMMs in streaming interaction scenarios, demonstrating its superior ability to handle dynamic video content, even outperforming LLaVA-Video-72B with a smaller 7B model. - StreamChat also achieves competitive performance on established image and video benchmarks. -  The model effectively captures video dynamics and adjusts responses accordingly by incorporating the latest video information at each decoding step for more temporally aligned responses. | ['Multimodal', 'Video-Text-to-Text'] | N/A | N/A |
| [StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements](https://arxiv.org/abs/2412.08503) | Chi Zhang, Hao Wang, Beier Zhu, Xue Song, Mingkun Lei | - StyleStudio, a novel text-driven style transfer model, addresses challenges like overfitting to reference styles, limited stylistic control, and misalignment with text content by employing three strategies. - It introduces cross-modal Adaptive Instance Normalization (AdaIN) to better integrate style and text features, Style-based Classifier-Free Guidance (SCFG) for selective style control, and a teacher model during early generation to stabilize layouts and reduce artifacts.  - Evaluations show significant improvements in style transfer quality, alignment with text prompts, and layout stability across different styles.  - The model achieves higher text alignment scores compared to existing methods and performs favorably in user studies assessing text alignment and style similarity. - The approach is versatile and can be integrated into various style transfer frameworks without requiring fine-tuning. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation](https://arxiv.org/abs/2412.07147) | Lijie Wen, Shaolin Zhu, liboaccn | - Introduced MIT-10M, a large-scale parallel corpus for multilingual image translation, containing over 10 million image-text pairs derived from real-world data and spanning 14 languages, 28 categories, and three difficulty levels. - The dataset underwent extensive cleaning and multilingual translation validation, including OCR annotation, NSFW detection, and sensitive content filtering. - Experiments demonstrated MIT-10M's superior performance in evaluating models on challenging real-world image translation tasks, particularly multi-line text in complex images.  - Fine-tuning Qwen2-VL with MIT-10M resulted in significant improvements, tripling performance compared to the baseline. - The dataset promotes the development of more robust and adaptable multilingual image translation models. | ['Multimodal', 'Image-to-Text', 'Translation'] | N/A | [Link](https://huggingface.co/datasets/liboaccn/MIT-10M) |
