

## Papers for 2024-12-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions](https://arxiv.org/abs/2412.09596) | Rui Qian, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Pan Zhang | - InternLM-XComposer2.5-OmniLive (IXC2.5-OL) is a multimodal system designed for real-time interaction with streaming video and audio inputs, addressing the limitations of current Multimodal Large Language Models (MLLMs) in continuous perception, memory, and reasoning. - IXC2.5-OL consists of three modules: a Streaming Perception Module processing multimodal information, a Multi-modal Long Memory Module integrating and retrieving short-term and long-term memories, and a Reasoning Module handling queries. - The system simulates human-like cognition by disentangling streaming perception, reasoning, and memory mechanisms, allowing simultaneous processing of information. - Evaluation across audio and video benchmarks demonstrates IXC2.5-OL's superior performance, achieving state-of-the-art results on StreamingBench for real-time video interactions and competitive results on other benchmarks like MLVU and Video-MME. - IXC2.5-OL excels in real-time video interactions while demonstrating competitive results among open-source models on several audio and video understanding benchmarks. | ['Multimodal', 'Video-Text-to-Text', 'Audio', 'Automatic Speech Recognition'] | [Link](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive) | N/A |
| [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905) | Ronen Eldan, Sébastien Bubeck, Harkirat Behl, Jyoti Aneja, Marah Abdin | - Phi-4 is a 14-billion parameter language model trained with an emphasis on data quality, incorporating synthetic data generated by diverse techniques like multi-agent prompting and instruction reversal. - Phi-4 surpasses its teacher model (GPT-4) on STEM-focused QA, demonstrating that the data generation and post-training techniques provide capabilities beyond distillation. - The training recipe focuses on three pillars: synthetic data generation, curation of high-quality organic data, and innovative post-training techniques like rejection sampling and a new approach to Direct Preference Optimization (DPO). - Phi-4's performance on reasoning tasks is comparable to or surpasses larger models, exceeding Llama-3.1 on benchmarks like GPQA and MATH, and scoring high on the November 2024 AMC math competitions, indicating robust reasoning abilities and lack of overfitting. - Post-training includes supervised fine-tuning (SFT), pivotal token search-based DPO, and judge-guided DPO, refining the model's alignment with human preferences, improving reasoning, safety, robustness, and mitigating hallucinations. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions](https://arxiv.org/abs/2412.08737) | Willie Neiswanger, Jinyi Hu, Tianyu Yu, Ollie Liu, jrzhang | - This paper introduces Euclid, a family of Multimodal Large Language Models (MLLMs) specifically optimized for enhanced low-level visual perception (LLVP) in geometric tasks. - Euclid employs a curriculum learning strategy with synthetically generated high-fidelity visual descriptions of geometric shapes and their relationships, addressing the limitations of existing MLLMs in accurately perceiving detailed geometric information.  -  A new benchmark dataset called *Geoperception*, derived from the Geometry-3K corpus, is introduced to evaluate the model’s ability to precisely transcribe 2D geometric information from images. -  Euclid outperforms leading open-source and closed-source models on the Geoperception benchmark, demonstrating strong generalization capabilities to novel geometry shapes.  - For instance, Euclid surpasses the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain tasks and 10.65% on average. | ['Multimodal', 'Visual Question Answering', 'Computer Vision'] | [Link](github.com/euclid-multimodal/Euclid) | [Link](huggingface.co/euclid-multimodal) |
| [Multimodal Latent Language Modeling with Next-Token Diffusion](https://arxiv.org/abs/2412.08635) | Li Dong, Zhiliang Peng, Wenhui Wang, Hangbo Bao, Yutao Sun | - LatentLM, a novel multimodal generative model, is introduced, which seamlessly integrates continuous and discrete data using causal transformers and next-token diffusion. - The model employs a variational autoencoder (VAE) to represent continuous data as latent vectors and next-token diffusion for autoregressive generation of these vectors, overcoming variance collapse issues with the use of σ-VAE. - LatentLM shows superior performance in image generation, surpassing Diffusion Transformers in both performance and scalability, as evidenced by its favorable FID and IS scores on ImageNet. - When integrated into multimodal large language models, LatentLM provides a unified interface, outperforming existing methods in language modeling, text-to-image generation, and vision-language understanding tasks, as demonstrated by its better perplexity scores and FID scores on MS-COCO. - In text-to-speech synthesis, LatentLM achieves state-of-the-art results, outperforming VALL-E 2 in speaker similarity and robustness while requiring 10x fewer decoding steps. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Text-to-Speech'] | [Link](https://github.com/facebookresearch/DiT) | N/A |
| [AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials](https://arxiv.org/abs/2412.09605) | Zhennan Shen, Dunjie Lu, Yiheng Xu, cxiong, ZeonLap | - AgentTrek is a novel data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials. - It automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. - A VLM-based evaluator ensures the correctness of the generated trajectories, resulting in a dataset with 10,398 trajectories. - Experiments demonstrate that training GUI agents with the synthesized trajectories significantly improves their grounding and planning performance compared to current models. - The proposed method is more cost-efficient than traditional human annotation methods, paving the way for large-scale GUI agent training. | ['Multimodal'] | [Link](https://agenttrek.github.io) | N/A |
| [Learned Compression for Compressed Learning](https://arxiv.org/abs/2412.09405) | Neeraja J. Yadwadkar, Dan Jacobellis | - This paper introduces WaLLoC (Wavelet Learned Lossy Compression), a novel neural codec architecture for compressed-domain learning. - WaLLoC combines linear transform coding with nonlinear dimensionality-reducing autoencoders, placing a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. - This design enables WaLLoC to achieve computationally efficient encoding, high compression ratios, and uniform dimensionality reduction—key requirements for effective compressed learning. - Experiments demonstrate WaLLoC's superior performance compared to existing autoencoders used in state-of-the-art latent diffusion models across several metrics, including image classification, colorization, document understanding, and music source separation. - WaLLoC achieves up to 20x dimensionality reduction, making it an effective drop-in replacement for resolution reduction in accelerating downstream models without sacrificing accuracy. | ['Computer Vision', 'Image Classification', 'Image-to-Image', 'Audio-to-Audio', 'Audio Classification'] | N/A | [Link](https://ut-sysml.org/walloc/) |
| [Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://arxiv.org/abs/2412.09501) | Longxiang Tang, Senqiao Yang, Yuqi Liu, Chengyao Wang, Zhisheng Zhong | - Lyra, an efficient Multimodal Large Language Model (MLLM), enhances multimodal abilities with a focus on speech, including long speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction. - Lyra leverages existing open-source large models and a multi-modality LoRA to reduce training costs, uses a latent multi-modality regularizer and extractor to strengthen relationships between modalities (especially speech), and introduces a high-quality dataset with 1.5M multimodal samples and 12K long speech samples. - Compared to other omni-models, Lyra achieves state-of-the-art performance on vision-language, vision-speech, and speech-language benchmarks. - Lyra demonstrates superior efficiency with fewer computational resources, less training data, and faster training and inference speed across speech, image, and video tasks. - The model supports sound and speech understanding and generation, handles complex long speech inputs, and exhibits enhanced omni-comprehension capabilities. | ['Multimodal', 'Visual Question Answering', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition'] | [Link](https://github.com/dvlab-research/Lyra) | N/A |
| [RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios](https://arxiv.org/abs/2412.08972) | Xiaobao Wu, Sitao Cheng, Liangming Pan, Wenyue Hua, Ruiwen Zhou | - RULEARENA, a benchmark designed to evaluate LLMs' ability to follow complex real-world rules in reasoning across three domains: airline baggage fees, NBA transactions, and tax regulations. - It assesses proficiency in handling intricate instructions requiring long-context understanding, logical reasoning, and accurate mathematical computation. - Two key distinctions from existing benchmarks: extends beyond first-order logic and grounds tasks in authentic scenarios. - Findings reveal LLM limitations: difficulty applying rules, inaccurate math, and overall poor performance. - Highlights challenges in rule-guided reasoning for LLMs in real-world applications. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/skyriver-2000/RuleArena) | N/A |
| [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/abs/2412.09569) | Lilach Eden, Roy Bar-Haim, Yotam Perlitz, Odellia Boni, Ariel Gera | - This paper introduces JuStRank, a novel benchmark designed to assess the efficacy of Large Language Models (LLMs) in performing system-level ranking of other LLMs. - The benchmark employs a dataset of responses generated by various LLMs to a set of instructions and evaluates the judges' ability to rank the systems based on the quality of their responses, using correlation with a human-generated ranking as the primary metric. -  The study evaluates 48 state-of-the-art judges, encompassing both general-purpose LLMs and specialized reward models, and reveals that several smaller reward models can perform comparably to much larger LLMs in this ranking task. - The authors also investigate the influence of various judge realizations (e.g., absolute numeric scores, Likert-scale ratings, comparative judgments) on ranking accuracy and observe a significant impact, with absolute scores generally outperforming comparative judgments. -  The analysis identifies two key emergent properties of system-level judges: decisiveness, characterized by the tendency to amplify differences between strong and weak systems, and system-specific bias. | ['Natural Language Processing'] | N/A | N/A |
| [OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation](https://arxiv.org/abs/2412.09585) | Jianwei Yang, Jianfeng Gao, Humphrey Shi, Zhengyuan Yang, Jitesh Jain | - OLA-VLM introduces a novel approach to enhance visual representations within Multimodal Large Language Models (MLLMs) by distilling knowledge from specialized teacher vision encoders into the LLM's intermediate layers during pretraining. - This method involves a coupled optimization of predictive visual embedding and next text-token prediction, incorporating embedding losses at specific LLM layers to align with target visual features from encoders trained on tasks like segmentation, depth estimation, and image generation. - The approach also utilizes specialized tokens enriched with target-specific information, creating an implicit visual chain of thought within the LLM's input sequence. - OLA-VLM demonstrates improved performance over single and multi-encoder baselines on various benchmarks, including up to an 8.7% boost on the Depth task in CV-Bench, showcasing its superior visual understanding capabilities. - The embedding optimization strategy is hypothesized to yield better projector initialization for the instruction fine-tuning stage, leading to efficient and accurate visual processing within the MLLM while using only a single vision encoder during inference. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Depth Estimation', 'Image Segmentation'] | [Link](https://github.com/SHI-Labs/OLA-VLM) | N/A |
| [The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective](https://arxiv.org/abs/2412.09460) | David Samuel, Freddy Wetjen, Lemei Zhang, Vladislav Mikhailov, Javier de la Rosa | - This research investigates the impact of copyrighted material on Norwegian Large Language Models (LLMs) by training models using various datasets, including copyrighted and non-copyrighted materials. - The study uses Mistral 7B v0.1 architecture as base and creates different versions by pre-training from scratch and also warm-starting and fine-tuning with copyrighted materials and instructions. - Evaluations are conducted using a new benchmarking suite with 28 NLP tasks designed for Norwegian, demonstrating that copyrighted material leads to performance improvements across diverse tasks, particularly specialized ones. - The paper finds that warm-starting with other languages reduces the impact of adding Norwegian copyrighted data. - The findings highlight ethical and legal considerations regarding the use of copyrighted materials in LLM development and provide empirical evidence for policy discussions on author compensation and copyright in the digital age. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/mistralai/Mistral-7B-v0), [Link](https://huggingface.co/datasets/mimir-project/mimir-bias), [Link](https://huggingface.co/datasets/ltg/nortruthfulqa_mc), [Link](https://huggingface.co/datasets/ltg/nortruthfulqa_gen), [Link](https://huggingface.co/datasets/ltg/noropenbookqa), [Link](https://huggingface.co/datasets/ltg/nrk), [Link](https://huggingface.co/datasets/ltg/norcommonsensega), [Link](https://huggingface.co/datasets/mimir-project/noridiom), [Link](https://huggingface.co/datasets/SamiaT/NorSumm) |
| [Word Sense Linking: Disambiguating Outside the Sandbox](https://arxiv.org/abs/2412.09370) | Roberto Navigli, Alberte Fernández-Castro, Luigi Procopio, Edoardo Barba, Andrei Stefan Bejgu | - This paper introduces Word Sense Linking (WSL), a new task that aims to bridge the gap between Word Sense Disambiguation (WSD) research and its practical application in downstream tasks. - WSL requires systems to identify and disambiguate all spans in a given text using only a reference sense inventory, without relying on pre-identified spans or candidate senses. - A novel retriever-reader architecture is proposed for WSL, which inverts the traditional concept detection and candidate generation steps of WSD to overcome limitations in handling unannotated spans. - The model outperforms state-of-the-art WSD systems adapted for the WSL setting by a significant margin, demonstrating its robustness and efficiency. - A new WSL benchmark dataset, built by expanding the existing WSD evaluation datasets, facilitates comprehensive evaluation of both precision and recall. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Babelscape/WSL) | N/A |
| [SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](https://arxiv.org/abs/2412.05552) | Mohit Bansal, Chongyang Zhao, Zun Wang, Yicong Hong, Gengze Zhou | - This paper proposes State-Adaptive Mixture of Experts (SAME), a novel model for versatile language-guided visual navigation that can interpret and execute instructions with varying levels of granularity. - SAME utilizes a mixture of expert networks specialized in different navigation skills, such as exploration and instruction-following, routed based on the agent's current state (attended language and visual observation). - The model effectively addresses the challenge of conflicting learning objectives in multi-task training by selectively activating experts based on the input state, promoting shared knowledge learning while maintaining task-specific capabilities. - Experimental results on seven navigation tasks including R2R, RxR-EN, REVERIE, OBJECTNAV, CVDN, SOON, and R2R-CE, demonstrate that SAME outperforms or achieves comparable performance to task-specific agents, showing the efficacy of the state-adaptive expert routing mechanism. - Further analysis reveals that applying MoE on visual queries in the cross-attention layer of the navigation policy yields superior results compared to applying it on feed-forward networks, highlighting the importance of cross-modal attention in action selection. | ['Computer Vision', 'Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/GengzeZhou/SAME) | N/A |
| [Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages](https://arxiv.org/abs/2412.09025) | Srinivasan Umesh, rumourscape | - This paper introduces Shiksha, a dataset and model for translating technical content into Indian languages. - The dataset comprises 2.8 million high-quality, parallel translation pairs across 8 Indian languages, extracted from NPTEL lecture transcriptions. - A 3.3B parameter NLLB model is fine-tuned using LoRA on this dataset, demonstrating improvements in both in-domain and out-of-domain translation tasks. - Evaluation on a held-out test set and the Flores+ benchmark shows improved performance over baseline NLLB and comparable results to IndicTrans2. - The models are integrated into a tool called Translingua, which aids human annotators in translating NPTEL lectures. | ['Translation', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/SPRINGLab) |
