

## Papers for 2024-12-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Apollo: An Exploration of Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2412.10360) | minione, lichengyu, YannDubs, nicholswang, orrzohar | - The paper introduces Apollo, a family of state-of-the-art Large Multimodal Models (LMMs) designed for enhanced video understanding, capable of processing hour-long videos efficiently. - Apollo utilizes a unified architecture employing a combination of InternVideo2 and SigLIP-SO400M encoders, with features concatenated and resampled using a Perceiver Resampler before being fed to a large language model (LLM). - The authors claim Apollo-3B outperforms most existing 7B models, achieving a score of 58.4 on Video-MME (without subtitles), 68.7 on MLVU, and 62.7 on their proposed benchmark, ApolloBench.  - Apollo-7B achieves state-of-the-art performance amongst 7B LMMs with scores of 61.2 on Video-MME, 70.9 on MLVU, and 66.3 on ApolloBench, demonstrating competitiveness with some 30B models. - The study also explores various design choices, such as video sampling strategies, encoder combinations, and data composition, introducing the concept of "Scaling Consistency," where design decisions from smaller models effectively transfer to larger models. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities](https://arxiv.org/abs/2412.07769) | Saeed Yahya Alseiari, Mohammed Irfan Kurpath, hishamcholakkal, HuggingSara, sahalshajim | - BiMediX2 is a bilingual (Arabic-English) Large Multimodal Model (LMM) with a unified architecture integrating text and visual modalities for advanced medical image understanding and applications. - It leverages the Llama 3.1 architecture with integrated text and visual capabilities, supporting text and multi-turn conversations involving medical images and trained on a 1.6M sample bilingual healthcare dataset (BiMed-V). - BiMediX2 outperforms state-of-the-art models in medical LLM and VLM evaluation benchmarks, exceeding GPT-4 by 9% in UPHILL factual accuracy and showing over 9% improvement in English and 20% in Arabic on multimodal medical evaluations. - A new bilingual GPT4-based medical LLM benchmark called BiMed-MBench was introduced. - The model excels in medical Visual Question Answering, Report Generation, and Report Summarization tasks across diverse imaging modalities. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text', 'Natural Language Processing', 'Question Answering', 'Summarization'] | [Link](https://github.com/mbzuai-oryx/BiMedix2) | N/A |
| [Large Action Models: From Inception to Implementation](https://arxiv.org/abs/2412.10047) | Eliblo1969, substill, shilhe, Lujunting, vyokky | - This paper introduces Large Action Models (LAMs), a new type of AI model designed to perform actions in both physical and digital environments, extending the capabilities of Large Language Models (LLMs). - LAMs are trained using a four-phase approach: task-plan pretraining, learning from experts, self-boosting exploration, and learning from a reward model. - The authors demonstrate the effectiveness of LAMs by integrating them into a Windows OS-based agent, showing superior performance in task completion compared to LLMs like GPT-40, particularly in scenarios requiring precise interaction and manipulation within specific environments. - The LAM achieved an 81.2% Task Success Rate (TSR), surpassing GPT-40's 67.2% and GPT-40 Mini's 62.3% in a Word application environment, demonstrating the effectiveness of LAMs over traditional LLMs in action-oriented tasks. - The paper concludes by discussing the current limitations of LAMs and identifying key areas for future research. | ['Natural Language Processing', 'Reinforcement Learning', 'Robotics'] | [Link](https://github.com/microsoft/UFO/tree/main/dataflow) | N/A |
| [ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation](https://arxiv.org/abs/2412.08645) | Dana Berman, Matan Cohen, Asaf Shul, yedid, danielwinter | - ObjectMate introduces a tuning-free method for object insertion and subject-driven generation, utilizing a novel "object recurrence prior." - This prior leverages the recurrence of everyday objects across large, unlabeled image datasets to create a massive, supervised training dataset with diverse poses, lighting, and scenes. - The model architecture is based on a straightforward text-to-image diffusion model trained on this dataset, taking object views and scene descriptions as input. - ObjectMate achieves state-of-the-art results on object insertion and subject-driven generation tasks, outperforming existing methods in identity preservation and photorealistic composition. - The paper also introduces a new object insertion evaluation dataset with ground truth data and proposes a new metric for identity preservation that aligns better with human perception, validated through a user study. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation](https://arxiv.org/abs/2412.09428) | morninghaze, baochenxi, wzk1015, JackyZhuo, wbs2788 | - This paper introduces Visuals Music Bridge (VMB), a novel multimodal music generation framework that uses text and music as explicit bridges for enhanced cross-modal alignment. - VMB consists of three core components: a Multimodal Music Description Model (MMDM) to convert visual input into text descriptions; a Dual-track Music Retrieval module to retrieve relevant music pieces; and an Explicitly Conditioned Music Generation framework to synthesize music. - The Explicitly Conditioned Music Generation module consists of a latent diffusion transformer (DiT) and employs Music ControlFormer and Stylization Module to enable high-quality generation. - The proposed method addresses challenges like data scarcity, weak cross-modal alignment, and limited controllability in existing multimodal music generation methods. - Experimental results on video-to-music, text-to-music, image-to-music, and controllable music generation tasks demonstrate that VMB significantly improves music quality, modality, and customization alignment compared to previous methods. | ['Multimodal', 'Text-to-Audio', 'Video-Text-to-Text', 'Image-to-Text'] | [Link](https://github.com/wbs2788/VMB) | N/A |
| [SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding](https://arxiv.org/abs/2412.09604) | wzk1015, Einsiedler, hehesang, Changyao, cpsxhao | - SynerGen-VL is a unified Multimodal Large Language Model (MLLM) designed for synergistic image understanding and generation using a single architecture and training process with a next-token prediction paradigm. - It introduces a token folding mechanism with a hierarchical architecture to compress input image token sequences, enabling efficient handling of high-resolution images and a decoder that reconstructs the image during generation. - Vision-expert-based progressive alignment pretraining integrates visual capabilities into the pretrained LLM, minimizing disruption to existing knowledge by using image-specific Feed-Forward Networks (FFNs) and aligning visual representations with the LLM's representation space. - Trained on large-scale mixed image-text data, SynerGen-VL achieves competitive performance compared to existing encoder-free unified MLLMs with comparable or smaller parameter sizes and narrows the gap with task-specific state-of-the-art models. - With 2.4B activated parameters, SynerGen-VL matches the performance of Emu3, which has 8B parameters, demonstrating its strong potential as a next-generation unified MLLM. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](https://arxiv.org/abs/2412.10319) | Chengruidong, luoxufang, qianhuiwu, iofu728, liyucheng | - Introduces SCBench, a benchmark designed to evaluate efficient long-context methods, particularly for shared context and multi-round interactions where KV Cache is reused. - Assesses four key long-context abilities: String Retrieval, Semantic Retrieval, Global Information processing, and Multi-tasking across 12 tasks with two shared context modes (multi-turn and multi-request). - Evaluates 13 long-context methods across four stages (generation, compression, retrieval, and loading) and eight categories on six open-source long-context LLMs. - Finds that sub-O(n) memory methods struggle in multi-turn scenarios, sparse encoding with O(n) memory performs robustly, and dynamic sparsity is more expressive for KV caches than static patterns. - Identifies attention distribution shift issues in long-generation scenarios, impacting performance even for O(n) memory methods. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | N/A | N/A |
| [SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs](https://arxiv.org/abs/2412.08347) | SultanR | - This paper introduces SmolTulu-1.7b-Instruct, an instruction-tuned language model based on Huggingface's SmolLM2-1.7B and adapted from AllenAI's Tulu 3 training pipeline. - The research focuses on the impact of learning rate to batch size ratios on model performance across different tasks, finding that higher ratios benefit reasoning tasks while lower ratios are optimal for pattern recognition tasks. - SmolTulu achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval (Î”11%) and 51.6% on GSM8K (13.4%) for mathematical reasoning. - The model also achieved 57.1% on ARC (15.4%) with an alternate version.  -  Training recipes and ablation studies are released to promote further research in efficient model alignment and optimization for small language models. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | [Link](https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct) |
