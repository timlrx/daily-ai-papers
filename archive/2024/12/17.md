

## Papers for 2024-12-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation](https://arxiv.org/abs/2412.11919) | douzc, Benen2024, wuyongkang, jinjiajie, lixiaoxi45 | - RetroLLM is a novel framework that integrates retrieval and generation within a unified auto-regressive decoding process in LLMs, allowing direct generation of fine-grained evidence from a corpus using constrained decoding. - It employs hierarchical FM-Index constraints, generating corpus-constrained clues to identify relevant documents before evidence generation to mitigate false pruning. - It introduces forward-looking constrained decoding, utilizing document FM-Index to identify future windows and a relevance model to score these windows for improved evidence accuracy. - Experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance in in-domain and out-of-domain tasks, outperforming traditional RAG and more complex RAG strategies. - RetroLLM also significantly reduces token consumption compared to existing RAG methods due to more precise retrieval granularity. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/sunnynexus/RetroLLM) | N/A |
| [BrushEdit: All-In-One Image Inpainting and Editing](https://arxiv.org/abs/2412.10316) | yshan2u, ZyZcuhk, juxuan27, BianYx, Yw22 | - BrushEdit is an interactive image editing framework that combines language models and a dual-branch inpainting technique for seamless edits such as adding/removing objects and making structural changes with free-form masks. - It leverages pre-trained multimodal large language models (MLLMs) to interpret user instructions, identify editing types and target objects, and generate textual descriptions of the edited image. - The Editing Conductor, built on BrushNet, uses a mixed fine-tuning strategy with random and segmentation masks, allowing it to handle diverse mask-based inpainting tasks. - Experimental results on PIE-Bench, BrushBench, and EditBench demonstrate BrushEditâ€™s superior performance in preserving unedited regions, ensuring accurate text-alignment, and outperforming existing methods in image editing and inpainting tasks. - BrushEdit offers flexible control over base diffusion model selection and scale adjustment, enhancing its practical value for diverse user needs. | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871) | spermwhale, Chunting, marg33, benjamin-mlr, artidoro | - BLT (Byte Latent Transformer) is a new byte-level LLM architecture that dynamically groups bytes into patches based on next-byte entropy, allocating more compute to complex segments. - It uses a local encoder and decoder for byte-patch transformations and a global latent transformer for patch processing, matching token-based models at scale while improving inference efficiency and robustness. - BLT achieves parity with Llama 3 in training FLOP-controlled performance while using up to 50% fewer FLOPS at inference, and shows better scaling trends with simultaneous increases in model and patch size. - It demonstrates qualitative improvements on reasoning, long-tail generalization, noisy input robustness, and sub-word aspect awareness, surpassing token-based models in these areas. - BLT's code is released for both training and inference. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/facebookresearch/blt) | N/A |
| [Smaller Language Models Are Better Instruction Evolvers](https://arxiv.org/abs/2412.11231) | Hua Zhou, Yaqi Zhang, Lulu Zhao, dongguanting, Chaox72 | - This paper investigates the effectiveness of smaller language models (SLMs) compared to larger language models (LLMs) in evolving more complex and diverse instructions for instruction tuning. - Through experiments across three instruction evolution scenarios (Evol-Instruct, AutoIF, and Auto Evol-Instruct), the study demonstrates that SLMs outperform LLMs in evolving instructions, leading to better performance in downstream tasks including instruction following, mathematical reasoning, and code generation. - The authors hypothesize that SLMs' broader output space during instruction generation, due to their relatively weaker instruction-following capabilities compared to LLMs, results in more complex and diverse instructions. - They propose a new metric called Instruction Complex-Aware IFD (IC-IFD), incorporating instruction complexity into the original IFD score for a more accurate evaluation of instruction data effectiveness without requiring instruction tuning. - Experimental results demonstrate that SLMs generate more complex and diverse instructions than LLMs leading to improved performance in downstream tasks. | ['Natural Language Processing'] | [Link](https://github.com/HypherX/Evolution-Analysis) | N/A |
| [SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models](https://arxiv.org/abs/2412.11605) | howang, yuxiaod, lrxl, wangcunxiang, CCCCCC | - This paper introduces SPaR, a self-play framework that uses tree-search refinement to enhance the instruction-following capabilities of Large Language Models (LLMs). - SPaR involves an actor LLM generating responses and a refiner LLM critiquing and refining them through a tree-search process to create preference pairs for training. - This method aims to highlight key differences for instruction following by minimizing extraneous variations often present in independently sampled responses used by other preference learning methods. - Experiments demonstrate that a LLaMA-8B model trained with SPaR surpasses GPT-4-Turbo on the IFEval benchmark and shows promising scalability with larger models like LLaMA3-70B. - The study also finds that scaling inference in tree search improves performance, and the refiner's abilities can exceed the initially distilled LLM, suggesting potential for continuous self-improvement. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/thu-coai/SPaR) | N/A |
| [GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs](https://arxiv.org/abs/2412.11258) | junweiliang, StarYDY, zhifeichen097, spongy, Xxlbigbrother | - GaussianProperty is a training-free framework that predicts physical properties of materials for 3D Gaussians using Segment Anything (SAM) and GPT-4V(ision). - It employs a global-local reasoning module for 2D images by leveraging SAM's segmentation capability and GPT-4V's recognition capability to estimate physical properties. - These properties are then projected from multi-view 2D images to 3D Gaussians using a voting strategy. -  The framework enables applications in physics-based dynamic simulation by leveraging Material Point Method (MPM) and robot grasping by developing a grasping force prediction strategy based on the estimated properties. - Experiments on material segmentation, dynamic simulation, and real-world robotic grasping demonstrate the effectiveness of GaussianProperty in enhancing downstream tasks. | ['Computer Vision', 'Multimodal', 'Image-to-3D', 'Robotics'] | N/A | N/A |
| [SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator](https://arxiv.org/abs/2412.12094) | Xiaozhe Ren, Yihang Gao, Jiawei Li, Guoxuan Chen, shihan96 | - SepLLM is a plug-and-play framework that accelerates LLM inference by compressing segments of text into separator tokens and eliminating redundant tokens.  - It leverages a data-dependent sparse attention mechanism, retaining only initial, neighboring, and separator tokens and implementing efficient kernels for training acceleration. - Experimental results show that using the Llama-3-8B backbone, SepLLM can reduce KV cache by over 50% while maintaining comparable performance on GSM8K-CoT.  - In streaming settings, SepLLM can effectively process sequences of up to 4 million tokens or more.  - SepLLM addresses the limitations of other methods by maintaining consistent performance between training and inference and by achieving substantial reductions in computational costs and training time. | ['Natural Language Processing', 'Text Generation'] | [Link](sepllm.github.io) | N/A |
| [Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture](https://arxiv.org/abs/2412.11834) | wubingheng, JingzeShi | - This paper introduces Wonderful Matrices, a novel foundation model architecture combining sequence and state transformations for enhanced efficiency and effectiveness in language modeling. - The architecture integrates Rotary Position Embedding (ROPE) for unified positional encoding in hybrid algorithms, Dynamic Mask Attention (DMAttn) for selective filtering of past states, and Cross Domain Mixture of Experts (CDMOE) for reduced parameter redundancy and efficient expert retrieval. - The paper demonstrates the effectiveness of each individual module (ROPE, DMAttn, CDMOE) through empirical validation, showing improvements in perplexity and multi-query associative recall. - Experimental results on language modeling tasks demonstrate that Wonderful Matrices outperforms other architectures like LlaMa3, Mamba2, and Jamba across various evaluation metrics, especially with increasing parameter scale. - The architecture uses a combination of State Space Duality (SSD) and DMAttn modules for sequence transformation and CDMOE modules for state transformation, achieving a balance between efficiency and performance. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/LoserCheems/Doge) | N/A |
| [Whisper-GPT: A Hybrid Representation Audio Large Language Model](https://arxiv.org/abs/2412.11449) | prateekv | - WHISPER-GPT, a novel hybrid large language model (LLM) for speech and music generation, leverages continuous audio representations (mel-spectrograms) alongside discrete acoustic tokens within a single Transformer decoder-only architecture. - This hybrid approach addresses context length limitations encountered in purely discrete token-based LLMs by incorporating continuous audio information while retaining the advantages of discrete tokens for sampling and generation. - Experimental results on LibriSpeech TTS and a music dataset demonstrate that WHISPER-GPT with 4M parameters achieves comparable performance to a 40M parameter purely token-based LLM, showcasing the efficiency of the hybrid representation. - The model predicts the next token given the past acoustic tokens and mel-spectrogram slices, improving the next token prediction metrics like negative log-likelihood and perplexity. - Future work involves using this hybrid LLM to fine-tune other audio tasks such as generating multi-scale acoustic tokens and generate high-fidelity audio samples conditioned on them. | ['Audio', 'Text-to-Audio', 'Text-to-Speech'] | N/A | N/A |
