

## Papers for 2024-12-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147) | Linchen Xiao, Hongwei Liu, Junnan Liu, zsytony, Harold-lkk | - This paper introduces G-Pass@k, a novel evaluation metric designed to assess both the potential and stability of Large Language Models (LLMs) in complex reasoning tasks, particularly mathematical problem-solving. - G-Pass@k quantifies an LLM's consistency in generating correct solutions across multiple generations by considering varying thresholds of correctness, thereby capturing limitations in traditional metrics like Greedy Accuracy and Pass@k, which often overlook output stability. - A new dynamic benchmark called LiveMathBench is introduced, comprising challenging mathematical problems from various competitions to minimize data leakage and ensure relevance to the latest advancements in LLM capabilities.  - Through extensive experiments on LiveMathBench and other datasets, the paper reveals that current LLMs, including specialized and chain-of-thought enhanced models, exhibit significant instability in their reasoning abilities, with performance drops of up to 90% in challenging scenarios.  - The findings underscore the inadequacy of conventional evaluation methods and highlight the need for stability-aware metrics like G-Pass@k for a more realistic assessment of LLM capabilities in complex reasoning tasks. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/open-compass/GPassk), [Link](https://github.com/open-compass/GPassK) | N/A |
| [Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models](https://arxiv.org/abs/2412.12606) | Xiaoshuai Song, Zhuoma GongQue, Runqi Qiao, Shanglin Lei, YiFan Zhang | - This paper introduces the Multi-Dimensional Insights (MDI) benchmark, a new benchmark for evaluating large multimodal models (LMMs) on real-world personalization tasks. - The MDI benchmark consists of over 500 images and 1.2k human-posed questions across six common real-world scenarios, focusing on two key dimensions: question complexity and age demographics. - Questions are categorized into simple and complex levels to assess basic understanding and reasoning abilities, respectively, while also being stratified across young, middle-aged, and older age groups to evaluate personalized responses. - Initial evaluations using the MDI benchmark reveal that while strong models like GPT-4 achieve a 79% accuracy on age-related tasks, there remains significant room for improvement in addressing the diverse needs and preferences of different age groups in real-world scenarios. - The MDI benchmark aims to foster development towards reliable, personalized human assistants by offering a comprehensive evaluation framework covering a broad spectrum of real-world personalized needs. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain](https://arxiv.org/abs/2412.13018) | Ji-Rong Wen, Zhicheng Dou, Jiejun Tan, ShootingWong | - This paper introduces OmniEval, an automatic and omnidirectional benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in the financial domain. - The benchmark employs a matrix-based evaluation system categorizing queries into five tasks and 16 financial topics for a comprehensive assessment of diverse query scenarios. - It uses a multi-dimensional data generation approach combining GPT-4-based automatic generation and human annotation, achieving an 87.47% acceptance ratio in human evaluations. - A multi-stage evaluation system assesses both retrieval and generation performance, and robust evaluation metrics from rule-based (MAP, Rouge) and LLM-based methods ensure reliable assessment. - Experiments on various retrievers and LLMs demonstrate OmniEval's comprehensiveness and highlight performance variations across topics and tasks, showing improvement opportunities for RAG systems in the financial domain. | ['Question Answering'] | [Link](https://github.com/RUC-NLPIR/OmniEval) | N/A |
| [Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers](https://arxiv.org/abs/2412.12276) | Pulkit Agrawal, Jeff Gore, Jinyeop Song, Seungwook Han | - This paper proposes a "concept encoding-decoding" mechanism to explain how transformers perform in-context learning (ICL). - The core idea is that transformers learn to encode different latent concepts (e.g., grammatical rules or arithmetic operations) into distinct, separable representations, and simultaneously develop concept-specific decoding algorithms. - Through experiments on synthetic and natural ICL tasks (part-of-speech tagging and bitwise arithmetic), the authors show that this mechanism emerges during training and exists in pretrained language models of varying scales (Gemma-2 and Llama). - They introduce a metric called "Concept Decodability" (CD) to quantify the separability of latent concepts in representations and demonstrate that CD is predictive of ICL performance. - Causal interventions and finetuning experiments further validate that concept encoding is causally related to and predictive of ICL performance. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
