

## Papers for 2024-12-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161) | Kritanjali Jain, Yuxuan Tang, Boxuan Li, Yufan Song, Frank F. Xu | - This paper introduces TheAgentCompany, a benchmark for evaluating AI agents on real-world tasks simulating a software company environment. - The benchmark includes 175 tasks across various job functions like software engineering, project management, and finance, requiring agents to interact with web interfaces, code, and simulated colleagues via chat and email. - The evaluation includes both autonomous completion and partial credit based on checkpoint achievements, assessing the agents' ability to manage complex workflows. - Experiments with different LLMs (Claude, Gemini, GPT-40, Llama, Qwen) reveal that even the best model (Claude 3.5 Sonnet) achieves only 24% full and 34.4% partial completion, showing limitations in tasks demanding social interactions and handling complex interfaces. - Despite the leading LLM's strong performance, the high cost per task ($6.34) highlights the need for further research and optimization of cost-effectiveness in real-world deployments. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/TheAgentCompany/TheAgentCompany), [Link](https://github.com/TheAgentCompany/experiments) | N/A |
| [FashionComposer: Compositional Fashion Image Generation](https://arxiv.org/abs/2412.14168) | Hao Luo, Xiaogang Xu, Xi Chen, Yiyang Wang, Sihui Ji | - FashionComposer is a novel diffusion-based model for compositional fashion image generation that takes multi-modal inputs such as text descriptions, parametric human models, garment images, and face images. - It employs a universal framework with a reference UNet and subject-binding attention to handle diverse input modalities and compose multiple visual assets in one pass, supporting applications like virtual try-on and album generation. - The model is trained on a scaled dataset constructed using existing datasets augmented with masked garments and generated captions, showing superior performance in multi-reference customization compared to existing methods like Emu2 and Collage Diffusion. - For consistent human image generation in albums, FashionComposer introduces correspondence-aware attention and latent code alignment to maintain both consistency and fidelity. - In virtual try-on tasks, FashionComposer achieves state-of-the-art results, outperforming other methods on standard benchmarks like VITON-HD for single garment and showing promising results on DressCode for multi-garment and outfit try-on scenarios. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities](https://arxiv.org/abs/2412.14123) | Loic Landrieu, Clement Mallet, Nicolas Gonthier, Guillaume Astruc | - AnySat, a novel multimodal and multiresolution model for Earth Observation, is introduced, leveraging a Joint Embedding Predictive Architecture (JEPA) and scale-adaptive spatial encoders. - Trained on GeoPlex, a diverse dataset comprising various modalities, resolutions, and scales, the model demonstrates state-of-the-art performance across several downstream tasks, including land cover mapping, tree species identification, and flood segmentation. - AnySat's versatility allows it to seamlessly handle diverse EO datasets with varying properties and modalities, eliminating the need for dataset-specific retraining. - Evaluation on GeoPlex and external datasets showcases performance improvements, particularly in classification tasks and smaller datasets, due to the enhanced representation learning from diverse data sources. - AnySat's efficiency allows for linear probing for semantic segmentation with competitive results, reducing training costs significantly and opening possibilities for wider application in environmental monitoring. | ['Image Segmentation', 'Image Classification', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/gastruc/AnySat) | N/A |
| [GUI Agents: A Survey](https://arxiv.org/abs/2412.13501) | Namyong Park, Gang Wu, Yu Wang, Jian Chen, dangmn | - This survey paper provides a comprehensive overview of Graphical User Interface (GUI) agents, which leverage Large Foundation Models (LFMs) to automate human-computer interaction. - It categorizes GUI agents based on benchmarks, evaluation metrics, architectures (perception, reasoning, planning, and acting), and training methods, proposing a unified framework for understanding their capabilities. - The paper discusses various datasets and interactive environments used for evaluating GUI agents, distinguishing between closed-world and open-world settings, and static and dynamic environments. - It also covers different architectural designs for perception (accessibility-based, HTML/DOM-based, screen-visual-based, and hybrid), reasoning, planning (with internal and external knowledge), and acting modules. - Finally, the survey summarizes training methods, including prompt-based and training-based approaches (pre-training, fine-tuning, and reinforcement learning), and identifies open challenges and future research directions in GUI agent research, such as intent understanding, security, and latency. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment](https://arxiv.org/abs/2412.13746) | Yubo Chen, Pengfei Cao, Tianyi Men, Hongbang Yuan, Zhuoran Jin | - This paper introduces RAG-RewardBench, the first benchmark designed for evaluating reward models (RMs) within Retrieval Augmented Generation (RAG) settings, aiming to improve preference alignment between RAG models and human preferences. - The benchmark includes 1,485 preference pairs across four RAG-specific scenarios: multi-hop reasoning, fine-grained citation, appropriate abstaining, and conflict robustness, sourced from 18 datasets using six retrievers and 24 RALMs. - An LLM-as-a-judge approach is employed to enhance preference annotation efficiency and achieve a strong correlation (0.84 Pearson correlation) with human annotations. - Evaluation results on 45 existing RMs show the top-ranked model reaches only 78.3% accuracy, highlighting the benchmark's challenging nature and the need for RMs specifically tailored for RAG. - The paper finds that existing trained RALMs demonstrate minimal improvement (0.6%) in preference alignment over base LLMs based on their performance on RAG-RewardBench, suggesting a need to shift training towards preference-aligned approaches. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/jinzhuoran/RAG-RewardBench) | [Link](https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/) |
| [Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN](https://arxiv.org/abs/2412.13795) | Shiwei Liu, Lu Yin, Pengxiang Li | - Mix-LN, a novel normalization technique for Large Language Models (LLMs), combines Pre-LN and Post-LN to address the inefficiency of deeper layers often observed in LLMs trained with Pre-LN. - Mix-LN applies Post-LN to early layers and Pre-LN to deeper layers, promoting more uniform gradients and enabling effective contribution from all layers during training. - Experiments across various model sizes (70M to 7B parameters) show Mix-LN consistently outperforms Pre-LN, Post-LN, and their variants, improving pre-training perplexity and demonstrating better performance in supervised fine-tuning and reinforcement learning from human feedback. - The improved performance is attributed to Mix-LN's ability to promote healthier gradient norms and representation diversity across all layers, leading to more effective learning and generalization. - The study highlights the importance of optimizing normalization techniques in LLMs to fully leverage the potential of deep layers and improve overall model capacity and efficiency. | ['Natural Language Processing'] | [Link](https://github.com/pixeli99/MixLN) | N/A |
| [Learning from Massive Human Videos for Universal Humanoid Pose Control](https://arxiv.org/abs/2412.14172) | Junjie Ye, Tianheng Shi, Siqi Song, Siheng Zhao, Jiageng Mao | - This paper introduces Humanoid-X, a large-scale dataset with over 20 million humanoid robot poses and corresponding text descriptions, designed for universal humanoid pose control. - A new large humanoid model, UH-1, is proposed. UH-1 uses a Transformer architecture to translate text instructions into corresponding actions for controlling humanoid robots. It supports both text-to-keypoint and text-to-action control modes. - UH-1 is trained on Humanoid-X and shows strong generalization in text-based humanoid control, outperforming existing two-stage methods on the HumanoidML3D benchmark by over 23% in FID score. - Extensive simulated and real-world experiments demonstrate that UH-1 can reliably translate textual commands into diverse and accurate humanoid actions, achieving nearly 100% success rate in real-world deployment. - The scalability of Humanoid-X is demonstrated to improve model performance by training UH-1 on various dataset sizes. | ['Robotics', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers](https://arxiv.org/abs/2412.12571) | Yupeng Shi, Zhi-Fan Wu, Wei Wang, Lianghua Huang, bibona | - ChatDiT is a novel zero-shot, general-purpose, interactive visual generation framework built upon pre-trained diffusion transformers (DiTs) without requiring fine-tuning or architectural modifications. - It leverages the inherent in-context generation capabilities of DiTs, allowing users to create complex multi-image outputs, edit images, generate illustrated articles, and design character settings through free-form natural language interaction. - This is achieved using a multi-agent system composed of an Instruction-Parsing Agent, a Strategy-Planning Agent, and an Execution Agent, which collaboratively interpret instructions, formulate generation plans, and execute actions using an in-context toolkit of DiTs. - Evaluation on IDEA-Bench shows that ChatDiT outperforms existing methods, including specialized multi-task frameworks and rephrasing-based models, achieving a top score of 23.19 out of 100. - Despite its strong performance, certain limitations exist, such as difficulty in preserving fine details and identity, especially when handling long contexts with multiple subjects or elements, highlighting areas for future research. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/ali-vilab/ChatDiT) | N/A |
| [AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge](https://arxiv.org/abs/2412.13670) | Shuai Zhao, Ruiwen Zhou, Yuxi Xie, Liangming Pan, Xiaobao Wu | - This paper introduces AntiLeak-Bench, an automated anti-leakage benchmarking framework for Large Language Models (LLMs). - It addresses data contamination issues in LLM evaluation by constructing test samples with updated real-world knowledge, ensuring the knowledge is absent from LLMs' training sets. - A fully automated workflow is designed to build and update the benchmark, eliminating the need for human labor and reducing maintenance costs. - Experiments with various LLMs demonstrate a performance drop after the cutoff time, highlighting data contamination issues in LLM evaluations. - Results manifest the effectiveness of AntiLeak-Bench for contamination-free evaluation. | ['Question Answering'] | [Link](https://github.com/bobxwu/AntiLeak-Bench) | N/A |
