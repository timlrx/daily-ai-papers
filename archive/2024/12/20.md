

## Papers for 2024-12-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115) | Losin94, bowenYu, bzheng, huybery, Baosong | - Qwen2.5 is a series of large language models (LLMs) trained on 18 trillion tokens of data, improving upon its predecessor Qwen2 through enhanced pre-training and post-training techniques. - The models range from 0.5B to 72B parameters in open-weight offerings and include Mixture-of-Experts (MoE) models, Qwen2.5-Turbo and Qwen2.5-Plus, for hosted solutions. - Qwen2.5-72B-Instruct demonstrates competitive performance against Llama-3-405B-Instruct, a model five times its size. - Qwen2.5-Turbo and Qwen2.5-Plus exhibit superior cost-effectiveness while competing with GPT40-mini and GPT40 respectively. - Qwen2.5 also serves as a foundation for specialized models like Qwen2.5-Math and Qwen2.5-Coder, broadening its applicability to specific domains. | ['Natural Language Processing', 'Text Generation', 'Multimodal'] | [Link](https://github.com/QwenLM/Qwen2.5) | [Link](https://huggingface.co/Qwen) |
| [MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval](https://arxiv.org/abs/2412.14475) | BoZhaoHuggingFace, yzwang, Shitao, zl101, JUNJIE99 | - MegaPairs, a novel data synthesis method that uses vision-language models (VLMs) and open-domain images with a massive synthetic dataset for universal multimodal retrieval is proposed. - This method constructs heterogeneous KNN triplets using three similarity models (CLIP vision encoder, DINO vision encoder, and CLIP text encoder) to sample correlated image pairs. - It then utilizes MLLM and LLM annotators for relationship description and pseudo retrieval instruction generation resulting in triplets (Image query, Text instruction, Image target).  - MMRet models trained on MegaPairs demonstrate SOTA zero-shot results on 4 Composed Image Retrieval benchmarks and MMEB's 36 datasets, outperforming baselines by 8.1% on CIRCO using 70x less data.  - Further downstream fine-tuning shows that the model maintains leading performance on the benchmarks mentioned above. | ['Multimodal', 'Image-to-Text', 'Computer Vision'] | N/A | N/A |
| [Progressive Multimodal Reasoning via Active Retrieval](https://arxiv.org/abs/2412.14835) | douzc, yutaozhu94, dengmengjie, Snow-Nation, dongguanting | - This paper introduces AR-MCTS, a framework designed to improve multi-step multimodal reasoning in Multimodal Large Language Models (MLLMs) by combining Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). - AR-MCTS employs a unified retrieval module to gather key insights from a hybrid-modal corpus, aiding in problem-solving. - It utilizes MCTS with active retrieval to automatically generate step-wise annotations, enhancing the diversity and reliability of the reasoning process. - A process reward model (PRM) is progressively aligned through step-wise Direct Preference Optimization (DPO) and Supervised Fine-tuning (SFT) for automated verification. - Experimental results across various MLLMs and benchmarks show AR-MCTS's effectiveness in boosting performance, optimizing sampling diversity and accuracy, and demonstrating improvement in complex reasoning scenarios, particularly on WE-MATH's S3 metrics and general reasoning tasks like GAOKAO-MM. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks](https://arxiv.org/abs/2412.15204) | wangxz098, haopeng01, NeoZ123, tsq2000, bys0318 | - Introduces LongBench v2, a challenging benchmark designed to evaluate the deep understanding and reasoning capabilities of Large Language Models (LLMs) in long-context scenarios across diverse real-world tasks. - The benchmark consists of 503 multiple-choice questions spanning six major task categories, with contexts ranging from 8k to 2M words, focusing on complex reasoning rather than simple information retrieval. - Data collection involves nearly 100 highly educated individuals and employs rigorous automated and manual review processes, resulting in a high-quality dataset where even human experts achieve only 53.7% accuracy under time constraints. - Evaluation shows that the best-performing model achieves 57.7% accuracy, surpassing the human baseline by 4% when leveraging chain-of-thought prompting during inference. - The results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle long-context challenges and call for further exploration in this direction. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/THUDM/LongBench) | N/A |
| [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/abs/2412.14689) | XingtaiHF, iseesaw, Hengli, daixuancheng, xuekai | - This paper proposes ToEdit (Token Editing), a novel technique for synthesizing text data that mitigates model collapse, a degenerative process where language models overfit to synthetic data distributions. - ToEdit employs token-level editing on human-produced data, guided by a pre-trained language model's probability distribution, to create semi-synthetic data. - This method theoretically constrains the test error within a fixed upper bound, preventing the error accumulation observed in iterative training on synthetic data. - Experimental results across pre-training, continual pre-training, and supervised fine-tuning demonstrate that ToEdit enhances model performance compared to using purely synthetic or mixed synthetic and human-produced data. - Statistical analyses reveal that synthetic data suffers from coverage collapse and over-concentration of n-gram features, issues addressed by ToEdit's approach. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Flowing from Words to Pixels: A Framework for Cross-Modality Evolution](https://arxiv.org/abs/2412.15213) | Andrew Brown, Alan Yuille, Xi Yin, mannatsingh, QHL067 | - CrossFlow, a novel framework for cross-modal flow matching, leverages variational encoders and a novel classifier-free guidance technique to directly map one modality's distribution to another's. - For text-to-image generation, CrossFlow uses a vanilla transformer without cross-attention, unlike existing methods that rely on complex architectures and conditioning mechanisms. - Demonstrating improved scaling, CrossFlow slightly outperforms standard flow matching baselines in zero-shot FID-30K and achieves comparable CLIP scores, given the same data, model size, and training budget. - CrossFlow exhibits semantic latent space arithmetic, enabling meaningful output edits through latent manipulation. - Its generalizability is showcased by comparable or superior performance in image captioning, depth estimation, and image super-resolution compared to state-of-the-art techniques. | ['Text-to-Image', 'Image-to-Text', 'Depth Estimation', 'Image-to-Image', 'Multimodal'] | [Link](https://cross-flow.github.io/) | N/A |
| [AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling](https://arxiv.org/abs/2412.15084) | wping, ctnzr, shoeybi, ychenNLP, zihanliu | - AceMath, a suite of large language models (LLMs) designed for complex math problem-solving and featuring specialized reward models for solution evaluation, is introduced. - The instruction-tuned math models are developed through a two-stage supervised fine-tuning (SFT) process, starting with general domain SFT and followed by targeted math domain fine-tuning using curated prompts and synthetically generated responses. - AceMath-72B-Instruct outperforms existing open-weight and proprietary LLMs, including Qwen2.5-Math-72B-Instruct, GPT-40, and Claude-3.5 Sonnet, on a variety of math reasoning benchmarks. - A new comprehensive benchmark, AceMath-RewardBench, is introduced for evaluating math reward models; the associated AceMath-72B-RM reward model achieves state-of-the-art performance. - Combining AceMath-72B-Instruct with AceMath-72B-RM yields the highest average rm@8 score across math reasoning benchmarks. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | N/A | N/A |
| [Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception](https://arxiv.org/abs/2412.14233) | Ke Zhu, Jing Hao, FuNz, cloud913, syp115 | - This paper introduces DCE (Descriptive Caption Enhancement), a novel image captioning engine that leverages off-the-shelf visual specialist models to extract detailed object attributes and relationships from images. - These attributes, combined with LLM-generated region captions and relational information, produce richer and more comprehensive descriptions than existing methods relying solely on LLMs or human annotation. - Experimental results demonstrate that DCE-generated captions significantly improve the performance of Large Multimodal Models (LMMs) across 14 visual question answering and multimodal benchmarks, exceeding human and other LLM-generated captions. - DCE utilizes open-source models for caption generation, reducing the costs and improving the efficiency compared to methods using expensive models like GPT-4V. - The authors plan to release the DCE source code and pipeline to promote further research and enable easy integration of other visual specialists into multimodal models. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/syp2ysy/DCE) | N/A |
| [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/abs/2412.14642) | Qing Li, Yunqing Liu, Jiatong Li, schrodingers-tiger, Duke-de-Artois | - This paper introduces TOMG-Bench, the first benchmark designed to evaluate the ability of Large Language Models (LLMs) to perform open-domain text-based molecule generation. - TOMG-Bench comprises three core tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom), each with three subtasks containing 5,000 samples. - It includes an automated evaluation system assessing generated molecules' quality and accuracy, alongside an instruction-tuning dataset OpenMolIns, extracted from PubChem, to enhance LLM performance. - The benchmarking of 25 LLMs showcases the current limitations and potential in this field; with OpenMolIns, Llama-3.1-8B outperforms open-source general LLMs, even surpassing GPT-3.5-turbo on the TOMG-Bench by 46.5%. - The researchers identified the challenge of open molecule generation for existing LLMs and constructed the corresponding benchmark and the instructional dataset. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/phenixace/TOMG-Bench) | N/A |
