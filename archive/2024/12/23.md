

## Papers for 2024-12-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation](https://arxiv.org/abs/2412.13649) | Yilong Lai, Zhenglin Wang, zhoudeyu, lzhang472, callanwu | - SCOPE is a novel framework designed to optimize Key-Value (KV) cache compression for long-context generation in Large Language Models (LLMs), addressing the often-overlooked decoding phase. - It decouples KV cache optimization for prefill and decoding phases, preserving essential information from the prefill while dynamically allocating heavy hitters during decoding using a sliding window approach. - Further memory optimization is achieved through adaptive and discontinuous strategies, reducing memory usage and transfer overhead. - Experimental results on LONGGENBENCH show that SCOPE maintains comparable performance to full KV cache with a 35% overall compression rate, outperforming existing unified compression methods. - SCOPE also demonstrates its generalizability and effectiveness as a plug-in to other prefill-only KV cache compression methods. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Linking-ai/SCOPE) | N/A |
| [Offline Reinforcement Learning for LLM Multi-Step Reasoning](https://arxiv.org/abs/2412.16145) | yiwu, ZhangShenao, hendrydong, Shibo-UCSD, jwhj | - This paper introduces OREO (Offline REasoning Optimization), an offline reinforcement learning method designed to improve the multi-step reasoning abilities of Large Language Models (LLMs). - OREO jointly learns a policy model and value function by optimizing the soft Bellman Equation, enabling it to leverage unpaired data with sparse rewards and perform better credit assignment compared to methods like Direct Preference Optimization (DPO). - The approach is evaluated on mathematical reasoning (GSM8K, MATH) and embodied agent control (ALFWorld) tasks, demonstrating consistent improvements over baseline methods including rejection sampling, DPO, and KTO across different model sizes. - Notably, a 1.5B model achieves 52.5% accuracy on MATH using only the original training set, and iterative OREO shows continued improvement with additional training rounds. - The learned value function can also guide tree search during inference, leading to further performance gains (up to 17.9% relative improvement over greedy decoding on MATH). | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/jwhj/OREO) | N/A |
| [Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/abs/2412.15322) | Akio Hayakawa, mittu1204, TakashiShibuyaSony, mi141, hkchengrex | - MMAudio, a novel multimodal joint training framework for synthesizing high-quality, synchronized audio from video and optional text conditions, is introduced.  - This model uses a transformer-based architecture with visual, text, and audio branches, jointly trained on audio-visual and text-audio data, and incorporates a conditional synchronization module for precise temporal alignment.  - MMAudio achieves state-of-the-art performance in video-to-audio generation on public benchmarks, outperforming existing methods in audio quality, semantic alignment, and audio-visual synchronization, while maintaining a low inference time.  - Notably, it also demonstrates competitive performance in text-to-audio generation without fine-tuning.  - The joint training strategy enables accessible data scaling and cross-modal understanding, which are key to the model's success. | ['Multimodal', 'Text-to-Audio', 'Video-Text-to-Text', 'Audio'] | [Link](hkchengrex.github.io/MMAudio) | N/A |
| [MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design](https://arxiv.org/abs/2412.14590) | chuanjieliu, xiaonans, JamesTheZ | - MixLLM is a novel large language model (LLM) quantization technique that employs a global mixed-precision approach between output features, assigning higher bit-widths to features with greater impact on model accuracy, resulting in reduced memory consumption without compromising performance. - It identifies high-salience output channels by estimating their contribution to the final loss globally across all model layers, unlike previous methods that focus on per-layer salience. - MixLLM uses 8-bit symmetric activation quantization and 4-bit asymmetric weight quantization in a group-wise manner to maintain accuracy and employs a two-step dequantization process leveraging int8 Tensor Cores and fast integer-to-float conversion for optimized system efficiency. - Evaluation on popular LLMs like Llama 3.1 and Qwen2.5 across various benchmarks, including perplexity and downstream tasks like MMLU-Pro and BBH, demonstrates that MixLLM with only 4.4 bits for weights outperforms existing 4-bit methods and achieves results comparable to 5-bit quantization, while also exceeding the system performance of float16 and state-of-the-art 4-bit solutions. - Additionally, MixLLM with 8-bit weight quantization shows negligible accuracy loss compared to the float16 baseline, underscoring the efficacy of its group-wise activation quantization and system design. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps](https://arxiv.org/abs/2412.15035) | navigli, mbrack, PSaiml, sted97, felfri | - This paper introduces M-ALERT, a multilingual benchmark for evaluating the safety of Large Language Models (LLMs) across five languages (English, French, German, Italian, and Spanish). - M-ALERT comprises 75k prompts (15k per language), translated and adapted from the ALERT benchmark, covering a wide range of safety categories. - Experiments on 10 state-of-the-art LLMs reveal inconsistencies in safety performance across languages and categories, with some models exhibiting language-specific vulnerabilities while others show consistently unsafe behavior in certain high-risk categories. - The study finds a less pronounced correlation between model safety and size compared to the impact of instruction tuning. - M-ALERT also facilitates category and policy-specific evaluations, highlighting its practical use for policy compliance assessment in LLMs. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/felfri/M-ALERT) |
| [Fietje: An open, efficient LLM for Dutch](https://arxiv.org/abs/2412.15450) | BramVanroy | - This paper introduces Fietje, a family of 2.7B parameter decoder-only transformer language models for Dutch based on Phi-2. - Fietje was trained on 28B Dutch tokens from Wikipedia and the CulturaX dataset and comes in three versions: base, instruct, and chat. - At the time of its release, Fietje achieved competitive results with larger language models, sometimes even outperforming 7B models on ARC and MMLU benchmarks. - Evaluations on various Dutch NLP benchmarks demonstrated its efficacy compared to similar-sized models and established it as a significant step toward accessible language technology for Dutch. - The benchmark results also highlight the rapid advancement of the field and show that smaller multilingual models that were released after Fietje generally perform better. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/BramVanroy/fietje-2), [Link](https://github.com/BramVanroy/clin34-benchmarks) | [Link](https://huggingface.co/collections/BramVanroy/fietje-2-662cb803ed5cc4f617404146) |
