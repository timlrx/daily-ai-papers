

## Papers for 2024-12-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners](https://arxiv.org/abs/2412.17256) | Zifei Shan, Yijun Wang, Lulu Zhao, Yuzhen Huang, Weihao Zeng | - B-STAR, a novel self-improving framework, balances exploration and exploitation by dynamically adjusting hyperparameters like temperature and reward thresholds to optimize a proposed balance score metric throughout training iterations. - This approach enhances the model's ability to generate both diverse and high-quality responses, addressing the limitations of current self-improving methods that often stagnate after a few iterations. - The effectiveness of B-STAR is validated across mathematical problem-solving (GSM8K and MATH datasets), coding challenges (APPS dataset), and commonsense reasoning (ARC-Challenge dataset). - Experimental results show significant performance improvement over various self-improving baselines (STaR/ReST-EM, Iterative RFT, Online RFT). - For example, B-STAR shows improved Pass@1 accuracy and sustained improvement over multiple training iterations without degradation, unlike other methods, demonstrating effective management of the exploration-exploitation trade-off. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/hkust-nlp/B-STaR) | N/A |
| [RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response](https://arxiv.org/abs/2412.14922) | Zhiping Xiao, Jingyang Yuan, Xiao Luo, Junyu Luo, kaize0409 | - ROBUSTFT is a robust supervised fine-tuning framework designed to enhance the performance of Large Language Models (LLMs) in the presence of noisy response data, which is a common issue in real-world applications. - It employs a two-stage process: noise detection and denoising.  Noise detection leverages a multi-expert system with reasoning-enhanced LLMs and a consistency checker, while denoising uses context-enhanced relabeling with a review agent and entropy-based data selection. - The method was evaluated on three LLMs (Gemma2-9B, Llama3.1-8B, Llama3.2-3B) and five datasets (MMLU, ARC, PubMedQA, Drop, FPB) under varying noise levels (30%, 50%, and 70%). - Experimental results show consistent performance improvements across various noise conditions and datasets compared to vanilla LLMs and baseline denoising methods. - The framework also proves particularly beneficial for smaller models and maintains stable performance even with rephrased instructions, validating its robustness and generalizability. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/luo-junyu/RobustFT) | [Link](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B), [Link](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT) |
| [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451) | Yu Cheng, Fan Zhou, Xiwen Zhang, Junlong Li, Wei Liu | - This paper introduces M-STAR, a novel self-evolving training framework for enhancing multimodal reasoning abilities of Large Multimodal Models (LMMs) without relying on human-annotated chain-of-thought data. - M-STAR systematically analyzes and optimizes three key components of self-evolving training: training methods, reward models, and prompt variations. - It presents a continuous self-evolving training scheme, trains the first process-based reward model for multimodal reasoning, and demonstrates that adding unlabeled data is only effective with reliable reward signals. - Additionally, M-STAR incorporates dynamic temperature adjustment to balance exploration and exploitation during training to counter exploration loss. - Experiments on five multimodal reasoning benchmarks show that M-STAR significantly improves the performance of models with varying sizes, such as MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B), and InternVL2 (2B), consistently surpassing pre-trained models across various subtasks. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/hkust-nlp/mstar) | N/A |
| [Deliberation in Latent Space via Differentiable Cache Augmentation](https://arxiv.org/abs/2412.17747) | Arthur Szlam, Jun Xie, Jiaxing Wu, Jonas Pfeiffer, Luyang Liu | - This paper introduces a novel method called "differentiable cache augmentation" to enhance frozen decoder-only Large Language Models (LLMs) by adding a coprocessor that operates on the model's key-value cache. - The coprocessor, trained using the language modeling loss, augments the cache with latent embeddings, improving the fidelity of subsequent decoding without modifying the original LLM architecture. - Experimental results show that this approach consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks, such as GSM8K and MMLU, even in zero/few-shot settings. - The approach allows asynchronous and offline coprocessor operation, opening possibilities for more deliberate and computationally intensive reasoning processes in future research. - The method outperforms the baseline model and a related method called "Pause Token" on tasks like GSM8K, showcasing the effectiveness of the learned context-dependent dynamic embeddings. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/abs/2412.16926) | Oh, Geunseob, Prakhar Gupta, Sun Jae Lee, Jinheon Baek | - This paper revisits In-Context Learning (ICL) with Long Context Language Models (LCLMs) and challenges the prevailing assumption that sophisticated example selection strategies are crucial for optimal performance. - Through experiments on 18 datasets across 4 tasks, the study finds that simple random sampling is as effective as more complex selection methods in many-shot ICL scenarios. - The paper identifies a new challenge with LCLMs: underutilization of expanded context capacity, especially in low-resource tasks.  - It proposes a data augmentation technique to address this which involves generating synthetic examples and filtering low-quality ones, leading to performance improvements of up to 5%. - The study also finds that while LCLMs benefit from larger contexts, performance plateaus and may decline when the context becomes extremely long, especially with noisy examples present, suggesting future research directions for improving robustness in LCLMs. | ['Natural Language Processing', 'Question Answering', 'Translation', 'Summarization'] | N/A | N/A |
| [Outcome-Refining Process Supervision for Code Generation](https://arxiv.org/abs/2412.15118) | Jindong Wang, Zhengran Zeng, Yidong Wang, Weizheng Gu, Zhuohao Yu | - This paper introduces Outcome-Refining Process Supervision (ORPS), a novel paradigm for enhancing code generation by treating outcome refinement as the process to be supervised. - It leverages a tree-structured exploration space, enabling models to maintain multiple reasoning trajectories, guided by execution feedback as objective anchors for evaluation.  - The framework combines beam search with a self-critique mechanism, where the model analyzes reasoning chains and execution outcomes before generating rewards, eliminating the need for trained Process Reward Models (PRMs). - ORPS demonstrates significant improvements across various benchmarks, achieving a 26.9% average increase in Pass@1 and a 42.2% reduction in execution time compared to existing methods.  - These results highlight the effectiveness of coupling structured reasoning space with concrete feedback signals for solving complex coding tasks, offering a scalable and efficient solution. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/zhuohaoyu/ORPS) | N/A |
| [DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/abs/2412.17498) | Jie Zhou, Yunlong Liang, Fandong Meng, Jiaan Wang | - This paper introduces DRT-01, a new model that integrates long chain-of-thought (CoT) into neural machine translation (MT), specifically targeting sentences with similes or metaphors from literature books.  - A multi-agent framework with a translator, advisor, and evaluator is used to iteratively refine translations, generating long-thought MT data. GPT-40 is then employed to enhance the readability of the generated data. - DRT-01 is trained on this data using Qwen2.5-7B/14B as backbones.  - Experimental results show improvements of 7.33~8.26 BLEU and 1.66~3.36 CometScore over the baselines. - Notably, DRT-01-7B surpasses QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showcasing its effectiveness. | ['Translation', 'Natural Language Processing'] | [Link](https://github.com/krystalan/DRT-o1) | [Link](https://huggingface.co/Unbabel/wmt22-cometkiwi-da), [Link](https://huggingface.co/Unbabel/wmt22-comet-da), [Link](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct) |
| [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470) | Junxiao Yang, Jingzhuo Zhou, Yida Lu, Shiyao Cui, Zhexin Zhang | - This paper introduces AGENT-SAFETYBENCH, a comprehensive benchmark designed to evaluate the safety of LLM agents. - The benchmark encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes. - An evaluation of 16 popular LLM agents reveals that none achieve a safety score above 60%. - Analysis identifies two key safety defects: a lack of robustness and a lack of risk awareness. - The study finds that relying solely on defense prompts is insufficient for addressing these safety issues, suggesting the need for more advanced strategies. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/Agent-SafetyBench) | N/A |
| [NILE: Internal Consistency Alignment in Large Language Models](https://arxiv.org/abs/2412.16686) | Hongru Wang, Bowei He, Yufei Wang, Qiyuan Zhang, Minda Hu | - The paper introduces NILE (iNternal consIstency aLignmEnt), a framework designed to improve the quality of Instruction Fine-Tuning (IFT) datasets for Large Language Models (LLMs) by aligning the datasets with the LLMs' internal knowledge. - NILE works by eliciting the internal knowledge of a pre-trained LLM, revising existing dataset answers using this internal knowledge, and filtering out inconsistent samples using a novel Internal Consistency Filtering (ICF) method. - Experiments show that NILE-aligned IFT datasets boost LLM performance across multiple benchmarks, including up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. - Ablation studies validate each component of the framework—Internal Knowledge Extraction, Knowledge-Aware Sample Revision, and Internal Consistency Filtering—confirming their contribution to the improved performance. - The results demonstrate the significance of dataset consistency with pre-trained internal knowledge for maximizing LLM potential. | ['Natural Language Processing'] | N/A | N/A |
| [LearnLM: Improving Gemini for Learning](https://arxiv.org/abs/2412.16429) | Andrea Huber, Aliya Rysbek, Aditya Srikanth Veerubhotla, Abhinit Modi, LearnLM Team | - This paper introduces LearnLM, a new large language model (LLM) based on Gemini 1.5 Pro and fine-tuned specifically for educational applications. - LearnLM is trained using a method called pedagogical instruction following, which uses system-level instructions to guide desired pedagogical behaviours, rather than defining specific pedagogical behaviours.  - It incorporates Reinforcement Learning from Human Feedback (RLHF) for enhanced adherence to nuanced instructions and user preferences.   - Human evaluation results show a significant preference for LearnLM over GPT-40, Claude 3.5, and Gemini 1.5 Pro across various learning scenarios.  - LearnLM is available as an experimental model on Google AI Studio. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [OpenAI o1 System Card](https://arxiv.org/abs/2412.16720) | Adam Richardson, Adam Lerer, Adam Kalai, Aaron Jaech, OpenAI | - OpenAI introduces the "o1" large language model family, trained with reinforcement learning for complex reasoning using chain-of-thought, enhancing safety and robustness. - The models demonstrate state-of-the-art performance in benchmarks related to generating illicit advice, stereotyped responses, and known jailbreaks due to deliberative alignment. - Trained on diverse public, proprietary, and custom datasets, o1 shows enhanced performance in jailbreak evaluations and adherence to safety guidelines compared to GPT-40. - The o1 models also demonstrate significant improvements in mitigating hallucinations, especially in factual question answering, and improved performance in tasks assessing demographic fairness. - Despite advancements, potential safety risks stemming from increased intelligence are acknowledged, highlighting the need for continuous improvement in alignment and safety methods and extensive stress-testing. | ['Multimodal', 'Image-to-Text', 'Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Summarization'] | N/A | N/A |
| [OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning](https://arxiv.org/abs/2412.16849) | Jinlin Xiao, Yuhang Wang, Jiangming Shu, Yuqi Yang, Yuxiang Zhang | - OpenRFT adapts a generalist reasoning model for domain-specific tasks using reinforcement fine-tuning (RFT), addressing challenges like limited training data and lack of reasoning step data. - It leverages domain-specific samples through question augmentation, synthesizing reasoning process data using a teacher model, and few-shot in-context learning (ICL) to enhance RL exploration. - Evaluated on SciKnowEval, OpenRFT demonstrates significant performance improvements with limited samples, averaging 11% accuracy increase compared to a baseline model. - The study highlights that data augmentation, stronger reasoning foundation models, and aligned action space contribute to better RFT performance. -  OpenRFT's effectiveness depends on the availability of high-quality generalist reasoning models and corresponding Process Reward Models (PRMs). | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ADaM-BJTU/OpenRFT) | [Link](https://huggingface.co/Skywork) |
| [Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding](https://arxiv.org/abs/2412.17295) | Qun Liu, Jianxin Liang, Xiaojun Meng, Yueqian Wang, ColorfulAI | - This paper introduces Friends-MMC, a multimodal multi-party conversation (MMC) dataset derived from the TV series *Friends*.  - The dataset includes over 24,000 utterances paired with video contexts, speaker annotations, and bounding boxes of faces, facilitating research on character-centered understanding in conversations. - The authors propose a baseline method for conversation speaker identification leveraging visual and textual models, combined with a quadratic binary optimization solver, demonstrating its effectiveness compared to existing pre-trained models. - For conversation response prediction, they fine-tune generative dialogue models on Friends-MMC and show that incorporating speaker information improves performance.  -  They argue for increased attention to modeling speaker information in conversational understanding research. | ['Multimodal', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/yellow-binary-tree/Friends-MMC) | N/A |
