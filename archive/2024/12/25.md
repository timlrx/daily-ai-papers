

## Papers for 2024-12-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding](https://arxiv.org/abs/2412.18450) | Dmitry Yudin, wingrune | - 3DGraphLLM introduces a novel approach for 3D scene understanding by combining semantic graphs and large language models (LLMs). - The model architecture includes pre-trained encoders for 3D point clouds and semantic relationships, and a pre-trained LLM, trained via a two-stage approach using ground truth and predicted instance segmentation of point clouds. - 3DGraphLLM represents a 3D scene as a flattened sequence of learnable embeddings of object subgraphs, including object identifiers and relationships with k-nearest neighbors, which is then fed to an LLM. - Experimental results on ScanRefer, Multi3DRefer, and Scan2Cap datasets demonstrate state-of-the-art performance for the proposed method, improving F1@0.5 scores by +5.8% and +4.4% and CIDEr@0.5 by +5.8% respectively on the mentioned datasets. - Ablation studies confirm the benefit of incorporating semantic graph representation for several 3D vision-language tasks. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Object Detection', 'Image-to-Text', 'Question Answering', 'Graph Machine Learning'] | [Link](https://github.com/CognitiveAISystems/3DGraphLLM) | N/A |
| [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/abs/2412.17739) | Ning Ding, Kaiyan Zhang, Xingtai Lv, Che Jiang, Ermo Hua | - This paper proposes Fourier Position Embedding (FoPE) to improve the length generalization of Language Models (LMs).  - FoPE enhances the periodic extension of attention by addressing the spectral damage caused by linear layers, activation functions, and under-trained frequency components.  - Unlike Rotary Position Embedding (ROPE), which treats each dimension as a single-frequency function, FoPE models each dimension as a Fourier series of different frequency components. - FoPE also clips inadequately trained frequency components, replacing them with zero-frequency components to preserve long-wavelength information.  - Experiments across various model scales demonstrate FoPEâ€™s superior length generalization compared to ROPE and ALiBi, maintaining stable perplexity and consistent accuracy in a needle-in-haystack task within varying context windows. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/TsinghuaC3I/Fourier-Position-Embedding) | N/A |
| [In Case You Missed It: ARC 'Challenge' Is Not That Challenging](https://arxiv.org/abs/2412.17758) | Borchmann | - This paper challenges the perceived difficulty of the ARC Challenge dataset, arguing that it appears harder for Large Language Models (LLMs) primarily due to an evaluation setup that hinders direct comparison of answer choices rather than inherent complexity. - The authors highlight a shift in evaluation practices where some researchers have adopted a fairer comparison scheme, allowing models to see all answer options simultaneously, which dramatically improves performance. - This fairer approach reduces performance gaps in other benchmarks, such as SocialIQa (SIQA), and even leads to superhuman results in OpenBookQA, suggesting that evaluation methods significantly shape perceived difficulty. - They demonstrate that switching from evaluating answers in isolation to evaluating them alongside other options leads to substantial performance gains, up to 35% improvement on ARC Challenge for some LLMs. - The paper proposes guidelines to ensure multiple-choice evaluations accurately reflect model capabilities by recommending the use of an evaluation setup where LLMs can compare answer options directly. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](https://arxiv.org/abs/2412.14711) | Jun Zhu, Jianfei Chen, Ziteng Wang | - ReMoE, a fully differentiable Mixture-of-Experts (MoE) architecture based on ReLU routing, is proposed as a drop-in replacement for standard TopK routing, offering continuous training and dynamic expert allocation. - ReLU routing manages experts' on/off states independently and is combined with an adaptive load balancing L1 regularization to control sparsity. - ReMoE outperforms traditional TopK MoE and other routing methods across various model sizes, expert counts, and granularity levels on the LLaMA architecture trained on The Pile dataset. - ReMoE demonstrates improved scalability and performance gains compared to TopK MoE with increasing expert counts, suggesting its effectiveness with larger expert pools. - ReMoE exhibits dynamic expert allocation based on token frequency and stronger domain specialization, leading to efficient resource utilization and improved model expressivity. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/thu-ml/ReMoE) | N/A |
| [SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval](https://arxiv.org/abs/2412.15443) | Divya Chaudhary, Vinija Jain, Aman Chadha, Vinesh Kumar Gande, Aakash Mahalingam | - This paper introduces SKETCH, a novel methodology that enhances the Retrieval Augmented Generation (RAG) retrieval process by integrating semantic text retrieval with knowledge graphs. - SKETCH merges structured and unstructured data for a more holistic comprehension, aiming to improve retrieval performance and maintain context integrity. - Evaluated across four diverse datasets (QUALITY, QASPER, NarrativeQA, and Italian Cuisine), SKETCH consistently outperformed baseline approaches on key RAGAS metrics, including answer_relevancy, faithfulness, context_precision, and context_recall. - Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. - These results highlight SKETCH's capability to deliver more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems by addressing challenges in handling large-scale discourse structures and complex queries across various domains. | ['Question Answering'] | N/A | N/A |
