

## Papers for 2024-12-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Token-Budget-Aware LLM Reasoning](https://arxiv.org/abs/2412.18547) | Zhenyu Chen, Shiqing Ma, Shiyu Zhao, Chunrong Fang, Tingxu Han | - This paper introduces TALE (Token-Budget-Aware LLM rEasoning), a framework designed to optimize the reasoning process in Large Language Models (LLMs) by dynamically managing token budgets. - TALE estimates a token budget for each problem based on its complexity, then incorporates this budget into the prompt to guide the LLM's reasoning. - This method addresses the issue of token redundancy in current LLMs, which often produce unnecessarily lengthy reasoning processes, leading to increased costs and computational overhead. - Experimental results demonstrate that TALE significantly reduces token usage by an average of 68.64% while maintaining competitive accuracy (less than 5% decrease). - This suggests that TALE offers a practical approach to balancing efficiency and accuracy in LLM reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/GeniusHTX/TALE) | N/A |
