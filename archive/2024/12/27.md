

## Papers for 2024-12-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/abs/2412.17743) | Jie Chen, Jiapeng Wang, Jia Deng, Huatong Song, Yiwen Hu | - This paper introduces YuLan-Mini, a 2.42B parameter open-source language model trained on 1.08T tokens. - The model uses a decoder-only transformer architecture with grouped-query attention, SwiGLU activation, rotary positional embeddings, and embedding tying. - Key innovations for pre-training include a robust optimization method to improve training stability, a focused data pipeline with data cleaning and scheduling, and an annealing approach for data selection and long context training. - The model is evaluated on benchmarks for math, code generation, reasoning, and language understanding, showing comparable performance to larger industry models trained on significantly more data. - YuLan-Mini achieves state-of-the-art results for similar-sized models and demonstrates significant improvements in training efficiency. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/RUC-GSAI/YuLan-Mini) | N/A |
| [A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/abs/2412.17483) | Xinting Huang, Shuaiyi Li, Kelong Mao, Zhisong Zhang, ChenlongDeng | - This paper investigates gist token-based context compression methods for improving long-context processing in large language models (LLMs). - The study evaluates different gist-based model architectures categorized by memory location (recurrent or key-value cache) and gist granularity (coarse or fine-grained). - Experiments across language modeling, reasoning, and long-context tasks show that fine-grained key-value cache models achieve near-lossless performance on some tasks, while struggling with others like reranking. - Three failure patterns are identified: "lost by the boundary," where generation quality degrades near segment boundaries; "lost if surprise," where unexpected details are lost; and "lost along the way," where models struggle to recover exact rehearsals.  - Two mitigation strategies, fine-grained autoencoding and segment-wise token importance estimation, show improvements, especially in addressing boundary effects and precise recall. | ['Natural Language Processing', 'Question Answering', 'Summarization'] | N/A | N/A |
