

## Papers for 2024-12-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs](https://arxiv.org/abs/2412.18925) | Wanlong Liu, Xidong Wang, Ke Ji, Zhenyang Cai, Junying Chen | - This paper introduces HuatuoGPT-01, a medical Large Language Model (LLM) designed for complex reasoning. - The model is trained in two stages: firstly, it learns complex reasoning by searching for correct reasoning trajectories guided by a medical verifier, and secondly, it refines this skill with reinforcement learning using verifier-based rewards. - It is trained on 40k verifiable medical problems converted from closed-set exam questions, supplemented by a general domain dataset for enhanced generalization. - HuatuoGPT-01 significantly outperforms existing general and medical-specific LLMs on multiple medical benchmarks, including MedQA, MedMCQA, and PubMedQA, as well as more challenging datasets MMLU-Pro and GPQA. - Ablation studies and other analysis demonstrate that the method of complex reasoning and reinforcement learning boosts performance compared to non-CoT or simple CoT approaches. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/FreedomIntelligence/HuatuoGPT-01) | N/A |
| [Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment](https://arxiv.org/abs/2412.19326) | Kunchang Li, Chenting Wang, Yinan He, Zhilin Li, Ziang Yan | - This paper introduces Task Preference Optimization (TPO), a novel method to enhance Multimodal Large Language Models (MLLMs) with fine-grained visual task alignment. - TPO utilizes learnable task tokens and corresponding task heads for region, temporal, and mask-related visual tasks and incorporates visual task data during training via a local-to-global training process to improve both multimodal dialogue and task-specific performance. - Experiments on VideoChat and LLaVA demonstrate a 14.6% average improvement in multimodal performance on benchmarks like MVBench, VideoMME, NExT-GQA, MLVU, and SEED-Bench2. - The model performs competitively with state-of-the-art models on tasks like spatial grounding, moment retrieval, highlight detection, and tracking. - Ablation studies validate the efficacy of different TPO components, co-training benefits, and task data scaling. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/OpenGVLab/TPO) | N/A |
| [Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging](https://arxiv.org/abs/2412.19512) | Shang-Tse Chen, Saurav Sahay, Shachi H Kumar, Hsuan Su, farnhua | - This paper introduces a method to mitigate safety degradation in fine-tuned Large Language Models (LLMs) without requiring additional safety data. - The method involves merging the weights of a pre-trained safety-aligned base model and its fine-tuned version after training on a downstream task. - Experimental results across various models (Llama-3, Gemma-2B-it), downstream tasks (reasoning, medical assistance, code generation, tool usage), and merging methods (linear merging, model stock, SLERP, DARE) show improvement in safety and downstream task performance. - The proposed approach reduces the Attack Success Rate (ASR) by up to 30% while enhancing downstream task performance, offering a simple yet robust solution. - Linear merging is highlighted as a practical method due to its efficacy and computational efficiency | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/allenai/wildguard) |
| [SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images](https://arxiv.org/abs/2412.17606) | Yoshitaka Ushiku, Tosho Hirasawa, Shohei Tanaka, Kuniaki Saito, Risa Shinoda | - This paper introduces SBS Figures (Stage-by-Stage Synthetic Figures), a new dataset for pre-training figure question answering (QA) models. - The dataset is generated through a novel three-stage pipeline involving visualization target data generation, figure rendering via Python code, and QA pair generation, leveraging LLMs at each stage and producing 1 million synthetic chart figures with associated data and QA pairs. - Models pre-trained on SBS Figures demonstrate a strong performance boost on real-world chart QA datasets like ChartQA, outperforming models trained from scratch or other synthetic datasets. - This method allows efficient training of QA models even with a limited amount of real-world data by first pre-training on the large-scale SBS Figures dataset. - An ablation study investigating various factors in the pipeline reveals the importance of diverse figure appearance, high-quality LLM-generated QA pairs, and the scale of the dataset for optimal pre-training effectiveness. | ['Visual Question Answering', 'Document Question Answering', 'Multimodal'] | [Link](https://github.com/omron-sinicx/SBSFigures) | N/A |
