

## Papers for 2024-12-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [On the Compositional Generalization of Multimodal LLMs for Medical Imaging](https://arxiv.org/abs/2412.20070) | Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai | - This paper introduces Med-MAT, a Visual Question Answering (VQA) dataset designed to investigate compositional generalization (CG) in Multimodal Large Language Models (MLLMs) applied to medical imaging. - Med-MAT comprises 106 medical datasets spanning 11 modalities, 14 anatomical areas, and 13 medical tasks, categorized by MAT-Triplets (Modality, Anatomical area, Task) to facilitate CG analysis. - Experiments using LLaVA and other MLLMs on Med-MAT confirm that these models can leverage CG to understand unseen medical image combinations, demonstrating improved performance in classification tasks compared to single-task or unrelated multi-task training. - The paper shows that increasing the volume of CG combinations improves model understanding, and that CG assists data-efficient learning even with limited target data. - The study also demonstrates the existence of CG across different MLLM backbones (LLaVA, Qwen2-VL, Llama 3.2) and its applicability in detection tasks, highlighting its broad applicability and robustness in medical image analysis. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Image Classification'] | [Link](https://github.com/FreedomIntelligence/Med-MAT) | N/A |
| [Efficiently Serving LLM Reasoning Programs with Certaindex](https://arxiv.org/abs/2412.20993) | Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu | - Dynasor optimizes inference-time compute for LLM reasoning queries by tracking and scheduling requests within reasoning queries and using certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. - Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost. - Dynasor reduces compute by up to 50% in batch processing and sustains 3.3x higher query rates or 4.7x tighter latency SLOs in online serving on diverse datasets and algorithms. - Certaindex quantifies how certain the LLM is in approaching its final answer during reasoning, which correlates with computational resources required for correct solutions. - Dynasor outperforms other systems by reducing token usage by 9-52% without sacrificing accuracy on offline workloads and achieving higher sustainable request rates and tighter SLO scales. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization](https://arxiv.org/abs/2412.21037) | Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung | - TANGOFLUX, a novel text-to-audio (TTA) model based on a hybrid architecture of Multimodal Diffusion Transformer (MMDIT) and Diffusion Transformer (DiT) blocks, is introduced.  - The model leverages rectified flows for audio generation, enabling faster inference speeds and utilizes CLAP-Ranked Preference Optimization (CRPO) for alignment.  - CRPO iteratively generates synthetic audio preference data by ranking model outputs based on CLAP similarity scores and then optimizes the model using a novel loss function that prevents performance degradation.  - Experimental results show that TANGOFLUX achieves state-of-the-art performance on objective metrics such as Frechet Distance and CLAP score while significantly reducing inference time compared to existing models.  - Furthermore, human evaluations confirm that TANGOFLUX generates audio of higher quality and greater relevance to the input text compared to other leading TTA models. | ['Text-to-Audio', 'Audio'] | [Link](https://github.com/declare-lab/TangoFlux) | [Link](https://huggingface.co/declare-lab/TangoFlux), [Link](https://huggingface.co/datasets/declare-lab/CRPO), [Link](https://huggingface.co/spaces/declare-lab/TangoFlux) |
| [HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation](https://arxiv.org/abs/2412.21199) | Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu | - This paper introduces "self-invoking code generation," a new task designed to evaluate LLMs' progressive reasoning and problem-solving abilities where models must use the solution from a base problem to address a more complex, related problem. - Three new benchmarks are created: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, which are derived from existing benchmarks using a proposed method. - Experimental results on 20 LLMs reveal a significant performance gap between traditional code generation and self-invoking code generation, where models often struggle to utilize their own generated code effectively. - It's shown that instruction-tuned models offer limited improvement over base models in self-invoking tasks, indicating a need for more advanced training methods for this type of problem. - Analysis of failure modes reveals challenges LLMs face with self-invocation, emphasizing areas for future improvement in code generation and reasoning capabilities. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Feature Extraction'] | [Link](github.com/CodeEval-Pro/CodeEval-Pro) | N/A |
| [Facilitating large language model Russian adaptation with Learned Embedding Propagation](https://arxiv.org/abs/2412.21140) | Daniil Chernyshev, RefalMachine | - This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models (LLMs) to new languages requiring less training data and minimal disruption of existing LLM knowledge. - LEP employs an embedding propagation technique, bypassing instruction-tuning and directly integrating new language knowledge.  - A new benchmark, Darumeru, is introduced to evaluate text generation robustness during training for Russian adaptation.  - Applying LEP to adapt LLaMa-3-8B and Mistral-7B for Russian, across four vocabulary adaptation scenarios, demonstrates competitive performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct.  - Further improvements are observed using self-calibration and additional instruction-tuning, exceeding existing models' performance. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/RefalMachine/ruadapt), [Link](https://github.com/RefalMachine/llmtf_open) | [Link](https://huggingface.co/RefalMachine) |
| [Training Software Engineering Agents and Verifiers with SWE-Gym](https://arxiv.org/abs/2412.21139) | Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan | - This paper introduces SWE-Gym, a new training environment for real-world software engineering (SWE) agents. - SWE-Gym consists of 2,438 Python tasks from GitHub, each with an executable runtime environment, unit tests, and a natural language task description. - Using SWE-Gym to train language model-based SWE agents led to up to a 19% improvement in resolve rate on SWE-Bench Verified and Lite datasets.  - By training verifiers on agent trajectories from SWE-Gym and combining them with fine-tuned SWE agents, the resolve rate further increased to 32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, setting a new state-of-the-art for open-weight agents. - The paper analyzes the scaling behavior of both the training process and the inference-time scaling using verifiers, revealing continuous improvements with increased compute budget. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/SWE-Gym/SWE-Gym) | N/A |
| [OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System](https://arxiv.org/abs/2412.20005) | Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo | - OneKE is a dockerized schema-guided knowledge extraction system based on LLMs and a multi-agent design. - The system uses three agents: a Schema Agent, an Extraction Agent, and a Reflection Agent to handle various extraction scenarios. - It supports extraction from various data sources like web pages and PDF documents without fine-tuning and incorporates a configurable knowledge base to improve performance and allow error debugging. - Experimental results on CrossNER and NYT-11-HRL datasets demonstrate the efficacy of OneKE, with the Case Retrieval component of the Extraction Agent showing significant improvement. - Case studies on web news and book knowledge extraction further illustrate the practical applicability of OneKE. | ['Natural Language Processing', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/zjunlp/OneKE) | N/A |
