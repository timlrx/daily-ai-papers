

## Papers for 2025-01-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [On the Compositional Generalization of Multimodal LLMs for Medical Imaging](https://arxiv.org/abs/2412.20070) | Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai | - This paper introduces Med-MAT, a visual question answering (VQA) dataset designed to investigate compositional generalization (CG) in multimodal large language models (MLLMs) applied to medical imaging. - Med-MAT comprises 106 medical datasets categorized by modality, anatomical area, and task (MAT-Triplet), forming 53 subsets with 11 modalities, 14 anatomical areas, and 13 medical tasks. - Experiments demonstrate that MLLMs can leverage CG to understand unseen medical images, which is a key driver of the generalization observed in multi-task training. - Further analysis shows that CG supports data-efficient training with limited data and demonstrates consistent performance across different MLLM backbones. - The authors also explore CG between detection and classification tasks, finding that MLLMs can combine knowledge from both to improve classification accuracy. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Image Classification'] | [Link](https://github.com/FreedomIntelligence/Med-MAT) | N/A |
| [Efficiently Serving LLM Reasoning Programs with Certaindex](https://arxiv.org/abs/2412.20993) | Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu | - Dynasor optimizes inference-time compute for LLM reasoning queries by tracking and scheduling requests within reasoning queries and using certaindex, a proxy that measures statistical reasoning progress. - Certaindex guides compute allocation dynamically, allocating more compute to hard queries, reducing compute for simpler ones, and terminating unpromising queries early. - Evaluated on diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustains 3.3× higher query rates or 4.7× tighter latency SLOs in online serving. - Dynasor is implemented as a scheduling layer compatible with existing serving engines. - The system utilizes certaindex as a narrow interface between itself and the applications to support various current and future reasoning algorithms. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization](https://arxiv.org/abs/2412.21037) | Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung | - TANGOFLUX, a 515M parameter text-to-audio (TTA) model based on a hybrid multimodal and diffusion transformer architecture, generates up to 30 seconds of 44.1kHz audio in 3.7 seconds on a single A40 GPU using rectified flows. - The model uses CLAP-Ranked Preference Optimization (CRPO), which iteratively generates synthetic audio preference data and employs CLAP as a proxy reward model to improve alignment with textual descriptions. - Evaluation on AudioCaps and a challenging out-of-distribution dataset reveals that TANGOFLUX achieves state-of-the-art performance across various objective metrics, including Frechet Distance, KL divergence, CLAP score, and Inception Score, outperforming models like Tango 2, AudioLDM 2, and Stable Audio Open. - Human evaluation further confirms TANGOFLUX's superior audio quality and strong alignment with textual prompts. - Ablation studies demonstrate the effectiveness of CRPO's online data generation, the use of CLAP as a reward model, and the improvement of LCRPO over LDPO-FM loss for preference optimization. | ['Text-to-Audio', 'Audio'] | [Link](https://github.com/declare-lab/TangoFlux) | [Link](https://huggingface.co/declare-lab/TangoFlux), [Link](https://huggingface.co/spaces/declare-lab/TangoFlux), [Link](https://huggingface.co/datasets/declare-lab/CRPO) |
| [Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs](https://arxiv.org/abs/2412.21187) | Jianhui Pang, Zhiwei He, Tian Liang, Jiahao Xu, Xingyu Chen | - This paper investigates the "overthinking" issue in large language models (LLMs), particularly those like OpenAI's o1, where excessive computational resources are used for simple problems. - The authors introduce novel efficiency metrics from both outcome (accuracy improvement relative to token usage) and process (diversity of reasoning strategies) perspectives to evaluate the rational use of computational resources. - They propose a self-training paradigm using the PRM12K dataset and strategies to mitigate overthinking by streamlining reasoning processes while preserving accuracy. - Experimental results on various mathematical reasoning datasets, including GSM8K, MATH500, GPQA, and AIME, demonstrate that the proposed approach reduces computational overhead without compromising model performance. -  For example, the approach reduces token output by 48.6% while maintaining accuracy on the MATH500 dataset when applied to QwQ-32B-Preview. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Facilitating large language model Russian adaptation with Learned Embedding Propagation](https://arxiv.org/abs/2412.21140) | Daniil Chernyshev, RefalMachine | - This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models (LLMs) to new languages without requiring extensive instruction-tuning data. - LEP works by propagating learned embeddings from a pre-trained, instruction-tuned LLM in one language to a new LLM initialized with a vocabulary tailored for the target language. - The method was evaluated on Russian adaptation of Mistral-7B and LLaMa-3-8B using a new benchmark called Darumeru, specifically designed for evaluating text generation robustness in Russian. - Results show that LEP achieves competitive performance compared to existing models like OpenChat 3.5 and LLaMa-3-8B-Instruct, demonstrating its effectiveness in language adaptation while reducing costs associated with traditional instruction-tuning. - Further improvements were observed by calibrating the adapted models through self-instruct tuning and additional instruction-tuning steps, highlighting the potential of LEP for enhancing LLM performance beyond existing benchmarks. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/RefalMachine/ruadapt), [Link](https://github.com/RefalMachine/llmtf_open) | [Link](https://huggingface.co/RefalMachine) |
| [OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System](https://arxiv.org/abs/2412.20005) | Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo | - OneKE is a dockerized schema-guided knowledge extraction system based on LLMs and a multi-agent design. - It supports various data formats (web, PDF) and domains (science, news) through different agents. - The system includes a configurable knowledge base for schema configuration and error debugging. - Evaluation on benchmark datasets and case studies demonstrate OneKE's efficacy and adaptability. - The system is open-source and supports different LLMs without fine-tuning. | ['Natural Language Processing', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/zjunlp/OneKE) | N/A |
| [Training Software Engineering Agents and Verifiers with SWE-Gym](https://arxiv.org/abs/2412.21139) | Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan | - This paper introduces SWE-Gym, a new training environment for software engineering (SWE) agents designed to address the limitations of current resources, which often lack executable environments and reward signals. - SWE-Gym contains 2,438 real-world Python tasks from GitHub issues, each with a codebase, an executable runtime environment, unit tests, and a natural language task description. - The authors demonstrate that fine-tuning a 32B Qwen-2.5 language model with SWE-Gym can achieve state-of-the-art resolve rates of 32.0% and 26.0% on the SWE-Bench Verified and Lite test sets, respectively. - This involves an improvement of 19% compared to existing methods on these benchmarks. - This is further enhanced by training a verifier on agent trajectories, enabling inference-time scaling through candidate solution selection. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/SWE-Gym/SWE-Gym) | N/A |
| [HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation](https://arxiv.org/abs/2412.21199) | Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu | - This paper introduces "self-invoking code generation," a new task to evaluate LLMs' progressive reasoning and problem-solving skills by requiring them to solve a base problem and then use its solution to address a more complex related problem. - Three new benchmarks, HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, are created by extending existing datasets using a proposed general recipe and involve Deepseek-V2.5 for problem generation and human expert review. - Experiments on over 20 LLMs reveal that while models excel in traditional code generation, their performance declines significantly on self-invoking tasks, with even top models like o1-mini showing a substantial drop. - Instruction-tuned models exhibit only marginal improvements in self-invoking tasks compared to base models. - Analysis of failure modes highlights LLMs' struggle with generating code that can effectively self-invoke and suggests limitations in instruction-based fine-tuning for such complex tasks. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Feature Extraction'] | [Link](https://github.com/CodeEval-Pro/CodeEval-Pro) | N/A |
