

## Papers for 2025-01-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723) | Yian Wang, Chuanyang Jin, Kanzhi Cheng, heroding77, QiushiSun | - OS-Genesis is a novel GUI data synthesis pipeline that automates the construction of high-quality and diverse agent trajectories without human supervision or predefined tasks. - Instead of relying on predefined tasks, OS-Genesis allows agents to explore environments freely and interact step-wise, retroactively deriving tasks from interactions. This interaction-driven approach reverses the conventional trajectory collection process and enables more effective exploration. - The pipeline incorporates a trajectory reward model (TRM) to filter and prioritize the synthesized trajectories for more effective utilization. - Evaluations on challenging online mobile benchmarks like AndroidWorld show that OS-Genesis doubles the performance of existing task-driven methods and outperforms other data synthesis approaches on unseen, out-of-distribution apps. -  Analysis suggests that OS-Genesis significantly increases the diversity of both generated instructions and trajectories, more closely mirroring human-like interactions with digital environments than existing synthetic data generation methods. | ['Multimodal'] | N/A | N/A |
| [Xmodel-2 Technical Report](https://arxiv.org/abs/2412.19638) | Jiang Ling, Qu Zhijiu, Lin Qingquan, Liu Yang, valeriaWong | - Xmodel-2 is a 1.2 billion parameter large language model specialized for reasoning tasks, using a unified set of hyperparameters across different model scales for efficient experimentation and configuration transfer. - Trained on 1.5 trillion tokens, Xmodel-2 employs the Warmup-Stable-Decay (WSD) learning rate scheduler from MiniCPM for enhanced training efficiency and stability. - It achieves state-of-the-art performance on complex reasoning and agent-based tasks compared to other models in the 1-2 billion parameter range, demonstrating the effectiveness of its design and training approach. - The model architecture is similar to Llama 2 and incorporates techniques like embedding sharing, deep-and-thin structure and grouped-query attention for optimized training and inference. - The model exhibits good calibration properties with predicted confidence closely aligned with actual correctness probabilities. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Text2Text Generation', 'Fill-Mask', 'Sentence Similarity', 'Feature Extraction', 'Summarization', 'Translation'] | [Link](https://github.com/XiaoduoAILab/Xmodel-2) | N/A |
