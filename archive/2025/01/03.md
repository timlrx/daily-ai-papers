

## Papers for 2025-01-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/abs/2501.00958) | Yongliang Shen, Jiashuo Sun, Xin Li, Hang Zhang, Wenqi Zhang | - This paper introduces "Textbook", a large-scale, high-quality multimodal dataset designed for vision-language pretraining. - The dataset is constructed from 2.5 years of online instructional videos, totaling 22,000 class hours and covering subjects like mathematics, physics, and chemistry.  - The videos are processed into an interleaved format of images and text, focusing on coherent context and knowledge density. - Experimental results demonstrate that models pretrained on "Textbook" achieve significant improvements on knowledge- and reasoning-intensive tasks, such as ScienceQA and MathVista. - The dataset improves few-shot performance, attributed to its video-centric interleaved design that enhances in-context learning capabilities. | ['Multimodal'] | [Link](https://github.com/DAMO-NLP-SG/multimodal_textbook) | N/A |
| [CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/abs/2501.01257) | Dayiheng Liu, Bo Zheng, Bowen Yu, Jiaxi Yang, Shanghaoran Quan | - This paper introduces CODEELO, a new benchmark for evaluating competition-level code generation capabilities of Large Language Models (LLMs). - CODEELO uses problems from the CodeForces platform, along with contest divisions, problem difficulty ratings, and problem algorithm tags.  - The benchmark employs a unique evaluation method where solutions are submitted directly to the CodeForces platform, ensuring accurate judgments with special judge support and a consistent execution environment.  - An Elo rating system is implemented to provide human-comparable ratings for LLMs based on their performance.  - Initial evaluation results demonstrate that OpenAI's o1-mini and QwQ-32B-Preview models achieve high Elo ratings, outperforming most open-source models. | ['Natural Language Processing', 'Text2Text Generation', 'Text Generation'] | N/A | [Link](https://hf.co/Qwen/CodeElo) |
| [VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM](https://arxiv.org/abs/2501.00599) | Boqiang Zhang, Zesen Cheng, Wentong Li, Hang Zhang, Yuqian Yuan | - The VideoRefer Suite is introduced, which consists of a new dataset (VideoRefer-700K), a model (VideoRefer), and a benchmark (VideoRefer-Bench) designed to improve the spatial-temporal object understanding capabilities of Video Large Language Models (LLMs). - VideoRefer, built upon VideoLLaMA2, incorporates a novel spatial-temporal object encoder that extracts pixel-level regional features within single frames and aggregates temporal information across multiple frames using a Temporal Token Merge Module. - The VideoRefer-700K dataset was created using a multi-agent data engine, incorporating diverse object-level instructions including descriptions, short captions, and multi-round question-answer pairs. - Experimental results demonstrate that VideoRefer outperforms existing generalist and specialist models on VideoRefer-Bench and a traditional video referring benchmark, HC-STVG, showing improved performance in metrics like subject correspondence, appearance and temporal description, and hallucination detection. - Moreover, VideoRefer also shows performance improvements on general video understanding benchmarks like Perception-Test, MVBench, and VideoMME, indicating its broader applicability. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/abs/2501.01264) | Wenbo Su, Jiaheng Liu, Weixun Wang, Yanan Wu, Xiaoshuai Song | - ProgCo, a novel program-driven self-correction method for Large Language Models (LLMs), is introduced, comprising program-driven verification (ProgVe) and refinement (ProgRe). - ProgVe employs self-generated, self-executing pseudo-programs for enhanced verification logic and validation, while ProgRe uses dual reflection and refinement on both responses and verification programs to mitigate misleading feedback in complex reasoning. - Experiments across instruction-following and mathematical reasoning tasks demonstrates ProgCo's efficacy in achieving effective self-correction. - Combining ProgCo with real program tools further enhances performance. - ProgCo shows greater improvement in mathematical reasoning tasks compared to existing baselines, highlighting its effectiveness in complex reasoning scenarios. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models](https://arxiv.org/abs/2501.00316) | Md Hasebul Hasan, Md Tanvir Parvez, Md Tanvir Hassan, Mahir Labib Dihan, eunus | - This paper introduces MAPEVAL, a benchmark designed to evaluate the geo-spatial reasoning capabilities of foundation models. - MAPEVAL features three task types: textual, API-based, and visual, requiring models to process diverse geo-spatial contexts, perform compositional reasoning, and interact with map tools. - An evaluation of 28 prominent foundation models using MAPEVAL revealed that while models like Claude-3.5-Sonnet, GPT-40, and Gemini-1.5-Pro performed competitively, substantial performance gaps exist, especially in the API-based tasks, and all models fall short of human performance. - The benchmark includes 700 unique multiple-choice questions spanning locations across 180 cities and 54 countries. - Further analyses suggest that integrating external tools, like calculators, can enhance performance in specific sub-tasks, such as calculating straight-line distances and cardinal directions. | ['Multimodal', 'Question Answering'] | [Link](https://github.com/MapEval) | N/A |
| [Dynamic Scaling of Unit Tests for Code Reward Modeling](https://arxiv.org/abs/2501.01054) | Sijia Luo, Jifan Yu, Jing Zhang, Xiaokang Zhang, KAKA22 | - This paper introduces CodeRM-8B, a lightweight unit test generator designed for efficient and high-quality unit test scaling in code reward modeling. - The authors demonstrate that scaling the number of unit tests improves the quality of the reward signal, particularly for more complex problems, leading to better identification of correct code solutions. - CodeRM-8B is trained using a novel data synthesis pipeline that produces high-quality unit tests from existing code instruction-tuning datasets. It leverages supervised fine-tuning (SFT) on Llama3.1-8B and implements a dynamic scaling mechanism adapting to problem difficulty for improved efficiency. - Experimental results on HumanEval Plus, MBPP Plus, and LiveCodeBench show significant performance improvements across various models, with CodeRM-8B achieving gains of 18.43% for Llama3-8B and 3.42% for GPT-40-mini on HumanEval Plus, comparable to the much larger Llama3.1-70B model. - Dynamic unit test scaling further boosts performance by allocating more computation resources to harder problems, leading to an additional 0.5% performance gain on MBPP Plus under fixed computational cost. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://code-reward-model.github.io) | [Link](https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction), [Link](https://huggingface.co/datasets/BAAI/TACO) |
| [MLLM-as-a-Judge for Image Safety without Human Labeling](https://arxiv.org/abs/2501.00192) | Felix Juefei-Xu, Xiaowen Lin, Shiyu Zhao, Shuming Hu, Zhenting Wang | - This paper introduces CLUE (Constitutional MLLM JUdgE), a novel framework for zero-shot image safety judgment using Multimodal Large Language Models (MLLMs) and a predefined safety constitution (a set of safety rules). - CLUE addresses challenges like subjective rules, complex constitutions, and model biases by objectifying rules, assessing rule-image relevance using contrastive models, and employing debiased token probabilities with logical precondition chains for judgments. - The method includes a deeper reasoning mechanism with cascaded chain-of-thought processes, if necessary, offering high confidence and explanations. - The experiments demonstrate CLUE's significant improvement over zero-shot and fine-tuning baselines for image safety classification using objective rules. - For example, CLUE with InternVL2-76B achieves 95.9% recall, 94.8% accuracy, and an F1-score of 0.949 on a newly created benchmark. | ['Multimodal', 'Zero-Shot Classification', 'Computer Vision'] | N/A | N/A |
| [Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing](https://arxiv.org/abs/2501.00658) | Jiajun Zhu, Yuehao Wang, Ruisi Cai, Peihao Wang, pragsri8 | - This paper identifies two key limitations of State Space Models (SSMs): strong recency bias, hindering recall of distant information and robustness, and over-smoothing in deeper architectures, making token representations indistinguishable. - The authors theoretically demonstrate the recency bias of SSMs, showing that influential scores between tokens decay exponentially with distance, and empirically validate this through a long-context retrieval task, where SSMs struggle compared to Transformers. - Over-smoothing is revealed through scaling experiments and theoretical analysis, demonstrating that SSM layers diminish pairwise differences between memory states, causing performance to plateau and decline with increasing depth. - A novel "polarization" technique is proposed, reserving dedicated channels in state transition matrices for values of zero and one, simultaneously addressing both recency and over-smoothing. - Experiments on associative recall demonstrate that polarization enhances long-range recall accuracy and enables SSMs to benefit from deeper architectures. | ['Natural Language Processing'] | [Link](https://github.com/VITA-Group/SSM-Bottleneck) | N/A |
| [MapQaTor: A System for Efficient Annotation of Map Query Datasets](https://arxiv.org/abs/2412.21015) | Md Rizwan Parvez, Mohammed Eunus Ali, mahirlabibdihan | - MAPQATOR is a web application designed to streamline the creation of map-based question answering (QA) datasets by integrating with any map API in a plug-and-play manner, reducing manual effort. - It offers visualization tools and caches API responses for consistent ground truth and data reliability, enabling complex geospatial reasoning evaluation and improvement. - Evaluation shows MAPQATOR speeds up annotation by at least 30 times compared to manual methods. - The system addresses the gap in efficiently annotating language-map reasoning tasks, aiding in developing reliable LLM training datasets. - MAPQATOR facilitates better geospatial understanding by centralizing data retrieval, annotation, and visualization, benefiting tasks like complex map reasoning. | ['Question Answering'] | [Link](https://github.com/mapqator/) | N/A |
