

## Papers for 2025-01-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction](https://arxiv.org/abs/2501.01957) | hertin, shenyunhang, yifanzhang114, xiongwang, linhaojia13 | - VITA-1.5 is a multimodal large language model (MLLM) that integrates vision, language, and speech modalities using a three-stage training approach, enabling real-time vision and speech interaction. - The model architecture consists of vision and audio encoders with adapters connected to an LLM, and an end-to-end speech generation module, eliminating the need for external ASR and TTS systems. - The training process involves vision-language training, followed by audio input tuning and audio output tuning stages which aims at minimizing the training conflicts between the multiple modalities. - VITA-1.5 achieves comparable performance to state-of-the-art models on image and video understanding benchmarks and exhibits significant improvements in speech capabilities. - Evaluation on ASR benchmarks shows that VITA-1.5 outperforms specialized speech models in both Mandarin and English tasks, highlighting the models ability to integrate effective real-time vision and audio-speech interaction. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Automatic Speech Recognition', 'Text-to-Speech'] | [Link](https://github.com/VITA-MLLM/VITA) | [Link](https://huggingface.co/OpenGVLab/InternViT-300M-448px) |
| [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904) | jrwen, whenfra, yifanli, JohnCage, Richard1999 | - This paper introduces Virgo, a multimodal slow-thinking system designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) for complex visual tasks. - The core approach involves fine-tuning a capable MLLM (Qwen2-VL-72B-Instruct) with a small amount of textual long-form thought data, hypothesizing that slow-thinking capacity is primarily associated with the language component and can transfer across modalities. - Experimental results on MathVerse, MathVision, OlympiadBench, and MMMU benchmarks demonstrate that Virgo achieves competitive performance compared to commercial reasoning systems, sometimes even surpassing them. - It was found that textual reasoning data is generally more effective than visual reasoning data for improving the reasoning ability of the MLLMs. - Further analysis suggests that harder tasks benefit more from long thought reasoning, but excessively long reasoning processes may lead to performance degradation; moreover, current visual instruction generation methods do not show significant advantages over textual ones. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/RUCAIBox/Virgo) | N/A |
| [BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery](https://arxiv.org/abs/2501.01540) | Louise Li, Lyle Goodyear, ngoodman, michaelyli, obiwan96 | - BoxingGym, a new benchmark, has been introduced to evaluate the performance of autonomous agents in experimental design and model discovery within a scientific context. - The benchmark uses 10 simulated environments based on real-world scientific models, allowing agents to actively experiment and revise theories based on data. - Evaluation metrics include Expected Information Gain for experiment design and a communication-based approach where an agent's explanation enables a novice agent to make predictions for model discovery. - Initial experiments show that current LLMs, augmented or not with statistical modeling capabilities, struggle with both experimental design and model discovery. - The benchmark aims to promote research on agents capable of iterative model discovery through active experimentation and communication. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/kanishkg/boxing-gym/tree/v0.1.0-beta) | N/A |
| [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/abs/2501.00874) | anoperson, Franck-Dernoncourt, ryanrossi, ntnghia1811, Hieuman | - LUSIFER is a novel zero-shot approach that adapts English LLM-based embedding models for multilingual tasks without requiring explicit multilingual supervision. - It leverages XLM-R's multilingual representations and a learnable connector to transfer language understanding to English-optimized LLM embedding models. - Experimental results on 123 datasets across 14 languages show a significant performance increase, averaging 3.19 points across all tasks, with substantial gains for medium and low-resource languages. - In cross-lingual scenarios involving over 100 languages, LUSIFER surpasses existing English-centric models by 5.75 points on average. - This approach enhances multilingual representation capabilities without the need for explicit multilingual supervision. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | [Link](https://github.com/hieum98/lusifer) | N/A |
