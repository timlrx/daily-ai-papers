

## Papers for 2025-01-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning](https://arxiv.org/abs/2501.03226) | lindahua, yhcao, KennyUTC, yuhangzang, BeichenZhang | - BoostStep, a novel method, enhances the mathematical reasoning capabilities of Large Language Models (LLMs) by improving single-step reasoning through refined in-context learning. - It addresses the granularity mismatch and negative-effect noise within traditional in-context learning examples by providing step-level guidance with a "first-try" strategy. - This strategy retrieves highly related in-context examples based on the model's initial reasoning attempt for each step, improving reasoning quality. - BoostStep improves the performance of GPT-40 and Qwen2.5-Math-72B by 3.6% and 2.0% respectively on various mathematical benchmarks, and by 7.5% when combined with Monte Carlo Tree Search (MCTS). - It seamlessly integrates with MCTS, improving both candidate generation and decision-making processes. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/beichenzbc/BoostStep) | N/A |
| [Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction](https://arxiv.org/abs/2501.03218) | myownskyW7, lindahua, yhcao, yuhangzang, Mar2Ding | - Dispider is a novel system designed for active, real-time interaction with streaming videos, disentangling perception, decision, and reaction into asynchronous modules. - It features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction using scene-based features and historical interactions. - An asynchronous interaction module provides detailed responses without interrupting video processing, ensuring real-time performance and multi-step reasoning capabilities. - Experiments on StreamingBench and a subset of ETBench show Dispider significantly outperforms existing online video LLM models in temporal grounding and proactive response generation. - It also maintains strong performance on conventional video QA tasks across long-video benchmarks such as EgoSchema, VideoMME, and MLVU, demonstrating effectiveness in handling long video lengths and complex interactions. | ['Video-Text-to-Text', 'Multimodal', 'Question Answering'] | [Link](https://github.com/Mark12Ding/Dispider) | N/A |
| [Personalized Graph-Based Retrieval for Large Language Models](https://arxiv.org/abs/2501.02157) | Franck-Dernoncourt, namyongp, Ojasmitha17, Tobilee, StevenAu | - This paper introduces PGraphRAG, a framework leveraging user-centric knowledge graphs for personalized text generation with LLMs. - PGraphRAG enhances personalization by augmenting prompts with user-relevant context retrieved from the knowledge graph, improving contextual understanding and output quality. - A new benchmark, the Personalized Graph-based Benchmark for Text Generation, is presented to evaluate the effectiveness of PGraphRAG. - Experimental results demonstrate that PGraphRAG significantly outperforms state-of-the-art personalization methods, especially in cold-start scenarios with limited user history. - The integration of structured user knowledge graphs through PGraphRAG allows for richer, contextually appropriate personalized responses. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/PGraphRAG-benchmark/PGR-LLM) | N/A |
| [Test-time Computing: from System-1 Thinking to System-2 Thinking](https://arxiv.org/abs/2501.02497) | Jia Xu, Kaixin Wu, Hai Ye, douvleplus, Yisam | - This paper surveys test-time computing methods, categorizing them into System-1 (for perceptual tasks) and System-2 (for cognitive tasks) models. - For System-1, test-time adaptation methods like parameter updates, input modification, representation editing, and output calibration are discussed, focusing on enhancing robustness and generalization. - For System-2, techniques such as repeated sampling, self-correction, and tree search are explored, aiming to improve reasoning and planning abilities. - The paper traces the evolution from System-1 to System-2 thinking, emphasizing test-time computing's role in this transition, and suggests potential future directions. - The o1 model is used as an example of the test-time computing scaling effect where increased computational effort at inference leads to improved performance in complex reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Dereck0602/Awesome_Test_Time_LLMs) | N/A |
| [METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring](https://arxiv.org/abs/2501.02045) | willieneis, oliu-io, upup-ashton-wang, Johannes, oliu-io | - METAGENE-1 is a 7-billion parameter autoregressive transformer model, pre-trained on a novel corpus of 1.5 trillion base pairs of metagenomic DNA and RNA sequences from wastewater. - The model utilizes byte-pair encoding (BPE) tokenization tailored for metagenomic sequences and a decoder-only architecture. - In benchmarks, METAGENE-1 achieves state-of-the-art results on genomic tasks like pathogen detection and sequence embedding, outperforming models trained on curated species genomes. - It demonstrates potential for pandemic monitoring and early detection of emerging health threats through wastewater analysis. - The model's open-source release aims to accelerate research in genomic anomaly detection while acknowledging safety considerations for future model development. | ['Natural Language Processing', 'Text Generation'] | [Link](github.com/metagene-ai) | [Link](huggingface.co/metagene-ai) |
| [Ingredients: Blending Custom Photos with Video Diffusion Transformers](https://arxiv.org/abs/2501.01790) | Di Qiu, MichaelFan, Changqian, Debang, onion | - Ingredients, a training-free framework, is introduced for customizing video generation with video diffusion transformers by incorporating multiple user-provided ID photos. - The framework includes a facial extractor, a multi-scale projector, and an ID router to handle ID features, embed them into the video diffusion transformer context, and allocate the ID embeddings to corresponding regions, respectively. - Ingredients supports multi-ID customization without prompt constraints, offering flexibility and precision in video synthesis. - Evaluations show Ingredients' superior performance in generating high-quality, editable videos with consistent multi-human customization, exceeding baseline methods quantitatively and qualitatively. - The data, code, and model weights are publicly available for research. | ['Text-to-Video', 'Image-to-Video', 'Multimodal'] | [Link](https://github.com/feizc/Ingredients) | [Link](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct), [Link](https://huggingface.co/openai/clip-vit-base-patch32) |
| [Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models](https://arxiv.org/abs/2501.01830) | Weiqiang Wang, Huijia Zhu, Yaojie Lu, Shuhen Zhou, Yanjiang Liu | - AUTO-RT, a reinforcement learning framework, automatically explores and optimizes complex attack strategies to uncover security vulnerabilities in Large Language Models (LLMs) through malicious queries. - It introduces Early-terminated Exploration to accelerate exploration by focusing on high-potential attack strategies and a Progressive Reward Tracking algorithm to dynamically refine the search trajectory towards successful vulnerability exploitation. - It operates in a black-box setting, requiring only access to a model's textual outputs, making it adaptable to diverse LLMs. - Experiments across various LLMs demonstrate that AUTO-RT detects a broader range of vulnerabilities with a faster detection speed and 16.63% higher success rate compared to existing methods. - AUTO-RT improves exploration efficiency and automatically optimizes attack strategies. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/icip-cas/Auto-RT) | N/A |
| [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506) | Yufei Xu, Xuesong Yao, Zhengyin Du, Junjie Ye, maverick1994 | - ToolHop, a new dataset with 995 multi-hop queries and 3,912 associated tools, is introduced to evaluate large language models (LLMs) in multi-hop tool use scenarios. - ToolHop employs a query-driven data construction approach involving tool creation, document refinement, and code generation. - An evaluation of 14 LLMs across five model families (LLaMA, Qwen, Gemini, Claude, and GPT) reveals that even the top-performing model (GPT-4) only achieves 49.04% accuracy. - Analysis suggests that providing LLMs with tools significantly improves their performance, but there are still significant challenges. - Different LLM families exhibit distinct tool-use patterns, with Qwen tending towards parallel calls that result in hallucinations, while GPT leverages tool feedback effectively to improve tool usage. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/bytedance-research/ToolHop) |
