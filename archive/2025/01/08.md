

## Papers for 2025-01-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/abs/2501.03895) | Yang Feng, Zhe Yang, Qingkai Fang, Shaolei Zhang | - LLaVA-Mini is an efficient large multimodal model (LMM) that minimizes the number of vision tokens while maintaining performance comparable to larger models like LLaVA. - It introduces a modality pre-fusion module to combine visual and textual information before compression, enabling the model to use only one vision token per image or video frame. - LLaVA-Mini incorporates a query-based compression module to reduce the number of vision tokens, leading to significant improvements in computational efficiency (77% FLOPs reduction), inference speed (2.92x faster), and memory usage. - Experiments across 11 image and 7 video benchmarks demonstrate LLaVA-Mini's ability to achieve comparable performance to LLaVA with just 1 vision token instead of 576, even on high-resolution images and long videos. - It also allows for processing long videos exceeding 10,000 frames on a 24GB GPU, a significant advancement in efficient real-time multimodal interaction. | ['Multimodal', 'Image-Text-to-Text', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/ictnlp/LLAVA-Mini) | [Link](https://huggingface.co/ICTNLP/1lava-mini-llama-3.1-8b) |
| [Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos](https://arxiv.org/abs/2501.04001) | Shilin Xu, Zilong Huang, Tao Zhang, Xiangtai Li, HarborYuan | - Sa2VA is a novel unified model for dense grounded understanding of images and videos, integrating the Segment Anything Model 2 (SAM-2) with Large Language and Vision Assistant (LLaVA)-like Multimodal Large Language Models (MLLMs). - This architecture unifies text, image, and video data within a shared token space, enabling instruction-guided mask generation by SAM-2, which facilitates grounded multimodal understanding. - Sa2VA supports various tasks such as image and video conversations, referring image/video segmentation, and grounded caption generation through a single-shot instruction-tuning process.  - The model achieves state-of-the-art performance across multiple tasks, including referring video object segmentation, outperforming previous methods by a significant margin on the Ref-SAV dataset (over 15% under zero-shot settings). -  A key contribution is the introduction of Ref-SAV, a new large-scale dataset for referring video segmentation, which consists of more than 72,000 video object expressions.  | ['Multimodal', 'Image Segmentation', 'Visual Question Answering', 'Video Classification', 'Mask Generation'] | N/A | [Link](https://huggingface.co/ByteDance/Sa2VA-4B) |
| [PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides](https://arxiv.org/abs/2501.03936) | Hongyu Lin, Jia Zheng, Hao Kong, Xinyan Guan, Forceless | - PPTAgent, a novel framework, redefines automatic presentation generation as an interactive, edit-based workflow using both a document and reference presentation as inputs. - The two-stage framework first analyzes reference presentations for structural patterns and content through slide clustering and schema extraction, and then generates new slides via code actions, ensuring consistency and alignment through feedback mechanisms. - PPTAgent introduces PPT Eval, a comprehensive evaluation framework for presentation quality across three dimensions: Content, Design, and Coherence, using a multi-dimensional LLM-as-a-judge approach. - Experimental results demonstrate that PPTAgent outperforms current end-to-end text-generation methods, achieving a 97.8% success rate and a 3.67 average PPT Eval score across three dimensions: Content, Design, and Coherence, indicating high-quality presentation generation. - A new presentation dataset Zenodo10K consisting of 10,448 presentations with diverse domains is collected from Zenodo. | ['Multimodal'] | [Link](https://github.com/icip-cas/PPTAgent) | N/A |
| [Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback](https://arxiv.org/abs/2501.03916) | Tao Chen, Botian Shi, Xiangchao Yan, Jiakang Yuan, BoZhang | - DOLPHIN is a closed-loop, open-ended automatic research framework designed to automate the scientific research process, encompassing idea generation, experimental verification, and feedback. - It retrieves and ranks relevant papers based on topic and task attributes, generates novel research ideas guided by these papers, filters them for novelty and independence, and formulates experimental plans using LLMs. - DOLPHIN automatically generates and debugs code using an exception-traceback-guided process and analyzes experimental results to provide feedback for subsequent idea generation rounds. - Experimental results on benchmarks like CIFAR-100, ModelNet40, and SST-2 show that DOLPHIN generates ideas that improve performance over baselines and, in some cases, achieves state-of-the-art results, demonstrating its potential for automated research. -  DOLPHIN automatically generated methods based on PointNet that showed comparable performance to human-designed state-of-the-art 3D classification methods on ModelNet40. | ['Computer Vision', 'Image Classification', 'Zero-Shot Object Detection', 'Text-to-3D', 'Image-to-3D', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
