

## Papers for 2025-01-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives](https://arxiv.org/abs/2501.04003) | ZwwWayne, Chonghao, THUdyh, ldkong, shaoyuanxie | - DriveBench, a new benchmark designed to assess the reliability of Vision-Language Models (VLMs) in autonomous driving, is introduced. - The benchmark comprises 19,200 images, 20,498 question-answer pairs, and covers four driving tasks (perception, prediction, planning, behavior) under 17 settings, including clean, corrupted, and text-only inputs, to evaluate VLM robustness and visual grounding. - Evaluations of 12 popular VLMs reveal that they often generate plausible but fabricated responses based on general knowledge rather than visual cues, especially with missing or degraded visual inputs. - This behavior poses risks in safety-critical scenarios like autonomous driving, and is masked by dataset imbalances and inadequate metrics. - The study emphasizes the need for refined evaluation metrics that focus on multi-modal understanding and robust visual grounding, and highlights the potential of using VLMs' awareness of corruptions to enhance their reliability. | ['Multimodal', 'Visual Question Answering', 'Computer Vision'] | [Link](https://github.com/drive-bench) | [Link](https://huggingface.co/datasets/drive-bench/arena) |
| [Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models](https://arxiv.org/abs/2501.04828) | Ece Elif Adak, tcTHEBESTMAN, fatihburakkaragoz, temretiras, sbozates | - This paper introduces the first Named Entity Recognition (NER) dataset (HisTR) and Universal Dependencies treebank (OTA-BOUN) for historical Turkish, alongside a cleaned text corpus (OTC) and transformer-based models for NER, dependency parsing, and part-of-speech tagging. - HisTR consists of 812 manually annotated sentences, while OTA-BOUN contains 514 sentences annotated with part-of-speech tags and dependency relations.  - The models were trained using BERTurk, mBERT, and TURNA architectures.  - Experimental results show that BERTurk outperforms mBERT in NER and dependency parsing of historical Turkish, and fine-tuning with a combination of modern and historical Turkish data improves performance. - The resources and models are publicly available, establishing a baseline for future research in historical Turkish NLP. | ['Natural Language Processing', 'Token Classification', 'Text Classification', 'Question Answering'] | [Link](https://github.com/UniversalDependencies/UD_Ottoman_Turkish-BOUN/tree/dev), [Link](https://github.com/Ottoman-NLP/ottominer-public) | [Link](https://huggingface.co/datasets/bucolin/HisTR), [Link](https://huggingface.co/datasets/bucolin/OTA-BOUN_UD_Treebank), [Link](https://huggingface.co/datasets/bucolin/OTC-Corpus), [Link](https://huggingface.co/bucolin) |
| [Entropy-Guided Attention for Private LLMs](https://arxiv.org/abs/2501.03489) | Brandon Reagen, nandan523 | - This research introduces an entropy-guided attention mechanism for enhancing the privacy of large language models (LLMs) during inference. - Researchers discovered that removing nonlinearities in LLMs can cause training instability due to entropy collapse in deeper layers and entropic overload in earlier layers. - The study presents a novel entropy regularization technique and proposes PI-friendly alternatives to layer normalization. - Experimental results show that the proposed methods reduce communication overhead by 3.94x and improve inference speed by 1.72x in a private setting. - The work bridges information theory and architectural design, utilizing entropy dynamics to guide the development of efficient privacy-preserving LLM architectures. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Nandan91/entropy-guided-attention-llm) | N/A |
| [Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model](https://arxiv.org/abs/2501.05122) | Radu Timofte, Chris Biemann, Carolin Holtermann, Florian Schneider, Gregor Geigle | - This paper introduces Centurio, a massively multilingual Large Vision-Language Model (LVLM) supporting 100 languages, trained by machine-translating high-quality English data and benchmarked across 13 downstream vision-language tasks covering 43 diverse languages. - The study investigates optimal language distributions of pre-training and instruction-tuning data, finding that including up to 100 languages with as little as 25-50% non-English data improves multilingual performance while maintaining strong English performance.  - The research also introduces a new benchmark, SMPQA (Synthetic Multilingual Plot Question Answering), for evaluating multilingual text-in-image understanding and finds that non-English OCR data in training is crucial for this task.  - Centurio achieves state-of-the-art results on 14 tasks covering 56 languages, matching popular models' performance on English while outperforming them on low-resource languages.  - One limitation is the heavy reliance on machine-translated data and the comparatively small image input resolution which affects performance on text-heavy tasks. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
