

## Papers for 2025-01-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [The Lessons of Developing Process Reward Models in Mathematical Reasoning](https://arxiv.org/abs/2501.07301) | RunjiLin, BeichenZhang, wuyangzhen, chujiezheng, Zhenru | - This paper introduces two new process reward models (PRMs) for mathematical reasoning, Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B, focusing on enhancing the process supervision capabilities. - The study demonstrates that conventional Monte Carlo (MC) estimation for training data synthesis is less effective for training PRMs compared to LLM-as-a-judge or human annotation methods. - Furthermore, they point out the bias of using Best-of-N (BoN) evaluation alone and advocate for incorporating step-level metrics, such as PROCESSBENCH, for a more comprehensive assessment. - A new consensus filtering mechanism which integrates MC estimation with LLM-as-a-judge is proposed to improve both model performance and data efficiency. - The proposed models and training mechanisms significantly improve error identification in mathematical reasoning, exceeding the capabilities of existing open-source models and providing guidelines for future PRM development. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://hf.co/Qwen/Qwen2.5-Math-PRM-7B), [Link](https://hf.co/Qwen/Qwen2.5-Math-PRM-72B) |
| [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425) | Huizhuo Yuan, Yifeng Liu, thughost, zhenqincn, yifAI | - This paper introduces Tensor Product Attention (TPA), a novel attention mechanism designed to reduce memory overhead in large language models (LLMs) during inference. - TPA leverages tensor decomposition to create compact representations of queries, keys, and values, thereby decreasing the size of key-value caches, a major memory consumer in LLMs. - Based on TPA, the authors create a new model architecture, Tensor ProducT ATTenTion Transformer (T6), and show through experiments that it improves performance on language modeling tasks, achieving lower perplexity and higher accuracy on various benchmarks compared to standard transformer models. - TPA's memory efficiency facilitates handling longer sequences under fixed resource constraints, directly addressing a key scalability challenge in current LLMs. - Additionally, TPA is shown to integrate seamlessly with Rotary Position Embedding (RoPE), simplifying its application in existing architectures like LLaMA and Gemma. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/tensorgi/T6) | N/A |
| [$\text{Transformer}^2$: Self-adaptive LLMs](https://arxiv.org/abs/2501.06252) | tyj2022, edoarc, lfsm | - This paper introduces TransformerÂ², a novel self-adaptation framework for Large Language Models (LLMs) that adapts to unseen tasks in real-time. - It employs a two-pass mechanism: first identifying task properties through a dispatch system and then dynamically mixing task-specific "expert" vectors, trained using reinforcement learning. - The "expert" vectors are generated by Singular Value Fine-tuning (SVF), a new parameter-efficient fine-tuning (PEFT) method that modifies singular values within weight matrices. - This method outperforms LoRA, a popular existing PEFT method, with fewer parameters and greater efficiency across tasks and models.  - The approach demonstrates versatility across different LLM architectures and modalities, including vision-language tasks, where it uses knowledge from language tasks to improve performance in visual question answering. | ['Natural Language Processing', 'Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/SakanaAI/self-adaptive-llms) | N/A |
| [VideoAuteur: Towards Long Narrative Video Generation](https://arxiv.org/abs/2501.06173) | Jiepeng Cen, Liangke Gui, Lu Qi, Feng Cheng, lambertxiao | - This paper introduces VideoAuteur, a two-stage auto-regressive pipeline for generating long-form narrative videos, consisting of a long narrative director and a visual-conditioned video generation model. - The long narrative director generates visual embeddings or keyframes along with captions and actions which capture the narrative flow using an interleaved auto-regressive model. - A novel cooking video dataset, CookGen, is created consisting of approximately 200,000 video clips sourced from existing video datasets (YouCook2 and HowTo100M) annotated with captions, actions, and visual states, which allows benchmarking long narrative video generation. - Experiments demonstrate that the generated videos contain improved semantic consistency and visual fidelity compared to existing methods. - Using CLIP embeddings for visual regression outperforms VAE embeddings. | ['Text-to-Video', 'Multimodal'] | [Link](https://videoauteur.github.io) | N/A |
| [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/abs/2501.07572) | zhoudeyu, Runnaning, ZekunXi, wzl0228, callanwu | - This paper introduces WebWalkerQA, a new benchmark designed to evaluate the web traversal capabilities of Large Language Models (LLMs). - WebWalkerQA focuses on information-seeking question-answering tasks that require navigating through website subpages, often involving multiple steps. - The benchmark contains 680 question-answer pairs across over 1373 webpages from diverse domains including conference, organization, education, and games, and is available in both English and Chinese. - A novel multi-agent framework called WebWalker, employing an explorer-critic paradigm, is proposed as a strong baseline for mimicking human-like web navigation and memory management. - Experimental results demonstrate that WebWalkerQA is challenging for LLMs, highlighting the need for better integration of LLMs with web traversal strategies. | ['Question Answering'] | N/A | N/A |
| [O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning](https://arxiv.org/abs/2501.06458) | Gui Geng, Pengfei, alanyoung058, ZhenHuang, zongzi | - This paper explores inference-time scaling in Large Language Models (LLMs) for medical reasoning tasks, including diagnosis and treatment planning. - Experiments on medical benchmarks (MedQA, Medbullets, JAMA Clinical Challenges) reveal that increasing inference time improves performance, with a 6-11% improvement observed using a modest training set of 500 samples. - Task complexity correlates with required reasoning chain length, and the model's differential diagnoses adhere to hypothetico-deductive principles. - The study utilizes a knowledge distillation approach from GPT-series models to enable journey learning during inference. - Findings highlight the potential of combining inference-time scaling and journey learning to improve real-world clinical reasoning in LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/SPIRAL-MED/Ophiuchus) | N/A |
| [MinMo: A Multimodal Large Language Model for Seamless Voice Interaction](https://arxiv.org/abs/2501.06282) | langgz, gaoruize, zhihaodu, Yingda, chenmengzhe | - MinMo, an 8-billion parameter multimodal large language model, is introduced for seamless voice interaction, addressing limitations of prior aligned multimodal models by training on 1.4 million hours of diverse speech data and a broad range of speech tasks. - MinMo achieves state-of-the-art performance in voice comprehension and generation benchmarks, maintains text LLM capabilities, and facilitates full-duplex conversations, outperforming models like Moshi, Freeze-Omni, and GLM-4-Voice across ASR, S2TT, SQA, VSC, SER, and LID tasks (Figure 1). - A novel voice decoder balancing simplicity and performance is proposed, utilizing a streaming Transformer mixing LLM hidden states with speech tokens. - MinMo demonstrates enhanced instruction-following, controlling speech generation with nuances like emotions, dialects, speaking rates, and voice mimicking with 98.4% accuracy. - MinMo supports full-duplex interaction with low latency (100ms for speech-to-text and 600ms theoretical/800ms practical for full-duplex) using a prediction module leveraging the text LLM's semantic understanding. | ['Multimodal', 'Audio', 'Text-to-Speech', 'Automatic Speech Recognition'] | N/A | [Link](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) |
| [SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training](https://arxiv.org/abs/2501.06842) | Zhangyang Wang, Lu Liu, Gaojie Jin, Ziquan Zhu, Tianjin Huang | - This paper introduces SPAM (Spike-Aware Adam with Momentum Reset), a novel optimizer designed to enhance the stability and efficiency of Large Language Model (LLM) training by mitigating the negative impact of gradient spikes. - SPAM incorporates two key innovations: periodic momentum reset and spike-aware gradient clipping to counteract the harmful accumulation of spiked gradients and preserve valuable directional information. - Extensive experiments demonstrate that SPAM surpasses Adam and its variants across various tasks, including LLM pre-training, quantization-aware training, reinforcement learning, and time series forecasting. - SPAM also facilitates memory-efficient training through sparse momentum, outperforming state-of-the-art memory-efficient optimizers like GaLore and Adam-Mini under memory constraints. - The analysis reveals that gradient spikes, often overlooked, coincide with subtle loss bumps during training and can reach up to 1000 times the magnitude of typical gradients, significantly impacting performance across different architectures, model sizes, and datasets. | ['Natural Language Processing', 'Reinforcement Learning', 'Time Series Forecasting'] | [Link](https://github.com/TianjinYellow/SPAM-Optimizer.git) | N/A |
| [BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature](https://arxiv.org/abs/2501.07171) | yeunglevy, yuhuizhang, jnirschl, minwoosun, lozanoe | - This paper introduces BIOMEDICA, a scalable open-source framework for creating a large-scale, deep-learning-ready biomedical image-caption dataset derived from scientific literature, along with a suite of associated CLIP-style models (BMCA-CLIP) pretrained on this data. - The BIOMEDICA dataset contains over 24 million image-text pairs from over 6 million open-access articles with rich metadata and expert annotations, significantly larger and more diverse than existing biomedical vision-language datasets. - BMCA-CLIP models are trained using continual pretraining on the BIOMEDICA dataset via streaming. - Evaluation across 40 standardized biomedical tasks demonstrates state-of-the-art zero-shot performance, with a 6.56% average improvement in classification (up to +29.8% on dermatology and +17.5% on ophthalmology tasks) and superior retrieval performance compared to previous methods, while using 10x less compute. - The authors release the code, dataset, and pretrained models to promote reproducibility and further research in biomedical vision-language modeling. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Zero-Shot Classification', 'Image Feature Extraction'] | [Link](https://github.com/minwoosun/biomedica-etl), [Link](https://github.com/Ale9806/open_clip_with_biomedica) | [Link](https://huggingface.co/BIOMEDICA) |
| [ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning](https://arxiv.org/abs/2501.06590) | Wangchunshu, siruo2, super-dainiu, CamelH, RTT1 | - ChemAgent, a new framework, enhances Large Language Models (LLMs) for chemical reasoning tasks using a dynamic self-updating library, improving performance by up to 46% (GPT-4). - The library compiles sub-tasks and solutions from decomposed chemical tasks, facilitating task decomposition and solution generation for new problems. - ChemAgent integrates three memory types: Planning Memory for high-level strategies, Execution Memory for specific solutions, and Knowledge Memory for fundamental chemistry principles, stored externally for efficient retrieval. - Experimental results on SciBench datasets demonstrate significant improvements over existing methods, including a 46% gain for GPT-4 and a 10% average improvement over StructChem. - The self-updating library system allows continuous refinement of problem-solving strategies and solutions over time, analogous to human learning from past experiences. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/gersteinlab/chemagent) | N/A |
