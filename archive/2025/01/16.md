

## Papers for 2025-01-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents](https://arxiv.org/abs/2501.08828) | Ruiming Tang, Dexun Li, Xin Deik Goh, Yujing Chang, daviddongdong | - This paper introduces MMDocIR, a novel benchmark for multi-modal document retrieval focusing on page-level and layout-level retrieval tasks. - MMDocIR comprises an evaluation set with 313 documents and 1,658 expert-annotated questions and a training set with 6,878 documents and 73,843 automatically annotated questions. - Experimental results demonstrate that visual retrievers outperform text-based methods, and models trained on MMDocIR exhibit superior performance. - The benchmark addresses limitations of existing datasets by focusing on retrieval granularity, offering complete page contexts, and improved question quality. - The benchmark also highlights the effectiveness of VLM-based text representations over OCR for multi-modal document retrieval. | ['Multimodal', 'Document Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/MMDocIR) |
| [Towards Best Practices for Open Datasets for LLM Training](https://arxiv.org/abs/2501.08365) | jending12, ayahbdeir, avi-skowron, stellaathena, stefan-baack | - This paper discusses the challenges and best practices for creating open datasets for LLM training, focusing on sourcing, processing, governing, and releasing data. - It emphasizes the importance of dataset transparency for accountability and innovation in AI, particularly given the increasing criticism of data practices by large AI companies. - The authors recommend prioritizing community resources, providing thorough documentation, adhering to preference signals, and promoting diversity in data sources. - They also outline the need for clear legal frameworks and ethical considerations in data governance and release. - The paper emerged from a convening hosted by Mozilla and EleutherAI and builds on case studies from prominent open datasets. | ['Natural Language Processing'] | [Link](https://github.com/r-three/common-pile) | [Link](https://huggingface.co/datasets/HuggingFaceH4/c-pile) |
| [RepVideo: Rethinking Cross-Layer Representation for Video Generation](https://arxiv.org/abs/2501.08994) | liuziwei7, Ziqi, cszy98, weepiess2383, ChenyangSi | - RepVideo, a novel framework designed for text-to-video generation, enhances video diffusion models by leveraging enriched intermediate representations. - It employs a feature cache module to aggregate features from adjacent transformer layers and a gating mechanism to combine these aggregated features with the original input, improving spatial detail and temporal consistency. - RepVideo addresses the issue of fragmented spatial semantics and reduced temporal coherence in existing transformer-based video diffusion models. - Experimental results on VBench show that RepVideo-2B outperforms the baseline CogVideoX-2B and other state-of-the-art methods in various metrics, including motion smoothness, object class, multiple objects, and spatial relationship. - Both automated and human evaluations demonstrate RepVideo's superiority in generating high-quality videos with enhanced temporal coherence, spatial fidelity, and alignment with text prompts. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework](https://arxiv.org/abs/2501.08809) | Wenjie Zhu, Wei Tan, Wei Yuan, Can Zhang, Sida Tian | - XMusic is a novel framework for generating symbolic music from various prompt types, including images, videos, text, tags, and humming. - The framework consists of two main components: XProjector, which parses prompts into symbolic music elements (emotions, genres, rhythms, notes), and XComposer, which generates music based on these elements and selects high-quality outputs. - XComposer utilizes a Transformer Decoder as its generative model and incorporates a multi-task learning Selector for quality assessment, emotion recognition, and genre recognition. - XMusic is trained on a new large-scale symbolic music dataset, XMIDI, containing over 108,000 MIDI files with detailed emotion and genre annotations. - Both objective and subjective evaluations demonstrate XMusicâ€™s superior performance in terms of music quality and controllability compared to existing methods across various prompt types, including video, text and image conditioned generation. | ['Audio', 'Text-to-Audio', 'Multimodal'] | N/A | N/A |
| [Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding](https://arxiv.org/abs/2501.07783) | douwh, Changyao, favor123, Einsiedler, wzk1015 | - PIIP (Parameter-Inverted Image Pyramid Networks) is proposed as a novel architecture for visual perception and multimodal understanding tasks. - It processes multi-scale images with different sized models: smaller models for higher resolutions and larger models for lower resolutions, which makes it more efficient than traditional image pyramids. - Cross-branch feature interaction and branch merging components allow information exchange and feature fusion between levels for enhanced performance. - PIIP-LLaVA, built on PIIP, adapts the architecture for efficient and effective high-resolution multimodal understanding. - PIIP demonstrates performance improvements of 1-2% with 40-60% less computation on object detection and semantic segmentation tasks, achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K with InternViT-6B. | ['Computer Vision', 'Object Detection', 'Image Segmentation', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/OpenGVLab/PIIP) | N/A |
| [Multimodal LLMs Can Reason about Aesthetics in Zero-Shot](https://arxiv.org/abs/2501.09012) | Vincentchang, Ruixiang | - This paper investigates the ability of Multimodal Large Language Models (MLLMs) to evaluate the aesthetic quality of artworks, focusing on artistic stylization. - It introduces MM-StyleBench, a new large-scale dataset with diverse content and style instances, and develops a method for modeling human aesthetic preferences for benchmarking. - The study reveals a hallucination issue in MLLMs' art evaluation, tied to response subjectivity, and proposes ArtCoT, a prompting method with explicit task decomposition to mitigate this. - ArtCoT enhances MLLMs' reasoning ability, leading to increased alignment with human preferences, by encouraging concrete language and reducing subjective interpretations. - The findings offer insights into MLLMs' application in art evaluation and suggest potential benefits for downstream tasks like style transfer and image generation. | ['Multimodal', 'Computer Vision', 'Image-to-Image'] | [Link](https://github.com/songrise/MLLM4Art) | N/A |
