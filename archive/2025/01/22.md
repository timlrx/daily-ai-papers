

## Papers for 2025-01-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training](https://arxiv.org/abs/2501.11425) | Zhengyin Du, Zhiheng Xi, Junjie-Ye, lovesnowbest, siyuyuan | - This paper introduces Agent-R, a novel iterative self-training framework designed to improve the error correction capabilities of large language model (LLM) agents in interactive environments. - Agent-R leverages Monte Carlo Tree Search (MCTS) to dynamically construct training samples, enabling agents to learn from their mistakes by revising erroneous trajectories. - The framework includes a model-guided critique construction mechanism where the actor model pinpoints the first error in a failed trajectory and splices it with the adjacent correct path, facilitating timely error correction. - Experimental results across three interactive and agentic environments (WebShop, SciWorld, and TextCraft) demonstrate that Agent-R surpasses baseline methods and agents trained on expert trajectories, achieving superior performance (+5.59%). - Agent-R also equips agents with the ability to more effectively identify and correct erroneous actions in real time while avoiding loops, addressing a key limitation of previous methods. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/bytedance/Agent-R) | [Link](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) |
| [MMVU: Measuring Expert-Level Multi-Discipline Video Understanding](https://arxiv.org/abs/2501.12380) | Lujing Xie, Yilun Zhao, Phil-01, entropyhu, freesky | - MMVU, a new expert-level multi-discipline benchmark, is introduced for evaluating foundation models in video understanding, comprising 3,000 expert-annotated questions across 27 subjects in Science, Healthcare, Humanities & Social Sciences, and Engineering. - MMVU emphasizes domain-specific knowledge application and complex reasoning for specialized-domain video analysis, going beyond basic visual perception common in current video benchmarks. - Each MMVU example includes expert annotations from scratch with stringent quality control, enriched by expert-written reasoning rationales and domain knowledge. - In evaluations of 32 prominent models, advanced System-2 models like o1 and Gemini 2.0 Flash Thinking achieved top performance, though still below human expertise. - This work provides valuable insights for enhancing expert-level, knowledge-intensive video understanding in specialized domains. | ['Multimodal', 'Visual Question Answering'] | [Link](github.com/yale-nlp/MMVU) | [Link](huggingface.co/datasets/yale-nlp/MMVU) |
| [Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/abs/2501.11873) | Kaiyue Wen, Bo Zheng, Zeyu Huang, Zihan Qiu, Losin94 | - This paper proposes a global-batch load balancing loss (LBL) strategy for training Mixture-of-Experts (MoE) models, addressing the limitations of the commonly used micro-batch LBL. - The micro-batch LBL enforces load balancing at the sequence level, hindering expert specialization, particularly in domain-specific tasks, whereas the global-batch LBL promotes load balancing at the corpus level, encouraging specialization. - The global-batch LBL involves synchronizing expert selection frequencies across parallel groups, introducing minimal computational overhead. - Experiments on various MoE model sizes (up to 42.8B parameters) trained on up to 400B tokens show that global-batch LBL significantly improves both pre-training perplexity and downstream task performance. - Analysis reveals that global-batch LBL leads to more interpretable expert specialization, aligning routing decisions with the language modeling task. | ['Natural Language Processing'] | N/A | N/A |
| [UI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/abs/2501.12326) | Shihao Liang, Haoming Wang, Junjie Fang, Yining Ye, Yujia Qin | - UI-TARS is a native GUI agent model that perceives screenshots and performs human-like interactions, such as keyboard and mouse operations, outperforming current agent frameworks. - It incorporates enhanced perception through a large-scale dataset of GUI screenshots for context-aware understanding, unified action modeling for multi-step execution across platforms, and system-2 reasoning for deliberate decision-making. - UI-TARS addresses the data bottleneck in end-to-end agent training by automatically collecting, filtering, and refining interaction traces on virtual machines, along with reflection tuning to recover from errors. - In experiments on 10+ GUI agent benchmarks, UI-TARS achieved SOTA performance in perception, grounding, and GUI task execution, surpassing models like GPT-40 and Claude. - Notably, UI-TARS achieved 24.6 on OSWorld (50 steps) and 46.6 on AndroidWorld, exceeding Claude's 22.0 and GPT-40's 34.5, respectively. | ['Multimodal'] | [Link](https://github.com/bytedance/UI-TARS) | N/A |
| [Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks](https://arxiv.org/abs/2501.11733) | Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy, mikewang | - This paper introduces Mobile-Agent-E, a novel hierarchical multi-agent framework for mobile task automation, featuring self-evolution capabilities through learning and applying reusable *Shortcuts* and *Tips* from past experiences. - Mobile-Agent-E consists of a *Manager*, *Perceptor*, *Operator*, *Action Reflector*, and *Notetaker* agents to handle planning, visual perception, action execution, error verification, and information aggregation respectively. - It also proposes Mobile-Eval-E, a new benchmark focusing on complex, long-horizon, multi-app mobile tasks, along with a *Satisfaction Score* metric based on human-written rubrics for evaluating open-ended tasks. - Experimental results on Mobile-Eval-E demonstrate that Mobile-Agent-E outperforms previous state-of-the-art approaches by a significant margin, achieving a 22.1% absolute improvement in Satisfaction Score with GPT-40. - The inclusion of a self-evolution module shows further performance gains and improved efficiency. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/x-plug/MobileAgent) | N/A |
| [Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments](https://arxiv.org/abs/2501.10893) | Tao Yu, Pengcheng Yin, Jinsung Yoon, Ruoxi Sun, Hongjin Su | - LEARN-BY-INTERACT is a data-centric framework designed to enable Large Language Model (LLM) agents to self-adapt to new environments without human annotations by synthesizing trajectories of agent-environment interactions based on documentation. - It constructs instructions by summarizing or abstracting interaction histories (backward construction) and uses these in training-based and training-free in-context learning scenarios with retrieval approaches optimized for agents. - Experiments across coding, web, and desktop environments (SWE-bench, WebArena, OSWorld, Spider2-V) show LEARN-BY-INTERACT improves baseline results, with up to 12.2% for in-context learning with Claude-3.5 and 19.5% for training with Codestral-22B. - Backward construction contributes significantly to performance, improving results by up to 14%. - The framework's agentic retrieval pipeline demonstrates superiority over conventional retrieval-augmented generation. | ['Reinforcement Learning', 'Robotics', 'Natural Language Processing'] | N/A | N/A |
| [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.11223) | Afonso Catarino, Ales Kubicek, Eric Schreiber, Julia Barth, Maciej Besta | - This paper introduces a blueprint for Reasoning Language Models (RLMs), providing a modular framework for their design and analysis. - The blueprint incorporates various reasoning structures, strategies, and training schemes, unifying diverse RLM approaches like MCTS, reinforcement learning, and structured prompting. - A modular implementation, x1, is presented for rapid RLM prototyping and experimentation, along with insights like multi-phase training and the importance of familiar training distributions. - Analysis of existing RLMs like LLaMA-Berry and QwQ demonstrates the blueprint's versatility and unifying potential. - The work aims to democratize advanced reasoning capabilities, fostering innovation and bridging the gap between "rich AI" and "poor AI". | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/spcl/x1) | N/A |
| [Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](https://arxiv.org/abs/2501.12273) | Chuyu Zhang, Mo Li, Taolin Zhang, Maosong Cao, zsytony | - Condor, a two-stage framework for synthetic data generation, enhances Large Language Model (LLM) alignment by leveraging a World Knowledge Tree and self-reflection refinement. - The first stage, Condor Void, uses a knowledge inspiration strategy with the World Knowledge Tree to create diverse questions and initial responses, forming the Dv dataset. - The second stage, Condor Refine, applies a self-reflection mechanism allowing the model to iteratively refine Dv responses based on self-generated critiques, generating a higher-quality DR dataset. - Experiments using various LLMs, including Qwen, InternLM, and Llama, demonstrate that Condor-generated data significantly improves performance on subjective chat benchmarks compared to officially released and RLHF-trained models, even without RLHF incorporated in the Condor training pipeline. - Additional experiments on knowledge-based benchmarks reveal that Condor maintains the models' knowledge QA capabilities while improving conversational ability. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/InternLM/Condor) | [Link](https://hf.co/datasets/internlm/Condor-SFT-20K) |
| [EMO2: End-Effector Guided Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2501.10687) | Liefeng Bo, Bang Zhang, Qi Wang, Siqi Hu, Linrui Tian | - EMO2 is a novel two-stage audio-driven talking head method that generates expressive facial expressions and synchronized hand gestures from a single reference image and audio input. - The first stage uses a motion diffusion model to generate hand poses from audio, leveraging the strong correlation between audio and hand movements. - The second stage employs a diffusion-based model with a ReferenceNet backbone to synthesize video frames, incorporating the generated hand poses to produce realistic facial expressions and body movements, guided by "pixels prior IK". - Experimental results demonstrate that EMO2 outperforms state-of-the-art methods, such as CyberHost and Vlogger, in terms of visual quality, synchronization accuracy, and motion diversity, particularly in generating more vivid and expressive hand motions. - The method addresses the challenge of weak correspondence between audio and full-body gestures by focusing on hand motion generation and leveraging the implicit IK knowledge within 2D generative models. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [MSTS: A Multimodal Safety Test Suite for Vision-Language Models](https://arxiv.org/abs/2501.10057) | Alicia Parrish, Janis Goldzycher, Felix Friedrich, Giuseppe Attanasio, Paul RÃ¶ttger | - This paper introduces MSTS, a Multimodal Safety Test Suite for Vision-Language Models (VLMs). - MSTS comprises 400 unsafe multimodal English-language prompts across 40 fine-grained hazard categories and is designed to test the safety of VLMs in a structured manner. - MSTS test prompts consist of both a textual and visual component designed to be safe individually but unsafe when combined. - Commercial VLMs generally respond safely to MSTS while open VLMs have clear safety issues often responding unsafely or failing to interpret the multimodal input correctly. - This research highlights the need for further research into VLM safety and the importance of multimodal inputs in safety evaluation. | ['Multimodal', 'Image-Text-to-Text'] | [Link](https://github.com/paul-rottger/msts-multimodal-safety) | N/A |
