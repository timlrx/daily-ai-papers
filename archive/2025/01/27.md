

## Papers for 2025-01-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Humanity's Last Exam](https://arxiv.org/abs/2501.14249) | Josephina Hu, Nathaniel Li, Ziwen Han, Alice Gatti, Long Phan | - This paper introduces HUMANITY'S LAST EXAM (HLE), a challenging multi-modal benchmark designed to assess the advanced academic capabilities of large language models (LLMs). - HLE comprises 3,000 multi-modal, multiple-choice, and exact-match questions across various subjects, emphasizing complex mathematics problems, and aims to be the final closed-ended academic benchmark of its kind. - The benchmark creation involved a rigorous process of expert contribution, LLM difficulty checks, and a multi-stage review process to ensure high quality and difficulty. - Initial evaluations demonstrate that state-of-the-art LLMs perform poorly on HLE and exhibit poor calibration, indicating significant room for improvement. - HLE's public release aims to provide a robust tool for researchers and policymakers to evaluate AI progress and inform discussions about AI development and governance. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/abs/2501.13953) | Chunyi Li, Xiangyu Zhao, Zicheng Zhang, KennyUTC, nebulae09 | - This paper introduces a framework for evaluating redundancy in Multimodal Large Language Model (MLLM) benchmarks, addressing the issue of numerous benchmarks with overlapping capabilities. - The framework quantifies redundancy across three key perspectives: dimensions (intra-benchmark), instances (intra-benchmark), and cross-benchmarks within specific domains. - It uses performance correlation to measure redundancy, leveraging data from VLMEvalKit, a comprehensive dataset containing results from diverse benchmarks and over 100 MLLMs. - The study reveals significant redundancy in many existing benchmarks, suggesting opportunities for optimization by reducing redundant dimensions and instances. - The paper concludes with recommendations for constructing more efficient and effective MLLM benchmarks, including guidance on redundancy checks during the benchmark design process. | ['Multimodal'] | N/A | [Link](https://huggingface.co/datasets/VLMEval/OpenVLMRecords) |
| [Chain-of-Retrieval Augmented Generation](https://arxiv.org/abs/2501.14342) | Zhicheng Dou, Xiaolong Huang, Nan Yang, Haonan Chen, Liang Wang | - This paper introduces CoRAG (Chain-of-Retrieval Augmented Generation), a novel approach for training RAG models that retrieve and reason over information step-by-step before generating answers. - Unlike conventional RAG methods that perform a single retrieval step, CoRAG allows dynamic query reformulation based on the model's evolving state, enhancing its effectiveness in handling complex queries. - CoRAG is trained effectively using rejection sampling to generate intermediate retrieval chains, augmenting existing RAG datasets and improving the model's ability to learn effective retrieval strategies. - Experiments across multiple benchmarks show CoRAG outperforming strong baselines, particularly in multi-hop question answering tasks, where it achieves more than 10 points improvement in EM score. - CoRAG establishes a new state-of-the-art performance on the KILT benchmark across various knowledge-intensive tasks, demonstrating its superior ability to handle diverse knowledge-intensive tasks. | ['Question Answering'] | N/A | N/A |
| [RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques](https://arxiv.org/abs/2501.14492) | Ruoyu Sun, Tian Ding, Zhenyang Xiao, Ziniu Li, Zhengyang Tang | - The paper introduces RealCritic, a novel benchmark designed to assess the critique capabilities of large language models (LLMs) in a closed-loop manner. - Unlike existing benchmarks that evaluate critiques in isolation, RealCritic evaluates critique quality based on the effectiveness of the generated corrections. - RealCritic incorporates various critique settings, including self-critique, cross-critique, and iterative critique, providing a comprehensive evaluation. - The benchmark is implemented using eight challenging reasoning tasks across mathematical reasoning and multiple-choice question domains. - Experiments show that reasoning-based models outperform classical LLMs, particularly in self-critique settings, emphasizing the value of a closed-loop evaluation approach. | ['Natural Language Processing'] | [Link](https://github.com/tangzhy/RealCritic) | N/A |
