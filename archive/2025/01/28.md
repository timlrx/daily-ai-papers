

## Papers for 2025-01-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Baichuan-Omni-1.5 Technical Report](https://arxiv.org/abs/2501.15368) | Song Chen, Tao Zhang, Tao Zhang, Jun Liu, AdamLee1 | - Baichuan-Omni-1.5 is an open-source omnimodal model with end-to-end audio generation capabilities, utilizing a visual branch (NaViT based), audio branch (RVQ tokenizer and flow matching decoder), and a pretrained LLM backbone. - The model is trained using a three-stage strategy, focusing on image-text pretraining, image-audio-text pretraining, and multimodal fine-tuning using a 500B token dataset. - It claims to outperform leading open-source omnimodal models like VITA-1.5 and MiniCPM-0 2.6 on various tasks, including image, video, and audio understanding, and even surpasses proprietary models like GPT-40-mini on certain benchmarks. - On MMLU, it achieves 72.2% accuracy, and on OpenMM-Medical, it reaches 83.8% with a 7B LLM, exceeding Qwen2-VL-72B's 80.7%. - An audio tokenizer is designed for semantic and acoustic information capture, supporting controllable bilingual real-time conversations and general multimodal understanding. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Text-to-Audio', 'Text-to-Speech', 'Text-to-Video', 'Visual Question Answering', 'Video Classification'] | [Link](https://github.com/baichuan-inc/Baichuan-Omni-1.5) | N/A |
| [Qwen2.5-1M Technical Report](https://arxiv.org/abs/2501.15383) | Fei Huang, Dayiheng Liu, Chengyuan Li, Bowen Yu, An Yang | - This paper introduces Qwen2.5-1M, a series of large language models (LLMs) that extend the context length to 1 million tokens, including open-source models Qwen2.5-7B/14B-Instruct-1M and an API-accessible model, Qwen2.5-Turbo. - Qwen2.5-1M employs several key techniques to enhance long context capabilities, such as long data synthesis, progressive pretraining, and multi-stage supervised fine-tuning. - It also includes a novel inference framework with training-free length extrapolation and sparse attention to reduce costs. - Evaluation shows significant improvement of Qwen2.5-1M on long-context tasks, sometimes outperforming GPT-4, while retaining comparable performance on short-context tasks to 128k versions. - Qwen2.5-Turbo stands out with faster inference speed and lower cost for long sequences, offering a practical solution for various real-world scenarios requiring extended contexts. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer](https://arxiv.org/abs/2501.15570) | Peter Yue, Li Zhiyuan, Lin Yueyu, xiaol | - This paper introduces ARWKV, a series of RNN-based language models distilled from Qwen 2.5, utilizing pure native RWKV-7 attention. - The architecture aims to enhance the expressiveness of RNNs and demonstrate state-tracking capabilities beyond transformers, replacing the self-attention mechanism in transformers with the RWKV-7 time mixing module. - The distillation process reduces the training time and resource requirements, enabling a 7B parameter model to be trained on a single A100 80G GPU, compared to the vast resources needed for training Qwen 2.5's 18 trillion tokens. - Initial experiments with a 7B model demonstrate competitive performance in benchmarks after stage 2 training.  - The key innovation is to replace transformer's self-attention with RWKV time mixing module | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/yynil/RWKVInside) | [Link](https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1) |
| [Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation](https://arxiv.org/abs/2501.15907) | Yicheng Gu, Xuyuan Li, Chaoren Wang, Zengqiang Shang, Haorui He | - This paper introduces Emilia, a large-scale, multilingual, and diverse dataset for speech generation derived from in-the-wild speech data. - Emilia comprises over 101k hours of speech across six languages (English, Chinese, German, French, Japanese, and Korean) and is expanded to 216k hours with Emilia-Large, making it the largest open-source speech generation dataset available. - The paper also introduces Emilia-Pipe, an open-source preprocessing pipeline used to create the dataset, which standardizes, separates sources, diarizes speakers, segments by VAD, performs ASR, and filters data. - Experiments demonstrated that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous, human-like speech, capturing diverse speaker timbres and speaking styles. - Further experiments validated the importance of scaling dataset size and showcased the effectiveness of Emilia for multilingual and crosslingual speech generation. | ['Audio', 'Text-to-Speech'] | [Link](https://github.com/open-mmlab/Amphion/tree/main/preprocessors/Emilia) | [Link](https://huggingface.co/datasets/amphion/Emilia-Dataset) |
| [Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity](https://arxiv.org/abs/2501.16295) | Luke Zettlemoyer, Ning Dong, Genghan Zhang, Junhong Shen, Weixin Liang | - Mixture-of-Mamba (MoM) introduces modality-aware sparsity to State Space Models (SSMs) by incorporating modality-specific parameterization within the Mamba block, enhancing multi-modal pretraining. - MoM dynamically selects modality-specific weights in each processing component, enabling efficient handling of diverse data types while preserving SSM computational benefits. - Across three multi-modal settings (Transfusion, Chameleon, and a three-modality setup), MoM consistently achieves equivalent or better loss values at significantly reduced computational costs, outperforming dense baselines like Mamba Dense and Flex-Attention Transformer. - In Transfusion, MoM matches image loss with 34.76% of the FLOPs at the 1.4B scale; in Chameleon, it reaches similar image loss with 42.50% and text loss with 65.40% of the FLOPs; and in the three-modality setting, it matches speech loss with just 24.80% of the FLOPs. - An ablation study reveals synergistic effects from decoupling projection components, with joint decoupling yielding larger performance gains than individual modifications, establishing modality-aware sparsity as an effective design principle for both SSMs and Transformers. | ['Multimodal'] | [Link](https://github.com/Weixin-Liang/Mixture-of-Mamba) | N/A |
| [Feasible Learning](https://arxiv.org/abs/2501.14912) | Meraj Hashemizadeh, Jose Gallego-Posada, Juan Elenter, Ignacio Hounie, Juan Ramirez | - This paper introduces Feasible Learning (FL), a sample-centric learning paradigm that trains models by solving a feasibility problem, ensuring bounded loss for each training sample, as opposed to optimizing for average performance like Empirical Risk Minimization (ERM). - The authors use a primal-dual optimization approach, dynamically re-weighting sample importance based on fitting difficulty. - They also introduce Resilient Feasible Learning (RFL) to address potential infeasibility issues by incorporating slack variables. - Empirical analysis on image classification, age regression, and large language model preference optimization shows comparable average performance to ERM but improved tail behavior, with fewer high-loss samples. - The results suggest that RFL's concentrated loss distribution is particularly beneficial when consistent performance across all data points is crucial. | ['Image Classification', 'Natural Language Processing', 'Text2Text Generation', 'Computer Vision'] | [Link](https://github.com/juan43ramirez/feasible-learning) | [Link](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs), [Link](https://huggingface.co/stabilityai/stablelm-zephyr-3b) |
