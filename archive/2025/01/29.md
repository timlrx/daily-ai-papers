

## Papers for 2025-01-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training](https://arxiv.org/abs/2501.17161) | Saining Xie, Shengbang Tong, Jihan Yang, Yuexiang Zhai, Tianzhe Chu | - This paper compares the effects of supervised fine-tuning (SFT) and reinforcement learning (RL) on the generalization and memorization abilities of foundation models. - The authors introduce a new arithmetic reasoning card game called GeneralPoints and utilize the existing V-IRL visual navigation environment to evaluate model performance across textual and visual domains. - Their findings reveal that RL, particularly when trained with outcome-based rewards, generalizes better to unseen variants than SFT, which tends to memorize the training data.  - Interestingly, SFT is found to be helpful in stabilizing the model's output format for subsequent RL training, which allows RL to achieve its performance gains. - The research highlights the advantages of RL in acquiring generalizable knowledge for complex, multimodal tasks. | ['Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2501.17116) | Guoshuai Zhao, Xiao Liu, Yeyun Gong, Ruizhe Wang, cp5555 | - This paper introduces the first FP4 training framework for large language models (LLMs), addressing the challenges of significant quantization errors and limited representational capacity. - Two key innovations are presented: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. - The framework integrates a mixed-precision training scheme and vector-wise quantization to ensure stability. - Experimental results demonstrate that the FP4 framework achieves accuracy comparable to BF16 and FP8, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. - The authors conclude that their framework sets a foundation for efficient ultra-low precision training, particularly with the emergence of next-generation hardware supporting FP4. | ['Natural Language Processing'] | N/A | N/A |
| [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/abs/2501.16975) | Ya Wang, Yutao Zeng, Banggu Wu, Defa Zhu, Hongzhi Huang | - This paper introduces Over-Tokenized Transformers, a framework that improves language modeling performance by decoupling input and output vocabularies and scaling up the input vocabulary with multi-gram tokens. - A log-linear relationship is observed between the input vocabulary size and training loss, showing that larger input vocabularies improve performance regardless of model size. - Using a large input vocabulary, the proposed method matches the performance of a model 2.5x larger (400M model matches 1B model) on the OLMo2 benchmark at no additional training cost. -  Over-encoding (OE) leverages hierarchical n-gram input vocabulary and over-decoding (OD) utilizes a large output vocabulary with multi-token prediction, which can be beneficial for large models. - The integration of OE and OD results in Over-Tokenized Transformer (OT), demonstrating further performance improvements. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496) | Jeff Wu, Jack Lindsey, Joshua Batson, Lee Sharkey, bilalchughtai | - This review paper discusses open problems in mechanistic interpretability, a field aiming to understand the computational mechanisms behind neural networks' abilities, particularly in natural language processing. - The authors categorize open problems into methods and foundations (reverse engineering, concept-based interpretability, proceduralization and automation), applications (monitoring, control, predictions, inference/training, microscope AI, and model/modality ranges), and socio-technical aspects. -  Key challenges include developing better decomposition methods beyond sparse dictionary learning, improving validation techniques, and connecting interpretability research to concrete engineering and scientific goals. - The paper also touches on the potential of mechanistic interpretability in AI governance, including risk assessment, policy development, and addressing social and philosophical implications. - This review focuses specifically on suggesting potential future research directions for the field, rather than simply summarizing the current state of mechanistic interpretability. | ['Natural Language Processing'] | N/A | N/A |
| [IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding](https://arxiv.org/abs/2501.15747) | Nikunj Kotecha, Ashutosh Kumar, Sankalp KJ, amanchadha, laxmaanb | - IndicMMLU-Pro, a comprehensive benchmark for evaluating large language models (LLMs) across nine major Indic languages, is introduced.  - The benchmark covers a wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the complexities of Indic languages. - IndicMMLU-Pro's design principles, task taxonomy, and data collection methodology are detailed, and baseline results from state-of-the-art multilingual models are presented. - The benchmark is publicly available, aiming to contribute to advancements in Indic language-based technologies. - The results reveal that GPT-4 consistently outperforms other models across all Indic languages, highlighting the importance of both scale and specialized training for Indic languages. | ['Natural Language Processing', 'Translation', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/LinguaLift/IndicMMLU-Pro) |
| [Low-Rank Adapters Meet Neural Architecture Search for LLM Compression](https://arxiv.org/abs/2501.16372) | Nilesh Jain, Jinjie Yuan, J. Pablo Mu√±oz | - This paper introduces a novel approach to compressing Large Language Models (LLMs) by combining low-rank adapters with Neural Architecture Search (NAS) techniques. - The method, called LoNAS, uses elastic low-rank adapters that can dynamically adjust their configurations during fine-tuning, improving efficiency. - LoNAS achieves competitive results compared to traditional LoRA, reducing the total number of parameters by up to 20% while maintaining similar accuracy. - Further enhancements, such as Shears and SQFT, build upon LoNAS to address challenges in handling sparse or low-precision models and improve efficiency even further. - The results demonstrate significant improvements in inference speedup (up to 1.4x) and compression, making LLMs more accessible for resource-constrained environments. | ['Natural Language Processing'] | [Link](https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning) | N/A |
| [Histoires Morales: A French Dataset for Assessing Moral Alignment](https://arxiv.org/abs/2501.17117) | Charlotte Laclau, Julien Velcin, Antoine Gourru, Irina Proskurina, Thibaud Leteno | - This paper introduces HISTOIRESMORALES, a new French dataset for evaluating the moral alignment of large language models (LLMs). - The dataset is created by translating and refining the MORALSTORIES dataset, ensuring grammatical accuracy and cultural relevance for the French context. - HISTOIRESMORALES includes annotations of moral values to align with French norms, covering diverse social situations. - Preliminary experiments reveal that while LLMs generally align with human moral norms, their alignment is easily influenced by user preferences, demonstrating a lack of robustness. - The authors demonstrate how their dataset can be used to investigate the alignment of LLMs with human moral norms and the impact of language on this alignment. | ['Natural Language Processing', 'Translation'] | [Link](https://github.com/upunaprosk/histoires-morales) | [Link](https://hf.co/datasets/LabHC/histoires_morales) |
