

## Papers for 2025-01-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate](https://arxiv.org/abs/2501.17703) | Xiang Yue, wenhu, ubowang | - This paper introduces Critique Fine-Tuning (CFT), a novel training strategy for Large Language Models (LLMs) that focuses on learning to critique responses rather than simply imitating correct ones. - CFT leverages the power of advanced LLMs such as GPT-40 to generate critiques for noisy responses, guiding the model towards deeper analysis and understanding. - CFT models consistently outperform SFT models across several benchmarks, demonstrating a 4-10% improvement on mathematical reasoning tasks while using significantly less data (50K samples vs. 2M+). - The effectiveness of CFT is shown across a variety of base models (Qwen2.5, DeepSeek-Math-7B) and datasets, showcasing its potential as an efficient training strategy. - CFT highlights that learning to critique offers a more effective way to train LLMs, promoting critical thinking and nuanced understanding often overlooked by standard SFT methods. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://arxiv.org/abs/2501.14334) | Simon Gosset, Caroline Vateau, Louis Ladan, Neyri56, clementdesroches | - This paper proposes a methodology to estimate the environmental impact of a company's AI portfolio, focusing on energy consumption and its multi-factor impacts like GHG emissions, water usage, and resource depletion. - The study finds that large generative AI models consume significantly more energy (up to 4600x) than traditional models and that energy consumption scales substantially with model size and workflow complexity. - The paper projects AI electricity use up to 2030 under various scenarios, with a potential 24.4x increase in a high-adoption scenario driven by widespread Generative AI and agents. - The authors advocate for standardized environmental assessment frameworks, greater transparency from AI providers, and a "Return on Environment" metric to align AI development with sustainability goals. - The paper explores the feasibility of achieving a 90% GHG reduction by 2030 through improvements in PUE, energy mix decarbonization, and hardware efficiency and proposes an environmental scoring label for AI similar to eco-scores in other industries. | ['Natural Language Processing', 'Other'] | N/A | N/A |
| [Atla Selene Mini: A General Purpose Evaluation Model](https://arxiv.org/abs/2501.17195) | Kyle Dai, Jackson Golden, Henry Broomfield, Andrei Alexandru, NinaCalvi | - Atla Selene Mini is a new small language model as a judge (SLMJ), designed for general purpose evaluation tasks. - It outperforms existing SLMJ and GPT-40-mini on 11 out-of-distribution benchmarks covering absolute scoring, classification, and pairwise preference tasks. - The model leverages a curated dataset augmented with synthetic critiques and a combined Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) training approach. - Selene Mini excels in real-world scenarios, demonstrating strong zero-shot agreement with human expert evaluations on financial and medical datasets and robustness to variations in prompt format.  - It also leads in a live community-driven Judge Arena benchmark, further highlighting its robust evaluation capabilities. | ['Natural Language Processing'] | N/A | [Link](https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) |
| [Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation](https://arxiv.org/abs/2501.17749) | Miriam Ugarte, ssegura, japarejo, pablovalle, aitorarrieta | - This research paper details the pre-deployment safety testing of OpenAI's o3-mini large language model (LLM) using the ASTRAL tool. - ASTRAL automatically generates unsafe test inputs categorized by style and persuasion technique across 14 safety areas. - A total of 10,080 tests were generated and executed, with 87 confirmed instances of unsafe behavior after manual verification. - The o3-mini model demonstrated improved safety compared to older OpenAI models, possibly due to integrated policy violation detection. - Key areas of concern still exist, particularly surrounding recent controversial topics and specific categories like terrorism and child abuse. | ['Natural Language Processing'] | [Link](https://github.com/Trust4AI/ASTRAL) | N/A |
| [Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation](https://arxiv.org/abs/2501.17433) | ling1119, sftekin25, tawreos, SihaoHu, TianshengHuang | - This paper introduces "Virus," a novel attack method targeting Large Language Models (LLMs) that bypasses guardrail moderation systems designed to prevent harmful fine-tuning. - Virus utilizes a dual-objective optimization approach to craft harmful training data, minimizing detectability by the guardrail while maximizing the degradation of the LLM's safety alignment. - Experimental results demonstrate Virus's effectiveness, achieving up to 100% leakage ratio (bypassing the guardrail) and increasing harmful scores by up to 21.8% compared to standard harmful fine-tuning attacks. - The key finding is that relying solely on guardrail moderation is insufficient for mitigating harmful fine-tuning risks, highlighting the need for more robust defense mechanisms. - The optimized datasets generated by Virus are publicly available for further research and analysis. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/git-disl/Virus) | [Link](https://huggingface.co/datasets/anonymous4486/Virus) |
