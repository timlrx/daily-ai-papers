

## Papers for 2025-02-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model](https://arxiv.org/abs/2502.02737) | Gabriel Martín Blázquez, Elie Bakouch, Anton Lozhkov, Loubna Ben Allal, lvwerra | - This paper introduces SmolLM2, a 1.7 billion parameter language model trained on 11 trillion tokens using a multi-stage, data-centric approach. - The model utilizes a Llama2 architecture and is trained with a combination of web text, specialized math, code, and instruction-following data. - New specialized datasets (Fine-Math, Stack-Edu, and SmolTalk) were created to address limitations in existing public datasets. - SmolLM2 outperforms other recent small language models, including Qwen2.5-1.5B and Llama3.2-1B, on various benchmarks including HellaSwag, ARC, and MMLU-Pro. - Both the base and instruction-tuned versions of SmolLM2 are released, along with the new specialized datasets, to facilitate future research. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | N/A | [Link](https://hf.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9), [Link](https://huggingface.co/datasets/HuggingFaceTB/finemath), [Link](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) |
| [Demystifying Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2502.03373) | Xiang Yue, Graham Neubig, Morry Niu, Yuxuan Tong, Edward Yeo | - This paper investigates the mechanics of long chain-of-thought (CoT) reasoning in large language models (LLMs), identifying key factors that enable the generation of extended reasoning trajectories. - Through supervised fine-tuning (SFT) and reinforcement learning (RL) experiments, the study finds that while SFT simplifies training, scaling verifiable reward signals is crucial for RL, and core abilities like error correction are present in base models but require significant compute for complex tasks. - The authors introduce a cosine length-scaling reward with a repetition penalty to stabilize CoT length growth and encourage emergent reasoning behaviors. - Leveraging noisy, web-extracted solutions with filtering shows promise, especially for out-of-distribution reasoning tasks. - The research provides practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/eddycmu/demystify-long-cot) | N/A |
| [LIMO: Less is More for Reasoning](https://arxiv.org/abs/2502.03387) | Shijie Xia, Ethan Chern, Yang Xiao, Zhen Huang, Yixin Ye | - This paper introduces LIMO, a novel approach to mathematical reasoning in large language models (LLMs) that challenges the conventional wisdom that complex reasoning requires massive datasets. - LIMO achieves state-of-the-art performance on several mathematical reasoning benchmarks using only 817 training samples, significantly outperforming models trained on datasets 100 times larger. - The core finding is that sophisticated reasoning capabilities can be elicited with minimal data if the model has a strong pre-trained knowledge foundation and the training data provides effective demonstrations of cognitive processes. - The authors propose the Less-Is-More Reasoning Hypothesis, which posits that the efficiency of eliciting complex reasoning depends on two factors: completeness of pre-trained knowledge and the effectiveness of cognitive templates in the training data. - The study provides empirical evidence and contributes towards a more data-efficient approach to developing advanced reasoning capabilities in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/GAIR-NLP/LIMO) | [Link](https://huggingface.co/datasets/GAIR) |
| [Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking](https://arxiv.org/abs/2502.02339) | Feihu Che, Ruihan Jin, Shuai Zhang, Mingkuan Feng, Jinyang Wu | - This paper introduces AStar, a novel automated structured thinking framework designed to boost multimodal reasoning capabilities in large language models (LLMs) by leveraging Monte Carlo Tree Search (MCTS). - AStar automates the derivation of cognitive reasoning patterns from a small seed dataset (500 samples) using MCTS-powered hierarchical structures. - A unified reasoning framework integrates both internal and external reasoning abilities, enabling efficient inference with minimal tree iterations, and outperforms GPT-40 (50.2%) on the MathVerse benchmark, achieving a 54.0% accuracy score with a 7B backbone. - AStar improves efficiency by reducing inference overhead by 6.4x compared to other tree-based methods and achieves comparable results to training-based methods with 520x less training data. - It demonstrates robustness across different languages and tasks, exhibiting strong generalization capabilities in both in-distribution and out-of-distribution settings. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data) |
| [Jailbreaking with Universal Multi-Prompts](https://arxiv.org/abs/2502.01154) | Shang-Tse Chen, Hsuan Su, Yu-Ling Hsu | - This paper introduces JUMP, a prompt-based method that uses universal multi-prompts to jailbreak LLMs. - JUMP optimizes universal multi-prompts, outperforming existing techniques in experimental results. - The method also generalizes to defense scenarios, which is called DUMP. - JUMP++ is the enhanced version of JUMP, which significantly outperforms JUMP* and current state-of-the-art methods. - The code is publicly available on Github. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ntuaislab/JUMP) | [Link](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B), [Link](https://huggingface.co/openai-community/gpt2-large) |
| [A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods](https://arxiv.org/abs/2502.01618) | Akash Srivastava, Kai Xu, Guangxuan Xu, Shivchander Sudalairaj, ishapuri-mit | - This paper introduces a novel inference-time scaling approach for large language models (LLMs) that frames the task as probabilistic inference rather than a search problem.  - The proposed method uses particle-based Monte Carlo methods, specifically particle filtering, to estimate the typical set of the state distribution, which is more robust to approximation errors in reward models compared to search-based methods.  - Empirical evaluations demonstrate that the proposed method achieves a 4-16x better scaling rate than deterministic search counterparts on challenging mathematical reasoning tasks.  -  The method allows smaller models to surpass the accuracy of larger models, for instance Qwen2.5-Math-1.5B-Instruct surpassing GPT-4 accuracy in only 4 rollouts.  - This approach not only provides an effective method for inference-time scaling but also connects probabilistic inference with LLM inference-time scaling. | ['Natural Language Processing', 'Text Generation'] | [Link](https://probabilistic-inference-scaling.github.io/) | N/A |
| [Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](https://arxiv.org/abs/2502.03275) | Yuandong Tian, Jiantao Jiao, Yingchen Xu, Hanlin Zhu, DiJia Su | - This paper introduces a novel hybrid representation for language model reasoning that mixes latent and text tokens, aiming to improve efficiency and performance. - The method uses a VQ-VAE to generate latent tokens representing reasoning steps, reducing the length of reasoning traces. - Experiments on various benchmarks (including Math, GSM8K, and Keys-Finding Maze) show consistent improvements in accuracy compared to baseline methods, with an average reduction of 17% in reasoning trace length. - A simple training procedure that randomly mixes latent and text tokens allows for quick adaptation to new latent tokens, improving the training efficiency. - The approach is effective for both logical and mathematical reasoning tasks across different model sizes. | ['Natural Language Processing'] | N/A | N/A |
| [On Teacher Hacking in Language Model Distillation](https://arxiv.org/abs/2502.02671) | Nino Vieillard, Sarah Perrin, Johan Ferret, Daniele Calandriello, Daniil Tiapkin | - This paper introduces the concept of "teacher hacking" in language model distillation, where the student model over-optimizes the teacher model instead of the true data distribution. - A novel controlled experimental setup involving an oracle model, teacher model, and student model is proposed to study teacher hacking. - Experiments show that teacher hacking occurs when using a fixed offline dataset for distillation, and that this can be detected by deviations from polynomial convergence laws. - Online data generation techniques effectively mitigate teacher hacking, highlighting data diversity as a key factor. - The findings offer insights into the benefits and limitations of distillation for building robust and efficient language models. | ['Natural Language Processing'] | N/A | N/A |
