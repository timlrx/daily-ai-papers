

## Papers for 2025-02-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [VideoRoPE: What Makes for Good Video Rotary Position Embedding?](https://arxiv.org/abs/2502.05173) | Pan Zhang, Xiaoyi Dong, Xilin Wei, yuhangzang, LiuXR | - VideoRoPE, a novel video position embedding strategy designed for video large language models (LLMs), is introduced, improving the encoding of spatiotemporal information in videos. - It addresses four key properties for effective video position encoding: 2D/3D structure, frequency allocation, spatial symmetry, and temporal index scaling. - VideoRoPE utilizes a low-frequency temporal allocation to mitigate periodic oscillation issues, diagonal layout to preserve spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. - It surpasses existing RoPE variants on tasks like long video retrieval (+12.4 on V-NIAH, +12.4 on V-NIAH-D), video understanding (+2.9 on LongVideoBench, +4.5 on MLVU, +1.7 on Video-MME), and video hallucination (+11.9 on VideoHallucer). - A new challenging benchmark, V-NIAH-D (Visual Needle-In-A-Haystack with Distractors), is introduced to evaluate the robustness of position embedding designs against distractors. | ['Multimodal', 'Video-Text-to-Text', 'Video Classification', 'Video-Text-to-Text'] | [Link](https://github.com/Wiselnn570/VideoROPE) | N/A |
| [QuEST: Stable Training of LLMs with 1-Bit Weights and Activations](https://arxiv.org/abs/2502.05003) | Jiale Chen, d-alistarh, mnikdan97, soroushtabesh, BlackSamorez | - QuEST, a new quantization-aware training (QAT) method, achieves stable training of large language models (LLMs) with weights and activations quantized down to 1-bit. - QuEST improves upon existing QAT methods by using Hadamard normalization and MSE-optimal fitting for accurate and fast quantization, and a new trust gradient estimator for minimizing the error between quantized and full-precision gradients. - When data and compute are scaled proportionally to model size, QuEST trains models with 4-bit weights and activations that achieve superior accuracy compared to BF16 models almost 4x larger. - The method demonstrates stable scaling laws across a range of hardware-supported precisions and model sizes, and can be extended to sparse representations. - GPU kernel support shows that QuEST-trained models can be executed efficiently on commodity hardware. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/IST-DASLab/QUEST) | N/A |
| [DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails](https://arxiv.org/abs/2502.05163) | Bo Li, Wei Wang, Junkai Zhang, Yu Yang, ydeng9 | - DuoGuard, a novel two-player Reinforcement Learning framework, is proposed for training multilingual Large Language Model (LLM) guardrails to enhance safety, particularly for under-resourced languages. - A generator and guardrail classifier co-evolve adversarially within the framework to produce synthetic training data, guided by theoretical analysis proving convergence to a Nash equilibrium. - Empirical evaluations demonstrate DuoGuard's superior performance, achieving a 10% improvement over the 8B parameter LlamaGuard3 model on English benchmarks while being 4.5x faster at inference with a smaller 0.5B model. - DuoGuard also addresses the language imbalance issue by generating synthetic data for lower-resource languages, showing substantial advancements in multilingual safety tasks and outperforming existing guardrails by over 20% on average for similarly sized models. - Ablation studies confirm the significant role of synthetic data generation in bridging the data gap between English and other languages, promoting safer and more responsible use of LLMs across different linguistic contexts. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/yihedeng9/DuoGuard) | N/A |
| [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171) | Siddharth Singh, John Kirchenbauer, Neel Jain, Sean McLeish, Jonas Geiping | - This paper introduces a novel language model architecture that scales test-time computation through latent reasoning using a recurrent block, enabling arbitrary depth at test time. - The model, trained on a massive dataset of 800 billion tokens with 3.5 billion parameters, iteratively refines its understanding in a latent space, contrasting with chain-of-thought prompting which scales by generating more tokens. - Results demonstrate improved performance on reasoning benchmarks compared to open-source models with similar training data and more parameters, reaching computational loads comparable to a 50 billion parameter model. - This recurrent depth approach naturally enables features like adaptive compute, speculative decoding, and KV-cache sharing at inference time. - Analysis of token trajectories in latent space reveal emergent computational behaviors, such as orbiting patterns for numerical reasoning, suggesting the model learns to use its latent space in novel ways. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/seal-rg/recurrent-pretraining) | [Link](huggingface.co/tomg-group-umd/huginn-0125) |
| [Linear Correlation in LM's Compositional Generalization and Hallucination](https://arxiv.org/abs/2502.04520) | Chengyu Dong, Shibo Hao, Chenyang An, Letian Peng, shangjingbo | - This paper uncovers the phenomenon of linear correlations in next-token prediction (NTP) logits from language models (LMs) during knowledge composition, showing that related knowledge pairs (e.g., "X lives in the city of" and "X lives in the country of") exhibit linear transformations in their logits. - The study finds that this linear transformation is resilient to large-scale fine-tuning and generalizes updated knowledge when aligned with real-world relationships but causes hallucinations when deviated. - Empirical results suggest linear correlation can serve as a potential identifier of LM generalization. - A simple feedforward network with pre-trained vocabulary representations can learn these linear correlations, indicating LM generalization heavily relies on these representations. - This paper investigates the generalization mechanism behind LMs and how it relates to knowledge composition, offering insights into compositional generalization and hallucination. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/KomeijiForce/LinCorr) | N/A |
| [Generating Symbolic World Models via Test-time Scaling of Large Language Models](https://arxiv.org/abs/2502.04728) | Fuxiang Frank Xia, Tim Z. Xiao, Yuhuan Yuan, Zhouliang Yu, zhangysk | - This paper introduces a novel test-time scaling approach for generating Planning Domain Definition Language (PDDL) world models using Large Language Models (LLMs), enhancing their planning capabilities without requiring model fine-tuning. - The method employs a two-stage process: Best-of-N (BoN) sampling to generate diverse initial PDDL solutions and instance Verbalized Machine Learning (iVML) to iteratively refine these solutions based on critiques from an optimizer LLM. - This approach achieves state-of-the-art performance on PDDL domain generation tasks, surpassing OpenAI's models, with an 85.2% success rate on NL2Domain and 71.4% on Prob2Domain using Qwen2.5-Coder-7B. - By using PDDL as an intermediate abstraction, the method enables more robust planning and mitigates LLM hallucinations, successfully handling complex planning scenarios where direct LLM-based planners fail. - The combination of BoN and iVML balances exploration and exploitation, enabling faster convergence and higher-quality PDDL domain generation compared to using either method alone. | ['Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference](https://arxiv.org/abs/2502.04416) | Wulong Liu, Xianzhi Yu, Hui-Ling Zhen, Lancheng Zou, Eleven-P | - CMoE is a novel framework designed to transform pre-trained dense Large Language Models (LLMs) into Mixture-of-Experts (MoE) models, enhancing inference efficiency without extensive retraining or resource demands. - This is achieved by "carving" experts from the dense model's Feed-Forward Network (FFN) layers, categorizing neurons into shared (always-on) and routed (conditionally activated) based on activation patterns. - CMoE's routing mechanism is analytically derived from activation statistics rather than trained, and incorporates load balancing and differentiability for performance.  - Using a 7B parameter dense model, CMoE creates a usable MoE model in approximately five minutes, and achieves recovery of dense model performance through lightweight fine-tuning within one hour using a modest dataset (2048 samples). - Experimental results demonstrate that CMoE maintains comparable perplexity to the original dense models, achieving a perplexity as low as 12.73 on WikiText-2 and 32.37 on C4 after fine-tuning with a sparsity of 75% and outperforms the baseline method LLaMA-MoE on various downstream tasks both with and without fine-tuning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/JarvisPei/CMoE) | N/A |
| [Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models](https://arxiv.org/abs/2502.04404) | Jie-Jing Shao, Ding-Chu Zhang, Wen-Da Wei, Xuan-Yi Zhu, yangxw | - This paper introduces Self-Backtracking, a novel technique to enhance the reasoning capabilities of Large Language Models (LLMs) by enabling them to learn when and where to backtrack during both training and inference. - The method addresses limitations of existing slow-thinking models, such as inefficient overthinking and over-reliance on external reward models, by internalizing the search process within the LLM. - Self-Backtracking facilitates dynamic search and expert iteration for self-improvement, transforming slow-thinking processes into fast thinking. - Empirical evaluations on the Countdown task demonstrate a performance improvement of over 40% compared to the optimal-path supervised fine-tuning method. - The proposed technique enhances reasoning flexibility and exhibits test-time scaling capabilities, contributing to the development of more advanced and robust reasoning LLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/LAMDASZ-ML/Self-Backtracking) | N/A |
| [QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05178) | Yuke Zhu, Linxi Fan, Scott Reed, Fuzhao Xue, zhaoyue-zephyrus | - QLIP (Quantized Language-Image Pre-training) introduces a novel visual tokenization method combining high-quality reconstruction with strong zero-shot image understanding by training a Binary Spherical Quantization (BSQ)-based autoencoder with both reconstruction and language-image alignment objectives. - A two-stage training approach addresses the conflicting demands of large-batch contrastive learning and memory-intensive reconstruction, first prioritizing semantic representation learning and then refining visual quality. - QLIP achieves competitive reconstruction quality compared to other state-of-the-art visual tokenizers, while maintaining comparable zero-shot image classification accuracy to CLIP. - QLIP's visual tokens can be integrated with LLMs for visual question answering, achieving performance comparable to CLIP-based methods, and also used for high-quality text-conditioned image generation with improved FID scores compared to other visual tokenizers. - QLIP further enables UMÂ³, a unified mixed-modality auto-regressive model, capable of handling language-only, image-to-text, and text-to-image tasks within a single model. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering', 'Zero-Shot Image Classification', 'Image Feature Extraction'] | N/A | [Link](https://huggingface.co/datasets/guangyil/laion-coco-aesthetic) |
| [CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance](https://arxiv.org/abs/2502.04350) | Chuchu Fan, Yang Zhang, Yueying Liu, Yilun Hao, Yongchao Chen | - CodeSteer, a novel framework to augment Large Language Models (LLMs) with symbolic computing capabilities by effectively guiding code/text generation. - Introduces SymBench, a comprehensive benchmark with 37 symbolic tasks of varying complexity, along with 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. - Employs a fine-tuned Llama-3-8B model as an assistant (CodeSteerLLM) to steer larger models like GPT-40 through multiple rounds of interaction using newly designed multi-round Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). - Incorporates symbolic and self-answer checkers to improve code quality and answer verification. - Demonstrates significant performance improvements on SymBench, boosting GPT-40's score from 53.3 to 86.4, outperforming leading models like OpenAI 01 (82.7) and DeepSeek R1 (76.8), with strong generalizability to other LLMs like Claude, Mistral, and GPT-3.5. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/yongchao98/CodeSteer-v1.0) | N/A |
| [ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning](https://arxiv.org/abs/2502.04689) | Giuseppe Carenini, yuweiyin | - This paper introduces ARR, a novel zero-shot prompting method designed to enhance the question-answering capabilities of Large Language Models (LLMs). - ARR guides LLMs through three key stages: analyzing the question's intent, retrieving pertinent information, and conducting step-by-step reasoning. - Through comprehensive evaluations across ten diverse multiple-choice question-answering datasets, ARR consistently demonstrates superior performance compared to baseline methods and existing Chain-of-Thought (CoT) prompting techniques. - Ablation studies confirm that each component of ARR contributes positively, with intent analysis notably enhancing performance. - Further experiments validate the generalizability of ARR across various model sizes, LLM architectures, generation settings, and few-shot learning scenarios, showcasing its robustness and adaptability. | ['Question Answering'] | [Link](https://github.com/YuweiYin/ARR) | N/A |
| [MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf](https://arxiv.org/abs/2502.04376) | Qingwei Lin, Jue Zhang, Xiaoting Qin, Shurun Yuan, Lingxiang Hu | - This paper introduces an LLM-powered meeting delegate system designed to represent users in meetings, generating relevant spoken content based on real-time meeting transcripts. - A prototype system was developed, focusing on the role of a participant rather than a facilitator, addressing challenges like navigating context-rich conversations and handling ambiguities in human speech. - A new benchmark dataset was created from real meeting transcripts, covering common scenarios like explicit cues, implicit cues, chiming in, and remaining silent. - Popular LLMs were evaluated on the benchmark, revealing that GPT-4/40 maintained balanced performance, Gemini 1.5 Pro was cautious, and Gemini 1.5 Flash and Llama3-8B/70B were more active.  - Overall, approximately 60% of generated responses included at least one key point from the ground truth, demonstrating potential but also highlighting the need for improvements in handling transcription errors and reducing irrelevant content. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
