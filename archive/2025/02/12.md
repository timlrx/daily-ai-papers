

## Papers for 2025-02-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Competitive Programming with Large Reasoning Models](https://arxiv.org/abs/2502.06807) | Borys Minaev, Andre Saraiva, Alexander Wei, Ahmed El-Kishky, OpenAI | - This research demonstrates that reinforcement learning significantly improves the performance of large language models (LLMs) on complex coding tasks, especially in competitive programming. - The study compares three OpenAI models: o1, a general-purpose reasoning model; o1-ioi, a specialized version fine-tuned for the International Olympiad in Informatics (IOI); and an early version of o3, a more advanced reasoning model. - o1-ioi achieved a gold medal at IOI 2024 using handcrafted test-time strategies, showcasing the benefit of specialized training and inference techniques. - Notably, o3 surpassed o1-ioi's performance without relying on specialized strategies, achieving a CODEFORCES rating comparable to elite human competitors. - This indicates that scaling general-purpose reinforcement learning is a promising direction for developing AI capable of state-of-the-art performance in reasoning domains. | ['Reinforcement Learning', 'Natural Language Processing', 'Text2Text Generation'] | N/A | N/A |
| [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316) | Yu Wu, Runxin Xu, Dejian Yang, Daya Guo, Junlong Li | - CODEI/O is a novel approach that enhances the reasoning capabilities of Large Language Models (LLMs) by transforming code into a code input-output prediction format and training the models to predict inputs or outputs given the code. - By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, they are exposed to universal reasoning primitives, decoupling structured reasoning from code-specific syntax while preserving procedural rigor. - The model is trained in two stages: a code input-output prediction training stage followed by a general instruction-tuning stage, which improves average scores across reasoning benchmarks including DROP, WinoGrande, GSM8K, MATH, MMLU-STEM, BBH, GPQA, CruxEval, ZebraGrid, KorBench, and LiveBench.  - CODEI/O outperforms other strong datasets like OpenMathInstruct2, OpenCoder-SFT-Stage1, WebInstruct, and PythonEdu.  -  Further improvements were observed with CODEI/O++ using multi-turn revisions based on execution feedback, boosting performance without trade-offs across models with parameter sizes ranging from 7B to 30B. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/hkust-nlp/CodeIO) | N/A |
| [Teaching Language Models to Critique via Reinforcement Learning](https://arxiv.org/abs/2502.03492) | Jingjing Xu, Weichao Mao, Liyu Chen, Jie chen, Zhihui | - CTRL, a novel framework, is introduced to train Large Language Model (LLM) critics for code generation and refinement through reinforcement learning. - The framework decouples the critic model from the task-performing model and trains it to generate feedback that maximizes the generator's correction performance without human supervision. - Evaluation on diverse benchmarks like CodeContests, LiveCodeBench, MBPP+, and JudgeBench demonstrates that critics trained with CTRL enhance pass rates and mitigate errors across various generator models, showing substantial improvements compared to self-critique methods and those using stronger critic models. - The critic demonstrates an effective weak-to-strong generalization capability by using weaker critic models to improve solutions generated by stronger LLM models such as GPT-40. - CTRL’s critic enables effective test-time scaling by providing targeted feedback and significantly reducing the number of required revision iterations, as shown by up to 106.1% relative improvements across challenging code generation benchmarks. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Expect the Unexpected: FailSafe Long Context QA for Finance](https://arxiv.org/abs/2502.06329) | Mateusz Russak, Dmytro Mozolevskyi, Melisa Russak, muayad, kiranr | - This paper introduces FailSafeQA, a new long-context financial benchmark designed to evaluate the robustness and context-awareness of Large Language Models (LLMs) in question-answering systems. - The benchmark focuses on two scenarios: Query Failure, where the input query is perturbed with spelling errors, incompleteness, and out-of-domain phrasing, and Context Failure, where the input document is degraded, irrelevant, or missing. - It employs an LLM-as-a-Judge methodology using Qwen2.5-72B-Instruct and evaluates 24 off-the-shelf LLMs based on Robustness, Context Grounding, and Compliance scores. - Initial findings reveal a trade-off between robustness and context grounding, with even high-performing models struggling to maintain accuracy under perturbed conditions; for instance, the most robust model, OpenAI 03-mini, fabricates information in 41% of cases. - FailSafeQA serves as a valuable tool for improving LLM dependability in financial applications, where reliable information processing is critical, especially in long-context scenarios. | ['Question Answering', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/Writer/FailSafeQA) |
| [Scaling Pre-training to One Hundred Billion Data for Vision Language Models](https://arxiv.org/abs/2502.07617) | Keran Rong, Zhe Li, Daniel Salz, Ibrahim Alabdulmohsin, Xiao Wang | - This paper investigates the impact of scaling pre-training data to 100 billion image-text pairs on vision-language models (VLMs). - While traditional benchmarks show performance saturation, significant gains are observed in cultural diversity tasks and low-resource language performance, demonstrating the importance of scale for building inclusive models. - Quality filtering, while improving performance on Western-centric tasks, is shown to negatively impact cultural diversity. - The paper introduces WebLI-100B, a novel dataset with 100 billion image-text pairs, and trains SigLIP models with varying sizes on different data scales. - It also shows that rebalancing low-resource languages in the dataset leads to improved performance on corresponding benchmarks and overall multilingual performance. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification', 'Computer Vision'] | N/A | N/A |
| [LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!](https://arxiv.org/abs/2502.07374) | Xiangxi Mo, Shu Liu, Tyler Griggs, Shiyi Cao, Dacheng Li | - This paper demonstrates that Large Language Models (LLMs) can effectively learn long chain-of-thought (Long CoT) reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). - Using only 17k training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on various math and coding benchmarks, including a 40% improvement on AIME 2024 and 8.1% on Live-CodeBench, making it competitive with proprietary models. - The structure of Long CoT reasoning is crucial for effective learning, while the specific content is less important; the model exhibits robustness to incorrect answers or perturbed numerical values in training, yet structural changes such as shuffling reasoning steps significantly impact accuracy. - Through controlled experiments modifying reasoning structures and contents, the model demonstrates its sensitivity to the logical flow of the reasoning process. - This research highlights the potential of distilling reasoning abilities in LLMs efficiently and emphasizes key factors for training future reasoning models. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/NovaSky-AI/SkyThought) | [Link](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k), [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-aime), [Link](https://huggingface.co/datasets/AI-MO/aimo-validation-amc) |
| [Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents](https://arxiv.org/abs/2502.04223) | Lukas Voegtle, Ilia Karmanov, jseppanen, katerynaCh, amalad | - ÉCLAIR, a multimodal large language model (MLLM) with a ViT-like encoder and autoregressive decoder, extracts formatted text, bounding boxes with semantic classes, and reading order from documents. - It addresses limitations of existing models like Kosmos-2.5 and GOT by predicting spatial information, semantic classes, and handling complex layouts. - A novel data generation pipeline was created to construct arXiv-5M, a dataset with maximum-information labels, and the model is fine-tuned on diverse datasets like DocLayNet, SynthTabNet, and a human-labeled Common Crawl subset. - ÉCLAIR achieves state-of-the-art accuracy on the new benchmark DROBS and shows competitive results on existing benchmarks for general OCR, layout understanding, and LLM training data extraction. - Multi-token inference significantly improves ÉCLAIR's inference speed while maintaining or enhancing accuracy compared to single-token decoding. | ['Image-to-Text', 'Multimodal'] | N/A | N/A |
| [CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing](https://arxiv.org/abs/2502.03997) | Jiang Bian, Qi Liu, Yu Yuan, ShizhaoSun | - CAD-Editor is the first framework for text-based CAD editing, allowing users to modify existing CAD models using textual instructions. - It utilizes a sequence-to-sequence approach, representing both instructions and CAD models as text sequences. - To overcome the lack of training data, CAD-Editor employs an automated data synthesis pipeline, combining design variation models with Large Vision-Language Models (LVLMs). - It features a locate-then-infill framework, where Large Language Models (LLMs) first identify regions needing modification and then generate appropriate edits, effectively handling the composite nature of the task. - Experimental results demonstrate that CAD-Editor achieves superior performance over baselines in terms of generated CAD model validity, alignment with editing instructions, and overall quality. | ['Text2Text Generation', 'Multimodal'] | N/A | N/A |
| [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/abs/2502.07527) | Chuan Cao, Liang He, Shufang Xie, Peiran Jin, Yingce Xia | - NatureLM is a sequence-based science foundation model designed for scientific discovery, trained on 143 billion tokens from various scientific domains using a transformer decoder architecture. - NatureLM excels at generating and optimizing molecules, proteins, RNA, and materials using text instructions; performing cross-domain generation (e.g., protein-to-molecule); and achieving state-of-the-art performance on tasks like retrosynthesis and SMILES-to-IUPAC translation. - Evaluation across 22 task categories reveals that larger NatureLM models generally outperform smaller ones, with the 46.7 billion parameter 8x7B model showing the best performance in most cases. - NatureLM demonstrates the potential of large foundation models for scientific discovery by integrating knowledge from multiple domains. - The researchers plan to further enhance the model's language capabilities and few-shot learning skills in future iterations. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Translation'] | N/A | N/A |
| [Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training](https://arxiv.org/abs/2502.06589) | Kewei Cheng, Xin Liu, Haoming Jiang, Jingfeng Yang, yczhuang | - This paper introduces Hephaestus, a continual pre-trained large language model (LLM) designed to improve the fundamental agent capabilities of LLMs, including API function calling, intrinsic reasoning and planning, and adaptation to environmental feedback. - It also introduces Hephaestus-Forge, a large-scale pre-training corpus created to enhance the abilities of LLM agents by combining diverse data sources like tool documentation, function-calling trajectories, and code examples. - The training process includes two stages of continual pre-training followed by instruction fine-tuning. - The paper uses scaling law experiments to determine the optimal data mixing ratio among agent, text, and code data, finding a 1:1:1 ratio to be most effective. - The authors claim Hephaestus-8B outperforms open-source LLMs and rivals commercial LLMs across several agent benchmarks, suggesting effectiveness at enhancing fundamental agent capabilities and generalizability. | ['Natural Language Processing'] | N/A | N/A |
| [Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon](https://arxiv.org/abs/2502.07445) | Seffi Cohen, Lior Rokach, Bracha Shapira, Yehonatan Elisha, Nurit Cohen-Inger | - This paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework to assess the overfitting of Large Language Models (LLMs) to benchmark datasets. - C-BOD systematically distorts evaluation prompts while preserving semantic content and labels using a distortion parameter (µ), and compares model performance on original and perturbed versions to detect overfitting. - Evaluated on MMLU benchmark with 26 LLMs, C-BOD revealed average performance degradation of 2.15% under modest perturbation (μ = 1.0), with 20 of 26 models showing statistically significant performance drops. - Models with higher baseline accuracy and larger LLMs were more sensitive to rephrasing, suggesting overreliance on prompt patterns. - This method provides a dataset and model-agnostic tool to detect and mitigate overfitting for more robust language understanding evaluation. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/SeffiCohen/CBOD) | N/A |
| [FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks](https://arxiv.org/abs/2502.04465) | Mirco Ravanelli, Cem Subakan, Francesco Paissan, lucadellalib | - FocalCodec, a novel low-bitrate hybrid speech codec based on focal modulation, compresses speech into a single binary codebook using a compressor-quantizer-decompressor architecture. - It operates at ultra-low bitrates (0.16 to 0.65 kbps), outperforming current state-of-the-art models in terms of reconstruction quality, voice conversion, and multilingual and noisy speech handling. - Evaluation on various downstream tasks showed that FocalCodec preserves sufficient semantic and acoustic information while being suitable for generative modeling. - Notably, it achieved the lowest differential word error rate (dWER) in both clean and noisy speech resynthesis benchmarks, surpassing state-of-the-art models like BigCodec. - FocalCodec demonstrated effective disentanglement of content and speaker information for voice conversion even with a single codebook design. | ['Audio', 'Text-to-Speech', 'Audio-to-Audio'] | N/A | [Link](https://huggingface.co/microsoft/wavlm-base-sv), [Link](https://huggingface.co/openai/whisper-small) |
| [Auditing Prompt Caching in Language Model APIs](https://arxiv.org/abs/2502.07776) | Percy Liang, Rohith Kuditipudi, Xiang Lisa Li, Chenchen Gu, thashim | - This paper introduces a novel auditing technique to detect prompt caching in Large Language Model (LLM) APIs by analyzing data-dependent timing variations. - Cached prompts are processed faster (cache hits) than non-cached prompts (cache misses), creating timing differences exploitable by attackers to infer information about other users' prompts if the cache is shared. - The audit employs statistical hypothesis testing with procedures designed to generate cache hits and misses, comparing the resulting time distributions. - Audits conducted on 17 real-world LLM API providers, including OpenAI, revealed global cache sharing across users in 7 providers, raising privacy concerns. - The study also demonstrates the potential leakage of model architecture information, specifically finding evidence that OpenAI's text-embedding-3-small model has a decoder-only Transformer architecture. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/chenchenygu/auditing-prompt-caching) | N/A |
| [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490) | Li Shen, Zhenyu Zhang, Jianjin Li, Zhikai Jia, Xialie Zhuang | - This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a novel training paradigm for Large Language Models (LLMs) that integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP). - MEAP randomly masks a fraction of input tokens before applying standard next-token prediction using a decoder-only transformer, eliminating the need for bidirectional attention or encoder-decoder architectures. - Experimental results demonstrate that MEAP significantly outperforms NTP on key information retrieval and long-context reasoning tasks, showing a 33% improvement on Needle in a Haystack and up to 27.2 percentage points improvement on Multi-Document Question Answering. It also excels in the Lost-In-The-Middle Question Answering setting.  - Analysis suggests that MEAP's effectiveness comes from its ability to promote more distinguishable attention scores by concentrating on fewer tokens, leading to better focus on task-relevant signals. - MEAP retains the scaling efficiency of decoder-only LLMs and is compatible with existing LLM pipelines and hardware. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
