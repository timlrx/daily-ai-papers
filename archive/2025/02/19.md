

## Papers for 2025-02-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Soundwave: Less is More for Speech-Text Alignment in LLMs](https://arxiv.org/abs/2502.12900) | Benyou, PhoenixAxis, FanBuCUHK, puccho, Yoohao |  - This paper introduces Soundwave, a novel two-stage training framework for speech-text alignment in LLMs that addresses the representation space gap and sequence length inconsistency issues. - Soundwave employs an efficient training strategy using an alignment adapter and a shrinking adapter to bridge the gap between speech and text representations. - Experimental results on speech translation and AIR-Bench speech tasks demonstrate that Soundwave outperforms the advanced Qwen2-Audio model while using significantly less training data (one-fiftieth). - Further analysis reveals that Soundwave maintains its conversational intelligence. - The project's code is available on GitHub. | ['Audio', 'Automatic Speech Recognition', 'Audio Classification'] | [Link](https://github.com/FreedomIntelligence/Soundwave) | N/A |
| [Continuous Diffusion Model for Language Modeling](https://arxiv.org/abs/2502.11564) | Sung Ju Hwang, harryjo97 | - This paper proposes Riemannian Diffusion Language Model (RDLM), a continuous diffusion model for language modeling that leverages the geometry of the underlying categorical distribution. - The model establishes a connection between discrete diffusion and continuous flow on the statistical manifold, and introduces a simple design for the diffusion process that generalizes previous discrete diffusion models. - It also proposes a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. - Experimental results on language modeling benchmarks (Text8 and One Billion Words Dataset), image modeling (CIFAR-10) and biological sequence design show that RDLM outperforms existing discrete diffusion models and approaches the performance of autoregressive models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/harryjo97/RDLM) | N/A |
| [Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity](https://arxiv.org/abs/2502.13063) | Aydar Bulatov, Mikhail Arkhipov, mbur, yurakuratov | - This paper explores the capacity of input embeddings in Large Language Models (LLMs) to compress and reconstruct long text sequences. - The authors introduce a method to compress text into trainable "memory" vectors using a per-sample optimization procedure, demonstrating compression ratios up to x1500, significantly exceeding previous methods. - They establish a connection between the latent capacity of input vectors and text cross-entropy, providing a quantifiable measure of information encoding. - The study shows that capacity limits are consistent across various text lengths and domains, including natural and random text, suggesting the capacity is inherent to the model rather than memorization. - Results indicate that capacity scales almost linearly with the number of trainable vectors, showcasing potential for efficient long-context processing and integration with memory-augmented architectures. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464) | Minki Kang, Dong Bok Lee, hbseong, dwgnr, Seanie-lee | - SafeRoute, a binary router, is proposed to enhance the efficiency and accuracy of safety guardrails in Large Language Models (LLMs). - SafeRoute selectively applies a larger safety guard model only to complex inputs identified by the router, while using a smaller model for simpler inputs. - This method improves the trade-off between computational cost and safety performance, outperforming baseline methods. - Experiments conducted on multiple benchmark datasets show that SafeRoute significantly improves F1 scores compared to individual use of smaller or larger models. - It reduces the computational overhead by only using the larger model on a small subset of the inputs while maximizing accuracy. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Llama-Guard-3-1B), [Link](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), [Link](https://huggingface.co/meta-llama/Llama-Guard-3-8B), [Link](https://huggingface.co/ibm-granite/granite-guardian-3.0-8b), [Link](https://huggingface.co/answerdotai/ModernBERT-large) |
| [Rethinking Diverse Human Preference Learning through Principal Component Analysis](https://arxiv.org/abs/2502.13131) | Hao Sun, Feng Luo, huanzhang12, CharlesDDDD, Ray2333 | - This paper introduces Decomposed Reward Models (DRMs), a novel approach for extracting diverse human preferences from binary comparisons, eliminating the need for fine-grained annotations. - DRMs represent preferences as vectors and utilize Principal Component Analysis (PCA) to identify orthogonal basis vectors capturing distinct preference aspects. - These basis vectors form reward models, linearly combined using test-time adaptation to align with individual user preferences. - Experiments on RewardBench and RPR datasets demonstrate DRMs' superior performance compared to single-head, ensemble-head, and random-head baselines in test-time preference adaptation. - DRMs effectively capture diverse preference attributes, offering interpretable representations and adapting well to user preferences without additional training. | ['Natural Language Processing', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/weqweasdas/preference_dataset_mixture2_and_safe_pku) |
| [Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation](https://arxiv.org/abs/2502.13145) | Qian Zhang, wenyuliu, wondervictor, HongyuanTao, LegendBC | - This paper introduces mmMamba, a framework for developing linear-complexity native multimodal state space models. - It achieves this through progressive distillation from existing Multimodal Large Language Models (MLLMs). - mmMamba offers two architectures: mmMamba-linear, with full linear complexity, and mmMamba-hybrid, which strategically transforms fixed intervals of Transformer layers into Mamba-2 layers for a balance between efficiency and performance. - By distilling from HoVLE, a Transformer-based decoder-only VLM, mmMamba-linear achieves competitive performance against other linear/quadratic-complexity VLMs while using 2x fewer parameters than EVE-7B. - At 103K tokens, mmMamba-linear shows a 20.6x speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid has a 13.5x speedup and 60.2% memory savings. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/hustvl/mmMamba) | N/A |
| [FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading](https://arxiv.org/abs/2502.11433) | ShirleyY, Acatsama, YupengCao, zdeng10, xionggj001 | - FLAG-TRADER, a novel framework fusing Large Language Models (LLMs) with Reinforcement Learning (RL), is introduced for financial trading. - A partially fine-tuned LLM acts as the policy network, processing market data via textual state representations, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. - A hybrid RL component integrates external environment reward gradients for policy optimization, aligning with trading performance metrics. - Empirical results on stock and cryptocurrency trading tasks demonstrate FLAG-TRADER's superior performance over buy-and-hold and LLM-agentic baselines, achieving higher cumulative returns and Sharpe ratios. - Notably, FLAG-TRADER enables a small-scale (135M parameter) open-source LLM to outperform significantly larger proprietary models, highlighting the effectiveness of RL fine-tuning. | ['Reinforcement Learning', 'Natural Language Processing', 'Time Series Forecasting'] | N/A | N/A |
| [You Do Not Fully Utilize Transformer's Representation Capacity](https://arxiv.org/abs/2502.09245) | kefirski, ummagumm-a, elephantmipt, yaraksen, gudleifrr | - This paper introduces Layer-Integrated Memory (LIMe), a novel mechanism designed to enhance the representational capacity of Transformer decoders by mitigating representation collapse. - LIMe modifies the attention mechanism to allow each head to access representations from all preceding layers via a learnable routing mechanism, rather than relying solely on the immediately prior layer.  - Through experiments on language modeling tasks, LIMe demonstrates consistent performance improvements over standard Transformer baselines and other state-of-the-art modifications, achieving lower perplexity and better results on one-shot benchmarks.  - Analysis reveals that LIMe successfully counteracts representation collapse by preserving higher entropy in deeper layers, increasing the separability of related tokens, and improving overall representational diversity.  - LIMe also exhibits superior scaling behavior with increasing model depth, suggesting potential for deeper and more robust Transformer architectures. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/corl-team/lime) | N/A |
| [Magma: A Foundation Model for Multimodal AI Agents](https://arxiv.org/abs/2502.13130) | cheryyunl, Baolin, rzheng12, qianhuiwu, tanreuben |  - Magma is a multimodal foundation model for AI agents capable of understanding and grounding multimodal inputs within its environment. It bridges verbal and spatial intelligence to navigate complex tasks.  - The model architecture is a significant extension of Vision-Language models, incorporating spatial-temporal intelligence for action planning and execution.  - Magma achieves state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models.  - The model is pretrained on heterogeneous datasets including images, videos, and robotics data, using Set-of-Mark (SoM) and Trace-of-Mark (ToM) for action grounding and planning.  - Magma's code and model are publicly available for reproducibility. | ['Multimodal', 'Robotics'] | [Link](https://microsoft.github.io/Magma) | N/A |
| [RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm](https://arxiv.org/abs/2502.12513) | Kaicheng Yang, JiankangDeng, SeriousBro, Nina0607, GaryGuuu | - This paper introduces RealSyn, a large-scale multimodal dataset designed for vision-language representation learning, addressing the underutilization of non-paired multimodal interleaved documents. - RealSyn integrates realistic and synthetic texts, overcoming limitations of existing datasets by employing hierarchical retrieval and an image semantic augmented generation module to create image-text pairs. - The dataset is available in three sizes (15M, 30M, and 100M), demonstrating scalability and strong performance on downstream tasks. - Extensive experiments show that pre-trained models on RealSyn achieve state-of-the-art performance on various downstream tasks, outperforming existing methods. - The RealSyn dataset and pre-trained model weights are publicly available to facilitate further research. | ['Multimodal'] | [Link](https://github.com/deepglint/RealSyn) | N/A |
| [PAFT: Prompt-Agnostic Fine-Tuning](https://arxiv.org/abs/2502.12859) | Fei Richard Yu, Ying Tiffany He, Mingwen Ou, Yao Shu, kittttttt | - PAFT dynamically adjusts prompts during fine-tuning, encouraging the model to learn underlying task principles rather than overfitting to specific prompt formulations. - It operates in two stages: constructing a diverse set of synthetic candidate prompts and randomly sampling from this set during fine-tuning. - Experiments across diverse datasets and LLMs show that PAFT-trained models exhibit strong robustness and generalization across various prompts, including unseen ones. - This enhanced robustness leads to improved model performance and inference speed while maintaining training efficiency. - PAFT achieves higher accuracy and lower variance across different reasoning and reading comprehension tasks compared to baseline models and other prompt engineering methods, demonstrating its effectiveness in improving prompt robustness. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections](https://arxiv.org/abs/2502.12170) | Xingyuan Yuan, Da Xiao, lishengping, Hilbertmeng | - MUDDFormer introduces Multiway Dynamic Dense (MUDD) connections, dynamically generating connection weights based on hidden states and input streams (query, key, value, residual) within a Transformer block. - Unlike static or shared-weight dense connections, MUDD enhances cross-layer information flow, mitigating limitations of residual connections in deep transformers. - MUDDFormer consistently outperforms standard Transformers across various architectures and scales in language modeling, achieving performance comparable to models trained with ~1.8x-2.4x more compute. - MUDDPythia-2.8B matches Pythia-6.9B in pretraining perplexity and downstream tasks, even rivaling Pythia-12B in few-shot settings with minimal added parameters (0.23%) and computation (0.4%). - Analysis reveals MUDD connections improve depth utilization, reduce representation collapse, and vitalize attention heads, especially in deeper layers, thereby enhancing in-context learning abilities. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Caiyun-AI/MUDDFormer) | N/A |
| [Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?](https://arxiv.org/abs/2502.12215) | Yunhua Zhou, Qinyuan Cheng, Zhiyuan Zeng, xpqiu, yinzhangyue | - This paper investigates the test-time scaling capabilities of large language models (LLMs) like QwQ, Deepseek-R1 (R1), and LIMO, which are similar to OpenAI's o1 series. - The study reveals that increasing the chain-of-thought (CoT) length does not consistently improve accuracy, and correct solutions often have shorter lengths than incorrect ones. - The authors attribute this phenomenon to the limited self-revision capabilities of these models, finding that longer CoTs contain more self-revisions which often lead to performance degradation. - A comparison of sequential and parallel scaling strategies shows that parallel scaling achieves better coverage and scalability. - Based on these findings, the paper introduces "Shortest Majority Vote," a method combining parallel scaling with CoT length characteristics to prioritize shorter solutions, significantly improving test-time scalability compared to conventional majority voting. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning](https://arxiv.org/abs/2502.11271) | Joseph Boen, Rahul Thapa, Sheng Liu, Bowen Chen, lupantech | This paper introduces OctoTools, a training-free, user-friendly, and extensible framework for complex reasoning.  OctoTools utilizes standardized tool cards to integrate diverse tools, a planner for high-level and low-level planning, and an executor to manage tool usage. Experiments on 16 diverse benchmarks demonstrate OctoTools's superiority over existing methods, showing substantial accuracy gains.  OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem-solving. The framework is designed to be easily extensible with new tools. | ['Any-to-Any', 'Question Answering', 'Multimodal'] | [Link](https://octotools.github.io) | N/A |
| [Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge](https://arxiv.org/abs/2502.12501) | zhangsan5421, lifengshang, horiz94, YuxinJiang, DonJoey | - This paper introduces Crowd-based Comparative Evaluation (CCE), a novel method for enhancing the reliability of LLM-as-a-Judge auto-evaluation. - CCE introduces additional crowd responses to compare with candidate responses, exposing deeper details and improving the comprehensiveness of CoT judgments. - Experiments across five benchmarks show that CCE improves accuracy by an average of 6.7% and produces higher-quality CoTs. - CCE is also applied to judge distillation and crowd rejection sampling for supervised fine-tuning (SFT), demonstrating its effectiveness. - The analysis reveals that CCE-generated CoTs are more comprehensive, and evaluation accuracy improves as inference scales. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation](https://arxiv.org/abs/2502.09838) | Binhe Yu, Yuqian Yuan, Sijing Li, Wenqiao Zhang, Tianwei Lin | - HealthGPT is a medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation within a unified autoregressive paradigm. - It uses a novel Heterogeneous Low-Rank Adaptation (H-LORA) technique for parameter-efficient fine-tuning and a hierarchical visual perception approach to effectively learn from heterogeneous medical data. - HealthGPT outperforms both state-of-the-art unified visual models and medical-specific models on various comprehension and generation tasks, highlighting its superior capability in complex healthcare applications. - A comprehensive medical domain-specific comprehension and generation dataset, called VL-Health, was created to train HealthGPT. - HealthGPT achieves performance comparable to or better than state-of-the-art models on several medical multi-modal tasks in data-constrained scenarios. | ['Multimodal', 'Image-to-Image', 'Image-to-Text', 'Text-to-Image', 'Visual Question Answering'] | [Link](https://github.com/DCDmllm/HealthGPT) | N/A |
| [HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/abs/2502.12574) | beidic, junjiehu, jinqixiao, ZefanCai, wdlctc |  - HEADINFER is a novel inference framework that significantly reduces the GPU memory footprint required for large language model (LLM) inference by employing a head-wise offloading strategy.  - It offloads the key-value (KV) cache to the CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU.  - This is achieved by maintaining only selective attention heads' KV cache on the GPU and computing attention output dynamically.  - Experimental results demonstrate that HEADINFER achieves a 92% reduction in GPU memory footprint and enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory.  - The approach is evaluated on various LLMs (Llama, Qwen, Mistral, and Gemma) and outperforms existing methods in terms of context length extension. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/wdlctc/headinfer) | [Link](None) |
| [Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2502.10708) | Mingzhe Li, Miao Fang, Yuhan Liu, Bin Yan, Ziruibest | - This paper provides a comprehensive survey of methods for injecting domain-specific knowledge into large language models (LLMs). - The authors categorize these methods into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. - Each approach is discussed in detail, highlighting its advantages, disadvantages, and trade-offs. - The authors also compare domain-specific LLMs against general-purpose LLMs and discuss the challenges and opportunities in this field. - Finally, they provide a summary of commonly used datasets and benchmarks for researchers interested in delving deeper into this area. | ['Natural Language Processing'] | [Link](https://github.com/abilliyb/Knowledge_Injection_Survey_Papers) | N/A |
| [Eager Updates For Overlapped Communication and Computation in DiLoCo](https://arxiv.org/abs/2502.12996) | Yanislav Donchev, Arthur Douillard, Satyen Kale | - This paper introduces "eager updates," a technique to improve the efficiency of distributed training for large language models (LLMs) using the DiLoCo method. - Eager updates mitigate communication bottlenecks by overlapping the communication of outer gradients with the computation of the next inner optimization phase. - A naive implementation of this approach leads to reduced convergence speed. Thus, the authors propose an "eager" version that utilizes local outer gradients as a proxy during the overlap, improving convergence compared to the naive version. - Experiments demonstrate that eager updates reduce bandwidth requirements significantly, approaching 95% compute utilization for 1B, 10B, and 100B parameter models with bandwidths between 1 and 5 Gbit/s. - Evaluating on C4 and HellaSwag datasets reveals competitive performance compared to standard DiLoCo and significant improvements over naive delayed outer gradient updates, particularly under low bandwidth conditions. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Atom of Thoughts for Markov LLM Test-Time Scaling](https://arxiv.org/abs/2502.12018) | Chenglin Wu, Jiayi Zhang, Quan Shi, Zhaoyang Yu, leavendough | - This paper introduces Atom of Thoughts (AOT), a novel reasoning framework designed to enhance the test-time scaling capabilities of Large Language Models (LLMs). - AOT employs a Markov-style reasoning process, decomposing complex questions into a dependency-based directed acyclic graph (DAG) and contracting subquestions into independent atomic question states. - This iterative decomposition-contraction process simplifies the reasoning process, eliminating the need for maintaining extensive historical information, and thus improving computational efficiency. - AOT can be used as a standalone framework or integrated with existing test-time scaling methods as a plug-in enhancement. - Experimental results across six benchmarks demonstrate that AOT significantly improves LLM performance, notably achieving an 80.6% F1 score on HotpotQA with gpt-4-o-mini, surpassing 03-mini by 3.4% and DeepSeek-R1 by 10.6%. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/qixucen/atom) | N/A |
| [FinMTEB: Finance Massive Text Embedding Benchmark](https://arxiv.org/abs/2502.10990) | Yi Yang, yixuantt | - This paper introduces FinMTEB, a comprehensive benchmark designed for evaluating financial text embedding models. - FinMTEB consists of 64 datasets across seven tasks, including semantic textual similarity, retrieval, classification, clustering, reranking, pair classification, and summarization, covering both Chinese and English financial texts. - The authors also develop Fin-E5, a finance-adapted embedding model based on e5-Mistral-7B-Instruct, trained using a persona-based data synthesis method to cover diverse financial embedding tasks. - Experimental results demonstrate that domain-adapted models like Fin-E5 outperform general-purpose models, achieving state-of-the-art performance on FinMTEB. - Surprisingly, a simple Bag-of-Words (BoW) approach surpasses sophisticated dense embeddings in financial Semantic Textual Similarity tasks, indicating limitations in current dense embedding techniques for handling complex financial texts. | ['Natural Language Processing', 'Feature Extraction', 'Sentence Similarity'] | [Link](https://github.com/yixuantt/FinMTEB) | N/A |
| [Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research](https://arxiv.org/abs/2502.12669) | Shuyan Chen, wenxinsiju, yongqi2023, sunpenglei, Dominic789654 | - This research introduces a comprehensive knowledge-enhanced system specifically designed for perovskite solar cell research, improving researchersâ€™ access to relevant information and reasoning abilities. - The system comprises a domain-specific knowledge graph (Perovskite-KG) containing 23,789 entities and 22,272 relationships extracted from 1,517 research papers; two specialized datasets (Perovskite-Chat with 55,101 question-answer pairs and Perovskite-Reasoning with 2,217 materials science problems); and two specialized large language models (Perovskite-Chat-LLM and Perovskite-Reasoning-LLM). - Experimental results demonstrate that the system outperforms current models in both domain-specific knowledge retrieval and scientific reasoning tasks. - Perovskite-Chat-LLM achieved a Rouge-L score of 41.25 and an LLM-Judge score of 2.97 on the Perovskite QA dataset, surpassing other baseline models like LLaMA and GPT variants. - Perovskite-Reasoning-LLM achieved state-of-the-art performance on GPQA and Minerva benchmarks within the 7B model category, achieving a score of 43.95 and 44.49 respectively with high data efficiency. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages](https://arxiv.org/abs/2502.10852) | XU Han, Jianing Liu, Guixian Xu, Ziyin Zhang, Zeli Su | - This paper introduces XLM-SWCM, a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages by reusing weights between the encoder and decoder. - The model architecture consists of a pre-trained encoder (CINO, a variant of XLM-R) and a decoder with two layer types: NormalDecoderLayer (randomly initialized) and CustomDecoderLayer (initialized with encoder weights). -  A weight-sharing mechanism is introduced to initialize and reuse weights between the encoder and decoder, maximizing the utilization of pre-trained encoder features. -  XLM-SWCM is pre-trained on a combined corpus of Simplified Chinese news articles (THUCNews), minority language data (MC2), and machine-translated parallel sentences. -  Experimental results show that XLM-SWCM significantly outperforms baselines like mBART and MC2-LLaMA-13B in text summarization, reading comprehension, and machine translation tasks, even exceeding much larger models in cross-lingual transfer settings. | ['Natural Language Processing', 'Text Generation', 'Translation', 'Summarization', 'Text2Text Generation'] | N/A | N/A |
