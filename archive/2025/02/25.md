

## Papers for 2025-02-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks](https://arxiv.org/abs/2502.17157) | Zhiyue Zhao, Mingyu Liu, Z-MU-Z, zhyya, Canyu | - DICEPTION is a multi-task visual generalist model based on a diffusion model (SD3 architecture) that unifies different computer vision tasks within the RGB space, leveraging the priors of pre-trained text-to-image diffusion models. - The model performs comparably to state-of-the-art specialized models on tasks such as depth estimation, surface normal estimation, and point-prompted segmentation using significantly less training data (e.g., 0.06% of SAM's data for comparable segmentation performance). - DICEPTION effectively adapts to new visual tasks with minimal fine-tuning (as few as 50 images and ~1% of its parameters). - The study also validates the effectiveness of random color assignment for instance segmentation and semantic segmentation tasks, contrary to previous findings. - While demonstrating strong qualitative results, quantitative evaluation for semantic segmentation and human keypoint detection remains a challenge due to post-processing limitations, and inference time is relatively long due to the diffusion process. | ['Computer Vision', 'Depth Estimation', 'Image Segmentation', 'Keypoint Detection', 'Multimodal'] | N/A | N/A |
| [Slamming: Training a Speech Language Model on One GPU in a Day](https://arxiv.org/abs/2502.15814) | Yossi Adi, avishai-elmakies, gallilmaimon | - Slam, a new training recipe, enables training high-quality Speech Language Models (SLMs) on a single academic GPU within 24 hours. - The recipe involves analyzing model initialization, architecture, synthetic data, preference optimization, and component tweaking. - This method scales well with more compute, achieving results comparable to leading SLMs using significantly fewer resources. - Empirical results outperform predictions from previous SLM scaling laws, showing a more optimistic compute requirement for training high-quality SLMs. - The study uses a decoder-only generative SLM architecture and various evaluation metrics, including likelihood evaluation and generative perplexity. | ['Audio', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models](https://arxiv.org/abs/2502.16614) | Yejie Wang, Wei Zhang, Jiaheng Liu, Marcus Dong, Alexander Zhang | This paper introduces CodeCriticBench, a comprehensive benchmark designed to holistically evaluate the code critique capabilities of large language models (LLMs).  It covers two mainstream code tasks: code generation and code QA, with varying difficulty levels. The benchmark uses basic and advanced evaluation protocols, including fine-grained evaluation checklists, for a thorough assessment. Extensive experiments on existing LLMs demonstrate the effectiveness of CodeCriticBench in evaluating different aspects of code critique capabilities. The results show that performance generally improves with model size, validating the benchmark's design. | ['Question Answering'] | [Link](https://github.com/multimodal-art-projection/CodeCriticBench) | N/A |
| [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/abs/2502.16894) | Wei Wei, Xiaoye Qu, Sichen Liu, Zhenyi Lu, Facico |  - This paper introduces GOAT, a novel framework that improves the performance of LoRA (Low-Rank Adaptation) by using adaptive singular values and mixture-of-experts (MoE) optimization alignment. - GOAT uses an SVD-structured MoE architecture, which adaptively integrates relevant priors and aligns optimization with full fine-tuned MoE. - The method includes a theoretical scaling factor that further boosts LoRA MoE's efficiency and performance. - Experiments across 25 datasets show GOAT achieving state-of-the-art performance, closing the gap with Full FT (Full Fine-Tuning). - GOAT is shown to significantly outperform other LoRA and LoRA-MoE baselines on multiple benchmarks, and it is also more parameter efficient than Full FT. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](https://arxiv.org/abs/2502.16033) | Yang Zhao, Shan Jiang, Hongquan Li, Yue Fan, Qianqi Yan | - Introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark, a new framework for evaluating the ability of Multimodal Large Language Models (MLLMs) to detect and reason about inconsistencies in visually and textually rich content like webpages, slides, and posters. - MMIR features 534 samples across five reasoning categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence, and employs two evaluation settings: open-ended and multiple-choice. - Evaluations of six state-of-the-art MLLMs, including o1, GPT-40, Qwen2.5-VL, LLaVA-NeXT, InternVL2.5, and Phi-3.5-Vision, reveal that current MLLMs struggle with multimodal inconsistency reasoning, with proprietary models significantly outperforming open-source alternatives. - Error analysis indicates that models perform best with single-modality (text-only) inconsistencies and struggle most with image-image inconsistencies, suggesting a limited capacity for visual and cross-modal reasoning. - Probing experiments show that Multimodal Interleaved Chain-of-Thought (MM-CoT) prompting, which combines visual and textual cues through an iterative reasoning process, leads to greater performance gains over traditional single-modality prompting methods like CoT and SoM. | ['Multimodal'] | N/A | N/A |
| [Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration](https://arxiv.org/abs/2502.17110) | Ji Zhang, Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy | - Mobile-Agent-V is a framework that leverages video demonstrations to teach mobile agents how to perform complex, device-specific operations, addressing the challenge of limited operational knowledge in current AI-driven mobile automation systems. - The framework employs a sliding window mechanism to process keyframes from instructional videos, reducing input length while retaining critical information for the decision-making agent. - A video agent dynamically adjusts the sliding window, a decision agent determines the appropriate action based on the video and current device state, and a deep-reflection agent validates and refines the chosen action to ensure consistency with the demonstrated operation. - Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks on tasks requiring operational knowledge, effectively learning and applying video-based knowledge for mobile device interaction. - The approach is scalable and adaptable, offering a practical alternative to manual programming or extensive data collection for training mobile automation agents. | ['Multimodal'] | N/A | N/A |
| [Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties](https://arxiv.org/abs/2502.16922) | Deyu Zhou, Yong Jiang, Pengfei LI, Jialong Wu, wzl0228 | - This paper introduces CTM, a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. - CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. - Experiments on various LLMs reveal that CTM poses significant challenges, highlighting potential avenues for improvement in LLMs' temporal reasoning abilities. - The benchmark includes eight challenging question-answering tasks and a Timeline Ito Game to evaluate the LLMâ€™s ability to align entities across temporal and other dimensions. - CTM's dataset is built upon a curated and authoritative Chinese cultural entity repository, which encompasses over 4,700 entities. | ['Question Answering'] | [Link](https://github.com/Linking-ai/ctm_bench) | N/A |
| [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/abs/2502.14132) | Isabelle Augenstein, Desmond Elliott, gretawarren, Nadav | - This paper investigates the extent to which community notes on Twitter/X rely on professional fact-checkers and explores the characteristics of posts and notes that utilize fact-checking sources. - The study reveals that community notes cite fact-checking sources significantly more often than previously reported, especially for posts related to broader misinformation narratives. - The researchers use language models to annotate a large corpus of community notes with attributes like topic, cited sources, and refutation of claims. - Their findings indicate that professional fact-checking plays a crucial role in the creation of high-quality community notes, highlighting the interdependency between professional and community-based fact-checking. - The study's conclusion emphasizes the importance of professional fact-checking in combating misinformation, particularly for complex or high-stakes issues. | ['Text Classification'] | N/A | N/A |
| [Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam](https://arxiv.org/abs/2502.17055) | Xiang Li, Gaojie Jin, Zhenyu Zhang, Haotian Hu, Tianjin Huang | - This paper introduces Stable-SPAM, a new optimizer for training Large Language Models (LLMs) with 4-bit precision, addressing the instability issues often encountered in low-precision training. - Stable-SPAM incorporates adaptive gradient normalization (AdaGN), adaptive spike-aware clipping (AdaClip), and momentum reset (MoRet) to stabilize gradient norms and prevent divergence. - Experiments on various LLaMA models show that Stable-SPAM significantly outperforms Adam and other recent optimizers in 4-bit training scenarios, sometimes even exceeding the performance of 16-bit Adam. - Notably, the 4-bit LLaMA-1B model trained with Stable-SPAM achieves comparable results to the BF16 version trained with Adam, while requiring only half the training steps when both are trained in 4-bit. - The proposed techniques are also shown to be effective when integrated with other optimizers, demonstrating their broad applicability in improving low-precision training stability. | ['Natural Language Processing'] | [Link](https://github.com/TianjinYellow/StableSPAM.git) | N/A |
| [X-Dancer: Expressive Music to Human Dance Video Generation](https://arxiv.org/abs/2502.17414) | Chenxu Zhang, You Xie, Guoxian Song, Hongyi Xu, Zeyuan Chen | - This paper introduces X-Dancer, a novel zero-shot music-driven image animation pipeline that generates diverse and long-range human dance videos from a single static image. - The model architecture consists of a unified transformer-diffusion framework, using an autoregressive transformer to synthesize music-synchronized token sequences for 2D body poses, and a diffusion model to generate coherent and realistic video frames. - X-Dancer models 2D human motion, leveraging widely accessible dance videos, enhancing scalability and addressing limitations of traditional 3D methods. - Experimental results demonstrate that X-Dancer surpasses existing methods in terms of diversity, expressiveness, and realism, achieving state-of-the-art performance on challenging benchmarks. - The approach incorporates a multi-part tokenization scheme, allowing it to capture nuanced alignment with musical beats through readily available monocular videos. | ['Image-to-Video', 'Text-to-Video', 'Multimodal'] | N/A | N/A |
| [M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment](https://arxiv.org/abs/2502.15167) | Weiming Zhang, Wen Shen, Zhihua Wei, Kejiang Chen, Chuan Cui | - This paper introduces M3-AGIQA, a novel multimodal framework for assessing the quality of AI-generated images (AGI). - M3-AGIQA leverages multimodal large language models (MLLMs) as joint text and image encoders and incorporates a structured multi-round evaluation mechanism. - The model uses Low-Rank Adaptation (LoRA) to fine-tune a local MLLM, achieving state-of-the-art performance on multiple benchmark datasets. - Extensive experiments demonstrate its effectiveness in capturing nuanced aspects of AGI quality, including perceptual quality, prompt correspondence, and authenticity. - Cross-dataset validation confirms its strong generalizability. | ['Multimodal'] | [Link](https://github.com/strawhatboy/M3-AGIQA) | N/A |
