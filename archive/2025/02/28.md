

## Papers for 2025-02-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Self-rewarding correction for mathematical reasoning](https://arxiv.org/abs/2502.19613) | Nan Jiang, Lichang Chen, Chenlu Ye, Hanning Zhang, Wei Xiong | - This paper introduces a self-rewarding reasoning framework for Large Language Models (LLMs) that enables autonomous error detection and self-correction in mathematical reasoning without external reward models. - The two-stage framework employs sequential rejection sampling to create synthetic training data containing self-rewarding and self-correction examples, which is used in the instruction fine-tuning phase. - It then refines these behaviors using reinforcement learning with a rule-based reward signal during the reinforcement learning phase. - Experiments on Llama-3 and Qwen-2.5 demonstrate superior performance over intrinsic self-correction methods and comparable results to systems with external reward models. - Results show that self-rewarding correction improves final accuracy and efficiency in test-time compute scaling compared to other baselines. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/RLHFlow/Self-rewarding-reasoning-LLM) | [Link](https://huggingface.co/AI-MO/NuminaMath-7B-COT) |
| [MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning](https://arxiv.org/abs/2502.19634) | Jiayuan Zhu, Fenglin Liu, Junde Wu, Jiazhen Pan, che111 | - MedVLM-R1, a novel 2B parameter medical Vision-Language Model (VLM), is introduced, designed for enhanced reasoning capabilities in radiological Visual Question Answering (VQA) tasks by leveraging Group Relative Policy Optimization (GRPO). - Unlike conventional Supervised Fine-Tuning (SFT) methods, MedVLM-R1 employs reinforcement learning, incentivizing the model to generate natural language reasoning alongside final answers, thus improving transparency and trustworthiness. - Demonstrating superior generalization, MedVLM-R1 achieves robust performance on out-of-distribution data (MRI → CT/X-ray), surpassing larger models like Qwen2VL-72B and Huatuo-GPT-Vision-7B trained on significantly more data. - Trained on only 600 samples, MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming significantly larger models trained on over a million samples. - By combining medical image analysis with explicit reasoning generation, MedVLM-R1 marks a significant advancement towards trustworthy and interpretable AI in clinical settings. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning', 'Computer Vision'] | N/A | N/A |
| [R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts](https://arxiv.org/abs/2502.20395) | Tianyi Zhou, Ziyue Li, Zhongyang Li | - This paper introduces R2-T2 (Re-Routing in Test-Time), a novel test-time optimization method for Multimodal Mixture-of-Experts (MoE) models that dynamically adjusts routing weights without requiring additional training. - R2-T2 addresses the suboptimality of pre-trained routers by leveraging information from similar samples in a reference set to refine the routing weights for test samples, thereby enhancing expert selection. - Three strategies for implementing R2-T2 are proposed including neighborhood gradient descent, kernel regression and mode finding. - Extensive experiments across various benchmarks demonstrate substantial improvements over strong baselines including larger VLM models. - Analysis shows that R2-T2 effectively refines routing, boosting correct predictions and mitigating the original router's over-reliance on a single expert. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/tianyi-lab/R2-T2) | N/A |
| [LongRoPE2: Near-Lossless LLM Context Window Scaling](https://arxiv.org/abs/2502.20082) | Gilsinia Lopez, Gaokai Zhang, Siyuan Wang, Li Lyna Zhang, Ning Shang | - LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) while preserving performance on the original shorter context window. - It achieves this through three contributions: (1) identifying insufficient training in higher RoPE dimensions as a key contributor to out-of-distribution (OOD) issues, (2) developing an effective RoPE rescaling algorithm using evolutionary search guided by "needle-driven" perplexity, and (3) employing mixed context window training to adapt model weights to rescaled RoPE for long sequences while maintaining short-context performance with original RoPE. - LongRoPE2 extends LLaMA3-8B to a 128K effective context length while retaining over 98.5% of short-context performance using only 10B training tokens — 80x fewer than Meta's approach, which fails to achieve the same effective context length. - It outperforms existing methods like YaRN, NTK, and LongRoPE on long context benchmarks like RULER and real-world datasets like LOFT, InfiniteBench, and LongBench. - Additionally, LongRoPE2 maintains strong performance on standard short-context benchmarks, minimizing the performance drop often observed in context window extension methods. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/microsoft/LongRoPE) | N/A |
| [FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving](https://arxiv.org/abs/2502.20238) | Chaoqun Liu, Hou Pong Chan, Hao Zhang, Weiwen Xu, Guizhen Chen | - Introduces FINEREASON, a logic-puzzle benchmark designed for granular evaluation of Large Language Models' (LLMs) reasoning capabilities, focusing on intermediate steps rather than just final-answer accuracy. - Proposes two evaluation tasks: *state checking*, predicting the solvability of a given state, and *state transition*, determining the next valid move in the puzzle-solving process. - Employs a tree-based decomposition of logic puzzles into atomic steps, allowing rigorous validation of the intermediate states and transitions. - Demonstrates that models trained on FINEREASON's state-checking and transition tasks show improved performance on mathematical reasoning benchmarks by up to 5.1% on GSM8K. - Reveals that reasoning-oriented models (OpenAI-o1 and Gemini-2.0-Flash-Thinking) significantly outperform general-purpose models on FINEREASON, but even leading reasoning models have substantial limitations in deep reasoning tasks, particularly in state transition accuracy. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/DAMO-NLP-SG/FineReason) | N/A |
| [CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale](https://arxiv.org/abs/2502.16645) | Kaiyue Qiu, Zhaoyang Chu, Chenlong Wang, yxy0807, zx10086 | - This paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. - Based on CODESYNC, the authors develop CODESYNCBENCH, a benchmark for assessing LLMs' ability to stay synchronized with code evolution, covering real-world updates for 220 APIs from six Python libraries. - The benchmark includes 3,300 test cases across three evaluation tasks (code completion, error correction, and multiple-choice questions) and an update-aware instruction tuning dataset with 2,200 training examples. - Experimental results on 14 state-of-the-art LLMs show that they struggle with dynamic code evolution, even with advanced knowledge updating methods. - The benchmark and dataset aim to facilitate the development of real-time code knowledge updating in LLMs. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Lucky-voyage/Code-Sync) | N/A |
| [Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance](https://arxiv.org/abs/2502.16944) | Zhixu Li, Pu Zhao, Lu Wang, Chenghua Huang, keanudicap | - This paper introduces Decoupled Value Policy Optimization (DVPO), a new framework for Reinforcement Learning from Human Feedback (RLHF) that replaces traditional reward modeling with a pre-trained global value model (GVM). - The GVM in DVPO predicts token-level return-to-go estimates and guides policy optimization, decoupling value and policy training to reduce computational complexity and instability. - DVPO reduces GPU memory usage by 40% and training time by 35% compared to conventional RLHF, while maintaining performance comparable to state-of-the-art methods like PPO. - Experiments on benchmarks such as MT-Bench, Alpaca-Eval, and Arena-Hard show DVPO outperforms efficient RLHF methods (e.g., DPO) and matches or exceeds PPO performance across different model sizes. - Theoretical analysis demonstrates that pretraining a reward model and a global value model are functionally interchangeable in offline RLHF where no new ground-truth rewards are available. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B), [Link](https://huggingface.co/meta-llama/Llama-3.2-3B), [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2), [Link](https://huggingface.co/openbmb/UltraRM-13b), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), [Link](https://huggingface.co/ziniuli/Mistral-7B-ReMax-v0.1) |
| [UniTok: A Unified Tokenizer for Visual Generation and Understanding](https://arxiv.org/abs/2502.20321) | Xin Yu, Jihan Yang, Junfeng Wu, Yi Jiang, Chuofan Ma | - UniTok, a novel discrete visual tokenizer, is introduced to bridge the gap between visual generation and understanding tasks by encoding fine-grained details for generation while capturing high-level semantics for understanding. - The paper argues that the performance bottleneck of existing unified tokenizers stems from the limited representational capacity of discrete tokens rather than conflicting learning objectives. - To address this limitation, UniTok employs multi-codebook quantization, which divides vector quantization with independent sub-codebooks, effectively scaling the latent code space and enhancing representation expressiveness. - UniTok incorporates attention factorization using multi-head attention modules to preserve richer semantics during token factorization. - Experimental results demonstrate that UniTok achieves state-of-the-art performance in both generation and understanding tasks, as evidenced by a 0.38 rFID on ImageNet reconstruction and 78.6% zero-shot accuracy, exceeding existing unified tokenizers and even outperforming some domain-specific models. | ['Multimodal', 'Text-to-Image', 'Visual Question Answering', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/FoundationVision/UniTok) | N/A |
| [Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think](https://arxiv.org/abs/2502.20172) | Haozhe Zhao, Weichu Xie, Wenhao Chai, Shuai Bai, Liang Chen | - DREAM ENGINE, a novel framework for multimodal image generation with text-image interleaved control, is introduced, which leverages Large Multimodal Models (LMMs) like QwenVL and a Diffusion Transformer (DiT) backbone like Stable Diffusion v3.5. - The architecture replaces traditional text encoders with an LMM and a lightweight projector layer for encoding text-image interleaved controls, employing a two-stage training process involving joint text-image alignment and multimodal instruction tuning. - It effectively generates images from complex, interwoven text and image instructions, including merging concepts from different images and handling object-driven compositional tasks.  - Evaluation on the GenEval benchmark shows a competitive overall score of 0.69, matching state-of-the-art models like SDv3.5 (0.71) and surpassing FLUX.1 Dev (0.66) and showing superior image reconstruction performance. - The model exhibits emergent capabilities, like synthesizing concepts from different input images, highlighting the potential of LMMs as unified multimodal instruction encoders. | ['Text-to-Image', 'Multimodal'] | [Link](https://github.com/chenllliang/DreamEngine) | N/A |
| [NeoBERT: A Next-Generation BERT](https://arxiv.org/abs/2502.19587) | Sarath Chandar, Mariam El Mezouar, Quentin Fournier, Lola Le Breton | - This paper introduces NeoBERT, a next-generation bidirectional encoder model for Natural Language Processing (NLP) tasks. - NeoBERT integrates state-of-the-art advancements in architecture (optimal depth-to-width ratio, RoPE positional embeddings, SwiGLU activation, RMSNorm), data (RefinedWeb), and pre-training methodologies (extended context length of 4096 tokens, two-stage pre-training). - With a compact size of 250 million parameters, NeoBERT achieves state-of-the-art results on the MTEB benchmark, outperforming larger models such as BERTlarge and ROBERTalarge, under identical fine-tuning conditions. - It also demonstrates superior performance on the GLUE benchmark, comparable to existing large models while being significantly smaller. - The authors release all code, data, checkpoints, and training scripts to facilitate further research and adoption. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | [Link](https://github.com/chandar-lab/NeoBERT) | [Link](https://huggingface.co/chandar-lab/NeoBERT) |
| [SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning](https://arxiv.org/abs/2502.20127) | Yanzhen Zou, Xiangxin Meng, Pengfei Gao, Chao Peng, mizersy | - This paper introduces SoRFT (Subtask-oriented Reinforced Fine-Tuning), a novel training approach to enhance the issue-resolving capability of Large Language Models (LLMs). - SoRFT decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation, and uses a two-stage training process. - The first stage involves rejection-sampled supervised fine-tuning using Chain of Thought (CoT) data filtered with ground truth. - The second stage employs rule-based reinforcement learning leveraging Proximal Policy Optimization (PPO) with ground-truth-based rewards. - Experimental results on SWE-Bench Verified and SWE-Bench Lite show SoRFT-trained LLMs achieve state-of-the-art performance among open-source models, resolving 21.4% of issues on SWE-Bench Verified with SoRFT-Qwen-7B. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/OpenDevin/CodeQwen1.5-7B-OpenDevin) |
| [R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning](https://arxiv.org/abs/2502.19735) | Hongyong Zeng, Yuanchang Luo, Shimin Tao, Yilun Liu, boommmmm | - R1-Translator (R1-T1) is a novel framework that uses reinforcement learning (RL) with human-aligned Chain-of-Thoughts (CoTs) to improve inference-time reasoning for general machine translation (MT). - The framework incorporates six common CoT patterns observed in human translation workflows, extending reasoning-based MT beyond specialized sub-tasks to diverse tasks and six languages. - R1-T1 uses a KL-constrained RL process to facilitate the discovery of new CoT trajectories and enable anti-forgetting adaptation for unseen translation scenarios. - Experimental results on the Flores-101 test set across 21 languages and 80 translation directions demonstrate consistent improvement, particularly in 15 languages unseen during training, while preserving general multilingual capabilities compared to plain supervised fine-tuning (SFT). - The approach addresses the limitations of existing methods by aligning CoTs with human strategies, enabling adaptability to new domains, and mitigating catastrophic forgetting. | ['Translation', 'Natural Language Processing', 'Reinforcement Learning'] | N/A | N/A |
