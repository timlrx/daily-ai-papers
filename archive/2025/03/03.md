

## Papers for 2025-03-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking](https://arxiv.org/abs/2502.20730) | luyaojie, sanmusunrise, xuanang, yhycai, lzq2021 | - This paper introduces SolutionRAG, a novel system designed for complex engineering solution design, addressing the gap in existing RAG research for this task. - SolutionRAG leverages a tree-based exploration approach, allowing for flexible improvement of solutions, moving from suboptimal to reliable designs by exploring different improvement directions through branching. - It employs a bi-point thinking mechanism, alternating between solution design and review during tree growth to ensure generated solutions meet all real-world constraints specified in the requirements. - A node evaluation and pruning mechanism is incorporated into SolutionRAG to enhance inference efficiency by prioritizing promising solution paths and helpful review comments. - Experimental results on the SolutionBench demonstrate that SolutionRAG achieves state-of-the-art performance, significantly outperforming deep reasoning models and existing RAG approaches. | ['Question Answering'] | [Link](https://github.com/Li-Z-Q/DeepSolution) | N/A |
| [Chain of Draft: Thinking Faster by Writing Less](https://arxiv.org/abs/2502.18600) | Lingxiao Zhao, Wenhao Xie, DeBERTa, sileixu | - This paper introduces Chain of Draft (CoD), a new prompting strategy for Large Language Models (LLMs) that prioritizes concise and efficient reasoning. - Inspired by how humans use drafts to capture key ideas, CoD encourages LLMs to generate minimal intermediate reasoning outputs, contrasting with the verbose nature of Chain-of-Thought (CoT) prompting. - Experiments across arithmetic, common sense, and symbolic reasoning tasks demonstrate that CoD achieves comparable or better accuracy than CoT while significantly reducing token usage and latency. - In GSM8K, CoD achieved 91% accuracy with only 40 tokens, an 80% reduction compared to CoT's 200 tokens and a 76% latency decrease. - This suggests that CoD can make LLMs more practical for real-world applications by improving efficiency without compromising performance. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents](https://arxiv.org/abs/2502.18017) | xpjandy, shihang, vickywu, lovesnowbest, autumncc | - Introduced ViDoSeek, a new benchmark dataset for visual document retrieval-augmented generation focusing on complex reasoning. - Proposed ViDoRAG, a multi-agent RAG framework incorporating iterative reasoning with seeker, inspector, and answer agents. - Employed a Gaussian Mixture Model (GMM)-based hybrid strategy for multimodal retrieval, combining visual and textual features. - Demonstrated state-of-the-art performance on ViDoSeek, outperforming baselines by over 10%. - Showed effectiveness in handling complex reasoning and diverse content types within visually rich documents. | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | [Link](https://github.com/Alibaba-NLP/ViDoRAG) | N/A |
| [SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers](https://arxiv.org/abs/2502.20545) | Coralia Cartis, Wenqi Zhu, Kechen Li, Shiweiliuiiiiiii, jitianbo | - This paper introduces SoS-1K, a dataset of approximately 1,000 polynomials designed to evaluate the reasoning capabilities of LLMs in solving Sum-of-Squares (SoS) problems, a computationally intractable mathematical problem related to Hilbert's 17th problem. - The authors also present SoS-specialized reasoning instructions based on five progressively challenging criteria to guide LLMs in solving SoS problems.  - Their evaluation shows that providing high-quality reasoning instructions significantly improves the accuracy of state-of-the-art LLMs in solving SoS problems, boosting performance by up to 21%. - Fine-tuning a 7B model (SoS-7B) on SoS-1K for 4 hours resulted in an accuracy of 70%, outperforming larger models like DeepSeek-V3 (671B) and GPT-40-mini while requiring significantly less computation time.  - Further analysis suggests that while LLMs demonstrate an understanding of underlying mathematical concepts, they benefit from structured guidance and may exhibit shortcut behavior when tackling complex problems. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Joe-2002/SoS1) | N/A |
| [LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation](https://arxiv.org/abs/2502.20583) | kasikci, kojimano, jungok, kamahori | - LITEASR, a novel low-rank compression method for Automatic Speech Recognition (ASR) encoders, is introduced, which leverages low-rank properties of intermediate activations during inference. - The method uses Principal Component Analysis (PCA) with a small calibration dataset to approximate linear transformations by a chain of low-rank matrix multiplications and optimizes self-attention to operate in the reduced dimension. - Evaluated on Whisper large-v3, LITEASR reduces encoder size by ~40%, leading to a 1.4x speedup with minimal accuracy loss.  - In other configurations, the encoder size is reduced by over 50%, matching Whisper medium's size but achieving better transcription accuracy.  - The approach achieves Pareto-optimal balance between speed and accuracy, which paves the way for efficient ASR deployment. | ['Automatic Speech Recognition', 'Natural Language Processing'] | [Link](https://github.com/efeslab/LiteASR) | N/A |
| [HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](https://arxiv.org/abs/2502.20811) | Fuzheng Zhang, Yuanxing Zhang, Jingyun Hua, Xiao Wang, lwher1996 | - This paper introduces a two-stage data annotation pipeline and two associated datasets (HAICTrain and HAICBench) to improve human action understanding and generation for Multi-modal Large Language Models (MLLMs). - The pipeline improves on existing video captioning methods by focusing on fine-grained details of human actions, including attributes to distinguish individuals, body movements, and interactions. - HAICTrain consists of 126K video-caption pairs, and HAICBench contains 500 human-annotated video-caption pairs and 1400 QA pairs. - Experimental results show that training with HAICTrain significantly improves human action understanding across four benchmarks. - Additionally, the refined captions also show improvements in text-to-video generation on MovieGenBench. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Text-to-Video'] | N/A | [Link](https://huggingface.co/datasets/KuaishouHAIC/HAIC) |
