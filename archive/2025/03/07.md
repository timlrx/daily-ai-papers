

## Papers for 2025-03-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [START: Self-taught Reasoner with Tools](https://arxiv.org/abs/2503.04625) | BeichenZhang, jx-yang, Zhenru, mingfengxue, ChengpengLi | - START, a novel tool-integrated long Chain-of-Thought (CoT) reasoning Large Language Model (LLM), enhances reasoning capabilities by leveraging external tools, such as Python code execution, for complex computations, self-checking, method exploration, and self-debugging. - The core innovation of START lies in its self-learning framework, comprising Hint-infer and Hint Rejection Sampling Fine-Tuning (Hint-RFT), stimulating the LLM's tool utilization abilities without demonstration data, improving sequential test-time scaling, and refining tool-integrated reasoning trajectories. - START achieves state-of-the-art performance on PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and code benchmark (LiveCodeBench), with accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively, surpassing existing tool-integrated and long CoT models, including QwQ, 01-mini, and 01-preview in MATH benchmarks. - Hint-infer strategically inserts hints during LLM inference to encourage tool use, while Hint-RFT refines these interactions through a scoring, filtering, and modification process followed by fine-tuning, enabling self-aware tool usage. - START is the first open-source tool-integrated long CoT reasoning model, setting a new standard for LLM performance in complex reasoning domains by combining the strengths of long CoT and external tool integration. | ['Question Answering', 'Text2Text Generation', 'Natural Language Processing'] | N/A | N/A |
| [LLM as a Broken Telephone: Iterative Generation Distorts Information](https://arxiv.org/abs/2502.20258) | Michalis Vazirgiannis, guokan-shang, mgeng, amr-mohamed | - This paper investigates how Large Language Models (LLMs) distort information through iterative generation, similar to the "broken telephone" game. - The study uses translation-based experiments, where a document is iteratively translated between English and other languages. - The results reveal that distortion accumulates over time and is influenced by language choice and chain complexity. - While degradation is inevitable, it can be mitigated through temperature control and constrained prompting techniques. - This work raises concerns about the reliability of LLM-generated content in iterative workflows. | ['Natural Language Processing', 'Translation'] | [Link](https://github.com/amr-mohamedd/LLM-as-a-Broken-Telephone) | N/A |
| [EgoLife: Towards Egocentric Life Assistant](https://arxiv.org/abs/2503.03803) | Zzitang, Alarak, fesvhtr, THUdyh, Jingkang | - This paper introduces EgoLife, a project focused on developing an egocentric life assistant using AI-powered wearable glasses. - A new 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset called EgoLife Dataset is collected, featuring six participants living together for a week and wearing Meta Aria glasses. - It also introduces a new question answering benchmark called EgoLifeQA which is based on the dataset that features five types of questions. - An agent system called EgoButler comprising two models, EgoGPT and EgoRAG are introduced to address the EgoLifeQA benchmark. - Experimental results demonstrate that EgoButler outperforms other models on the EgoLifeQA benchmark, showcasing its effectiveness in long-context question answering and multimodal understanding in egocentric scenarios. | ['Multimodal', 'Video-Text-to-Text', 'Question Answering'] | [Link](https://github.com/EvolvingLMMs-Lab/EgoLife) | N/A |
| [LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation](https://arxiv.org/abs/2503.02972) | Vlad Neacs, Lingyi Yang, Simi Hellsten, Karolina Korgul, Jude Khouja | - This paper introduces LINGOLY-TOO, a new benchmark for evaluating linguistic reasoning in Large Language Models (LLMs) designed to mitigate the impact of memorization by using orthographically obfuscated versions of linguistic problems. - The obfuscation process alters the writing systems of problems while preserving the underlying reasoning steps needed for solutions. - Experiments demonstrate that state-of-the-art LLMs, including Claude 3.7 Sonnet and other frontier models struggle with the benchmark's obfuscated problems indicating a gap in genuine reasoning capabilities. - The paper also introduces a framework for creating obfuscated evaluation datasets for other reasoning tasks and provides insights on the effects of data exposure and tokenization on LLMs' performance. - Results of an RCT with 172 participants suggest that obfuscation increases the perceived difficulty for humans, but does not significantly change underlying reasoning process indicating its viability as an evaluation method. | ['Natural Language Processing', 'Question Answering', 'Zero-Shot Classification'] | N/A | [Link](https://huggingface.co/spaces/jkhouja/lingoly-too) |
| [HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization](https://arxiv.org/abs/2503.04598) | Ya Wang, Breeze0417, LLIXQ, Taoer, BryceZhuo | - This paper introduces HybridNorm, a novel hybrid normalization strategy for training deep transformer models that combines the strengths of Pre-Norm and Post-Norm. - HybridNorm applies QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block.  - This design stabilizes training and improves performance, particularly in Large Language Models (LLMs). - Experimental results across various benchmarks, including dense and Mixture-of-Experts (MoE) models, demonstrate that HybridNorm consistently outperforms both Pre-Norm and Post-Norm, showing improved training stability and achieving state-of-the-art performance on downstream tasks. - A variant, HybridNorm*, with a specially-treated first transformer block, further improves training stability and downstream task performance. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/BryceZhuo/HybridNorm) | [Link](https://huggingface.co/datasets/allenai/OLMOE-mix-0924) |
| [FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion](https://arxiv.org/abs/2503.04222) | passerqxj, OnewayLab, GGLS, Wanfq, AALF | - FuseChat-3.0 is a suite of large language models (LLMs) created by fusing heterogeneous source LLMs into more compact target LLMs using implicit model fusion (IMF). - The training process involves Supervised Fine-tuning (SFT) using high-reward responses from source models, followed by Direct Preference Optimization (DPO) using preference pairs from the same source model, eliminating reward bias and variance from heterogeneous sources. - Evaluation across 14 benchmarks shows FuseChat-3.0 significantly outperforming its corresponding target LLMs across diverse tasks including instruction following, general knowledge, mathematics, and coding. - Using Llama-3.1-8B-Instruct as the target model, FuseChat-3.0 achieved a 6.8 point average improvement and state-of-the-art performance gains of 37.1 and 30.1 points on instruction following benchmarks AlpacaEval-2 and Arena-Hard respectively. - The results demonstrate FuseChat-3.0's effectiveness as a scalable framework for enhancing LLM performance through implicit knowledge fusion without requiring architectural modifications or massive computational resources. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/SLIT-AI/FuseChat-3.0) | N/A |
| [Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/abs/2503.04130) | zhiqilinv, MuyangLI, zhijianliu, xiuyul, jdps | - This paper introduces STORM (Spatiotemporal Token Reduction for Multimodal LLMs), a novel architecture for enhancing long-video understanding in multimodal large language models (LLMs). - STORM incorporates a dedicated temporal encoder, based on the Mamba State Space Model, between the image encoder and the LLM to integrate temporal information into image tokens, improving video reasoning and enabling token reduction strategies. - The Mamba layer compresses historical frame information, allowing for temporal and spatial pooling (or test time sampling) for efficient token reduction without discarding crucial details. - Evaluation across various video understanding benchmarks demonstrates state-of-the-art results, with over 5% improvement on MLVU and LongVideoBench, while also reducing computational costs by up to 8× and decoding latency by 2.4-2.9×. - Qualitative analysis reveals that STORM effectively retains crucial visual information despite significant token compression, enabling robust comprehension of complex long-video content. | ['Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval](https://arxiv.org/abs/2503.04644) | Mingsheng Shang, yilunzhao, guo9, songtingyu | - IFIR is a new benchmark designed for evaluating instruction-following information retrieval in specialized domains such as finance, law, healthcare, and science literature. - The benchmark consists of 2,426 instruction-following queries across four expert domains with an average of 6.14 ground-truth passages per query. - A novel LLM-based evaluation metric, INSTFOL, is introduced to measure a retriever’s instruction-following ability.  - Experimental results on 15 state-of-the-art information retrievers reveal that existing models face significant challenges in complex, domain-specific instructions following and LLM-based retrievers show better potential in handling complex retrieval tasks. - The benchmark also confirms that lexical search is a promising method for complex instructions retrieval in specific domains. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/SighingSnow/IFIR) | N/A |
| [Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities](https://arxiv.org/abs/2503.03983) | manocha, rafaelvalle, firecomputer, ZhifengKong, SreyanG-NVIDIA | - This paper introduces Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) designed for enhanced audio understanding and reasoning, particularly with long audio segments (30 seconds to 5 minutes). - AF2 leverages a custom CLAP model trained on a large audio-caption dataset with a novel contrastive loss, synthetic Audio Question Answering (AQA) data (AudioSkills) targeting seven specific reasoning skills, and a 3-stage curriculum learning strategy. - It also introduces LongAudio, a dataset with 263k AQA pairs and 80k audio segments up to 5 minutes long, and LongAudioBench, an expert-validated benchmark derived from LongAudio for evaluating long-audio understanding. - AF2 achieves state-of-the-art performance on over 20 benchmarks, outperforming larger, open-source, and proprietary models despite using only a 3B parameter language model, surpassing even advanced models like Gemini 1.5 Pro in expert-level audio reasoning tasks. - Ablation studies validate the effectiveness of AF2's components, especially the AudioSkills dataset for improving reasoning abilities and the novel contrastive loss for AF-CLAP which shows that quality data plays a more significant role compared to model size. | ['Audio', 'Question Answering', 'Multimodal'] | N/A | N/A |
| [Identifying Sensitive Weights via Post-quantization Integral](https://arxiv.org/abs/2503.01901) | Weiyu Huang, surfingtomchen, jt-zhang, zcliang22, yuezhouhu | - This paper proposes Post-quantization Integral (PQI), a novel sensitivity metric to accurately estimate the influence of each quantized weight in Large Language Models (LLMs), addressing the inaccuracy of existing gradient and Hessian-based metrics. - PQI leverages local continuity and considers both original and quantized weights to calculate sensitivity, enabling fine-grained prediction of quantization's impact on loss function. - The paper introduces ReQuant, a framework utilizing PQI for Dense-and-Sparse decomposition, enhancing quantized model quality via self-adaptive outlier selection and step-wise significant weights detach. - ReQuant improves state-of-the-art post-training quantization methods, showing a 2.66 perplexity gain on Llama 3.2 1B with QTIP and nearly 3% improvement on MATH few-shot accuracy. - Experimental results demonstrate the effectiveness of ReQuant on various quantization methods like AWQ, SqueezeLLM, and QTIP with Llama 3.2 1B and 3B models, enhancing both perplexity and few-shot performance. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling](https://arxiv.org/abs/2503.04725) | Marin Soljačić, Di Luo, Zhuotao Jin, oriolmayne, zhuoc3 | - This paper introduces a bipartite mutual information scaling law, distinct from traditional two-point mutual information, to govern long-range dependencies in natural language and understand long-context language modeling. - It formulates the Long-context Language Modeling (L2M) condition, linking a model's effective long context length modeling capacity to its latent state size scaling for storing past information. - The bipartite mutual information scaling law is validated across different natural language datasets and LLMs (LLaMA, DeepSeek) showing consistent power-law growth. - The L2M condition is empirically verified with transformer and state space models trained on varying sequence lengths. - The study provides a theoretical foundation for improving LLM architecture design for longer context lengths and more effective applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/LSquaredM/mutual_info_scaling_law) | N/A |
| [Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks](https://arxiv.org/abs/2503.04378) | Ellie Evans, Daniel Egert, Jiaqi Zeng, Zhilin Wang, odelalleau | - This paper introduces a novel Feedback-Edit system for enhancing the performance of large language models (LLMs) on open-ended, general-domain tasks, addressing the limitations of existing inference-time scaling techniques that rely on verifiable answers. - The system employs three dedicated models: one generates initial responses, a second provides detailed textual feedback on these responses, and a third edits the responses based on the feedback.  - It leverages a new dataset curated from over 7,000 annotators providing both feedback and edits for diverse prompts, sourced from ShareGPT and WildChat.  - The Feedback and Edit models are trained with supervised fine-tuning and reinforcement learning. - Evaluation on Arena Hard, a challenging benchmark, shows that an optimized Feedback-Edit system using 70B Llama 3 models achieves state-of-the-art performance of 92.7, surpassing OpenAI's ol-preview-2024-09-12 and DeepSeek R1. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer](https://arxiv.org/abs/2503.02495) | Linhui Li, Jing Lian, yjyangwork | - The paper introduces Union-of-Experts (UoE), a novel Mixture-of-Experts (MoE) architecture that decomposes transformers into equivalent expert groups, enabling selective routing on both input data and experts. - UoE enhances expert collaboration through mechanisms like patch-wise data selection and expert selection for improved performance and efficiency. - The architecture consists of Selective Multi-Head Attention (SMHA) for diverse representation learning and Union-of-MLP-Experts (UoME) for dense model integration of activated experts.  - Parallel implementation and hardware optimization further boost efficiency, reducing FLOPs by at least 30% and achieving a speedup of over 2x.  - Experimental results across language modeling, long-range sequence modeling, and image classification tasks demonstrate UoE's superior performance compared to existing state-of-the-art MoEs and efficient transformers. | ['Natural Language Processing', 'Image Classification', 'Computer Vision'] | [Link](https://github.com/YujiaoYang-work/UoE) | N/A |
| [Lost in Literalism: How Supervised Training Shapes Translationese in LLMs](https://arxiv.org/abs/2503.04369) | Leyang Cui, Huajian Zhang, Zhilin Wang, Ronghao Zhang, yaful | - This paper investigates the phenomenon of "translationese" in Large Language Model (LLM) based machine translation, where translations are overly literal and unnatural despite LLMs being trained on vast amounts of text data. - The authors conduct a systematic study involving human evaluation of LLM translations across different domains and language pairs, finding that a significant portion of the translations exhibit translationese. - They analyze translationese prevalence in supervised fine-tuning datasets and find a substantial number of training samples with translationese characteristics, suggesting that supervised training biases LLMs towards literal translation. - Two mitigation strategies are introduced: refining the golden training references using LLMs and filtering out unnatural training instances based on perplexity scores. - Experimental results demonstrate that these strategies significantly reduce translationese and improve translation naturalness, as validated by human evaluations and automatic metrics across multiple LLMs and additional languages. | ['Translation', 'Natural Language Processing'] | [Link](https://github.com/yafuly/LLM_Translationese) | N/A |
| [Understanding and Predicting Derailment in Toxic Conversations on GitHub](https://arxiv.org/abs/2503.02191) | Rebekah Copeland, Robert Zita, kdamevski, rahat-rizvi, imranraad | - This research introduces a novel approach to proactively moderate online discussions on GitHub by predicting conversational derailment, which is when a conversation shifts from productive discourse to negativity and potential toxicity. - The authors curated a dataset of 202 toxic GitHub conversations with annotated derailment points and 696 non-toxic conversations, identifying linguistic and conversational dynamics features unique to toxic conversations. - Leveraging these features and a Least-to-Most (LtM) prompting technique, they developed an approach that generates Summaries of Conversation Dynamics (SCD) to predict if a conversation will derail into toxicity. - This LLM-powered approach, utilizing a two-step prompting process with a large language model (LLaMA-3.1-70B), achieves a 0.69 F1-score and 0.76 precision, significantly outperforming baseline methods like CRAFT and a generic SCD approach. | ['Natural Language Processing', 'Text Classification'] | [Link](https://anonymous.4open.science/r/derailment-oss-replication-C8B1) | N/A |
