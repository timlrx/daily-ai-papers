

## Papers for 2025-03-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Feature-Level Insights into Artificial Text Detection with Sparse
  Autoencoders](https://arxiv.org/abs/2503.03601) | Kristian Kuznetsov, natriistorm, razzant, plina2polina, Kushnareva | - This paper investigates the interpretability of Artificial Text Detection (ATD) using Sparse Autoencoders (SAEs) applied to the residual stream of the Gemma-2-2b model. - The study introduces a categorization of extracted features into discourse, noise, and style features, offering valuable insights into how machine-generated text differs from human-written content. - The authors analyze the semantics and relevance of these features through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. - The results demonstrate that SAE-derived features effectively detect artificial text, sometimes outperforming existing methods, with certain features showing strong generalizability across domains and models, while others exhibit domain- or model-specific performance. - The analysis reveals that modern LLMs have distinct writing styles, particularly in information-dense domains, making their text detectable, but adversarial techniques using less formal prompts can make generated text more human-like and harder to detect. | ['Natural Language Processing', 'Feature Extraction'] | N/A | [Link](https://mgtsaevis.github.io/
mgt-sae-visualization/) |
| [MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale
  Reinforcement Learning](https://arxiv.org/abs/2503.07365) | wangwhcore, friskit, hflqf88888, Cierra0506, FanqingM | - The paper introduces MM-Eureka, a multimodal reasoning model that extends large-scale rule-based reinforcement learning to multimodal reasoning tasks. - MM-Eureka successfully reproduces key characteristics of text-based RL systems in the multimodal space, including steady increases in accuracy, reward, and response length. - The model demonstrates strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showcasing superior data efficiency compared to alternative approaches. - MM-Eureka exhibits "visual aha moments", where the model re-examines intermediate steps using visual information to improve accuracy. - The authors open-source their complete pipeline, including code, models, and data, to facilitate further research in this area. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/ModalMinds/MM-EUREKA) | N/A |
| [SEAP: Training-free Sparse Expert Activation Pruning Unlock the
  Brainpower of Large Language Models](https://arxiv.org/abs/2503.07605) | Xun Liang, BO1022, Ki-Seki, siminniu, UglyToilet | - This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method for Large Language Models (LLMs) that selectively retains task-relevant parameters to reduce inference overhead. - Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model accordingly. - SEAP dynamically adjusts sparsity based on task type, leading to improved efficiency and preserved performance.  - Experimental results show that SEAP significantly reduces computational overhead while maintaining accuracy comparable to the dense model, outperforming existing baselines like WandA and FLAP by over 20% at 50% pruning and incurring only a 2.2% performance drop at 20% pruning. - The method involves constructing task-specific knowledge corpora, analyzing activation patterns, computing neuron importance scores, dynamically distributing sparsity, and applying task-specific pruning strategies. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/IAAR-Shanghai/SEAP) | N/A |
| [Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue
  Learning](https://arxiv.org/abs/2503.07002) | Zongqing Lu, Jiazheng Liu, tellarin, sipeng9527 | - Introduces MMDiag, a multi-turn multimodal dialogue dataset with strong correlations between questions, images, and image regions, generated using rules and GPT assistance. - Presents DiagNote, an MLLM with multimodal grounding and reasoning capabilities, using Deliberate and Gaze modules for Chain-of-Thought and annotation, respectively. - DiagNote demonstrates improved grounding and joint reasoning with vision and language information compared to existing MLLMs. - MMDiag serves as a challenging benchmark for multi-turn multimodal dialogue learning, focusing on saliency tracking and recall. - Empirical results show DiagNote's advantages in handling complex multi-turn dialogues and reasoning tasks. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Automated Movie Generation via Multi-Agent CoT Planning](https://arxiv.org/abs/2503.07314) | Zeyu Zhu, AnalMom, weijiawu | - MovieAgent, a novel framework for automated movie generation, leverages multi-agent Chain of Thought (CoT) planning to automate the creation of multi-scene, multi-shot videos from a script synopsis and character bank. - This hierarchical framework utilizes specialized LLM agents, simulating roles like director, screenwriter, and storyboard artist, to handle high-level narrative structuring and low-level cinematography, ensuring narrative coherence, character consistency, and synchronized subtitles. - MovieAgent introduces a hierarchical CoT reasoning process for automated scene structuring, camera settings, and cinematography, significantly reducing manual effort compared to traditional filmmaking and existing video generation methods. - Experimental results on the MoviePrompts dataset demonstrate MovieAgent's state-of-the-art performance in automated storytelling and movie generation, excelling in narrative coherence and character consistency. - The framework provides new insights into fully automated movie generation and offers a scalable solution for AI-driven storytelling. | ['Text-to-Video', 'Multimodal'] | [Link](https://github.com/showlab/MovieAgent) | N/A |
| [FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA
  Subparameter Updates](https://arxiv.org/abs/2503.07216) | Sung Ju Hwang, matbambbang, Seanie-lee, Sangsang | - FedRand is a privacy-enhanced federated learning framework for vision-language models (VLMs) that involves randomly selecting and updating a subset of LoRA parameters, while keeping the remaining parameters private on the client-side. - It addresses the vulnerability of VLMs to membership inference attacks in federated learning settings by reducing the exposure of client model parameters. - FedRand achieves comparable performance to FedAvg, the oracle method, on several benchmark datasets (ScienceQA, MSCOCO, and NoCaps) for visual question answering and image captioning tasks. - It improves robustness against membership inference attacks compared to other baselines like FedPer and FedPara, which employ partial parameter sharing. - The method reduces communication costs by approximately 25% compared to FedAvg by transmitting only a subset of updated parameters back to the server. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Image Feature Extraction'] | N/A | N/A |
| [FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation
  for Feature Implementation](https://arxiv.org/abs/2503.06680) | Wei Li, lisijia0504, yangyu90, dawnmsg, CharonBony | - FEA-Bench, a benchmark designed to assess Large Language Models' (LLMs) ability to perform incremental development of new features within existing code repositories. - It comprises of 1401 task instances derived from pull requests across 83 GitHub repositories, focusing specifically on adding new components. -  Each task instance includes a feature request, definitions of new components, environment setup details, a patch describing code changes, and corresponding unit test files for verification. - The benchmark tasks involve a significant amount of code modification, with an average of 128.5 lines changed per instance, making it more complex than existing benchmarks. -  Experimental results show that current LLMs struggle with these complex, repository-level tasks, with even the best model, DeepSeek-R1 only achieving a success rate of 9.92%. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/microsoft/FEA-Bench) | N/A |
| [AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via
  Reinforcement Learning and Reasoning](https://arxiv.org/abs/2503.07608) | Qian Zhang, xinggangw, wenyuliu, Atan-0221, rb93dett | - AlphaDrive, a vision-language model (VLM) designed for high-level planning in autonomous driving, is introduced, integrating Group Relative Policy Optimization (GRPO)-based reinforcement learning (RL) with planning reasoning. - The model utilizes four novel GRPO rewards tailored for planning: planning accuracy, action-weighted, planning diversity, and planning format rewards.  - A two-stage training strategy is employed, combining supervised fine-tuning (SFT) for knowledge distillation from larger models with subsequent RL for planning exploration. - On the MetaAD driving dataset, AlphaDrive demonstrates a substantial improvement in planning accuracy, outperforming the SFT-trained model by 25.52% overall and by 35.31% with only 20% of the training data. -  Following RL training, AlphaDrive exhibits emergent multimodal planning capabilities, generating multiple feasible driving plans in complex scenarios, which holds significant potential for enhancing driving safety and efficiency. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/hustvl/AlphaDrive) | N/A |
| [Agent models: Internalizing Chain-of-Action Generation into Reasoning
  models](https://arxiv.org/abs/2503.06580) | Jitao Sang, Xinyan Wen, Jiangming Shu, tzteyang, TokerZ | - This paper introduces AutoCoA, a framework for training Large Agent Models (LAMs) that internalize Chain-of-Action (CoA) generation, allowing them to autonomously decide when and how to use tools. - AutoCoA combines supervised fine-tuning (SFT) and reinforcement learning (RL) to train LAMs to generate CoA by interleaving reasoning and actions, managing environment interactions efficiently. - The framework incorporates step-level action triggering, trajectory-level CoA optimization, and an internal world model to minimize real-world interaction costs. - Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models outperform ReAct-based workflows, especially in tasks requiring long-term reasoning and multiple actions, as evidenced by higher task completion rates. - The paper focuses on smaller reasoning models with search as a testbed, with future research planned for scaling to larger models, integrating more tools, and evaluating on open-ended tasks. | ['Question Answering', 'Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/ADAM-BJTU/AutoCoA) | N/A |
| [WritingBench: A Comprehensive Benchmark for Generative Writing](https://arxiv.org/abs/2503.05244) | SHaopeng Lai, Chenliang Li, Ming Yan, Jiahao Mei, AQuarterMile | - WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. - Proposes a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. - Complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format, and length. - Framework's validity is demonstrated by its data curation capability, enabling 7B-parameter models to approach state-of-the-art (SOTA) performance. - Open-sourced the benchmark, along with evaluation tools and modular framework components. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/X-PLUG/WritingBench) | N/A |
| [Vision-R1: Incentivizing Reasoning Capability in Multimodal Large
  Language Models](https://arxiv.org/abs/2503.06749) | Zheyu Ye, Shaosheng Cao, Zijie Zhai, Bohan Jia, Wenxuan Huang | - This paper introduces Vision-R1, a novel Multimodal Large Language Model (MLLM) designed to enhance reasoning capabilities in visual question answering by integrating cold-start initialization with reinforcement learning. - Vision-R1 leverages a two-stage training process: cold-start initialization using a 200K multimodal Chain-of-Thought (CoT) dataset generated via modality bridging and data filtering, followed by reinforcement learning using Group Relative Policy Optimization (GRPO) and Progressive Thinking Suppression Training (PTST). - PTST progressively loosens context length restrictions during training, enabling Vision-R1 to acquire increasingly complex reasoning processes. - Vision-R1-7B achieves 73.5% accuracy on MathVista, outperforming existing MLLMs with 10x more parameters and approaching OpenAI O1's performance. - The authors also demonstrate the effectiveness of their approach by achieving state-of-the-art results on other math reasoning benchmarks (MathVerse, MM-Math) and general multimodal benchmarks (MM-Star, ChartQA, MME, HallBench). | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Osilly/Vision-R1) | N/A |
| [SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and
  Multi-dimensional Evaluation for Automated Survey Writing](https://arxiv.org/abs/2503.04629) | Bin Wang, Renqiu Xia, Jiakang Yuan, Shiyang Feng, Xiangchao Yan | - SURVEYFORGE is an automated framework for generating survey papers that leverages LLMs, addressing the limitations of existing methods in outline quality and citation accuracy. - It employs a two-stage process: outline generation using heuristic learning from human-written surveys and topic-relevant literature, followed by content generation driven by a memory-driven scholar navigation agent. - The agent retrieves high-quality literature using a temporal-aware reranking engine, combining and refining content into a coherent survey. - A new benchmark, SurveyBench, featuring 100 human-written surveys and multi-dimensional evaluation metrics (reference, outline, and content quality), facilitates systematic assessment of generated surveys. - Experimental results show SURVEYFORGE outperforms the baseline AutoSurvey across all evaluation dimensions, demonstrating improvements in outline structure, reference quality, and content coherence. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Alpha-Innovator/SurveyForge) | [Link](https://huggingface.co/datasets/U4R/SurveyBench) |
| [LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted
  Contrastive Learning](https://arxiv.org/abs/2503.04812) | Jinsong Su, Jie Zhou, Fandong Meng, lqniu, zhibinlan | - LLaVE, a new framework for training large language and vision embedding models, addresses the challenge of overlapping similarity distributions between positive and negative pairs in existing LMM-based embedding models. - This framework utilizes hardness-weighted contrastive learning, assigning larger weights to harder negative pairs, and employs a cross-device negative sample gathering strategy to increase the number of negative pairs without substantial memory overhead. - LLaVE models, trained in various sizes (0.5B, 2B, and 7B), achieve state-of-the-art results on the MMEB benchmark across multiple tasks, including retrieval, visual question answering, and classification. - LLaVE-7B surpasses the previous best model by 6.2 points on the MMEB benchmark, demonstrating strong scalability and efficiency. - Despite being trained solely on image-text data, LLaVE generalizes well to zero-shot text-video retrieval tasks. | ['Multimodal', 'Image Feature Extraction'] | N/A | N/A |
| [MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning](https://arxiv.org/abs/2503.07459) | Jiapeng Chen, Jiwoong Sohn, Daniel Shao, wshi83, RTT1 | - This paper introduces MEDAGENTSBENCH, a new benchmark designed to evaluate complex medical reasoning capabilities of large language models (LLMs) and agent frameworks, focusing on challenging questions requiring multi-step reasoning and diagnosis formulation. - The benchmark addresses limitations of existing evaluations by using adversarially filtered questions from seven established medical datasets, ensuring diversity, and incorporating human annotations to verify reasoning depth. - Experiments reveal that thinking models like DEEPSEEK R1 and OPENAI 03 outperform traditional approaches by 15-25% on complex questions. - Advanced search-based agent methods, like AFLOW, offer the best performance-to-cost ratios compared to traditional approaches, approaching the accuracy of thinking models with fewer computational resources. - Open-source models show competitive performance at lower costs, with DEEPSEEK-R1 demonstrating comparable or superior accuracy to several closed-source alternatives at a fraction of the computational cost. | ['Question Answering'] | [Link](https://github.com/gersteinlab/medagents-benchmark) | N/A |
| [Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive
  Reinforcement](https://arxiv.org/abs/2503.06520) | Fanbin Lu, Zihao Yue, Zhisheng Zhong, Bohao Peng, Yuqi Liu | - Seg-Zero, a novel framework for reasoning segmentation, is introduced, demonstrating emergent test-time reasoning abilities through a pure reinforcement learning (RL) strategy. - Seg-Zero uses a decoupled architecture with a reasoning model (Qwen2.5-VL) and a segmentation model (SAM2), where the reasoning model generates a reasoning chain and positional prompts (bounding box and points) for the segmentation model to produce pixel-level masks. - The model is trained using GRPO with a sophisticated reward mechanism that integrates format and accuracy rewards to enhance the reasoning process and regulate outputs. - Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities, surpassing the previous LISA-7B by 18% on ReasonSeg, achieving 57.5% zero-shot. - This improvement highlights Seg-Zero's ability to generalize across domains while presenting explicit reasoning. | ['Image Segmentation', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/dvlab-research/Seg-Zero) | N/A |
| [This Is Your Doge, If It Please You: Exploring Deception and Robustness
  in Mixture of LLMs](https://arxiv.org/abs/2503.05856) | Ilija Bogunovic, Sangwoong Yoon, Llwo | - This paper investigates the robustness of Mixture of LLM Agents (MoA) architectures to deceptive agents providing misleading responses. - The study uses the AlpacaEval 2.0 question answering benchmark and the QUALITY multiple-choice comprehension task to uncover vulnerabilities. - Results show that even a single malicious agent can significantly degrade performance, negating the gains of using MoA and dropping accuracy to near-baseline levels. - Various factors influencing vulnerability, such as the number and location of deceptive agents, aggregator model size, and information access, are examined. - Inspired by the Doge of Venice voting system, several unsupervised defense mechanisms are proposed to mitigate the impact of deceptive agents and recover performance. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/lorenzflow/robust-moa) | N/A |
| [State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for
  State Space Models](https://arxiv.org/abs/2503.03499) | Hyung Il Koo, Minjae Lee, Yuchen Zeng, Kevin Galim, Wonjun Kang | - This paper introduces state-based Parameter-Efficient Fine-Tuning (PEFT) methods for State Space Models (SSMs), proposing a new method called State-offset Tuning. - State-offset Tuning directly adjusts state-related features within the SSM at each time step, offering a more effective adaptation strategy compared to prompt-based methods that rely on external virtual tokens and suffer from diminishing influence over time. - State-offset Tuning inserts a constant, learnable state-offset to the hidden state before output generation, mitigating the inconsistent effects of the time-varying coefficients present in existing methods like Initial State Tuning.  - The paper demonstrates State-offset Tuning's effectiveness through extensive experiments on various NLU and NLG datasets, including GLUE, SAMSum, Spider, and DART, using pre-trained Mamba and Mamba-2 models. - Experimental results show that State-offset Tuning consistently outperforms other PEFT methods, including prompt-based and parameter-based approaches, achieving performance comparable to full fine-tuning while using significantly fewer parameters. | ['Natural Language Processing', 'Summarization', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/furiosa-ai/ssm-state-tuning) | [Link](https://huggingface.co/datasets/nyu-mll/glue), [Link](https://huggingface.co/datasets/Samsung/samsum), [Link](https://huggingface.co/datasets/xlangai/spider), [Link](https://huggingface.co/datasets/Yale-LILY/dart), [Link](https://huggingface.co/state-spaces/mamba-{130m,1.4b,2.8b}), [Link](https://huggingface.co/state-spaces/mamba2-{130m,1.3b}) |
| [Should VLMs be Pre-trained with Image Data?](https://arxiv.org/abs/2503.07603) | Igor Vasiljevic, Kushal Arora, Samir Yitzhak Gadre, Jean Mercat, Sedrick Keh | - This paper investigates the impact of incorporating image data during pre-training of Vision-Language Models (VLMs), challenging the conventional two-stage training approach. - The authors train a suite of 300 models with varying scales, datasets, image-text ratios, and pre-training lengths, finding that introducing visual data during the "cooldown" phase of text pre-training, specifically at 80% completion, yields superior performance on vision-language tasks compared to adding images after full text pre-training. - An optimal image-to-text ratio of 10-20% is identified for 1B parameter models during the image-text pre-training phase, with varying optimums depending on the scale of the model. - Fine-tuning on instruction data further improves performance, with 2-4 epochs achieving a balance between vision and text task performance. - On a suite of six diverse tasks, introducing visual tokens at 80% of pre-training for a 1B model leads to a 2% average improvement compared to adding visual tokens after full pre-training, demonstrating the benefit of the proposed integrated approach. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text', 'Natural Language Processing'] | [Link](https://github.com/TRI-ML/vlm-evaluation/) | [Link](https://huggingface.co/TRI-ML/DCLM-1B), [Link](https://huggingface.co/meta-llama/Llama-3.2-1B), [Link](https://huggingface.co/apple/DCLM-7B) |
| [ProBench: Judging Multimodal Foundation Models on Open-ended
  Multi-domain Expert Tasks](https://arxiv.org/abs/2503.06885) | Liu Liu, Bei Chen, Haoning Wu, dxli1, HelloKKMe | - ProBench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on open-ended, expert-level tasks requiring professional knowledge and reasoning. - It contains 4,000 expert-designed samples spanning 10 professional fields and 56 sub-fields, supporting 17 languages and conversations with up to 13 turns. - Evaluations of 24 leading MLLMs using ProBench and an MLLM-as-a-Judge reveal significant challenges in visual perception, textual understanding, domain knowledge, and advanced reasoning. - The best open-source models show competitive performance with proprietary models, highlighting the progress and remaining challenges in multimodal AI research. - A distilled version of Llama-vision is provided for efficient local evaluation of MLLMs. | ['Multimodal'] | N/A | N/A |
| [Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by
  Learning Language-Agnostic Speech Representations](https://arxiv.org/abs/2503.06273) | Yong Man Ro, Stavros Petridis, Chae Won Kim, Minsu Kim, JeongHun0716 | - This paper introduces Zero-AVSR, a zero-shot audio-visual speech recognition (AVSR) framework capable of recognizing speech in languages it hasn't been explicitly trained on. - Zero-AVSR leverages an Audio-Visual Speech Romanizer (AV-Romanizer) which predicts language-agnostic pronunciations (Roman text) from audio-visual speech, and a Large Language Model (LLM) to convert the Roman text into language-specific graphemes. - The authors introduce a Multilingual Audio-Visual Romanized Corpus (MARC) of 2,916 hours of data across 82 languages including both language-specific and romanized transcriptions to train the system. - Experiments demonstrate the effectiveness of Zero-AVSR on unseen languages, outperforming a baseline zero-shot model and achieving competitive performance with existing multilingual models on seen languages. - It is also shown that Zero-AVSR improves noise robustness compared to audio-only approaches and demonstrates the ability to process data from language families unseen during training. | ['Automatic Speech Recognition', 'Multimodal'] | N/A | N/A |
| [Words or Vision: Do Vision-Language Models Have Blind Faith in Text?](https://arxiv.org/abs/2503.02199) | Bryan Hooi, Tri Cao, Ailin Deng, ryanchen42 | - This paper investigates the modality preference of Vision-Language Models (VLMs) when faced with inconsistencies between visual and textual data in vision-centric tasks. - The study reveals a "blind faith in text" phenomenon, where VLMs disproportionately trust textual information even when it contradicts visual evidence, leading to significant performance degradation under corrupted text. - Analysis of ten VLMs across four vision-centric tasks reveals that instruction prompts and language model size have limited impact on mitigating text bias, while text relevance and token order can exacerbate it. - Supervised fine-tuning with text augmentation is shown to effectively reduce text bias. - A theoretical analysis suggests the "blind faith in text" may stem from an imbalance of pure text and multi-modal data during VLM training. | ['Multimodal', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/d-ailin/blind-faith-in-text) | N/A |
| [Detection Avoidance Techniques for Large Language Models](https://arxiv.org/abs/2503.07595) | Gabi Dreo Rodosek, Joao A. G. Schneider, Florian Steuber, SinclairSchneider | - This paper explores techniques to evade detection by large language model (LLM) classifiers, including shallow detectors, transformer-based detectors, and zero-shot detectors. - The authors investigate the effects of temperature, sampling methods, and model size on detection rates using a Naive Bayes classifier with Bag-of-Words features. - Reinforcement learning with constraints is employed to guide LLMs in generating text that evades transformer-based detectors while maintaining linguistic quality and coherence. - A novel paraphrasing model, trained on a dataset of masked and permuted LLM-generated text, is introduced to minimize detectability while maximizing content similarity to the original text. The model outperforms general-purpose paraphrasing models in detection evasion tasks. - The study highlights the potential for malicious actors to circumvent LLM detection and calls for further research into robust detection mechanisms and the ethical implications of undetectable LLM-generated text. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Link](https://huggingface.co/datasets/google-research-datasets/natural_questions), [Link](https://huggingface.co/kalpeshk2011/dipper-paraphraser-xxl), [Link](https://huggingface.co/Qwen/Qwen1.5-4B) |
| [Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge
  Reasoning](https://arxiv.org/abs/2503.04973) | Fabio Petroni, Orion Weller, papotti, giulio98 | - This paper proposes task-aware key-value (KV) cache compression, a novel technique to enhance large language models (LLMs) ability to perform knowledge reasoning by compressing external knowledge into a compact representation suitable for zero- or few-shot learning. - The approach outperforms both Retrieval-Augmented Generation (RAG) and task-agnostic compression methods, achieving up to 7 point improvement in accuracy on LongBench v2 with a 30x compression rate while also reducing inference latency. - Unlike query-aware compression which requires recompression per query, the task-aware compression precomputes a cache for a wider task context, enabling efficient and reusable caching. - On a synthetic dataset and Longbench v2, task-aware compression is shown to excel in tasks requiring broad knowledge synthesis where RAG struggles.  - This technique demonstrates the potential of KV cache compression for scaling LLM reasoning beyond traditional retrieval-based methods. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [RePO: ReLU-based Preference Optimization](https://arxiv.org/abs/2503.07426) | Jinyang Gao, Xue Wang, Kexin Huang, Junkang Wu, xiangwang1223 | - This paper introduces ReLU-based Preference Optimization (RePO), a streamlined algorithm for aligning Large Language Models (LLMs) with human preferences using offline data. - RePO simplifies existing methods like DPO and SimPO by eliminating the hyperparameter β and using a ReLU-based max-margin loss, requiring only a single hyperparameter (γ) for tuning.  - It retains the reference-free reward margins of SimPO while using a ReLU activation for these margins.  -  Evaluations on AlpacaEval 2 and Arena-Hard across multiple LLMs show RePO matches or outperforms DPO and SimPO, demonstrating competitive performance with reduced complexity. - The ReLU-based max-margin loss in RePO acts as the convex envelope of the 0-1 loss which enables tractable gradient-based optimization while preserving the properties of global optimality. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/junkangwu/REPO) | [Link](https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback-armorm), [Link](https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm) |
| [Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal
  LLMs](https://arxiv.org/abs/2503.06362) | Stavros Petridis, Minsu Kim, Umberto Cappellazzo | - This paper introduces Llama-MTSK, a Matryoshka-based Multimodal Large Language Model (MLLM) for Audio-Visual Speech Recognition (AVSR). - Llama-MTSK encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels and enabling flexible adaptation of audio-visual token allocation based on specific computational constraints.  - Three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules are introduced for efficient fine-tuning of the LLM. - Evaluations on the LRS2 and LRS3 datasets show that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels for ASR, VSR, and AVSR tasks. | ['Audio', 'Automatic Speech Recognition', 'Multimodal'] | N/A | N/A |
| [Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent
  Spaces](https://arxiv.org/abs/2503.05283) | Qixing Huang, Diego Gomez, Luca Moschella, Souhail Hadgi, teelinsan | - This paper investigates aligning the latent spaces of pre-trained 3D and text encoders without joint training, a task not previously explored in depth. - It reveals that unimodal 3D encoders, when trained independently, exhibit weak alignment with text representations compared to image-text counterparts. - The paper introduces a novel approach combining Canonical Correlation Analysis (CCA) for subspace selection and existing alignment methods (affine transformation and local Centered Kernel Alignment) in the reduced space to align 3D and text feature spaces.  - Experimental results demonstrate significant improvements in matching and retrieval tasks by projecting representations onto these shared subspaces, outperforming existing alignment methods applied directly to the full latent spaces. - Further analysis reveals a complementary structure between the subspaces and original feature spaces, suggesting a division between geometric and semantic information within the learned representations. | ['Multimodal', 'Text-to-3D'] | N/A | N/A |
