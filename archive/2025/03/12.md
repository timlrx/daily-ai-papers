

## Papers for 2025-03-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural
  Vision-Language Dataset for Southeast Asia](https://arxiv.org/abs/2503.07920) | davidanugraha, rifqifarhansyah, tackhwa, holylovenia, samuelcahyawijaya | - SEA-VL is a new open-source initiative focused on developing high-quality, culturally relevant vision-language datasets for Southeast Asian languages, addressing the underrepresentation of these languages in AI research. - This initiative involves three image collection strategies: crowdsourcing from individuals within SEA, crawling existing online image sources, and generating synthetic images using AI models. - SEA-VL analyzed the trade-offs between manual and automated data collection methods and found that image crawling offered a good balance of cultural relevance (~85%) and efficiency. - While crowdsourcing yielded the most culturally relevant data, it was the most time-consuming and resource-intensive method. Image generation proved inadequate for capturing the cultural nuances of SEA.  - The resulting SEA-VL dataset is the largest of its kind for the region, exceeding existing datasets by more than 50 times and aiming to facilitate more culturally aware and inclusive AI systems. | ['Image-to-Text', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/SEACrowd/sea-vl-experiments) | [Link](https://huggingface.co/collections/SEACrowd/sea-vl-multicultural-vl-dataset-for-southeast-asia-67cf223d0c341d4ba2b236e7) |
| [LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through
  Two-Stage Rule-Based RL](https://arxiv.org/abs/2503.07536) | Jie Liu, Zhiyuan You, Miaosen Zhang, Gongrui Zhang, Yingzhe Peng | - LMM-R1 is a two-stage framework designed to improve the multimodal reasoning capabilities of Large Multimodal Models (LMMs), especially those with limited parameters (e.g., 3B), using rule-based reinforcement learning (RL). - The first stage, Foundational Reasoning Enhancement (FRE), uses text-only data with rule-based RL to bolster the model's core reasoning skills. - The second stage, Multimodal Generalization Training (MGT), generalizes this enhanced reasoning to multimodal tasks.  - Experiments on the 3B parameter Qwen2.5-VL-Instruct model show average improvements of 4.5% and 4.83% on text-only and multimodal benchmarks, respectively, demonstrating the framework's effectiveness. - Notably, LMM-R1 achieves a 3.63% performance gain on complex Football Game tasks, further showcasing its ability to improve real-world applicable reasoning skills. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/TideDra/lmm-r1) | N/A |
| [YuE: Scaling Open Foundation Models for Long-Form Music Generation](https://arxiv.org/abs/2503.08638) | HKUST-Audio, Liam-Liu, dododododo, zhangysk, a43992899 | - YuE is a family of open-source foundation models based on LLaMA2 for generating high-quality, long-form (up to 5 minutes) music from lyrics and control signals. - It uses a track-decoupled next-token prediction strategy, modeling vocal and accompaniment tracks separately, and structural progressive conditioning to handle long lyrical sequences, addressing coherence and alignment challenges. - A novel in-context learning framework allows versatile style transfer, voice cloning, and bidirectional content creation. - Human evaluations show YuE matches or surpasses some commercial systems in musicality and controllability, particularly excelling in vocal agility and duration. - YuE achieves state-of-the-art performance on the GS key recognition task within the MARBLE benchmark and shows competitive results in other music understanding tasks, demonstrating the quality of its learned representations. | ['Text-to-Audio', 'Audio'] | [Link](https://github.com/multimodal-art-projection/YuE) | N/A |
| [UniF^2ace: Fine-grained Face Understanding and Generation
  with Unified Multimodal Models](https://arxiv.org/abs/2503.08120) | Liya Guo, Linrui Xu, Xuerui Qiu, delinqu, tulvgengenr | - UniF^2ace is a unified multimodal model (UMM) designed for fine-grained face understanding and generation, addressing limitations of existing models that handle coarse facial attributes or treat understanding and generation as separate tasks. - It leverages a novel dual discrete diffusion (D3Diff) training strategy, connecting score matching and masked generative models, and a multi-level grouped Mixture-of-Experts (MoE) architecture for fine-grained facial feature processing. - A new dataset, UniF^2ace-130K, containing 130K facial image-text pairs and one million visual question-answering (VQA) pairs spanning 46 attributes, is introduced to support the model's training and evaluation. - Experimental results on UniF^2ace-130K show that UniF^2ace outperforms existing UMMs and generative models on various metrics for both understanding and generation, achieving state-of-the-art performance for models of similar size. - Qualitative analysis demonstrates UniF^2ace's ability to capture fine-grained facial details from text, generating more realistic face images and providing accurate descriptions for complex facial attributes. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by
  Imitating Human Annotator Trajectories](https://arxiv.org/abs/2503.08625) | Qingpei Guo, Chunluan Zhou, Hao Chen, Yuzhuo Tian, Z-MU-Z | - This paper introduces SegAgent, a new approach for image segmentation that leverages Multimodal Large Language Models (MLLMs) by mimicking human annotators using interactive segmentation tools. - The proposed method models the segmentation task as a multi-step Markov Decision Process, enabling MLLMs to iteratively generate text-based click points to refine segmentation masks. - SegAgent achieves performance comparable to state-of-the-art methods on referring segmentation datasets and also supports additional tasks like mask refinement and annotation filtering. - A new dataset, High-quality Referring Expression Segmentation (HRES), is also introduced in this work, to evaluate the decision making capabilities of MLLMs on more complex dataset. - The paper further explores enhancement techniques like policy improvement with StaR+ and tree search with process reward modeling (PRM), which further improve SegAgent's performance, especially in complex segmentation scenarios. | ['Image Segmentation', 'Multimodal'] | [Link](https://github.com/aim-uofa/SegAgent) | N/A |
| [Gemini Embedding: Generalizable Embeddings from Gemini](https://arxiv.org/abs/2503.07891) | Madhuri Shanbhogue, Daniel Cer, Sahil Dua, Feiyang Chen, Jinhyuk Lee | - This paper introduces Gemini Embedding, a new embedding model initialized from Google's Gemini large language model and trained on a diverse set of embedding tasks. - Gemini Embedding leverages Gemini's multilingual and code understanding capabilities to generate generalizable embeddings for various text modalities and over 100 languages. - It outperforms state-of-the-art models on the Massive Multilingual Text Embedding Benchmark (MMTEB), achieving a score of 68.32, a +5.09 improvement over the second-best model. - It also demonstrates exceptional performance on other benchmarks like XOR-Retrieve for cross-lingual retrieval. - The model uses a contrastive learning objective and incorporates task prompts and a pre-finetuning stage to enhance performance and is made available publicly via an API. | ['Natural Language Processing', 'Sentence Similarity', 'Feature Extraction'] | N/A | [Link](https://huggingface.co/spaces/mteb/leaderboard) |
| [Implicit Reasoning in Transformers is Reasoning through Shortcuts](https://arxiv.org/abs/2503.07604) | Deqing Yang, Siyu Yuan, Tianhe Lin, hsaest | - This paper investigates the implicit reasoning mechanism in Transformers, revealing that they rely on shortcuts, especially when trained on fixed-pattern data. - These shortcuts are effective for in-domain and out-of-domain generalization when the premise order is fixed, achieving near-perfect accuracy. - However, when trained on data with unfixed premise order, the model overfits to shortcut patterns, failing to generalize and exhibiting poor performance, specifically struggling with "Variable as Subtrahend Plight." - The paper suggests that current LLMs' implicit reasoning capability is limited by their reliance on shortcuts rather than true step-by-step reasoning. - This limitation is observed in state-of-the-art large language models (LLMs) as well, emphasizing the need for future research to develop methods that encourage true reasoning capabilities in LMs. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [OmniMamba: Efficient and Unified Multimodal Understanding and Generation
  via State Space Models](https://arxiv.org/abs/2503.08686) | Xinggang Wang, Wenyu Liu, Qian Zhang, Bencheng Liao, Jialv Zou | - OmniMamba is a unified multimodal model based on a linear state-space model (Mamba-2), enabling both understanding and generation tasks (including text-to-image) with a single model. - It uses decoupled encoders, vocabularies, and task-specific LoRA modules for parameter-efficient adaptation to different modalities and tasks. - Trained on only 2M image-text pairs, OmniMamba achieves competitive performance with JanusFlow and surpasses Show-o on various multimodal benchmarks, using 1000x less training data than Show-o. - Notably, OmniMamba demonstrates exceptional inference speed, achieving up to a 119.2x speedup and a 63% GPU memory reduction for long sequences compared to Transformer-based models. - On MS-COCO, it achieves state-of-the-art visual generation results (FID 5.50) compared to other unified and generation-specific models. | ['Multimodal', 'Text-to-Image', 'Visual Question Answering'] | [Link](https://github.com/hustvl/OmniMamba) | N/A |
| [Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.07572) | Edward Emanuel Beeching, Lewis Tunstall, Amrith Setlur, Matthew Y. R. Yang, CohenQu | - This paper introduces Meta Reinforcement Fine-Tuning (MRT), a new method for optimizing large language models (LLMs) to efficiently utilize test-time compute for improved reasoning. - MRT frames the optimization problem as a meta-reinforcement learning problem, where the LLM's output stream is segmented into episodes, enabling a principled perspective on resource allocation. - By minimizing cumulative regret, a measure of the difference between the LLM's performance and an optimal comparator, MRT balances exploration and exploitation in the output token sequence, leading to efficient progress. -  MRT prescribes a dense reward bonus during training, quantifying progress by the change in likelihood of eventual success after each generated episode. - Experiments on math reasoning tasks using DeepScaleR, DeepSeek, and Llama models demonstrate that MRT achieves 2-3x relative performance gains and 1.5-1.7x improvements in token efficiency compared to standard outcome-reward RL fine-tuning. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [BiasEdit: Debiasing Stereotyped Language Models via Model Editing](https://arxiv.org/abs/2503.08588) | Julian McAuley, Ningyu Zhang, Wei Xu, XinXuNLPer | - BIASEDIT, a novel model editing method, is proposed to mitigate stereotypical biases in language models by using lightweight editor networks to generate parameter updates, focusing on local edits of partial parameters.  - The architecture employs a debiasing loss to guide these edits, along with a retention loss to preserve general language modeling capabilities. - Experiments on StereoSet and Crows-Pairs demonstrate that BIASEDIT outperforms existing debiasing methods by achieving lower Stereotype Scores (SS) while minimizing impact on Language Modeling Scores (LMS).  - BIASEDIT exhibits robustness to gender reversal and semantic generality, indicating broader applicability.  - Further analysis shows that edits to upper model blocks have fewer negative impacts on modeling abilities compared to edits on lower blocks. | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/BiasEdit) | [Link](https://huggingface.co/openai-community/gpt2-medium), [Link](https://huggingface.co/google/gemma-2b), [Link](https://huggingface.co/mistralai/Mistral-7B-v0.3), [Link](https://huggingface.co/meta-llama/Meta-Llama-3-8B) |
| [^RFLAV: Rolling Flow matching for infinite Audio Video generation](https://arxiv.org/abs/2503.08307) | Claudio Ferrari, Tomaso Fontanini, Filippo Botti, Giuseppe Gabriele Tarollo, MaverickAlex | - RFLAV, a transformer-based model for generating infinite-length audio-video (AV) sequences, is introduced, addressing key challenges in AV generation such as quality, synchronization, and temporal coherence. - The model uses a rolling flow matching framework and novel cross-modality interaction modules to align audio and video during training. - It bypasses the need for audio/video encoders, enabling variable-length video generation and avoiding restrictions imposed by fixed encoder output sizes. - Experimental results show that RFLAV outperforms existing state-of-the-art models on standard AV generation benchmarks like AIST++ and Landscape. - Further analysis demonstrates RFLAV's ability to maintain quality and avoid repetitive loops in extended video sequences, making it a significant advancement in the field of infinite AV generation. | ['Text-to-Video', 'Audio', 'Multimodal'] | [Link](https://github.com/ErgastiAlex/R-FLAV) | N/A |
| [QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long
  Video Comprehension](https://arxiv.org/abs/2503.08689) | Shukang Yin, Weizhong Huang, Xiawu Zheng, Wang Chen, Yongdong Luo | - QuoTA, a training-free modular extension for Large Video-Language Models (LVLMs), enhances long video comprehension through query-oriented visual token assignment. - QuoTA strategically assesses frame-level importance based on query relevance using a lightweight scoring LVLM, enabling efficient token allocation before cross-modal interactions. - Using Chain-of-Thoughts reasoning, QuoTA decouples complex queries into more interpretable questions for enhanced frame scoring precision by the scoring LVLM.  - QuoTA with LLaVA-Video-7B achieves an average 3.2% performance improvement across six benchmarks, including Video-MME and MLVU, while maintaining the same token budget as the baseline.  - QuoTA also consistently outperforms recent state-of-the-art token reduction methods (AIM and FrameFusion) across various token budget configurations. | ['Video-Text-to-Text', 'Multimodal', 'Computer Vision', 'Video Classification'] | [Link](https://github.com/MAC-AutoML/QuoTA) | N/A |
| [Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents](https://arxiv.org/abs/2503.08684) | Xiao Zhang, Liang Pang, Haiyuan Zhao, Sunhao Dai, Haoyu Wang | - This paper introduces Causal Diagnosis and Correction (CDC), a novel debiasing method for Pretrained Language Model (PLM)-based retrievers to address source bias, which is the tendency of these retrievers to favor LLM-generated content due to its lower perplexity. - The study identifies perplexity as a causal factor contributing to source bias, confirmed through intervention experiments and two-stage least squares regression analysis demonstrating that lower perplexity leads to higher relevance scores, irrespective of semantic quality. - A theoretical analysis of Masked Language Modeling (MLM) and retrieval tasks reveals a positive correlation in their gradients, indicating that retrievers inadvertently incorporate perplexity during relevance estimation. - This positive correlation explains the observed trade-off between retrieval performance and source bias, where higher performance correlates with increased bias due to greater sensitivity to perplexity. - CDC operates at inference time, calibrating relevance scores by separating the biased influence of perplexity and shows robust debiasing effectiveness across diverse datasets and LLMs without requiring retriever retraining. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/WhyDwelledOnAi/Perplexity-Trap) | N/A |
| [Benchmarking AI Models in Software Engineering: A Review, Search Tool,
  and Enhancement Protocol](https://arxiv.org/abs/2503.05860) | Maliheh Izadi, philippedebekker, RohamKoohestani | - This paper introduces BenchFrame, a unified method to enhance benchmark quality for AI4SE models, along with BenchScout, a semantic search tool to find relevant benchmarks. - The researchers conducted a systematic review of 173 studies, identifying 204 AI4SE benchmarks, classifying them, and analyzing their limitations. - A user study with 22 participants demonstrated BenchScout's usability, effectiveness, and intuitiveness, with average scores of 4.5, 4.0, and 4.1 out of 5, respectively. - Applying BenchFrame to HumanEval resulted in HumanEvalNext, addressing limitations such as errors, language conversion, test coverage, and difficulty. - Evaluating ten state-of-the-art code language models showed a pass@1 score reduction of 31.22% and 19.94% on HumanEvalNext compared to HumanEval and HumanEvalPlus. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering', 'Summarization', 'Translation'] | [Link](https://github.com/AISE-TUDelft/AI4SE-benchmarks) | [Link](https://huggingface.co/datasets/openai/openai_humaneval), [Link](https://huggingface.co/datasets/codeparrot/instructhumaneval) |
| [Evaluating Intelligence via Trial and Error](https://arxiv.org/abs/2502.18858) | Bo Zhang, Yiqun Liu, Jiayu Li, Jiahao Zhao, jingtao | - This paper introduces "Survival Game", a framework inspired by Natural Selection to evaluate intelligence based on the number of failed attempts in a trial-and-error process. - The framework categorizes intelligence into three levels: Limited, Capable, and Autonomous, based on the convergence of the expectation and variance of failure counts. - Through comprehensive evaluation of existing AI systems on various tasks including vision, search, recommendation, and language, the study finds that while AI reaches the Autonomous Level in simple tasks, it falls short in complex ones, often remaining at the Limited Level. - The paper projects that achieving Autonomous Level for general tasks would require an astronomical 10^26 parameters, highlighting the gap between current AI and human intelligence. - A theoretical analysis suggests that human tasks possess "criticality," demanding deep understanding of underlying mechanisms, which current AI systems, relying on superficial mimicry, lack, thus explaining the difficulty in achieving Autonomous Level. | ['Computer Vision', 'Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/jingtaozhan/IntelligenceTest) | N/A |
| [Referring to Any Person](https://arxiv.org/abs/2503.08507) | Yuda Xiong, Tianhe Ren, Zhaoyang Zeng, Lin Wu, Qing Jiang | - This paper introduces RexSeek, a novel detection-oriented multimodal large language model for the task of "referring to any person," which involves detecting all individuals in an image matching a given natural language description. - RexSeek integrates a person detector for robust perception and Qwen2.5 as the large language model (LLM) for enhanced language comprehension. - The model is trained using a four-stage approach, including image-captioning, detection-oriented data, multimodal data, and finally, the HumanRef dataset. - Experimental results on the HumanRef benchmark show that RexSeek outperforms existing state-of-the-art models, particularly in multi-instance referring scenarios where multiple individuals match the description. - RexSeek also demonstrates generalization capabilities for referring to arbitrary objects beyond human-centric tasks, highlighting its potential for broader applications in vision-language tasks. | ['Object Detection', 'Multimodal'] | [Link](https://github.com/IDEA-Research/RexSeek) | N/A |
| [AI-native Memory 2.0: Second Me](https://arxiv.org/abs/2503.08102) | Jingbo Shang, Felix Tao, Tao Gao, Xiang Ying, Jiale Wei | - SECOND ME is introduced as an AI-native, persistent memory offload system designed to enhance human-computer interaction by reducing redundant information exchange. - SECOND ME leverages LLM-based memory parameterization for structured organization, contextual reasoning, and adaptive knowledge retrieval, acting as a personalized intermediary. - The system employs supervised fine-tuning (SFT) and direct preference optimization (DPO) to improve LLM performance on tasks such as memory-based Q&A, context completion, and context critique. - An automated data synthesis strategy integrates local and global data perspectives using a multi-agent framework and Chain-of-Thought (CoT) reasoning for enhanced performance. - Experimental results demonstrate that diverse data sources with strong CoT normalization and DPO lead to significant performance improvements in automated evaluations, with human case studies suggesting even greater real-world effectiveness. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/Mindverse/Second-Me) | N/A |
| [Mixture of Experts Made Intrinsically Interpretable](https://arxiv.org/abs/2503.07639) | Puneet K. Dokania, Christian Schroeder de Witt, Ashkan Khakzar, Constantin Venhoff, Xingyi Yang | - This paper introduces MoE-X, a Mixture-of-Experts (MoE) language model designed for intrinsic interpretability by leveraging sparsity and width in the model architecture. - MoE-X consists of ReLU experts and employs sparsity-aware routing, ensuring only the most relevant and interpretable experts are activated during inference. - Evaluating MoE-X on chess and natural language tasks demonstrates that it maintains performance comparable to dense models while enhancing interpretability. - Notably, MoE-X surpasses the interpretability of sparse autoencoder (SAE) methods without sacrificing performance and achieves perplexity better than GPT-2 on language modeling tasks. - MoE-X successfully disentangles polysemantic features and clusters related concepts within individual experts, offering a more transparent and interpretable model. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/adamkarvonen/chess_games), [Link](https://github.com/EleutherAI/sae-auto-interp) |
| [Beyond Decoder-only: Large Language Models Can be Good Encoders for
  Machine Translation](https://arxiv.org/abs/2503.06594) | Qinghong Zhang, Bei Li, Yongyu Mu, Tong Zheng, luoyingfeng | - This paper introduces LaMaTE (Large Language Models as Machine Translation Encoders), a novel architecture for machine translation that utilizes LLMs as encoders coupled with a lightweight NMT decoder. - LaMaTE incorporates an adaptor module to bridge the gap between the LLM encoder and the NMT decoder, enhancing representation alignment and facilitating training. - A two-stage training process is proposed, where the adaptor and decoder are pre-trained initially, followed by fine-tuning of all model parameters, enabling efficient learning and knowledge retention. - A new comprehensive benchmark dataset, ComMT, is introduced to evaluate machine translation models across various tasks, including general translation, document-level translation, domain-specific translation, terminology-constrained translation, and automatic post-editing. - Experimental results demonstrate that LaMaTE achieves state-of-the-art performance on the ComMT benchmark, showing significant improvements in both translation quality and efficiency, with 2.4x to 6.5x faster decoding speeds and a 75% reduction in KV cache memory compared to traditional LLM-based methods. | ['Translation', 'Natural Language Processing'] | N/A | [Link](https://huggingface.co/NiuTrans/LaMaTE) |
| [VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large
  Vision-Language Models in Fact-Seeking Question Answering](https://arxiv.org/abs/2503.06492) | Lixin Liu, Shasha Guo, Xiaodong Chen, Yihan Zhao, WYLing | - This paper introduces VisualSimpleQA, a multimodal fact-seeking question answering benchmark designed for decoupled evaluation of visual and linguistic modules in large vision-language models (LVLMs). - The benchmark facilitates detailed analysis of LVLMs by including text-only questions paired with multimodal questions derived from images, and rationales and difficulty criteria for improved evaluation. - The authors evaluate 15 state-of-the-art LVLMs and show that even top-performing models like GPT-40 achieve only around 60% accuracy on VisualSimpleQA and 30% on a harder subset (VisualSimpleQA-hard), indicating significant room for improvement.  - The decoupled evaluation process reveals substantial performance differences across LVLMs, particularly in their visual recognition capabilities. - VisualSimpleQA aims to promote research and development in the field of LVLMs to enhance factuality in multimodal question answering. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/WYLing/VisualSimpleQA) |
