

## Papers for 2025-03-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Block Diffusion: Interpolating Between Autoregressive and Diffusion
  Language Models](https://arxiv.org/abs/2503.09573) | Zhixuan Qi, Zhihan Yang, Justin T Chiu, Aaron Gokaslan, Marianne Arriola | - This paper introduces Block Discrete Denoising Diffusion Language Models (BD3-LMs), a new class of language models that interpolate between autoregressive and diffusion models. - BD3-LMs address limitations of both approaches by generating blocks of text autoregressively, while using diffusion models within each block, which supports variable-length generation, KV caching, and parallel sampling. - The authors propose a recipe for training BD3-LMs, including efficient training algorithms, estimators of gradient variance, and data-driven noise schedules designed to minimize this variance and improve training stability. - Experiments on language modeling benchmarks show BD3-LMs achieve state-of-the-art perplexity among diffusion models and match autoregressive models in perplexity given appropriate noise schedules. - BD3-LMs generate arbitrary length sequences, outperforming other diffusion models on this task. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/kuleshov-group/bd3lms) | N/A |
| [GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based
  VLM Agent Training](https://arxiv.org/abs/2503.08525) | Zongqing Lu, Yuanchun Shi, Junliang Xing, Yijun Yang, Tong Wei | - Proposed Guided Thought Reinforcement (GTR), a framework to prevent thought collapse in RL-based VLM agent training by introducing automated thought correction. - GTR leverages an off-the-shelf VLM as a corrector model, which first evaluates the agent's thought and then corrects inconsistencies or errors based on the agent's outputs. - Integrates a simple SFT loss over the thought tokens to align the agent's reasoning with the corrected trajectories. - Addresses the distribution shift issue in thought cloning by employing Dataset Aggregation (DAgger), which continuously expands the dataset for thought cloning. - Achieved a 3x-5x increase in success rate and higher returns on the Points24 task compared to existing models, demonstrating the importance of process guidance in RL training. | ['Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [More Documents, Same Length: Isolating the Challenge of Multiple
  Documents in RAG](https://arxiv.org/abs/2503.04388) | Gabriel Stanovsky, Michael Hassid, Nir Mazor, Shahar Levy, LihiShalmon | - This paper investigates the impact of the number of retrieved documents on the performance of Retrieval-Augmented Generation (RAG) models while controlling for context length. - The study uses a custom dataset derived from a multi-hop QA task, where the context length and relevant information position are kept constant while varying the number of documents. - Results indicate that increasing the number of documents in RAG poses significant challenges for LLMs, even with a fixed context window, with performance drops of up to 10% observed. - This suggests that processing multiple documents is a distinct challenge from handling long contexts, likely due to factors such as redundancy, conflicting information, and implicit inter-document relationships. - The authors make their datasets and code publicly available to facilitate further research in multi-document retrieval. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/shaharl6000/MoreDocsSameLen) | N/A |
| [Motion Anything: Any to Motion Generation](https://arxiv.org/abs/2503.06955) | Rui Zhao, Danning Li, Wei Mao, Yiran Wang, SteveZeyuZhang | - Motion Anything is a multimodal motion generation framework using an Attention-based Mask Modeling approach for fine-grained spatial and temporal control over generated human motion. - The model architecture includes Temporal and Spatial Adaptive Transformers to align motion sequences with text, music, or both, enabling controllable generation under multimodal conditions.  - A new dataset, Text-Music-Dance (TMD), containing 2,153 text, music, and dance pairs, is introduced to facilitate research in multi-conditional motion generation.  - Experiments on HumanML3D, KIT-ML, AIST++, and TMD demonstrate a 15% FID improvement on HumanML3D and consistent gains on other benchmarks, outperforming state-of-the-art methods. - The framework excels in generating diverse and high-quality motion while aligning well with given text, music, and multimodal conditions, exceeding the performance of existing single or multi-task models. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [Quantizing Large Language Models for Code Generation: A Differentiated
  Replication](https://arxiv.org/abs/2503.07103) | Gabriele Bavota, Saima Afrin, Antonio Mastropaolo, mdiipenta, Devy1 | - This paper investigates the impact of low-bit quantization using Additive Quantization with Learned Multi-Codebooks (AQLM) on the performance of large language models (LLMs) for code generation in Python and Java. - The authors quantize CodeLlama (7B, 13B, 34B) and DeepSeek Coder (1B, 7B, 33B) to 8, 4, 3, and 2 bits per parameter and evaluate their performance on MultiPL-E and McEval benchmarks. - They find that 4-bit quantization is a "safe bet", reducing memory footprint by 70% without significant performance loss, and that code-specific calibration datasets improve performance at extreme quantization levels (3 and 2 bits). - Larger models exhibit greater resilience to information loss during extreme quantization. - Post-quantization fine-tuning improves the performance of 2-bit quantized models, particularly smaller ones. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/codellama), [Link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) |
| [WildIFEval: Instruction Following in the Wild](https://arxiv.org/abs/2503.06573) | Liat Ein-Dor, Ariel Gera, Asaf Yehudai, Gili Lior | - This paper introduces WILDIFEVAL, a new dataset of 12K real-world user instructions focusing on multi-constraint text generation tasks. - The dataset allows for fine-grained analysis of LLM performance on complex instructions and diverse constraint types. - The paper analyzes the dataset and benchmarks 14 different large language models (LLMs) using a novel evaluation method focusing on relative constraint fulfillment. - The results show that all tested LLMs struggle with length-based constraints and experience performance degradation as the number of constraints increases. - This work highlights the limitations of current state-of-the-art LLMs and provides a valuable resource for advancing research in constrained text generation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/gililior/wild-if-eval-code) | [Link](https://huggingface.co/datasets/gililior/wild-if-eval) |
| [VLog: Video-Language Models by Generative Retrieval of Narration
  Vocabulary](https://arxiv.org/abs/2503.09402) | Mike Zheng Shou, KevinQHLin | - VLog is a novel video understanding framework that generates concise video narrations using a generative retrieval model, offering a new perspective on video understanding. - It leverages a narration vocabulary, contrasting with subword vocabularies in existing generative video-language models, enabling faster decoding times (up to 20x) when processing videos. - This framework uses a novel generative retrieval architecture, combines language model's reasoning capabilities with contrastive retrieval's efficient similarity search, and employs hierarchical vocabulary indexing for efficiency. -  A vocabulary update strategy is also introduced to handle novel events during inference. - Experiments on VidCab-Eval, EgoSchema, COIN, and HiREST demonstrate that VLog can generate concise, contextually accurate, and efficient narrations, outperforming baselines on casual retrieval tasks while being significantly faster than generative models. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/showlab/VLog) | N/A |
| [Cost-Optimal Grouped-Query Attention for Long-Context LLMs](https://arxiv.org/abs/2503.09579) | Maosong Sun, Zhiyuan Liu, Xu Han, Yutong Wu, chen-yingfa | - This paper introduces a cost-optimal Grouped-Query Attention (GQA) method for long-context Large Language Models (LLMs), decoupling the number of attention heads from the model's hidden dimension, enabling flexible compute allocation to the attention operator. - The authors modify existing scaling laws to account for context length and attention head configuration, modeling language modeling quality as a function of compute and memory costs. - They establish that loss is a power-plus-constant function of attention heads, enabling loss prediction before training. - Experiments show that commonly used GQA configurations can be suboptimal. For instance, with Llama-3.2-1B at 128K context length, using fewer attention heads and a larger model can reduce training and inference FLOPs and memory by almost 50% without increasing loss. - This work provides insights into developing practical LLMs, especially in long-context scenarios. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) |
| [Self-Taught Self-Correction for Small Language Models](https://arxiv.org/abs/2503.08681) | Irina Nikishina, Chris Biemann, VityaVitalich | - This paper introduces Self-Taught Self-Correction (STaSC), a novel algorithm enabling small language models (SLMs) to self-correct using only self-generated data, eliminating the need for external tools or large models. - STaSC iteratively refines model outputs through answer sampling, correction generation, filtering of successful corrections, and fine-tuning on these corrections, thereby improving initial answer quality. - Experiments on the Natural Questions dataset demonstrate that STaSC significantly improves SLMs' self-correction capabilities, even boosting initial answer quality despite training solely on corrections. - The authors provide and analyze the impact of various STaSC design choices, including initial answer exploration, correction filtering, and fine-tuning strategies. - Open-source code and lightweight models are released to foster further research on self-correction in SLMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/VityaVitalich/STASC) | N/A |
| [MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented
  Generation System](https://arxiv.org/abs/2503.09600) | Simin Niu, Hanyu Wang, Zhaoxin Fan, Zhiyuan Ji, Robot2050 | - This paper introduces MoC (Mixture-of-Chunkers), a framework designed to improve the chunking process within Retrieval-Augmented Generation (RAG) systems. - MoC addresses the limitations of traditional and semantic chunking methods by incorporating a three-stage process involving a multi-granularity-aware router, specialized meta-chunkers, and a post-processing algorithm. - The router dynamically selects the appropriate chunker based on input text granularity, while meta-chunkers generate regular expressions for chunk extraction, optimizing computational efficiency. - An edit distance recovery algorithm refines the generated content, ensuring accurate chunk extraction. - Experimental results across multiple QA datasets demonstrate that MoC enhances chunking quality and improves RAG system performance compared to baseline methods and state-of-the-art LLM-based approaches. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Multimodal Language Modeling for High-Accuracy Single Cell
  Transcriptomics Analysis and Generation](https://arxiv.org/abs/2503.09427) | Xiang Wang, Junfeng Fang, Sihang Li, Jiaqi Yang, Yaorui Shi | - This paper introduces scMMGPT, a novel multimodal pre-trained transformer for analyzing and generating single-cell transcriptomics data. - scMMGPT integrates a cell-based PLM (scGPT) with a text-based PLM (Llama-2) using cross-modal projectors to facilitate information exchange. - Pre-trained on 27 million cells, scMMGPT excels in cell description generation with an 84% improvement in textual discrepancy and a 97% reduction in Earth Mover's Distance over baselines. - It also achieves state-of-the-art performance in text-conditioned pseudo-cell generation with a 4% k-NN accuracy improvement, and a 20.5% higher accuracy for cell type annotation. - Ablation studies validate the contribution of key components, especially the importance of integrating both cellular and textual data through the unified framework. | ['Multimodal', 'Text Generation', 'Natural Language Processing'] | [Link](https://github.com/syr-cn/SCMMGPT) | N/A |
| [When Large Vision-Language Model Meets Large Remote Sensing Imagery:
  Coarse-to-Fine Text-Guided Token Pruning](https://arxiv.org/abs/2503.07588) | Qi Zhu, Kang Wu, Xue Yang, Yingying Zhang, Junwei Luo | - This paper proposes a novel text-guided token pruning method for efficient vision-language understanding of large Remote Sensing Images (RSIs), addressing the challenge of information loss and high computational cost with traditional grid-based methods. - The method introduces a Region Focus Module (RFM) to identify text-relevant key vision tokens and a Dynamic Image Pyramid (DIP) for coarse-to-fine image tile selection and token pruning. - A new benchmark, LRS-VQA, featuring 7,333 question-answer pairs across 8 categories with image lengths up to 27,328 pixels, is also introduced to reflect the challenges of large RSIs perception.  - The proposed method demonstrates performance improvements and efficiency gains compared to existing high-resolution strategies and token reduction methods on various datasets, including achieving higher accuracy on all four LRS-VQA subsets compared to adapted general LVLMs like EarthDial and GeoPixel, as well as improved accuracy and FPS on MME-RealWorld-RS compared to other token pruning methods. - The method is architecture-agnostic, and ablation studies validate the effectiveness of its core components: dynamic tile selection with DIP, text-guided key region localization with RFM, and token pruning strategy. | ['Computer Vision', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/VisionXLab/LRS-VQA) | N/A |
| [Multi Agent based Medical Assistant for Edge Devices](https://arxiv.org/abs/2503.05397) | Pragya Sahu, Jagdish Samant, Chinmay Kulkarni, Shivam Akhouri, Sakharam Gawade | - This research introduces an on-device, multi-agent healthcare assistant that addresses privacy, latency, and internet dependency challenges posed by traditional cloud-based Large Action Models (LAMs) in healthcare applications. - The system utilizes smaller, task-specific agents like Planner and Caller, powered by the Qwen Code Instruct 2.5 7B model, optimized for resource allocation and on-device execution. - These agents achieve average RougeL scores of 85.5 for planning and 96.5 for calling and handle tasks such as intelligent diagnosis and appointment scheduling, emergency SOS, vitals tracking, and daily health reporting. - The multi-agent architecture enables modular collaboration, with each agent functioning independently while working harmoniously for complex workflows, and facilitates system scaling through additional agents. - A user-friendly application integrates the multi-agent system with smartwatches, enhancing personalized data retrieval and agent capabilities through retrieval augmented generation. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct) |
