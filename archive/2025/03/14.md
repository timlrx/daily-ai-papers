

## Papers for 2025-03-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing](https://arxiv.org/abs/2503.10613) | Dang Nguyen, zhoutianyi, nandakiran09, advaitgupta | - CoSTA*, a cost-sensitive toolpath agent, is introduced for multi-turn image editing, combining LLMs for subtask planning and A* search for efficient tool selection. - This hierarchical approach leverages an LLM to generate a subtask tree, which is then used to construct a Tool Subgraph (TS) based on tool dependencies. - A* search on the TS determines the optimal toolpath by balancing computational cost and output quality using a cost function that incorporates a trade-off parameter. - Experimental results on a novel benchmark dataset demonstrates CoSTA*'s superiority over existing baselines in complex multimodal tasks, pushing the Pareto frontier of cost-quality trade-offs. - CoSTA* effectively handles intricate workflows, dynamic feedback and retry mechanisms, along with support for a broader range of tasks, making it adaptable for complex image and text-in-image editing. | ['Multimodal', 'Image-to-Image', 'Text-to-Image'] | [Link](https://github.com/tianyi-lab/CoSTAR) | N/A |
| [World Modeling Makes a Better Planner: Dual Preference Optimization for
  Embodied Task Planning](https://arxiv.org/abs/2503.10480) | xpqiu, Jinlan, CyberDJ, ngc7293, sinwang | - Introduces Dual Preference Optimization (D2PO), a framework jointly optimizing state prediction and action selection for embodied task planning using preference learning, allowing Large Vision-Language Models (LVLMs) to better understand environment dynamics. - Presents a tree search mechanism to automatically collect trajectories and preference data without human annotation, facilitating extensive exploration via trial-and-error. - Evaluates on VoTa-Bench, a vision-enhanced extension of LoTa-Bench, demonstrating D2PO's significant improvement over existing methods and GPT-40 across multiple LVLMs (Qwen2-VL 7B, LLaVA-1.6 7B, LLaMA-3.2 11B). - Shows superior task success rates and more efficient planning with the 7B parameter model outperforming GPT-40, highlighting the approach's effectiveness. - Addresses limitations like the sim-to-real gap and data collection efficiency, while suggesting future work on more complex error patterns. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [Charting and Navigating Hugging Face's Model Atlas](https://arxiv.org/abs/2503.10633) | yedid, LielAmar, jonkahana, nitzankur, Eliahu | - This paper introduces a method for charting and navigating large model repositories, focusing on Hugging Face, to improve model discovery, reuse, and analysis of machine learning trends. - The proposed method addresses the challenge of incomplete model documentation by leveraging structural patterns in real-world model training practices, such as quantizations, near-duplicates, and merges, to accurately predict model relationships and attributes. - The method constructs a directed acyclic graph (DAG) representing the model relationships (e.g., fine-tuning, merging) and demonstrates its utility for various applications, including trend analysis, model attribute prediction, and measuring model impact. - Experimental results on Hugging Face datasets show that this approach significantly outperforms baseline methods in atlas recovery accuracy and allows for better model impact measurement and more robust IP tracking than simple download counts. - The atlas and dataset are publicly released to enable more effective model exploration and research. | ['Graph Machine Learning', 'Computer Vision', 'Natural Language Processing', 'Multimodal'] | N/A | [Link](https://horwitz.ai/model-atlas), [Link](https://huggingface.co/datasets/cfahlgrenl/hub-stats) |
| [GoT: Unleashing Reasoning Capability of Multimodal Large Language Model
  for Visual Generation and Editing](https://arxiv.org/abs/2503.10639) | zengxingyu, shilinyan, LjHuang, gogoduan, LucasFang | - The paper introduces Generation Chain-of-Thought (GoT), a novel paradigm that integrates multimodal large language model (MLLM) reasoning capabilities into visual generation and editing through explicit semantic-spatial reasoning chains. - GoT transforms visual generation from direct mapping to a reasoning-guided process with precise spatial control over object layout, relationships, and attributes. - The proposed framework leverages a semantic-spatial aware MLLM to generate structured reasoning chains and a multi-guided diffusion model with a Semantic-Spatial Guidance Module (SSGM). - The SSGM integrates semantic understanding, spatial awareness, and reference knowledge to ensure generated images accurately reflect the reasoning process. - Experimental results demonstrate state-of-the-art performance on text-to-image generation and editing benchmarks while enabling interactive control through modifiable reasoning chains. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/rongyaofang/GoT) | N/A |
| [Transformers without Normalization](https://arxiv.org/abs/2503.10622) | Zhuang Liu, Kaiming He, ylecun, endernewton, JiachenZhu | - This paper introduces Dynamic Tanh (DyT), a simple element-wise operation, as a replacement for normalization layers in Transformers. - DyT is defined as DyT(x) = tanh(ax), where 'a' is a learnable parameter, aiming to mimic the input-output mapping of Layer Normalization (LN). - Experiments across various tasks, including image classification, self-supervised learning, diffusion models, and large language models, show that Transformers with DyT achieve comparable or better performance than their LN counterparts, often without hyperparameter tuning. - The paper challenges the conventional wisdom that normalization layers are indispensable in modern neural networks. - The findings offer insights into the role of normalization by demonstrating its similarity to a scaled tanh function. | ['Computer Vision', 'Image Classification', 'Natural Language Processing', 'Text Generation', 'Unconditional Image Generation'] | N/A | N/A |
| [GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding](https://arxiv.org/abs/2503.10596) | wenyuliu, steelozazala, wondervictor, LianghuiZhu, RuiHu | - This paper introduces GroundingSuite, a comprehensive suite for evaluating pixel grounding, including an automated annotation framework (GSSculpt), a large-scale training dataset (GSTrain-10M with 9.56 million image-text pairs), and a curated evaluation benchmark (GSEval with 3800 images). - GSSculpt leverages Vision-Language Models for generating and refining pixel masks and descriptions and employs a noise filtering process, enhancing the quality and efficiency of automatic annotation compared to existing methods. - GSTrain-10M and GSEval address limitations in previous datasets by encompassing stuff-class, part-level, and multi-object segmentation with diverse and detailed textual descriptions. - Models trained on GroundingSuite achieve state-of-the-art results, demonstrating its effectiveness; for example, reaching a cIoU of 68.9 on gRefCOCO and gIoU of 55.3 on RefCOCOm. - This benchmark evaluates models in zero-shot settings and covers fine-grained and complex scene understanding scenarios. | ['Image Segmentation', 'Multimodal'] | [Link](https://github.com/hustvl/GroundingSuite) | N/A |
| [New Trends for Modern Machine Translation with Large Reasoning Models](https://arxiv.org/abs/2503.10351) | acecamel1977, longyuewang, minghaowu, ChenyangLyu, SNF | - This paper explores the transformative potential of Large Reasoning Models (LRMs) in redefining Machine Translation (MT) by showcasing how they reframe translation as a reasoning task, going beyond traditional text-to-text mapping. - LRMs exhibit self-reflection and auto-pivot translation capabilities, which are new characteristics introduced to MT. - The authors present three fundamental shifts brought about by LRMs: 1) contextual coherence 2) cultural intentionality 3) self-reflection. - By leveraging Chain-of-Thought (CoT) reasoning, LRMs address challenges in stylized translation, document-level translation, and multimodal translation. - The paper also identifies critical challenges such as over-localization, inference efficiency, and limitations in handling complex ciphers and specialized multimodal inputs, presenting opportunities for future research. | ['Translation', 'Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [Shifting Long-Context LLMs Research from Input to Output](https://arxiv.org/abs/2503.04723) | mingshan, tsq2000, Zhiqiang007, bys0318, mozhu | - This paper argues for a shift in Natural Language Processing (NLP) research from focusing on long-input Large Language Models (LLMs) to developing long-output LLMs. - It highlights the increasing demand for generating extended, coherent, and contextually rich text in various real-world applications, such as novel writing, long-term planning, and complex reasoning, which require outputs exceeding 4,000 tokens. - The paper identifies key challenges hindering progress in long-output generation, including data limitations, task execution complexities, and computational cost constraints. - It defines long-output LLMs as models specifically designed to excel at long-output tasks, requiring capabilities in handling extensive contexts and generating high-quality, logically consistent text. - The paper explores the current state of long-output LLMs, discussing existing datasets, benchmarks, and models, and analyzes their limitations and potential. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [VisualWebInstruct: Scaling up Multimodal Instruction Data through Web
  Search](https://arxiv.org/abs/2503.10582) | Bo Li, Xiang Yue, wenhu, jiachenli-ucsb, jymmmmm | - This paper introduces VisualWebInstruct, a new dataset for visual question answering (VQA) focused on complex reasoning, created by leveraging web search and automated data curation. - Starting with 30,000 seed images, they use Google Image Search to find similar images and related web pages, extracting about 900K question-answer pairs from over 700K unique URLs with around 40% being visual QA pairs and others as text QA pairs. - The dataset creation pipeline involves HTML processing, content extraction using an LLM, answer synthesis using GPT-4, and a consistency filtering process to ensure high-quality and diversity of the collected data. - Fine-tuning existing VLMs on VisualWebInstruct shows significant performance gains, with improvements ranging from 5-20% on several reasoning benchmarks including MMMU, MathVista, and DynaMath. - Their best model, MAmmoTH-VL2 (7B parameter), achieves state-of-the-art performance in its size category on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Open-Sora 2.0: Training a Commercial-Level Video Generation Model in
  $200k](https://arxiv.org/abs/2503.09642) | Xinying Guo, Tom Young, Chenhui Shen, Zangwei Zheng, Xiangyu Peng | - Open-Sora 2.0 is a commercial-level text-to-video and image-to-video generation model trained for $200k, 5-10 times cheaper than comparable models. - It uses a novel Video DC-AE autoencoder with deep compression and a DiT (Diffusion Transformer) architecture incorporating 3D Rotary Position Embedding (ROPE) for enhanced motion representation. - Trained using a three-stage process involving low and high resolution videos, it leverages open-source image models (Flux) and hierarchical data filtering for efficiency. - Evaluations with VBench show that Open-Sora 2.0 outperforms open-source models like CogVideo and HunyuanVideo, closing the performance gap with OpenAI's Sora to 0.69%. - Supports resolutions up to 768x768 pixels and video lengths up to 5 seconds at 24 FPS. | ['Text-to-Video', 'Image-to-Video', 'Multimodal'] | [Link](https://github.com/hpcaitech/Open-Sora) | N/A |
| [4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large
  Language Models](https://arxiv.org/abs/2503.10437) | hpfister, Qmh, wrencanfly, rpzhou, EthanTaylor | - 4D LangSplat, a novel framework for building dynamic 4D language fields, enabling time-sensitive and time-agnostic open-vocabulary queries in dynamic scenes, is introduced. - It leverages a multimodal object-wise video prompting method, combining visual and textual prompts with a Multimodal Large Language Model (MLLM) and a Large Language Model (LLM), to generate and encode object-specific video captions into pixel-aligned features for training the 4D language field. - To capture evolving object semantics, 4D LangSplat incorporates a status deformable network, modeling smooth transitions between limited semantic states. - Experimental results on HyperNeRF and Neu3D datasets show state-of-the-art performance in both time-agnostic and time-sensitive querying, significantly outperforming baseline methods by substantial margins in accuracy and intersection-over-union scores. - The approach effectively handles dynamic object transformations and temporal semantics, enabling precise spatiotemporal querying in complex 4D scenes. | ['Computer Vision', 'Text-to-3D', 'Multimodal'] | N/A | N/A |
| [UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](https://arxiv.org/abs/2503.10630) | Ziwei Wang, Lingqing Zhao, jiwenlu, xuxw98, hangyin | - UniGoal is a universal zero-shot goal-oriented navigation framework that handles object-goal, instance-image-goal, and text-goal navigation without training or finetuning. - It employs a uniform graph representation for both the agent's observations (scene graph) and the goal (goal graph), which minimizes information loss and facilitates explicit graph-based reasoning by an LLM. - A multi-stage exploration policy guides navigation based on graph matching scores: zero matching triggers subgraph searching, partial matching initiates coordinate projection and anchor pair alignment, and perfect matching leads to graph correction and goal verification. - A blacklist mechanism avoids repeated exploration of unsuccessful areas and enables robust switching between stages. - UniGoal achieves state-of-the-art zero-shot performance on various benchmarks, exceeding task-specific and supervised universal methods. | ['Robotics', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and
  Beyond](https://arxiv.org/abs/2503.10460) | tanglifu, JunchenLiu, yyy99, duan901010, cizhenshi | - The Light-R1 series introduces a new approach for training long chain-of-thought (COT) reasoning models, focusing on efficiency and effectiveness for resource-constrained environments. - The approach employs a curriculum training strategy involving two-stage Supervised Fine-tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO), validated by training Light-R1-32B from Qwen2.5-32B-Instruct, outperforming DeepSeek-R1-Distill-Qwen-32B on math tasks. - A 3k dataset curated for the second SFT stage significantly boosts performance across other models, resulting in state-of-the-art (SOTA) results for 7B and 14B models, and competitive performance for the 32B model (Light-R1-32B-DS) compared to QwQ-32B and DeepSeek-R1. - Applying Generalized Reinforcement Learning from Preference Optimization (GRPO) on long-COT models, specifically Light-R1-14B-DS, yields SOTA results for 14B models in math, exceeding many 32B models and DeepSeek-R1-Distill-Llama-70B on AIME benchmarks, with simultaneous improvement in response length and reward scores. - The Light-R1 series releases all models, data, and code, showing that long-COT models can be effectively trained from scratch and demonstrating SOTA performance improvements on long-COT and RL training of 14B models. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/Qihoo360/Light-R1) | N/A |
| [CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance](https://arxiv.org/abs/2503.10391) | brotherhuang, u302117, BestWishYsh, angtian, dyf | - CINEMA is a novel framework for generating coherent multi-subject videos, guided by a Multimodal Large Language Model (MLLM), which enhances coherence and contextual alignment. - The framework utilizes MM-DiT for video generation, integrating multimodal information through a multimodal large language model, semantic alignment network (AlignerNet), and visual entity encoding. - AlignerNet bridges the representation gap between the MLLM and the text encoder of the base video generation model, ensuring compatibility. - Visual entity encoding extracts detailed visual features from reference images using a VAE, ensuring subject consistency. - Experimental results demonstrate that CINEMA effectively preserves subject identity, adheres to text prompts, and maintains temporal coherence in generated videos, outperforming baseline models in visual quality and subject consistency. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [Quantization for OpenAI's Whisper Models: A Comparative Analysis](https://arxiv.org/abs/2503.09905) | allisonandreyev | - This paper analyzes the impact of quantization on OpenAI's Whisper ASR models, including two variants: Whisper_Streaming and whisper-timestamped. - It quantizes the base Whisper model using INT4, INT5, and INT8 methods and evaluates their performance on the LibriSpeech dataset. - The study finds that quantization reduces model size by up to 45% and latency by 19% while preserving transcription accuracy. - This makes Whisper deployment feasible on resource-constrained devices like smartphones and IoT systems. - The paper also qualitatively compares the three Whisper variants, highlighting their strengths and limitations for different applications. | ['Audio', 'Automatic Speech Recognition'] | [Link](https://github.com/allisonandreyev/WhisperQuantization) | N/A |
| [R1-Onevision: Advancing Generalized Multimodal Reasoning through
  Cross-Modal Formalization](https://arxiv.org/abs/2503.10615) | Xiaoxuan He, Yi Yang, twilightsnow, dcyin, Emilia515 | - Introduces R1-Onevision, a multimodal reasoning model employing a cross-modal reasoning pipeline that transforms images into formal textual representations for enhanced language-based reasoning. - Presents a new dataset, R1-Onevision, containing detailed step-by-step multimodal reasoning annotations across various domains, created using a role-playing strategy. - Develops a two-stage post-training strategy involving supervised fine-tuning and reinforcement learning to enhance reasoning abilities and generalization of the model. - Introduces R1-Onevision-Bench, a comprehensive benchmark designed to assess grade-level multimodal reasoning skills across different educational levels and scientific domains. - Achieves state-of-the-art performance, surpassing models such as GPT-40 and Qwen2.5-VL on various multimodal reasoning benchmarks, including significant improvements on MathVerse and MathVision. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/Fancy-MLLM/R1-onevision) | N/A |
| [VisualPRM: An Effective Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2503.10291) | Einsiedler, Yeshenglong, Decaux, chenlj22, Weiyun1025 | - This paper introduces VisualPRM, an 8B parameter multimodal Process Reward Model (PRM) designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). - VisualPRM enhances MLLM performance across various model scales and families by using Best-of-N (BoN) evaluation strategies, achieving a 5.9-point improvement with InternVL2.5-78B across seven benchmarks. - To train VisualPRM, the authors created a 400K sample multimodal process supervision dataset, VisualPRM400K, using an automated data pipeline to annotate step-wise correctness. - A new benchmark, VisualProcess-Bench, with 2,866 human-annotated samples, was also developed for evaluating multimodal PRMs and MLLMs' ability to detect erroneous reasoning steps. - Experimental results demonstrate that VisualPRM consistently outperforms Outcome Reward Models and Self-Consistency in BoN evaluations, establishing its effectiveness as a critic model for enhancing MLLM reasoning. | ['Multimodal', 'Question Answering'] | N/A | N/A |
| ["Silent Is Not Actually Silent": An Investigation of Toxicity on Bug
  Report Discussion](https://arxiv.org/abs/2503.10072) | Jaydeb Sarker, imranraad | - This study investigates toxicity in GitHub bug report discussions, a critical aspect of open-source software development often overlooked in previous research. - Through qualitative analysis of 203 bug threads (81 toxic), the research identifies key factors contributing to toxicity, such as misaligned perceptions of bug severity and maintainers' prioritization, unresolved frustrations with tools, and lapses in professional communication. - Findings reveal that external participants and bug reporters themselves are more prone to initiating toxic comments. Toxic threads have lower resolution rates and are less likely to be linked to pull requests, hindering collaboration and bug resolution.  - The study also highlights the detrimental impact of toxicity on project discussions and offers actionable recommendations for improving bug resolution by mitigating toxicity. - Recommendations for future work include developing more robust and context-aware automated systems for toxicity detection in software engineering discussions and conducting large-scale empirical studies to gain deeper insights into this phenomenon. | ['Natural Language Processing'] | N/A | N/A |
| [On the Limitations of Vision-Language Models in Understanding Image
  Transforms](https://arxiv.org/abs/2503.09837) | Saquib Sarfraz, Hasnain Ali, Ahmad Mustafa Anis | - This research paper investigates the limitations of Vision-Language Models (VLMs), particularly CLIP and SigLIP, in understanding basic image transformations such as rotations, flips, color adjustments, and distortions. - The study reveals that while VLMs excel in semantic understanding, they struggle with comprehending and reasoning about image-level augmentations, impacting their performance in downstream tasks like image editing. - An augmented version of the Flickr8k dataset is created, pairing each image with detailed descriptions of applied transformations, to evaluate VLM performance on understanding augmented descriptions, matching augmented images with descriptions, and classifying transformations. - Experiments show that VLMs have difficulty associating augmented descriptions with corresponding images and struggle to correctly classify image transformations. - The findings highlight a crucial gap in current VLMs' spatial understanding, emphasizing the need for new training paradigms that balance invariance with explicit transformation awareness for improved performance in image editing and related tasks. | ['Multimodal', 'Computer Vision', 'Image-to-Image'] | N/A | N/A |
