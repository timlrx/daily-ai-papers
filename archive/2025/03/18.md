

## Papers for 2025-03-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal
  Consistent Video Generation](https://arxiv.org/abs/2503.06053) | Runze Zhang, NeilXu, EllenAP, lixiaochuan, georgedu | - This paper introduces DropletVideo, a novel text-to-video model trained on a new dataset called DropletVideo-10M, which contains 10 million videos with dynamic camera motion and object actions, each accompanied by detailed captions averaging 206 words describing camera movements and plot developments. - The DropletVideo model focuses on maintaining *integral spatio-temporal consistency*, considering the interplay between plot progression and camera techniques, and the influence of prior content on subsequent generation, enabling generation of complex multi-plot narratives. - DropletVideo-10M is significantly larger than existing open-source datasets addressing spatio-temporal consistency and includes richer textual descriptions than comparable datasets like Panda-70M. - It leverages a 3D causal Variational Autoencoder and a Multi-Modal Diffusion Transformer for encoding video and text, and employs a motion adaptive generation strategy to control the motion speed in generated videos.  - Qualitative results demonstrate that DropletVideo effectively preserves content consistency and generates realistic camera movements, outperforming or matching other state-of-the-art models on several qualitative metrics. | ['Text-to-Video', 'Computer Vision', 'Multimodal'] | [Link](https://dropletx.github.io) | N/A |
| [Being-0: A Humanoid Robotic Agent with Vision-Language Models and
  Modular Skills](https://arxiv.org/abs/2503.12533) | tellarin, SherryXu, takenpeanut, fuyh, Yaya041 | - Being-0 is a hierarchical agent framework designed to effectively control a full-sized humanoid robot to perform complex, long-horizon tasks in real-world environments. - The framework integrates a Foundation Model (FM) for high-level reasoning, a lightweight Vision-Language Model (VLM) called Connector for bridging the gap between high-level plans and low-level skills, and a Modular Skill Library for robust locomotion and manipulation. - The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands, dynamically coordinating locomotion and manipulation, and adjusting the robot's pose for optimal skill execution. - Experimental results on a Unitree H1-2 humanoid robot show an average completion rate of 84.4% on challenging long-horizon tasks, demonstrating the framework's efficiency and robustness. - Being-0 leverages an active camera and dexterous hands, and deploys all modules onboard, except the FM, enabling efficient and real-time performance. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [reWordBench: Benchmarking and Improving the Robustness of Reward Models
  with Transformed Inputs](https://arxiv.org/abs/2503.11751) | Yoon Kim, Andrew Cohen, mghazvininejad, michiyasunaga, ZhaofengWu | - This paper introduces reWordBench, a new benchmark designed to evaluate the robustness of reward models (RMs) used in natural language processing. - reWordBench consists of transformed inputs from the original RewardBench, using meaning- or ranking-preserving transformations to assess how RMs handle slight input alterations. - The authors find that state-of-the-art RMs are brittle, exhibiting significant performance degradation even with minor transformations. - To address this, they propose a regularization method that trains RMs to assign similar scores to paraphrased inputs, improving robustness across different transformation types. - This regularization method also enhances the utility of RMs in alignment tasks, leading to higher-quality outputs in downstream applications. | ['Natural Language Processing'] | N/A | N/A |
| [MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based
  Scientific Research](https://arxiv.org/abs/2503.13399) | lundbergemma, chadliu, shcohen, suyc21, jmhb | - MicroVQA, a novel visual question answering (VQA) benchmark, is introduced to evaluate multimodal reasoning within the context of microscopy-based biological research. - The benchmark consists of 1,042 multiple-choice questions spanning three key research tasks: expert image understanding, hypothesis generation, and experiment proposal. - A two-stage process is employed for generating these questions, starting with expert-created samples and then refining them using an agent-based system ('RefineBot') to minimize language shortcuts. - State-of-the-art multimodal large language models (MLLMs) achieve a peak performance of 53% on MicroVQA, demonstrating its challenge and utility in assessing research-level reasoning. - An expert error analysis reveals that perception errors pose the most significant challenge, emphasizing the need for improved visual understanding capabilities in MLLMs, while specialized training on biomedical data shows performance improvements. | ['Multimodal', 'Visual Question Answering', 'Computer Vision'] | [Link](https://github.com/jmhb0/microvqa) | [Link](https://huggingface.co/datasets/jmhb/microvqa) |
| [Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2503.12605) | Yuecheng Zhang, scofield7419, liuziwei7, ChocoWu, Gh0stAR | - This survey paper provides a comprehensive overview of Multimodal Chain-of-Thought (MCOT) reasoning, a technique that enhances multimodal reasoning by enabling large language models (LLMs) to decompose complex tasks into a series of intermediate steps. - The authors categorize MCoT methodologies based on rationale construction, structure, information enhancement, objective granularity, multimodal rationale integration, and test-time scaling. - They also present a timeline of key milestones in MCoT research, discuss relevant applications in various domains like robotics, healthcare, and autonomous driving, and provide a curated list of available datasets and benchmarks. - The paper identifies key challenges in MCoT research, such as computational sustainability, the limitations of reasoning in general scenarios, error propagation in extended reasoning chains, the symbolic-neural integration gap, and hallucination prevention. - Finally, it proposes promising future research directions, including dynamic environment adaptation, adaptive chain length, and integration with cognitive science, to address these challenges and further advance the development of MCoT. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Natural Language Processing'] | [Link](https://github.com/yaotingwangofficial/Awesome-MCOT) | N/A |
| [Free-form language-based robotic reasoning and grasping](https://arxiv.org/abs/2503.13082) | Matteo Bortolon, Alice Fasoli, Runyu Jiao, SPovoli, FGiuliari | - This research introduces FreeGrasp, a novel method for robotic grasping that interprets free-form language instructions and reasons about spatial relationships between objects, leveraging pre-trained Vision-Language Models (VLMs). - The method detects objects as keypoints, annotates them on images for improved VLM spatial reasoning, and determines the grasping sequence for a target object, even if obstructed. - Using a new synthetic dataset, FreeGraspData, based on MetaGraspNetV2, and real-world robotic experiments, FreeGrasp demonstrated superior performance in grasp reasoning and execution compared to ThinkGrasp, particularly in cluttered scenes with ambiguous object descriptions. - FreeGrasp's robust performance is attributed to its mark-based visual prompting and contextualized reasoning, which addresses the limitations of VLMs in visual-spatial reasoning tasks. - Despite improvements, limitations persist in handling complex object occlusion and adapting to scene changes. Future work is needed to explore memory mechanisms and adaptive instructions in these scenarios. | ['Robotics', 'Computer Vision', 'Object Detection', 'Multimodal'] | N/A | N/A |
| [R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization](https://arxiv.org/abs/2503.12937) | Jingyi Zhang, Xikun, liushunyu, HuanjinYao, huangjiaxing | - This paper introduces R1-VL, a series of Multimodal Large Language Models (MLLMs) enhanced for step-by-step reasoning using a novel reinforcement learning approach. - The key contribution is Step-wise Group Relative Policy Optimization (StepGRPO), which addresses the sparse reward issue in MLLM reasoning by incorporating dense step-wise reasoning rewards. - StepGRPO employs two rule-based reward mechanisms: Step-wise Reasoning Accuracy Reward (StepRAR) to encourage the inclusion of necessary intermediate steps and Step-wise Reasoning Validity Reward (StepRVR) to promote well-structured, logically consistent reasoning. - Experimental results across 8 benchmarks demonstrate that R1-VL significantly outperforms baseline MLLMs and achieves state-of-the-art performance on multiple reasoning tasks, exhibiting improvements of 4.6% and 3.8% over Qwen2-VL-2B and Qwen2-VL-7B respectively.  - R1-VL's competitive results against both closed and open-source models like GPT-40 and Mulberry-7B illustrate the effectiveness of StepGRPO in advancing MLLM reasoning capabilities. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | N/A | N/A |
| [V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning](https://arxiv.org/abs/2503.11495) | Wei Li, Ziquan Liu, ChenyangSi, lwpyh, Cade921 | - Introduces V-STaR, a benchmark designed to evaluate the spatio-temporal reasoning abilities of Video Large Language Models (Video-LLMs). - Proposes a Reverse Spatio-Temporal Reasoning (RSTR) task that decomposes video understanding into identifying "what" objects are present, "when" events occur, and "where" they are located. - Constructs a dataset with coarse-to-fine Chain-of-Thought (CoT) questions generated by a semi-automated GPT-4 pipeline, mimicking human cognitive processes for sequential reasoning. - Introduces the Logarithmic Geometric Mean (LGM) to comprehensively assess spatio-temporal reasoning by combining model scores at each step of the reasoning chain. - Experiments on 14 Video-LLMs reveal performance gaps, especially in grounding answers temporally and spatially, highlighting the need for more robust spatio-temporal reasoning in Video-LLMs. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering'] | [Link](https://V-STaR-Bench.github.io/) | N/A |
| [VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](https://arxiv.org/abs/2503.13444) | Chang Wen Chen, Ye Liu, AnalMom, KevinQHLin | - VideoMind, a novel video-language agent designed for temporal-grounded video understanding, is introduced, incorporating a role-based agentic workflow (Planner, Grounder, Verifier, and Answerer) and a Chain-of-LoRA strategy for efficient role-switching. - The Chain-of-LoRA strategy, built upon a single base Multimodal Large Language Model (MLLM), enables seamless transitions between roles using lightweight LoRA adaptors, balancing efficiency and flexibility by avoiding the overhead of multiple models. - VideoMind achieves state-of-the-art performance on 14 public benchmarks across diverse video understanding tasks, including grounded video question-answering, video temporal grounding, and general video question-answering. - On the challenging CG-Bench with long videos (average duration: 27 minutes), VideoMind's 2B model outperforms all open-source models and most closed-source models, while the 7B model surpasses even GPT-40. - Ablation studies confirm the effectiveness and efficiency of the proposed design choices, especially the Chain-of-LoRA mechanism for enhanced performance and computational efficiency. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Question Answering'] | N/A | N/A |
| [Sightation Counts: Leveraging Sighted User Feedback in Building a
  BLV-aligned Dataset of Diagram Descriptions](https://arxiv.org/abs/2503.13369) | Jaime-Choi, sangryul, namin0202, eunkey, soarhigh | - SIGHTATION, a new large-scale, BLV-aligned dataset of diagram descriptions, is introduced, addressing the need for accessible visual information for blind and low-vision (BLV) individuals. - The dataset leverages sighted user feedback through a multi-pass inference process where sighted users assess descriptions generated by vision-language models (VLMs), rather than producing the descriptions themselves, reducing bias and cost. - SIGHTATION includes a variety of training data for tasks such as completion, preference, retrieval, question answering, and reasoning, totaling 5k diagrams and 137k samples. - The effectiveness of the dataset is demonstrated through fine-tuning various sized models, with results showing a preference-tuned 2B model achieving a 1.67 increase in usefulness ratings by the BLV group and an instruction-tuned 2B model outperforming a 3B model on chart comprehension in 8 out of 11 automatic metrics. - A BLIP-2 model fine-tuned for retrieval on SIGHTATION achieves a 65%p improvement on Precision@1 compared to a COCO-tuned model. | ['Image-to-Text', 'Multimodal'] | N/A | [Link](https://hf.co/Sightation) |
| [Basic Category Usage in Vision Language Models](https://arxiv.org/abs/2503.12530) | KyleMoore, JesseTNRoberts, HTSawyer | - This paper investigates basic level categorization in two vision-language models (VLMs): Llama 3.2 Vision Instruct (11B) and Molmo 7B-D. - The study uses the Ecoset dataset, containing 1.5 million images across 565 basic-level categories. - Results show that both VLMs prefer basic level categorization, consistent with human behavior, exceeding the estimated 50% lower bound of basic-level usage. - Furthermore, the models exhibit nuances consistent with human categorization, such as the biological vs. non-biological basic level effects and the expert basic level shift. - These findings suggest that VLMs learn cognitive categorization behaviors from human training data. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
| [Investigating Human-Aligned Large Language Model Uncertainty](https://arxiv.org/abs/2503.12528) | Pamela Wisniewski, Daryl Watson, Kyle Moore, JesseTNRoberts | - This paper investigates how well different uncertainty measures in large language models (LLMs) align with human uncertainty. - The study uses non-factual questions from the Pew Research Center's American Trends Panel to compare human uncertainty with LLM uncertainty, focusing on agreement with the provided choices. - It evaluates various uncertainty measures, including Bayesian measures, entropy variations, and nucleus size, across different LLM sizes (1B to 7B parameters) and types (base and instruction-finetuned). - The findings indicate that top-k entropy tends to align best with human behavior, with better performance in smaller models, and a combination of multiple measures provides comparable alignment while reducing size dependency. - Future research directions include exploring other uncertainty measures, optimizing existing ones for better human alignment, and expanding the study to a broader range of contexts | ['Natural Language Processing'] | N/A | N/A |
