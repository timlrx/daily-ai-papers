

## Papers for 2025-03-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time
  Exploration and Exploitation](https://arxiv.org/abs/2503.13288) | Qika, haitengzhao, changma, Meituannnnnn, xufangzhi | - Introduces φ-Decoding, a novel inference-time optimization algorithm utilizing adaptive foresight sampling for enhanced exploration and exploitation during LLM inference. - Employs simulated future steps to estimate step values, balancing exploration and exploitation to find globally optimal reasoning paths. - Proposes in-width and in-depth pruning strategies for adaptive computation allocation, enhancing efficiency by prioritizing challenging steps. - Achieves >14% average performance improvement on LLaMA3.1-8B-Instruct over auto-regressive CoT across seven reasoning benchmarks. - Demonstrates consistent superiority across various LLMs (3B, 7B, 8B, and 70B) and computational budgets, matching suboptimal baseline performance with 6x efficiency. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/xufangzhi/phi-Decoding) | N/A |
| [TULIP: Towards Unified Language-Image Pretraining](https://arxiv.org/abs/2503.15485) | XuDong Wang, Seun Eisape, Long Lian, yala, ZinengTang | - TULIP, a novel image-text contrastive learning model, enhances fine-grained visual feature learning while preserving language grounding capabilities. - It incorporates patch-level global/local multi-crop augmentations, objectives, and a reconstruction objective to preserve high-frequency visual details and spatial information. - It also employs generative data augmentation with diffusion models to create challenging negative examples for better semantic grounding. - TULIP achieves state-of-the-art zero-shot performance on ImageNet-1K and significant improvements on other benchmarks, including RxRx1, and MMVP. - Its ability to encode both fine-grained visual and robust textual features makes it beneficial as a visual encoder for large-scale multimodal models like LLaVA. | ['Multimodal', 'Computer Vision', 'Image Classification', 'Zero-Shot Image Classification', 'Image Feature Extraction'] | [Link](https://tulip-berkeley.github.io) | N/A |
| [Cube: A Roblox View of 3D Intelligence](https://arxiv.org/abs/2503.15475) | Karun Channa, Nishchaie Khanna, Kiran Bhat, Foundation AI Team, marcelvanworkum | - Roblox researchers introduce Cube, a new foundation model for 3D intelligence designed to support developers in creating various aspects of Roblox experiences, including 3D object and scene generation, character rigging, and behavior scripting. - The model leverages a novel 3D shape tokenization technique, converting 3D shapes into discrete tokens, enabling applications such as text-to-shape, shape-to-text, and text-to-scene generation.  - This tokenizer utilizes a Perceiver-based transformer with phase-modulated positional encoding, optimal transport vector quantization, and a stochastic gradient shortcut to improve training stability and reconstruction quality. - The model demonstrates strong performance in shape reconstruction, outperforming existing methods like Craftsman in both surface and volumetric IoU on the Toys4K dataset.  - The paper also presents applications showcasing text-to-shape generation and shape-to-text capabilities, along with initial steps toward text-to-scene generation by combining these models with a large language model for scene analysis and reasoning. | ['Text-to-3D', 'Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://github.com/Roblox/cube) | N/A |
| [STEVE: AStep Verification Pipeline for Computer-use Agent Training](https://arxiv.org/abs/2503.12532) | Chi-Wing Fu, Shu Liu, Ziqin Wei, Zhisheng Zhong, Fanbin Lu | - STEVE (Step Verification Pipeline) is introduced for training computer-use agents, which leverages a large language model (GPT-40) to assess the correctness of each action within a trajectory, providing dense reward signals. - The pipeline starts by training a vision-language model specialized in UI grounding using a dataset of web pages and desktop screenshots, and then fine-tuning this model as a computer-use agent using supervised learning with limited trajectory data. - The agent is then deployed to collect more trajectories, with each action evaluated by GPT-40, resulting in a step-verified trajectory dataset. - Kahneman & Tversky Optimization (KTO) is employed to train the agent from this dataset, enabling efficient utilization of both positive and negative actions. - Experiments show that STEVE outperforms supervised fine-tuning, allows training of a 7B parameter vision-language model as a high-performing agent, and achieves state-of-the-art results on the WinAgentArena benchmark. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/FanbinLu/STEVE) | N/A |
| [LEGION: Learning to Ground and Explain for Synthetic Image Detection](https://arxiv.org/abs/2503.15264) | Weijia Li, Junyan Ye, Siwei Wen, zichenwen, khr0516 | - This paper introduces LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework for fully synthetic image detection. - LEGION integrates artifact detection, segmentation, and explanation within a single framework. - The authors also introduce SynthScars, a high-quality and diverse dataset of synthetic images with fine-grained annotations, including pixel-level segmentation, textual explanations, and artifact category labels.  - On SynthScars, LEGION surpasses the second-best traditional expert by 3.31% in mIoU and 7.75% in F1 score, demonstrating state-of-the-art performance. - Beyond detection, LEGION acts as a controller, guiding image refinement pipelines for enhanced realism and quality in synthetic image generation. | ['Computer Vision', 'Image Segmentation', 'Multimodal'] | N/A | [Link](https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1), [Link](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2) |
| [MusicInfuser: Making Video Diffusion Listen and Dance](https://arxiv.org/abs/2503.14505) | Steven M. Seitz, Brian Curless, Ira Kemelmacher-Shlizerman, Susung Hong | - MusicInfuser adapts pre-trained text-to-video diffusion models to generate dance videos synchronized with user-provided music. - It uses a Zero-Initialized Cross-Attention (ZICA) adapter and a low-rank adapter (High-Rank LoRA) to condition video generation on music while preserving text-based control over style and scene elements. - Trained on a combined dataset of AIST dance videos and in-the-wild YouTube dance videos, MusicInfuser leverages existing knowledge of human motion and diverse dance styles without needing motion capture data. - Evaluations using Video-LLMs demonstrate that MusicInfuser produces higher quality and more realistic dance movements, better aligned with music than existing methods, and is generalizable to novel music and longer videos. - The model allows for difficulty control through prompt engineering, enabling users to customize their choreography. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [ViSpeak: Visual Instruction Feedback in Streaming Videos](https://arxiv.org/abs/2503.12769) | Kun-Yu Lin, maybetomorrow, Lymann, PhilipC, fushh7 | - Introduces ViSpeak, a novel streaming video understanding Large Multi-modal Model (LMM) designed for Visual Instruction Feedback, a new task requiring models to actively respond to visual instructions in streaming videos, enhancing real-time human-agent interaction. - Presents ViSpeak-Bench, a benchmark with 1,000 videos and 1,000 QA pairs across seven visual instruction subtasks, and ViSpeak-Instruct, a 34k sample training dataset, to facilitate research in this area.  - Employs a three-stage fine-tuning approach, adapting an existing omni-modal model to streaming input, improving streaming question-answering and proactive output, and finally training on ViSpeak-Instruct for visual instruction feedback. - Achieves state-of-the-art (SOTA) performance on StreamingBench and OVO-Bench, comparable to GPT-40, demonstrating its strong streaming video understanding capabilities and exceeding existing open-source models on ViSpeak-Bench.  - While showing promising results, acknowledges limitations in dataset size and diversity, context length, and degraded Automatic Speech Recognition due to audio segmentation. | ['Multimodal', 'Video-Text-to-Text'] | [Link](https://github.com/HumanMLLM/ViSpeak), [Link](https://github.com/HumanMLLM/ViSpeak-Bench) | N/A |
| [GKG-LLM: A Unified Framework for Generalized Knowledge Graph
  Construction](https://arxiv.org/abs/2503.11227) | Jun Liu, haiping Zhu, Shihao Qi, Bifan Wei, VentureZJ | - This paper proposes GKG-LLM, a unified framework for constructing Generalized Knowledge Graphs (GKGs), encompassing Knowledge Graphs (KGs), Event Knowledge Graphs (EKGs), and Commonsense Knowledge Graphs (CKGs). - GKG-LLM employs a three-stage curriculum learning approach to fine-tune Large Language Models (LLMs), starting with KG data, then EKG data, and finally CKG data, along with counter-task data to enhance generalization. - The model is evaluated on a dataset comprising 15 sub-tasks across 29 datasets, categorized into in-domain, counter-task, and out-of-distribution (OOD) data. - Experimental results show that GKG-LLM outperforms existing closed-source and open-source LLMs on various GKG construction tasks, demonstrating the effectiveness of the unified approach and the curriculum learning strategy. -  GKG-LLM achieves an average improvement of 7.49% over the strongest baseline across all GKG sub-tasks and exhibits strong performance on OOD data, indicating good generalization capability. | ['Natural Language Processing', 'Text2Text Generation', 'Graph Machine Learning'] | N/A | N/A |
| [Mitigating Visual Forgetting via Take-along Visual Conditioning for
  Multi-modal Long CoT Reasoning](https://arxiv.org/abs/2503.13360) | Han-Jia Ye, Houwen Peng, Zhun Sun, Allen8 | - This paper introduces Take-along Visual Conditioning (TVC), a novel method to mitigate visual forgetting in Multimodal Large Language Models (MLLMs) during long-chain reasoning. - TVC shifts and compresses visual tokens at critical reasoning stages through Dynamic Visual Reaffirmation (DVR) and Periodic Visual Calibration (PVC). - DVR injects visual content at intervals during training, and PVC reactivates visual information after token compression during inference. - Experiments show that TVC improves performance on average by 3.4% across five mathematical reasoning datasets compared to previous state-of-the-art models. - TVC's effectiveness is demonstrated through improved reasoning capabilities, especially on complex benchmarks such as MathVerse, which focuses on multimodal reasoning. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | N/A |
| [ELTEX: A Framework for Domain-Driven Synthetic Data Generation](https://arxiv.org/abs/2503.15055) | Eugene Dmitriev, Julien Capitaine, Sofia Sedlova, Kseniia Murasheva, lavriz | - ELTEX (Efficient LLM Token Extraction), a new domain-driven framework, generates high-quality synthetic training data for specialized domains by integrating domain indicator extraction with dynamic prompting. - This framework addresses the scarcity of domain-specific training data that limits large language models (LLMs) in specialized domains like cybersecurity. - The authors demonstrated ELTEX's effectiveness in blockchain-related cyberattack detection by fine-tuning Gemma-2B and achieving performance comparable to GPT-4 with fewer computational resources. - ELTEX-enhanced model shows competitive results across standard classification metrics and uncertainty calibration (measured by Brier score). - A curated synthetic dataset of social media texts for cyberattack detection in blockchain is released. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](huggingface.co) |
