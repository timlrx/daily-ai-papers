

## Papers for 2025-03-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MAPS: A Multi-Agent Framework Based on Big Seven Personality and
  Socratic Guidance for Multimodal Scientific Problem Solving](https://arxiv.org/abs/2503.16905) | Xinyu Zhang, Zhangqi Wang, Zhiyuan Wang, Qika, VentureZJ | - This paper introduces MAPS, a Multi-Agent framework based on the Big Seven Personality theory and Socratic questioning, for solving Multimodal Scientific Problems (MSPs). - The framework employs seven distinct agents with specific skills to perform problem-solving in a cooperative manner. - A progressive four-agent solving strategy is proposed where each agent focuses on a specific stage of the problem-solving process (Interpreter, Aligner, Scholar, and Solver), and a Critic agent provides feedback to refine reasoning, simulating human reflection. - Experiments conducted on EMMA, Olympiad, and MathVista datasets demonstrates that MAPS outperforms current state-of-the-art models by 15.84% across all tasks and even surpasses human expert performance by 3.58%. | ['Multimodal', 'Question Answering'] | [Link](https://github.com/exoskeletonzj/MAPS) | N/A |
| [MARS: A Multi-Agent Framework Incorporating Socratic Guidance for
  Automated Prompt Optimization](https://arxiv.org/abs/2503.16874) | Jun Liu, Haiping Zhu, Zhangqi Wang, Qika, VentureZJ | - This paper introduces MARS (Multi-Agent framework Incorporating Socratic guidance), a novel multi-agent framework designed for Automated Prompt Optimization (APO). - MARS utilizes a seven-agent architecture, including a Planner to devise flexible optimization paths and a Teacher-Critic-Student module for iterative prompt refinement through Socratic dialogue. - MARS addresses the limitations of existing APO methods, such as the inflexibility of fixed templates and inefficient search in prompt spaces. - Experimental results on 12 general tasks and 5 domain-specific datasets show that MARS surpasses previous state-of-the-art methods and significantly improves performance compared to original prompts and Chain-of-Thought (CoT) prompts. - An efficiency analysis demonstrates MARS effectively balances resource consumption and performance improvement. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [RoboFactory: Exploring Embodied Agent Collaboration with Compositional
  Constraints](https://arxiv.org/abs/2503.16408) | Xiaohong Liu, Zhenfei Yin, Xiufeng Song, FACEONG, IranQin | - RoboFactory, a framework for automated data collection in multi-agent embodied systems, is introduced, focusing on generating safe and efficient training data. - The framework uses compositional constraints (logical, spatial, and temporal) to govern agent behavior and ensure collaboration effectiveness. - RoboBrain, a large language model (e.g., GPT-40), generates subgoals and constraints based on task instructions and feedback, while RoboChecker utilizes constraint interfaces to validate trajectories and prevent violations. - A new benchmark for embodied multi-agent manipulation, also named RoboFactory, featuring 11 tasks with varying agent numbers and environment settings within the ManiSkill simulator, is proposed. - Evaluation using Diffusion Policy on this benchmark showed promising results, highlighting the importance of sufficient data and the framework's ability to generate high-quality datasets, but also revealed performance degradation with increasing agent numbers and limitations in long-term temporal dependency learning. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://iranqin.github.io/robofactory/) | N/A |
| [When Less is Enough: Adaptive Token Reduction for Efficient Image
  Representation](https://arxiv.org/abs/2503.16660) | Andrey Kuznetsov, Elizaveta Goncharova, Eduard Allakhverdov | - This paper introduces a novel method for reducing the number of visual tokens generated by vision encoders in multimodal models, aiming to improve efficiency without compromising performance. - The method employs an autoencoder with a Gumbel-Softmax selection mechanism to identify and retain the most informative visual tokens, allowing for the reconstruction of less valuable features from more valuable ones. - Experiments on LLaVA-NeXT and LLaVA-OneVision models show that up to 50% of the visual context can be discarded with minimal performance loss on OCR-based tasks using the proposed selection method, outperforming random selection. - In general domain tasks, the performance is less affected by the visual token reduction, where retaining as little as 30% of the original tokens often yields performance on par with utilizing the full set.  - This adaptive token reduction approach provides a promising direction for more scalable and efficient multimodal inference without significant performance degradation. | ['Multimodal', 'Image Feature Extraction'] | N/A | N/A |
| [OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning
  via Iterative Self-Improvement](https://arxiv.org/abs/2503.17352) | Wei Wang, Nanyun Peng, Fan Yin, Hritik Bansal, Yihe Deng | - OpenVLThinker-7B, a large vision-language model (LVLM), is introduced, trained using iterative self-improvement involving supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance complex reasoning abilities. - The model leverages a warm-start approach where SFT establishes initial reasoning structure and RL drives performance gains and generalization, utilizing distilled reasoning from text-based models in a vision-language context. - OpenVLThinker iteratively refines through RL, specifically Group Relative Policy Optimization (GRPO), using the improved model to generate refined SFT datasets for subsequent iterations. - Evaluation on visual reasoning benchmarks, including MathVista, MathVerse, and MathVision, demonstrates consistent improvement, surpassing or matching the performance of existing models like GPT-4 and Qwen2.5-VL-7B. - The work suggests the potential of combining SFT and RL for complex reasoning in LVLMs and provides early evidence for integrating R1-style reasoning into multimodal contexts. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text'] | [Link](https://github.com/yihedeng9/OpenVLThinker) | N/A |
| [Modifying Large Language Model Post-Training for Diverse Creative
  Writing](https://arxiv.org/abs/2503.17126) | Max Kreminski, Yuqian Sun, Melissa Roemmele, Vishakh Padmakumar, John Joon Young Chung | - This paper introduces diversified versions of Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO) for training large language models (LLMs) to generate diverse and high-quality creative writing. - The core idea is to incorporate deviation, a measure of how different a training instance is from others with the same prompt, into the training objective. - This approach encourages the model to learn from rarer, high-quality examples and promotes diversity in generated text. - Experiments show that the diversified methods achieve comparable quality to state-of-the-art models while significantly improving diversity, even surpassing human-generated text in some metrics. - The approach is robust to variations in dataset size and outperforms existing diversification methods like DivPO. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/mj-storytelling/DiversityTuning) | [Link](https://huggingface.co/datasets/euclaise/WritingPrompts_preferences) |
| [MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical
  Problems](https://arxiv.org/abs/2503.16549) | Jun Cen, Tao Feng, Yunqiu Xu, Felix Chen, JacobYuan | - This paper introduces MathFlow, a modular problem-solving pipeline designed to improve Multimodal Large Language Models (MLLMs) performance on visual mathematical problems by decoupling perception and inference stages. - A novel benchmark called FlowVerse is proposed which categorizes information into four components: Descriptive, Essential, Reasoned Property, and Question.   - MathFlow-P-7B, a dedicated perception model trained using a two-stage approach is developed to extract key information from diagrams. - Results on FlowVerse and MathVerse show that MathFlow-P-7B significantly improves performance when integrated with various inference models, achieving state-of-the-art results with some combinations.  - The results highlight the importance of strong perception capabilities in visual mathematical problem-solving and the effectiveness of the modular design. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/MathFlow-zju/MathFlow) | N/A |
| [ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question
  Generation and Answering](https://arxiv.org/abs/2503.16867) | Wei Liu, Peng Zhang, Yuchong Sun, Zhengfeng Lai, Guan123 | - This paper introduces ETVA, a novel evaluation method for assessing text-to-video alignment, addressing limitations of existing metrics by employing fine-grained question generation and answering. - ETVA uses a multi-agent system to parse prompts into scene graphs for generating atomic questions and a knowledge-augmented multi-stage reasoning framework with auxiliary LLMs and video LLMs for question answering.  - A comprehensive ETVA-Bench benchmark featuring 2k diverse prompts and 12k atomic questions across 10 categories is also presented. - Evaluation results on ETVA-Bench demonstrate that ETVA achieves significantly higher correlation with human judgment (Spearman's correlation of 58.47) compared to existing metrics like VideoScore which achieves 31.0. - Systematic evaluation of 15 T2V models reveals their limitations, especially in handling temporal dynamics and physics, highlighting areas for improvement in next-generation text-to-video models. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [FastCuRL: Curriculum Reinforcement Learning with Progressive Context
  Extension for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287) | Xuan Luo, Wenjie Yang, Zheng Li, Mao Zheng, Mingyang Song | - FASTCURL is a Curriculum Reinforcement Learning approach with a context window extension strategy designed for efficient training of R1-like reasoning models. - It involves length-aware training data segmentation and context window extension training. - The former splits training data by input prompt length into three levels, and the latter uses these segmented datasets with progressively increasing context window length to train the model. - FASTCURL-1.5B-Preview outperforms DeepScaleR-1.5B-Preview on five datasets (MATH 500, AIME 2024, AMC 2023, Minerva Math, and OlympiadBench) using only 50% of the training steps. - All training for FASTCURL-1.5B-Preview is completed on a single 8-GPU node. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/nick7nlp/FastCuRL) | [Link](https://huggingface.co/datasets/Nickyang/FastCuRL), [Link](https://huggingface.co/Nickyang/FastCuRL-1.5B-Preview) |
| [PVChat: Personalized Video Chat with One-Shot Learning](https://arxiv.org/abs/2503.17069) | Yuchen Li, Yumeng Li, Gang Xu, Weilong Yan, Master-Shi | - PVChat is a personalized Video Large Language Model (ViLLM) that enables subject-aware question answering from a single video using a one-shot learning approach. - It utilizes a Mixture-of-Heads (MoH) enhanced ViLLM architecture with ReLU Routing and two novel training objectives: Smooth Proximity Regularization and Head Activation Enhancement, to facilitate personalized learning. - A systematic data augmentation pipeline generates identity-preserving positive and hard negative samples, along with question-answer pairs across existence, appearance, action, and location categories. - A two-stage training strategy, transitioning from image pre-training to video fine-tuning, progressively develops the model's capacity from static attributes to dynamic representations. - Experimental results on diverse video datasets demonstrate that PVChat achieves state-of-the-art performance in personalized video understanding and question answering. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [From Head to Tail: Towards Balanced Representation in Large
  Vision-Language Models through Adaptive Data Calibration](https://arxiv.org/abs/2503.12821) | Yu Cheng, Jiawei Zhou, Xiaoye Qu, hitsmy | - This paper introduces the Adaptive Data Refinement (ADR) framework, a method designed to improve Large Vision-Language Models (LVLMs) by addressing the long-tail problem in training data, where certain concepts are overrepresented while others are underrepresented. - ADR consists of two stages: Data Rebalancing (DR) and Data Synthesis (DS).  The DR stage filters redundant data based on entity distributions to mitigate overfitting, while the DS stage utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic data for underrepresented concepts. - Evaluation across eleven benchmarks shows ADR improves the average performance of LLaVA 1.5 by 4.36% relative to the baseline without increasing the training data volume.  The improvement is observed not only in overall performance but also specifically on tail data, indicating effective mitigation of the long-tail problem. - The framework is model-agnostic and data-agnostic, allowing for easy integration with existing LVLMs and datasets. - Analysis of the long-tail problem identifies four key perspectives contributing to the imbalance: tokens, objects, co-occurrences, and interrogations. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Implicit Bias-Like Patterns in Reasoning Models](https://arxiv.org/abs/2503.11572) | Calvin K. Lai, l048596 | - This paper introduces the Reasoning Model Implicit Association Test (RM-IAT), a method for studying implicit bias-like patterns in AI reasoning models, focusing on the model's processing rather than just its outputs. - The RM-IAT adapts the human Implicit Association Test (IAT) by measuring the number of reasoning tokens used by LLMs like OpenAI's 03-mini when categorizing words related to social groups and attributes in association-compatible and -incompatible pairings. - Results reveal that 03-mini requires significantly more tokens for association-incompatible pairings in 9 out of 10 RM-IATs, indicating greater computational effort and mirroring human implicit bias in processing efficiency. - This bias in computational effort has potential implications for real-world applications, as it suggests that reasoning models may unintentionally perpetuate social stereotypes due to their reliance on pre-existing patterns in data. - Further research on how RLHF and other alignment techniques interact with these implicit-like processing patterns in reasoning models is necessary to address the concerns raised. | ['Natural Language Processing'] | N/A | N/A |
| [Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language
  Model](https://arxiv.org/abs/2503.16282) | Junlin Han, Runjia Li, Yun Liu, Guolei Sun, Zhaochong An | - This paper introduces GFS-VL, a generalized few-shot 3D point cloud segmentation framework that leverages the open-world knowledge of 3D Vision-Language Models (3D VLMs) to enhance the performance of few-shot learning. - GFS-VL addresses the noisy nature of 3D VLM predictions by using a prototype-guided pseudo-label selection method, filtering noisy regions and adaptively infilling unlabeled areas using contextual information and few-shot samples. - To further enhance learning, a novel-base mix strategy is employed, integrating support samples into training scenes while preserving context. - The authors introduce two new benchmarks with increased diversity and quantity of novel classes for a comprehensive evaluation. - Experiments demonstrate state-of-the-art performance across existing and new benchmarks, validating the efficacy of GFS-VL in generalizing to diverse novel classes. | ['Image Segmentation', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/ZhaochongAn/GFS-VL) | N/A |
