

## Papers for 2025-03-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [CoMP: Continual Multimodal Pre-training for Vision Foundation Models](https://arxiv.org/abs/2503.18931) | Yu-Gang Jiang, Zuxuan Wu, Wujian Peng, Lingchen Meng, Row11n | - COMP, a continual multimodal pre-training framework, enhances pre-trained Vision Foundation Models (VFMs) by enabling them to process native resolution images and aligning visual features with the text embedding space of Large Language Models (LLMs). - It introduces C-ROPE, which combines learned absolute position embeddings with Rotary Position Embedding, allowing VFMs to handle arbitrary image sizes without resizing and preserving pre-trained knowledge. - It employs an Alignment Loss, a cross-entropy loss based on language prototypes, to align representations between VFMs and LLMs. - This three-stage continual pre-training method combines a vision-language adapter warm-up, native resolution adaptation, and optional instruction tuning. - Experimental results show that COMP-enhanced VFMs achieve superior performance not only in multimodal understanding tasks like ChartQA and DocVQA but also maintain competitive results in image classification and segmentation tasks like ImageNet-1K and ADE20K. | ['Multimodal', 'Image Classification', 'Image Segmentation', 'Visual Question Answering', 'Document Question Answering'] | [Link](https://github.com/SliMM-X/CoMP-MM) | N/A |
| [Exploring Hallucination of Large Multimodal Models in Video
  Understanding: Benchmark, Analysis and Mitigation](https://arxiv.org/abs/2503.19622) | Yue Liu, Baolong Bi, Jingyi Tang, Jiashu Qu, Hongcheng Gao | - This paper introduces HAVEN, a benchmark for evaluating hallucinations in video understanding for Large Multimodal Models (LMMs), focusing on object, scene, and event hallucinations arising from prior knowledge conflicts, in-context conflicts, and inherent model limitations. - The benchmark includes 6,497 questions across various formats, evaluating 16 LMMs and revealing that Valley-Eagle-7B and GPT40-mini exhibit the lowest hallucination rates. - The study analyzes the impact of video duration, frame count, question length, and model size on hallucination, finding that performance initially improves with longer videos and more frames but degrades beyond a certain point, while longer questions consistently decrease performance and larger models tend to hallucinate less. - A novel training strategy combining Supervised Reasoning Fine-tuning (SRFT) and Thinking-based Direct Preference Optimization (TDPO) is proposed to mitigate hallucinations by enhancing reasoning and grounding the thinking process. - Applying this strategy to LLaVA-NEXT-Video-DPO-7B shows a 7.65% improvement in accuracy on HAVEN and a 4.5% reduction in bias score, demonstrating the effectiveness of the proposed approach in reducing hallucinations and enhancing response consistency. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Hongcheng-Gao/HAVEN) | N/A |
| [Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection
  with Artifact Explanation](https://arxiv.org/abs/2503.14905) | Zichen Wen, Hengrui Kang, Peilin Feng, Junyan Ye, Siwei Wen | - Introduces FakeVLM, a large multimodal model for synthetic image detection and artifact explanation, based on the LLaVA architecture and trained to provide natural language explanations for image artifacts. - Presents FakeClue, a dataset of over 100,000 real and synthetic images across seven categories with fine-grained artifact annotations generated using a multi-LMM strategy. - Achieves comparable performance to expert models in authenticity classification without requiring additional classifiers, demonstrated through extensive evaluations on multiple datasets including FakeClue and LOKI. - Outperforms existing general-purpose large models in both synthetic detection and artifact explanation tasks on FakeClue and LOKI, showing significant improvements in accuracy and F1 scores. - Provides more human-interpretable explanations for synthetic detection results compared to traditional methods that rely on probability thresholds, as showcased in qualitative examples across various image categories. | ['Multimodal', 'Computer Vision', 'Image Classification'] | [Link](https://github.com/opendatalab/FakeVLM) | N/A |
| [Scaling Vision Pre-Training to 4K Resolution](https://arxiv.org/abs/2503.19903) | Sifei Liu, Yao Lu, Han Cai, Boyi Li, Baifeng Shi | The paper introduces PS3, a new method for scaling CLIP-style vision pre-training to 4K resolution with near-constant cost.  Instead of contrastive learning on global image representations, PS3 selectively processes local regions and contrasts them with local detailed captions.  When integrated into a multimodal large language model (MLLM), the resulting model (VILA-HD) significantly improves high-resolution visual perception, outperforming previous MLLMs on multiple benchmarks and achieving better efficiency than existing token pruning approaches. The authors introduce a new benchmark, 4KPro, demonstrating VILA-HD's superior performance on image QA at 4K resolution.  PS3 unlocks appealing scaling properties, including free resolution scaling and test-time compute trading for better performance. | ['Multimodal', 'Visual Question Answering', 'Image Feature Extraction'] | [Link](https://nvlabs.github.io/PS3) | N/A |
| [Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time
  Thinking](https://arxiv.org/abs/2503.19855) | Yunjie Ji, Shuaiting Chen, Haotian Wang, Sitong Zhao, Xiaoyu Tian | - This paper introduces "Multi-round Thinking," a test-time scaling method for Large Language Models (LLMs) that enhances reasoning abilities through iterative answer refinement. - The method leverages previous answers as prompts for subsequent reasoning rounds, encouraging independent re-evaluation and error correction, mimicking human cognitive processes. - Experiments across multiple models and benchmarks (AIME 2024, MATH-500, GPQA-diamond, LiveCodeBench) demonstrate performance improvements with this approach. For example, QwQ-32B's accuracy on AIME 2024 increased from 80.3% to 82.1% and DeepSeek-R1 from 79.7% to 82.0% using just two rounds. - Analysis shows a correlation between improved performance and shorter response lengths with increasing rounds, suggesting enhanced conciseness and confidence in reasoning. - A preliminary exploration of combining Multi-round Thinking with supervised fine-tuning opens promising directions for future research into further enhancing LLM reasoning. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [CoLLM: A Large Language Model for Composed Image Retrieval](https://arxiv.org/abs/2503.19910) | Son Tran, Mubarak Shah, Ashish Tawari, Jinyu Yang, Chuong Huynh | This paper introduces CoLLM, a novel framework for composed image retrieval (CIR). CoLLM leverages large language models (LLMs) to generate joint embeddings of reference images and modification texts, enabling supervised training without manual annotation. It addresses limitations of existing methods by generating triplets on-the-fly from image-caption pairs and introducing a new large-scale dataset, MTCIR.  CoLLM achieves state-of-the-art performance on various CIR benchmarks, showing significant improvements of up to 15% compared to existing methods.  The refined benchmarks contribute towards more reliable evaluation metrics for future CIR research. | ['Multimodal'] | [Link](https://github.com/collm-cvpr25) | N/A |
| [MDocAgent: A Multi-Modal Multi-Agent Framework for Document
  Understanding](https://arxiv.org/abs/2503.13964) | Yun Li, Tong Sun, Ruiyi Zhang, Peng Xia, Siwei Han | - MDocAgent is a novel Retrieval Augmented Generation (RAG) framework employing a multi-agent system with specialized agents for document understanding. - It leverages both text and image modalities, using ColBERTv2 and ColPali for context retrieval, and employs five specialized agents: general, critical, text, image, and summarizing agents. - MDocAgent outperforms existing LVLMs and RAG-based methods on five benchmarks (MMLongBench, LongDocURL, PaperTab, PaperText, and FetaTab), achieving an average improvement of 12.1%. - Ablation studies validate the importance of each agent and the synergistic benefit of combining textual and visual modalities. - A case study highlights the framework's ability to accurately synthesize information from multiple modalities within a document, even with imperfect retrieval. | ['Document Question Answering', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/aiming-lab/MDocAgent) | N/A |
| [ReSearch: Learning to Reason with Search for LLMs via Reinforcement
  Learning](https://arxiv.org/abs/2503.19470) | Chenzheng Zhu, Yijie Zhou, Haoze Sun, Tianpeng Li, Mingyang Chen | - ReSearch, a novel framework, trains LLMs to reason with search via reinforcement learning, eliminating the need for supervised data on reasoning steps. - The framework integrates search operations as part of the reasoning chain, where text-based thinking guides search execution, and search results influence subsequent reasoning. - Trained on Qwen2.5 models, ReSearch demonstrates significant improvements (8.9% to 22.4%) over baselines on multi-hop question answering benchmarks. - The model exhibits strong generalizability, performing well on various benchmarks despite training on a single dataset (MuSiQue). - Analysis reveals ReSearch's ability to elicit advanced reasoning capabilities like reflection and self-correction during training. | ['Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Agent-RL/ReSearch) | N/A |
| [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041) | Mengshu Sun, Lin Yuan, Yujie Luo, Mengru Wang, Kangwei Liu | - LookAhead Tuning, a novel method for mitigating safety degradation in large language models (LLMs) during fine-tuning, is introduced, which involves modifying training data by previewing partial answer prefixes to preserve inherent safety mechanisms. - Two variants of LookAhead Tuning are presented: the Real Answer approach incorporates actual initial tokens for explicit guidance, while the Virtual Answer approach uses a generic prefix to avoid revealing answer content, both aiming to minimize perturbations to initial token distributions and maintain safety. - Experimental results on LLaMA2-7B-Chat, fine-tuned on GSM8K and SAMSum datasets and evaluated for safety with HEx-PHI, demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing downstream task performance compared to Vanilla Fine-tuning, SDFT, and Constrained SFT. - Further analysis confirms that fine-tuning safety correlates with reduced KL divergence of early tokens, validating the theoretical framework and showing that previewing more tokens increases safety at a potential cost to downstream performance, and virtual prefix variations demonstrate robustness. - LookAhead Tuning offers a simple, resource-efficient (only 1.65% and 2.56% more computational time for real and virtual methods, respectively), and effective solution for safe and effective LLM adaptation, applicable in practical deployments due to unchanged inference data and greedy decoding. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zjunlp/LookAheadTuning) | N/A |
| [When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only
  Training For Human-Centered Decision Making](https://arxiv.org/abs/2503.16965) | Yu Yin, Jing Li, Zhe Hu | - This paper introduces a novel text-only training approach for enhancing Visual Language Models (VLMs) in human-centered decision-making tasks. - The study reveals that Large Language Models (LLMs) often outperform their VLM counterparts in such scenarios, suggesting that visual processing may hinder decision-making capabilities. - By leveraging text-only training data generated by GPT-4, the proposed method strengthens the language components of VLMs, leading to significant performance gains. - Furthermore, the research demonstrates that VLMs can achieve self-improvement by using training data generated by smaller LLMs, offering a more efficient and scalable alternative to traditional training methods. - Experimental results on the VIVA benchmark show that text-only training improves the accuracy of various VLMs, with Qwen2-VL achieving the highest improvement from 80.32% to 83.15%. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Towards a Unified Copernicus Foundation Model for Earth Vision](https://arxiv.org/abs/2503.11849) | Thomas Dujardin, Adam J. Stewart, Chenying Liu, Zhitong Xiong, Yi Wang |  - This paper introduces Copernicus-FM, a unified foundation model for Earth vision, which integrates data from multiple Copernicus Sentinel missions, covering both surface and atmospheric features.  - The model architecture utilizes dynamic hypernetworks to handle various sensor modalities and metadata encoding, enhancing flexibility and scalability.  - A comprehensive benchmark, Copernicus-Bench, is presented, including 15 hierarchical tasks assessing performance across different Sentinel missions and data types.  - The authors demonstrate that their model outperforms existing methods on multiple downstream tasks, particularly in processing lower-resolution sensors and atmospheric data.  - The work also explores using grid embeddings derived from Copernicus-FM to connect EO and climate research, indicating promising results in climate prediction tasks. | ['Image Classification', 'Image Segmentation', 'Image Feature Extraction', 'Multimodal'] | [Link](https://github.com/zhu-xlab/Copernicus-FM) | N/A |
