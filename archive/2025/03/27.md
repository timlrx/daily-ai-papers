

## Papers for 2025-03-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Dita: Scaling Diffusion Transformer for Generalist
  Vision-Language-Action Policy](https://arxiv.org/abs/2503.19757) | TTTTTony, MIASANMIA, robot-haonan, TianyiZhang0213, zhihou | - Dita, a novel Diffusion Transformer (DiT) policy for generalist robotic learning, leverages a Transformer architecture to denoise continuous action sequences via a unified multimodal diffusion process. - Unlike previous methods that condition denoising on fused embeddings, Dita uses in-context conditioning, aligning actions with raw visual and language instruction tokens. - This approach allows Dita to explicitly model action deltas and environmental nuances, resulting in state-of-the-art or competitive performance on multiple simulation benchmarks. - Dita also demonstrates robust real-world 10-shot adaptation to complex, long-horizon tasks in novel environments, handling object arrangements and lighting variations with third-person camera input. - Its simple 334M parameter architecture offers a lightweight, versatile, and open-source baseline for generalist robot policy learning. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://robodita.github.io) | N/A |
| [Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215) | JialinWang, chenkq, bluelike, jinzheng-he, ZhifangGuo | - Qwen2.5-Omni is an end-to-end multimodal model based on the Thinker-Talker architecture, where the Thinker processes multimodal inputs (text, image, audio, and video) and generates text, while the Talker utilizes these representations to generate streaming speech and text. - It introduces Time-aligned Multimodal RoPE (TMRoPE), a novel positional embedding approach to synchronize timestamps of video and audio inputs and leverages a block-wise streaming processing method to enable streaming multimodal information input. - It utilizes a sliding-window DiT model to stream audio output and reduce latency. - Qwen2.5-Omni achieves comparable performance with single-modality models like Qwen2.5-VL and Qwen2-Audio and state-of-the-art results on multimodal benchmarks such as OmniBench and AV-Odyssey. - Its end-to-end speech instruction following capabilities are on par with its text input performance, and the streaming Talker outperforms alternatives in speech generation robustness and naturalness. | ['Multimodal', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition', 'Image-to-Text', 'Video-Text-to-Text', 'Any-to-Any'] | [Link](https://github.com/QwenLM/Qwen2.5-Omni) | [Link](https://huggingface.co/Qwen) |
| [LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?](https://arxiv.org/abs/2503.19990) | Leoxing, KennyUTC, zengyh1900, favourisnotyou, KexianTang | - Introduced LEGO-Puzzles, a novel benchmark to evaluate the multi-step spatial reasoning capabilities of Multimodal Large Language Models (MLLMs). - LEGO-Puzzles consists of 1,100 curated visual question-answering (VQA) samples across 11 distinct tasks related to spatial understanding and sequential reasoning, inspired by LEGO construction. - Evaluations on 20 state-of-the-art MLLMs reveal significant performance gaps compared to humans, especially in multi-step reasoning and spatially coherent visual output generation. - Even the most advanced MLLMs struggle with complex spatial relationships, highlighting the need for further research in multimodal spatial reasoning. - The benchmark also includes an image generation component, demonstrating the difficulty of generating spatially coherent and instruction-following image outputs even for top-performing MLLMs. | ['Multimodal', 'Visual Question Answering', 'Image-to-Image'] | N/A | N/A |
| [Wan: Open and Advanced Large-Scale Video Generative Models](https://arxiv.org/abs/2503.20314) | HermanZ, chenweix7, chaojiemao, baoleai, ang-annng | - This paper introduces Wan, a suite of open-source, large-scale video foundation models based on the diffusion transformer architecture. - Wan incorporates a novel spatio-temporal variational autoencoder (VAE), scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. - The 14B parameter model demonstrates state-of-the-art performance across multiple benchmarks, outperforming existing open-source models and commercial solutions like RunwayML, Hunyuan Video, and CogVideoX. - Wan offers multiple capabilities, including text-to-video and image-to-video generation, video editing, visual text generation in both English and Chinese, and real-time video generation using an efficient 1.3B parameter model.  - Both the code and model weights for the entire Wan series are open-sourced to facilitate community growth and advancement in video generation technology. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Text-to-Image', 'Image-to-Image', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/Wan-Video/Wan2.1) | N/A |
| [Open Deep Search: Democratizing Search with Open-source Reasoning Agents](https://arxiv.org/abs/2503.20201) | speedyarda, ljirwin, pchiniya, cabxyz, salzubi401 | - This paper introduces Open Deep Search (ODS), an open-source framework designed to democratize access to advanced search AI capabilities. - ODS enhances open-source Large Language Models (LLMs) with reasoning agents and a novel web search tool, enabling them to answer complex queries by leveraging real-time information retrieval. - ODS outperforms existing state-of-the-art closed-source solutions such as Perplexity AI and OpenAI's GPT-40 Search Preview on benchmarks like SimpleQA and FRAMES. - For example, ODS-v2 achieves 75.3% accuracy on FRAMES, a 9.7% improvement over GPT-40 Search Preview. - The framework consists of Open Search Tool for context retrieval and Open Reasoning Agent to orchestrate tools, including search and calculator, showcasing its effectiveness in complex reasoning tasks. | ['Question Answering'] | [Link](https://github.com/sentient-agi/OpenDeepSearch) | N/A |
| [GenHancer: Imperfect Generative Models are Secretly Strong
  Vision-Centric Enhancers](https://arxiv.org/abs/2503.19480) | yshan2u, yxgeee, aether25, tttoaster, msj9817 | - GenHancer, a two-stage post-training method, enhances the fine-grained visual representations of discriminative models like CLIP by leveraging lightweight, randomly initialized generative models. - This method uses visual features, specifically the [CLS] token, as conditions for self-supervised reconstruction with the generative model, transferring fine-grained knowledge to the discriminative model. - Experiments show that perfect generation is not essential for representation enhancement, and a two-stage training approach effectively mitigates irrelevant information transfer. - GenHancer consistently outperforms previous methods on the MMVP-VLM benchmark, achieving a 6.0% improvement on OpenAICLIP and demonstrating its effectiveness across various CLIP backbones. - The enhanced CLIP model can be seamlessly integrated into multimodal large language models for improved vision-centric performance, without negatively impacting global semantic understanding. | ['Image Feature Extraction', 'Multimodal'] | N/A | N/A |
| [BizGen: Advancing Article-level Visual Text Rendering for Infographics
  Generation](https://arxiv.org/abs/2503.20672) | YuanYuhui, kevinlin311tw, bohanChen, Marseclipse, wukeming11 | - BizGen is a novel framework for generating business content like infographics and slides from article-length text prompts and ultra-dense layouts, addressing the challenges of long context lengths and data scarcity. - It uses a two-stage approach: 1) Building a scalable dataset (INFOGRAPHICS-650K) of high-quality infographics with dense layouts and captions through layered retrieval augmentation. 2) Implementing a layout-guided cross-attention mechanism in Glyph-SDXL-v2 that refines subsections using layout conditional CFG. - BizGen significantly outperforms DALL-E3, SD3 Large, and FLUX in visual text spelling accuracy and prompt following on the BizEVAL benchmark, particularly with complex layouts exceeding 20 layers. - It introduces layout conditional classifier-free guidance to further refine layer artifacts. - BizGen also supports multilingual generation across ten languages, achieving approximately 90% visual text spelling precision. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [Gemini Robotics: Bringing AI into the Physical World](https://arxiv.org/abs/2503.20020) | abalakrishna123, TravisAStrong, montse90, jalayrac, saminda | - Introduces Gemini Robotics, a family of AI models designed for robotics, built upon Gemini 2.0, featuring a Vision-Language-Action (VLA) model and an Embodied Reasoning (ER) model. - Gemini Robotics controls robots directly to perform complex manipulation tasks, showing robustness and generalization to unseen environments and open-vocabulary instructions. - Gemini Robotics-ER extends Gemini's multimodal reasoning to the physical world, enabling object detection, pointing, trajectory and grasp prediction, and 3D understanding. - Demonstrates specialization of Gemini Robotics to long-horizon dexterous tasks like origami and card playing, adapting to novel robot embodiments, and few-shot learning. - Addresses safety considerations for large robotics models like Gemini Robotics. | ['Robotics', 'Multimodal'] | N/A | N/A |
| [MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree
  Search](https://arxiv.org/abs/2503.20757) | armanc, chenzhao, yilunzhao, AlexCCtop | - MCTS-RAG is introduced, a novel approach that combines Monte Carlo Tree Search (MCTS) with Retrieval-Augmented Generation (RAG) to enhance reasoning capabilities in small language models. - Unlike standard RAG or MCTS methods, MCTS-RAG dynamically integrates retrieval and reasoning, enabling adaptive retrieval strategies and improved knowledge integration. - Experimental results on ComplexWebQA, GPQA, and FoolMeTwice show significant performance improvements, exceeding 20% on some datasets with smaller language models, and demonstrating competitive performance with larger models like GPT-40. - MCTS-RAG effectively scales inference-time compute, refining both retrieval and reasoning through a search-based process to achieve higher accuracy. - The iterative refinement of queries and integration of retrieved information leads to enhanced decision-making and reduced hallucinations in knowledge-intensive tasks. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/yale-nlp/MCTS-RAG) | N/A |
| [ViLBench: A Suite for Vision-Language Process Reward Modeling](https://arxiv.org/abs/2503.20271) | cihangxie, xianft, alihiker, Helicopt, PahaII | - This paper introduces VILBENCH, a new benchmark designed to evaluate vision-language process reward models (PRMs), which provide step-wise feedback during reasoning processes, unlike output reward models (ORMs) that only evaluate final answers. - The researchers benchmark seven Vision-Language Large Language Models (VLLMs) across five existing datasets and find that better VLLMs don't always correlate with superior reward capabilities, and neither ORMs or PRMs consistently outperforms the other. - They construct VILBENCH, a new benchmark with 600 examples that emphasizes the necessity of step-wise feedback and demonstrate that OpenAI's GPT-40 with Chain-of-Thought only achieves 27.3% accuracy, while benefiting 3.0% more from PRM than ORM. - This paper also introduces ViLReward-73k, a 73.6k step-wise vision-language reward dataset, enabling the training of a specialized 3B parameter vision-language PRM (ViLPRM). - ViLPRM improves the average evaluation accuracy by 3.3% over standard CoT methods and up to 2.5% over its untrained counterpart on VILBENCH. | ['Multimodal', 'Visual Question Answering'] | [Link](https://ucsc-vlaa.github.io/ViLBench) | N/A |
| [LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior
  Accuracy Preservation](https://arxiv.org/abs/2503.19950) | Pingyi Luo, Bingsheng He, deciding, Zicong99, Concyclics | - LogQuant, a novel 2-bit quantization technique for KV Cache in large language model (LLM) inference, is introduced, delivering substantial memory savings while preserving performance. - LogQuant employs a log-based filtering mechanism to selectively compress the KV Cache across the entire context, outperforming existing methods that prioritize recent tokens or predict important tokens based on past attention patterns. - Benchmark tests show LogQuant improves throughput by 25%, increases batch size by 60% without raising memory consumption, and boosts accuracy on challenging tasks like Math and Code Completion by 40% to 200% compared to similar compression ratio techniques. - LogQuant's position-agnostic approach to attention calculation maintains accuracy while improving efficiency. - The technique integrates seamlessly with popular inference frameworks like Python's transformers library. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/Concyclics/LogQuantKV) | N/A |
| [ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving
  Systems](https://arxiv.org/abs/2503.20756) | xzwnlp, bozhong, xiangchen-dvi, JizhanFang, Chenxiwang | - This paper introduces ADS-Edit, a multimodal knowledge editing dataset for autonomous driving systems.  - The dataset addresses challenges such as traffic knowledge misunderstanding, complex road conditions, and diverse vehicle states in autonomous driving.  - ADS-Edit includes various real-world scenarios, multiple data types (video, multi-view images, single image), and comprehensive evaluation metrics (reliability, generality, and locality).  - The authors evaluate four knowledge editing baselines (Prompt, AdaLora, GRACE, and WISE) under single and lifelong editing scenarios.  - Experimental results demonstrate the effectiveness of memory-based editing methods (GRACE and WISE) in modifying LMM behavior for autonomous driving tasks, with GRACE achieving a 100% modification rate. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/zjunlp/EasyEdit) | N/A |
| [Beyond Words: Advancing Long-Text Image Generation via Multimodal
  Autoregressive Models](https://arxiv.org/abs/2503.20198) | Min Li, Lijuan, zyang39, linjieli222, Awiny | - This paper introduces LongTextAR, a novel multimodal autoregressive model specifically designed for generating images containing long text sequences, such as paragraphs or multi-sentence descriptions. - LongTextAR addresses limitations in existing text-to-image models that struggle with extended text inputs by introducing TextBinarizer, a specialized text-focused tokenizer that enhances text preservation in generated images. - LongTextAR combines TextBinarizer with a Llama 2-based autoregressive decoder and employs a hybrid tokenization strategy incorporating both visual and textual tokens for coherent image generation. - Extensive experiments demonstrate that LongTextAR outperforms current state-of-the-art models like Stable Diffusion 3.5 and GPT-40 with DALL-E 3 in generating long text accurately and consistently, especially on interleaved document and PowerPoint generation tasks. - The model also offers robust controllability over text properties such as font style, size, color, and alignment, enabling customization of generated text within images. | ['Text-to-Image', 'Multimodal'] | N/A | [Link](https://fingerrec.github.io/longtextar) |
| [Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs](https://arxiv.org/abs/2503.16870) | kw1jjang, Rock222, AndrewAhn, ya-mehdi, Anshumann | - This paper introduces Random Sampling Knowledge Distillation (RS-KD), an importance-sampling based method to accelerate knowledge distillation in Large Language Models (LLMs) by efficiently storing sparse logits. - RS-KD addresses the biases and limitations of existing sparse knowledge distillation approaches like Top-K caching by providing unbiased estimates of the teacher's probability distribution and preserving the gradient in expectation. - Using significantly less storage (only 12 tokens in experiments), RS-KD achieves performance comparable to full distillation, maintaining model performance while using only 0.01% of pre-computed teacher logits and significantly reducing training time overhead to under 10%. - The method demonstrates consistent improvements over cross-entropy training as student model size increases and remains effective across various model sizes, training tokens, and evaluation metrics, from 300M to 3B parameter models trained on up to 100B tokens. -  Further combining RS-KD with techniques like adaptive learning rates based on token confidence allows it to even surpass the performance of full knowledge distillation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
