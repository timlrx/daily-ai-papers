

## Papers for 2025-03-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through
  Lightweight Vocabulary Adaptation](https://arxiv.org/abs/2503.19693) | Roi Reichart, ehoffer, eyalbd, nitay, itaynakash | - AdaptiVocab, a novel end-to-end approach, adapts LLM vocabularies to enhance efficiency in domain-specific, low-resource settings by replacing general tokens with domain-specific n-grams. - It involves a vocabulary modification algorithm, a tokenization patching algorithm, an embedding initialization technique using exponential weighting, and a lightweight fine-tuning strategy focusing on the embedding and outer layers. - AdaptiVocab reduced token usage by over 25% in two 7B LLMs across three niche domains without compromising performance on automatic generation quality metrics or human evaluations. - Lightweight fine-tuning proved crucial, boosting domain-specific question-answering performance even for off-the-shelf LLMs, whereas other fine-tuning techniques like LoRA underperformed. - The method is tokenizer and architecture agnostic, making it widely applicable with minimal adaptation overhead. | ['Natural Language Processing', 'Text Generation'] | [Link](github.com/itay-nakash/AdaptiVocab) | N/A |
| [Exploring Data Scaling Trends and Effects in Reinforcement Learning from
  Human Feedback](https://arxiv.org/abs/2503.22230) | amusingchao, qingping95, zhengwu07, glnbyte, Swtheking | - This paper explores data scaling in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs), identifying reward hacking and decreasing response diversity as key bottlenecks. - It introduces a hybrid reward system combining Reasoning Task Verifiers (RTV) and a Generative Reward Model (GenRM) to mitigate reward hacking, and a prompt selection method (Pre-PPO) to improve response diversity. - Prioritizing mathematical and coding tasks in early RLHF training is found to boost performance due to their inherent fine-grained distinctions and clear ground truths. - Experiments across two model sizes show that RTV exhibits the strongest resistance to reward hacking, and Pre-PPO enhances model performance and generalization, especially on challenging tasks. - The work highlights the importance of careful data construction and provides practical methods for overcoming critical performance barriers in RLHF, demonstrating improved effectiveness and scalability. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for
  Sequential Recommendation](https://arxiv.org/abs/2503.22675) | Xu Chen, Jun Xu, TengShi, KID-22, TangJiakai5704 | - ReaRec, a novel reasoning-enhanced sequential recommendation framework, is proposed to improve user representation learning by incorporating multi-step implicit reasoning during inference. - ReaRec leverages reasoning chains before making final predictions, enabling increased computational depth for better capturing evolving user preference and long-tail item understanding. - Two learning strategies, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), are introduced to address optimization challenges and mitigate reasoning degradation in ReaRec. - ERL uses ensemble learning to aggregate diverse reasoning results, while PRL employs progressive temperature annealing and contrastive learning for refined reasoning. - ReaRec consistently outperforms existing ID-based and text-based sequential recommendation models, enhancing performance by up to 50% on five real-world datasets via optimal reasoning step selection. | ['Natural Language Processing', 'Question Answering', 'Summarization', 'Feature Extraction', 'Text Generation'] | [Link](https://github.com/TangJiakai/ReaRec) | N/A |
| [A Survey of Efficient Reasoning for Large Reasoning Models: Language,
  Multimodality, and Beyond](https://arxiv.org/abs/2503.21614) | Elliott, weigao266, Warrieryes, yaful, Xiaoye08 | - This paper surveys recent efforts to improve the reasoning efficiency of Large Reasoning Models (LRMs), focusing on reducing the tendency of these models to produce excessively long and often redundant reasoning traces. - The authors categorize common patterns of inefficiency, such as redundant content, overthinking simple questions, and incoherent reasoning, and discuss the challenges unique to optimizing reasoning efficiency in LRMs, including quantifying reasoning utility, controlling thinking length, architectural limitations, and cross-task generalization. - The survey organizes methods for improving reasoning efficiency across the LRM lifecycle, covering pretraining techniques like latent space pretraining and subquadratic attention, supervised fine-tuning strategies like reasoning chain compression and latent-space SFT, reinforcement learning methods with and without length rewards, and inference-time techniques like length budgeting, system switching, model switching, and parallel search. -  The paper further explores potential future research directions, including efficient multimodal and video reasoning, efficient test-time scaling and infinity thinking, efficient and trustworthy reasoning, and building efficient reasoning applications. - The authors aim to provide a comprehensive overview of the current state and future trends in efficient reasoning for LRMs, serving as a valuable resource for researchers in this rapidly evolving field. | ['Natural Language Processing', 'Question Answering', 'Multimodal'] | N/A | N/A |
| [PHYSICS: Benchmarking Foundation Models on University-Level Physics
  Problem Solving](https://arxiv.org/abs/2503.21821) | armanc, jsous, henryL7, yilunzhao, Carrie777 | - PHYSICS, a benchmark dataset comprising 1,297 university-level physics problems across six core domains (classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics), is introduced to assess the problem-solving skills of foundation models. - A robust automated evaluation system using SymPy and GPT-40 is implemented to standardize mathematical expressions, verify numerical content, and ensure the logical correctness of solutions. - Evaluation across 33 foundation models reveals that even the best-performing model (o3-mini) achieves only 59.9% accuracy, highlighting the significant challenges physics problem-solving presents for current models. - Key areas for improvement are identified through comprehensive error analysis and diverse prompting strategies, and retrieval-augmented generation (RAG) is shown to improve performance across top models. - The benchmark and analysis provide guidance for developing more advanced AI models capable of handling complex reasoning tasks in scientific domains. | ['Question Answering', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/yale-nlp/Physics) | N/A |
| [OThink-MR1: Stimulating multimodal generalized reasoning capabilities
  via dynamic reinforcement learning](https://arxiv.org/abs/2503.16081) | Changwang Zhang, Feng Liu, Yuting Zhang, Zhiyuan Liu, jwanglux | - This paper introduces OThink-MR1, a Multimodal Large Language Model (MLLM) enhanced with a dynamic reinforcement learning approach called Group Relative Policy Optimization with Dynamic KL divergence (GRPO-D) for improved generalized reasoning across diverse multimodal tasks. - GRPO-D addresses limitations of standard reinforcement learning and supervised fine-tuning by dynamically balancing exploration and exploitation during training, leading to better performance in both same-task and cross-task evaluations. - Experiments on visual counting and geometry reasoning tasks demonstrate that OThink-MR1 with GRPO-D achieves relative improvements exceeding 5.72% over supervised fine-tuning and 13.59% over standard GRPO in same-task settings, and over 61.63% improvement in cross-task generalization. - The proposed dynamic KL divergence strategy in GRPO-D enhances the model's ability to transfer knowledge effectively across different multimodal tasks by balancing exploration and exploitation, showing stronger generalized reasoning capabilities compared to traditional methods. - The paper also highlights the effectiveness of GRPO-D with smaller models like Qwen2-VL-2B-Instruct, where strategic exploration-exploitation balance compensates for limited model capacity and outperforms larger models trained with less efficient post-training methods. | ['Multimodal', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/datasets/leonardPKU/GEOQA_R1V_Train_8K), [Link](https://huggingface.co/datasets/leonardPKU/clevr_cogen_a_train), [Link](https://huggingface.co/datasets/leonardPKU/superclevr/tree/main) |
| [4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object
  Understanding](https://arxiv.org/abs/2503.17827) | mhelhoseiny, ajhamdi, TonNew, bing-li-ai, vxuanz | - This paper introduces 4D-Bench, the first benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in understanding 4D objects (3D objects with temporal evolution). - The benchmark features two tasks: 4D object question answering (with five subtasks related to different understanding dimensions) and 4D object captioning, both requiring the model to analyze multi-view videos of dynamic 3D objects. - The authors evaluate several open-source and closed-source MLLMs on 4D-Bench and find that even state-of-the-art MLLMs like GPT-4 perform significantly worse than humans, especially in object counting, action recognition and temporal reasoning. - Experimental results from 4D object caption generation reveal that current MLLMs are better at understanding object appearance than motion. - The benchmark and the experimental findings highlight a significant performance gap in 4D object understanding and offer valuable insights for improving future MLLMs in this critical direction. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text'] | N/A | [Link](https://huggingface.co/datasets/KAUST-AILab/4D-Bench) |
| [A Refined Analysis of Massive Activations in LLMs](https://arxiv.org/abs/2503.22329) | Fabian Güra, akanyaani, nilabhra, louisowen6 | - This paper analyzes "massive activations" in LLMs, finding that suppressing them isn't always detrimental to performance and that mitigation strategies like Attention KV bias are model-specific. - The study investigates novel hybrid mitigation strategies, pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT), which successfully balances mitigation with preserved downstream performance. - The analysis challenges prior assumptions about the detrimental nature of massive activations and proposes architecture-agnostic strategies for managing them. - The study uses perplexity and downstream task performance, including commonsense reasoning, question answering, and multi-step reasoning, on datasets like WikiText, C4, PG-19, HellaSwag, PIQA, SIQA, WinoGrande, TriviaQA, and ARC, with a wide range of LLMs like GPT-2, Falcon, OPT, LLaMA, Gemma, OLMo, Phi, and Mistral. - For LLaMA-1B, TVR and hybrid strategies improve downstream task accuracy by up to 2.2% compared to the baseline and outperform single mitigation strategies like KV Bias and DyT. | ['Natural Language Processing'] | [Link](https://github.com/bluorion-com/refine_massive_activations) | N/A |
| [ReFeed: Multi-dimensional Summarization Refinement with Reflective
  Reasoning on Feedback](https://arxiv.org/abs/2503.21332) | jasoncai, hwany-j, Myyhlee, hyang0503, hamzzi | - ReFeed is a novel multi-dimensional summarization refinement pipeline that leverages reflective reasoning on feedback to enhance faithfulness, completeness, and conciseness. - It addresses challenges like trade-offs between dimensions, ordering bias from feedback presentation, and inaccuracies within feedback itself by using backtracking, simultaneous reasoning, and noise filtering. - A new dataset, SumFeed-CoT, is introduced, comprising long chain-of-thought (Long-CoT) reasoning examples on feedback for training ReFeed, along with data shuffling across dimensions for mitigating ordering bias. - Experiments demonstrate ReFeed's superior performance compared to sequential and single-dimension baselines, and its comparable performance to a larger teacher model with significantly faster inference. - ReFeed also exhibits robustness to varying feedback quality, unlike other pipelines, showcasing its ability to process and validate information effectively. | ['Summarization'] | N/A | N/A |
| [MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via
  Reasoning Agentic Workflow](https://arxiv.org/abs/2503.18968) | Yueming Jin, Chang Han Low, morson, ZiyueWang | - MedAgent-Pro, a novel reasoning agentic workflow for evidence-based multi-modal medical diagnosis, is proposed.  - The model leverages a hierarchical structure with task-level reasoning for formulating unified diagnostic plans using retrieved clinical criteria and case-level diagnosis utilizing specialized vision models as tools for detailed analysis.  - Experimental results on glaucoma and heart disease diagnosis datasets demonstrate that MedAgent-Pro surpasses both general Multi-modal Large Language Models (MLLMs) and task-specific solutions, improving mean Average Classification Accuracy (mACC) by up to 32.3% and F1 scores by up to 55.1%. - Ablation studies confirm that using multiple indicators and the Mixture of Experts (MOE) decider contributes to better diagnostic performance.  - The interpretability of the system through visual evidence and clinical guidelines enhances reliability and transparency in medical decision-making. | ['Multimodal', 'Visual Question Answering', 'Image Segmentation'] | [Link](https://github.com/jinlab-imvr/MedAgent-Pro) | N/A |
| [On Large Multimodal Models as Open-World Image Classifiers](https://arxiv.org/abs/2503.21851) | Yiming Wang, Enrico Fini, paolorota, massimilianom, altndrr | - This paper evaluates the performance of Large Multimodal Models (LMMs) in open-world image classification, where the categories are not predefined. - The authors introduce a comprehensive evaluation protocol with four metrics: text inclusion, Llama inclusion, semantic similarity, and concept similarity to analyze different aspects of model predictions. - Evaluating 13 LMMs across 10 image classification benchmarks, this study shows that LMMs outperform contrastive methods in open-world settings but lag behind closed-world models. - The paper identifies challenges related to granularity and fine-grained classification, providing insights into the types of errors LMMs make. - It is shown that these errors can be reduced through tailored prompting and reasoning strategies. | ['Computer Vision', 'Image Classification', 'Zero-Shot Classification', 'Multimodal'] | [Link](https://github.com/altndrr/lmms-owc) | N/A |
