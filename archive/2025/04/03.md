

## Papers for 2025-04-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Improved Visual-Spatial Reasoning via R1-Zero-Like Training](https://arxiv.org/abs/2504.00883) | Zijian Kong, Yanhao Zhang, Qingsong Xie, Zhenyi Liao, zhijie3 | - This paper introduces a new method for improving the visual-spatial reasoning of Multimodal Large Language Models (MLLMs) using R1-Zero-like training. - The authors found that Chain of Thought (CoT) prompting is ineffective for activating visual-spatial reasoning in small- to medium-sized Qwen2-VL models. - They created a 100k sample video-based question answering dataset called VSI-100k based on ScanNet and used it for Group Relative Policy Optimization (GRPO) training. - Their vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, outperforms the base model by 12.1% on VSI-bench and surpasses GPT-40.  - The vsGRPO-7B model achieves performance comparable to LLaVA-NeXT-Video-72B. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/zhijie-group/R1-Zero-VSI) | N/A |
| [AnimeGamer: Infinite Anime Life Simulation with Next Game State
  Prediction](https://arxiv.org/abs/2504.01014) | Ying Shan, Jing Liao, Yixiao Ge, Yuying Ge, Howe666 | - AnimeGamer is a novel framework for infinite anime life simulation, leveraging Multimodal Large Language Models (MLLMs) to generate dynamic animation shots and update character states based on user instructions. - It introduces action-aware multimodal representations for animation shots, decoded into video clips using a video diffusion model, ensuring contextual consistency and dynamic movements. - AnimeGamer uses historical multimodal representations and character states to predict subsequent game states, resulting in a coherent and immersive gaming experience. - Evaluations using automated metrics and human assessments show that AnimeGamer outperforms existing methods in instruction following, contextual consistency, and overall gaming experience. -  A data collection pipeline from anime films is also proposed, enabling customized model training for diverse character experiences. | ['Text-to-Video', 'Multimodal'] | [Link](https://github.com/TencentARC/AnimeGamer) | N/A |
| [Understanding R1-Zero-Like Training: A Critical Perspective](https://arxiv.org/abs/2503.20783) | Tianyu Pang, Wenjun Li, QPHutu, Cameron-Chen, lkevinzc | - This paper analyzes R1-Zero-like training, focusing on base models and reinforcement learning (RL) components, and introduces Dr. GRPO, an unbiased optimization method to improve token efficiency. - The authors investigate various base models, including DeepSeek-V3-Base and Qwen2.5, to understand how pretraining influences RL performance and reveal potential biases such as Qwen2.5's potential pretraining on question-answer pairs. - The paper identifies an optimization bias in Group Relative Policy Optimization (GRPO) that artificially inflates response length, especially in incorrect outputs, and proposes Dr. GRPO as a solution to improve token efficiency without sacrificing reasoning performance. - A minimalist R1-Zero recipe, employing Dr. GRPO, Qwen2.5-Math-7B, and a math dataset, achieves state-of-the-art 43.3% accuracy on AIME 2024 with a 7B model using limited compute. - The study's findings on base models and RL aim to enhance future research in LLM post-training and online alignment through code and model releases. | ['Reinforcement Learning', 'Question Answering', 'Natural Language Processing'] | [Link](https://github.com/sail-sg/understand-r1-zero), [Link](https://github.com/sail-sg/oat) | [Link](https://huggingface.co/HuggingFaceTB/FineMath-Llama-3B), [Link](https://huggingface.co/AI-MO/NuminaMath-1.5) |
| [ScholarCopilot: Training Large Language Models for Academic Writing with
  Accurate Citations](https://arxiv.org/abs/2504.00824) | Zhiheng Lyu, Huaye Zeng, Ping Nie, Xueguang Ma, Yubo Wang | - ScholarCopilot, a new framework designed to enhance existing large language models (LLMs) for generating professional academic articles with accurate and contextually relevant citations, is introduced. - It dynamically determines when to retrieve scholarly references by generating a retrieval token ([RET]), which is then used to look up relevant citations from a database.  - Trained on 500K papers from arXiv, ScholarCopilot achieves a top-1 retrieval accuracy of 40.1% on the evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). - In terms of generation quality, using LLM-as-judge, ScholarCopilot scores 16.2/25 on a 1000 samples dataset across five dimensions, outperforming larger models such as Qwen-2.5-72B-Instruct (15.8).  - Human studies confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience. | ['Text Generation'] | N/A | N/A |
| [ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and
  Diffusion Refinement](https://arxiv.org/abs/2504.01934) | Yunlong Yuan, Guansong Lu, Junwei Yang, Chunwei Wang, Runhui Huang | - ILLUME+ is a unified multimodal large language model (MLLM) that integrates understanding, generation, and editing using dual visual tokenization and a diffusion decoder. - It employs DualViTok, a dual-branch vision tokenizer, to preserve both semantic and fine-grained texture information for robust image representation. - A diffusion model is used as the image detokenizer, improving generation quality and enabling efficient super-resolution. - ILLUME+ utilizes a continuous-input, discrete-output scheme within the MLLM and adopts a progressive training procedure supporting dynamic resolution across components. - The model achieves competitive performance on various multimodal benchmarks, showing strong results in understanding, high-resolution generation (up to 1024x1024), and improved texture preservation in editing compared to its predecessor, ILLUME. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image', 'Visual Question Answering'] | N/A | N/A |
| [Safeguarding Vision-Language Models: Mitigating Vulnerabilities to
  Gaussian Noise in Perturbation-based Attacks](https://arxiv.org/abs/2504.01308) | Zhendong Liu, Yushen Zuo, sofyc, AllenChai, Jarvis1111 | - This paper introduces Robust-VLGuard, a multimodal safety dataset with aligned and misaligned image-text pairs, combined with noise-augmented fine-tuning to mitigate vulnerabilities of Vision-Language Models (VLMs) to Gaussian noise in perturbation-based attacks. - It also proposes DiffPure-VLM, a defense pipeline integrating diffusion models with Gaussian-noise-tolerant VLMs, leveraging the distribution-shifting property of diffusion models to transform adversarial perturbations into Gaussian-like noise. - Experiments demonstrate that DiffPure-VLM effectively mitigates adversarial perturbations across varying intensities, significantly improving VLM robustness against Gaussian noise and other optimization-based attacks. - The authors systematically evaluate the robustness of three mainstream VLMs (InternVL2-8B, LLaVA-v1.5-7B, and MiniGPT-4-13B) against Gaussian noise and find significant performance degradation in both helpfulness and safety. - The Robust-VLGuard dataset addresses limitations of existing datasets like VLGuard by including image-text misalignment scenarios and detailed responses generated by GPT-4V, which improved helpfulness and safety of the tested VLMs. | ['Multimodal'] | [Link](https://github.com/JarvisUSTC/DiffPure-RobustVLM) | N/A |
| [DASH: Detection and Assessment of Systematic Hallucinations of VLMs](https://arxiv.org/abs/2503.23573) | Matthias Hein, Maximilian Augustin, YanNeu | - This research introduces DASH (Detection and Assessment of Systematic Hallucinations), an automated, large-scale pipeline for identifying systematic object hallucinations in Vision-Language Models (VLMs). - DASH employs two methods: DASH-LLM, using text-based retrieval with LLM-generated queries, and DASH-OPT, which optimizes a latent diffusion model to generate images that trigger VLM hallucinations while having low object detector confidence. - Applied to PaliGemma and LLaVA-NeXT models on 380 object classes, DASH identified over 19,000 hallucination clusters encompassing 950,000 images. - These hallucinations transferred to other VLMs, including top-performing models like QwenV2-72B.  - Fine-tuning PaliGemma with DASH-generated images demonstrated a reduction in object hallucinations. | ['Multimodal', 'Computer Vision', 'Object Detection'] | [Link](https://YanNeu.github.io/DASH) | N/A |
| [VerifiAgent: a Unified Verification Agent in Language Model Reasoning](https://arxiv.org/abs/2504.00406) | Ehsan Shareghi, Wray Buntine, Jiuzhou Han | - VerifiAgent, a novel verification framework, enhances the reliability of Large Language Models (LLMs) by verifying and improving their outputs across diverse reasoning tasks, including mathematical, logical, commonsense, and hybrid reasoning. - This two-layer verification agent employs both meta-verification, assessing completeness and consistency, and tool-based adaptive verification, selecting appropriate external tools like Python interpreters or symbolic solvers for enhanced accuracy. - Experimental results demonstrate VerifiAgent's superior performance compared to existing baselines such as deductive and backward verifiers, achieving higher accuracy while maintaining competitive precision and recall. - Additionally, VerifiAgent effectively integrates with inference scaling methods, improving accuracy with fewer samples and lower cost than traditional Process Reward Models (PRMs). - This framework benefits from the capabilities of its backbone LLM, scaling its verification effectiveness alongside improvements in the underlying language model. | ['Question Answering', 'Natural Language Processing'] | [Link](https://github.com/Jiuzhouh/VerifiAgent) | N/A |
| [Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal
  Representations](https://arxiv.org/abs/2503.18817) | Sangheum Hwang, mawjdgus | - This paper introduces Cross-Modal Alignment (CMA), a novel multi-modal fine-tuning method for Vision-Language Models (VLMs) designed to enhance Out-of-Distribution (OoD) detection. - CMA addresses the modality gap in embedding space by aligning image and text embeddings of in-distribution data, improving the utilization of pretrained textual knowledge, especially negative concept labels, for OoD detection. - The method achieves state-of-the-art performance on OoD detection benchmarks such as MOS and OpenOOD v1.5, surpassing existing zero-shot, prompt learning, and other multi-modal fine-tuning approaches. - On the MOS benchmark, CMA achieves a 19.93% FPR95 and a 95.13% AUROC, significantly outperforming other methods. - CMA also demonstrates strong performance in In-Distribution (ID) classification tasks, indicating its efficacy in enhancing both OoD detection and ID classification accuracy. | ['Computer Vision', 'Zero-Shot Classification', 'Multimodal'] | [Link](https://github.com/ma-kjh/CMA-OoDD) | N/A |
