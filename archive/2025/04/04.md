

## Papers for 2025-04-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Advances and Challenges in Foundation Agents: From Brain-Inspired
  Intelligence to Evolutionary, Collaborative, and Safe Systems](https://arxiv.org/abs/2504.01990) | KaitaoSong, JinlinW, Peiyan, xinfeng1i, Bang-UdeM-Mila | - This survey explores the intersection of Large Language Models (LLMs) and intelligent agents and maps human brain functionalities with corresponding modules in agentic architectures. - The paper offers a modular framework for building advanced agents, incorporating key elements like memory, world modeling, emotions, goals, and reward systems. - It discusses self-enhancement mechanisms in AI agents that leverage adaptive learning and self-reflection for continuous improvement, along with collaboration and evolution in multi-agent systems. - The survey also covers critical safety and security concerns in LLM-based agents, including threats like jailbreaking, data poisoning, and misalignment, while suggesting potential defense mechanisms. - Lastly, it discusses the concept of “superalignment” and the “scaling law of AI safety,” focusing on enhancing the safety and reliability of agents as their capabilities expand. | ['Multimodal', 'Reinforcement Learning', 'Robotics'] | [Link](https://github.com/FoundationAgents/awesome-foundation-agents) | N/A |
| [GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image
  Generation](https://arxiv.org/abs/2504.02782) | shawnxyh, BestWishYsh, SereinH, liweijia, Yejy53 | - This research paper introduces GPT-ImgEval, a benchmark designed to evaluate the image generation capabilities of GPT-40 across generation quality, editing proficiency, and world knowledge. - GPT-40's performance is quantitatively and qualitatively assessed through the GenEval, Reason-Edit, and WISE datasets.  - Across all tasks, GPT-40 demonstrates strong performance, outperforming existing methods.  - The paper also investigates the potential architecture of GPT-40 using a classification-model-based approach, with results suggesting a diffusion-based image decoding mechanism. - An analysis of GPT-40's limitations, safety implications, and multi-round editing capabilities is also provided. | ['Text-to-Image', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/PicoTrex/GPT-ImgEval) | N/A |
| [Rethinking RL Scaling for Vision Language Models: A Transparent,
  From-Scratch Framework and Comprehensive Evaluation Scheme](https://arxiv.org/abs/2504.02587) | Pengfei, IanZhong, Ryan1122, steffichern, ManTle | - This paper introduces MAYE, a transparent, from-scratch framework for Reinforcement Learning (RL) applied to Vision Language Models (VLMs), focusing on improving reasoning capabilities. - The framework uses a four-step process—data flow, response collection, trajectory generation, and policy update—and is validated across multiple VLMs and datasets, including Qwen-VL and mm_math5k/geometry3k. - A standardized evaluation scheme is also proposed, emphasizing training dynamics and reflective behaviors to ensure robust and reproducible benchmarks. - Experimental results demonstrate that RL consistently outperforms Supervised Fine-Tuning (SFT) in generalization across multiple visual reasoning tasks, even with high-quality SFT data. - Analysis reveals a strong correlation between response length and reflective behavior, and RL effectively leverages this to enhance performance. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/GAIR-NLP/MAYE) | N/A |
| [Scaling Analysis of Interleaved Speech-Text Language Models](https://arxiv.org/abs/2504.02398) | adiyoss, MajoRoth, hassid, gallilmaimon | - This paper conducts a scaling analysis of interleaved speech-text language models (SLMs), demonstrating that they scale more efficiently with compute compared to textless SLMs. - The analysis involves training dozens of interleaved SLMs with varying sizes, compute budgets, and model families, leading to practical insights for optimizing SLM performance.  - The study finds that allocating more compute budget towards increasing model size than training tokens results in better performance. - It also investigates the role of synthetic data and TextLM model families, suggesting that scaled-up interleaved SLMs achieve comparable performance to leading models on speech semantic metrics with less compute and data. -  All trained models and code are open-sourced to encourage further community exploration. | ['Audio', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/slp-rl/slamkit) | [Link](https://huggingface.co/hexgrad/Kokoro-82M) |
| [ShortV: Efficient Multimodal Large Language Models by Freezing Visual
  Tokens in Ineffective Layers](https://arxiv.org/abs/2504.00502) | xphan, sanmusunrise, luyaojie, chenjiawei-icip, yuanqianhao | - ShortV is a training-free method for optimizing Multimodal Large Language Models (MLLMs) by identifying and freezing visual token computations in ineffective layers, thereby reducing computational costs. - It introduces a novel metric called Layer Contribution (LC) to quantify the impact of a layer's transformations on visual and text tokens, identifying layers with minimal contribution to the model's output. - ShortV freezes visual tokens by replacing standard layers with sparse ShortV layers where only text tokens are processed. - Experiments across various benchmarks (MME, MMBench, MMMU, MMStar, SEED-Bench, GQA, Flickr30K) demonstrate that ShortV can freeze visual tokens in approximately 60% of MLLM layers, achieving a 50% reduction in FLOPs on LLaVA-NeXT-13B while maintaining performance. - ShortV is orthogonal to and compatible with visual token pruning methods like FastV, allowing combined use for further efficiency gains. | ['Multimodal', 'Computer Vision', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/icip-cas/ShortV) | N/A |
| [ZClip: Adaptive Spike Mitigation for LLM Pre-Training](https://arxiv.org/abs/2504.02507) | gueraf, nilabhra, louisowen6, akanyaani | - This paper introduces ZClip, a novel adaptive gradient clipping algorithm designed to mitigate loss spikes during large language model (LLM) pre-training. - ZClip dynamically adjusts the clipping threshold based on the running mean and standard deviation of gradient norms, using a z-score based anomaly detection mechanism.  - Unlike traditional methods with fixed thresholds, ZClip adapts to evolving training dynamics and avoids over-clipping.  - Experimental results on a 1B parameter LLaMA model demonstrate that ZClip stabilizes training at high learning rates, leading to faster convergence without compromising performance.  - At lower learning rates, ZClip effectively handles minor fluctuations and improves downstream task performance on HellaSwag and Winogrande benchmarks. | ['Natural Language Processing'] | [Link](https://github.com/bluorion-com/ZClip) | N/A |
| [Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495) | Chong Ruan, Shirong Ma, Runxin Xu, Peiyi Wang, Zijun Liu | - This paper introduces Self-Principled Critique Tuning (SPCT), a novel learning method to improve the inference-time scalability of generalist reward modeling (RM) for large language models (LLMs). - SPCT enables pointwise generative reward models (GRMs) to generate principles and critiques adaptively through online reinforcement learning, resulting in higher quality reward signals and better performance-compute scaling. - DeepSeek-GRM, a 27B parameter model trained with SPCT, outperforms existing methods and models on various RM benchmarks. - Inference-time scaling with parallel sampling and a meta RM further improves the performance of DeepSeek-GRM, exceeding the performance gains from training-time scaling with larger model sizes. - The proposed method addresses the challenge of generating accurate and robust rewards for general queries in diverse domains, crucial for broader applications of LLMs. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [GenPRM: Scaling Test-Time Compute of Process Reward Models via
  Generative Reasoning](https://arxiv.org/abs/2504.00891) | Saputello, dmux, ChetKao, iseesaw, RyanLiu112 | - Introduced GenPRM, a generative process reward model that enhances the reasoning capabilities of Large Language Models (LLMs) by performing explicit Chain-of-Thought (CoT) reasoning with code verification for each step. - Proposed Relative Progress Estimation (RPE) and a novel rationale data synthesis framework involving code verification to generate high-quality supervision labels. - Demonstrated significant performance improvements over previous PRMs on ProcessBench and mathematical reasoning tasks, using only 23K training data from the MATH dataset. - Showed that smaller GenPRM models can outperform much larger PRMs (e.g., 1.5B GenPRM outperforms GPT-40, and 7B GenPRM surpasses Qwen2.5-Math-PRM-72B) through test-time scaling. - Presented GenPRM's potential as a critic model for refining policy model outputs and its effectiveness in test-time scaling for enhanced process supervision capabilities. | ['Natural Language Processing', 'Question Answering'] | [Link](https://ryanliu112.github.io/GenPRM) | N/A |
| [Sparse Autoencoders Learn Monosemantic Features in Vision-Language
  Models](https://arxiv.org/abs/2504.02821) | Zeynep Akata, Serge Belongie, Quentin Bouniot, Shyamgopal Karthik, Mateusz Pach | - This paper introduces a framework for enhancing the interpretability and control of Vision-Language Models (VLMs) using Sparse Autoencoders (SAEs). - The authors propose a Monosemanticity Score (MS) to quantify the similarity of images activating a given neuron, demonstrating that SAE training significantly improves neuron monosemanticity in VLMs like CLIP. - The Matryoshka SAE architecture is shown to further enhance monosemanticity and reveal concept hierarchies that align with expert-defined structures. - The study demonstrates the ability to steer Multimodal Large Language Models (MLLMs), such as LLaVA, by intervening on SAE activations in the vision encoder without modifying the underlying LLM. - This method enables controlled manipulation of MLLM outputs towards specific concepts discovered by the SAE, showcasing the potential of SAEs for enhancing both understanding and control of VLMs. | ['Multimodal', 'Image Feature Extraction', 'Computer Vision'] | [Link](https://github.com/ExplainableML/sae-for-vlm) | N/A |
| [Whisper-LM: Improving ASR Models with Language Models for Low-Resource
  Languages](https://arxiv.org/abs/2503.23542) | Ibon Saratxaga, Eva Navas, inmahernaez, zuazo | - This paper introduces Whisper-LM, a method for enhancing Automatic Speech Recognition (ASR) performance, especially in low-resource languages, by integrating traditional (n-gram) and novel (large language models - LLMs) language models with fine-tuned Whisper models. - Whisper-LM leverages a structured fine-tuning process for Whisper models across various sizes (Tiny to Large-V3) using the Common Voice dataset and integrates language models at inference time. - Results demonstrate improvements up to 51% for in-distribution datasets and 34% for out-of-distribution sentences using statistical Language Models, while LLMs yield more moderate but robust gains and enhance model robustness. - A detailed analysis of sentence-level overlap confirms minimal leakage in evaluation datasets, ensuring the observed improvements are not due to memorization. - An ablation study reveals the substantial impact of evaluation parameters, particularly language specification and beam search, on performance, emphasizing the need for careful consideration of settings. | ['Automatic Speech Recognition', 'Natural Language Processing'] | [Link](http://www.github.com/hitz-zentroa/whisper-lm) | [Link](https://huggingface.co/openai) |
