

## Papers for 2025-04-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving](https://arxiv.org/abs/2504.02605) | Linhao Zhang, Hanwu Chen, Wei Liu, Zhirong Huang, Daoguang Zan | - This paper introduces Multi-SWE-bench, a multilingual benchmark for issue resolving comprising 1,632 human-validated issues across seven programming languages (Java, TypeScript, JavaScript, Go, Rust, C, and C++). - It also launches Multi-SWE-RL, an open-source community and dataset with 4,723 instances for building reinforcement learning environments for issue resolution. -  The authors evaluate nine state-of-the-art large language models (LLMs) across three representative methods (Agentless, SWE-agent, and OpenHands), demonstrating limited generalization of current LLMs beyond Python. - Analysis reveals that model performance is sensitive to issue difficulty, description length, and the complexity of fix patches, with shorter, single-file patches generally being easier to resolve. - The benchmark and community aim to catalyze the development of more robust, adaptable, and scalable multilingual code agents, fostering progress toward automated issue resolution in diverse software ecosystems. | ['Natural Language Processing', 'Reinforcement Learning', 'Text2Text Generation', 'Question Answering', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/multi-swe-bench/multi-swe-bench) |
| [Agentic Knowledgeable Self-awareness](https://arxiv.org/abs/2504.03553) | Xiangyuan Ru, Xiaobin Wang, Baochang Ren, Zhisong Qiu, Shuofei Qiao | - KnowSelf, a novel data-centric approach, equips Large Language Model (LLM)-based agents with agentic knowledgeable self-awareness, enabling them to dynamically regulate knowledge use during decision-making. - It uses a heuristic criterion to mark agent's self-explored trajectories for collecting training data according to three situation types: fast thinking, slow thinking, and knowledgeable thinking. - A two-stage training process is employed, involving supervised fine-tuning and an RPO loss, to instill self-awareness in the agent. - KnowSelf outperforms strong baselines on agent planning tasks with minimal external knowledge and reflection steps, demonstrating its effectiveness and efficiency. - Further analysis reveals KnowSelf effectively mitigates planning pattern overfitting, demonstrates better generalization in agent planning and performance scales with model and data size. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/zjunlp/KnowSelf) | N/A |
| [MegaMath: Pushing the Limits of Open Math Corpora](https://arxiv.org/abs/2504.02807) | Liping Tang, Zhoujun Cheng, Nikhil Ranjan, Zengzhi Wang, Fan Zhou | - MegaMath, a new open-source English math corpus totaling 371.6B tokens, pushes the boundaries of existing open math corpora in terms of scale and quality. - It incorporates diverse sources, including 279B tokens of web data, 28.1B of code, and 64.5B of synthetic data generated through optimized pipelines that ensure high-quality mathematical content and diversity.  - A premium subset, MegaMath-Web-Pro, utilizes LLM-based filtering and refining to provide an even higher-quality data source ideal for advanced training.  - Through ablation studies and benchmark evaluations, MegaMath demonstrates its effectiveness, significantly improving the mathematical reasoning capabilities of state-of-the-art large language models like the Llama-3 series by 15%-20% on tasks such as GSM8K and MATH.  - Its versatile data variants and open-source nature make MegaMath a valuable resource for research and development in the field of mathematical reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/LLM360/MegaMath) | [Link](https://hf.co/datasets/LLM360/MegaMath) |
| [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge
  Refinement](https://arxiv.org/abs/2504.03561) | Jialong Wu, Shuofei Qiao, Yuan Liang, Xiaobin Wang, Runnan Fang | - SynWorld, a novel framework, assists Large Language Model (LLM)-based agents in learning unfamiliar actions within new environments by synthesizing virtual scenarios involving multiple coordinated actions. - Through iterative Monte Carlo Tree Search (MCTS) optimization within these virtual scenarios, SynWorld refines the action descriptions and workflow patterns, ensuring better alignment with environmental constraints. - This framework addresses the limitations of existing methods that rely on single-action synthetic scenarios and linear iterative optimization processes which are prone to stagnation. - Experimental results on ToolBench and HotpotQA datasets demonstrate SynWorldâ€™s effectiveness in refining action knowledge in virtual environments. - It also improves the agent's ability to generalize this knowledge to real-world scenarios, outperforming other iterative optimization methods. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/zjunlp/SynWorld) | N/A |
| [MME-Unify: A Comprehensive Benchmark for Unified Multimodal
  Understanding and Generation Models](https://arxiv.org/abs/2504.03641) | Bingyan Nie, Yang Shi, Chaoyou Fu, Yi-Fan Zhang, Wulin Xie | - Introduces MME-Unify (MME-U), a benchmark designed to comprehensively evaluate unified multimodal large language models (U-MLLMs) on understanding, generation, and mixed-modality generation tasks. - Curates a diverse set of tasks from existing datasets, standardizing formats and metrics for consistent evaluation across 12 U-MLLMs including Janus-Pro, EMU3, and Gemini2-Flash. - Designs five novel subtasks specifically for assessing mixed-modality generation, evaluating how well models' understanding and generation capabilities enhance each other. - Reveals a significant performance gap between current U-MLLMs and specialized models in both understanding and generation, highlighting areas for improvement such as instruction following and image generation quality. - Finds a substantial variance in performance across models and a notable struggle with mixed-modality generation tasks, even among leading U-MLLMs. | ['Multimodal', 'Computer Vision', 'Image-to-Image', 'Image-to-Video', 'Text-to-Image', 'Text-to-Video'] | N/A | N/A |
| [VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via
  Iterative Instruction Tuning and Reinforcement Learning](https://arxiv.org/abs/2504.02949) | Liming Liang, Dongchao Yang, Yufan Deng, Yuxin Xie, Xianwei Zhuang | - VARGPT-v1.1 is a visual autoregressive large unified model that improves upon its predecessor, VARGPT, by incorporating iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), an expanded training corpus, and an upgraded language model backbone using Qwen2. - The model architecture consists of an LLM (Qwen2-7B-Instruct), a visual encoder, a visual decoder, and projectors for both understanding and generation, employing causal attention in the LLM and block causal attention in the decoder. - It achieves state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, outperforming comparable unified MLLMs and demonstrating significant improvements in both comprehension and generation metrics as shown in Figure 2. - The model supports text-and-image instructions and outputs text-and-image mixed modal data simultaneously. - It also exhibits flexible image-editing capabilities without architectural modifications. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/VARGPT-family/VARGPT-v1.1) | N/A |
| [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated
  Agent-Human Interplay](https://arxiv.org/abs/2504.03601) | Ming Zhu, Jianguo Zhang, Weiran Yao, Zuxin Liu, Akshara Prabhakar | - APIGen-MT is a two-phase framework for generating verifiable and diverse multi-turn agent data.  - The first phase involves an agentic pipeline that generates task blueprints with ground-truth actions using LLM reviewers and feedback loops.  - These blueprints are then transformed into interaction trajectories through simulated human-agent interplay in the second phase. - The xLAM-2-fc-r models, trained on this synthetic data, outperform models like GPT-40 and Claude 3.5 on T-bench and BFCL, especially in multi-turn settings, with smaller models often exceeding larger counterparts. - Both the synthetic data and trained xLAM-2-fc-r models are open-sourced. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://apigen-mt.github.io) | [Link](https://huggingface.co/Salesforce/xLAM-2) |
| [TransMamba: Flexibly Switching between Transformer and Mamba](https://arxiv.org/abs/2503.24067) | Shuaipeng Li, Xingwu Sun, Ruobing Xie, andyyang, Yixinglee | - TransMamba is a novel framework that merges Transformer and Mamba models for sequence modeling, enabling dynamic switching between attention and state-space mechanisms based on token length and layer. - It employs shared parameter matrices (QKV and CBx) for both Transformer and Mamba and introduces a Memory Converter to bridge the two mechanisms by transforming attention outputs into SSM-compatible states. - TransMamba incorporates a flexible TransPoint scheduling strategy to optimize the switching points between the two models, enhancing training efficiency. - Experimental results demonstrate that TransMamba achieves superior training efficiency and performance compared to standalone Transformer, Mamba2, and hybrid models. - The study validates the underlying consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling, particularly for long-sequence processing. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [EvMic: Event-based Non-contact sound recovery from effective
  spatial-temporal modeling](https://arxiv.org/abs/2504.02402) | Lu Zhang, Xudong XU, Xu Jia, Shi Guo, yyzqy | - EvMic, a novel learning-based pipeline for event-based non-contact sound recovery, leverages the high temporal resolution and large field of view of event cameras. - It uses a network with sparse convolutions for feature extraction, a multi-head self-attention spatial aggregation block, and a Mamba-based temporal modeling module to process event data and reconstruct sound. - A new synthetic dataset, EvMic, was created using Blender and an event simulator to train the model, along with a speckle-based data augmentation technique and real world data for improved generalization. - Experimental results on synthetic and real-world data demonstrated superior performance compared to existing event-based and frame-based methods, achieving higher SNR and STOI scores. -  A custom-designed imaging system with a laser matrix was used to amplify surface gradients and capture subtle vibrations in real-world scenarios. | ['Audio', 'Computer Vision', 'Multimodal'] | N/A | N/A |
| [BEATS: Bias Evaluation and Assessment Test Suite for Large Language
  Models](https://arxiv.org/abs/2503.24310) | Lisa Erickson, tbandopa, alokabhishek | - This research paper introduces BEATS (Bias Evaluation and Assessment Test Suite), a novel framework for evaluating bias, ethics, fairness, and factuality (BEFF) in Large Language Models (LLMs). - The framework utilizes a curated dataset of evaluation questions that measure performance across 29 distinct metrics spanning demographic, cognitive, and social biases, as well as ethical reasoning, group fairness, and factuality. - A consortium of LLMs acts as judges to evaluate model responses, enabling a statistically rigorous and scalable bias assessment methodology. - Empirical results reveal that 37.65% of outputs from industry-leading LLMs contain some form of bias, highlighting the risk of using these models in critical decision-making systems. - The BEATS framework provides a methodology for benchmarking LLMs, diagnosing sources of bias, and developing mitigation strategies to promote more responsible and ethical AI development. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/SocialGrep/one-million-reddit-questions) |
