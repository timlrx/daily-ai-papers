

## Papers for 2025-04-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [One-Minute Video Generation with Test-Time Training](https://arxiv.org/abs/2504.05298) | guestrin, zhaoyue-zephyrus, GashonHussein, koceja, karansdalal | - This paper introduces a new approach to generate one-minute videos from text storyboards using Test-Time Training (TTT) layers within a pre-trained Diffusion Transformer model. - TTT layers, whose hidden states are neural networks, offer increased expressiveness compared to traditional RNN layers with fixed-size hidden states, enabling the model to capture complex, multi-scene stories with dynamic motion. - The authors curate a text-to-video dataset based on Tom and Jerry cartoons to demonstrate the effectiveness of their approach, focusing on complex narratives and dynamic motion. - In human evaluations, the proposed method with TTT layers outperforms baselines like Mamba 2, Gated DeltaNet, and sliding-window attention by a significant margin (34 Elo points), demonstrating superior coherence and motion naturalness in generated videos. - Despite the promising results, the generated videos still contain some artifacts, and improving the efficiency of the TTT-MLP kernel is identified as future work. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/abs/2504.05299) | eliebak, mervenoyan, mfarre, orrzohar, andito | - SmolVLM, a family of compact, multimodal models designed for resource-efficient inference, are introduced, demonstrating strong performance on image and video tasks with minimal memory footprints. - Architectural configurations, tokenization strategies, and data curation are systematically explored for low computational overhead. - SmolVLM-256M uses <1GB GPU memory during inference and outperforms the much larger Idefics-80B.  - The largest model, SmolVLM-2.2B, rivals state-of-the-art VLMs while using half the GPU Memory. - Strategic architectural optimizations, aggressive tokenization, and curated training data significantly enhance performance, enabling practical deployments at smaller scales. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Video-Text-to-Text'] | N/A | [Link](spaces/smolvlm2), [Link](spaces/smolvlm-webgpu), [Link](apple/huggingsnap), [Link](community/smol-research) |
| [URECA: Unique Region Caption Anything](https://arxiv.org/abs/2504.05305) | Heeji Yoon, seungryong, crepejung00, junwann, SammyLim | - This paper introduces URECA, a new region-level captioning model designed to generate unique captions for multi-granularity regions. - The model leverages a mask encoder network and dynamic mask modeling to preserve essential region properties such as position and shape, addressing the challenge of generating distinguishable captions for visually similar regions. - The authors also present a novel dataset, URECA dataset, specifically curated for unique captioning of multi-granularity regions using a stage-wise data pipeline with Multimodal Large Language Models (MLLMs). - URECA achieves state-of-the-art performance on the URECA dataset and demonstrates strong generalization on benchmark datasets like Visual Genome and RefCOCOg. - Experimental results show that fine-tuning existing captioning models on URECA dataset enhances their multi-granularity captioning capabilities. | ['Image-to-Text', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/cvlab-kaist/URECA) | N/A |
| [LiveVQA: Live Visual Knowledge Seeking](https://arxiv.org/abs/2504.05288) | Yao Wan, Mingyang Fu, shuaishuaicdp, Tim666, Ayiirep | - LIVEVQA, a new dataset designed to assess AI systems' ability to answer questions requiring up-to-date visual knowledge. - It consists of 3,602 visual questions from 6 news websites and 14 categories, pairing images with basic comprehension questions and two multi-hop reasoning questions requiring news context. - Evaluation across 15 Multimodal Large Language Models (MLLMs) shows that while larger models perform better, challenges persist in handling multi-hop visual questions demanding real-world knowledge.  - Integrating GUI-based multimodal search substantially improves performance, especially on complex questions, with Gemini-2.0-Flash achieving 29% accuracy.  - Despite strong textual skills, search engine-equipped models struggle with visual questions needing recent visual information, indicating areas for further research. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [Are You Getting What You Pay For? Auditing Model Substitution in LLM
  APIs](https://arxiv.org/abs/2504.04715) | Tianneng Shi, Will Cai, dawnsong, Xuandong | - This paper explores the challenge of Large Language Model (LLM) substitution within black-box APIs where providers might substitute advertised models with cheaper alternatives. - The authors investigate the effectiveness of several detection methods, including output-based statistical tests, benchmark evaluations, log probability analysis, and identity prompting. - Results reveal the limitations of methods relying solely on text outputs, particularly against attacks such as model quantization and randomized substitution.   - The study finds that log probability analysis provides more reliable verification but depends on API access. - The authors also propose using Trusted Execution Environments (TEEs) for hardware-based model integrity verification.  | ['Natural Language Processing'] | [Link](https://github.com/sunblaze-ucb/llm-api-audit) | N/A |
| [DiaTool-DPO: Multi-Turn Direct Preference Optimization for
  Tool-Augmented Large Language Models](https://arxiv.org/abs/2504.02882) | Donghun Lee, dsindex, junrae, gaeunseo, hash2430 | - DiaTool-DPO, a novel method to enhance the conversational abilities of Tool-Augmented Large Language Models (TA-LLMs) by using Direct Preference Optimization (DPO) is introduced. - The approach models TA-LLM interactions as a Markov Decision Process (MDP) and introduces a specialized objective loss that contrasts preferred and rejected dialogue trajectories to control conversation flow. - The method constructs paired trajectory datasets automatically and improves on existing techniques by addressing challenges in handling incomplete queries and out-of-scope requests. - The proposed method approaches GPT-40's performance achieving 94.8% in information gathering and 91% in tool call rejection. - The presented approach substantially improves baseline performance which only achieved 44% and 9.6% respectively for the same tasks. | ['Natural Language Processing', 'Text2Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [VAPO: Efficient and Reliable Reinforcement Learning for Advanced
  Reasoning Tasks](https://arxiv.org/abs/2504.05118) | Ruofei Zhu, Xiaochen Zuo, Qiying Yu, Yufeng Yuan, YuYue | - VAPO, a Value-based Augmented Proximal Policy Optimization framework, is introduced for enhancing reasoning models within the value-based paradigm. - When evaluated on the AIME 2024 dataset, VAPO, utilizing the Qwen 32B pre-trained model, achieves a state-of-the-art score of 60.4, outperforming DeepSeek-R1-Zero-Qwen-32B and DAPO by over 10 points under identical settings. - The framework addresses three key challenges in value-based methods for long chain-of-thought (long-CoT) reasoning: value model bias, heterogeneous sequence lengths, and reward signal sparsity. - VAPO incorporates Value-Pretraining, Length-adaptive GAE, and a combination of Clip-Higher, Positive Example LM Loss, and Group-Sampling techniques to overcome these challenges. - VAPO is shown to not only outperform but uses fewer training steps than previous methods, showcasing stable and efficient training without crashes across multiple independent runs. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Clinical ModernBERT: An efficient and long context encoder for
  biomedical text](https://arxiv.org/abs/2504.03964) | Jeffrey N. Chiang, Anthony Wu, Simonlee711 | - This paper introduces Clinical ModernBERT, a transformer-based encoder pre-trained on a large-scale biomedical corpus of literature, clinical notes, and medical ontologies, including PubMed abstracts, MIMIC-IV clinical data, and medical codes with textual descriptions. - Building upon ModernBERT, it incorporates architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and an extended context length of up to 8,192 tokens. - Clinical ModernBERT excels at generating semantically rich representations specifically tailored for long-context medical tasks, outperforming domain baselines in benchmarks such as named entity recognition (NER), retrieval, and classification tasks, including achieving state-of-the-art performance on long-context tasks like i2b2 concept extraction. - The model's weights and tokenizer are publicly available. - Latent space visualizations demonstrate improved alignment with clinical ontologies, showing its ability to internalize medical taxonomies. | ['Natural Language Processing', 'Token Classification', 'Text Classification'] | [Link](https://github.com/Simonlee711/Clinical_ModernBERT) | [Link](https://huggingface.co/Simonlee711/Clinical_ModernBERT) |
| [JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language
  Model](https://arxiv.org/abs/2504.03770) | Li Li, Yi Nian, yuehanqi, yuehanqi, Chouoftears | - JAILDAM, a memory-centered test-time adaptive framework, is introduced for detecting jailbreak attempts in vision-language models (VLMs). - It uses a memory bank of unsafe concepts generated by GPT-40 based on safety guidelines. - Multimodal safe inputs are encoded using CLIP and compared to the memory bank through attention mechanism to generate attention features which is then feed into an autoencoder. - During inference, if an input's similarity to the memory is low, and the reconstruction error is high, the input is considered harmful, and the least-used concept in memory is updated with a residual representation of the input, enabling adaptation to new attacks. - Experimental results show that JAILDAM outperforms existing methods in jailbreak detection accuracy and speed. | ['Multimodal'] | [Link](https://github.com/ShenzheZhu/JailDAM) | N/A |
| [GlotEval: A Test Suite for Massively Multilingual Evaluation of Large
  Language Models](https://arxiv.org/abs/2504.04155) | Ona de Gibert, Sawal Devkota, Joseph Attieh, Zihao Li, zuenmin | - GlotEval is a lightweight framework designed for massively multilingual evaluation of large language models (LLMs), supporting seven key tasks across dozens to hundreds of languages. - It features consistent multilingual benchmarking by standardizing language codes, language-specific prompt templates for diverse linguistic settings, and non-English-centric machine translation evaluation. - GlotEval integrates 20+ existing multilingual benchmarks and supports customizable prompts for each language, along with automated translation of prompt templates to 130+ languages. - A case study comparing EMMA-500 and Llama-2-7B on multilingual translation tasks demonstrates the framework's ability to reveal performance differences under various prompting strategies and language-centric settings. - The framework promotes more inclusive LLM evaluation by focusing on both widely spoken and underrepresented languages. | ['Natural Language Processing', 'Translation', 'Text Classification', 'Summarization'] | [Link](github.com/MaLA-LM/GlotEval) | N/A |
