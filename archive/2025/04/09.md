

## Papers for 2025-04-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniSVG: A Unified Scalable Vector Graphics Generation Model](https://arxiv.org/abs/2504.06263) | Jiaxu Zhang, Xianfang Zeng, Yiying Yang, CH3COOK, wchengad | - OmniSVG is a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal complex SVG generation. - It parameterizes SVG commands and coordinates into discrete tokens, decoupling structural logic from low-level geometry, and mitigating the "coordinate hallucination" problem. - OmniSVG introduces MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. - Experimental results demonstrate that OmniSVG outperforms existing methods across various SVG generation tasks, including Text-to-SVG, Image-to-SVG, and Character-Reference SVG generation. - OmniSVG shows potential for integration into professional SVG design workflows due to its ability to generate high-quality, complex, and editable SVG content from diverse modalities. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](http://omnisvg.github.io) | N/A |
| [Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought](https://arxiv.org/abs/2504.05599) | Jiangbo Pei, Yichen Wei, Xiaokun Wang, Chris, Yi Peng | - Skywork R1V, a 38B parameter multimodal reasoning model, extends the R1-series Large Language Models (LLMs) to visual modalities using a lightweight visual projector and a hybrid optimization strategy combining Iterative Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). - The model employs an adaptive-length Chain-of-Thought distillation approach for efficient reasoning and outperforms other similarly sized models, achieving scores of 69.0 on MMMU and 67.5 on MathVista. - It also maintains robust textual reasoning performance, with scores of 72.0 on AIME and 94.0 on MATH500. - The model architecture involves a staged approach, first training an MLP adapter to align a vision encoder with a substitute language model and then transferring this adapter to bridge the encoder with the reasoning-capable R1 LLM. -  The adaptive-length Chain-of-Thought distillation dynamically optimizes the reasoning chain length, improving inference efficiency. | ['Multimodal', 'Visual Question Answering', 'Image-Text-to-Text', 'Question Answering'] | N/A | [Link](https://huggingface.co/Skywork/Skywork-R1V-38B) |
| [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261) | Vage Egiazarian, George Yakushev, Alina Shutova, Roman Garipov, Gleb Rodionov | - This paper introduces Hogwild! Inference, a novel parallel LLM inference method designed to enhance reasoning and generation speed by enabling multiple LLM instances to collaborate dynamically during inference. - The core idea is to allow concurrent access to a shared key-value cache, which eliminates the need for recomputing key-value representations for each worker and facilitates real-time interaction between instances using Rotary Position Embeddings (ROPE). - The authors experiment with three cache layouts: contiguous (token-wise), interleaved (step-wise), and combined, with their preliminary results demonstrating improved performance on mathematical reasoning tasks compared to single-thread baselines. - Through prompting, the LLMs are encouraged to collaborate by devising and adjusting their own strategies for problem-solving, rather than relying on pre-defined frameworks. - The initial experiments suggest that reasoning-capable LLMs like QwQ and DeepSeek-R1 can effectively leverage this shared cache approach to improve efficiency and potentially quality on tasks requiring long-chain reasoning, such as those in the LIMO dataset. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/eqimp/hogwild_llm) | [Link](https://huggingface.co/Qwen/QwQ-32B), [Link](https://huggingface.co/datasets/GAIR/LIMO) |
| [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810) | Baizhou Huang, Ruilin Yan, Xiangyu Wang, YitaoLiang, pkuHaowei | - This paper introduces KUMO, a generative evaluation framework for assessing reasoning in Large Language Models (LLMs). - KUMO generates diverse, multi-turn reasoning tasks across various domains,  using LLMs combined with symbolic engines to create partially observable environments with adjustable difficulty. - The authors evaluated 23 state-of-the-art LLMs on 5,000 KUMO tasks and found that reasoning-scaled LLMs perform comparably to university students on complex reasoning challenges, while even standard LLMs outperform humans on simpler tasks. - The paper shows strong correlation between LLM performance on KUMO and other reasoning benchmarks like MMLU-Pro and LiveBench-Reason, and demonstrates KUMO's resistance to data contamination by showing that fine-tuned LLMs overfit to specific domains. - An analysis of parsing errors and token consumption revealed that some LLMs struggle with KUMO's format and generate excessively long or irrelevant outputs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/linhaoweil/kumo) | N/A |
| [V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric
  Capabilities in Multimodal Large Language Models](https://arxiv.org/abs/2504.06148) | Alex Jinpeng Wang, Ping Yu, Zhengyuan Yang, Linjie Li, Fengx1nn |  - V-MAGE is a novel game-based benchmark designed to evaluate visual-centric capabilities of Multimodal Large Language Models (MLLMs).  - It features five diverse games with over 30 handcrafted levels, testing various visual reasoning skills such as positioning, tracking, timing, and visual memory.  - V-MAGE reveals significant challenges in MLLMs' visual perception and reasoning, with top-performing models exhibiting substantial performance gaps compared to humans.  - The benchmark uses an Elo-based ranking system for comparing model performance and facilitates iterative model improvements.  - The findings highlight crucial limitations of current MLLMs and suggest avenues for improvement from an agent-centric perspective. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/CSU-JPG/V-MAGE) | N/A |
| [CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs
  with Controllable Puzzle Generation](https://arxiv.org/abs/2504.00043) | William W. Cohen, Bill Yuchen Lin, Langlin Huang, Chengsong Huang, Jixuan Leng | - CrossWordBench, a novel benchmark for evaluating the reasoning capabilities of LLMs and LVLMs using crossword puzzles, is introduced. - The benchmark features controllable puzzle generation in multiple formats (text and image) and offers various evaluation strategies. - Evaluations on over 20 models demonstrate that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints. - LVLMs struggle with the task, showing a strong correlation between their performance and grid-parsing accuracy. - Findings offer insights into the limitations of current LLMs and LVLMs and provide an effective approach for creating multimodal constrained tasks. | ['Multimodal'] | [Link](https://github.com/SeanLeng1/CrossWordBench) | [Link](https://huggingface.co/datasets/HINT-lab/CrossWordBench) |
| [Accelerate Parallelizable Reasoning via Parallel Decoding within One
  Sequence](https://arxiv.org/abs/2503.20533) | Yijiong Yu | - This paper introduces "Parallel Decoding in One Sequence," a novel decoding method designed to accelerate the reasoning process of Large Language Models (LLMs) for parallelizable tasks. - The method modifies the attention mask and position IDs, enabling parallel decoding of multiple tokens within a single sequence, thereby improving efficiency without requiring additional memory or KV cache recomputation. - Experiments on retrieval, multi-document QA, and planning tasks demonstrate a significant speedup in decoding time (over 100% in some cases) while maintaining answer quality. - The method is generally applicable across different LLMs without requiring additional training or modules. - Evaluation shows improved decoding speed by approximately 70% and 40% for Qwen2.5-14b and Qwen2.5-7b on the planning task with little to no reduction in answer quality as measured by GPT-4o rating scores. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/yuyijiong/parallel-decoding-in-one-sequence) | N/A |
