

## Papers for 2025-04-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
  Tokens](https://arxiv.org/abs/2504.07096) | Yensung, sewon, yanaiela, taylorb, liujch1998 | - OLMOTRACE is a system that traces the output of large language models (LLMs) back to their original training data, which consists of trillions of tokens. - It identifies verbatim matches between segments of the LLM's output and documents within its training corpora. - Uses an extended version of infini-gram, allowing for real-time tracing results within seconds. - Aims to enhance user understanding of LLM behavior by revealing potential sources of information or learned sequences within the training data. - OLMOTRACE is publicly available and open-source. | ['Natural Language Processing'] | [Link](https://github.com/allenai/infinigram-api) | [Link](https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct) |
| [A Unified Agentic Framework for Evaluating Conditional Image Generation](https://arxiv.org/abs/2504.07046) | Yiyu Wang, Longyue Wang, Xue Yang, Jifang Wang, imryanxu | - This paper introduces CIGEVAL, a unified agentic framework for evaluating conditional image generation tasks using large multimodal models (LMMs) like GPT-40 and open-source 7B models. - CIGEVAL integrates a multi-functional toolbox for nuanced analysis, including grounding, difference, highlighting, and scene graph tools, and uses a divide-and-conquer approach for fine-grained evaluation of multiple conditions like text prompts, subject images, and control signals. -  It synthesizes evaluation trajectories for fine-tuning, enabling smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. - Experiments on ImagenHub across seven tasks show CIGEVAL (GPT-40 version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47 and surpassing previous state-of-the-art methods. -  Fine-tuning with just 2.3K trajectories allows 7B open-source LMMs to exceed the prior GPT-40-based state-of-the-art. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/HITsz-TMG/Agentic-CIGEval) | N/A |
| [Missing Premise exacerbates Overthinking: Are Reasoning Models losing
  Critical Thinking Skill?](https://arxiv.org/abs/2504.06514) | Ming Li, zhoutianyi, sunlichao137, Fcr09 | - This paper introduces the concept of "Overthinking under Missing Premise (MiP-Overthinking)", a phenomenon where reasoning Large Language Models (LLMs) generate excessively long responses to ill-posed questions with missing premises. - The authors curate four MiP datasets across varying difficulty levels, employing three distinct generation strategies: Rule-Based Generation, Body-Question Swapping, and Essential-Premise Removal. - Experimental results on a diverse set of LLMs demonstrate that reasoning models produce significantly longer responses (2x-4x more tokens) for MiP questions compared to well-defined questions and non-reasoning models. - Despite the extended reasoning, these models exhibit low abstain rates on MiP questions, indicating a lack of genuine critical thinking skills, contradicting test-time scaling law. - Further analysis reveals that reasoning models often detect the missing premise early in the reasoning process but continue to generate redundant content, while non-reasoning models efficiently identify and abstain from such questions. | ['Question Answering'] | [Link](https://github.com/tianyi-lab/MiP-Overthinking) | N/A |
| [FantasyTalking: Realistic Talking Portrait Generation via Coherent
  Motion Synthesis](https://arxiv.org/abs/2504.04842) | Yunpeng Zhang, Yaqi Fan, Mengchao Wang, fanjiang, wangqiang9 | - FantasyTalking is a novel framework that leverages a pretrained video diffusion transformer model to generate realistic and coherent talking portraits with controllable motion dynamics from a single portrait image, voice, and text. - It employs a dual-stage audio-visual alignment strategy, utilizing clip-level training for global motion coherence and frame-level training for precise lip synchronization. - Instead of a reference network, it uses a facial-focused cross-attention module to maintain facial identity. - A motion intensity modulation module allows control over expression and body motion intensity. - Experimental results on tame and wild talking head datasets demonstrate superior performance compared to existing state-of-the-art methods in video quality, temporal consistency, and motion diversity. | ['Text-to-Video', 'Multimodal'] | N/A | N/A |
| [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089) | Cxxs, Wayne-lc, Dakerqi, JiakangYuan, yeeeeeyy | - OmniCaptioner is a versatile visual captioning framework generating fine-grained textual descriptions across diverse visual domains, including natural images, visual text, and structured visuals. - Unlike existing methods limited to specific image types, OmniCaptioner provides a unified solution, bridging the gap between visual and textual modalities by converting pixel information into semantic textual representations. - It outperforms existing models in visual reasoning tasks when integrated with LLMs, enhances image generation by providing more accurate descriptions, and allows for a more efficient supervised fine-tuning (SFT) process due to its diverse pretraining dataset. - The framework consists of Seed-Caption Generation for precise pixel-to-word mapping using GPT-40 and Caption Extension to enrich styles and incorporate reasoning knowledge using Qwen2.5-32B. - Evaluation across visual reasoning, image generation, and SFT benchmarks demonstrates OmniCaptioner's superior performance and versatility in bridging visual and language modalities. | ['Image-to-Text', 'Multimodal'] | [Link](https://github.com/Alpha-Innovator/OmniCaptioner) | [Link](https://huggingface.co/U4R/OmniCaptioner) |
| [RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts](https://arxiv.org/abs/2504.06947) | Anna Lapanitsyna, Natalia Tkachenko, Natalia Loukachevitch, nicolay-r, RefalMachine | - This paper introduces RuOpinionNE-2024, a shared task focused on extracting structured opinion tuples from Russian news text. - The task involves identifying sentiment holders, targets, expressions, and polarity within sentences, contributing to a deeper understanding of sentiment dynamics in news. - The competition saw over 100 submissions, primarily employing large language models (LLMs) in zero-shot, few-shot, and fine-tuning settings. - Fine-tuning a large language model yielded the best performance on the test set. - A comparison of 30 prompts and 11 open-source LLMs with varying parameter sizes (3-32 billion) in 1-shot and 10-shot learning revealed optimal model and prompt configurations. | ['Natural Language Processing', 'Text Classification', 'Question Answering'] | N/A | N/A |
| [DiTaiListener: Controllable High Fidelity Listener Video Generation with
  Diffusion](https://arxiv.org/abs/2504.04010) | chaubeyG, hongkung, minhtran, Boese0601, havent-invented | - DiTaiListener is a novel diffusion-based model for generating high-fidelity listener response videos from speaker audio and facial motion inputs, also incorporating text-based control for customized responses. - It uses a Causal Temporal Multimodal Adapter (CTM-Adapter) within a Diffusion Transformer (DiT) architecture to process multimodal inputs and generate realistic facial expressions directly in pixel space, rather than relying on intermediate 3DMM representations and rendering. - For long video generation, DiTaiListener-Edit refines transitions between independently generated segments, ensuring smooth and coherent motions. - On benchmark datasets like RealTalk and VICO, DiTaiListener achieves state-of-the-art performance in both photorealism (+73.8% FID on RealTalk) and motion representation (+6.1% FD metric on VICO). - User studies confirm DiTaiListener's superior performance regarding feedback, diversity, and smoothness. | ['Text-to-Video', 'Multimodal', 'Computer Vision'] | N/A | N/A |
| [VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement
  Fine-Tuning](https://arxiv.org/abs/2504.06958) | Lanxingxuan, donglu, desenmeng, Aurorana, xinhaoli | - This paper introduces VideoChat-R1, a video multimodal large language model (MLLM) enhanced for spatio-temporal perception using Reinforcement Fine-Tuning (RFT) with Group Relative Policy Optimization (GRPO). - RFT is shown to be highly data-efficient, improving performance on specific tasks without impacting general capabilities or chat abilities.  - VideoChat-R1 achieves state-of-the-art results on spatio-temporal perception tasks such as temporal grounding (+31.8 compared to Qwen2.5-VL-7B) and object tracking (+31.2), and it also shows improvements on general video QA benchmarks. - The paper suggests that training on spatio-temporal perception tasks can strengthen a model's spatio-temporal reasoning ability.  - The authors provide comprehensive analysis and ablation studies demonstrating the efficacy of RFT for Video MLLMs. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/OpenGVLab/VideoChat-R1) | N/A |
