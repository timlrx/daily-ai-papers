

## Papers for 2025-04-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Kimi-VL Technical Report](https://arxiv.org/abs/2504.07491) | dongliangwang, congcongwang, DuChenZhuang, tzzcl, xingbowei | - Kimi-VL is an open-source Mixture-of-Experts (MoE) vision-language model with advanced multimodal reasoning, long-context understanding, and agent capabilities. - It consists of a 2.8B parameter MoE language decoder (Kimi-VL-A3B) paired with a 400M native-resolution MoonViT vision encoder. - It outperforms other efficient vision-language models like DeepSeek-VL2 and Qwen2.5-VL-7B on various benchmarks, including college-level image and video comprehension, OCR, and mathematical reasoning. - Kimi-VL-Thinking, a long-thinking variant fine-tuned with long chain-of-thought (CoT) and reinforcement learning, further improves the performance on complex multimodal reasoning tasks, achieving scores of 61.7 on MMMU, 36.8 on MathVision and 71.3 on MathVista. - Kimi-VL also excels in processing long contexts with a 128K extended context window and high-resolution visual inputs. | ['Multimodal', 'Visual Question Answering', 'Image-to-Text', 'Document Question Answering', 'Video-Text-to-Text'] | [Link](https://github.com/MoonshotAI/Kimi-VL) | N/A |
| [VCR-Bench: A Comprehensive Evaluation Framework for Video
  Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.07956) | lovesnowbest, Lin-Chen, Osilly, ChthollyTree, yukunqi | - VCR-Bench, a novel benchmark designed to comprehensively evaluate Large Vision-Language Models' (LVLMs) video Chain-of-Thought (CoT) reasoning capabilities. - It consists of 859 videos with diverse content, along with 1,034 question-answer pairs, each manually annotated with stepwise CoT rationales. - Introduces CoT score by categorizing CoT steps into visual perception and logical reasoning and evaluates them across various task dimensions (recall, precision). - Exposes limitations of current LVLMs, with even top-performing models showing subpar CoT scores, particularly in perception tasks involving temporal-spatial information extraction. - Validates framework's effectiveness through a strong positive correlation between CoT scores and accuracy, indicating its crucial role in complex video reasoning. | ['Video-Text-to-Text', 'Multimodal', 'Question Answering', 'Visual Question Answering'] | N/A | [Link](https://vlm-reasoning.github.io/VCR-Bench/) |
| [MM-IFEngine: Towards Multimodal Instruction Following](https://arxiv.org/abs/2504.07957) | yhcao, sweetFruit, KennyUTC, yuhangzang, ChrisDing1105 | - This paper introduces MM-IFEngine, a new pipeline for generating high-quality image-instruction pairs for multimodal instruction following. - It also presents MM-IFEval, a challenging benchmark for multimodal instruction following with diverse constraints and a hybrid evaluation approach. - The authors create two datasets using the pipeline: MM-IFInstruct-23k for supervised fine-tuning and MM-IFDPO-23k for direct preference optimization. - Experiments show that fine-tuning MLLMs on these datasets significantly improves performance on various instruction following benchmarks, including MM-IFEval, MIA, and IFEval. - The models fine-tuned on these datasets maintains comparable performance on other visual question answering benchmarks. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/SYuan03/MM-IFEngine) | N/A |
| [DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning](https://arxiv.org/abs/2504.07128) | parishadbehnam, miladink, vaibhavad, arkilpatel, spaidartaigar | - This paper introduces a taxonomy for analyzing the reasoning chains of Large Reasoning Models (LRMs), focusing on DeepSeek-R1. - The study finds that DeepSeek-R1's reasoning chains follow a consistent structure, starting with problem definition, followed by decomposition ('Bloom cycle'), and iterative reconstruction cycles ('rumination'). - The research reveals a 'sweet spot' for reasoning length, beyond which performance declines, and that DeepSeek-R1 struggles to adhere to specified token budgets. - The analysis also shows DeepSeek-R1 prioritizes context over parametric knowledge, exhibits safety vulnerabilities compared to its non-reasoning counterpart, and displays cultural biases in moral reasoning. - It highlights correlations between model reasoning chains and human processing of challenging sentences, while also noting limitations in the model's iterative refinement and world modeling capabilities during visual reasoning tasks. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | N/A | N/A |
| [C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization
  for Test-Time Expert Re-Mixing](https://arxiv.org/abs/2504.07964) | Ziyue Li, zhoutianyi, Lzy01241010 | - C3PO, Critical-Layer, Core-Expert, Collaborative Pathway Optimization, is a novel test-time optimization method for Mixture-of-Experts (MoE) Large Language Models (LLMs) that improves expert pathway selection. - It addresses the sub-optimality of expert pathways generated by pre-trained routers by jointly optimizing the core experts' mixing weights in critical layers for each test sample based on similar, successful samples from a reference set.  - C3PO employs three surrogate objectives and algorithms: mode-finding, kernel regression, and average loss of similar samples, and leverages gradient-free or gradient-based optimization depending on the objective. - Experiments on two MoE LLMs (OLMOE and DeepSeekMoE) across six benchmarks demonstrate a consistent 7-15% accuracy improvement over base models, outperforming existing test-time learning methods like in-context learning and prompt tuning. - Notably, C3PO enables MoE LLMs with 1-3B active parameters to outperform larger 7-9B parameter LLMs, highlighting its efficiency benefits. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/tianyi-lab/C3PÐž) | N/A |
| [MOSAIC: Modeling Social AI for Content Dissemination and Regulation in
  Multi-Agent Simulations](https://arxiv.org/abs/2504.07830) | Marzyeh Ghassemi, saadia, elisakreiss, salmannyu, genglinliu | - MOSAIC, a novel open-source multi-agent social network simulation framework, uses generative language agents (LLMs) to model user behaviors like liking, sharing, and flagging content, combined with a directed social graph, to analyze emergent deception behaviors and how users assess online content veracity. - MOSAIC constructs user representations from diverse, fine-grained personas, enabling multi-agent simulations to model content dissemination and engagement dynamics at scale, and evaluating three content moderation strategies (community-based, third-party, and hybrid fact-checking) with simulated misinformation. - The simulations demonstrate that these strategies not only mitigate the spread of misinformation but also increase user engagement, unlike human social media behavior. - The study explores content popularity trajectories, finding that agents' articulated reasoning for interactions doesn't always align with collective engagement patterns, and observed that misinformation didn't spread faster than real news with LLM-based agents. - The framework is open-sourced to promote research in AI and social science, enabling controlled experiments for studying online behavior and content moderation strategies. | ['Natural Language Processing'] | [Link](https://github.com/genglinliu/MOSAIC) | N/A |
| [SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual
  Reasoning Self-Improvement](https://arxiv.org/abs/2504.07934) | furongh-lab, kevinlin311tw, linjieli222, zyang39, russwang | - This paper introduces ThinkLite-VL, a novel approach for enhancing the visual reasoning abilities of Vision-Language Models (VLMs) using a data-efficient training method that relies on self-improvement without knowledge distillation. - The key contribution is an MCTS-guided sample selection mechanism, which quantifies the difficulty of training examples based on the number of MCTS iterations required by the base VLM to solve each problem. - Using only 11k training samples, the resulting ThinkLite-VL-7B model demonstrates state-of-the-art performance on eight visual reasoning benchmarks, outperforming other 7B-level reasoning VLMs. - Notably, ThinkLite-VL-7B achieves 75.1% accuracy on MathVista, surpassing larger open-sourced models, GPT-40, and O1. - Ablation studies confirm the importance of MCTS-based sample selection by revealing that training exclusively on easy samples does not improve model reasoning ability, and that unsolved samples identified by MCTS pose significant challenges and contribute substantially to enhancing the model's reasoning capabilities during reinforcement learning. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/si0wang/ThinkLite-VL) | N/A |
| [Scaling Laws for Native Multimodal Models Scaling Laws for Native
  Multimodal Models](https://arxiv.org/abs/2504.07951) | Joshua Susskind, Matthieu Cord, Victor Guilherme Turrisi da Costa, Enrico Fini, Mustafa Shukor | - This paper investigates the scaling properties of native multimodal models (NMMs), which are trained from scratch on multimodal data, without relying on pre-trained components. - The study conducts extensive experiments on early and late fusion architectures, training 457 models with different architectures and training mixtures, and finds that early fusion models exhibit stronger performance at lower parameter counts and are more efficient to train. - The research derives scaling laws for NMMs and demonstrates that they follow similar trends as LLMs, albeit with slight variations in scaling coefficients, suggesting model parameters and training tokens should be scaled roughly equally for optimal performance. - It explores the benefits of incorporating Mixture of Experts (MoEs) into NMMs and observes significant performance improvements, with scaling laws indicating that scaling training tokens is more crucial than scaling parameters for MoEs. - Analysis reveals that experts in MoE models tend to specialize in different modalities, particularly in the early and last layers, demonstrating the potential for multimodal specialization within a unified architecture. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Towards Visual Text Grounding of Multimodal Large Language Model](https://arxiv.org/abs/2504.04974) | Franck-Dernoncourt, YfZ, JoshuaGu, zhangry868, MingLiiii | - This paper introduces TRIG, a novel Text-Rich Image Grounding task and benchmark for evaluating and improving the visual text grounding capabilities of Multimodal Large Language Models (MLLMs) in document question-answering. - A new dataset, TRIG-Bench, consisting of 800 manually annotated question-answer pairs from DocVQA, ChartQA, InfographicsVQA, and TRINS, along with a 90k synthetic training dataset generated using an OCR-LLM-human pipeline is also presented. - Two methods are proposed: an instruction-tuning method and an embedding-based method; evaluation shows that both outperform current MLLMs on the benchmark, with the embedding method being more efficient. - Existing MLLMs struggle with visual text grounding on text-rich document images, often failing to follow customized instructions requiring spatial understanding. - The authors suggest that this task is under-explored and needs more attention from the community, emphasizing the need for MLLMs to better handle complex document layouts and understand instructions requiring deep spatial reasoning. | ['Multimodal', 'Document Question Answering', 'Visual Question Answering'] | N/A | N/A |
