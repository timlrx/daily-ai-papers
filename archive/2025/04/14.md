

## Papers for 2025-04-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [SQL-R1: Training Natural Language to SQL Reasoning Model By
  Reinforcement Learning](https://arxiv.org/abs/2504.08600) | Ran Chen, Xuhui Jiang, Chengjin Xu, Peixian Ma, ZhuangXialie | - This paper introduces SQL-R1, a novel Natural Language to SQL (NL2SQL) reasoning model trained using reinforcement learning (RL). - SQL-R1 focuses on improving inference performance in complex scenarios with multi-table joins and nested queries, which are challenging for traditional supervised fine-tuning methods. - The model uses a specialized RL reward function to guide SQL generation and addresses the cold-start problem for effective training. - Experiments on Spider and BIRD benchmarks show SQL-R1 achieves competitive accuracy of 88.7% and 66.6%, respectively, using only a 7B base model. - The paper also explores data engineering for RL and emphasizes the model's ability to produce an explicit reasoning process, enhancing interpretability. | ['Natural Language Processing', 'Text2Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on
  Transformer Encoder Models Performance](https://arxiv.org/abs/2504.08716) | Djamé Seddah, Benoît Sagot, Wissam Antoun | - This paper investigates the performance of ModernBERT, a transformer-encoder model designed for efficiency, against DeBERTaV3 and RoBERTa models by pretraining them on the same French dataset (CamemBERTaV2's dataset) to control for data variations. - The results reveal that DeBERTaV3 surpasses ModernBERT in benchmark performance and training sample efficiency, attributing this to its superior architecture and training objective optimization. - ModernBERT, however, demonstrates significantly faster training and inference speeds due to its efficiency-focused design, making it practically advantageous. - Additionally, the study finds that training on a higher-quality, filtered dataset enhances convergence speed but does not substantially improve final performance, suggesting a potential saturation of current NLP benchmarks. - The authors release their pretrained French ModernBERT models and training scripts publicly to facilitate reproducibility and further research. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification'] | N/A | [Link](https://huggingface.co/collections/almanach/moderncamembert-67f7e6d85ede5f7cfc1ce012) |
| [Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend
  NPUs](https://arxiv.org/abs/2504.07866) | Xueyu Wu, Yehui Tang, Kaikai Song, Wenyong Huang, Yichun Yin | - This paper introduces Pangu Ultra, a 135 billion parameter, 94-layer dense Large Language Model (LLM) trained on Huawei's Ascend NPUs. - The model utilizes novel techniques like depth-scaled sandwich normalization and tiny initialization to stabilize training and prevent loss spikes, which are common in deep LLMs. - Trained on 13.2 trillion tokens and incorporating techniques for long context extension (up to 128K tokens), Pangu Ultra performs competitively with state-of-the-art LLMs, including sparse models with significantly more parameters like DeepSeek-R1, and outperforms other dense LLMs like Llama 405B and Mistral Large 2 on various benchmarks. - System-level optimizations like kernel fusion, context parallelism, and optimized attention mechanisms allow for efficient training on 8,192 Ascend NPUs, achieving over 52% Model FLOPs Utilization (MFU). - The enhanced reasoning capabilities of Pangu Ultra are demonstrated through its superior performance on specialized benchmarks like AIME 2024, MATH-500, GPQA Diamond, and LiveCodeBench. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [CoRAG: Collaborative Retrieval-Augmented Generation](https://arxiv.org/abs/2504.01883) | Virginia Smith, Mona Diab, Aashiq Muhamed | - Introduces CoRAG, a framework for collaborative Retrieval-Augmented Generation (RAG) where multiple clients jointly train a shared model using a collaborative passage store, but use their local passage stores during inference. - Proposes CRAB, a homogeneous open-domain question answering benchmark for evaluating CoRAG and exploring the impact of passage composition within the collaborative store.  - Demonstrates through experiments on CRAB that CoRAG consistently outperforms parametric collaborative learning methods and locally trained RAG models, especially in few-shot settings, achieving up to a 33.8% improvement over local RAG in 16-shot scenarios. - Reveals that relevant passages are critical for generalization, surprisingly irrelevant passages can be beneficial, while hard negatives negatively impact performance. - Identifies a key challenge in CoRAG, where clients must balance the advantages of an enriched store with the risk of incorporating detrimental passages from others, and introduces mechanisms for encouraging participation. | ['Question Answering'] | [Link](https://github.com/aashiqmuhamed/CORAG) | N/A |
| [Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning
  vs. Memorization in Large Language Models](https://arxiv.org/abs/2504.05262) | Zhenzhong Lan, Renjun Xu, Yu Lu, Yang Yan | - This paper investigates whether Large Language Models (LLMs) genuinely understand mathematical principles or rely on memorization by evaluating their performance on elementary two-integer addition. - The study probes two core properties: commutativity (A + B = B + A) and compositional generalization using symbolic mappings (e.g., 7 → Y). - Despite high accuracy on numerical addition, LLMs' performance collapses under symbolic mapping and exhibits non-monotonic scaling with digit count, along with frequent commutativity violations. - Providing explicit addition rules degrades performance, while self-explanation maintains baseline accuracy, suggesting misalignment between LLM arithmetic processing and human-defined principles. - The findings indicate current LLMs rely on memory patterns over rule learning, highlighting limitations and the need for new approaches for genuine mathematical reasoning. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
