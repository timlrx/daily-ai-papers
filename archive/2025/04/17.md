

## Papers for 2025-04-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ColorBench: Can VLMs See and Understand the Colorful World? A
  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514) | zhoutianyi, jiuhai, shweta12, kweCobi, Fcr09 | - This paper introduces COLORBENCH, a new benchmark designed to evaluate the color perception, reasoning, and robustness of Vision-Language Models (VLMs). - COLORBENCH includes 11 tasks covering various real-world applications, such as identifying colors in paintings, reading test kits, interpreting satellite images, and recognizing camouflaged objects. - An evaluation of 32 VLMs reveals that larger models generally perform better, but the absolute performance across all models is relatively low, suggesting that color understanding is a current weakness of VLMs. - Chain-of-Thought (CoT) reasoning improves color understanding accuracy and robustness, highlighting the importance of reasoning abilities in these tasks. - Color clues are helpful for most COLORBENCH tasks but can mislead VLMs in tasks involving illusions or mimicry. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/tianyi-lab/ColorBench) | N/A |
| [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285) | thegenerality, THU-CHUNXIA, buaahsh, hongyuw, shumingma | - BitNet b1.58 2B4T is a 2-billion parameter, native 1-bit Large Language Model (LLM) trained on 4 trillion tokens, using a modified transformer architecture with custom BitLinear layers incorporating weight quantization (1.58 bits), activation quantization (8 bits), and subln normalization. - It achieves performance comparable to leading open-weight, full-precision LLMs of similar size across various tasks including language understanding, reasoning, coding, and conversation. - This model offers substantial efficiency benefits, with significant reductions in memory footprint, energy consumption, and decoding latency compared to full-precision counterparts. - Open-source implementations optimized for both GPU (custom CUDA kernels) and CPU (bitnet.cpp) are provided to encourage wider adoption and research. - BitNet b1.58 2B4T advances the state-of-the-art in 1-bit LLMs, demonstrating the potential of extreme quantization for deploying powerful language models in resource-constrained environments. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/microsoft/LMOps) | [Link](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T) |
| [AlayaDB: The Data Foundation for Efficient and Effective Long-context
  LLM Inference](https://arxiv.org/abs/2504.10326) | FeTieTer, YuanPeiqi, Qilong00, BenjaminXIANG, YangshenDeng | - AlayaDB, a new vector database system, is designed specifically for efficient and effective long-context Large Language Model (LLM) inference. - It decouples key-value (KV) cache and attention computation from the LLM inference engine, encapsulating them within the database system.  - A novel dynamic inner product range (DIPR) query is introduced to enhance sparse attention by dynamically selecting critical tokens.  - AlayaDB employs a query optimizer and various optimizations, including a novel vector file system and a data-centric attention engine, to minimize resource consumption and improve performance.  - Experimental results on LLM inference benchmarks demonstrate AlayaDB's ability to reduce resource consumption and improve generation quality while adhering to service level objectives (SLOs). | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction
  Fine-Tuning](https://arxiv.org/abs/2504.09081) | Jian Xie, Rupak Vignesh Swaminathan, svinxz, vijaygirish2001, panprabh | - The paper introduces SIFT-50M, a 50 million example multilingual dataset for instruction fine-tuning and pre-training of speech-text large language models (LLMs). - The dataset covers five languages and focuses on diverse speech understanding and controllable speech generation instructions, generated using LLMs and expert models. - The authors also introduce EvalSIFT, a benchmark dataset to evaluate speech-text LLMs and  SIFT-LLM, a speech-text LLM trained on SIFT-50M. - SIFT-LLM outperforms existing speech-text LLMs on instruction following benchmarks while remaining competitive on foundational speech tasks according to Table 3. - The model also shows promising results for controllable speech generation, with synthesized speech exhibiting characteristics specified in the instructions as seen in Table 6. | ['Audio', 'Automatic Speech Recognition', 'Text-to-Speech', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/amazon-agi/SIFT-50M) |
| [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536) | chijx, imjcqt, YujiaHi, zhangysk, JoeYing | - ReTool, a reinforcement learning framework, enhances LLMs' ability to use external tools, like code interpreters, for complex problem-solving, such as mathematical reasoning. - It combines dynamic interleaving of real-time code execution within natural language reasoning and automated RL for training tool invocation strategies based on outcome feedback. - On the AIME math benchmark, ReTool-32B achieves 67% accuracy with 400 training steps, outperforming a text-based RL baseline (40% accuracy, 1080 steps) and surpasses OpenAI's model by 27.9% in extended settings (72.5% accuracy). - Analysis reveals emergent behaviors like code self-correction, indicating autonomous mastery of adaptive tool use. - ReTool highlights the potential of outcome-driven tool integration for complex problem-solving and provides insights into hybrid neuro-symbolic systems. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952) | ashay-sriv, jebish7, DrishtiSharma, Siddartha10, 1024m | - This paper introduces a new dataset of 2.4M human-machine co-authored texts in 23 languages generated using popular LLMs, including GPT-4, Gemini, and Claude. - It proposes a set of multilingual transformer models with a CRF layer for token classification, trained to distinguish writing styles within a text. - These models achieve better performance over texts with unseen features (domain, generator, adversarial inputs, non-native speakers) compared to binary classification approaches. - The models effectively separate human-authored from machine-generated portions in co-authored texts. The findings of the paper are based on tests conducted on various benchmarks and datasets, including Mgtd-bench and Raid-bench. Additional findings include a comparison of performance against various adversarial methods and the characteristics of generated text compared to human-authored text. | ['Natural Language Processing', 'Token Classification'] | N/A | N/A |
| [Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution](https://arxiv.org/abs/2504.09566) | Qigan Sun, Jiaquan Zhang, Yi Lu, Chaoning Zhang, Chenghao Li | - This paper introduces Syzygy of Thoughts (SoT), a novel reasoning framework that enhances Chain-of-Thought (CoT) prompting for Large Language Models (LLMs) by incorporating principles of Minimal Free Resolution (MFR) from algebraic geometry. - SoT decomposes complex problems into interconnected reasoning paths, capturing deeper logical dependencies and improving structured problem-solving by introducing concepts like "Module", "Freeness", and "Mapping". - Experimental results on datasets like GSM8K and MATH demonstrate that SoT achieves comparable or superior performance to other CoT methods across various LLMs (GPT40-mini, Qwen2.5). - SoT's structured approach reduces redundant computations and logical inconsistencies, leading to more efficient and transparent reasoning. - The framework shows improved inference accuracy and scalability, addressing CoT limitations in high-dimensional and complex logical problem-solving, particularly in mathematical reasoning tasks, where it approaches the performance of larger models like GPT-4 on lightweight models. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/dIMARiA/Syzygy-of-thoughts) | N/A |
