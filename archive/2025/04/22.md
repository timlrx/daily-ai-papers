

## Papers for 2025-04-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [FlowReasoner: Reinforcing Query-Level Meta-Agents](https://arxiv.org/abs/2504.15257) | P2333, bhooi, dreamerdeo, yueliu1999, HongchengGao | - This paper introduces FLOWREASONER, a query-level meta-agent designed to automate the creation of multi-agent systems tailored to individual user queries, enhancing adaptability compared to traditional task-level approaches. - FLOWREASONER employs a reasoning-driven approach, leveraging external execution feedback and reinforcement learning to optimize workflows without relying on complex search algorithms or predefined search sets. - A multi-purpose reward function guides the RL training, focusing on performance, complexity, and efficiency of the generated multi-agent systems. - Experimental results on code generation benchmarks show FLOWREASONER-14B outperforming all baselines, including manually designed and existing automated workflow methods, achieving a 5 percentage point improvement over the strongest baseline (MaAS) and a 10.52% improvement over its underlying worker model (o1-mini) across three benchmarks. - The model demonstrates generalization capability by effectively adapting its planning strategies to different worker models and exhibits flexibility in workflow structure and granularity based on task complexity. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/sail-sg/FlowReasoner) | N/A |
| [Eagle 2.5: Boosting Long-Context Post-Training for Frontier
  Vision-Language Models](https://arxiv.org/abs/2504.15271) | WonminByeon, deahuang, lulidong, RealZhiqiLi, cg1177 | - Eagle 2.5 is a family of frontier vision-language models (VLMs) designed for long-context multimodal learning, addressing challenges in video and high-resolution image understanding. - It uses a generalist architecture based on LLaVA, using an MLP projection to align vision embeddings from SigLIP with the LLM representation space, and employs image tiling for any-resolution image inputs. -  Eagle 2.5 introduces an information-first sampling strategy (including Image Area Preservation and Automatic Degradation Sampling) and progressive mixed post-training. - It leverages a diverse data recipe, combining open-source data with the new Eagle-Video-110K dataset designed for long video understanding, featuring hierarchical story-level and clip-level annotations. - Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching top commercial models like GPT-40 and open-source models such as Qwen2.5-VL-72B and InternVL2.5-78B, showing robust scaling with increased frames. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Image-Text-to-Text'] | N/A | N/A |
| [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958) | Cheng Qian, Gokhantur, XtremSup, Merlin-Hongru, emrecanacikgoz | - This paper introduces ToolRL, a novel approach for enhancing tool-integrated reasoning in Large Language Models (LLMs) using reinforcement learning, specifically Group Relative Policy Optimization (GRPO). - The authors propose a principled reward design framework tailored for tool use tasks, incorporating both structural (format) and semantic (correctness) components. - ToolRL consistently outperforms supervised fine-tuning and other RL baselines by 17% and 15% respectively across multiple tool use and question-answering benchmarks. - The trained model exhibits strong generalization to unseen scenarios and task objectives, along with emergent behaviors such as proactiveness and metacognitive reasoning. - Comprehensive analysis of different reward strategies reveals the importance of dynamic reward scaling and fine-grained reward decomposition for effective tool learning. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/qiancheng0/ToolRL) | N/A |
| [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203) | hamidpalangi, mparvez, genglinliu, liweijiang, salmannyu | - Introduces X-Teaming, a multi-turn jailbreaking framework that utilizes collaborative agents for planning, executing, and optimizing attacks against language models (LMs). - Achieves state-of-the-art multi-turn jailbreak success rates (up to 98.1%) across diverse LMs, including a 96.2% success rate against Claude 3.7 Sonnet. - Generates XGuard-Train, a 30K multi-turn safety training dataset 20x larger than previous resources, enabling improved multi-turn safety alignment for LMs. - Shows that models fine-tuned on XGuard-Train exhibit a 28.3% improvement in multi-turn attack resistance while maintaining single-turn safety and general capabilities. - Open-sources the framework, dataset, and trained models to facilitate the development of robust multi-turn defenses for conversational AIs. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/marslabucla/XGuard-Train) |
| [UFO2: The Desktop AgentOS](https://arxiv.org/abs/2504.14603) | rujiawang, liqul, duchao, shilhe, vyokky | - UFO² is a novel multi-agent operating system for Windows desktops designed for robust and practical computer-using agent (CUA) automation. - It features a centralized HostAgent for task coordination and specialized AppAgents for application-specific interactions, using native APIs and a hybrid GUI-API action layer for improved control and efficiency. - UFO² incorporates a hybrid control detection pipeline combining Windows UI Automation (UIA) with visual grounding, continuous knowledge integration from documentation and execution logs, and speculative multi-action planning for reduced latency. - Evaluation across 20 real-world Windows applications shows UFO² outperforms existing CUAs in robustness and accuracy, achieving a 10% higher task completion rate than the best-performing CUA, Operator, with a 50% relative improvement using deeper OS-level integration. - A Picture-in-Picture (PiP) interface allows for non-disruptive automation within an isolated virtual desktop. | ['Multimodal', 'Natural Language Processing'] | [Link](https://github.com/microsoft/UFO/) | N/A |
| [Seeing from Another Perspective: Evaluating Multi-View Understanding in
  MLLMs](https://arxiv.org/abs/2504.15280) | Shengbang Tong, yubei, chengtim, ch-chenyu, danielchyeh | - This paper introduces All-Angles Bench, a new benchmark designed to evaluate the multi-view understanding capabilities of Multimodal Large Language Models (MLLMs). - The benchmark consists of over 2,100 human-annotated multi-view question-answer pairs across 90 diverse real-world scenes, categorized into six tasks: counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation. - Experiments on 27 representative MLLMs reveal a substantial performance gap between current models and human-level proficiency, particularly in tasks involving cross-view correspondence for partially occluded views and establishing coarse camera poses. - An in-depth analysis suggests that existing MLLMs struggle with identifying the same object across multiple viewpoints, and that current chain-of-thought prompting techniques are insufficient to address these limitations. - The findings highlight the need for domain-specific refinements or specialized modules to enhance multi-view awareness in MLLMs. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/danielchyeh/All-Angles-Bench) | N/A |
| [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient
  Training of Code LLMs](https://arxiv.org/abs/2504.14655) | Yan Wang, Yunhui Xia, chuyi777, jasonkleinlove, Swtheking | - This paper introduces LeetCodeDataset, a benchmark for evaluating and training code-generation LLMs.  - The dataset consists of curated LeetCode Python problems with rich metadata, 100+ test cases per problem, and temporal splits for contamination-free evaluation and efficient supervised fine-tuning (SFT). - Experiments reveal that reasoning models significantly outperform non-reasoning counterparts on this dataset. - SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. - This demonstrates the high-quality and efficiency of the LeetCodeDataset for training code LLMs. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/newfacade/LeetCodeDataset) | [Link](https://huggingface.co/datasets/newfacade/LeetCodeDataset) |
| [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to
  Deliberative Reasoners](https://arxiv.org/abs/2504.14239) | Xavier Hu, Yuhang Liu, xiaotianhan, xieck13, pengxiang | - This paper introduces InfiGUI-R1, a Multimodal Large Language Model (MLLM)-based GUI agent trained with the novel Actor2Reasoner framework, designed to enhance GUI agent reasoning capabilities. - The two-stage framework first injects spatial reasoning into the base MLLM through Spatial Reasoning Distillation, leveraging a teacher model, to transition from a Reactive Actor to a Basic Reasoner.  - The second stage, Deliberation Enhancement, refines the Basic Reasoner into a Deliberative Reasoner using Reinforcement Learning with Sub-goal Guidance and Error Recovery Scenario Construction to bolster planning and reflection.  - InfiGUI-R1 achieves state-of-the-art performance on cross-platform GUI grounding (87.5% on ScreenSpot) and strong results on complex, long-horizon tasks (71.1% on AndroidControl-High), outperforming existing models with comparable or even larger parameters. - This demonstrates the framework's effectiveness in improving agent performance, particularly in tasks requiring complex reasoning and planning. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/Reallm-Labs/InfiGUI-R1) | N/A |
| [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language
  Models](https://arxiv.org/abs/2504.15133) | Linear-Matrix-Probability, HaomingXu, xukewei, Saberlve, xzwnlp | - EasyEdit2 is a framework for controlling Large Language Model (LLM) behavior at test time without modifying model parameters. - It features a new architecture with key modules like the steering vector generator and applier for seamless model steering. - EasyEdit2 supports various interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features, and allows control with a single example. - Experimental results demonstrate EasyEdit2's effectiveness across different LLMs and dimensions, showing improvements over baseline methods and prior work. - The framework is open-sourced with an online demo and video tutorial for easy use and experimentation. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zjunlp/EasyEdit) | N/A |
| [LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration
  Benchmark](https://arxiv.org/abs/2504.13805) | dkeeeee, Yuxiang007, zhimingc, Pengxiangzhao, lgy0404 | - LearnAct, a novel multi-agent framework, enhances mobile GUI agent performance through few-shot demonstration learning, addressing long-tail scenarios and personalization gaps. - LearnGUI, the first comprehensive dataset for demonstration-based learning in mobile GUI agents, includes 2,252 offline and 101 online tasks with human demonstrations, enabling personalized adaptation. - LearnAct incorporates three agents: DemoParser extracts knowledge, KnowSeeker retrieves relevant knowledge, and ActExecutor combines instructions, GUI context, and demonstrations for task completion. - Evaluations demonstrate substantial improvements: Gemini-1.5-Pro accuracy increases from 19.3% to 51.7% (198.9% relative improvement) offline, and UI-TARS-7B-SFT online task success rate rises from 18.1% to 32.8% (+14.7%). - The results establish demonstration-based learning as a promising direction for adaptable and personalized mobile GUI agents, offering personalized assistance in diverse scenarios. | ['Multimodal'] | N/A | N/A |
| [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217) | Somayeh Sojoudi, Jonah Casebeer, Njb, Bai-YT | - DRAGON, a novel framework for fine-tuning generative models using distributional rewards, is introduced, allowing optimization of instance-wise, instance-to-distribution, and distribution-to-distribution rewards, unlike traditional RLHF or pairwise preference methods. - Novel reward functions are designed by selecting an encoder (e.g., CLAP) and reference examples, enabling optimization of metrics like FAD and enhancing generations by comparing them to an exemplar distribution, even across modalities. - Evaluated on an audio-domain text-to-music diffusion model with 20 reward functions, including custom aesthetics, CLAP, Vendi, and FAD, DRAGON achieved an average 81.45% win rate across targets. - Human evaluation shows a 60.95% win rate in perceived music quality without human preference annotations, solely based on appropriate exemplar sets. - Example generations can be found at https://ml-dragon.github.io/web. | ['Text-to-Audio', 'Audio'] | N/A | N/A |
| [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary
  Quality-Diversity Search](https://arxiv.org/abs/2504.15047) | Truong-Son Hy, tnngo2, quyanh | - RAINBOWPLUS, a novel red-teaming framework, enhances adversarial prompt generation for Large Language Models (LLMs) using an adaptive quality-diversity search. - It employs a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, overcoming limitations of prior methods like Rainbow Teaming. - Experiments across various datasets and LLMs demonstrate superior attack success rate (ASR) and diversity compared to Rainbow, generating up to 100 times more unique prompts. - On HarmBench with 12 LLMs, RAINBOWPLUS achieves 81.1% average ASR, outperforming AutoDAN-Turbo by 3.9% and running 9 times faster. - The open-source implementation facilitates further research and development in LLM red-teaming and safety assessment. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/knoveleng/rainbowplus) | N/A |
| [NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning](https://arxiv.org/abs/2504.13941) | yejinchoinka, ericnyberg, ekmb, shrimai19, SieraL | - NEMOTRON-CROSSTHINK is a novel framework that uses multi-domain corpora for reinforcement learning (RL) training of large language models (LLMs), improving generalization across diverse reasoning tasks. - It addresses the challenge of verifiable reward modeling for non-deterministic domains by employing templates on curated data to limit answer space diversity, enabling scalable RL training. - The framework incorporates diverse data sources (synthetic and real-world question-answer pairs), applies structured templates (multiple-choice and open-ended), filters for verifiable answers, and optimizes data blending strategies. - Experimental results show significant accuracy improvements on math (MATH-500: +30.1%, AMC23: +27.5%) and non-math (MMLU-PRO: +12.8%, AGIEVAL: +15.1%) benchmarks. - NEMOTRON-CROSSTHINK also improves response efficiency, using 28% fewer tokens for correct answers compared to math-only training. | ['Reinforcement Learning', 'Question Answering', 'Natural Language Processing'] | N/A | N/A |
