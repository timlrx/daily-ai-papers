

## Papers for 2025-04-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120) | Omar Hadid, Sara Chrouf, ZeinaD, Moatasem444, Hennara | - This paper introduces Kuwain 1.5B, a compact multilingual Arabic-English language model trained by injecting Arabic into the English-centric TinyLlama 1.1B model, adding 8 new layers and expanding the vocabulary with 26K Arabic tokens. - This method reduces training costs by 70% compared to training a new model from scratch and improves Arabic language performance by 8% while maintaining original English proficiency with only 20% of the original English training data and 1% improvement compared to the base TinyLlama model. - The approach involves freezing the original model's layers and training only the new layers, achieving stability by keeping the final encoder layer trainable.  - Evaluation on Arabic benchmarks shows competitive performance against larger models, demonstrating efficient language model expansion. - A fine-tuned version, Lahajawi, achieves impressive results in Arabic cross-dialect translation, showcasing the method's potential for diverse language tasks. | ['Natural Language Processing', 'Translation', 'Text Generation'] | [Link](https://github.com/misraj-ai/Kuwain-Arabic-cleaner) | [Link](https://huggingface.co/spaces/OALL/Open-Arabic-LLM-Leaderboard) |
| [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521) | Huifeng Yin, Sinuo Liu, Weixuan Wang, Minghao Wu, ChenyangLyu | - This paper examines over 2,000 non-English multilingual benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. - The findings reveal that English remains significantly overrepresented in these benchmarks, most benchmarks rely on original language content rather than translations, and the majority of content is sourced from high-resource countries. - A comparison of benchmark performance with human judgments highlights notable disparities, with STEM-related tasks exhibiting stronger correlations than traditional NLP tasks. - The study finds that translating English benchmarks into other languages is insufficient, as localized benchmarks show significantly higher alignment with local human judgments. - The paper identifies six key limitations in current multilingual evaluation practices, proposes guiding principles for effective multilingual benchmarking, and outlines five critical research directions, including addressing imbalance in Natural Language Generation tasks and improving representation for low-resource languages. | ['Natural Language Processing'] | N/A | N/A |
| [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072) | Yifan Ding, richardaecn, yala, Boyiliee, longlian | - This paper introduces the Describe Anything Model (DAM) for detailed localized image and video captioning.  - DAM uses a focal prompt for high-resolution encoding of target regions and a localized vision backbone to integrate precise localization with broader context.  - It also introduces a semi-supervised learning data pipeline (DLC-SDP) to address data scarcity and DLC-Bench, a benchmark to evaluate detailed localized captioning without reference captions.  - DAM achieves state-of-the-art performance on seven benchmarks, including keyword, phrase, and detailed multi-sentence localized captioning for images and videos.  - The model surpasses strong API-only baselines like GPT-40 and o1. | ['Image-to-Text', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/richardaecn/describe-anything) | [Link](describe-anything.github.io) |
| [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466) | Charlie Snell, Long Lian, Jiayi Pan, yala, xiuyul | - This paper introduces Adaptive Parallel Reasoning (APR), a novel framework that allows language models to dynamically switch between serial and parallel computation during inference, improving reasoning capabilities. - APR utilizes a parent-child threading mechanism where parent threads can spawn child threads to explore different reasoning paths concurrently, returning results via a join operation, and it is optimized using reinforcement learning. - Experiments on the Countdown reasoning task show APR achieves higher accuracy within the same context window (83.4% vs. 60.0% at 4k context), better scalability with increased compute (80.1% vs. 66.6% at 20k tokens), and improved accuracy at equivalent latency (75.2% vs. 57.3% at ~5000ms) compared to serial methods. - APR's reinforcement learning focuses on optimizing when and how to parallelize, improving performance by increasing both sequence length and the number of child threads rather than just deepening the search. - The efficiency improvements stem from APR's ability to distribute reasoning across multiple threads, reducing the pressure on context window limitations and enabling exploration of more complex solutions. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Parallel-Reasoning/APR) | N/A |
| [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning
  in Multimodal LLMs](https://arxiv.org/abs/2504.15415) | Yifan Yao, Jarvis Guo, Yuanxing Zhang, JinChengRen, mdh98 | - IV-Bench is introduced as the first comprehensive benchmark designed for evaluating image-grounded video perception and reasoning capabilities of Multimodal Large Language Models (MLLMs). - The benchmark comprises 967 videos and 2,585 image-text queries across 13 tasks, categorized into perception and reasoning, and spanning five representative video categories. - Evaluations conducted on 27 state-of-the-art MLLMs, including both open and closed-source models, reveal that existing models struggle with these tasks, achieving a maximum accuracy of only 28.9%. - Larger models demonstrate moderate performance gains compared to smaller models, and factors like the number of frames and video resolution are shown to impact model performance. - A simple data synthesis approach is also proposed, suggesting that data format is not the sole challenge of IV-Bench tasks. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/multimodal-art-projection/IV-Bench) | N/A |
| [BookWorld: From Novels to Interactive Agent Societies for Creative Story
  Generation](https://arxiv.org/abs/2504.14538) | Yanghua Xiao, Jiaqing Liang, Tian Qiu, Xintao Wang, Yiting Ran | - BookWorld is introduced, a novel system designed for constructing and simulating book-based multi-agent societies for creative story generation. - The system extracts character data and background knowledge from source books, constructs a multi-agent system where characters interact within a simulated world, and uses LLMs to weave the simulation's events into novel-style narratives. - BookWorld incorporates a specialized method for extracting worldview details from source books and a dynamic attribute updating mechanism enabling characters to evolve while maintaining fidelity to their original traits. - Experimental results demonstrate that BookWorld generates high-quality, creative stories while maintaining fidelity to the source material, outperforming direct generation and a previous story generation method (HoLLMwood) in 75.36% of cases. - The system offers diverse applications, including interactive story generation and social simulation within fictional worlds, opening new possibilities for creative exploration and enjoyment of beloved literary works. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992) | Jianqiao Lu, Sijun Zhang, Shen Yan, Taoer, bongbohong | - The paper introduces PHD-Transformer, a novel framework for efficient length scaling during pre-training of large language models (LLMs) while preserving inference efficiency. - PHD-Transformer employs a KV cache management strategy that differentiates between original and hidden decoding tokens, retaining only the KV cache of original tokens for long-range dependencies. - The paper introduces two optimized variants: PHD-SWA uses sliding window attention to maintain local dependencies, and PHD-CSWA implements chunk-wise sliding window attention to reduce pre-filling time. - Experiments demonstrate consistent improvements across benchmarks like ARC, HellaSwag, PIQA, Winogrande, MMLU, and CommonsenseQA. - PHD-CSWA-3-16-32 shows an average 2.0% accuracy improvement and 0.034 decrease in training loss compared to the baseline 1.2B model. | ['Natural Language Processing'] | N/A | N/A |
| [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030) | Zejun Ma, Wei Li, Yiqi Lin, Ziyun Zeng, Joya Chen | - This paper introduces LiveCC, a novel Video Large Language Model (Video LLM) trained on a large-scale dataset of ASR transcripts, focusing on real-time video commentary. - A streaming training approach densely interleaves ASR words and video frames according to their timestamps, enabling fine-grained temporal modeling. - LiveCC-7B-Instruct outperforms existing 72B parameter models in commentary quality and achieves SOTA on video QA benchmarks. - A new benchmark, LiveSports-3K, is introduced for evaluating real-time video commentary using LLM-as-a-judge. - The proposed model showcases low-latency commentary generation capabilities and broad generalizability in video understanding tasks. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering'] | [Link](showlab.github.io/livecc) | N/A |
| [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681) | Fan Chen, Chia-Wen Kuo, Celong Liu, Vidi Team, daviddousa | - Vidi is a family of Large Multimodal Models (LMMs) designed for video understanding and editing tasks, specializing in temporal retrieval. - The model identifies time ranges in videos corresponding to given text queries, handling hour-long videos and multiple modalities (vision, audio, text). - Vidi utilizes a Decomposed Attention (D-Attn) architecture, enabling efficient processing of long videos and facilitating multimodal alignment. - It is trained in three stages: adapter training on image/audio captions, synthetic data training for temporal alignment, and real video training with dense captions and subtitles. - Evaluation on the VUE-TR benchmark demonstrates that Vidi surpasses leading proprietary models, showcasing its efficacy in temporal retrieval tasks. | ['Video-Text-to-Text', 'Multimodal', 'Video Classification'] | N/A | N/A |
| [LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making
  Abilities](https://arxiv.org/abs/2504.16078) | Razvan Pascanu, Markus Wulfmeier, Jordi Grau-Moya, Jörg Bornschein, Thomas Schmied | - This paper investigates why Large Language Models (LLMs) often exhibit suboptimal performance in decision-making scenarios, focusing on greediness, frequency bias, and the knowing-doing gap as key failure modes. - It proposes Reinforcement Learning Fine-Tuning (RLFT) on self-generated Chain-of-Thought (CoT) rationales to mitigate these shortcomings, demonstrating improved decision-making abilities in multi-armed bandits, contextual bandits, and Tic-tac-toe environments. - The study shows that RLFT enhances exploration and narrows the knowing-doing gap, but also explores classic and LLM-specific exploration mechanisms to further improve RLFT effectiveness. - Experiments with different model sizes (2B, 9B, 27B) reveal that RLFT consistently improves performance across various tasks and scales, indicating its potential for enhancing LLM agents. - Ablation studies highlight the importance of CoT reasoning, expert data, and providing sufficient "thinking" tokens for achieving optimal performance. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082) | Yu-Xiong Wang, Ziqi Pang | - MR. Video is a new framework for long video understanding based on the MapReduce principle: independently perceiving short video clips (Map) and jointly aggregating information (Reduce). - It addresses limitations of sequence-to-sequence Vision-Language Models (VLMs) and video agents, strategically performing detailed short video perception without context length constraints and enabling sequence-parallel perception with comprehensive context aggregation. - Employs two MapReduce stages: (A) Captioning: generates short video captions and standardizes repeated elements; (B) Analysis: analyzes information from individual videos for each question and integrates them into a final answer. - Leverages Large Language Models (LLMs), specifically Gemini-Flash for vision and GPT-4 for text, achieving over 10% accuracy improvement on LVBench compared to state-of-the-art VLMs and video agents. - Demonstrates the effectiveness of MapReduce for long video understanding, suggesting a simple yet powerful approach for processing lengthy video content. | ['Video-Text-to-Text', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/ziqipang/MR-Video) | N/A |
| [Progent: Programmable Privilege Control for LLM Agents](https://arxiv.org/abs/2504.11703) | Hongwei Li, Linyu Wu, Zhun Wang, Jingxuan He, stneng | - Progent, a new privilege control framework for Large Language Model (LLM) agents, is introduced to mitigate security risks associated with these agents interacting with external environments. - Progent employs a domain-specific language (DSL) based on JSON Schema, enabling developers to define fine-grained access control policies, specifying permissible tool calls and their conditions, along with fallback actions for disallowed calls. - These policies are enforced during agent execution, dynamically updated based on agent feedback and new information, and managed either manually or through automated policy generation and updates using LLMs familiar with JSON. - Evaluation across various scenarios, including AgentDojo, ASB, and AgentPoison benchmarks, demonstrates Progent's effectiveness in drastically reducing attack success rates (e.g., from 41.2% to 2.2% on AgentDojo) with minimal impact on agent utility, and even enhancing it in some cases. - Further analysis reveals Progent's modular design, enabling easy integration with minimal code changes, and showcases its resilience against adaptive attacks targeting policy generation LLMs. | ['Natural Language Processing'] | [Link](https://github.com/sunblaze-ucb/progent) | N/A |
| [IPBench: Benchmarking the Knowledge of Large Language Models in
  Intellectual Property](https://arxiv.org/abs/2504.15524) | Minghui Zhu, Huaren Liu, Hongbo Wang, Guhong Chen, QiYao-Wang | - Introduces IPBench, a benchmark designed to evaluate the knowledge and capabilities of Large Language Models (LLMs) in intellectual property applications. - Covers 8 IP mechanisms and 20 tasks, including information processing, logical reasoning, discriminant evaluation, and creative generation. - A benchmark of 16 LLMs showed that even the best-performing model only reached 75.8% accuracy, demonstrating substantial room for improvement. - Open-source IP and law-oriented models lag behind closed-source general-purpose models. - IPBench data and code are publicly available, and it will be updated in the future. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/IPBench/IPBench) |
| [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via
  Occluded Object Counting](https://arxiv.org/abs/2504.15485) | Mohit Bansal, Jaemin Cho, Elias Stengel-Eskin, Atin Pothiraj | - This paper introduces CAPTURE (Counting Amodally for Patterns Through Unseen REgions), a novel benchmark designed to evaluate the spatial reasoning and world modeling capabilities of Vision Language Models (VLMs) in the presence of occlusions. - CAPTURE involves presenting VLMs with images of objects arranged in patterns, where some objects are occluded, and requires the models to count the total number of objects by inferring the continuation of the pattern behind the occluder. - The benchmark includes two datasets: CAPTUREreal, consisting of real-world images, and CAPTUREsynthetic, a controlled set of generated images to isolate specific factors influencing performance. - Experiments with strong VLMs like GPT-40 reveal that these models struggle with amodal counting, performing worse on occluded compared to unoccluded images, while humans achieve near-perfect accuracy on the task.  - Providing auxiliary information, such as object coordinates or inpainted occluded regions, significantly improves VLM performance, indicating a weakness in visual world modeling and integrating visual and textual information rather than counting itself. | ['Multimodal', 'Computer Vision', 'Visual Question Answering'] | [Link](https://github.com/atinpothiraj/CAPTURE) | N/A |
| [DiffVox: A Differentiable Model for Capturing and Analysing Professional
  Effects Distributions](https://arxiv.org/abs/2504.14735) | Wei-Hsiang Liao, Ben Hayes, Junghyun Koo, Marco A. Martínez-Ramírez, yoyolicoris | - This paper introduces DiffVox, a differentiable model for capturing and analyzing professional vocal effects distributions in music production.  - DiffVox integrates parametric equalization, dynamic range control, delay, and reverb with efficient differentiable implementations, enabling gradient-based optimization for parameter estimation. - Analysis of parameter correlations from two datasets (MedleyDB and a private collection) reveals relationships between effects and parameters, such as delay time correlating with intensity and high-pass/low-shelf filters shaping low-end frequencies. - Principal component analysis connects these correlations to McAdams' timbre dimensions, highlighting the influence of spaciousness and spectral brightness. - The study confirms the non-Gaussian nature of parameter distributions, setting the foundation for future research in vocal effects modeling and automatic mixing. | ['Audio', 'Audio-to-Audio'] | [Link](https://github.com/SonyResearch/diffvox) | N/A |
