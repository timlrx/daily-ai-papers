

## Papers for 2025-04-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/abs/2504.17761) | Peng Xing, Yucheng Han, Shiyu Liu, skicy, wchengad | - Step1X-Edit is an open-source, general-purpose image editing model that leverages a Multimodal Large Language Model (MLLM) and a Diffusion in Transformer (DiT) architecture. - The MLLM processes the image and text instruction, generating editing tokens that guide the DiT to produce the edited image.  - A novel dataset generation pipeline and a new benchmark called GEdit-Bench support training and evaluation.  - Experimental results on GEdit-Bench show Step1X-Edit substantially outperforming existing open-source methods and achieving comparable performance to closed-source models like GPT-40 and Gemini. - Step1X-Edit combines the advantages of both MLLMs and DiTs, enabling general image edits based on complex instructions while maintaining image fidelity. | ['Image-to-Image', 'Multimodal'] | [Link](https://github.com/stepfun-ai/Step1X-Edit) | N/A |
| [Paper2Code: Automating Code Generation from Scientific Papers in Machine
  Learning](https://arxiv.org/abs/2504.17192) | Sung Ju Hwang, Seongyun Lee, jinheon, iaminju | - Paper2Code is a multi-agent LLM framework that transforms machine learning papers into functional code repositories, addressing the reproducibility challenge in ML research by automating code generation directly from research papers. - The framework operates in three stages: planning (creating a roadmap, system architecture, and configuration files), analysis (interpreting implementation details), and generation (producing modular code). - Each stage utilizes specialized LLM agents designed for effective collaboration, enabling the system to emulate the typical workflow of human developers. - Evaluation on a Paper2Code benchmark of papers from top-tier venues and the PaperBench benchmark shows PaperCoder significantly outperforms baselines in generating valid and faithful code. - Human evaluations by original paper authors show 77% rate PaperCoder as best, with 85% finding the generated repositories helpful, and analysis indicates high executability with minor modifications. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Breaking the Modality Barrier: Universal Embedding Learning with
  Multimodal LLMs](https://arxiv.org/abs/2504.17432) | Yanzhao Zhang, Xingjun Wang, Ziyong Feng, Tiancheng Gu, Kaichengalex | - UniME, a two-stage framework, leverages Multimodal Large Language Models (MLLMs) to learn universal representations for various vision-language tasks. - The first stage, Textual Discriminative Knowledge Distillation, uses a powerful LLM-based teacher model to enhance the MLLM's language component's embedding capabilities. - The second stage, Hard Negative Enhanced Instruction Tuning, improves discriminative representation learning by filtering false negatives and using hard negative sampling. - Evaluations on the MMEB benchmark and multiple retrieval tasks show UniME achieves state-of-the-art performance, demonstrating strong discriminative and compositional understanding. - UniME surpasses existing models like E5-V and VLM2Vec on tasks such as short & long caption retrieval and compositional retrieval, showcasing its robust representation learning capabilities. | ['Multimodal', 'Image Feature Extraction', 'Image-to-Text'] | N/A | N/A |
| [Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery
  Simulation](https://arxiv.org/abs/2504.17207) | Leonidas Guibas, Mikaela Angelina Uy, Chanho Park, Jihyeon Je, Phillip Y. Lee | - This paper introduces Abstract Perspective Change (APC), a framework for enhancing perspective-aware spatial reasoning in Vision-Language Models (VLMs). - APC simulates the mental imagery process by constructing a 3D scene abstraction from an input image and question, using vision foundation models for object detection, segmentation, and orientation estimation. - The framework then transforms the scene abstraction to align with the perspective of a designated reference object in the image, converting the allocentric reasoning task into an egocentric one. - This transformed scene is then presented to the VLM as either a textual prompt with 3D coordinates or a rendered visual prompt depicting the scene from the new perspective. - Experiments on synthetic and real-world datasets show APC significantly outperforms existing VLMs and spatial reasoning models, demonstrating its effectiveness in handling alternative viewpoints. | ['Multimodal', 'Visual Question Answering'] | N/A | [Link](https://apc-vlm.github.io/) |
| [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM
  Pretraining](https://arxiv.org/abs/2504.16511) | Yifan Zhang, Zhimiao Yu, Binbin Liu, Weidong Zhou, Fengze Liu | - QuaDMix, a novel data selection framework, jointly optimizes data quality and diversity for Large Language Model (LLM) pretraining, addressing the trade-off between these two crucial aspects. - The framework employs multiple quality scorers and domain classification to label the pretraining data, and then utilizes a parameterized sampling function to determine the sampling frequency of each data point based on its quality and domain. - QuaDMix uses simulated experiments with smaller models and a LightGBM regressor to efficiently search for optimal parameters within the framework, reducing the computational cost of large-scale training. - Experiments on diverse models and datasets show that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks, outperforming methods that focus solely on quality or diversity. - The results highlight QuaDMix's ability to effectively balance data quality and diversity for enhanced LLM pretraining. | ['Natural Language Processing'] | N/A | N/A |
| [Token-Shuffle: Towards High-Resolution Image Generation with
  Autoregressive Models](https://arxiv.org/abs/2504.17789) | Chih-Yao Ma, Hao Tang, Haoyu Ma, Peize Sun, Xu Ma | - Token-Shuffle, a novel method reduces the number of image tokens in Transformers, enabling efficient high-resolution image generation within Multimodal Large Language Models (MLLMs). - It leverages the dimensional redundancy of visual vocabularies in MLLMs by merging spatially local tokens along the channel dimension (token-shuffle) and restoring the spatial arrangement after processing (token-unshuffle). - This approach allows for the generation of images up to 2048x2048 resolution using a unified next-token prediction framework, maintaining efficient training and inference. - The 2.7B model achieves a 0.77 overall score on hard prompts in the GenAI-benchmark, outperforming other autoregressive and diffusion models. - Large-scale human evaluations further demonstrate its superior capabilities in text-alignment, visual flaw handling, and overall visual appearance. | ['Text-to-Image', 'Multimodal'] | N/A | N/A |
| [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040) | Heng Ji, Silvio Savarese, Caiming Xiong, Senthil Purushwalkam, Zhenhailong Wang | - DyMU is a training-free framework that dynamically reduces the computational burden of Vision-Language Models (VLMs) by decreasing the number of visual tokens based on image complexity. - Dynamic Token Merging (DToMe) component merges similar visual tokens based on image complexity, and Virtual Token Unmerging (VTU) reconstructs the attention dynamics of a full token sequence for the language model. - Experiments on image and video understanding tasks show DyMU reduces visual tokens by 32-85% while maintaining comparable performance to full-length models, across diverse VLM architectures. - This method is training-free and readily applicable to any VLM architecture. - The approach provides greater control over computational cost and allows combination of visual reasoning tools and DYMU to further improve efficiency while maintaining performance. | ['Multimodal', 'Image Feature Extraction', 'Visual Question Answering'] | [Link](https://mikewangwzhl.github.io/dymu) | N/A |
| [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921) | Areg Mikael Sarvazyan, Álvaro Romo Herrero, Ian Borrego Obrador, José Ángel González, mchinea | - IberBench, a comprehensive benchmark designed to evaluate Large Language Models (LLMs) on both fundamental and industry-relevant Natural Language Processing (NLP) tasks in Iberian languages. - The benchmark includes 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment analysis, toxicity detection, and summarization. - Addresses limitations in current evaluation practices, such as lack of linguistic diversity and static evaluation by enabling continuous updates and community submissions. - Evaluates 23 LLMs ranging from 100 million to 14 billion parameters, finding that LLMs perform better in fundamental tasks than in industry-relevant tasks and Galician and Basque present greater challenges. - Offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard. | ['Natural Language Processing', 'Question Answering', 'Text Classification', 'Token Classification', 'Summarization'] | [Link](https://github.com/IberBench/iberbench-evaluation) | [Link](https://huggingface.co/spaces/iberbench/leaderboard) |
| [ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](https://arxiv.org/abs/2504.15921) | Mariano Beguerisse-Diaz, Shaogang Gong, Dimitrios Korkinof, Jian Hu | - ViSMaP (Video Summarisation by Meta-Prompting) is introduced for unsupervised hour-long video summarization, leveraging a three-stage approach involving short-form video learning, pseudo-summary generation with LLMs, and hour-long video adaptation. - The model employs TimeSformer as a feature encoder, DistilBERT for visual-language alignment, and GPT2 as a text decoder, utilizing cross-entropy and contrastive loss during training and symmetric cross-entropy loss during adaptation. - ViSMaP utilizes a novel iterative meta-prompting process to generate and refine pseudo-summaries for long videos, utilizing GPT-3.5 as optimizer and generator, and Gemini as evaluator to select key information from short video descriptions. - Experimental results on Ego4D-HCap show performance comparable to state-of-the-art supervised methods, outperforming zero-shot models and unsupervised baselines. - Further evaluations on MSRVTT, MSVD, and YouCook2 demonstrate generalization capabilities across diverse video captioning datasets, achieving results comparable to or exceeding supervised models. | ['Video-Text-to-Text', 'Summarization', 'Computer Vision'] | N/A | N/A |
| [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming
  Videos](https://arxiv.org/abs/2504.17343) | Shuhuai Ren, Lei Li, Yuancheng Wei, Yicheng Li, Linli Yao | - This paper introduces TimeChat-Online, an online Video Large Language Model (VideoLLM) designed for real-time interaction with streaming video content. - The core component is the Differential Token Drop (DTD) module, inspired by the Change Blindness phenomenon, which selectively preserves significant temporal changes in video frames while discarding redundant static visual information, achieving 82.8% token reduction. - TimeChat-Online maintains over 98% performance compared to full-token models on StreamingBench and shows a 5.7-point accuracy improvement on the VideoMME long set (30-60 minute videos) when integrated with Qwen2.5VL-7B. - It incorporates proactive responding, triggered by scene transitions detected by analyzing the token drop ratio curve, allowing the model to anticipate and answer questions related to future video content. - A new instruction-tuning dataset, TimeChat-Online-139K, is introduced, featuring diverse question-answer pairs specifically designed for streaming video question-answering scenarios including backward-tracing, current-time perception, and future-responding. | ['Video-Text-to-Text', 'Multimodal'] | N/A | [Link](https://timechat-online.github.io) |
| [Process Reward Models That Think](https://arxiv.org/abs/2504.16828) | Hao Peng, Jaekyeom Kim, Lajanugen Logeswaran, Rishabh Agarwal, Muhammad Khalifa | - This paper introduces THINKPRM, a generative process reward model (PRM) for verifying step-by-step reasoning, trained with minimal supervision on synthetic data using chain-of-thought (CoT) reasoning. - THINKPRM leverages the inherent reasoning capabilities of large language models (LLMs) and is fine-tuned on significantly fewer process labels than discriminative PRMs. - Experimental results demonstrate that THINKPRM outperforms both discriminative PRMs and LLM-as-a-Judge baselines across various benchmarks, including ProcessBench, MATH-500, and AIME '24, using only 1% of the process labels. - THINKPRM also excels in out-of-domain tasks like GPQA-Diamond and LiveCodeBench, showcasing its robustness and generalization ability. - The work highlights the potential of generative long CoT PRMs for scaling test-time compute for verification while requiring minimal supervision for training. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/mukhal/thinkprm) | N/A |
