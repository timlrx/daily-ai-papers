

## Papers for 2025-04-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with
  Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734) | Sung Ju Hwang, Soyeong Jeong, jinheon, KangsanKim71, wgcyeo | - UniversalRAG, a novel Retrieval-Augmented Generation (RAG) framework, retrieves and integrates knowledge from diverse corpora spanning multiple modalities (text, image, and video) and granularities (paragraph, document, image, clip, and video). - It addresses the "modality gap" in unified representation spaces by employing a modality-aware routing mechanism that dynamically selects the most suitable modality-specific corpus for retrieval based on the query. - Within each modality, UniversalRAG further organizes corpora into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. - Experimental results on 8 multimodal benchmarks demonstrate that UniversalRAG consistently outperforms modality-specific and unified baselines, showing robust performance and efficient resource allocation. - Further analyses reveal the effectiveness of trained routers, the importance of granularity in retrieval, and the framework's scalability with larger LVLMs. | ['Multimodal', 'Question Answering', 'Visual Question Answering', 'Document Question Answering', 'Video-Text-to-Text'] | N/A | N/A |
| [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595) | pangwei, sewon, Muennighoff, volpato30, rulins | • The paper introduces REASONIR-8B, a novel retriever model specifically trained for general reasoning tasks, addressing the limitations of existing retrievers on such tasks. • REASONIR-8B utilizes a synthetic data generation pipeline to create challenging and relevant queries with hard negatives for each document, enhancing its ability to handle complex reasoning. • The model achieves state-of-the-art performance on the BRIGHT benchmark, a widely used reasoning-intensive IR benchmark, without and with a reranker. • When applied to RAG tasks, REASONIR-8B demonstrates significant performance improvements on MMLU and GPQA compared to existing baselines. • The code, data, and model are open-sourced to facilitate future research and development in the area of reasoning-intensive information retrieval. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/facebookresearch/ReasonIR) | [Link](https://huggingface.co/reasonir/ReasonIR-8B) |
| [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998) | Yong Jae Lee, Trung Bui, Jing Shi, Krishna Kumar Singh, Thao Nguyen | - Yo'Chameleon, a novel approach for personalizing Large Multimodal Models (LMMs), enables tailored image and text generation for user-defined concepts using only 3-5 images. - The method addresses catastrophic forgetting by using "soft-positive" images, dynamically adjusting prompt length based on visual similarity, and introduces a self-prompting mechanism to optimize dual soft prompts for text and image generation tasks. - It leverages soft prompt tuning with a dynamic prompt length mechanism and a self-prompting optimization process to balance performance across both modalities. - Qualitative and quantitative results demonstrate Yo'Chameleon efficiently learns concepts with fewer tokens and outperforms prompting baselines in encoding visual attributes. - Evaluations on visual question answering, recognition accuracy, CLIP Image Similarity, and facial similarity reveal Yo'Chameleon's superior performance compared to existing baselines. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879) | Daniel D'Souza, Alex Wang, Yiyang Nan, Shivalika Singh, yuntian-deng | This work identifies systematic issues resulting in a distorted playing field in the Chatbot Arena, a benchmark for ranking AI systems.  The authors find that undisclosed private testing practices, selective score reporting, and biased model sampling rates lead to skewed results.  They establish that data access asymmetries exist, providing some providers unfair advantages in achieving higher rankings.  The paper concludes with actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking. | ['Natural Language Processing'] | [Link](https://github.com/lm-sys/FastChat/blob/0e6d3e4beaab66f4d3f93db72541a4abab8af28d/fastchat/serve/monitor/monitor_md.py#L7) | [Link](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) |
| [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046) | Daniel Khashabi, Benjamin Van Durme, Marc Marone, Jiacan Yu, jackzhang | - This paper introduces BLOOMSCRUB, a novel inference-time method for mitigating worst-case LLM copyright infringement by eliminating long verbatim quotes from copyrighted sources. - BLOOMSCRUB employs Bloom filters for efficient quote detection and dynamic rewriting techniques to transform potentially infringing segments, ensuring scalability and adaptability. - Experimental results demonstrate that BLOOMSCRUB significantly reduces infringement risk while preserving text quality and outperforms existing methods. - The method offers certified risk reduction through adaptive abstention when quotes cannot be rewritten, providing a reliable safeguard against legal liabilities. - BLOOMSCRUB's simple yet effective design makes it a practical and robust framework for certified copyright takedown in deployed LLMs. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting](https://arxiv.org/abs/2504.20630) | Tao Jin, Zhiyuan Zhu, Changhao Pan, Wenxiang Guo, AaronZ345 |  - This paper introduces ISDrama, a novel multimodal immersive spatial drama generation model that produces high-quality continuous multi-speaker binaural speech with dramatic prosody.  - The model architecture comprises a Multimodal Pose Encoder, which extracts unified pose information from diverse input modalities, and an Immersive Drama Transformer, a flow-based Mamba-Transformer model with Drama-MOE.  - ISDrama outperforms several baseline models on various objective and subjective metrics, demonstrating its ability to generate high-quality, immersive, spatial audio with dramatic prosody.  - The study also introduces MRSDrama, a new multimodal recorded spatial drama dataset used to train and evaluate the model, consisting of binaural audios, scripts, videos, geometric poses, and textual prompts.  - The proposed ISDrama model shows improved performance in various aspects like quality, speaker similarity, pose consistency and prosodic expressiveness due to utilizing various advanced techniques. | ['Audio', 'Text-to-Speech'] | N/A | N/A |
| [X-Fusion: Introducing New Modality to Frozen Large Language Models](https://arxiv.org/abs/2504.20996) | Yijun Li, Siddharth Srinivasan Iyer, Xun Huang, Thao Nguyen, Sicheng Mo | - X-Fusion is a novel framework that adapts pre-trained Large Language Models (LLMs) to multimodal tasks, preserving language capabilities while enabling image understanding and generation using a dual-tower architecture.  - This dual tower processes image data with trainable vision-specific weights and language data with frozen LLM weights, which enables cross-modal interaction between different modalities.  - X-Fusion outperforms other baseline architectures on image-to-text and text-to-image tasks, demonstrating the effectiveness of the dual-tower design.  - The study also reveals that using clean images for image understanding improves both generation and understanding performance, and there is an asymmetric relationship where understanding data benefits generation, but not vice-versa.  - Additionally, aligning vision features with pre-trained representations benefits smaller models but has less impact on larger ones. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://sichengmo.github.io/XFusion/) | N/A |
| [TreeHop: Generate and Filter Next Query Embeddings Efficiently for
  Multi-hop Question Answering](https://arxiv.org/abs/2504.20114) | Xuming Hu, Shuliang Liu, Jinghuai Ou, Zhonghao Li, kpzhang1028 | - TreeHop is an embedding-level framework for multi-hop question answering (MHQA) that dynamically updates query embeddings by fusing information from prior queries and retrieved documents, eliminating the need for LLMs in query refinement. - TreeHop replaces the traditional "Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined "Retrieve-Embed-Retrieve" loop, reducing computational overhead. - A rule-based stop criterion is introduced to prune redundant retrievals, balancing efficiency and recall. - Experimental results show TreeHop rivals advanced RAG methods on three open-domain MHQA datasets with only 5%-0.4% of the parameter size and 99% latency reduction. - TreeHop achieves this by using a gated cross-attention mechanism to extract salient information from retrieved chunks and is trained with contrastive learning. | ['Question Answering'] | [Link](https://github.com/allen-li1231/TreeHop-RAG) | N/A |
