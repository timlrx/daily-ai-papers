

## Papers for 2025-05-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [WebThinker: Empowering Large Reasoning Models with Deep Research
  Capability](https://arxiv.org/abs/2504.21776) | Yutao Zhu, Hongjin Qian, Guanting Dong, Jiajie Jin, Xiaoxi Li | - WebThinker is a novel deep research agent that empowers Large Reasoning Models (LRMs) to conduct autonomous web searches, navigate web pages, and generate research reports within their reasoning process. - It integrates a Deep Web Explorer module for dynamic information gathering and an Autonomous Think-Search-and-Draft strategy for real-time report writing. - RL-based training with online Direct Preference Optimization (DPO) is used to refine tool utilization. - Experimental results on complex reasoning (GPQA, GAIA, WebWalkerQA, HLE) and report generation (Glaive) benchmarks show WebThinker significantly outperforms current methods, including proprietary systems. - This approach enhances LRM reliability and opens possibilities for more capable deep research systems. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation'] | [Link](https://github.com/RUC-NLPIR/WebThinker) | N/A |
| [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318) | Harkirat Behl, Vidhisha Balachandran, Ahmed Awadallah, Sahaj Agarwal, Marah Abdin | - This technical report introduces Phi-4-reasoning and Phi-4-reasoning-plus, two 14-billion parameter reasoning models fine-tuned from the Phi-4 LLM and enhanced for complex reasoning tasks. - Phi-4-reasoning is trained via supervised fine-tuning on curated "teachable" prompts with reasoning demonstrations generated using 03-mini, while Phi-4-reasoning-plus incorporates a subsequent phase of outcome-based reinforcement learning on math problems. - Both models demonstrate significant performance improvements over the base Phi-4 model and other larger open-weight models across a variety of reasoning benchmarks, including math, science, coding, and algorithmic problem-solving, approaching the accuracy of the larger DeepSeek-R1 model. - Evaluation highlights the importance of data curation and the potential of combining supervised fine-tuning with reinforcement learning for developing efficient, smaller reasoning models.  - The report also underscores the need for more robust evaluation practices in reasoning benchmarks, emphasizing the impact of non-determinism and small dataset sizes on single-score accuracy reporting. | ['Question Answering', 'Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | N/A | [Link](https://huggingface.co/datasets/lchen001/AIME1983_2024), [Link](https://huggingface.co/datasets/lchen001/AIME2025) |
| [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720) | Tong Liu, Zhenlin Yang, Yixin Ji, Juntao Li, zenRRan | - This paper surveys methods for efficient Large Language Model (LLM) inference serving, categorizing them into instance-level optimizations, cluster-level strategies, emerging scenarios, and miscellaneous areas. - Instance-level methods include model placement, request scheduling, decoding length prediction, and KV cache optimization. - Cluster-level strategies involve GPU cluster configurations, service-oriented scheduling, and load balancing techniques to handle diverse workloads. - Emerging scenarios cover efficient serving strategies for long context, Retrieval-Augmented Generation (RAG), Mixture of Experts (MoE), Low-Rank Adaptation (LoRA), speculative decoding, augmented LLMs, and test-time reasoning. - Miscellaneous areas address critical aspects like hardware considerations, privacy concerns, the use of simulators, ensuring fairness, and managing energy consumption. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/zenrran4nlp/Awesome-LLM-Inference-Serving) | N/A |
| [UniBiomed: A Universal Foundation Model for Grounded Biomedical Image
  Interpretation](https://arxiv.org/abs/2504.21336) | Hao Chen, Jiaxin Zhuang, Sunan He, Yuxiang Nie, Linshan Wu | - UniBiomed, a novel universal foundation model, is introduced for grounded biomedical image interpretation, unifying the generation of clinical texts and segmentation of corresponding objects using a Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM). - This model tackles a wide array of tasks across ten biomedical imaging modalities using a newly curated dataset of over 27 million image-annotation-text triplets. - Extensive validation across 84 datasets demonstrates state-of-the-art performance in segmentation, exceeding existing models like BiomedParse by an average of 10.25% Dice score. - UniBiomed automates end-to-end grounded interpretation, removing the need for expert pre-diagnosis and manual prompt creation required by previous methods, optimizing clinical workflow. - This model also excels in grounded disease recognition, region-aware diagnosis, visual question answering, and report generation across diverse imaging modalities. | ['Image Segmentation', 'Text Generation', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/Luffy03/UniBiomed) | N/A |
