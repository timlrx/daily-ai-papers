

## Papers for 2025-05-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Voila: Voice-Language Foundation Models for Real-Time Autonomous
  Interaction and Voice Role-Play](https://arxiv.org/abs/2505.02707) | Yu Shu, Yemin Shi, zhitinghu, Jaward, guangyil | - Voila, a family of large voice-language foundation models, is introduced for real-time, natural, and flexible voice interaction.  - Voila uses a hierarchical Transformer architecture with streaming audio encoding and tokenization, a multi-scale Transformer with an LLM backbone and audio generator, and is trained end-to-end with audio-text data. - Voila-e2e focuses on low-latency, nuanced voice conversations, while Voila-autonomous allows continuous listening, reasoning, and responses in a full-duplex manner. - On the Voila benchmark, Voila outperforms SpeechGPT and Moshi, achieving 30.56% accuracy.  - Voila also performs competitively on ASR and TTS tasks, with a WER of 2.7% and 2.8% respectively when trained with LibriSpeech data. | ['Multimodal', 'Audio', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition'] | [Link](github.com/maitrix-org/Voila) | [Link](hf.co/spaces/maitrix-org/Voila-demo), [Link](hf.co/maitrix-org/Voila-base), [Link](hf.co/maitrix-org/Voila-chat), [Link](hf.co/maitrix-org/Voila-autonomous-preview), [Link](hf.co/maitrix-org/Voila-Tokenizer), [Link](hf.co/datasets/maitrix-org/Voila-Benchmark), [Link](hf.co/datasets/maitrix-org/Voila-million-voice) |
| [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387) | Ziqi Wang, zhangdenghui123, Merlin-Hongru, gaotang, XtremSup | - This paper introduces Reasoning Reward Models (REASRMS), a new class of generative reward models that frame reward modeling as a reasoning task, enhancing interpretability and performance. - The proposed model, RM-R1, employs a two-stage training process: distillation of high-quality reasoning chains and reinforcement learning with verifiable rewards. - RM-R1 leverages a novel Chain-of-Rubrics (CoR) prompting framework to elicit structured reasoning, classifying tasks as reasoning or chat and generating tailored evaluations. - Empirical results demonstrate state-of-the-art or near state-of-the-art performance on multiple reward model benchmarks, outperforming larger open-weight and proprietary models by up to 13.8%. -  The enhanced performance is validated across various domains, including chat, safety, and reasoning, highlighting the effectiveness of the reasoning-based training approach. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/RM-R1-UIUC/RM-R1) | N/A |
| [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop
  Reasoning with Transformers](https://arxiv.org/abs/2504.20752) | Gjergji Kasneci, Roman Abramov, fsteinbauer | - This paper explores grokking, a phenomenon where neural networks transition from memorization to generalization, in the context of real-world multi-hop reasoning using transformers. - It addresses the challenge of sparse real-world data by augmenting existing knowledge graphs with synthetic data to achieve the threshold ratio of inferred to atomic facts required for grokking. - Surprisingly, even factually incorrect synthetic data can improve reasoning circuits, possibly by encouraging reliance on relational structure over memorization. - The approach achieves up to 95-100% accuracy on 2WikiMultiHopQA, surpassing baselines and matching or exceeding state-of-the-art results. - The analysis suggests that data augmentation based on grokking can unlock implicit multi-hop reasoning capabilities in large language models, leading to more robust and interpretable factual reasoning. | ['Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language
  Models](https://arxiv.org/abs/2505.02735) | ZhengYuan, yifanzhang114, Liam-Liu, prt66, zhouliang | - FormalMATH, a large-scale Lean4 benchmark with 5,560 formally verified problems ranging from high-school Olympiad challenges to undergraduate-level theorems is introduced to address limitations of existing benchmarks. - A human-in-the-loop autoformalization pipeline integrating LLMs for statement autoformalization, multi-LLM semantic verification, and negation-based disproof filtering is proposed to mitigate manual formalization costs, retaining 72.09% of statements while maintaining accuracy. - Evaluation of state-of-the-art LLM-based theorem provers revealed significant limitations, with the best model achieving only a 16.46% success rate under practical sampling budgets. - Existing provers exhibit domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics; counterintuitively, natural-language solution guidance is found to decrease proof success in chain-of-thought reasoning. - FormalMATH provides a robust benchmark for evaluating and improving formal mathematical reasoning capabilities in LLMs. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link]([data]) |
| [ReplaceMe: Network Simplification via Layer Pruning and Linear
  Transformations](https://arxiv.org/abs/2505.02819) | szagoruyko121, stamatisl, madrugado, ammarali32, dimitriish | - ReplaceMe is a training-free structured pruning method for LLMs that replaces transformer blocks with linear transformations, thereby reducing computational overhead without significant performance loss. - Unlike other pruning methods, ReplaceMe doesn't require any retraining or fine-tuning, making it computationally efficient. - It estimates linear transformations using a small calibration dataset and merges them into existing model weights without introducing additional parameters. - Experimental results show that ReplaceMe outperforms other training-free approaches and remains competitive with state-of-the-art pruning methods that involve retraining, achieving up to 25% pruning while retaining ~90% of the original model's performance. - The method also generalizes to other transformer architectures like ViT, showing its effectiveness beyond text generation tasks. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Practical Efficiency of Muon for Pretraining](https://arxiv.org/abs/2505.02222) | cadarsh-essential, monk-essential, karlstratos, ampolloreno, ishaan-essential | - This paper introduces Muon, a novel second-order optimizer for pretraining large language models, and demonstrates its superior performance over AdamW. - Muon achieves greater data efficiency at large batch sizes, expanding the Pareto frontier on the compute-time tradeoff. - The authors propose a simple telescoping algorithm that efficiently addresses hyperparameter tuning challenges across different model sizes. - Extensive experiments with models up to four billion parameters validate Muon's improved performance and the effectiveness of the telescoping algorithm. - The work demonstrates that Muon is a practical and efficient alternative to AdamW for large-scale pretraining. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/EssentialAI) |
| [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement
  Learning](https://arxiv.org/abs/2505.02835) | KevinTowne, KaiyuValley, bhsc24, XingyuLu, yifanzhang114 | - This paper introduces R1-Reward, a novel multimodal reward model trained using a novel reinforcement learning algorithm called StableReinforce. - StableReinforce addresses instability issues in existing RL algorithms by refining the training loss, advantage estimation, and reward design. - R1-Reward significantly outperforms existing state-of-the-art models on three benchmark datasets: VL Reward-Bench, Multimodal Reward Bench, and MM-RLHF Reward Bench. - The model's performance further improves with increased inference compute, highlighting the potential of RL algorithms in optimizing MRMs. - The authors collected 200K preference data to facilitate MRM training, demonstrating the model's efficiency in utilizing data. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/yfzhang114/r1_reward) | N/A |
| [A Survey on Inference Engines for Large Language Models: Perspectives on
  Optimization and Efficiency](https://arxiv.org/abs/2505.01658) | Sungryeol Jeon, leejaymin, Devcow, oos2, inputsh | - This paper presents a comprehensive survey of 25 Large Language Model (LLM) inference engines, both open-source and commercial, focusing on optimization and efficiency. - The paper analyzes each engine based on ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation, categorizing optimization techniques such as batching, parallelism, compression, fine-tuning, caching, attention optimization, sampling optimization, and structured outputs. - It further explores the design goals and supported optimization techniques of each engine, examining ecosystem maturity for open-source solutions and performance/cost policies for commercial offerings. - The study also assesses recent trends like reasoning-centric test-time scaling and LLM-based AI agents which require multiple inference calls, therefore requiring higher inference efficiency. - Finally, it outlines future research directions including support for complex LLM-based services, diverse hardware support, and enhanced security. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/sihyeong/Awesome-LLM-Inference-Engine) | N/A |
| [Think on your Feet: Adaptive Thinking via Reinforcement Learning for
  Social Agents](https://arxiv.org/abs/2505.02156) | Xinghua Zhang, Haobo Wang, bingliwu, Yongbin-Li, iiiiwis | This paper introduces Adaptive Mode Learning (AML), a novel framework that dynamically adjusts reasoning depth in social agents using reinforcement learning.  AML strategically selects from four thinking modes (intuitive reaction to deep contemplation) based on real-time context. The core of AML is the Adaptive Mode Policy Optimization (AMPO) algorithm, which improves upon existing methods by incorporating multi-granular thinking modes and context-aware switching.  Experiments show that AML outperforms state-of-the-art methods by 15.6% on social intelligence tasks, achieving a 7% performance gain and 32.8% reduction in token usage compared to GRPO.  The results demonstrate AML's ability to enable more human-like adaptive reasoning. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/MozerWang/AMPO) | N/A |
| [Low-Precision Training of Large Language Models: Methods, Challenges,
  and Opportunities](https://arxiv.org/abs/2505.01043) | Li Shen, Guoxia, csdvT, GGJY, Zhiwei840 | This paper surveys existing low-precision training methods for large language models (LLMs), categorizing them by numerical format (fixed-point, floating-point, and custom).  It systematically organizes the approaches and discusses quantization-aware training.  The study highlights several promising research directions such as advanced quantization methods, ultra low-precision training, and fine-grained scaling strategies.  It also emphasizes the need for unified training frameworks, standardized benchmarks, and integration with other efficient training paradigms.  A collection of papers is provided in Awesome-Low-Precision-Training. | ['Natural Language Processing'] | [Link](https://github.com/Hao840/Awesome-Low-Precision-Training) | N/A |
| [Ming-Lite-Uni: Advancements in Unified Architecture for Natural
  Multimodal Interaction](https://arxiv.org/abs/2505.02471) | bear-xxy, jianxinsun, chenjingdong, zhengdd0422, BiaoGong | - Ming-Lite-Uni is an open-source multimodal framework with a unified visual generator and a native multimodal autoregressive model, integrating MetaQueries and M2-omni framework with multi-scale learnable tokens and a multi-scale representation alignment strategy. - It leverages a fixed MLLM and a learnable diffusion model, enabling both text-to-image generation and instruction-based image editing, outperforming closed-source models like GPT-40 and Gemini-1.5-Pro in multimodal understanding benchmarks. - The framework enhances visual generation through a FlowMatching loss in the diffusion model, enabling concurrent optimization with a frozen MLLM, leading to improvements in generation quality. - Ming-Lite-Uni uses a multi-scale representation alignment strategy which enhances high-res reconstruction quality and boosts GenEval by 1.5%. - A multimodal dataset was curated for Ming-Lite-Uni exhibiting robust control fluency and contextual understanding, addressing fine-grained image editing and text-to-image QA tasks through natural language. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/inclusionAI/Ming/tree/main/Ming-unify) | N/A |
| [TEMPURA: Temporal Event Masked Prediction and Understanding for
  Reasoning in Action](https://arxiv.org/abs/2505.01583) | vibhav-vineet, yilche, wchai, hsiangwei0903, andaba | - TEMPURA, a two-stage training framework, enhances video temporal understanding in large multimodal models (LMMs) by combining dense event segmentation with masked event prediction. - The model architecture involves a vision encoder and a large language model (LLM), trained first on masked event prediction for reasoning and then on video segmentation and dense captioning for fine-grained temporal grounding. - TEMPURA outperforms the baseline Qwen2.5-VL and other state-of-the-art models on Charades-STA (for temporal grounding) and QVHighlights (for highlight detection) benchmarks without requiring target-task fine-tuning. - The model achieves a 39.2 mIoU on Charades-STA, surpassing the baseline by 6.3 points, and a 51.7 HIT@1 score on QVHighlights, outperforming the baseline by 6.9 points. - A new large-scale dataset, VER, comprising 500K untrimmed videos with dense event captions and structured reasoning steps, is introduced to facilitate the training process. | ['Video-Text-to-Text', 'Multimodal', 'Computer Vision', 'Video Classification'] | N/A | N/A |
| [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive
  Streaming Speech Synthesis](https://arxiv.org/abs/2505.02625) | Yang Feng, Yan Zhou, zhangshaolei, guoshoutao, poeroz | - This paper introduces LLaMA-Omni 2, a series of speech language models (SpeechLMs) with sizes ranging from 0.5B to 14B parameters, designed for real-time spoken chatbot applications. - The model architecture consists of a Whisper encoder for speech understanding, a Qwen2.5 series LLM as the core, and an autoregressive streaming speech decoder with a text-to-speech language model and a flow matching model for speech generation. - Trained on 200K synthesized multi-turn speech dialogues, LLaMA-Omni 2 demonstrates superior performance in spoken question answering and speech instruction following tasks, outperforming existing SpeechLMs like GLM-4-Voice and LLaMA-Omni, even with significantly less training data (millions of hours vs. 200K samples). - The model achieves low latency (around 600ms) for real-time interaction and maintains high consistency between generated speech and text. - Ablation studies highlight the efficacy of various components, including the gate fusion module and the TTS pretraining strategy. | ['Multimodal', 'Text-to-Speech', 'Text-to-Audio', 'Automatic Speech Recognition', 'Audio-to-Audio', 'Text2Text Generation'] | [Link](https://github.com/ictnlp/LLaMA-Omni2) | [Link](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), [Link](https://huggingface.co/fishaudio/), [Link](https://huggingface.co/datasets/Stanford/web_questions), [Link](https://huggingface.co/datasets/google-research-datasets/LLaMA-Test-Set), [Link](https://huggingface.co/tatsu-lab/alpaca_eval) |
| [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and
  Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456) | Jie Peng, Peter Hase, mohitbansal, a2889184, vaidehi99 | - This paper introduces UNLOK-VQA, a novel benchmark designed for evaluating the targeted unlearning of sensitive information from Multimodal Large Language Models (MLLMs). - It proposes an "attack-and-defense" framework to assess the robustness of unlearning methods against various attack strategies, including white-box attacks leveraging model internals and black-box attacks employing input variations. - The dataset creation involves an automatic pipeline generating rephrase and neighborhood samples for nuanced evaluation of generalization and specificity, followed by manual filtering for quality control. - Experimental results on LLaVA-v1.5 demonstrate the effectiveness of multimodal extraction attacks and show that larger models exhibit greater resilience to attacks after unlearning. - The best defense mechanism, removing answer information from internal hidden states, significantly reduces attack success rates, establishing UNLOK-VQA as a valuable benchmark for future research in multimodal unlearning. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/Vaidehi99/UnLOK-VQA) | N/A |
