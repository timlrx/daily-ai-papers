

## Papers for 2025-05-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Unified Multimodal Understanding and Generation Models: Advances,
  Challenges, and Opportunities](https://arxiv.org/abs/2505.02567) | Minghao Fu, Jintao Guo, Xinjie Zhang, Flourish, Suikong |  - This paper surveys recent advancements in unified multimodal understanding and generation models, addressing the independent evolution of autoregressive and diffusion-based approaches.  - It categorizes existing unified models into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches.  - The survey analyzes the structural designs and innovations of each category, compiling datasets and benchmarks tailored for unified models.  - Key challenges in this field are discussed, including tokenization strategies, cross-modal attention, and data limitations.  - The paper aims to inspire further research and provide a valuable resource for the community by regularly updating the survey. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image'] | [Link](https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models) | N/A |
| [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588) | Yingyan Hou, Xuanbo Fan, Zile Qiao, Hao Sun, SpaceProduct | - This paper introduces ZEROSEARCH, a novel reinforcement learning framework that enhances the search capabilities of LLMs without needing to interact with real search engines. - ZEROSEARCH uses a lightweight supervised fine-tuning process to transform an LLM into a retrieval module, capable of producing both relevant and irrelevant documents to answer a query. - The framework employs a curriculum-based strategy that gradually decreases the quality of documents during the RL training, which helps to elicit the model's reasoning ability through increasingly challenging scenarios. - Experiments show that ZEROSEARCH significantly improves the search capabilities of LLMs, with a 7B retrieval module achieving performance comparable to a real search engine, and a 14B module even surpassing it. - The approach is shown to generalize effectively across various model sizes and RL algorithms, making it a scalable and versatile solution. | ['Reinforcement Learning', 'Question Answering'] | N/A | N/A |
| [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video
  Generation](https://arxiv.org/abs/2505.04512) | Yuan Zhou, Sen Liang, Zhengguang Zhou, Zhentao Yu, Teng Hu | - The paper introduces HunyuanCustom, a novel multi-modal customized video generation framework that prioritizes subject consistency. - The model architecture leverages LLaVA for enhanced multi-modal understanding, an image ID enhancement module for reinforcing identity features, and modality-specific condition injection mechanisms for audio and video. - HunyuanCustom outperforms state-of-the-art methods in ID consistency, realism, and text-video alignment across various downstream tasks, including audio and video-driven customized video generation. - The model's effectiveness is validated through extensive experiments on single- and multi-subject scenarios, demonstrating its robustness across diverse input modalities. - The code and models are publicly available, facilitating further research and development in controllable video generation. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://hunyuancustom.github.io) | N/A |
| [Beyond Recognition: Evaluating Visual Perspective Taking in Vision
  Language Models](https://arxiv.org/abs/2505.03821) | Maciej Wołczyk, Michał Nauman, Piotr Miłoś, Alicja Ziarko, Gracjan | - This paper introduces a novel benchmark for evaluating visual perspective-taking capabilities in vision language models (VLMs). - The benchmark consists of 144 unique visual tasks, each paired with seven diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. - The experimental results reveal that while state-of-the-art VLMs excel in scene understanding, their performance significantly declines on spatial reasoning and further deteriorates on perspective-taking tasks. - This indicates a substantial gap between surface-level object recognition and deeper spatial and perspective reasoning required for complex visual tasks. - The findings highlight the need for incorporating explicit geometric representations and tailored training protocols in future VLM development to improve their visual perspective-taking abilities. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal
  Problem-Solving](https://arxiv.org/abs/2505.04528) | Qinxiang Cao, Xingzhi Qi, Renqiu Xia, Xinhao Zheng, purewhite42 | This paper introduces a novel framework for formal problem-solving that leverages formal theorem proving environments. The framework, FPS (Formal Problem-Solving), is based on a deterministic Markov decision process and is proven to be sound and complete.  A deductive version, D-FPS, is also presented, improving human alignment by decoupling solving and verification.  Three benchmarks are constructed to evaluate the framework, and results show that FPS and D-FPS outperform existing methods.  A new symbolic approach, RPE (Restricted Propositional Equivalence), is proposed to ensure that answers are faithfully and interpretably aligned with human intuition. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Purewhite2019/formal_problem_solving_main) | N/A |
| [OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue
  Resolution](https://arxiv.org/abs/2505.04606) | Jiachi Chen, Yanlin Wang, Runhan Jiang, Lianghong Guo, itaowe | OmniGIRL is a novel multilingual and multimodal benchmark designed for evaluating the effectiveness of Large Language Models (LLMs) in resolving GitHub issues.  It addresses the limitations of existing benchmarks by incorporating issues from four programming languages, spanning eight domains, and including multimodal information (text, images, and web links).  The benchmark consists of 959 instances and demonstrates the challenges LLMs face in this complex task, with the best-performing model achieving a resolution rate of only 8.6%.  Furthermore, the analysis reveals that current LLMs struggle with issues that involve understanding images and resolving issues requiring multi-file modifications. | ['Multimodal'] | [Link](https://github.com/DeepSoftwareAnalytics/OmniGIRL) | N/A |
| [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570) | Sinéad Ryan, Arturo Márquez Flores, Patrick Barker, Daniel Jeffries, mariya-davydova | - This paper introduces OSUniverse, a benchmark designed for evaluating the performance of multimodal GUI-navigation AI agents. - OSUniverse is comprised of complex tasks that increase in difficulty, ranging from basic clicking to multi-step actions across multiple applications, requiring advanced reasoning and dexterity. - The benchmark's design prioritizes ease of use, extensibility, thorough test case coverage, and automated validation, using Gemini models for automatic scoring with an average error rate below 2%. - The benchmark is designed to assess agent capabilities across different levels of complexity, ensuring that state-of-the-art agents achieve scores below 50% while average office workers demonstrate perfect accuracy. - The authors provide a detailed architecture of OSUniverse, including its test cases, checks, runners, and validators, highlighting its open-ended nature in terms of supported agents and models. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/agentsea/osuniverse) | N/A |
| [Knowledge Augmented Complex Problem Solving with Large Language Models:
  A Survey](https://arxiv.org/abs/2505.03418) | Yuqi Zhu, Yuchen Tian, Junwei Su, Lun Du, Da Zheng | This paper surveys the use of large language models (LLMs) for complex problem-solving, focusing on the capabilities and limitations of LLMs in various domains.  The paper explores techniques such as chain-of-thought reasoning and knowledge augmentation.  Three key components of complex problem-solving are identified: multi-step reasoning, domain knowledge, and result verification. The fundamental limitations of current LLM solutions are analyzed, and future research directions are discussed.  The study covers various domains including software engineering, mathematics, data science, and scientific research, highlighting their specific challenges.  Finally, the work suggests approaches for enhancing LLM capabilities in addressing complex real-world problems. | ['Natural Language Processing'] | N/A | N/A |
| [R&B: Domain Regrouping and Data Mixture Balancing for Efficient
  Foundation Model Training](https://arxiv.org/abs/2505.00358) | Ziyi Chu, Avi Trost, John Cooper, Tzu-Heng Huang, Albert Ge | - This paper introduces R&B, a novel framework for efficient foundation model training that addresses the limitations of existing data mixing strategies by re-partitioning training data based on semantic similarity and dynamically optimizing data composition using domain gradients. - R&B achieves significant computational efficiency compared to existing methods, reducing compute overhead by orders of magnitude while matching or exceeding performance on diverse datasets. - The effectiveness of R&B is validated through theoretical analysis under standard regularity conditions and empirical evaluations on five datasets, encompassing natural language, instruction-following, reasoning, and multimodal tasks. - The proposed framework's ability to dynamically optimize data mixtures throughout training is shown to be crucial for achieving high performance and efficiency. - R&B's success is attributed to its ability to capture fine-grained semantic nuances and efficiently leverage information already computed during standard training, thereby avoiding the need for additional compute-intensive evaluations. | ['Natural Language Processing'] | N/A | N/A |
