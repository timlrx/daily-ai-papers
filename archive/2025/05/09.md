

## Papers for 2025-05-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Perception, Reason, Think, and Plan: A Survey on Large Multimodal
  Reasoning Models](https://arxiv.org/abs/2505.04921) | imryanxu, xyidealist, TerenceL-TL, foggyforest, YunxinLi |  - This paper provides a comprehensive survey of large multimodal reasoning models (LMRMs).  - It introduces a three-stage roadmap for the development of LMRMs, from modular reasoning to multimodal chain-of-thought (MCoT), and finally to long-horizon reasoning.  - The survey also introduces and analyzes Native Large Multimodal Reasoning Models (N-LMRMs), a forward-looking paradigm where reasoning natively emerges from omnimodal perception and interaction.  - Existing datasets and benchmarks are reorganized to clarify their categories and evaluation dimensions.  - The paper identifies limitations of current LMRMs and outlines research directions to advance the field. | ['Multimodal'] | [Link](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models) | N/A |
| [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in
  Large Language Models](https://arxiv.org/abs/2505.02847) | Peisong Wang, Qingxuan Jiang, Bang Zhang, zptu, vvibt | - This paper introduces Sentient Agent as a Judge (SAGE), a novel automated evaluation framework for assessing higher-order social cognition in LLMs. - SAGE uses a Sentient Agent that simulates human-like emotional changes and inner thoughts to provide more realistic evaluation during multi-turn conversations. - Experiments on 100 supportive-dialogue scenarios demonstrate that the Sentient emotion score correlates strongly with established human-centric instruments, validating psychological fidelity. - A public Sentient Leaderboard is created, revealing substantial gaps between frontier systems and earlier baselines, which are not reflected in conventional leaderboards. - SAGE provides a principled, scalable, and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents. | ['Natural Language Processing'] | [Link](https://github.com/tencent/digitalhuman/SAGE) | N/A |
| [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315) | cxiong, JunnanLi, doyensahoo, hendrydong, yuhuixu | - The paper introduces Elastic Reasoning, a novel framework that enhances the scalability of chain-of-thought (CoT) reasoning in large language models (LLMs) by separating the reasoning process into two phases: thinking and solution, each with independent budget allocations. - The proposed framework addresses the challenge of uncontrolled output lengths in CoT reasoning, which often hinders real-world deployment due to constraints on inference-time budgets. - A lightweight budget-constrained rollout strategy is introduced to improve the model's robustness to truncated thinking, enabling it to generalize effectively to unseen budget constraints without additional training. - Empirical results on mathematical and programming benchmarks demonstrate that Elastic Reasoning significantly improves reliability under tight resource constraints and produces more concise and efficient reasoning than baseline methods. - The method outperforms existing approaches like Long2Short and length control methods in terms of both accuracy and efficiency, particularly under strict resource constraints. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](null) | [Link](null) |
| [ICon: In-Context Contribution for Automatic Data Selection](https://arxiv.org/abs/2505.05327) | Zhifang Sui, soliz1998, yaolily, Rsy24, yyxsghx | - This paper introduces ICON, a novel gradient-free method for automatic data selection in instruction tuning that leverages in-context learning (ICL). - ICON measures sample contribution without gradient computation or manual indicators, offering a computationally efficient alternative to existing methods. - Experiments on three LLMs across 12 benchmarks demonstrate ICON's effectiveness, with models trained on 15% of ICON-selected data outperforming full datasets. - ICON-selected high-contribution samples exhibit diverse tasks and appropriate difficulty levels. - The ICON-guided selection paradigm enables scalable inference through linear-complexity calls. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://annayang2020.github.io/ICon_Data_Selection/) | N/A |
| [X-Reasoner: Towards Generalizable Reasoning Across Modalities and
  Domains](https://arxiv.org/abs/2505.03981) | RustyArchimedes, sidkiblawi, hiaoxui, shengz, qianchu | - This paper introduces X-REASONER, a vision-language model that achieves strong generalizable reasoning capabilities across modalities and domains through general-domain text-based post-training. - The model is trained using a two-stage approach: supervised fine-tuning with distilled long chain-of-thoughts followed by reinforcement learning with verifiable rewards. - Experiments demonstrate that X-REASONER outperforms existing state-of-the-art models on various general and medical benchmarks, showcasing its ability to transfer reasoning capabilities to both multimodal and out-of-domain settings. - A medical-specialized variant, X-REASONER-MED, is also introduced, achieving new state-of-the-art results on numerous medical benchmarks. - The findings support the hypothesis that reasoning is generalizable across modalities and domains, and that general-domain text-based post-training can effectively enable strong generalizable reasoning. | ['Multimodal'] | [Link](https://github.com/microsoft/x-reasoner) | N/A |
| [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language
  Models in Chinese](https://arxiv.org/abs/2504.19314) | Bruce Leon, HawkFaust, yeeeqichen99, MindYing, PALIN2018 | - This paper introduces BrowseComp-ZH, a new benchmark designed to evaluate the web browsing capabilities of Large Language Models (LLMs) specifically in Chinese. - The benchmark consists of 289 multi-hop questions covering 11 diverse domains, each reverse-engineered from a short, verifiable answer. - The dataset underwent a two-stage quality control process to ensure question difficulty and answer uniqueness. - Evaluation of over 20 state-of-the-art LLMs and search systems revealed that most models struggle, with accuracy rates below 10% for many. - The best-performing system achieved only 42.9% accuracy, highlighting the challenge of web browsing for current LLMs. | ['Question Answering'] | [Link](https://github.com/PALIN2018/BrowseComp-ZH) | N/A |
| [Chain-of-Thought Tokens are Computer Program Variables](https://arxiv.org/abs/2505.04955) | Zhifang Sui, peiyiwang89, soliz1998 | - This paper explores the role of chain-of-thought (CoT) tokens in large language models (LLMs), proposing the hypothesis that they function like computer program variables. - The authors conduct empirical studies on multi-digit multiplication and dynamic programming tasks, demonstrating that CoT tokens primarily store intermediate results. - Their experiments reveal that models can maintain comparable performance even when non-result tokens are removed or intermediate results are represented in latent forms. - Randomly intervening values in CoT tokens causes corresponding changes in subsequent tokens and the final answer, further supporting the variable-like behavior of CoT tokens. - The findings suggest a potential limit to computation complexity between CoT tokens, influencing model performance on complex tasks. | ['Natural Language Processing'] | [Link](https://github.com/solitaryzero/CoTs_are_Variables) | N/A |
