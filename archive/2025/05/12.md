

## Papers for 2025-05-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Bielik v3 Small: Technical Report](https://arxiv.org/abs/2505.02550) | Adrian Gwoździej, Łukasz Flis, djstrong, Remek, chrisociepa | This paper introduces Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B parameters) optimized for Polish language processing.  The models utilize a custom Polish tokenizer (APT4) and Adaptive Learning Rate for improved efficiency.  Bielik v3 models achieve performance comparable to much larger models on various benchmarks including the Open PL LLM Leaderboard and the Complex Polish Text Understanding Benchmark.  This demonstrates the effectiveness of the proposed techniques for resource-efficient language modeling in less-resourced languages. The models were trained on a large, high-quality Polish corpus of 292 billion tokens and achieve state-of-the-art results on several Polish NLP benchmarks. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/SpeakLeash) |
| [Bielik 11B v2 Technical Report](https://arxiv.org/abs/2505.02410) | Adrian Gwoździej, Łukasz Flis, Remek, djstrong, chrisociepa |  - This paper introduces Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing, built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling. - Two key technical innovations are introduced: Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, enhancing performance and efficiency. - Bielik 11B v2 outperforms many larger models across Polish language benchmarks while maintaining cross-lingual capabilities. - The model demonstrates parameter efficiency and offers various quantization options enabling deployment on diverse hardware configurations. - Extensive evaluation across multiple benchmarks establishes new benchmarks for resource-efficient language modeling in under-resourced languages. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/spaces/speakleash/open_pl_llm_leaderboard), [Link](https://huggingface.co/spaces/speakleash/cptu_bench), [Link](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard) |
| [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/abs/2505.05026) | Yejin Choi, Sumin Shim, Min Soo Kim, Jang Han Yoon, jeochris | - This paper introduces WISERUI-BENCH, a benchmark dataset for pairwise UI design persuasiveness assessment, comprising 300 real-world UI image pairs labeled with A/B test results and expert rationales. - It proposes G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by mitigating position bias and improving accuracy. - G-FOCUS surpasses existing inference strategies in terms of consistency and accuracy for pairwise UI evaluation, as demonstrated by experimental results. - The study uses various VLMs (e.g., GPT-40, Claude 3.5, LLaMA 3.2) for evaluation and analysis. - The contributions include a new benchmark dataset and a novel inference-time reasoning strategy for VLM-based UI design persuasiveness assessment. | ['Multimodal', 'Image-to-Text', 'Image-to-Image', 'Visual Question Answering', 'Natural Language Processing'] | N/A | N/A |
| [Sailing AI by the Stars: A Survey of Learning from Rewards in
  Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686) | Xiaobao Wu |  - This survey paper provides a comprehensive overview of the learning from rewards paradigm in post-training and test-time scaling of large language models (LLMs). - The authors categorize and analyze the learning from rewards strategies across different stages: training, inference, and post-inference, and discuss the applications and benchmarks for reward models. - The unified conceptual framework of learning from rewards for LLMs is introduced, abstracting the key components and interactions involved. - The paper explores the taxonomy of learning from rewards for LLMs, categorizing existing works with a unified conceptual framework regarding reward model design and learning strategies. - Finally, the authors highlight the challenges and promising future directions in this field. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Multimodal'] | [Link](https://github.com/bobxwu/learning-from-rewards-llm-papers) | N/A |
