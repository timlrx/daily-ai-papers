

## Papers for 2025-05-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062) | kuma-zhao, yuanlp, 0nejiawei, chb1997, anyuzx | This paper introduces Seed1.5-VL, a vision-language multimodal model with a 532M parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) LLM.  Seed1.5-VL achieves state-of-the-art performance on 38 out of 60 public benchmarks. The model demonstrates strong reasoning abilities in various tasks like GUI control and visual puzzles, surpassing existing multimodal models.  Its development involved novel data synthesis strategies and hybrid parallelism training techniques.  The model is now accessible through Volcano Engine. | ['Multimodal'] | N/A | N/A |
| [MiMo: Unlocking the Reasoning Potential of Language Model -- From
  Pretraining to Posttraining](https://arxiv.org/abs/2505.07608) | whatseeker, Prestonprom, HugoZHL, dwzhu, xiabingquan | - MiMo-7B is a large language model designed for reasoning tasks, optimized across pre-training and post-training stages. - The model architecture uses a decoder-only Transformer with Grouped-Query Attention, SwiGLU activation, and Rotary Positional Embedding, incorporating Multi-Token Prediction for enhanced performance and speed. - Post-training involves reinforcement learning on a dataset of 130K verifiable math and programming problems, using a test-difficulty-driven code-reward scheme and data resampling. - MiMo-7B-RL outperforms larger 32B models on mathematics, code, and general reasoning benchmarks, surpassing OpenAI 01-mini. - Model checkpoints are available at the provided GitHub URL. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/xiaomimimo/MiMo) | [Link](null) |
| [Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured
  3D Assets](https://arxiv.org/abs/2505.07747) | PanJianxiong, flybirdtian, shian7, wchengad, xuanyangz | - This paper introduces Step1X-3D, an open-source framework for high-fidelity and controllable textured 3D asset generation. - Step1X-3D employs a two-stage architecture: a hybrid VAE-DiT geometry generator and a diffusion-based texture synthesis module.  The geometry generator uses a perceiver-based latent encoding with sharp edge sampling, while the texture module ensures cross-view consistency through geometric conditioning and latent-space synchronization. - The framework bridges 2D and 3D generation paradigms, allowing for the direct transfer of 2D control techniques (e.g., LoRA) to 3D synthesis. - Benchmark results show that Step1X-3D outperforms existing open-source methods and achieves competitive quality with proprietary solutions. - Step1X-3D's open-source release includes models, training code, and adaptation modules, promoting reproducibility and further research in 3D asset generation. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://github.com/stepfun-ai/Step1X-3D) | [Link](https://huggingface.co/docs/diffusers/index) |
| [Learning from Peers in Reasoning Models](https://arxiv.org/abs/2505.07787) | Benyou, tangzhy, Jiaxi0775, wydu, Zeno-Luo | This paper introduces LeaP, a novel approach to enhance the reasoning capabilities of Large Reasoning Models (LRMs). LeaP addresses the "Prefix Dominance Trap," where LRMs struggle to recover from poor initial reasoning paths, by enabling cross-path interaction during parallel inference.  Experimental results across several benchmarks demonstrate substantial improvements, with QwQ-32B with LeaP achieving nearly 5 absolute points higher than the baseline.  A fine-tuned version, LeaP-T, further enhances the performance of smaller models.  LeaP's robust error correction and strong handling of varied task difficulty showcase the effectiveness of collaboration during reasoning. | ['Question Answering'] | [Link](https://learning-from-peers.github.io/) | N/A |
| [REFINE-AF: A Task-Agnostic Framework to Align Language Models via
  Self-Generated Instructions using Reinforcement Learning from Automated
  Feedback](https://arxiv.org/abs/2505.06548) | Pawan Goyal, Somak Aditya, Aniruddha Roy, abhi1nandy2, Pretam | - This paper introduces REFINE-AF, a task-agnostic framework for aligning language models using self-generated instructions and reinforcement learning from automated feedback. - REFINE-AF utilizes small, open-source LLMs (LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B) to generate instructions, reducing the need for expensive and time-consuming human annotation. - The framework incorporates reinforcement learning to improve the quality of generated instruction-input-output triplets. - Experiments demonstrate that REFINE-AF achieves significant improvements (63-66%) across various tasks compared to previous approaches. - A large synthetic dataset of 45K instructions generated by REFINE-AF is released to facilitate further research. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/abs/2505.07818) | appleluo, ChenMnZ, ltzhu, wujie10, xzyhku | - DanceGRPO is a novel unified framework that adapts Group Relative Policy Optimization (GRPO) to visual generation tasks, achieving seamless integration across various generative paradigms and tasks. - The framework handles diffusion models and rectified flows, supporting text-to-image, text-to-video, and image-to-video generation tasks, demonstrating consistent and substantial improvements over existing methods. - DanceGRPO exhibits significant performance gains, outperforming baselines by up to 181% on benchmark datasets, showcasing effectiveness and scalability. - This unified approach addresses critical limitations of existing RL-based methods, including incompatibility with ODE-based sampling, instability in large-scale training, and lack of validation for video generation. - The code will be released, fostering further research and development in reinforcement learning for visual synthesis. | ['Reinforcement Learning', 'Text-to-Image', 'Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://dancegrpo.github.io/) | N/A |
| [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong
  Pretraining Data Selection](https://arxiv.org/abs/2505.07293) | Steven Wu, Kai Hua, shenke18, zhangysk |  - AttentionInfluence is proposed as a novel, training-free method for pretraining data selection that leverages the intrinsic attention head mechanisms of LLMs.   -  The approach identifies retrieval heads and computes loss differences when masking these heads, using a small pretrained language model as a data selector.   - The method demonstrates a weak-to-strong scaling property, meaning small models can significantly improve the performance of larger models.   - Experimental results on multiple benchmarks show improvements ranging from 1.4pp to 3.5pp, showcasing AttentionInfluence's effectiveness.   - The study highlights AttentionInfluence's ability to select high-quality, well-distributed data that enhances the reasoning capabilities of downstream models. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus) |
| [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional
  Websites from Scratch](https://arxiv.org/abs/2505.03733) | shiwk20, hht1113, Houxing, Yqy6, luzimu | This paper introduces WebGen-Bench, a new benchmark designed to evaluate Large Language Models' (LLMs) ability to generate functional websites from scratch.  WebGen-Bench includes diverse website generation instructions and corresponding test cases for thorough evaluation.  Experiments using multiple code-agent frameworks showed that the best-performing model achieves only 27.8% accuracy, highlighting the benchmark's difficulty.  A training set, WebGen-Instruct, was also created and used to fine-tune a model which reached 38.2% accuracy, surpassing existing proprietary models. The datasets and code are publicly available. | ['Text Generation', 'Text2Text Generation'] | [Link](https://github.com/mnluzimu/WebGen-Bench) | N/A |
| [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796) | Daniel Dajun Zeng, Lu Wang, Xingjin Wang, linjinglian, Howe77 | *- This paper introduces a novel CPT scaling law that accurately predicts loss at any training step in continual pre-training, considering both distribution shift and learning rate annealing. - The scaling law integrates various factors impacting CPT performance, including loss potential, peak learning rate, training steps, and replay ratio. - Extensive experiments demonstrate the law's accuracy across different datasets, training hyperparameters, and model sizes. - The scaling law helps optimize hyperparameters for diverse CPT goals, such as balancing general and domain-specific performance. - The study provides deeper insights into the learning dynamics of LLMs, facilitating better adaptation to downstream tasks. | ['Natural Language Processing'] | N/A | N/A |
| [Skywork-VL Reward: An Effective Reward Model for Multimodal
  Understanding and Reasoning](https://arxiv.org/abs/2505.07263) | Yi Peng, Wei Shen, Jiangbo Pei, OrlandoHugBot, shawn0wang | - This paper introduces Skywork-VL Reward, a novel multimodal reward model designed for both multimodal understanding and reasoning tasks. - The model architecture is based on Qwen2.5-VL-7B-Instruct, incorporating a reward head and utilizing a multi-stage fine-tuning process with pairwise ranking loss. - Experimental results demonstrate that Skywork-VL Reward achieves state-of-the-art performance on the VL-RewardBench benchmark and shows competitive results on RewardBench. - The preference data generated by Skywork-VL Reward is highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. - The model has been publicly released to ensure transparency and reproducibility. | ['Multimodal'] | N/A | [Link](https://huggingface.co/Skywork/Skywork-VL-Reward-7B) |
| [Reinforced Internal-External Knowledge Synergistic Reasoning for
  Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596) | Kang Liu, Jun Zhao, Yiming Ju, Xiaowei Yuan, hzy | - This paper introduces IKEA, a novel reinforcement learning-based adaptive search agent designed to synergistically integrate internal and external knowledge for efficient question answering. - IKEA utilizes a knowledge-boundary aware reward function and training dataset to incentivize the model to prioritize internal knowledge and only resort to external retrieval when necessary, thereby reducing retrieval redundancy and latency. - The proposed agent demonstrates robust generalization capabilities and significantly outperforms baseline methods across multiple knowledge-intensive reasoning tasks. - Evaluations show that IKEA effectively minimizes unnecessary retrievals while maintaining high accuracy compared to existing search agents. - The authors demonstrate the effectiveness of their approach through comprehensive evaluations on multiple datasets, showcasing improved performance and reduced retrieval frequency. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/hzy312/knowledge-r1) | N/A |
| [Overflow Prevention Enhances Long-Context Recurrent LLMs](https://arxiv.org/abs/2505.07793) | rgiryes, leokarlin, OmegaLittleBob, ItamarZ, assafbk | The paper introduces OPRM, a chunk-based inference strategy for enhancing long-context recurrent LLMs.  OPRM mitigates memory overflow issues by processing only the most relevant portion of the input, improving performance on several long-context tasks.  The method achieves state-of-the-art results on LongBench v2, surpassing comparable Transformer models.  Results are shown across multiple benchmarks and demonstrate consistent improvements in long-context tasks.  The work also challenges the assumption that recurrent models genuinely utilize long-range dependencies. | ['Natural Language Processing'] | [Link](https://github.com/assafbk/OPRM) | [Link](https://huggingface.co/tiiuae/falcon-mamba-7b-instruct), [Link](https://huggingface.co/tiiuae/Falcon3-Mamba-7B-Instruct), [Link](https://huggingface.co/google/recurrentgemma-9b-it), [Link](https://huggingface.co/RWKV/v6-Finch-7B-HF), [Link](https://huggingface.co/state-spaces/mamba-1.4b), [Link](https://huggingface.co/state-spaces/mamba-130m), [Link](https://huggingface.co/assafbk/decimamba-130m-niah), [Link](https://huggingface.co/assafbk/mamba-130m-niah) |
| [UMoE: Unifying Attention and FFN with Shared Experts](https://arxiv.org/abs/2505.07260) | Jing Li, Chaozheng Wang, ysngkil | - This paper introduces UMoE, a novel unified Mixture-of-Experts (MoE) architecture that combines both attention and feed-forward network (FFN) layers with shared experts. - UMoE achieves this unification by reformulating the attention mechanism to reveal an underlying FFN-like structure, enabling efficient parameter sharing between the two components. - The model demonstrates superior performance compared to existing attention-based and FFN-based MoE approaches, achieving this result in both language modeling pretraining and zero-shot evaluation tasks. - UMoE's effectiveness stems from its unified design and efficient parameter sharing, which allows for improved performance with the same number of parameters. - The authors conduct extensive experiments on large-scale language models across various datasets and tasks, showing consistent superiority in performance and efficiency. | ['Natural Language Processing'] | [Link](https://github.com/ysngki/UMoE) | N/A |
| [Document Attribution: Examining Citation Relationships using Large
  Language Models](https://arxiv.org/abs/2505.06324) | Nedim Lipka, Vipula Rawte, Franck-Dernoncourt, ryanrossi | - This paper introduces two novel techniques for addressing the challenge of attribution in large language models (LLMs), focusing on tracing generated outputs back to their source documents. - The first technique is a zero-shot approach that frames attribution as a textual entailment task, demonstrating improvements of 0.27% and 2.4% over baseline methods on the AttributionBench dataset for in-distribution and out-of-distribution sets respectively. - The second technique explores the role of the attention mechanism in improving attribution accuracy, showing that a smaller LLM outperforms the baseline across multiple layers. - Experiments were conducted using AttributionBench, evaluating performance using F1 score, false positives, and false negatives. - The findings highlight the effectiveness of both zero-shot textual entailment and attention mechanisms for enhancing LLM attribution, advancing the interpretability and reliability of these models. | ['Natural Language Processing', 'Question Answering', 'Zero-Shot Classification'] | [Link](null) | [Link](null) |
