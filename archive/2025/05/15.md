

## Papers for 2025-05-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,
  Training and Dataset](https://arxiv.org/abs/2505.09568) | Zhiyang Xu, Jiuhai Chen, xurantju, zhoutianyi, xcpan | - This paper introduces BLIP3-0, a family of fully open unified multimodal models that excel in both image understanding and generation tasks. - The model architecture employs a diffusion transformer to generate semantically rich CLIP image features, enhancing training efficiency and improving generation quality. - BLIP3-0 utilizes a sequential pretraining strategy, initially focusing on image understanding and subsequently on image generation, effectively preserving image understanding capabilities while developing robust image generation abilities. - A high-quality instruction-tuning dataset, BLIP3-0-60k, is introduced, which is carefully curated using GPT-4 to enhance model alignment with human preferences. - BLIP3-0 outperforms existing state-of-the-art unified multimodal models across various benchmark tasks, demonstrating its superior performance in both image understanding and generation. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://github.com/JiuhaiChen/BLIP30) | [Link](https://huggingface.co/BLIP30/BLIP30-Model), [Link](https://huggingface.co/datasets/BLIP30/BLIP30-Pretrain), [Link](https://huggingface.co/datasets/BLIP30/BLIP30-60k) |
| [Insights into DeepSeek-V3: Scaling Challenges and Reflections on
  Hardware for AI Architectures](https://arxiv.org/abs/2505.09343) | Huazuo Gao, Damai Dai, Chong Ruan, Chengqi Deng, Chenggang Zhao | - The paper presents DeepSeek-V3, a large language model (LLM) trained on 2048 NVIDIA H800 GPUs, showcasing cost-efficient training and inference at scale through hardware-aware model co-design. - Key architectural innovations include Multi-head Latent Attention (MLA) for memory efficiency, Mixture of Experts (MoE) for optimized computation-communication trade-offs, and FP8 mixed-precision training to leverage hardware capabilities. - DeepSeek-V3 achieves state-of-the-art performance while using significantly fewer computational resources than comparable dense models, demonstrating the efficacy of MoE architectures in cost-effective training. - The paper analyzes hardware bottlenecks encountered during DeepSeek-V3's development and proposes potential future hardware directions, such as precise low-precision computation units and innovations in low-latency communication fabrics. - This work offers a practical blueprint for innovation in next-generation AI systems by highlighting the critical role of hardware and model co-design in meeting the escalating demands of AI workloads. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/deepseek-ai/3FS), [Link](https://github.com/deepseek-ai/dualpipe), [Link](https://github.com/deepseek-ai/DeepGEMM), [Link](https://github.com/deepseek-ai/DeepEP), [Link](https://github.com/deepseek-ai/profile-data) | N/A |
| [SweRank: Software Issue Localization with Code Ranking](https://arxiv.org/abs/2505.07849) | Xuan Phi Nguyen, Ye Liu, JaeHyeok Doo, Tarun Suresh, Revanth Gangi Reddy | - This paper introduces SWERANK, a novel two-stage retrieve-and-rerank framework for software issue localization that significantly outperforms existing agent-based and code-ranking methods. - SWERANK consists of a bi-encoder embedding model (SWERANKEMBED) for retrieval and an instruction-tuned LLM (SWERANKLLM) for reranking. - The model achieves state-of-the-art performance on SWE-Bench-Lite and LocBench datasets, demonstrating its effectiveness in localizing code relevant to issue descriptions. - To train SWERANK, the authors created SWELOC, a new large-scale dataset curated from public GitHub repositories, which contains real-world issue descriptions paired with the corresponding code modifications. - SWERANK shows a considerably better performance-to-cost ratio compared to agent-based approaches that rely on closed-source LLMs. | ['Natural Language Processing'] | [Link](https://gangiswag.github.io/swerank) | N/A |
| [VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large
  Video Language Models](https://arxiv.org/abs/2505.08455) | Ali Etemad, pritamqu | - This paper introduces VCRBench, a novel benchmark designed to evaluate the long-form causal reasoning capabilities of Large Video Language Models (LVLMs). - VCRBench uses procedural videos of simple everyday activities, where the steps are deliberately shuffled, to test if LVLMs can correctly sequence events to achieve a specific goal. - The evaluation of state-of-the-art LVLMs on VCRBench shows that these models struggle with video-based long-form causal reasoning due to difficulty in modeling long-range dependencies. - To improve LVLMs' performance, the authors propose a Recognition-Reasoning Decomposition (RRD) approach that separates video recognition and causal reasoning, enhancing accuracy by up to 25.2%. - The thorough analysis of the results reveals interesting insights into the reasoning capabilities of LVLMs, indicating that they primarily rely on language knowledge when tackling complex video-based long-form causal reasoning tasks. | ['Video Classification', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?](https://arxiv.org/abs/2505.09439) | Hilde Kuehne, Samuel Thomas, Edson Araujo, Saurabhchand Bhati, h9LtLSb | - The paper introduces Omni-R1, a new state-of-the-art model for audio question answering, which fine-tunes the Qwen2.5-Omni multi-modal LLM using the reinforcement learning method GRPO. - Omni-R1 achieves the highest accuracy on the MMAU benchmark across various categories, outperforming existing methods. - The authors propose a method for automatically generating audio question answering datasets, further improving the model's performance. - A key finding is that text-only fine-tuning can significantly improve audio-based performance, suggesting that enhanced text reasoning contributes substantially to the model's success. - The code, models, and datasets will be publicly released. | ['Audio Classification', 'Question Answering', 'Reinforcement Learning'] | N/A | N/A |
