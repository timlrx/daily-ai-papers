

## Papers for 2025-05-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large
  Reasoning Models](https://arxiv.org/abs/2505.10554) | cxiong, amritasaha87, yuhuixu, hendrydong, zhiyuanhucs | - This paper introduces a novel approach to enhance the reasoning capabilities of large reasoning models (LRMs) by explicitly aligning them with three meta-abilities: deduction, induction, and abduction. -  The proposed method uses automatically generated, self-verifiable tasks to train the models on each meta-ability individually, followed by merging the parameter spaces of the trained models and further fine-tuning with domain-specific reinforcement learning. - Experiments demonstrate that this three-stage pipeline (individual alignment, parameter-space merging, and domain-specific reinforcement learning) improves performance by over 10% compared to instruction-tuned baselines. - The study also shows that resuming domain-specific reinforcement learning from the aligned checkpoint yields an additional 2% average gain in performance across various benchmarks, highlighting the scalability and dependability of the proposed method. - The authors' code is publicly available on GitHub, making their approach reproducible and facilitating further research in the field. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/zhiyuanhubj/Meta-Ability-Alignment) | N/A |
| [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666) | Sung Ju Hwang, jinheon, YuminChoi | This paper introduces a novel bilevel system prompt optimization problem for Large Language Models (LLMs).  The proposed MetaSPO framework meta-learns a robust system prompt that generalizes effectively to unseen tasks and diverse user prompts. Experiments on fourteen datasets across five domains demonstrate MetaSPO's superior performance compared to baseline methods in both unseen generalization and test-time adaptation scenarios. The optimized system prompts enable rapid adaptation to unseen tasks, requiring fewer optimization steps while achieving improved performance.  MetaSPO's iterative optimization process ensures synergy between system and user prompts.  The framework is flexible and compatible with various prompt optimization techniques. | ['Natural Language Processing'] | [Link](https://github.com/Dozi01/MetaSPO) | N/A |
| [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a
  Reasoning Model will Think](https://arxiv.org/abs/2505.10185) | hbin0701, dreamgonfly, Minju2136, seungone, Seongyun | The CoT Encyclopedia is a novel bottom-up framework for analyzing and controlling long chain-of-thought (CoT) reasoning in large language models.  It automatically extracts diverse reasoning criteria from model-generated CoTs and clusters them into representative categories. Human evaluations demonstrate that the framework is more interpretable than existing methods and enables performance gains.  The study reveals that training data format has a greater impact on reasoning behavior than data domain. Finally, it provides insights into the controllability of reasoning strategies via model merging. | ['Natural Language Processing'] | N/A | N/A |
| [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562) | RobertLuo1, Paranioar, YufengCui, ryanzhangfan, gilnore | - This paper introduces ETT, an end-to-end vision tokenizer tuning approach that jointly optimizes vision tokenization and target autoregressive tasks.  Unlike previous methods that use only discrete indices from a frozen vision tokenizer, ETT leverages visual embeddings and optimizes the vision tokenizer end-to-end.  - ETT significantly improves performance on multimodal understanding and visual generation tasks (2-6% gains compared to baselines) while maintaining original reconstruction capabilities.  - The method is simple to implement and integrate without requiring adjustments to the original codebooks or architectures of large language models.  - Extensive experiments demonstrate the effectiveness of ETT across various benchmarks, showcasing consistent performance improvements over existing state-of-the-art approaches.  - This method empowers multimodal foundation models beyond image generation and understanding. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | N/A | N/A |
| [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine
  Learning Engineering](https://arxiv.org/abs/2505.07782) | percyliang, Solute, yinghaoli-yh, yczhuang, Jerrycool | This paper introduces MLE-Dojo, a new benchmark and environment designed to evaluate and improve large language models (LLMs) for machine learning engineering (MLE) tasks.  The benchmark features 200+ real-world Kaggle challenges covering diverse MLE tasks.  MLE-Dojo provides an interactive environment, enabling agents to experiment and refine solutions iteratively.  Extensive evaluations across eight frontier LLMs demonstrate that current models show limitations, but iterative improvements are meaningful.  The framework's modular and extensible design promotes reproducibility and fosters community-driven innovation. | ['Reinforcement Learning', 'Tabular', 'Tabular Classification', 'Tabular Regression', 'Time Series Forecasting', 'Natural Language Processing', 'Text Classification', 'Computer Vision', 'Image Classification', 'Image Segmentation'] | [Link](https://github.com/MLE-Dojo/MLE-Dojo) | [Link](https://huggingface.co/spaces/MLE-Dojo/Leaderboard) |
| [Achieving Tokenizer Flexibility in Language Models through Heuristic
  Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738) | Vinayak Pahalwan, Shaurya Sharthak, adarshxs, adi-kmt | - This paper introduces TokenAdapt, a novel framework for achieving tokenizer flexibility in large language models (LLMs). - TokenAdapt employs a hybrid heuristic initialization strategy that combines local and global estimates to effectively transplant tokenizers. - The framework also introduces supertoken learning to improve compression and reduce fragmentation. - Empirical results demonstrate that TokenAdapt consistently outperforms existing methods such as ReTok and TransTokenizer, achieving up to a 2-fold improvement in perplexity. - The authors conclude that TokenAdapt offers a practical and computationally efficient method for adapting LLMs to new tokenization schemes. | ['Natural Language Processing'] | [Link](None) | [Link](None) |
| [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320) | Xian Li, Ping Yu, Tianlu Wang, Chenxi Whitehouse, swarna92 | - This paper introduces J1, a novel reinforcement learning approach for training LLMs to perform better judgment tasks. - J1 converts both verifiable and non-verifiable prompts into judgment tasks with verifiable rewards that incentivize stronger chain-of-thought reasoning. - The proposed method outperforms existing 8B and 70B LLMs, including models distilled from DeepSeek-R1, on various benchmarks. - J1's improved judgment ability stems from its learning to outline evaluation criteria, comparing against self-generated answers, and re-evaluating responses. - The authors provide comprehensive analysis and ablations on the model's performance, including comparisons between Pairwise-J1 and Pointwise-J1 models, offline vs. online training, reward strategies, and variations in thought content. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [PointArena: Probing Multimodal Grounding Through Language-Guided
  Pointing](https://arxiv.org/abs/2505.09990) | Boyang Li, Haoquan Fang, Yi Ru Wang, Jiafei Duan, Long Cheng | - PointArena is a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios, comprising Point-Bench (a curated dataset), Point-Battle (an interactive arena for model comparison), and Point-Act (a real-world robotic manipulation system). - PointArena's Point-Bench dataset contains approximately 1,000 pointing tasks across five reasoning categories, enabling a more thorough evaluation of multimodal pointing capabilities compared to existing benchmarks that focus primarily on object localization. - Extensive evaluations on PointArena demonstrate that Molmo-72B consistently outperforms other models, although proprietary models are increasingly demonstrating comparable performance, and that supervised training specifically targeting pointing tasks significantly enhances model performance. - PointArena's multi-stage evaluation pipeline reveals strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. - The platform facilitates blind, pairwise model comparisons through user voting in Point-Battle and real-world robotic manipulation evaluation in Point-Act. | ['Multimodal', 'Robotics'] | [Link](https://pointarena.github.io) | N/A |
| [OpenThinkIMG: Learning to Think with Images via Visual Tool
  Reinforcement Learning](https://arxiv.org/abs/2505.08617) | Zhengyuan Yang, Yunzhuo Hao, Mingyang Song, Linjie Li, Zhaochen Su | - This paper introduces OPENTHINKIMG, an open-source framework for tool-augmented large vision-language models (LVLMs). - It proposes a novel reinforcement learning framework, V-TOOLRL, to enable LVLMs to learn adaptive policies for invoking external vision tools. - V-TOOLRL significantly outperforms its supervised fine-tuning counterpart and surpasses established supervised tool-learning baselines on chart reasoning tasks. - The framework features standardized vision tool interfaces, scalable trajectory generation, and a flexible training environment to address the challenges of integrating diverse tools and training robust agents. - OPENTHINKIMG aims to advance dynamic, tool-augmented visual reasoning and provides a foundational framework for developing AI agents that can genuinely "think with images". | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/zhaochen0110/OpenThinkIMG) | N/A |
| [ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible
  Long-term Tracking](https://arxiv.org/abs/2505.08581) | Guanyi Qin, Ziyue Wang, Xuxiao Luo, Mingqi Gao, HeverLaw | - This paper introduces ReSurgSAM2, a two-stage framework for referring surgical segmentation in videos that uses Segment Anything Model 2 (SAM2). - ReSurgSAM2 first performs text-referred target detection using a Cross-Modal Spatial-Temporal Mamba and then performs tracking with a Diversity-Driven Long-term Memory. - The model achieves real-time performance at 61.2 FPS and outperforms existing methods on benchmark datasets. - The proposed method addresses the limitations of existing methods by improving efficiency and long-term tracking capabilities. - ReSurgSAM2 is evaluated on Ref-EndoVis17 and Ref-EndoVis18 datasets, demonstrating significant performance improvements over other state-of-the-art methods. | ['Video-Text-to-Text', 'Image Segmentation', 'Multimodal', 'Robotics'] | [Link](https://github.com/jinlab-imvr/ReSurgSAM2) | N/A |
| [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475) | Dayiheng Liu, Jiaxi Yang, Zeyu Cui, Binyuan Hui, Mouxiang Chen |  - This paper introduces a novel scaling paradigm for language models called Parallel Scaling (PARSCALE), which increases the model's parallel computation during training and inference.  - PARSCALE applies diverse and learnable transformations to the input, executes forward passes in parallel, and aggregates the outputs dynamically, improving inference efficiency without significantly increasing memory or latency.  - The authors propose a new scaling law showing that PARSCALE with P parallel streams is similar to scaling parameters by O(log P), offering superior inference efficiency compared to parameter scaling that achieves the same performance improvement.  - Experiments on large-scale pre-training and various downstream tasks validate the proposed scaling law and demonstrate PARSCALE's effectiveness.  - A two-stage training strategy is presented to reduce the training cost of PARSCALE and the method is demonstrated on an off-the-shelf pre-trained model, showing its versatility and applicability. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/QwenLM/ParScale) | [Link](https://huggingface.co/ParScale) |
