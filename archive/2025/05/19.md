

## Papers for 2025-05-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388) | huybery, BeichenZhang, Baosong, laf070810, yangapku |  - This paper introduces Qwen3, a series of large language models (LLMs) with parameter scales ranging from 0.6B to 235B, designed to improve performance, efficiency, and multilingual capabilities.   - Qwen3 integrates a thinking mode and a non-thinking mode within a unified framework, enabling dynamic mode switching based on user queries.   - The models achieve state-of-the-art results across diverse benchmarks, outperforming previous models and some leading open-source models. - Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility.  -  All Qwen3 models are publicly accessible under Apache 2.0 to promote research and development. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/QwenLM/Qwen3) | [Link](https://huggingface.co/Qwen) |
| [MMLongBench: Benchmarking Long-Context Vision-Language Models
  Effectively and Thoroughly](https://arxiv.org/abs/2505.10610) | Yu Zhao, Jipeng Zhang, Xiyu Ren, Wenhao Yu, Zhaowei Wang |  - This paper introduces MMLONGBENCH, a benchmark for evaluating long-context vision-language models (LCVLMs). - MMLONGBENCH includes 13,331 examples across five diverse downstream tasks and various image types to thoroughly evaluate LCVLMs.  - The benchmark uses a cross-modal tokenization scheme that combines vision patches and text tokens, delivering examples at five standardized input lengths (8K-128K tokens). - Through benchmarking 46 LCVLMs, the authors found that single-task performance is a weak proxy for overall LCVLM capability, closed-source and open-source models face challenges in long-context vision-language tasks, and models with strong reasoning ability tend to perform better. - MMLONGBENCH provides a foundation for diagnosing and improving LCVLMs. | ['Multimodal'] | [Link](https://github.com/EdinburghNLP/MMLongBench) | N/A |
| [GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049) | Tri Cao, Yulin Chen, Mingzhe Du, Shengfang Zhai, Yue Liu | - This paper introduces GuardReasoner-VL, a novel reasoning-based VLM guard model that uses online reinforcement learning (RL) to encourage deliberative reasoning before making moderation decisions. - GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, is constructed to enhance the model's reasoning capabilities. - The model is cold-started using supervised fine-tuning (SFT) and further improved through online RL, which incorporates rejection sampling, safety-aware data concatenation, a dynamic clipping parameter, and a length-aware safety reward. - Extensive experiments show that GuardReasoner-VL surpasses the runner-up by 19.27% F1 score on average across multiple multimodal guardrail benchmarks. - The data, code, and model weights (3B/7B) are publicly available. | ['Multimodal'] | [Link](https://github.com/yueliu1999/GuardReasoner-VL/) | N/A |
| [Simple Semi-supervised Knowledge Distillation from Vision-Language
  Models via texttt{D}ual-texttt{H}ead
  texttt{O}ptimization](https://arxiv.org/abs/2505.07675) | Sung Ju Hwang, Hyungjoon Jang, Seongjae Kang, dongboklee |  * This paper introduces Dual-Head Optimization (DHO), a novel semi-supervised knowledge distillation framework for vision-language models (VLMs).  * DHO uses two prediction heads which learn independently from labeled data and teacher predictions, mitigating gradient conflicts between supervised and distillation signals.  * Extensive experiments on ImageNet and eleven other datasets demonstrate that DHO consistently improves accuracy across multiple domains and fine-grained datasets.  * The findings show that DHO achieves state-of-the-art performance on ImageNet, improving accuracy by up to 3% with only 1% labeled data and 0.1% with 10% labeled data.  * The framework's simplicity and effectiveness make it a promising approach for deploying large-scale VLMs in resource-constrained environments. | ['Image Classification', 'Image Feature Extraction', 'Zero-Shot Image Classification', 'Multimodal'] | N/A | N/A |
| [Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token
  Level Granularity](https://arxiv.org/abs/2505.11107) | Yi-Chang Chen, Feng-Ting Liao, Jamie McGowan, Davide Buffelli, Splend1dchan | - The paper introduces GroupThink, a novel approach for collaborative reasoning in LLMs that enables multiple reasoning agents to collaborate concurrently at the token level.  - Unlike traditional turn-based methods, GroupThink allows agents to adapt dynamically to one another's progress mid-sentence, reducing redundancy and improving reasoning quality. - The proposed method is shown to significantly reduce latency compared to existing methods for various tasks (enumeration, divide-and-conquer, programming) without sacrificing accuracy. - GroupThink is implemented as a simple modification to any existing LLM, making it easily deployable on resource-constrained devices such as edge devices. - Experimental results demonstrate that GroupThink achieves significant latency improvements on multiple benchmarks, even when using LLMs not specifically trained for this collaborative paradigm. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Mergenetic: a Simple Evolutionary Model Merging Library](https://arxiv.org/abs/2505.11427) | erodola, crisostomi, teelinsan, tmencatt, adrianrob | - Mergenetic, an open-source library for evolutionary model merging, is introduced to facilitate flexible experimentation with evolutionary algorithms and merging methods in language models. - It offers comprehensive support for various merging methods and evolutionary algorithms, including lightweight fitness estimators to reduce evaluation costs. - Mergenetic demonstrates competitive results across tasks and languages using modest hardware, showcasing its efficiency and accessibility. - The library provides a Python API, CLI, and GUI for easy usage and customization, catering to both power users and those with limited coding experience. -  Future work will focus on extending Mergenetic's capabilities to handle extremely low-resource languages or domains where fine-tuned models may be unavailable. | ['Natural Language Processing'] | [Link](https://github.com/tommasomncttn/mergenetic) | N/A |
| [MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective
  Search and Data Curation](https://arxiv.org/abs/2505.10962) | Tao Yang, Yang Li, haitaominlp, freesunshine0316, invokerliang | - This paper introduces MPS-Prover, a novel stepwise automated theorem proving system that utilizes a multi-perspective search strategy and post-training data curation. - MPS-Prover incorporates a learned critic model with strategically designed heuristic rules to diversify tactic selection, enhancing search robustness and preventing unproductive states. - The post-training data curation strategy effectively eliminates redundant training data, improving model performance without sacrificing efficiency. - Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, outperforming existing 7B parameter models. - MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. | ['Natural Language Processing'] | N/A | N/A |
| [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518) | Nikos Komodakis, Spyros Gidaris, nasos10 | - MuToR, a novel multi-token prediction method, is introduced, which interleaves learnable register tokens into the input sequence to predict future tokens. - Compared to existing methods, MuToR introduces minimal additional parameters, requires no architectural changes, and is well-suited for supervised fine-tuning and parameter-efficient fine-tuning (PEFT). - The effectiveness and versatility of MuToR are demonstrated across a range of use cases, including supervised fine-tuning, PEFT, and pre-training, on challenging generative tasks in both language and vision domains. - MuToR consistently outperforms baselines (Next-Token and Multi-Token) in mathematical reasoning and summarization tasks, achieving superior results with negligible parameter overhead. - The 2D extension of MuToR adapts seamlessly to autoregressive image generation, showcasing its broader applicability and potential across diverse domains and training settings. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/nasosger/MuToR) | N/A |
| [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140) | rubis, bjerva, jjzha | - This paper introduces fs1, a new dataset containing 6K knowledge-graph enhanced reasoning traces designed to improve the factual accuracy of LLMs in open-domain question answering. - The authors fine-tune six variants of the Qwen2.5 architecture on both original and KG-enhanced reasoning traces and evaluate them across six benchmarks, encompassing over 22.6K questions. - Their findings indicate that smaller instruction-tuned models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts when using KG-enhanced reasoning traces. - Furthermore, test-time scaling consistently improves factual accuracy by 2-8%, demonstrating that longer reasoning chains enhance factual accuracy in open-domain QA. - All experimental artifacts, including reasoning traces and models, are publicly available for further research. | ['Question Answering'] | [Link](https://github.com/jjzha/fs1) | [Link](https://huggingface.co/jjzha/fs1) |
| [MatTools: Benchmarking Large Language Models for Materials Science Tools](https://arxiv.org/abs/2505.10852) | David J. Srolovitz, Bo Hu, Beilin Ye, Jiamin Xu, SiyuLiu | This paper introduces MatTools, a novel benchmark designed to evaluate the proficiency of Large Language Models (LLMs) in handling materials science tasks. MatTools comprises two key components: a QA benchmark and a real-world tool-usage benchmark. The QA benchmark assesses an LLM's understanding of materials science tools using 69,225 question-answer pairs from the pymatgen codebase.  The real-world benchmark challenges LLMs to generate functional Python code for materials property calculations, comprising 49 tasks with 138 subtasks. Experiments revealed that general-purpose LLMs significantly outperformed specialized models, with LLM-generated documentation proving superior as a retrieval source in Retrieval-Augmented Generation (RAG) systems.  A self-reflection LLM-doc RAG approach demonstrated state-of-the-art performance in real-world tool-usage tasks. | ['Document Question Answering', 'Question Answering'] | [Link](https://github.com/Grenzlinie/MatTools) | N/A |
