

## Papers for 2025-05-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/abs/2505.14683) | Ziang, codecaution, whyu, gouc, Andy1621 | The paper introduces BAGEL, a unified, decoder-only multimodal foundation model pretrained on trillions of tokens from interleaved text, image, video, and web data.  BAGEL's architecture is a Mixture-of-Transformer-Experts (MoT), maximizing capacity without task-specific constraints.  It significantly outperforms existing open-source models on multimodal generation and understanding benchmarks.  Furthermore, BAGEL exhibits advanced multimodal reasoning capabilities, including free-form image manipulation and future frame prediction. | ['Multimodal'] | [Link](https://github.com/ByteDance/BAGEL) | N/A |
| [SageAttention3: Microscaling FP4 Attention for Inference and An
  Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594) | surfingtomchen, whx1003, haofeng666, Guyan, jt-zhang | - This paper introduces SageAttention3, a novel FP4 attention mechanism that achieves a 5x speedup over the fastest existing FlashAttention implementation on RTX5090 GPUs. - SageAttention3 leverages the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation, reaching 1038 TOPS. - The authors also explore 8-bit training for attention mechanisms, proposing SageBwd, which achieves lossless performance in fine-tuning tasks but exhibits slower convergence during pretraining. - Experimental results demonstrate that both SageAttention3 and SageBwd significantly accelerate inference and training across various models and tasks. - The code for SageAttention3 is publicly available on GitHub. | ['Text2Text Generation', 'Text-to-Image', 'Text-to-Video', 'Video Classification'] | [Link](https://github.com/thu-ml/SageAttention) | N/A |
| [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14246) | sweetFruit, steins1096, zyshan, yuhangzang, ziyuliu | - This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT), a novel framework that enhances Large Vision-Language Models (LVLMs) with agentic reasoning and tool-use capabilities. - Visual-ARFT utilizes a reward-driven training strategy to enable LVLMs to perform complex multimodal reasoning tasks, such as browsing websites and writing code to manipulate images. - The proposed framework is evaluated on a new Multimodal Agentic Tool Bench (MAT) and existing benchmarks, demonstrating significant improvements over baseline methods. - Visual-ARFT outperforms GPT-4 on MAT-Coding and achieves considerable gains on multi-hop QA benchmarks like 2Wiki and HotpotQA. - The results suggest that Visual-ARFT presents a promising approach toward building robust and generalizable multimodal agents. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT) | N/A |
| [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388) | annariasdu, pabberpe, danihinjos, adriantormos, JordiBayarri-bsc | This paper introduces Aloe Beta, a new family of open-source large language models (LLMs) specialized for healthcare.  The models are created using a multi-stage training pipeline that involves supervised fine-tuning, model merging, and model alignment.  Evaluation results on various benchmarks show that Aloe Beta models achieve competitive performance compared to closed-source models and demonstrate improved safety against adversarial attacks.  The models and datasets used in this research are publicly available. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/axolotl-ai-cloud/axolotl), [Link](https://github.com/OpenRLHF/OpenRLHF), [Link](https://github.com/microsoft/DeepSpeed), [Link](https://github.com/HPAI-BSC/prompt_engine), [Link](https://github.com/HPAI-BSC/medical-specialities) | [Link](https://huggingface.co/collections/HPAI-BSC/healthcare-llms-aloe-family-6701b6a777f7e874a2123363), [Link](https://huggingface.co/collections/HPAI-BSC/aloe-beta-datasets-672374294ed56f43dc302499), [Link](https://huggingface.co/datasets/HPAI-BSC/medprompt_database_llama31), [Link](https://huggingface.co/datasets/aligner/aligner-20K), [Link](https://huggingface.co/datasets/BAAI/Infinity-Preference), [Link](https://huggingface.co/datasets/omi-health/medical-dialogue-to-soap-summary), [Link](https://huggingface.co/datasets/BI55/MedText), [Link](https://huggingface.co/datasets/ZahrizhalAli/mental_health_conversational_dataset), [Link](https://huggingface.co/datasets/gamino/wiki_medical_terms) |
| [Latent Flow Transformer](https://arxiv.org/abs/2505.14513) | Pei-Chen Ho, dsshiu, menghsichen, FengTing, yenchen | - The paper introduces the Latent Flow Transformer (LFT), a novel architecture that replaces a block of transformer layers with a single learned transport operator trained via flow matching, achieving significant compression while maintaining compatibility with the original architecture. - The LFT addresses the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm, which enhances the alignment of latent transport across distant transformer layers. - Experiments on the Pythia-410M model demonstrate that LFT, when trained with flow matching, compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of 0.407 vs. 0.529). - When trained with FW, LFT further distills 12 layers into one, reducing the KL divergence to 0.736, surpassing that from skipping 3 layers (0.932). - The results significantly narrow the gap between autoregressive and flow-based generation paradigms, showing the potential of LFT for efficient language modeling. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/mtkresearch/latent-flow-transformer) | [Link](null) |
| [Neurosymbolic Diffusion Models](https://arxiv.org/abs/2505.13138) | Antonio Vergari, ducdauge, pminervini, HEmile |  - This paper introduces Neurosymbolic Diffusion Models (NESYDMs), a novel class of neurosymbolic predictors that leverage discrete diffusion models to capture dependencies between symbolic concepts.  - NESYDMs address limitations of existing methods that assume conditional independence between symbols by reusing the independence assumption at each step of the diffusion process.  - The model architecture integrates masked diffusion models with symbolic programs, enabling scalable learning while capturing dependencies and uncertainty. - Experimental results on various benchmarks, including visual path planning and autonomous driving, demonstrate that NESYDMs achieve state-of-the-art accuracy and strong calibration among neurosymbolic predictors. - The authors demonstrate that their model improves on existing neurosymbolic methods in both accuracy and reliability. | ['Computer Vision', 'Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/HEmile/neurosymbolic-diffusion) | N/A |
| [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547) | Liangqiong-QU, limingcv, MENGTINGLIU, jcccy, gpx333 | - This paper introduces FedPrLLM, a federated pruning framework for Large Language Models (LLMs) that preserves data privacy. - FedPrLLM enables collaborative pruning of a global LLM model without sharing local calibration data. - Experiments demonstrate that one-shot pruning with layer comparison is optimal within the FedPrLLM framework. - The study explores various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and weight scaling. - Results show that weight scaling does not improve performance, and iterative pruning offers no significant benefits over one-shot pruning. | ['Natural Language Processing'] | [Link](https://github.com/Pengxin-Guo/FedPrLLM) | N/A |
| [Visionary-R1: Mitigating Shortcuts in Visual Reasoning with
  Reinforcement Learning](https://arxiv.org/abs/2505.14677) | Yixuan Li, Peng Gao, kaiyangzhou, yuhangzang, Jiaer-Xia | - This paper introduces Visionary-R1, a novel visual language model trained using reinforcement learning to mitigate shortcut learning in visual reasoning tasks. - Unlike existing methods that rely on chain-of-thought supervision, Visionary-R1 uses only question-answer pairs and a caption-reason-answer output format to encourage deeper image understanding. - The model outperforms strong multimodal models, including GPT-40, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks. - Visionary-R1 uses only reinforcement learning and addresses shortcut learning by requiring the model to generate a caption before reasoning, thereby improving generalization. - The results highlight the effectiveness of reinforcement learning with appropriate output formatting in mitigating shortcuts and improving the robustness of visual reasoning models. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/maifoundations/Visionary-R1) | N/A |
| [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652) | wenhu, zhangysk, DongfuJiang, SivilTaram, MrLight | - This paper introduces GENERAL-REASONER, a novel training paradigm designed to enhance Large Language Model (LLM) reasoning capabilities across diverse domains.  - It constructs a large-scale, high-quality dataset of questions with verifiable answers covering various disciplines and develops a generative model-based answer verifier. - GENERAL-REASONER outperforms existing baseline methods across 12 benchmarks, demonstrating robust and generalizable reasoning performance.  - The model-based verifier is shown to be superior to traditional rule-based methods, enabling effective reinforcement learning across diverse reasoning tasks.  - The research addresses the limitations of previous LLM reasoning works that primarily focus on mathematical and coding domains due to data abundance and ease of verification. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://tiger-ai-lab.github.io/General-Reasoner/) | N/A |
| [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489) | YongilKim, Sunkyoung, soheeyang, seungone, DKYoon | - This paper demonstrates that reasoning models, which utilize chain-of-thought (CoT) reasoning, exhibit superior performance in both problem-solving and accurately expressing confidence compared to their non-reasoning counterparts. - The study benchmarks six reasoning models across six datasets, revealing that reasoning models achieve better confidence calibration in 33 out of 36 settings. - Detailed analysis attributes this improved calibration to the slow thinking behaviors inherent in reasoning models, such as exploring alternatives and backtracking, allowing for dynamic confidence adjustments. - The authors find that reasoning models' calibration improves as their CoT unfolds, a trend not observed in non-reasoning models, further supporting the role of slow thinking. - This improved calibration is not exclusive to reasoning models; non-reasoning models also benefit when guided to perform slow thinking via in-context learning. | ['Question Answering'] | [Link](https://github.com/MattYoon/reasoning-models-confidence) | N/A |
| [Reasoning Path Compression: Compressing Generation Trajectories for
  Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866) | Jae-Joon Kim, YulhwaKim, dongwonjo, jiwonsong | - This paper introduces Reasoning Path Compression (RPC), a training-free method to accelerate inference in large language models (LLMs) that generate lengthy reasoning paths. - RPC leverages the semantic sparsity of reasoning paths by periodically compressing the KV cache, retaining only important entries based on an importance score computed using a selector window. - Experiments demonstrate that RPC improves the generation throughput of QwQ-32B by up to 1.60x compared to full KV cache inference, with minimal accuracy loss (1.2% drop on the AIME 2024 benchmark). - RPC offers a practical approach to deploying reasoning LLMs efficiently by mitigating memory usage and computational overhead associated with long reasoning sequences. - The method is training-free and easily integrated into existing LLM inference pipelines. | ['Natural Language Processing'] | [Link](https://github.com/jiwonsong-dev/ReasoningPathCompression) | N/A |
| [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680) | Wenjie Wang, chuats, jrwen, pl8787, KID-22 | - This paper introduces NExT-Search, a novel paradigm that aims to rebuild the user feedback ecosystem for generative AI search. - NExT-Search integrates two complementary modes: User Debug Mode, allowing engaged users to intervene at key stages of the search pipeline, and Shadow User Mode, using a personalized user agent to simulate user preferences. - The proposed framework leverages fine-grained feedback through online adaptation and offline updates, refining the search process in real-time and periodically fine-tuning models. - It introduces a feedback store that incentivizes user participation, further driving the continuous improvement of generative AI search systems. - The paper concludes by highlighting potential research directions in personalized user simulation, human-centric interface design, and efficient feedback integration. | ['Question Answering'] | N/A | N/A |
| [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the
  Limits of Large Language Models](https://arxiv.org/abs/2505.13559) | Eng Siong Chng, Lim Zhi Hao, Tanmay Surana, SkAndMl | - The paper introduces CS-Sum, a new benchmark dataset for evaluating code-switching dialogue summarization.  It contains 900-1300 human-annotated dialogues per language pair across three language pairs: Mandarin-English, Tamil-English, and Malay-English. - CS-Sum is the first benchmark for CS dialogue summarization across multiple languages and is designed to evaluate the comprehensibility of CS in LLMs. - Ten LLMs were evaluated, including open and closed-source models, using various approaches such as few-shot, translate-summarize, and fine-tuning. - The findings show that although LLMs achieve high scores on automatic metrics, they often make subtle mistakes affecting the overall meaning of the dialogues.  This highlights limitations of current LLMs in processing code-switched data. - Three main types of errors made by LLMs when summarizing CS dialogues were identified and analyzed: Code-Switching Loss, Meaning Shift from Poor Translation, and Speaker Misattribution. | ['Summarization'] | N/A | N/A |
| [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631) | Zewen Chi, Qingxiu Dong, Shaohan Huang, YUSHUIWX, lingjie23 | - This paper introduces Large Hybrid-Reasoning Models (LHRMs), a novel model architecture designed to adaptively determine whether to engage in extended reasoning processes based on the query's contextual information. - The LHRMs architecture utilizes a two-stage training pipeline: Hybrid Fine-Tuning (HFT) and Hybrid Group Policy Optimization (HGPO). - HFT provides a robust initialization, while HGPO implicitly learns to select the appropriate reasoning mode. - Experimental results across various benchmarks demonstrate that LHRMs significantly outperforms existing LLMs and LRMs in both reasoning and general capabilities while substantially improving efficiency. - The proposed Hybrid Accuracy (HAcc) metric provides a quantitative assessment of the model's ability to perform hybrid thinking. | ['Natural Language Processing'] | N/A | N/A |
| [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430) | Minxian Li, Jiayi Zhou, kaiyangzhou, chenyulin, sifengshang | - This paper introduces Quantized Zeroth-order Optimization (QZO), a novel technique for fine-tuning quantized neural networks.  - QZO minimizes memory usage by eliminating gradients and optimizer states using zeroth-order optimization and by employing model quantization. - Compared to full-parameter fine-tuning in bfloat16, QZO reduces the total memory cost by more than 18× for 4-bit LLMs.  - The effectiveness of QZO is demonstrated on various NLP benchmarks and on fine-tuning Stable Diffusion 3.5 Large using only 12.4GB of memory.  - QZO is orthogonal to existing post-training quantization methods and pushes the limits of memory-efficient training. | ['Natural Language Processing', 'Text Classification', 'Text Generation', 'Text-to-Image'] | [Link](https://github.com/maifoundations/QZO) | N/A |
| [SSR: Enhancing Depth Perception in Vision-Language Models via
  Rationale-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448) | Han Zhao, Pengxiang Ding, Xiaomin Yu, Ming Ma, yliu-cs |  - This paper introduces a novel framework, SSR, to improve spatial reasoning in Vision-Language Models (VLMs) by incorporating depth information.  - SSR translates raw depth data into structured textual rationales that serve as intermediate representations, enhancing spatial reasoning capabilities.   - Knowledge distillation is used to compress generated rationales into compact latent embeddings, enabling efficient integration into existing VLMs without retraining. - A new dataset, SSR-COT (a million-scale visual-language reasoning dataset with intermediate spatial reasoning annotations), and a benchmark, SSRBENCH, are introduced for comprehensive evaluation.  - Extensive experiments demonstrate that SSR substantially improves depth utilization and enhances spatial reasoning in VLMs. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464) | Sitong Zhao, Shuaiting Chen, Haotian Wang, Yunjie Ji, Emperorizzis | - This paper introduces three parallel datasets created by distilling reasoning data from three state-of-the-art teacher models: AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1. - The datasets comprise 1.89 million verified outputs on a shared corpus of queries. - Models trained on the AM-Thinking-v1-distilled dataset consistently outperform models trained on the other two datasets across multiple reasoning benchmarks (AIME2024, AIME2025, MATH500, and LiveCodeBench). - The AM-Thinking-v1-distilled model demonstrates adaptive output behavior, generating longer responses for more complex tasks and shorter responses for simpler ones. - The authors release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support further research. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://github.com/huggingface/Math-Verify), [Link](https://huggingface.co/datasets/hivaze/LOGIC-701) |
| [Towards eliciting latent knowledge from LLMs with mechanistic
  interpretability](https://arxiv.org/abs/2505.14352) | Emil Ryd, NeelNanda, srdm, bcywinski | - The paper introduces a novel "Taboo" model, a language model trained to describe a secret word without explicitly mentioning it, to investigate the ability of LLMs to conceal information. - It evaluates different methods for uncovering this hidden knowledge, including both black-box (non-interpretability) and white-box (mechanistic interpretability) approaches. - The findings suggest that interpretability-based techniques, such as Logit Lens and Sparse Autoencoders, are effective in eliciting the secret word, outperforming simpler black-box methods. - The study highlights the potential of mechanistic interpretability for addressing the crucial problem of detecting and extracting hidden knowledge in LLMs, a step towards safer and more reliable deployment. - Future work focuses on expanding to more complex scenarios and testing on more sophisticated models to confirm these findings. | ['Natural Language Processing'] | [Link](https://github.com/EmilRyd/eliciting-secrets) | [Link](https://huggingface.co/bcywinski) |
| [Hunyuan-Game: Industrial-grade Intelligent Game Creation Model](https://arxiv.org/abs/2505.14135) | vcvcvn, tangjs, YellowAddice, zhengsj, lslrh |  - This paper introduces Hunyuan-Game, a novel AI model for generating high-fidelity game assets, encompassing both images and videos.  - The model's architecture uses a combination of diffusion models and domain-specific knowledge to achieve state-of-the-art results in visual fidelity and motion naturalness.  - Extensive experiments show that Hunyuan-Game outperforms existing methods such as Midjourney and Kling in game scenarios.  - The model offers several key functionalities, including text-to-image generation, game visual effects generation, transparent image generation, and game character generation, as well as several video generation capabilities.   - The researchers aim to encourage community-driven innovation and foster collaborative development, paving the way for broader applications in the gaming industry. | ['Text-to-Image', 'Image-to-Video', 'Image-to-Image', 'Multimodal'] | N/A | N/A |
| [Reward Reasoning Model](https://arxiv.org/abs/2505.14674) | Qingxiu Dong, Zewen Chi, Jiaxin Guo, YUSHUIWX, unilm | - This paper introduces Reward Reasoning Models (RRMs), a novel approach to reward modeling that incorporates a chain-of-thought reasoning process before generating final rewards. - RRMs leverage additional test-time compute to enhance performance, particularly on complex queries where appropriate rewards are not immediately apparent. - The authors implement a reinforcement learning framework to train RRMs, enabling them to self-evolve reward reasoning capabilities without requiring explicit reasoning traces as training data. - Experimental results show that RRMs outperform previous reward models across various benchmarks and model sizes, demonstrating their ability to adaptively exploit test-time compute to improve reward accuracy. - The pre-trained RRM models are available at https://huggingface.co/Reward-Reasoning. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/Reward-Reasoning) |
| [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534) | cchoquette, julsh, tux, iliashum, chongyangs | This paper presents a novel framework for evaluating and improving the robustness of large language models against indirect prompt injection attacks.  The researchers developed adaptive attack techniques that continuously evolve against past, current, and future versions of Gemini, Google's multimodal large language model.  They demonstrate the effectiveness of adversarial fine-tuning for enhancing security.  The findings highlight the importance of adaptive evaluation and defense-in-depth strategies for building resilient models.  They show how more capable models are not necessarily more secure. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [Warm Up Before You Train: Unlocking General Reasoning in
  Resource-Constrained Settings](https://arxiv.org/abs/2505.13718) | Keith Ross, xanubhav81, AadimNepal, guactastesgood, safal312 | - This paper introduces a two-stage training strategy for developing reasoning LLMs in resource-constrained settings.  - The first stage involves "warming up" the model by distilling Long CoTs from a toy domain (Knights & Knaves logic puzzles) to acquire general reasoning skills.  - The second stage applies Reinforcement Learning with Verifiable Rewards (RLVR) to the warmed-up model using a limited set of target-domain examples.  - Experiments demonstrate that this two-phase approach improves reasoning performance across various tasks, including MATH, HumanEval+, and MMLU-Pro, even when training data is scarce.  - The warmed-up model consistently outperforms the base model after RLVR training on the same small dataset and maintains cross-domain generalizability. | ['Question Answering', 'Reinforcement Learning', 'Natural Language Processing'] | [Link](https://anonymous.4open.science/r/warmup-before-you-train-0EEF/) | N/A |
| [Truth Neurons](https://arxiv.org/abs/2505.12182) | ZiningZhu, jordansuchow, ShirleyY, YupengCao, Acatsama | - This paper proposes a novel method to identify "truth neurons" in language models, which are neurons that encode truthfulness in a subject-agnostic manner. - The method uses integrated gradients to measure neuron attribution scores for truthful vs. untruthful responses, identifying neurons positively contributing to truthfulness and negatively correlated with untruthfulness. - Experiments across models of varying scales validate the existence of truth neurons, showing that the encoding of truthfulness at the neuron level is a property shared by many language models. - Suppressing the activations of truth neurons degrades performance on multiple benchmarks, indicating that the truthfulness mechanisms are not tied to a specific dataset. - The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness, suggesting that truthfulness mechanisms primarily appear in the middle to later stages of language models. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365) | Matteo Dora, inoki-giskard, bmalezieux, pierlj | This paper introduces Phare, a multilingual diagnostic framework designed to evaluate LLMs across three safety dimensions: hallucinations, social biases, and harmful content. Unlike traditional benchmarks, Phare focuses on exposing failure modes rather than ranking models.  The evaluation of 17 state-of-the-art LLMs reveals systematic vulnerabilities across all safety dimensions, including sycophancy and stereotype reproduction.  Phare provides actionable insights to build more robust and trustworthy language systems.  The framework includes three modules: Hallucination, Biases and Stereotypes, and Harmful Content. | ['Natural Language Processing', 'Text Classification', 'Text Generation'] | [Link](https://github.com/giskard-ai/phare) | [Link](https://huggingface.co/datasets/giskardai/phare) |
| [MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8](https://arxiv.org/abs/2505.09569) | Lin Chen, Qiang Zhou, omidvarb, sliuxl, linboliu | - Introduced MigrationBench, a novel benchmark dataset for repository-level code migration from Java 8 to Java 17 and 21. - The dataset comprises a comprehensive set of repositories, categorized into subsets based on complexity and the presence of test cases. - Proposed an evaluation framework designed to provide a rigorous and standardized assessment of LLMs in this complex task. - Demonstrated the efficacy of using LLMs with a proposed feedback mechanism to address repository-level code migration challenges. - The benchmark dataset and source code are publicly available on GitHub and HuggingFace. | ['Natural Language Processing'] | [Link](https://github.com/amazon-science/MigrationBench) | [Link](https://huggingface.co/collections/AmazonScience) |
| [Two Experts Are All You Need for Steering Thinking: Reinforcing
  Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681) | Jiahao Xu, Zhiwei He, Yue Wang, Xingyu Chen, Mengru Wang | - This paper introduces a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE) to improve reasoning performance in Mixture-of-Experts (MoE) reasoning models without additional training. - RICE leverages normalized Pointwise Mutual Information (nPMI) to identify specialized experts, termed cognitive experts, that orchestrate meta-level reasoning operations. - Empirical evaluations on DeepSeek-R1 and Qwen3-235B LRMs demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. - The RICE method substantially outperforms existing reasoning-steering techniques, such as prompt design and decoding constraints. - The results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models. | ['Natural Language Processing'] | N/A | N/A |
| [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic
  Reasoning Limits](https://arxiv.org/abs/2505.14178) | Yiwei Xu, Jiaqi Wei, Juntai Cao, Charlesyooo, Wyattz23 |  - This paper investigates the impact of tokenization on the symbolic and arithmetic reasoning capabilities of large language models (LLMs).  - The authors introduce the concept of "Token Awareness" to assess how well tokenization schemes align with the granularity of reasoning tasks.  - They demonstrate that poorly designed tokenization schemes hinder symbolic computation, even with techniques like Chain-of-Thought prompting.  - Through systematic evaluation on arithmetic and symbolic tasks, they showcase how atomically aligned tokenization improves reasoning performance.  - Their findings highlight that the success of symbolic reasoning in LLMs is not solely dependent on model architecture but also on token-level representations. | ['Natural Language Processing'] | [Link](None) | [Link](None) |
| [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via
  Competition](https://arxiv.org/abs/2505.13380) | Van Nguyen, Quang Pham, Huy Nguyen, nhatho, DavidNguyen |  - This paper introduces CompeteSMoE, a novel algorithm for training large language models using a sparse mixture-of-experts (MoE) approach.  - The core innovation is a competition mechanism for routing tokens to experts, which improves sample efficiency and convergence compared to traditional softmax routing.  - CompeteSMoE demonstrates superior zero-shot performance across multiple vision-language and language pre-training benchmarks compared to existing MoE algorithms.  - The competition mechanism is theoretically analyzed, showing its statistical guarantees.  - The algorithm includes a scheduled training approach for efficiency, including a novel diversity loss to encourage diverse representations and a distillation loss to guide the router. | ['Multimodal'] | [Link](https://github.com/Fsoft-AIC/CompeteSMoE) | N/A |
| [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010) | grohg, amosharafa, himel7 | - This paper introduces a novel sentence-level media bias detection model, Bias-Detector, which is a fine-tuned RoBERTa-based model. - Bias-Detector outperforms the existing state-of-the-art DA-ROBERTa model by achieving a significantly higher macro F1 score across multiple folds of cross-validation, as demonstrated through McNemar's Test and a 5x2 paired t-test. - The model's superior performance is attributed to its ability to attend more meaningfully to contextually relevant tokens, thereby avoiding oversensitivity to politically charged terms. - An attention-based analysis reveals that Bias-Detector successfully focuses on crucial contextual elements for accurate bias classification. - The work also integrates a bias type classifier to further enhance media bias analysis, paving the way for more comprehensive future research. | ['Natural Language Processing', 'Text Classification'] | N/A | N/A |
| [Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative
  Verifier](https://arxiv.org/abs/2505.11966) | Kezhi Li, Zhijian Xu, Zeju Li, XiangyuWen, Jianyuan1 | - This paper introduces FlexiVe, a novel generative verifier that dynamically balances computational resources between fast and slow thinking modes using a flexible allocation of verification budget strategy. - FlexiVe is integrated into a Solve-Detect-Verify pipeline, which proactively identifies solution completion points to trigger targeted verification and provide solver feedback, resulting in an efficient inference-time scaling framework.  - Experiments demonstrate that FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces and outperforms baselines in reasoning accuracy and inference efficiency on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO). - The Solve-Detect-Verify pipeline significantly outperforms baselines like self-consistency on mathematical reasoning benchmarks in terms of both accuracy and efficiency. - The study also includes ablation studies and comparisons with other methods, such as self-consistency, showing that the proposed method is superior in terms of accuracy and efficiency. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [Masking in Multi-hop QA: An Analysis of How Language Models Perform with
  Context Permutation](https://arxiv.org/abs/2505.11754) | Jeff Z. Pan, Mirella Lapata, pvougiou, hwy9855 | - This paper explores how Language Models (LMs) perform on multi-hop question answering (MHQA) tasks when the order of retrieved documents is permuted. - The authors find that encoder-decoder models generally outperform causal decoder-only models on MHQA, even when significantly smaller. - They observe that optimal performance is achieved when the order of documents aligns with the reasoning chain, and that bi-directional attention can improve the performance of causal decoder-only models. - Furthermore, the study reveals that attention weights tend to peak at higher values when the answer is correct, suggesting a potential heuristic for improving LM performance. - The code for this research is publicly available on Github. | ['Document Question Answering', 'Question Answering'] | [Link](https://github.com/hwy9855/MultiHopQA-Reasoning) | N/A |
| [Incorporating brain-inspired mechanisms for multimodal learning in
  artificial intelligence](https://arxiv.org/abs/2505.10176) | Xin Yang, Qingqun Kong, Yang Li, Dongcheng Zhao, Xiang He | - This paper introduces a novel multimodal fusion strategy called Inverse Effectiveness-driven Multimodal Fusion (IEMF) which dynamically adjusts the fusion module's weights according to the reliability of the unimodal signals. - IEMF is inspired by the biological inverse effectiveness mechanism observed in the brain's multimodal integration. - The experimental results across several audio-visual tasks (classification, continual learning, question answering) and different neural network architectures (ANNs, SNNs) demonstrate that IEMF consistently outperforms the baseline models by achieving higher accuracy and reducing computational costs. - Ablation studies validate the effectiveness of IEMF's key components, showing its ability to handle various levels of unimodal input quality, thus improving the robustness of multimodal fusion. - The proposed IEMF model exhibits good generalizability across different datasets, fusion methods, and network architectures | ['Multimodal', 'Audio-to-Audio', 'Audio Classification', 'Video Classification', 'Audio-to-Audio', 'Visual Question Answering'] | [Link](https://github.com/Brain-Cog-Lab/IEMF) | N/A |
| [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for
  Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306) | Shangbin Feng, Wenhao Yu, Yuwei Zhang, shangjingbo, KomeijiForce | - Introduced WIKIDYK, a novel, real-world, large-scale benchmark for knowledge injection that continuously evolves over time without human intervention. - Proposed WIKIDYK leverages recently-added and human-written facts from Wikipedia's "Did You Know..." entries, which are carefully selected by expert Wikipedia editors. - Extensive experiments using continued pre-training revealed that Bidirectional Language Models (BiLMs) exhibit significantly stronger knowledge memorization capabilities compared to Causal Language Models (CLMs). - Introduced a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to compensate for the smaller scales of current BiLMs and further improve the reliability accuracy. - Showcased that the framework further improves the reliability accuracy by up to 29.1%. | ['Question Answering'] | [Link](https://github.com/zhang-yu-wei/WikiDYK) | [Link](https://huggingface.co/datasets/YWZBrandon/wikidyk) |
| [Understanding Gen Alpha Digital Language: Evaluation of LLM Safety
  Systems for Content Moderation](https://arxiv.org/abs/2505.10588) | Fausto Giunchiglia, Manisha Mehta | - This research introduces a novel dataset comprising 100 contemporary Gen Alpha expressions, gathered from diverse online platforms. - The study systematically evaluates the capacity of four leading AI systems (GPT-4, Claude, Gemini, and Llama 3) to interpret and moderate Gen Alpha communication, focusing on masked harassment and manipulation. - A multi-perspective evaluation framework is developed, assessing comprehension levels of Gen Alpha users, their parents, and professional content moderators, along with the four AI systems. - The findings reveal significant gaps in AI systems' comprehension capabilities, particularly concerning context-dependent meanings and the rapid evolution of Gen Alpha slang. - The paper highlights the urgency for enhanced AI safety systems to effectively protect Gen Alpha users and addresses the broader ethical considerations of youth online safety. | ['Natural Language Processing'] | N/A | N/A |
