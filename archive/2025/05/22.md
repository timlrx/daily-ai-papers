

## Papers for 2025-05-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302) | Zeyue Xue, Yutao Zeng, Jing Liu, Chaoyi Zhang, ChenMnZ | - This paper introduces a unified scaling law for quantization-aware training (QAT) of large language models (LLMs), which models quantization error as a function of model size, training data volume, and quantization group size. - The proposed scaling law is validated through 268 QAT experiments, showing that quantization error decreases with increasing model size but increases with more training tokens and coarser quantization granularity.  - The paper decomposes quantization error into weight and activation components, finding that weight quantization error increases more rapidly with more training tokens, while activation quantization error in the FC2 layer is identified as a primary bottleneck. - A mixed-precision quantization approach is proposed to address the bottleneck, demonstrating that weight and activation quantization errors can converge to similar levels with sufficient training data.  - The findings provide key insights for improving QAT research and development, suggesting that reducing both weight and activation quantization error is important for efficient quantized LLMs. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement
  Learning](https://arxiv.org/abs/2505.14231) | Jing Tang, Yong Liu, Mingxing Li, Sule Bai, xiaochonglinghu | - UniVG-R1, a novel reasoning-guided multimodal large language model (MLLM), is proposed for universal visual grounding, enhancing reasoning capabilities through reinforcement learning and cold-start data. - A high-quality Chain-of-Thought (CoT) grounding dataset is constructed, annotated with detailed reasoning chains to guide the model towards correct reasoning paths. - Rule-based reinforcement learning is performed to encourage the model to identify correct reasoning chains, and a difficulty-aware weight adjustment strategy is introduced to address a difficulty bias in RL training. - UniVG-R1 achieves state-of-the-art performance on MIG-Bench, surpassing the previous method by 9.1%, and demonstrates strong generalizability with an average improvement of 23.4% in zero-shot performance across four benchmarks. - The effectiveness of UniVG-R1 is demonstrated through extensive experiments on various datasets, showcasing its superior performance compared to existing methods. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/abs/2505.15809) | Ke Shen, Bowen Li, Ling Yang, comin, tyfeld |  - MMADA is a novel class of multimodal diffusion foundation models that unifies textual reasoning, multimodal understanding, and text-to-image generation.  - It adopts a unified diffusion architecture with a shared probabilistic formulation and modality-agnostic design, eliminating the need for modality-specific components.  - A mixed long-chain-of-thought (CoT) fine-tuning strategy is implemented to curate a unified CoT format across modalities and enhance cold-start training.  - UniGRPO, a unified policy-gradient-based RL algorithm tailored for diffusion foundation models, is proposed to unify post-training.  - MMADA-8B surpasses existing models like LLaMA-3-7B and Qwen2-7B in textual reasoning, Show-o and SEED-X in multimodal understanding, and SDXL and Janus in text-to-image generation. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/Gen-Verse/MMaDA) | N/A |
| [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909) | Pengfei Liu, zizi-0123, henryhe0123 | - The paper introduces PC Agent-E, an efficient agent training framework that significantly reduces the reliance on large-scale human demonstrations for training computer use agents. - PC Agent-E leverages a small set of human-annotated trajectories, further improved by synthesizing diverse action decisions with Claude 3.7 Sonnet, to train a model that outperforms the strong Claude 3.7 Sonnet baseline on WindowsAgentArena-V2. - The model achieves a remarkable 141% relative improvement compared to the Qwen2.5-VL-72B baseline. - PC Agent-E demonstrates strong generalizability to different operating systems, showcasing its ability to adapt to various environments. - The authors open-source their code, data, and models to facilitate future research. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/GAIR-NLP/PC-Agent-E) | N/A |
| [Diffusion vs. Autoregressive Language Models: A Text Embedding
  Perspective](https://arxiv.org/abs/2505.15045) | Anh Tuan Luu, Arman Cohan, LYGeng, yilunzhao, siyue | This paper introduces DIFFEMBED, a novel approach for text embedding using diffusion language models.  DIFFEMBED utilizes a bidirectional attention architecture, which addresses limitations found in autoregressive models' unidirectional attention.  Experiments demonstrate DIFFEMBED's superior performance on various retrieval tasks such as long-document retrieval, reasoning-intensive retrieval, and instruction-following retrieval. The results show that bidirectional attention is crucial for encoding global context in long and complex text.  Finally, the authors introduce REASONAUG, a new dataset for training embedding models on reasoning-intensive tasks. | ['Natural Language Processing'] | [Link](https://github.com/anonymous) | N/A |
| [When to Continue Thinking: Adaptive Thinking Mode Switching for
  Efficient Reasoning](https://arxiv.org/abs/2505.15400) | Haodong Zhao, Yaawennn, Machine981, Amanda2023, DadaCloud01 | - This paper introduces Adaptive Self-Recovery Reasoning (ASRR), a novel framework that dynamically adjusts reasoning length in large reasoning models (LRMs) based on problem difficulty. - ASRR leverages an "Internal Self-Recovery Mechanism" where models implicitly supplement reasoning during answer generation, suppressing unnecessary reasoning for simple tasks. - The framework incorporates an accuracy-aware length reward regulation, allocating reasoning effort efficiently based on problem difficulty. - Experiments across multiple benchmarks and models show that ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss. - Results highlight ASRR's potential for efficient, adaptive, and safer reasoning in LRMs, improving performance and safety alignment on various benchmarks. | ['Natural Language Processing'] | N/A | N/A |
| [Deliberation on Priors: Trustworthy Reasoning of Large Language Models
  on Knowledge Graphs](https://arxiv.org/abs/2505.15210) | Jun Liu, Rui Xing, Zhitao Gao, Jie Ma, stillqu | - This paper introduces Deliberation over Priors (DP), a novel framework that enhances the trustworthiness of Large Language Models (LLMs) reasoning on Knowledge Graphs (KGs). - DP employs a progressive knowledge distillation strategy to enhance LLMs' structural pattern awareness of KGs and a reasoning-introspection strategy to verify the reasoning reliability. - Experiments on three benchmark datasets demonstrate that DP achieves state-of-the-art performance, particularly a 13% improvement on the Complex WebQuestions dataset, while generating highly trustworthy responses. - The frameworkâ€™s flexibility is shown by integrating it with various LLMs, and its practicality is demonstrated via various analysis on its efficiency and interaction. - DP shows superior performance to existing methods, even in scenarios requiring high precision and where the correct answer must be in the top-ranked position. | ['Question Answering'] | [Link](https://github.com/reml-group/Deliberation-on-Priors) | N/A |
| [lmgame-Bench: How Good are LLMs at Playing Games?](https://arxiv.org/abs/2505.15146) | Eric P. Xing, Haoyang Yu, Mingjia Huo, Yuxuan13, Snyhlxde | This paper introduces Imgame-Bench, a new benchmark for evaluating large language models (LLMs) in playing video games.  It addresses challenges like brittle vision perception and prompt sensitivity through modular harnesses (perception, memory, reasoning modules) and data contamination mitigation.  Imgame-Bench is shown to effectively differentiate 13 leading LLMs across six diverse games, revealing unique capability blends.  Reinforcement learning on Imgame-Bench games is also demonstrated to transfer to unseen games and external planning tasks. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/lmgame-org/GamingAgent/lmgame-bench) | N/A |
| [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781) | Xinchao Wang, Gongfan Fang, Runpeng Yu, Xinyin Ma | - This paper introduces dKV-Cache, a novel KV-cache-like mechanism designed to accelerate the inference speed of Diffusion Language Models (DLMs). - The core idea is to address the incompatibility of DLMs with traditional KV-cache by employing a delayed and conditioned caching strategy for key and value states. - Two variants of dKV-Cache are proposed: dKV-Cache-Decode, which offers near lossless acceleration and even improves performance on long sequences, and dKV-Cache-Greedy, which achieves higher speedups with a slightly reduced performance. - Experiments on various benchmarks demonstrate that dKV-Cache achieves 2-10x speedup in inference, significantly narrowing the gap between autoregressive and diffusion language models. - The code for dKV-Cache is publicly available on GitHub. | ['Text Generation'] | [Link](https://github.com/horseee/dKV-Cache) | N/A |
| [How Should We Enhance the Safety of Large Reasoning Models: An Empirical
  Study](https://arxiv.org/abs/2505.15404) | Qi Zhu, Victor Shea-Jay Huang, Xian Qi Loye, Zhexin Zhang, yangjunxiao2021 | - This paper presents a comprehensive empirical study on enhancing the safety of Large Reasoning Models (LRMs). - It identifies three key failure patterns in directly distilling safe responses from LRMs and proposes methods to address these issues. - The study demonstrates that simpler reasoning processes (short or template-based) can achieve comparable safety performance to complex reasoning, which is more efficient for models to learn. - It explores the impact of incorporating benign reasoning data during safety fine-tuning and shows that it helps balance safety and over-refusal. - The findings offer insights into improving the safety of LRMs and highlight the trade-offs between reasoning, safety, and over-refusal. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/LRM-Safety-Study) | N/A |
| [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817) | Heng Huang, R. Thomas McCoy, Simeng Han, Lichang Chen, TongZheng1999 | This paper introduces Mixture-of-Thought (MoT), a novel framework that leverages multiple reasoning modalities (natural language, code, and truth tables) to enhance the logical reasoning capabilities of LLMs.  MoT employs a self-evolving training process which jointly learns across modalities, resulting in improved performance compared to single-modality baselines.  Experiments on the FOLIO and ProofWriter benchmarks show significant improvements in accuracy, achieving up to a +11.7pp average gain.  The improved performance is attributed to the synergistic combination of modalities during both training and inference.  The approach shows robustness on harder problems and addresses key bottlenecks in natural language inference. | ['Natural Language Processing'] | [Link](https://github.com/zhengkid/Truth_Table_Logical_Reasoning) | N/A |
| [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data
  Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656) | Hongning Wang, Shiyao Cui, Yuhao Sun, Zhexin Zhang, yangjunxiao2021 | - This paper reveals a novel security risk in fine-tuning open-source LLMs with proprietary data: the original creators of the LLM can extract the private fine-tuning data using a simple backdoor training technique. - The attack leverages a backdoor instruction to force the model to reproduce the training queries, requiring only black-box access to the downstream model. - Experiments across four popular open-source LLMs and two datasets demonstrate high extraction performance: up to 76.3% of the data can be extracted in realistic settings, increasing to 94.9% under ideal conditions. - A detection-based defense strategy is explored, but it is shown to be bypassable, highlighting the significance of the discovered vulnerability. - The authors emphasize the urgency of addressing this risk and release their code and data to foster further research in mitigating this security concern. | ['Natural Language Processing'] | [Link](https://github.com/thu-coai/Backdoor-Data-Extraction) | N/A |
| [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934) | Mingsheng Long, Ningya Feng, Shaofeng Yin, manchery | This paper introduces RLVR-World, a framework that uses reinforcement learning with verifiable rewards to train world models across diverse modalities.  It unifies world modeling into an autoregressive generation framework, treating states and actions as a sequence of tokens.  RLVR-World demonstrates significant performance gains on language and video-based world models in various domains. The model achieves +30.7% accuracy on text-based game state prediction and +9.2% relative improvement on LPIPS for robot manipulation trajectory prediction. Finally, it shows the utility of reinforced world models in downstream applications like policy evaluation and model-predictive control. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://thuml.github.io/RLVR-World) | N/A |
| [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous
  Concept Space](https://arxiv.org/abs/2505.15778) | Chenyang Zhao, Ao Shen, Weixiang Yan, Xuehai He, Zhen Zhang | - This paper introduces Soft Thinking, a training-free method that improves the reasoning capabilities of large language models (LLMs) by enabling reasoning in a continuous concept space. - Soft Thinking generates soft, abstract concept tokens as probability-weighted mixtures of token embeddings, allowing for smoother transitions and richer representations compared to traditional discrete token-based methods. - Empirical evaluations on mathematical and coding datasets show that Soft Thinking consistently improves both accuracy (up to 2.48% on pass@1 accuracy) and generation efficiency (up to 22.4% reduction in generation length). - The method's effectiveness is demonstrated across various LLMs, including QwQ-32B, DeepSeek-R1-Distill-Qwen-32B, and DeepSeek-R1-Distill-Llama-70B. - Qualitative analysis reveals that Soft Thinking generates highly interpretable and readable outputs, highlighting its potential to enhance LLM reasoning. | ['Natural Language Processing'] | [Link](https://github.com/eric-ai-lab/Soft-Thinking) | [Link](null) |
| [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search
  with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776) | Xipeng Qiu, Kai Song, Ruijun Feng, Siyin Wang, BeastyZ | - ConvSearch-R1 is a novel self-driven framework for conversational query reformulation that eliminates the need for external rewrite supervision by leveraging reinforcement learning. - It uses a two-stage approach: Self-Driven Policy Warm-Up (addresses the cold-start problem) and Retrieval-Guided Reinforcement Learning (addresses sparsity issues in conventional retrieval metrics). - ConvSearch-R1 significantly outperforms state-of-the-art methods on TopiOCQA and QReCC datasets, achieving over 10% improvement on TopiOCQA while using smaller (3B parameter) models. - The model employs a rank-incentive reward shaping mechanism to address sparsity in conventional retrieval metrics, enhancing stable and efficient exploration. - ConvSearch-R1 demonstrates strong generalization ability across models of various scales and datasets. | ['Reinforcement Learning', 'Question Answering', 'Natural Language Processing'] | [Link](https://github.com/BeastyZ/ConvSearch-R1) | N/A |
| [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529) | Chujie Zheng, Xiaoce Wang, Haoran Liu, Jinzhe Tu, yangjunxiao2021 | - This paper introduces BARREL, a novel framework that improves the factual reliability and reduces overconfidence in Large Reasoning Models (LRMs). - BARREL addresses two key pathological reasoning patterns: last-minute guessing and second-thought spiraling, which contribute to overconfident and incorrect answers. - The framework consists of three main components: Knowledge Labeling, Reasoning Trace Construction for SFT, and GRPO Stage, designed to improve concise and boundary-aware factual reasoning. - Experimental results demonstrate that BARREL training significantly enhances the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while maintaining accuracy comparable to models finetuned on reasoning data. - The study also highlights the importance of medium-level rewards in encouraging uncertainty-aware refusal, addressing the root cause of models' inability to admit ignorance. | ['Question Answering'] | [Link](https://github.com/thu-coai/BARREL) | [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1), [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B), [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B), [Link](https://huggingface.co/Qwen/QwQ-32B) |
| [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827) | Jianfeng Gao, Jingbo Shang, Chandan Singh, Liyuan Liu, Yufan Zhuang | - This paper introduces Mixture of Inputs (MOI), a training-free method that improves autoregressive text generation by incorporating the previously discarded token distribution into the generation process. - MOI uses a Bayesian estimation method to blend the sampled token with its distribution, resulting in a richer internal representation and improved text quality. - The method consistently improves performance across multiple LLMs (QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B) on mathematical reasoning, code generation, and PhD-level question answering tasks. - MOI is computationally efficient with negligible overhead and is immediately applicable to existing models without requiring additional training or architectural changes. - Experiments show that MOI outperforms standard autoregressive generation and an ablation study shows that the Bayesian approach is crucial for the performance gains. | ['Text Generation'] | [Link](https://github.com/EvanZhuang/mixinputs) | N/A |
| [RL Tango: Reinforcing Generator and Verifier Together for Language
  Reasoning](https://arxiv.org/abs/2505.15034) | Duane S. Boning, Zhang-Wei Hong, Maohao Shen, Zhengqi Gao, sunshinekevin | - TANGO, a novel framework, concurrently trains a large language model (LLM) generator and a generative LLM verifier using reinforcement learning (RL). - The verifier is trained solely on outcome-level verification correctness rewards without explicit process-level annotations, improving robustness and generalization. - TANGO achieves state-of-the-art results among 7B/8B-scale models on five competition-level math benchmarks and four challenging out-of-domain reasoning tasks. - Both components of TANGO exhibit substantial improvements on difficult mathematical reasoning problems. - The framework addresses limitations of previous approaches by utilizing a generative, process-level verifier that co-evolves with the generator. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/kaiwenzha/rl-tango) | N/A |
| [Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM](https://arxiv.org/abs/2505.15816) | Ziwei Liu, Lewei Lu, Penghao Wu | - This paper introduces ProxyV, a novel approach to reduce computational redundancy in large multimodal models (LMMs) by focusing on computation-level redundancy rather than token-level redundancy. - ProxyV uses proxy vision tokens to alleviate the computational burden on original vision tokens, enhancing efficiency without sacrificing performance and potentially improving performance. - The method is shown to be flexible and can be combined with token reduction methods to further boost efficiency. - Experimental results demonstrate that ProxyV consistently achieves no performance loss or even improvements on various benchmarks. - The code for ProxyV will be made publicly available. | ['Multimodal'] | [Link](https://github.com/penghao-wu/ProxyV) | N/A |
| [Evaluate Bias without Manual Test Sets: A Concept Representation
  Perspective for LLMs](https://arxiv.org/abs/2505.15524) | Zirui Song, Chenxi Wang, Wei Liu, Kaiyang Wan, Lang Gao | This paper introduces BIASLENS, a novel framework for evaluating bias in large language models (LLMs) without relying on manually curated test sets.  BIASLENS leverages concept activation vectors (CAVs) and sparse autoencoders (SAEs) to extract interpretable concept representations from the model's internal feature space.  Bias is quantified by measuring the asymmetry in representational similarity between a target concept and reference concepts.  The method demonstrates strong agreement with existing bias evaluation metrics and reveals new, subtle biases that are difficult to detect with current methodologies.  The framework is scalable and efficient, facilitating fully automatic bias evaluation. | ['Natural Language Processing'] | [Link](https://github.com/jbloomAus/SAELens) | N/A |
| [Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large
  Audio-Language Models](https://arxiv.org/abs/2505.15406) | Lang Gao, Mingzhe Li, Mingxuan Cui, Qian Jiang, Zirui Song | - This paper introduces AJailBench, the first benchmark for evaluating jailbreak vulnerabilities in Large Audio-Language Models (LAMs). - AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, is created by converting textual jailbreak attacks using realistic text-to-speech synthesis. - The Audio Perturbation Toolkit (APT) generates dynamic adversarial variants by applying targeted distortions across time, frequency, and amplitude domains, while enforcing semantic consistency. - Evaluation of state-of-the-art LAMs reveals that none exhibit consistent robustness across attacks, and even small, semantically preserved perturbations significantly reduce the safety performance. - AJailBench and APT are released to facilitate future research on LAM safety. | ['Audio', 'Audio Classification'] | [Link](https://github.com/mbzuai-nlp/AudioJailbreak) | N/A |
| [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818) | Haidong Wang, Jun Zheng, Leon Lin | - WebNovelBench, a novel benchmark for evaluating long-form Chinese web novel generation by LLMs, is introduced. - The benchmark uses a dataset of over 4,000 Chinese web novels and frames evaluation as a synopsis-to-story generation task. - A multifaceted evaluation framework encompassing eight narrative quality dimensions, automatically assessed via an LLM-as-Judge approach, is proposed. - The performance of 24 state-of-the-art LLMs is comprehensively analyzed, ranking their storytelling abilities and providing insights for future development. - WebNovelBench offers a scalable, replicable, and data-driven methodology for evaluating and advancing LLM-driven narrative generation. | ['Text Generation'] | [Link](https://github.com/OedonLestrange42/webnovelbench) | [Link](https://huggingface.co/datasets/0edon42/webnovelbench) |
| [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990) | Dilek Hakkani-TÃ¼r, Nimet Beyza Bozdag, Ishika Agarwal | - This paper introduces Language Specific Knowledge (LSK), a phenomenon where language models exhibit stronger performance or preference for certain languages when responding to specific topics. - The authors propose LSKEXTRACTOR, a two-stage framework that identifies expert languages for specific knowledge regions and leverages code-switching to improve inference. -  LSKEXTRACTOR maps LSK and their corresponding expert languages by conducting chain-of-thought reasoning in 13 languages on training queries from various datasets. - During test-time inference, LSKEXTRACTOR embeds an unseen query to identify its corresponding cluster and retrieves the optimal language for reasoning, showing an average relative improvement of 10% in accuracy. - The research contributes to the development of more inclusive language models aligned with cultural and linguistic contexts. | ['Natural Language Processing'] | N/A | N/A |
| [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation
  of LLM Hallucinations](https://arxiv.org/abs/2505.14101) | Johannes Bjerva, Katja Hose, Russa Biswas, ernlavr | - This paper introduces MultiHal, a novel multilingual, multi-hop benchmark for evaluating Large Language Model (LLM) hallucinations grounded in knowledge graphs. - MultiHal leverages 7 existing benchmarks, enriching them with 25,905 high-quality KG paths mined from Wikidata and translated into 5 languages. - A novel unified scalable framework systematically integrates entity linking methods, mapping question-answer pairs to KG paths, to curate factual information. - The baseline evaluation shows an absolute improvement in semantic similarity scores across various LLMs in KG-based retrieval augmented generation (RAG) compared to vanilla question answering. - MultiHal fosters research towards graph-based hallucination mitigation and fact-checking tasks. | ['Question Answering'] | [Link](https://github.com/ernlavr/multihal) | [Link](https://huggingface.co/datasets/ernlavr/multihal) |
| [HumaniBench: A Human-Centric Framework for Large Multimodal Models
  Evaluation](https://arxiv.org/abs/2505.11454) | Mukund S. Chettiar, Ashmal Vayani, Vahid Reza Khazaie, Aravind Narayanan, shainaraza | *- HumaniBench is introduced, a benchmark for evaluating large multimodal models (LMMs) on human-centered criteria including fairness, ethics, and empathy, addressing limitations in existing benchmarks. - The benchmark consists of 32K real-world image-question pairs, annotated using a GPT-4 assisted pipeline and verified by experts, probing seven HCAI principles through diverse tasks such as multilingual QA and visual grounding. - Evaluation of 15 state-of-the-art LMMs reveals that proprietary models generally perform better; however, gaps remain in robustness and visual grounding, with open-source models often lagging in human-aligned principles like ethics and inclusivity. - The dataset, annotation prompts, and evaluation code are publicly released to promote transparency and encourage future research. - HumaniBench is the first benchmark specifically designed to assess human-centered AI principles in LMMs, addressing the gap in evaluating their social responsibility and genuine alignment with human values. | ['Multimodal'] | [Link](https://vectorinstitute.github.io/HumaniBench/) | [Link](https://huggingface.co/vectorinstitute/HumaniBench) |
