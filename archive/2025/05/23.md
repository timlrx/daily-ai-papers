

## Papers for 2025-05-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Scaling Reasoning, Losing Control: Evaluating Instruction Following in
  Large Reasoning Models](https://arxiv.org/abs/2505.14810) | Yu Cheng, Xiaoye Qu, Jiawei Gu, yaful, TingchenFu | - This paper introduces MathIF, a new benchmark for evaluating instruction following in large language models (LLMs) specifically designed for mathematical reasoning tasks. - The benchmark includes 420 high-quality evaluation samples with varying difficulty and constraints, allowing for a comprehensive evaluation of instruction following capabilities. - Empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability in LLMs, with models that reason more effectively often struggling to adhere to user instructions. - The authors find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning degrade in instruction adherence, especially when generation length increases. - This work highlights a fundamental tension in current LLM training paradigms and motivates the need for more instruction-aware reasoning models. | ['Natural Language Processing'] | [Link](https://github.com/TingchenFu/MathIF) | N/A |
| [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement
  Learning](https://arxiv.org/abs/2505.16410) | Hongjin Qian, Jiajie Jin, Xiaoxi Li, Yifei Chen, Guanting Dong | This paper introduces Tool-Star, a reinforcement learning framework that enables LLMs to autonomously utilize multiple external tools during reasoning.  Tool-Star incorporates six types of tools and systematic designs in data synthesis and training to address the scarcity of tool-use data.  A two-stage training framework enhances multi-tool collaborative reasoning through cold-start fine-tuning and a multi-tool self-critic RL algorithm. Experimental results on over 10 challenging reasoning benchmarks demonstrate Tool-Star's effectiveness and efficiency.  Tool-Star outperforms other TIR baselines across various reasoning tasks, showcasing strong overall reasoning performance and reliability in tool usage. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/dongguanting/Tool-Star) | N/A |
| [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with
  Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966) | Fangzhen Lin, Weimin Ren, Haozhe Wang, Alex Su, wenhu | - This paper introduces Pixel Reasoner, a novel framework that introduces the concept of reasoning in pixel space to enhance the reasoning capabilities of Vision-Language Models (VLMs). - Pixel Reasoner is a 7B parameter model that improves VLM performance across diverse visual reasoning benchmarks, achieving state-of-the-art results on several datasets. - The model is trained using a two-phase approach: instruction tuning on synthesized reasoning traces, and reinforcement learning with a curiosity-driven reward scheme. - The proposed method addresses the challenges of imbalanced competence and reluctance to adopt pixel-space operations in VLMs through a two-stage training process. - This framework highlights the importance of pixel-space reasoning and the effectiveness of the proposed training approach for improving VLM performance on visually intensive tasks. | ['Multimodal', 'Visual Question Answering'] | [Link](https://tiger-ai-lab.github.io/Pixel-Reasoner/) | N/A |
| [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933) | Jun Zhou, Jun Hu, Xiaolu Zhang, Shen Nie, Zebin You | - LLaDA-V is a purely diffusion-based multimodal large language model (MLLM) that integrates visual instruction tuning with masked diffusion models. - The model architecture incorporates a vision encoder and an MLP connector to project visual features into the language embedding space, enabling effective multimodal alignment. - LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts. - When trained on the same instruction data, LLaDA-V is highly competitive with LLaMA3-V across multimodal tasks and achieves state-of-the-art performance in multimodal understanding. - The findings suggest that large language diffusion models show promise in multimodal contexts. | ['Multimodal'] | [Link](https://github.com/ml-gsai/LLaDA-V-demo/) | N/A |
| [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604) | Wenqi Zhang, Haolei Xu, Yongliang Shen, Yuchen Yan, Haoran Zhao | - This paper introduces Self-Braking Tuning (SBT), a novel framework that enables large language models (LLMs) to autonomously regulate their reasoning process and reduce overthinking. - SBT tackles overthinking by allowing the model to self-regulate, eliminating the need for external control mechanisms. - The proposed method uses a systematic approach to identify redundant reasoning steps, generating training signals to enhance the model's self-regulation capabilities. - Experiments on mathematical reasoning benchmarks show that SBT reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models. - The framework includes two data construction strategies: SBT-E (Exact) and SBT-D (Dynamic), each with unique strengths to help the model learn when to stop reasoning. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ZJU-REAL/Self-Braking-Tuning) | N/A |
| [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684) | Guiyang Hou, Wenqi Zhang, Yongliang Shen, Yuchen Yan, Haolei Xu | - This paper introduces a novel method called CoT-Bridge to improve the quality of chain-of-thought (CoT) reasoning in large language models (LLMs) by addressing the issue of "Thought Leaps", which are missing intermediate steps in CoT datasets. - CoT-Bridge automatically identifies and generates the missing steps, resulting in improved completeness and coherence in the reasoning chains. This is achieved by training a model on a specialized dataset called ScaleQM+, which is constructed from the ScaleQuestMath dataset by systematically removing intermediate steps. - Experiments demonstrate that fine-tuning LLMs on datasets with bridged CoTs consistently outperforms those trained on the original datasets with Thought Leaps. Improvements of up to +5.87% on NuminaMath and +3.36% on MetaMathQA were observed. - CoT-Bridge can be integrated with other techniques such as knowledge distillation and reinforcement learning, further enhancing model performance. - The effectiveness of the approach is validated across various benchmarks, including both mathematical and logical reasoning tasks, highlighting the broad applicability and generalizability of the method. | ['Natural Language Processing'] | [Link](https://zju-real.github.io/CoT-Bridge) | N/A |
| [Backdoor Cleaning without External Guidance in MLLM Fine-tuning](https://arxiv.org/abs/2505.16916) | Xun Xiao, Jinhe Bi, Jian Liang, Wenke Huang, Xuankun Rong | - The paper introduces Believe Your Eyes (BYE), a novel unsupervised data filtering framework that leverages attention entropy patterns to identify and filter backdoor samples in Multimodal Large Language Models (MLLMs) without external guidance. - BYE operates via a three-stage pipeline: extracting attention maps, computing entropy scores and profiling sensitive layers, and performing unsupervised clustering to remove suspicious samples. - Unlike prior defenses, BYE requires no clean supervision, auxiliary labels, or model modifications, making it robust and generalizable. - Extensive experiments across various datasets, models, and trigger types demonstrate BYE's effectiveness, achieving near-zero attack success rates while maintaining clean-task performance. - The code is publicly available at the provided GitHub URL. | ['Multimodal'] | [Link](https://github.com/XuankunRong/BYE) | N/A |
| [Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel
  Decoding](https://arxiv.org/abs/2505.16990) | Xinchao Wang, Xinyin Ma, Runpeng Yu | - Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM), is proposed, combining a vision encoder with a discrete diffusion language model. - A novel training paradigm combines an initial autoregressive phase with a subsequent diffusion phase, addressing training instability and length bias issues. - Dimple-7B surpasses LLaVA-NEXT by 3.9% in performance, demonstrating the effectiveness of the proposed approach. - Confident decoding dynamically adjusts the number of tokens generated per step, significantly improving inference efficiency. - Structure priors allow fine-grained control over the response structure, a capability difficult to achieve in autoregressive models. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/yu-rp/Dimple) | [Link](https://huggingface.co/rp-yu/Dimple-7B), [Link](https://huggingface.co/spaces/rp-yu/Dimple-7B) |
| [VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game
  Quality Assurance](https://arxiv.org/abs/2505.15952) | Nabajeet Barman, Saman Zadtootaghaj, Abhijay Ghildyal, corpaul, taesiri |  - This paper introduces VideoGameQA-Bench, a comprehensive benchmark dataset for evaluating vision-language models (VLMs) on video game quality assurance (QA) tasks.   - The benchmark includes a wide range of QA activities such as visual unit testing, visual regression testing, glitch detection, and bug report generation, covering both images and videos.   - The dataset comprises samples from over 800 games and synthetic game scenes, ensuring diversity and real-world applicability.   - Experiments show that while VLMs perform well on some tasks, they struggle with fine-grained details and certain types of glitches.   - This work highlights the need for standardized benchmarks to evaluate VLM performance in the video game QA domain. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text'] | [Link](https://asgaardlab.github.io/videogameqa-bench/) | N/A |
| [SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward](https://arxiv.org/abs/2505.17018) | Xiangyu Yue, Dongzhan Zhou, Haoming Lyu, Kaituo Feng, Kaixuan Fan | - SophiaVL-R1 is a novel multimodal large language model (MLLM) that integrates a thinking reward model to improve reasoning capabilities.  - The model enhances the quality of the thinking process by evaluating logical soundness, consistency, and redundancy.  - Trust-GRPO, a new training algorithm that weighs the thinking reward based on its reliability, reduces the risk of reward hacking.  - Experimental results demonstrate that SophiaVL-R1 outperforms several state-of-the-art MLLMs on various reasoning benchmarks, including MathVista and MMMU.  - Notably, SophiaVL-R1-7B surpasses LLaVA-OneVision-72B despite having 10 times fewer parameters. | ['Multimodal', 'Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/kxfan2002/SophiaVL-R1) | N/A |
| [SpatialScore: Towards Unified Evaluation for Multimodal Spatial
  Understanding](https://arxiv.org/abs/2505.17012) | Yanfeng Wang, Ya Zhang, Yaohui Chen, Xiao Huang, Haoning Wu | This paper introduces SpatialScore, a comprehensive benchmark for evaluating multimodal large language models' (MLLMs) spatial understanding capabilities.  It integrates existing datasets and proposes VGBench, a specialized benchmark for visual geometry perception.  The paper also introduces SpatialAgent, a novel multi-agent system that leverages specialized tools for enhanced spatial understanding.  Extensive experiments show that SpatialAgent significantly improves the performance of various existing MLLMs on the SpatialScore benchmark.  The findings highlight that current MLLMs still face challenges in tasks involving visual geometry perception, suggesting future directions for MLLM development. | ['Multimodal', 'Visual Question Answering', 'Computer Vision', 'Depth Estimation', 'Image Classification', 'Object Detection', 'Image Segmentation', 'Image Feature Extraction', 'Keypoint Detection', 'Natural Language Processing', 'Question Answering', 'Robotics'] | [Link](https://haoningwu3639.github.io/SpatialScore) | N/A |
| [LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/abs/2505.16839) | Yusuke Kato, Akash Gokul, Hritik Bansal, Konstantinos Kallidromitis, Shufan Li | - LaViDa is a novel family of Vision-Language Models (VLMs) based on discrete diffusion models (DMs), which offer advantages such as parallel decoding for faster inference and bidirectional context for controllable generation. - The LaViDa architecture comprises a vision encoder (SigLIP-400M), a diffusion language model (LLaDA-8B or Dream-7B), and an MLP projection network, jointly fine-tuned for multimodal instruction following. - LaViDa incorporates novel techniques like complementary masking for efficient training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling, addressing challenges in adapting DMs to multimodal tasks. - Experiments on various benchmarks (MMMU, MathVista, ChartQA, ScienceQA) demonstrate that LaViDa achieves competitive or superior performance to existing AR VLMs, while providing unique advantages in terms of speed-quality trade-off, controllability, and bidirectional reasoning. - On COCO captioning, LaViDa surpasses Open-LLaVa-Next-Llama3-8B by +4.1 CIDEr with 1.92× speedup; on bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. | ['Multimodal'] | [Link](https://github.com/jacklishufan/LaViDa) | N/A |
| [Training-Free Reasoning and Reflection in MLLMs](https://arxiv.org/abs/2505.16151) | Zhenzhong Chen, Hongchen Wei | - This paper introduces FRANK, a training-free multimodal large language model (MLLM) that enhances existing MLLMs with reasoning and reflection capabilities without any gradient updates or extra supervision. - FRANK leverages two key insights: homologous model merging, which treats both vision and reasoning models as task-fine-tuned variants of the same base LLM, and layer-wise functional specialization, which recognizes that shallow layers in MLLMs focus on visual perception while deeper layers focus on text. - It uses a layer-wise Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow layers. - Experiments demonstrate FRANK's effectiveness on challenging multimodal reasoning benchmarks, outperforming the strongest baseline InternVL2.5-38B by +5.3 on the MMMU benchmark and even surpassing the proprietary GPT-40 model. - FRANK is a training-free method that addresses the challenges of retraining and data scarcity associated with reinforcement learning approaches for multimodal reasoning. | ['Multimodal'] | N/A | N/A |
| [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879) | Ching-Chen Kuo, Kaizhi Zheng, Diji Yang, Xuehai He, Yue Fan | - The paper introduces GRIT, a novel method that teaches large language models (LLMs) to perform grounded reasoning with images by generating reasoning chains that interleave natural language with explicit bounding box coordinates. - GRIT employs a reinforcement learning approach, GRPO-GR, with rewards focusing on answer accuracy and the format of grounded reasoning, eliminating the need for data with reasoning chain annotations. - The method achieves high data efficiency, requiring only 20 image-question-answer triplets for training. - Experiments demonstrate that GRIT effectively trains LLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities. - The results indicate that models trained with GRIT outperform baselines across various testing sets in terms of both answer accuracy and grounding quality. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://grounded-reasoning.github.io) | N/A |
| [AGENTIF: Benchmarking Instruction Following of Large Language Models in
  Agentic Scenarios](https://arxiv.org/abs/2505.16944) | Youfeng Liu, Amy Xin, Xiaozhi Wang, Hao Peng, Yunjia Qi | - This paper introduces AGENTIF, the first benchmark designed for evaluating the instruction-following capabilities of LLMs in realistic agentic scenarios. - AGENTIF is composed of 707 instructions derived from 50 real-world agentic applications, featuring an average length of 1723 words and 11.9 constraints per instruction. - The benchmark systematically evaluates existing advanced LLMs and demonstrates that current models perform poorly, particularly in handling complex constraint structures and tool specifications. - An error analysis reveals that challenges mainly stem from condition and tool constraints, highlighting the need for further research in these areas. - The code and data for AGENTIF are publicly available to facilitate future research. | ['Natural Language Processing'] | [Link](https://github.com/THU-KEG/AgentIF) | [Link](https://huggingface.co/datasets/THU-KEG/AgentIF) |
| [Think or Not? Selective Reasoning via Reinforcement Learning for
  Vision-Language Models](https://arxiv.org/abs/2505.16854) | Mike Zheng Shou, James Cheng, Kevin Qinghong Lin, Jiaqi Wang | - This paper introduces TON, a two-stage training framework that enhances reasoning in vision-language models by enabling selective reasoning. - The first stage uses a supervised fine-tuning method with a "thought dropout" operation, which randomly replaces reasoning traces with empty thoughts, creating a "think-or-not" format for selective reasoning. - The second stage utilizes Group Relative Policy Optimization (GRPO) to enable the model to freely learn when to engage in reasoning, maximizing task-aware outcome rewards. - Experimental results demonstrate that TON reduces completion length by up to 90% compared to vanilla GRPO without sacrificing performance, and in some cases, even improving it. - TON consistently demonstrates improved efficiency across diverse vision-language tasks, indicating the model progressively learns to bypass unnecessary reasoning steps. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://github.com/kokolerk/TON) | N/A |
| [VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced
  Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.16192) | Haiyang Xu, Han Yang, Wei Ye, Yongrui Heng, Chaoya Jiang | - This paper introduces VLM-R³, a novel multimodal large language model (MLLM) framework that incorporates region recognition, reasoning, and refinement for improved chain-of-thought reasoning. - The core of VLM-R³ is Region-Conditioned Reinforcement Policy Optimization (R-GRPO), a training paradigm that rewards the model for selecting informative regions, applying appropriate transformations, and integrating visual context into reasoning steps. - To bootstrap the R-GRPO policy, a new Visuo-Lingual Interleaved Rationale (VLIR) dataset was created providing step-level supervision on region selection and textual justification. - Experiments on various benchmarks such as MathVista, ScienceQA, and others demonstrate that VLM-R³ achieves state-of-the-art results in zero-shot and few-shot settings, especially on questions requiring subtle spatial reasoning or fine-grained visual cue extraction. - The largest gains are observed on tasks demanding subtle spatial reasoning or fine-grained visual cue extraction. | ['Multimodal'] | N/A | N/A |
| [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963) | Cheng Zeng, Jianxiang Wang, Zejun Li, Siyuan Wang, Shujun Liu | - This paper introduces OViP, an online vision-language preference learning framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. - OViP addresses the issue of Large Vision-Language Models (LVLMs) hallucinating by using a diffusion model to generate more relevant negative samples. - The framework incorporates both response-centric and image-centric preference learning to effectively reduce hallucinations while preserving core multimodal capabilities. - Experiments demonstrate that OViP effectively reduces hallucinations and outperforms other methods on several benchmark datasets. - The paper also introduces refined evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. | ['Multimodal', 'Image-to-Text', 'Visual Question Answering'] | N/A | N/A |
| [Let Androids Dream of Electric Sheep: A Human-like Image Implication
  Understanding and Reasoning Framework](https://arxiv.org/abs/2505.17019) | Yazhe Niu, Chenhao Zhang | - This paper introduces LAD, a novel framework for human-like image implication understanding and reasoning, addressing the challenge of contextual gaps in existing models. - LAD is a three-stage framework: Perception (converting visual information into textual representations), Search (iteratively searching and integrating cross-domain knowledge), and Reasoning (generating context-aligned image implications). - Using the lightweight GPT-4o-mini model, LAD achieves state-of-the-art performance on English image implication benchmarks and significant improvement on Chinese benchmarks. - The framework outperforms other models on Multiple-Choice Questions and Open-Style Questions, demonstrating the effectiveness of its contextual alignment approach. - LAD provides insights into how AI can effectively interpret image implications, advancing vision-language reasoning and human-AI interaction. | ['Multimodal'] | [Link](https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep) | N/A |
| [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186) | Aosong Feng, Jayanth Srinivasa, Gaowen Liu, Xuandong Zhao, Kaiwen Zhou | - This paper introduces SafeKey, a novel framework designed to enhance the safety of Large Reasoning Models (LRMs) by amplifying "aha-moment" insights during safety reasoning. - SafeKey incorporates two main objectives: a Dual-Path Safety Head to improve safety signals in the model's internal representations and a Query-Mask Modeling objective to focus the model's attention on query understanding. - Experiments across various safety benchmarks show that SafeKey significantly improves safety generalization, reducing the average harmfulness rate by 9.6% while maintaining general abilities. - The effectiveness of SafeKey is demonstrated through analysis of internal attention patterns and improved quality of hidden representations. - SafeKey addresses the limitation of supervised fine-tuned models that struggle to generalize to unseen malicious queries, a critical challenge in ensuring robust safety for LRMs. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://safekeylrm.github.io) | N/A |
| [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot
  Manipulation Datasets](https://arxiv.org/abs/2505.15517) | Ken Goldberg, Zehan Ma, Shuangyu Xie, keplerccc | - The paper introduces Robo2VLM, a novel framework for generating visual question answering (VQA) datasets from real-world robot manipulation trajectories.  - Robo2VLM leverages multiple sensory modalities (RGB images, stereo depth, end-effector pose, gripper state, force-torque) to segment trajectories into manipulation phases and generate VQA questions based on spatial, goal-conditioned, and interaction reasoning. - A large-scale VQA dataset, Robo2VLM-1, is created using 176k real robot trajectories from the Open X-Embodiment dataset, containing 684,710 questions covering 463 scenes and 3,396 robotic manipulation tasks.  - Experiments demonstrate that fine-tuning state-of-the-art VLMs on Robo2VLM-1 improves their performance on spatial and interaction reasoning tasks, with significant gains observed in some categories. - Comparison with human performance shows that the best performing models achieve near-human accuracy in object-centric categories but still show a considerable gap in complex reasoning tasks. | ['Visual Question Answering', 'Robotics', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/keplerccc/Robo2VLM-1) |
| [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal
  Large Language Models](https://arxiv.org/abs/2505.17015) | Xiaodong Wang, Xingyu Chen, Hao Tang, Weiyao Wang, Runsen Xu | - This paper introduces Multi-SpatialMLLM, a novel framework that enhances multi-modal large language models (MLLMs) with robust multi-frame spatial understanding. - The core of this framework is the MultiSPA dataset, a large-scale collection of over 27 million samples encompassing diverse 3D and 4D scenes, along with a comprehensive benchmark for evaluating spatial reasoning tasks. - Multi-SpatialMLLM significantly outperforms baseline and proprietary systems on the proposed MultiSPA benchmark, demonstrating its effectiveness in various spatial tasks. - The model showcases multi-task learning benefits and emergent capabilities in challenging scenarios, showcasing its potential applications in robotics. - Additionally, the research explores the use of Multi-SpatialMLLM as a multi-frame reward annotator for robotics. | ['Multimodal', 'Robotics', 'Visual Question Answering', 'Computer Vision', 'Depth Estimation'] | N/A | N/A |
| [When Do LLMs Admit Their Mistakes? Understanding the Role of Model
  Belief in Retraction](https://arxiv.org/abs/2505.16170) | Robin Jia, ayyyq | - This paper introduces the concept of retraction in LLMs, defining it as the acknowledgement of errors in previously generated answers. - The authors construct model-specific datasets to evaluate the frequency and causes of retraction in LLMs. - They demonstrate a strong correlation between a model's internal belief and its decision to retract, showing that models are less likely to retract answers they believe to be correct. - Through experiments, they establish a causal link between belief and retraction, showing that manipulating the model's belief influences its retraction behavior. - Finally, they demonstrate that supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/ayyyq/llm-retraction) | N/A |
| [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal
  Reasoning](https://arxiv.org/abs/2505.16088) | Wei Zhao, Maxime Peyrard, Gagan Bhatia | - This paper introduces DATEAUGBENCH, a new benchmark dataset comprising 6,500 examples with 21 date formats, designed to evaluate the impact of date tokenization on temporal reasoning in large language models (LLMs). - They introduce a new metric, the date fragmentation ratio, which measures how faithfully a tokenizer preserves multi-digit date components. - Experiments reveal an emergent date-abstraction mechanism in LLMs, where models stitch together fragmented date components for temporal reasoning. - The study finds that excessive date fragmentation correlates with accuracy drops of up to 10 points on uncommon dates. - Analysis shows that larger models compensate for date fragmentation more effectively and quickly than smaller models. | ['Natural Language Processing'] | N/A | N/A |
| [How Do Large Vision-Language Models See Text in Image? Unveiling the
  Distinctive Role of OCR Heads](https://arxiv.org/abs/2505.15865) | Hwanhee Lee, Sunghyun Ryu, Hwan Chang, Ingeol Baek | - This paper introduces a novel method to identify Optical Character Recognition (OCR) heads within Large Vision-Language Models (LVLMs). - The proposed method leverages a scoring mechanism to distinguish OCR heads from other retrieval heads based on their activation patterns and ability to extract textual information from images. - The study reveals three key properties of OCR heads: reduced sparsity, qualitative distinctiveness, and static activation patterns. - Experiments on downstream tasks, including chain-of-thought prompting and attention masking, validate the specialized role of OCR heads in processing embedded textual information and improve performance. - The findings provide a deeper understanding of LVLMs' internal mechanisms for handling embedded textual information and offer insights for enhancing multimodal reasoning and reducing hallucination in OCR-centric applications. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
| [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation
  Capabilities in Any Language](https://arxiv.org/abs/2505.14395) | Jiho Jin, Eunsu Kim, Seogyeong Jeong, aliceoh, seyoungsong | MUG-Eval is a novel evaluation framework designed to assess the multilingual text generation capabilities of large language models (LLMs) across a wide range of languages, particularly those with limited resources.  It overcomes limitations of existing methods by transforming existing benchmarks into conversational tasks, thus avoiding the need for language-specific tools or LLM-as-judges.  The framework uses task success rate as a proxy for evaluating generation quality, demonstrating strong correlation with established benchmarks. MUG-Eval offers a robust and efficient solution, readily scalable to thousands of languages. | ['Text Generation'] | [Link](https://github.com/seyoungsong/mugeval) | N/A |
