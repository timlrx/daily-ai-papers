

## Papers for 2025-05-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.21327) | Renrui Zhang, Yiting Lu, Yilei Jiang, Tianshuo Peng, Jiakang Yuan |  - MME-Reasoning is a comprehensive benchmark for evaluating the logical reasoning capabilities of multimodal large language models (MLLMs).  - It addresses the limitations of existing benchmarks by explicitly categorizing logical reasoning types (inductive, deductive, and abductive) and carefully curating data to focus on reasoning ability.  - Evaluation reveals substantial limitations of current state-of-the-art MLLMs in holistic logical reasoning, highlighting performance imbalances across reasoning types.  - An in-depth analysis investigates the impact of 'thinking mode' and rule-based reinforcement learning on reasoning performance.  - The findings provide valuable insights for understanding and evaluating reasoning capabilities in MLLMs. | ['Multimodal'] | [Link](https://github.com/Alpha-Innovator/MME-Reasoning) | [Link](https://huggingface.co/datasets/U4R/MME-Reasoning) |
| [Paper2Poster: Towards Multimodal Poster Automation from Scientific
  Papers](https://arxiv.org/abs/2505.21497) | Philip Torr, Xi He, HideOnBush, KevinQHLin, weipang142857 |  - This paper introduces Paper2Poster, the first benchmark and metric suite for academic poster generation.   - Paper2Poster pairs recent conference papers with corresponding author-designed posters, evaluating outputs on visual quality, textual coherence, holistic assessment, and PaperQuiz (poster's ability to convey core content).   - A multi-agent pipeline called PosterAgent is proposed, which parses papers into structured assets, plans text-visual alignments, and refines panel layouts using VLM feedback.   - Evaluations show that GPT-40, while visually appealing, produces noisy text and low PaperQuiz scores, highlighting reader engagement as a primary bottleneck.   - Open-source variants of PosterAgent significantly outperform GPT-40-driven systems across most metrics, using fewer tokens. | ['Multimodal'] | [Link](https://github.com/Paper2Poster/Paper2Poster) | N/A |
| [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189) | Ivan Oseledets, glebzok | - This paper demonstrates that frozen LLMs can generate hundreds of accurate tokens in a single forward pass without iterative decoding, using only two learned embeddings. - The study reveals a surprising capability of LLMs, namely multi-token generation without iterative decoding. - The authors investigated the behavior of the embeddings, providing insight into the information they encode, and empirically showed that these representations form connected and local regions in embedding space. -  The authors also examined the reconstruction capability variations with model size and target sequence characteristics (e.g., natural vs. synthetic text). - This research challenges the assumption that autoregressive generation is essential for reconstructing accurate multi-token sequences from compressed representations. | ['Text Generation'] | [Link](https://github.com/Glebzok/OneStepLLMGeneration) | [Link](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) |
| [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM
  Reasoning](https://arxiv.org/abs/2505.17813) | Roy Schwartz, Yossi Adi, Gabriel Synnaeve, Michael Hassid | - This paper introduces short-m@k, a novel LLM inference method that prioritizes shorter reasoning chains to improve efficiency and accuracy. - The method executes k independent generations in parallel and stops once m (m â‰¤ k) thinking processes are complete, using majority voting to determine the final answer. - Experiments on three reasoning LLMs and three benchmarks show that short-m@k, particularly short-1@k and short-3@k, significantly outperforms standard majority voting in terms of accuracy and speed, often using fewer thinking tokens. - It demonstrates that shorter reasoning chains are more likely to yield correct answers, challenging the conventional wisdom that longer chains lead to better reasoning. - Further, finetuning an LLM using short reasoning chains improves performance, reinforcing the paper's main argument that prioritizing brevity in reasoning can yield significant improvements. | ['Question Answering'] | N/A | N/A |
| [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based
  Mobile GUI Agents](https://arxiv.org/abs/2505.21496) | Zimu Lu, Yuxiang Chai, Guozhi Wang, AJZhou, HanXiao1999 |  - The paper introduces UI-Genie, a self-improving framework for training MLLM-based mobile GUI agents that addresses the challenges of outcome verification and scalability of high-quality training data.   - UI-Genie uses a reward model, UI-Genie-RM, with an image-text interleaved architecture to process historical context and unify action and task-level rewards.  - UI-Genie includes data generation strategies like rule-based verification, controlled trajectory corruption, and hard negative mining to support UI-Genie-RM training.   - The self-improvement pipeline in UI-Genie iteratively enhances both the agent and reward models through reward-guided exploration and outcome verification, expanding solvable GUI tasks.  - Experiments show that UI-Genie achieves state-of-the-art performance on multiple GUI agent benchmarks across three generations of self-improvement. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/Euphoria16/UI-Genie) | N/A |
| [MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks](https://arxiv.org/abs/2505.16459) | Chaoran Hu, Ruihang Zhang, Tianhe Gu, Guiyao Tie, zhouxueyang | This paper introduces MMMR, a new benchmark for evaluating multi-modal reasoning in large language models.  MMMR includes a challenging dataset of 1,083 questions spanning six diverse reasoning types and a modular evaluation pipeline assessing reasoning quality beyond accuracy. Results show that models incorporating explicit thinking traces perform better but still exhibit reasoning pathologies.  MMMR provides a scalable and comprehensive evaluation for multi-modal reasoning systems. The benchmark is designed to rigorously evaluate multi-modal reasoning with explicit thinking traces. | ['Multimodal'] | [Link](https://mmmr-benchmark.github.io/) | N/A |
| [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for
  Subject-to-Video Generation](https://arxiv.org/abs/2505.20292) | chongyangma, Jinfa, dyf, pkuhexianyi, BestWishYsh |  - This paper introduces OpenS2V-Nexus, a comprehensive benchmark and million-scale dataset for Subject-to-Video (S2V) generation.  - OpenS2V-Eval, a fine-grained benchmark, is proposed to evaluate S2V models on seven categories, using three automatic metrics focusing on subject consistency, naturalness, and text relevance. - OpenS2V-5M, a large-scale dataset with 5.4 million high-quality 720P subject-text-video triples, incorporates subject diversity through cross-video associations and GPT-Image prompting.  - Evaluations on 16 S2V models highlight the strengths and weaknesses of existing methods, showing that closed-source models generally outperform open-source models.  - The dataset and benchmark aim to accelerate future S2V generation research by addressing the three major challenges in this area: poor generalization, copy-paste issues, and inadequate human fidelity. | ['Text-to-Video', 'Image-to-Video', 'Multimodal'] | [Link](https://github.com/PKU-YuanGroup/OpenS2V-Nexus) | [Link](https://huggingface.co/collections/BestWishYsh) |
| [GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient
  Fine-Tuning](https://arxiv.org/abs/2505.20355) | Eunhyeok Park, Taesu Kim, Hyungjun Kim, Daehyun Ahn, yeonjoon-jung | - This paper introduces GraLoRA, a novel method for parameter-efficient fine-tuning of large language models that addresses the limitations of existing low-rank adaptation methods like LoRA. - GraLoRA partitions weight matrices into sub-blocks, each with its own low-rank adapter, overcoming LoRA's overfitting issues at higher ranks. - Experimental results on code generation and commonsense reasoning benchmarks demonstrate that GraLoRA consistently outperforms LoRA and other baselines, achieving significant gains in accuracy. - The improvement is consistent across different model sizes and rank settings, showcasing the scalability and robustness of GraLoRA. - GraLoRA's enhanced expressivity and robustness to input outliers make it a promising solution for parameter-efficient fine-tuning. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/SqueezeBits/GraLoRA.git) | N/A |
| [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale
  Verified Dataset](https://arxiv.org/abs/2505.21297) | Xudong Zhou, Bingcheng Dong, Yi Zhu, Li Lyna Zhang, YF-L | - The paper introduces rStar-Coder, a large-scale (418K problems, 580K solutions) verified dataset for improving code reasoning in large language models. - rStar-Coder significantly improves LLM code reasoning capabilities by providing high-difficulty competition-level code problems and solutions with rich test cases. - The dataset construction involves three core contributions: curating competitive programming problems, building a reliable input-output test case synthesis pipeline, and augmenting problems with high-quality, test-case-verified long-reasoning solutions. - Experiments on Qwen models demonstrate rStar-Coder's effectiveness, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. - rStar-Coder consistently improves the performance of LLMs on various code reasoning benchmarks, surpassing existing state-of-the-art models in several cases. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/microsoft/rStar) | N/A |
| [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent
  Systems](https://arxiv.org/abs/2505.18943) | Yixuan Li, Yuxuan Chen, samuelyeh, XUANMINGZHANG | MetaMind is a novel multi-agent framework designed for emulating human-like social reasoning.  It decomposes social understanding into three collaborative stages: a Theory-of-Mind Agent, a Domain Agent, and a Response Agent.  MetaMind achieves state-of-the-art performance across multiple benchmarks, including a 35.7% improvement in real-world social scenarios and surpasses human-level performance on key ToM tasks for the first time.  Ablation studies demonstrate the importance of each component in balancing contextual plausibility, social appropriateness, and user adaptation. The framework advances AI towards human-like social intelligence. | ['Natural Language Processing'] | [Link](https://github.com/XMZhangAI/MetaMind) | N/A |
| [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language
  Neurons Perspective](https://arxiv.org/abs/2505.21505) | Xiao Liu, Shuaijie She, Xiang Liu, DreamW1ngs, Shimao-Zhang | - This paper proposes a novel finer-grained neuron identification algorithm to enhance LLMs' multilingual capabilities by detecting language neurons (including language-specific and language-related neurons) and language-agnostic neurons. - The proposed algorithm improves upon existing methods by more precisely categorizing neurons, leading to a more comprehensive understanding of LLMs' multilingual mechanisms. - The paper divides the LLMs' internal process for multilingual inference into four parts: multilingual understanding, shared semantic space reasoning, multilingual output space transformation, and vocabulary space outputting, providing a detailed analysis of each stage. - The study systematically analyzes models before and after alignment, focusing on different neuron types, and reveals how multilingual alignment significantly enhances the activation of relevant neuron types. - Empirical results demonstrate the effectiveness of the proposed algorithm and provide valuable insights into multilingual alignment and the multilingual capabilities of LLMs, including the phenomenon of "Spontaneous Multilingual Alignment". | ['Natural Language Processing', 'Translation'] | [Link](https://github.com/NJUNLP/Language-Neurons-Alignment) | [Link](https://huggingface.co/kevinpro/MistralMathOctopus-7B), [Link](https://huggingface.co/kevinpro/MetaMathOctopus-7B), [Link](https://huggingface.co/facebook/nllb-200-distilled-600M) |
| [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with
  Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952) | Yong Dai, Zhongwei Wan, Jiazhen Pan, Haozhe Wang, Che Liu | - AlphaMed is a novel medical LLM that achieves state-of-the-art results on six medical QA benchmarks by using reinforcement learning with minimalist rule-based rewards, without relying on supervised fine-tuning or distilled chain-of-thought data. - It surpasses larger and closed-source models, demonstrating the effectiveness of its minimalist approach. - The study shows that dataset informativeness is a key factor influencing reasoning performance. - Analysis reveals that minimalist rule-based RL can effectively incentivize reasoning in LLMs, even with small datasets. - The authors underscore the limitations of current medical QA benchmarks and call for more challenging, reasoning-oriented benchmarks to fully evaluate medical LLMs. | ['Question Answering'] | [Link](https://cheliu-computation.github.io/AlphaMed/) | N/A |
| [Active-O3: Empowering Multimodal Large Language Models with Active
  Perception via GRPO](https://arxiv.org/abs/2505.21457) | Zongze Du, Hao Zhong, MingyuLiu, Canyu, Z-MU-Z | - ACTIVE-03 is a novel reinforcement learning-based training framework that equips Multimodal Large Language Models (MLLMs) with active perception capabilities using Group Relative Policy Optimization (GRPO). - The model significantly enhances active perception capabilities compared to existing methods, such as Qwen-VL2.5-CoT, across various tasks including open-world object grounding and domain-specific scenarios like remote sensing and autonomous driving. - ACTIVE-03 employs a two-stage policy separating region proposal and task execution, incorporating structured instruction prompts and a dual-form reward design for effective training. - A comprehensive benchmark suite is established to evaluate ACTIVE-03, covering diverse tasks and scenarios. The code and evaluation protocols are publicly released. - Experiments demonstrate that ACTIVE-03 consistently improves search efficiency and accuracy under fixed computational budgets, showing remarkable zero-shot generalization on challenging tasks. | ['Multimodal', 'Reinforcement Learning', 'Robotics', 'Computer Vision', 'Object Detection', 'Image Segmentation', 'Zero-Shot Object Detection'] | [Link](https://github.com/aim-uofa/Active-03) | N/A |
| [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering
  Target Atoms](https://arxiv.org/abs/2505.20322) | Shumin Deng, Shengyu Mao, Ziwen Xu, Mengru Wang, Ningyu | - This paper introduces Steering Target Atoms (STA), a novel method for precise behavior control in large language models (LLMs). - STA manipulates disentangled knowledge components to enhance safety and reliability, surpassing conventional steering techniques. - Comprehensive experiments demonstrate STA's effectiveness, particularly in adversarial scenarios, exhibiting superior robustness and flexibility. - The method is applied to a large reasoning model, confirming its effectiveness in precise reasoning control. - An analysis comparing STA to prompting techniques reveals that STA provides finer-grained control and robustness. | ['Natural Language Processing'] | [Link](https://github.com/zjunlp/steer-target-atoms) | N/A |
| [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in
  Brain MRI](https://arxiv.org/abs/2505.14064) | Lena Schmitzer, Evamaria O. Riedel, Philipp Raffler, Jun Li, ci-ber | - This paper introduces NOVA, a new benchmark dataset for anomaly localization and clinical reasoning in brain MRI. - NOVA contains approximately 900 brain MRI scans spanning 281 rare pathologies and diverse acquisition protocols, along with detailed clinical narratives and expert annotations. - The benchmark evaluates models on three tasks: anomaly localization, image captioning, and diagnostic reasoning. - Experiments on state-of-the-art vision-language models reveal substantial performance drops across all tasks, highlighting the challenge of generalizing to truly unknown anomalies in medical imaging. - NOVA serves as a rigorous testbed for advancing models that can detect, localize, and reason about unseen anomalies in real-world clinical settings. | ['Multimodal'] | N/A | [Link](https://huggingface.co/datasets/Ano-2090/Nova) |
| [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in
  Vision-Language Models](https://arxiv.org/abs/2505.21500) | Hang Zhang, Yuchen Yan, Zixuan Wang, Hongxing Li, Dingming Li | - ViewSpatial-Bench, a comprehensive benchmark for evaluating multi-perspective spatial localization in vision-language models (VLMs), is introduced.  The benchmark contains over 5,700 question-answer pairs across five distinct task types, systematically assessing VLMs' spatial reasoning from both camera and human perspectives. - An automated 3D spatial annotation pipeline efficiently generates large-scale, precisely annotated multi-view datasets. This pipeline provides rich spatial relationship data for VLM training. - A Multi-View Spatial Model (MVSM) is developed and trained on a large-scale multi-viewpoint VQA dataset.  The model achieves a 46.24% overall performance improvement over baselines, demonstrating the effectiveness of the proposed methodology. - The study reveals a significant performance disparity between models' performance on camera-perspective and human-perspective tasks, highlighting a critical limitation in current VLMs' ability to generalize to allocentric viewpoints. - The MVSM is evaluated on VSI-Bench and a custom VSI-App dataset, showing that it generalizes well to tasks requiring perspective transformation in both indoor and outdoor scenarios. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics
  Reasoning](https://arxiv.org/abs/2505.19099) | Zirong Liu, Terry Jingchen Zhang, Kun Xiang, yinyahuang, HengLi29 | This paper introduces SEEPHYS, a large-scale multimodal benchmark for evaluating large language models' (LLMs) physics reasoning capabilities.  SEEPHYS comprises 2000 rigorously validated questions across seven core physics domains and 21 diagram types, spanning from middle school to PhD levels. The benchmark features a significant proportion of vision-essential problems (75%) where visual information is crucial for solving, revealing significant challenges for even the most advanced models, which achieve below 60% accuracy.   The analysis highlights challenges in coupling visual interpretation with physics reasoning and the models' over-reliance on textual cues.  The dataset and experimental results are available on GitHub and Hugging Face. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/SeePhys/seephys-project) | [Link](https://huggingface.co/datasets/SeePhys/SeePhys) |
| [Adversarial Attacks against Closed-Source MLLMs via Feature Optimal
  Alignment](https://arxiv.org/abs/2505.21494) | Chao Du, Tianyu Pang, Simeng Qin, Sensen Gao, jiaxiaojunQAQ |  - This paper introduces FOA-Attack, a novel targeted transferable adversarial attack method for multimodal large language models (MLLMs). - FOA-Attack achieves superior transferability compared to existing methods, especially against closed-source MLLMs, by aligning both global and fine-grained features. - The method uses a cosine similarity-based global feature loss and a local clustering optimal transport loss to refine feature alignment. - A dynamic ensemble model weighting strategy is employed to further enhance transferability. - Extensive experiments demonstrate that FOA-Attack outperforms state-of-the-art methods across various models. | ['Multimodal'] | [Link](https://github.com/jiaxiaojunQAQ/FOA-Attack) | N/A |
| [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.21178) | Mao Zheng, Nickyang | - This paper introduces ConciseR, a novel two-stage reinforcement learning framework for achieving concise reasoning in LLMs. - ConciseR addresses the overthinking phenomenon in LLMs by optimizing response length only after all rollouts are correct, following a "walk before you run" principle. - The framework uses Group Relative Policy Optimization with clip-higher and dynamic sampling in the first stage to improve reasoning capabilities and Length-aware Group Relative Policy Optimization in the second stage to enforce conciseness. - Experimental results demonstrate that ConciseR outperforms state-of-the-art reasoning models across various benchmarks, achieving significant improvements in accuracy and efficiency. - The code, training dataset, and model checkpoints will be publicly released. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/nick7nlp/ConciseR) | [Link](https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset), [Link](https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k), [Link](https://huggingface.co/datasets/EleutherAI/hendrycks_math) |
| [VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual
  Tool Selection](https://arxiv.org/abs/2505.20289) | Wen Xiao, Zefan Cai, Yuyang Ji, AniSundar18, ZeyiHuang1010 | - This paper introduces VisualToolAgent (VisTA), a novel reinforcement learning framework designed for visual tool selection in complex visual reasoning tasks. - VisTA dynamically explores, selects, and combines tools from a diverse library based on empirical performance, unlike previous methods that rely on training-free prompting or large-scale fine-tuning. - The framework uses Group Relative Policy Optimization (GRPO) to enable agents to autonomously discover effective tool-selection pathways without explicit reasoning supervision. - Experimental results on ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. - VisTA's ability to enhance generalization, adaptively utilize diverse tools, and improve efficiency highlights its potential for flexible, experience-driven visual reasoning systems. | ['Reinforcement Learning', 'Multimodal', 'Visual Question Answering'] | [Link](https://oodbag.github.io/vista_web/) | N/A |
| [Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic
  Capabilities in LLM Compression](https://arxiv.org/abs/2505.19433) | Xiaowen Chu, Lujun Li, Xiang Liu, Zhenheng Tang, Peijie Dong |  - This paper introduces Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression affects LLMs' agentic capabilities.  - ACBench evaluates 12 tasks across four capabilities: workflow generation, tool use, long-context understanding, and real-world application.  - The benchmark includes quantization and pruning methods (GPTQ, AWQ, Wanda, SparseGPT), and uses 15 models of different sizes and reasoning abilities.  - Experiments reveal compression trade-offs: 4-bit quantization preserves workflow generation and tool use but degrades real-world application accuracy.  -  Three novel metrics (ERank, Top-k Ranking Correlation, Energy) are introduced for systematic analysis of compression impacts. | ['Natural Language Processing'] | [Link](https://github.com/pprp/ACBench) | [Link](null) |
| [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs
  via Reinforcement Learning](https://arxiv.org/abs/2505.17005) | Zhipeng Chen, Wenqing Tian, Jinhao Jiang, Huatong Song, EliverQ | - R1-Searcher++, a novel framework, is introduced to enhance LLMs' ability to dynamically leverage both internal and external knowledge sources. - It employs a two-stage training strategy: an initial Supervised Fine-Tuning (SFT) phase for format learning followed by Reinforcement Learning (RL) for dynamic knowledge acquisition. - The RL stage incorporates outcome supervision to incentivize exploration, a reward mechanism for internal knowledge utilization, and a memorization mechanism to assimilate retrieved information. - Experimental results demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods, achieving efficient retrieval while reducing the number of retrievals. - The code is publicly available on GitHub. | ['Question Answering'] | [Link](https://github.com/RUCAIBox/R1-Searcher-plus) | N/A |
| [DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in
  Digital Forensics and Incident Response](https://arxiv.org/abs/2505.19973) | Saeed Alshehhi, Aaesha Aldahmani, Richard A. Dubniczky, Tamas Bisztray, Bilel Cherif | - This paper introduces DFIR-Metric, a novel benchmark dataset designed for evaluating the performance of Large Language Models (LLMs) in Digital Forensics and Incident Response (DFIR). - DFIR-Metric is composed of three modules: Knowledge Assessment (multiple-choice questions), Realistic Forensic Challenges (CTF-style tasks), and Practical Analysis (disk and memory forensics cases). - The benchmark rigorously evaluates LLMs across various DFIR tasks, focusing on technical accuracy and procedural rigor.  - A new metric, the Task Understanding Score (TUS), is introduced to enhance the evaluation by considering partial successes in complex multi-step tasks.  - All scripts, artifacts, and results are publicly available to ensure reproducibility and facilitate further research and development in the field. | ['Question Answering'] | [Link](https://github.com/DFIR-Metric) | N/A |
| [SoloSpeech: Enhancing Intelligibility and Quality in Target Speech
  Extraction through a Cascaded Generative Pipeline](https://arxiv.org/abs/2505.19314) | Kai Li, Chen Chen, Dongchao Yang, Jiarui Hai, Helin Wang | - This paper introduces SoloSpeech, a novel cascaded generative pipeline for target speech extraction (TSE). - SoloSpeech consists of three main components: a generative audio compressor, a generative target extractor, and a generative corrector. - The target extractor in SoloSpeech is speaker-embedding-free, utilizing conditional information from the cue audio's latent space. - Experimental results on the Libri2Mix dataset show that SoloSpeech achieves state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks. - SoloSpeech demonstrates strong generalization capabilities on out-of-domain data and real-world scenarios. | ['Audio-to-Audio'] | [Link](https://github.com/WangHelin1997/SoloSpeech) | [Link](https://wanghelin1997.github.io/SoloSpeech-Demo/) |
| [MLLMs are Deeply Affected by Modality Bias](https://arxiv.org/abs/2505.18657) | Yuanhuiyi Lyu, Kaiyu Lei, Yuqian Fu, Xu Zheng, Chenfei-Liao | - This paper investigates modality bias in Multimodal Large Language Models (MLLMs), demonstrating that MLLMs heavily rely on language while underutilizing other modalities. - The authors propose a research roadmap to mitigate modality bias, highlighting three key directions: measuring bias through benchmarks, avoiding bias through dataset construction, and reducing bias through specific methods. - Five key factors contributing to modality bias are identified: data characteristics, imbalanced backbone capabilities, training objectives, asymmetric modal backbone capabilities, and modal interactions and integrations. - Experiments demonstrate the influence of each factor, highlighting the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. - The authors call for interdisciplinary efforts to address these challenges, offering actionable suggestions for future research to advance progress towards Artificial General Intelligence. | ['Multimodal'] | N/A | N/A |
| [ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and
  Reactive Feedback](https://arxiv.org/abs/2505.17908) | Jinsong Zhou, Jiantao Lin, Luozhou Wang, Xinli Xu, Litao Guo | - ComfyMind is a general-purpose generative framework that uses tree-based planning and reactive feedback to improve the robustness and scalability of complex generation workflows. - It utilizes a semantic workflow interface (SWI) to abstract low-level node graphs into callable functional modules, enabling high-level composition and reducing structural errors. - ComfyMind's search tree planning mechanism models generation as a hierarchical decision process and allows for adaptive corrections at each stage, improving stability and flexibility. - Evaluations on three public benchmarks (ComfyBench, GenEval, and Reason-Edit) demonstrate that ComfyMind consistently outperforms existing open-source baselines. - The framework achieves performance comparable to GPT-Image-1, paving a path for open-source general-purpose generative AI systems. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Video', 'Image-to-Text', 'Any-to-Any'] | [Link](https://github.com/LitaoGuo/ComfyMind) | N/A |
| [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large
  Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673) | Yibo Wang, Min Yang, Jingyi Zhang, Qixiang Yin, Huanjin Yao | - Share-GRPO, a novel reinforcement learning approach, is proposed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs). - The method addresses the sparse reward and advantage vanishing issues in reinforcement learning by expanding the question space and sharing diverse reasoning trajectories. - Share-GRPO employs semantically consistent transformations to expand the question space, encouraging the exploration of diverse reasoning paths. - Reward information is shared during advantage computation, estimating advantages hierarchically, improving accuracy and stability. - Extensive evaluations on six reasoning benchmarks demonstrate Share-GRPO's superior performance compared to existing state-of-the-art methods. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | [Link](https://github.com/HJYao00/R1-ShareVL) | N/A |
| [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning
  of LLMs](https://arxiv.org/abs/2505.11277) | Junfeng Fang, Zhiyuan Liu, Chang Wu, Shihan Li, yrshi | - AutoRefine, a novel reinforcement learning post-training framework, is proposed to improve the reasoning capabilities of LLMs by incorporating a "search-and-refine-during-think" paradigm. - The model iteratively refines retrieved knowledge through explicit refinement steps between successive search calls, enhancing the accuracy of reasoning. - AutoRefine utilizes both outcome-based and retrieval-specific rewards, optimizing the model's ability to effectively retrieve and utilize relevant information. - Experiments on various QA benchmarks demonstrate that AutoRefine significantly outperforms existing methods, especially in complex multi-hop reasoning scenarios. - Detailed analysis shows that AutoRefine issues more frequent, higher-quality searches and synthesizes evidence effectively. | ['Question Answering'] | [Link](https://github.com/syr-cn/AutoRefine) | N/A |
| [Modality Curation: Building Universal Embeddings for Advanced Multimodal
  Information Retrieval](https://arxiv.org/abs/2505.19650) | Shi Feng, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, friedrichor |  - UNITE, a universal multimodal framework, addresses multimodal information retrieval challenges by introducing data curation and modality-aware training.   - It achieves state-of-the-art results on multiple benchmarks, surpassing existing methods by significant margins.    - The model architecture uses a large language model, a vision encoder, and a vision projector, handling various combinations of modalities.  -  A Modal-Aware Masked Contrastive Learning (MAMCL) strategy is used to improve the performance of training.  -  Extensive experiments demonstrate the effectiveness of UNITE across various retrieval scenarios. | ['Multimodal'] | [Link](https://friedrichor.github.io/projects/UNITE) | N/A |
| [Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion
  Enhances Protein Representations](https://arxiv.org/abs/2505.20052) | Ahmed Elnaggar, Mohamed Elkerdawy, Mohamed Elshaffei, Hazem Alsamkary | - This paper introduces Ankh3, a multi-task protein language model (PLM) that enhances protein representations by incorporating sequence denoising and completion tasks during pre-training. - Ankh3 leverages a T5 transformer architecture with an encoder-decoder structure and is trained on two tasks: masked language modeling with varying masking probabilities and protein sequence completion. - The model's performance was evaluated on various downstream tasks such as secondary structure prediction, fluorescence prediction, GB1 fitness, and contact prediction, demonstrating improved accuracy compared to previous Ankh models and competitive results against other state-of-the-art PLMs like ESM2 and ESM3. - The experiments show that Ankh3 benefits from the multi-task learning approach which leads to improved robustness and generalization across downstream tasks. - The Ankh3 models and pre-training data are made publicly available on HuggingFace for reproducibility and further research. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/agemagician/uniref50), [Link](https://huggingface.co/ElnaggarLab/ankh3-large), [Link](https://huggingface.co/ElnaggarLab/ankh3-xl) |
| [Beyond Simple Concatenation: Fairly Assessing PLM Architectures for
  Multi-Chain Protein-Protein Interactions Prediction](https://arxiv.org/abs/2505.20036) | Abdallah Amr, Sara Ossman, Mohamed Soudy, Mohamed Elshaffei, Hazem Alsamkary | - This paper introduces a meticulously curated version of the PPB-Affinity dataset containing 8,207 unique protein-protein interaction entries, addressing annotation inconsistencies and duplicates. - Four novel architectures for adapting PLMs to PPI binding affinity prediction are proposed and evaluated: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). - The HP and PAD architectures consistently outperform EC and SC methods, achieving up to a 12% increase in Spearman correlation, highlighting the importance of sophisticated architectural designs. - The study uses two training methods: full fine-tuning and a lightweight approach using ConvBERT heads. - Experiments were conducted across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3), demonstrating the superior performance of HP and PAD. | ['Natural Language Processing'] | [Link](https://github.com/Proteinea/ppiseq) | [Link](https://huggingface.co/datasets/proteinea/ppb_affinity) |
| [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561) | Fabrizio Silvestri, Yoelle Maarek, Guy Horowitz, Simone Filice, florin-hf | - This paper investigates the impact of positional bias in Retrieval Augmented Generation (RAG) systems, focusing on how the LLM's weighting of information based on its position in the prompt affects both relevant and distracting passages. - Through extensive experiments on three benchmarks, the authors demonstrate that state-of-the-art retrieval pipelines tend to retrieve highly distracting passages alongside relevant ones. - The results reveal that the impact of positional bias is marginal in real-world RAG scenarios since both relevant and distracting passages are penalized by the LLM's positional preferences. - Sophisticated strategies aimed at rearranging passages based on LLM positional preferences do not outperform random shuffling, suggesting that focusing on retrieval quality and LLM robustness is more crucial than optimizing passage ordering. - The study's findings highlight the need for future research to improve retrieval quality and reduce the LLM's susceptibility to distraction rather than focusing on mitigating positional bias. | ['Question Answering'] | N/A | N/A |
