

## Papers for 2025-05-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in
  Learning to Reason](https://arxiv.org/abs/2505.22653) | Rui Yan, Zhanhui Kang, Xingwu Sun, Ang Lv, Ruobing-Xie | - This paper investigates the impact of noisy rewards in reinforcement learning for large language models (LLMs) focusing on reasoning tasks. - The authors found that LLMs demonstrate strong robustness to substantial reward noise, even when a significant portion of the rewards are flipped randomly. - They propose a novel reasoning pattern reward (RPR) method, which rewards the appearance of key reasoning phrases in the model's output, regardless of the correctness of the final answer. - Experiments show that RPR, combined with noisy reward models, improves LLM performance on open-ended tasks and mitigates the impact of false negatives. - The findings highlight the importance of improving LLMs' foundational abilities during pre-training and provide insights for advancing post-training techniques. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason) | N/A |
| [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial
  Intelligence](https://arxiv.org/abs/2505.23747) | Yueqi Duan, Yi-Hsin Hung, Fangfu Liu, Diankun Wu | - This paper introduces Spatial-MLLM, a novel framework for visual-based spatial reasoning from 2D observations, enhancing the capabilities of existing video Multimodal Large Language Models (MLLMs). - The model architecture comprises a dual-encoder design: a pretrained 2D visual encoder to extract semantic features and a spatial encoder (initialized from a visual geometry model) to extract 3D structure features, integrated via a connector. - A space-aware frame sampling strategy is proposed, focusing on spatially informative frames for enhanced spatial understanding, even with limited token lengths. - Spatial-MLLM achieves state-of-the-art performance on various real-world datasets in a wide range of visual-based spatial understanding and reasoning tasks. - The model is trained on the Spatial-MLLM-120k dataset using supervised fine-tuning and Group Relative Policy Optimization (GRPO). | ['Video-Text-to-Text', 'Visual Question Answering', 'Multimodal'] | [Link](https://diankun-wu.github.io/Spatial-MLLM/) | N/A |
| [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762) | Yue Yu, Xuan Dong, Shi Liu, Shiqian Su, cyyang822 |  - ZeroGUI is a novel online learning framework for training GUI agents that eliminates the need for manual data annotation.   - It uses a VLM for automatic task generation and reward estimation, enabling continuous learning from the GUI environment.   - Experiments on two advanced GUI agents (UI-TARS and Aguvis) in OSWorld and AndroidLab show significant performance boosts compared to offline learning methods.  - ZeroGUI incorporates a two-stage reinforcement learning strategy: a first stage that trains on automatically generated tasks and a second stage for test-time adaptation.  - The proposed framework achieves zero human cost and scalability to diverse GUI environments. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/OpenGVLab/ZeroGUI) | N/A |
| [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software
  Engineering](https://arxiv.org/abs/2505.23604) | Subhro Das, Zhenting Qi, Delin Chen, Guangtao Zeng, maohaos2 | This paper introduces EvoScale, a novel sample-efficient test-time scaling method for improving the performance of small language models on software engineering tasks.  EvoScale leverages reinforcement learning to enable self-evolution, eliminating the need for external reward models during inference.  Experimental results show that Satori-SWE-32B, a 32B parameter model using EvoScale, achieves performance comparable to models exceeding 100B parameters on the SWE-Bench-Verified dataset.  EvoScale reduces the number of samples required by iteratively refining outputs through selection and mutation. The model learns to self-improve, making it more efficient and reducing sampling costs. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/satori-reasoning/Satori-SWE) | [Link](None) |
| [VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video
  Reasoning?](https://arxiv.org/abs/2505.23359) | Lin Sui, Yi Liu, Haoning Wu, Yuanxin Liu, RUBBISHLIKE | - This paper introduces VIDEOREASONBENCH, a new benchmark designed to evaluate vision-centric complex video reasoning capabilities of large language models (LLMs). - VIDEOREASONBENCH features videos depicting sequences of fine-grained operations on a latent state, requiring models to recall visual information, infer latent states, and predict future information. - Evaluation of 18 state-of-the-art MLLMs reveals that most perform poorly on complex video reasoning, with GPT-4 achieving only 6.9% accuracy. - Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy, highlighting the importance of extended thinking chains for complex video reasoning. - The benchmark shows that the accuracy of models drastically degrades when visual information is reduced or the 'thinking mode' is disabled, highlighting strong vision reliance. | ['Video Classification', 'Visual Question Answering', 'Multimodal'] | [Link](https://github.com/llyx97/video_reason_bench) | [Link](huggingface.co/datasets/lyx97/reasoning_videos) |
| [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646) | Junfeng Fang, Jianhui Chen, Yanxu Chen, Yantao Liu, Zijun Yao | - This paper investigates whether reasoning models are more prone to hallucination than non-reasoning models. - The authors conduct a holistic evaluation of hallucination in large reasoning models (LRMs) across various post-training pipelines, including supervised fine-tuning (SFT) and reinforcement learning (RL). - They identify two key cognitive behaviors that affect the factuality of LRMs: Flaw Repetition and Think-Answer Mismatch. - The findings suggest that LRMs developed with both SFT and RL are generally less prone to hallucination, while RL-only and SFT-only LRMs show higher rates of hallucination. - Finally, the authors analyze the mechanism behind hallucination from the perspective of model uncertainty, showing that increased hallucination is often associated with misalignment between model uncertainty and factual accuracy. | ['Question Answering'] | [Link](https://github.com/THU-KEG/LRM-FactEval) | N/A |
| [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement
  Learning](https://arxiv.org/abs/2505.22914) | Ilya Zisman, Alexander Nikulin, Denis Tarasov, Maksim Kolodiazhnyi, zhemchuzhnikov | - The paper introduces cadrille, a novel multi-modal CAD reconstruction model that leverages vision-language models (VLMs) and processes point cloud, image, and text modalities simultaneously. - The model employs a two-stage training pipeline: supervised fine-tuning (SFT) on procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback. - cadrille outperforms existing single-modal approaches on the DeepCAD benchmark across all three input modalities. - The study shows that online RL algorithms, such as Group Relative Preference Optimization (GRPO), are superior to offline methods for CAD reconstruction. - After RL fine-tuning, cadrille establishes new state-of-the-art results on three challenging datasets, including a real-world dataset. | ['Text-to-3D', 'Image-to-3D', 'Multimodal', 'Reinforcement Learning'] | N/A | N/A |
| [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088) | Roi Reichart, Liat Ein-Dor, Nitay Calderon | This paper introduces a novel, fully automated method for generating local and global concept-based explanations of preferences across multiple domains.  A hierarchical multi-domain regression (HMDR) model is proposed to capture both domain-general and domain-specific effects on preferences.  The method achieves strong preference prediction performance, outperforming baselines.  Furthermore, the paper demonstrates two application-driven settings where the generated explanations successfully improve the performance of LLM-as-judges. | ['Natural Language Processing', 'Text Classification', 'Text Generation'] | [Link](https://github.com/nitaytech/PrefExplain) | N/A |
| [UniRL: Self-Improving Unified Multimodal Models via Supervised and
  Reinforcement Learning](https://arxiv.org/abs/2505.23380) | Mike Zheng Shou, Zhenheng Yang, Weijia Mao | - This paper introduces UniRL, a self-improving post-training approach for unified multimodal models that requires no external image data. - UniRL enables the model to generate images from prompts and use them as training data in each iteration, allowing the model to improve both generation and understanding tasks simultaneously. - The model is optimized using supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), which enables the two tasks to enhance each other. - UniRL achieves a GenEval score of 0.77 for Show-o and 0.65 for Janus, outperforming several existing baselines and demonstrating its effectiveness in improving both generation and understanding performance. - The method also reduces the imbalance between generation and understanding in unified multimodal models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Reinforcement Learning'] | [Link](https://github.com/showlab/UniRL) | N/A |
| [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419) | Bowen Li, Yu Kang, Chaoyun Zhang, Shilin He, Linghao Zhang | - This paper introduces SWE-bench-Live, a continuously updated benchmark for evaluating large language models (LLMs) on real-world issue-resolution tasks. - The benchmark addresses limitations of existing benchmarks, such as staleness, limited repository coverage, and heavy reliance on manual effort. - SWE-bench-Live uses REPOLAUNCH, an automated curation pipeline, to streamline the entire process from instance creation to environment setup. - Evaluations reveal a substantial performance gap compared to static benchmarks, highlighting the importance of dynamic, contamination-resistant evaluation. - The benchmark provides a diverse set of tasks derived from real GitHub issues, facilitating rigorous evaluation of LLMs in dynamic, real-world settings. | ['Natural Language Processing'] | [Link](https://github.com/SWE-bench-Live) | [Link](https://huggingface.co/SWE-bench-Live) |
| [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255) | Nikita Balagansky, Daniil Gavrilov, Daniil Laptev, Yaroslav Aksenov, Vadim Kurochkin | - This paper introduces KronSAE, a novel sparse autoencoder (SAE) architecture that improves the efficiency of training SAEs by factorizing the latent representation using the Kronecker product and employing a differentiable AND-like activation function. - KronSAE significantly reduces the computational and memory overhead compared to traditional SAEs, especially when dealing with large dictionary sizes, making it suitable for scaling to large language models. - Experimental results on several language models demonstrate that KronSAE achieves comparable or better reconstruction fidelity with significantly fewer parameters than existing SAE models, particularly under tight compute constraints. - The authors also show that KronSAE improves the interpretability of the latent features by reducing feature absorption, a common problem in sparse autoencoders. - The improved efficiency and interpretability of KronSAE suggest that it could be a valuable tool for interpreting and probing the internals of large language models. | ['Natural Language Processing'] | N/A | N/A |
| [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV
  Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618) | Shizhe Diao, Hao Zhang, Chengyue Wu, zhijianliu, Cauthyyy | - This paper introduces Fast-dLLM, a novel method to accelerate inference in diffusion-based large language models (LLMs) without additional training. - Fast-dLLM incorporates a block-wise approximate key-value (KV) cache mechanism and a confidence-aware parallel decoding strategy. - The KV cache reuse mechanism is designed for bidirectional diffusion models, minimizing the performance drop. - The confidence-aware parallel decoding addresses the quality degradation in parallel decoding by selectively decoding tokens exceeding a confidence threshold. - Experiments on LLaDA and Dream models across multiple benchmarks demonstrate a speed improvement of up to 27.6x with minimal accuracy loss. | ['Text Generation'] | N/A | N/A |
| [Muddit: Liberating Generation Beyond Text-to-Image with a Unified
  Discrete Diffusion Model](https://arxiv.org/abs/2505.23606) | Kaidong Yu, Wenhao Chai, Zhuoran Zhao, BryanW, QingyuShi | - Muddit is a novel unified discrete diffusion transformer model that performs fast and parallel generation across text and image modalities. - The model integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation. - Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency on various benchmarks, including GenEval, CIDEr, VQAv2, MME, and GQA. - The model's architecture comprises a text encoder, image encoder, transformer generator, sampler, text decoder, and image decoder. The generator is initialized from a pre-trained text-to-image backbone to leverage strong image priors. - The work demonstrates that discrete diffusion, when equipped with strong visual priors, is a promising approach for building scalable and efficient unified generation models. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/M-E-AGI-Lab/Muddit) | N/A |
| [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735) | Yuan Deng, Majid Daliri, Praneeth Kacham, Zeman Li, Ali Behrouz |  - The paper introduces ATLAS, a novel long-term memory module designed to enhance the context memorization capabilities of recurrent neural networks.  - ATLAS addresses limitations of existing methods by using a sliding window update rule and optimizing memory based on past and current tokens, overcoming the online nature of previous models.  - The model architecture incorporates higher-order feature mappings to increase memory capacity and utilizes the Muon optimizer for efficient parallel training.  - Experiments on language modeling, common-sense reasoning, and long-context understanding tasks demonstrate that ATLAS surpasses the performance of Transformers and recent linear recurrent models, achieving significant improvements in long-context performance (+80% accuracy at 10M context length on the BABILong benchmark). - Theoretical justifications are provided to support the enhancements in memory capacity and the effectiveness of the proposed learning rules and optimization methods. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416) | Sangdoo Yun, Jae W. Lee, Sangwoo Kwon, jusjinuk, Jang-Hyun | - This paper introduces KVzip, a novel query-agnostic key-value (KV) cache eviction method for large language models (LLMs). - KVzip quantifies the importance of KV pairs by reconstructing the original contexts from compressed caches, subsequently evicting pairs with lower importance. - Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4x and FlashAttention decoding latency by approximately 2x, with negligible performance loss. - KVzip outperforms existing query-aware KV eviction methods, which suffer from performance degradation in multi-query scenarios. - The proposed method is evaluated on various models and datasets, showing consistent improvement across diverse benchmarks. | ['Question Answering'] | [Link](https://github.com/snu-mllab/KVzip) | N/A |
| [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559) | Ziheng Qi, Jiaxun Zhang, HakHan, m-serious, Leozkl | - This paper introduces SafeScientist, a novel AI scientist framework designed to enhance safety and ethical responsibility in AI-driven scientific exploration. - SafeScientist proactively refuses ethically inappropriate or high-risk tasks and integrates multiple defensive mechanisms, including prompt monitoring and an ethical reviewer component. - The framework is evaluated using SciSafetyBench, a new benchmark comprising 240 high-risk scientific tasks across six domains and 30 specially designed scientific tools. - Experiments demonstrate that SafeScientist improves safety performance by 35% compared to traditional AI scientist frameworks without compromising scientific output quality. - The robustness of SafeScientist is further validated against diverse adversarial attack methods. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/ulab-uiuc/SafeScientist) | N/A |
| [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961) | Jiaxuan You, m-serious, HakHan | - This paper introduces ToMAP, a novel framework for training opponent-aware large language model (LLM) persuaders by incorporating two theory-of-mind (ToM) modules. - ToMAP utilizes a counterclaim predictor to anticipate potential objections and an opponent attitude predictor to estimate the persuadee's stance on these counterclaims. - The model, with only 3 billion parameters, outperforms larger baselines like GPT-40 by a relative gain of 39.4% across multiple datasets. - Experiments demonstrate that ToMAP generates more diverse and effective arguments, reducing repetition and exhibiting complex reasoning chains. - Ablation studies highlight the importance of both ToM modules and reinforcement learning for achieving effective persuasion. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/ulab-uiuc/ToMAP) | N/A |
| [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient
  Interactions](https://arxiv.org/abs/2505.17818) | Jae Ho Sohn, Jiho Kim, Seongsu Bae, Hyunseung Chung, Daeun Kyung | This paper introduces PatientSim, a novel patient simulator designed to generate realistic and diverse patient personas for training and evaluating doctor LLMs in realistic multi-turn doctor-patient interaction settings.  PatientSim leverages real-world clinical data from MIMIC-ED and MIMIC-IV datasets, generating 37 unique combinations of persona traits.  The authors evaluate eight different LLMs, finding Llama 3.3 performs best at generating realistic responses based on diverse patient personas.  Clinical validation confirms the simulator's robustness, demonstrating its usefulness as a scalable and privacy-compliant tool for research and education. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural
  Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754) | Qiuzhi Liu, Tian Liang, Zhiwei He, Jiahao Xu, Ziyin Zhang |  - DeepTheorem is a novel framework that uses natural language to enhance LLMs' mathematical reasoning abilities for theorem proving.  - It introduces a large-scale benchmark dataset of 121K high-quality informal mathematical theorems and proofs, annotated for correctness, difficulty, and topic categories.  - A novel reinforcement learning strategy, RL-Zero, is proposed to improve LLM performance in informal theorem proving.  - Comprehensive evaluation metrics assess proof correctness and reasoning quality.  - DeepTheorem significantly outperforms existing methods and datasets, showcasing its potential to advance automated informal theorem proving. | ['Natural Language Processing'] | [Link](https://github.com/Jiahao004/DeepTheorem) | [Link](https://huggingface.co/datasets/Jiahao004/DeepTheorem) |
| [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic
  Reasoning in Chest X-rays](https://arxiv.org/abs/2505.18087) | Hyuk Gi Hong, Hangyul Yoon, Jung-Oh Lee, Geon Choi, ttumyche |  - This paper introduces CheXStruct and CXReasonBench, a structured pipeline and benchmark for evaluating structured diagnostic reasoning in chest X-rays. - CheXStruct automatically extracts clinically relevant reasoning steps from chest X-rays, including segmentation, measurements, and indices. - CXReasonBench leverages CheXStruct to evaluate model reasoning through multiple intermediate steps, supporting multi-path, multi-stage evaluation. - The benchmark includes 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, with up to 4 visual inputs per pair. - Even strong LVLMs struggle with structured reasoning and generalization, highlighting the challenge of linking abstract knowledge with anatomical visual interpretation. | ['Multimodal', 'Visual Question Answering', 'Mask Generation'] | [Link](https://github.com/ttumyche/CXReasonBench) | N/A |
| [Differential Information: An Information-Theoretic Perspective on
  Preference Optimization](https://arxiv.org/abs/2505.23761) | Minjoon Seo, Hyeonbin Hwang, Hyunji Lee, yunjae-won | This paper introduces the Differential Information Distribution (DID) to analyze Direct Preference Optimization (DPO).  The DID captures information gained during policy updates in DPO, revealing that the log-ratio reward is uniquely optimal under specific conditions. The paper explores the relationship between DID entropy and policy dynamics, offering insights into log-likelihood displacement. Experiments on synthetic data validate the theoretical findings, demonstrating that DPO using the log-ratio reward optimally recovers the target policy under conditions where preferences encode DID.  Real-world instruction-following datasets are analyzed, showcasing that high-entropy DID is crucial for general instruction-following tasks, while low-entropy DID benefits knowledge-intensive tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | N/A | N/A |
| [ZeroSep: Separate Anything in Audio with Zero Training](https://arxiv.org/abs/2505.23625) | Yunlong Tang, Susan Liang, Junxuan Huang, Yuesheng Ma, Chao Huang | - ZeroSep is a novel zero-shot audio source separation framework that leverages pre-trained text-guided audio diffusion models. - It achieves this by inverting mixed audio into the diffusion model's latent space and using text conditioning to guide the denoising process to recover individual sources. - ZeroSep outperforms existing supervised and unsupervised methods on multiple separation benchmarks, demonstrating its effectiveness and efficiency. - It inherently supports open-set scenarios due to its reliance on rich textual priors from the pre-trained model. - The method is compatible with various pre-trained audio diffusion models, showcasing its versatility and adaptability. | ['Audio-to-Audio'] | [Link](https://wikichao.github.io/ZeroSep/) | N/A |
| [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765) | Yossi Adi, gallilmaimon, iyosha | - This paper introduces StressTest, a benchmark designed to evaluate speech language models' ability to understand sentence stress. - It introduces a novel synthetic data generation pipeline and creates Stress-17k, a training set that simulates changes in meaning implied by stress variation. - The paper empirically shows that fine-tuning models with Stress-17k improves performance on sentence stress reasoning and detection tasks. - A model called StresSLM significantly outperforms existing models on both sentence stress reasoning and detection tasks. - The results demonstrate the importance of sentence stress for understanding spoken language and highlight the limitations of existing models in this area. | ['Audio', 'Automatic Speech Recognition', 'Natural Language Processing'] | N/A | N/A |
| [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282) | Bryan Dai, Joey Zhou, Lynx Chen, zgao3186 | - This paper introduces one-shot entropy minimization (EM), a novel post-training method for large language models (LLMs) that requires only a single unlabeled data point and 10 optimization steps.  - The proposed method surpasses the performance of existing reinforcement learning (RL) methods which use thousands of data points and carefully designed rewards, as demonstrated by the experiments on various mathematical reasoning benchmarks.  - EM's effectiveness stems from its ability to reduce the model's uncertainty by minimizing token-level entropy, which results in a rightward shift of the logits distribution.  - The study reveals that EM is a distribution-shaping tool rather than a learning method, as evidenced by the inconsistency between the loss-reasoning curve and the logit shift effect.  - Further research is warranted to investigate the full potential of EM, including its applicability to other domains and its combination with other post-training techniques. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/zitian-gao/one-shot-em) | N/A |
| [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360) | Ryan A. Rossi, Nedim Lipka, Manan Suri, Franck-Dernoncourt, puneetm | - This paper introduces ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques and set-of-marks prompting with multimodal LLMs for fine-grained visual attribution. - ChartLens improves fine-grained attributions by 26-66% compared to baselines. - The authors introduce ChartVA-Eval, a new benchmark with synthetic and real-world charts from diverse domains, featuring fine-grained attribution annotations. - ChartLens leverages Segment Anything Model (SAM) for instance segmentation and Lineformer for line segmentation to extract visual features for attribution. -  The proposed method employs a set-of-marks prompting technique with multimodal LLMs to facilitate accurate attribution. | ['Visual Question Answering', 'Multimodal', 'Image Segmentation', 'Mask Generation', 'Question Answering'] | N/A | N/A |
| [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of
  Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943) | Gunhee Kim, Dayoon Ko, Heeseung Yun, ahnpersie | This paper introduces Multimodal Adversarial Compositionality (MAC), a benchmark to evaluate the compositional vulnerabilities of pre-trained multimodal representations like CLIP.  MAC leverages LLMs to generate deceptive text samples that exploit these vulnerabilities, evaluating both sample-wise attack success rate and group-wise entropy-based diversity.  A self-training approach with diversity-promoting filtering is proposed to improve zero-shot methods.  Experiments show the approach's superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.  The proposed method outperforms existing approaches across multiple modalities. | ['Multimodal'] | N/A | N/A |
| [When Models Reason in Your Language: Controlling Thinking Trace Language
  Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888) | Danielle S. Bitterman, Raquel Fernández, Zidi Xiong, Shan Chen, Jirui Qi | - This paper introduces XReasoning, a novel multilingual reasoning benchmark to evaluate the capabilities of Large Reasoning Models (LRMs) in various languages. - The study reveals a significant trade-off between the accuracy of LRM answers and their ability to generate thinking traces in the user's specified language. - The researchers demonstrate that prompt hacking can improve the thinking trace language matching rate but at the cost of reduced answer accuracy. - Post-training with a small number of instances can mitigate the language mismatch problem but also leads to a decrease in accuracy. - This work highlights the need for future research to improve the multilingual reasoning capabilities and user-friendliness of LRMs. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Betswish/mCoT-XReasoning) | [Link](https://huggingface.co/collections/shanchen/xreasoning-models-68377e15a2e86143dc4b0383), [Link](https://huggingface.co/collections/shanchen/xreasoning-681e7625c7a9ec4111a634b6) |
| [CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian
  Splatting](https://arxiv.org/abs/2505.22854) | Marcin Mazur, Tadeusz Dziarmaga, Piotr Borycki, Joanna Waczyńska, Kornel Howil | - CLIPGaussian is a novel universal style transfer model that operates directly on Gaussian primitives, supporting various data modalities such as images, videos, 3D objects, and 4D dynamic scenes. - The model architecture leverages a two-stage training process: first training a Gaussian Splatting model tailored to a specific data modality, then using a composite loss function (content preservation, background preservation, local style transfer, and global style transfer) to leverage training images, randomly sampled patches, and conditioning inputs (image or text) in the feature spaces of VGG-19 and CLIP models. - CLIPGaussian demonstrates superior style fidelity and consistency compared to existing baselines (I-GS2GS, DGE, StyleGaussian, and G-Style) across various tasks and modalities, as shown through quantitative comparisons (CLIP-S, CLIP-SIM, CLIP-F, CLIP-CONS) and user studies. - It integrates as a plug-in module into existing Gaussian Splatting pipelines without requiring retraining or large generative models, enhancing efficiency and versatility. -The method achieves temporal coherence in videos, handles multiple data modalities through a unified architecture, and performs end-to-end optimization of Gaussian parameters, enabling joint optimization of both color and geometry. | ['Multimodal', 'Image-to-Image', 'Image-to-Video', 'Text-to-Image', 'Text-to-Video', 'Image-to-3D', 'Text-to-3D'] | [Link](https://github.com/kornelhowil/CLIPGaussian) | N/A |
| [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199) | Ruichuan An, Renrui Zhang, Joey Tsai, Shilin Yan, Pengxiang Li | - This paper introduces Adaptive Classifier-Free Guidance (A-CFG), a novel method to enhance the controllability of iterative masked language models during text generation. - A-CFG dynamically adjusts the unconditional input in Classifier-Free Guidance (CFG) based on the model's predictive confidence, focusing guidance on uncertain areas. - Experiments on various benchmarks demonstrate that A-CFG significantly improves the performance of LLaDA, outperforming standard CFG and achieving substantial gains in accuracy. - A-CFG's effectiveness is demonstrated across different tasks including general language understanding, mathematical reasoning, and planning, highlighting its adaptability. - The method improves the results on multiple benchmarks, for instance, a 3.9 point gain on GPQA and an 8.0 point improvement on the Sudoku task, showcasing A-CFG's efficacy. | ['Text Generation'] | [Link](https://github.com/pixeli99/A-CFG) | N/A |
| [Evaluating Text Creativity across Diverse Domains: A Dataset and Large
  Language Model Evaluator](https://arxiv.org/abs/2505.19236) | Fang Luo, Yahui Liu, Yuzhuo Yuan, Xiting Wang, Aman | This paper introduces CreataSet, a large-scale dataset with over 100K human-level and 1M+ synthetic creative instruction-response pairs across diverse domains.  A novel pairwise-comparison framework is proposed for evaluating text creativity, improving evaluation consistency.  An LLM-based evaluator, CrEval, is developed and trained on CreataSet, demonstrating superior performance over existing methods in alignment with human judgments.  CrEval shows significant improvements in F1-score, Kappa score, and agreement rate compared to strong baselines,  and exhibits strong generalization capabilities across various domains.  All data, code, and models are publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://creval-creative-evaluation.github.io/) | N/A |
