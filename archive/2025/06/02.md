

## Papers for 2025-06-02

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/abs/2505.24863) | Haoran Geng, Xuying Ning, Han Wang, RunpeiDong, jyzhang1208 |  - ALPHAONE (a1) is a novel framework designed to control the reasoning process in large reasoning models (LRMs) at test time.   - The framework introduces an 'alpha moment' which dynamically schedules slow and fast thinking transitions, thereby improving efficiency and generalizing existing methods.  -  a1 models the insertion of reasoning transition tokens as a Bernoulli stochastic process before the alpha moment, transitioning deterministically to fast reasoning afterwards.  - Extensive experiments across various benchmarks (mathematical, coding, scientific) demonstrate a1's superior reasoning capability and efficiency compared to baseline methods, achieving significant improvements in accuracy and efficiency.  - The paper analyzes various scheduling strategies for the activation of slow thinking, finding that a "slow thinking first, then fast thinking" approach yields better results. | ['Question Answering'] | [Link](https://alphaone-project.github.io/) | [Link](string) |
| [Don't Look Only Once: Towards Multimodal Interactive Reasoning with
  Selective Visual Revisitation](https://arxiv.org/abs/2505.18842) | Min Soo Kim, Jaeyoung Lee, Jiwan Chung, siyeolkim, kjunh | - This paper introduces v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that allows for selective visual revisitation during inference. - v1 incorporates a point-and-copy mechanism that enables the model to dynamically retrieve relevant image regions throughout the reasoning process, augmenting existing architectures with minimal modifications. - The authors create v1g, a dataset of 300K multimodal reasoning traces with visual grounding annotations, to train this capability. - Experiments on three multimodal mathematical reasoning benchmarks demonstrate consistent performance improvements over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. - The results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. | ['Multimodal'] | N/A | N/A |
| [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and
  Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878) | Xiaohan Zhao, Jiacheng Liu, Zhaoyi Li, Yaxin Luo, jiachengcui888 | - The paper introduces OpenCaptchaWorld, a web-based benchmark designed to evaluate the visual reasoning and interaction capabilities of multimodal large language models (MLLMs) through diverse CAPTCHA puzzles. - The benchmark comprises 20 modern CAPTCHA types, totaling 225 CAPTCHAs, and is annotated with a new metric, CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. - Experimental results reveal that humans consistently achieve near-perfect scores, while state-of-the-art MLLM agents struggle significantly, with success rates far below human-level performance. - OpenCaptchaWorld serves as a vital benchmark for diagnosing the limitations of current multimodal agents and guiding the development of more robust multimodal reasoning systems. - The platform is designed to test generalization and reasoning depth, not memorization from massive data. | ['Multimodal'] | [Link](https://github.com/MetaAgentX/OpenCaptchaWorld) | [Link](https://huggingface.co/spaces/OpenCaptchaWorld/platform) |
| [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/abs/2505.24196) | Ziqiang Liu, Lu Wang, Huiming Wang, Renke Shan, Longze Chen | - This paper introduces CLaSp, a novel in-context layer-skipping strategy for self-speculative decoding that accelerates the decoding process of large language models. - Unlike previous methods, CLaSp does not require additional modules or training, employing a plug-and-play mechanism by skipping intermediate layers of the verify model. - CLaSp uses a dynamic programming algorithm to optimize the layer-skipping process, dynamically adjusting its strategy after each verification stage without pre-optimized sets of skipped layers. - Experimental results demonstrate that CLaSp achieves a speedup of 1.3x~1.7x on LLaMA3 series models without altering the original distribution of generated text. - The method's efficiency stems from its dynamic adjustment to layer skipping based on the current context, eliminating the need for pre-optimization or retraining. | ['Text Generation'] | N/A | N/A |
| [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858) | Tim G. J. Rudner, Idan Szpektor, Avi Caciularu, Gal Yona, Gabrielle Kaili-May Liu | The paper introduces MetaFaith, a novel prompt-based calibration approach for Large Language Models (LLMs) that improves the alignment between a model's intrinsic uncertainty and its linguistically expressed uncertainty.  MetaFaith leverages metacognitive prompting strategies to elicit more faithful expressions of uncertainty.  Experiments across various LLMs, datasets, and prompting strategies demonstrate that MetaFaith significantly outperforms existing methods, achieving up to a 61% improvement in faithfulness.  Human evaluations further confirm the effectiveness of MetaFaith, showing an 83% win rate over baseline models. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/yale-nlp/MetaFaith) | N/A |
| [Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual
  Large Language Models](https://arxiv.org/abs/2505.20873) | Joon Son Chung, Jongmin Choi, Youngjoon Jang, Chae0 | - This paper introduces Fork-Merge Decoding (FMD), a novel inference-time strategy designed to enhance multimodal understanding in audio-visual large language models (AV-LLMs) without requiring additional training or architectural modifications. - FMD involves a two-stage decoding process: a fork phase, where audio and video inputs are processed separately; and a merge phase, where the resulting representations are combined for joint reasoning. - Experiments on two AV-LLMs, VideoLLaMA2 and video-SALMONN, across three benchmark datasets (AVQA, MUSIC-AVQA, and AVHBench) demonstrate consistent performance improvements compared to existing methods. - The attention-guided fusion mechanism in FMD promotes balanced modality contributions, effectively mitigating modality bias and enhancing robust multimodal understanding. - The proposed FMD is computationally efficient and compatible with both token-wise and channel-wise fusion strategies, improving inference speed while achieving higher accuracy. | ['Multimodal', 'Video Classification', 'Visual Question Answering', 'Audio Classification'] | N/A | N/A |
| [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293) | jamesgolden1 | - This paper demonstrates that the inference operations of several large language models (LLMs) can be mapped to an exactly equivalent linear system for a given input sequence. - The authors achieve this by strategically altering the gradient computation with respect to the input sequence, without modifying model weights or altering output predictions. - This approach reveals that LLMs operate in extremely low-dimensional subspaces, even with their expressive power and global nonlinearity. - The authors demonstrate this across multiple LLMs and show that many of the largest singular vectors decode to concepts related to the most-likely output token, providing insights into internal representations. - This technique enables examination of each layer's operation as nearly-exact linear systems and reveals interpretable semantic structures in next-token prediction. | ['Text Generation'] | [Link](https://github.com/jamesgoldenl/llms-are-llms) | N/A |
| [Harnessing Large Language Models for Scientific Novelty Detection](https://arxiv.org/abs/2505.24615) | Erik Cambria, Thanh-Son Nguyen, Soujanya Poria, Yan Liu, ZonglinY | - This paper proposes a novel method for scientific novelty detection using large language models (LLMs). - Two new benchmark datasets in marketing and NLP are introduced to evaluate the method. - The method leverages LLMs to construct datasets by extracting closure sets of papers and summarizing their main ideas. - A lightweight retriever is trained using knowledge distillation from LLMs to align ideas with similar conceptions. - Experiments demonstrate that the proposed method consistently outperforms existing methods on the benchmark datasets. | ['Natural Language Processing'] | [Link](https://anonymous.4open.science/r/NoveltyDetection-10FB/) | N/A |
| [un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via
  Inverting unCLIP](https://arxiv.org/abs/2505.24517) | Shiguang Shan, Ruibing Hou, Hong Chang, Jiahe Zhao, yinqi | - This paper introduces un²CLIP, a novel method to improve CLIP's visual detail capturing ability by inverting the unCLIP model. - un²CLIP finetunes the CLIP image encoder using a pretrained unCLIP image generator, transferring the generator's rich visual knowledge into the encoder while preserving its alignment with the original text encoder. - The method is evaluated on various tasks, including the MMVP-VLM benchmark, dense prediction, and multimodal large language model tasks, showing significant improvements over the original CLIP and previous methods. - un²CLIP addresses the limitation of CLIP in capturing visual details without modifying the network architecture or requiring additional training data. - The proposed method achieves state-of-the-art results on several benchmark datasets, demonstrating its effectiveness in improving CLIP for various visual tasks. | ['Multimodal', 'Image Feature Extraction', 'Zero-Shot Image Classification'] | [Link](https://github.com/LiYinqi/un2CLIP) | N/A |
| [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,
  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009) | Alex Smola, Mu Li, Xingjian Shi, Yuzhi Tang, ruskinmanku |  - The paper introduces EmergentTTS-Eval, a comprehensive benchmark for evaluating TTS models on complex prosodic, expressiveness, and linguistic challenges.  - It uses a model-as-a-judge approach, employing a Large Audio Language Model (LALM) to automate evaluation across multiple dimensions.  - The benchmark includes 1,645 diverse test cases generated iteratively using LLMs, covering six challenging scenarios.  - The model-as-a-judge approach demonstrates high correlation with human preferences and provides robust assessment of TTS systems.  - EmergentTTS-Eval is open-sourced, allowing for easy extensibility and reproducibility. | ['Text-to-Speech'] | [Link](https://github.com/boson-ai/EmergentTTS-Eval-public) | [Link](https://huggingface.co/datasets/bosonai/EmergentTTS-Eval) |
| [Enabling Flexible Multi-LLM Integration for Scalable Knowledge
  Aggregation](https://arxiv.org/abs/2505.23844) | Xin Meng, Yifan Gong, Shiyue Hou, Zheng Zhan, Zhenglun Kong | - This paper introduces a novel framework for integrating multiple large language models (LLMs) to enhance knowledge aggregation. - The framework incorporates an adaptive selection network to dynamically select the most relevant LLMs for a given task, along with a dynamic weighted fusion strategy and a feedback-driven loss function to reduce knowledge interference. - Experimental results demonstrate that the proposed method significantly improves performance compared to existing approaches while reducing knowledge interference by up to 50%. - The adaptive selection network efficiently evaluates diverse LLMs and chooses the subset that best improves the fused model's performance, mitigating the interference issues. - The method achieves stability and scalability without increasing the target model's parameter size or computation costs. | ['Question Answering'] | [Link](https://github.com/ZLKong/LLM_Integration) | N/A |
| [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157) | Yvan Peter, Julian Alvarez, Walter Nuninger, yelboudouri | - This paper introduces Role-Playing Eval (RPEval), a novel benchmark designed to assess Large Language Model (LLM) role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. - RPEval uses single-turn interactions to ensure cost efficiency and reproducibility, focusing on dimensions easily verifiable with automated methods. - The benchmark was constructed using a character profile generator and OpenAI's GPT-40 to create a diverse set of characters and scenarios, which were annotated through crowdsourcing. - Evaluation results on GPT-40, Gemini-1.5-Pro, and Llama 3.2 1B showed that Gemini-1.5-Pro achieved the highest average score, demonstrating balanced performance across dimensions. - RPEval's design choices, such as focusing on single-turn interactions, offer efficiency and reproducibility but limit the assessment of more nuanced, long-term role-playing attributes. | ['Natural Language Processing'] | [Link](https://github.com/yelboudouri/RPEval) | N/A |
| [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements
  Generation](https://arxiv.org/abs/2505.23832) | Wonseok Hwang, Jinu Lee, Chaeeun Kim | - This paper introduces LEGAR BENCH, a large-scale Korean Legal Case Retrieval (LCR) benchmark with 1.2M cases and 411 diverse crime types in queries. - It proposes LEGAL SEARCHLM, a novel retrieval model that performs legal element reasoning and directly generates content grounded in target cases using constrained decoding. - LEGAL SEARCHLM outperforms baselines by 6-20% on LEGAR BENCH, demonstrating state-of-the-art performance and strong generalization to out-of-domain cases. - The model employs a first-token-aware generation strategy and self-supervised fine-tuning (SSFT), which contribute to its improved performance and generalization ability. - Experimental results highlight that LEGAL SEARCHLM significantly outperforms naive generative models by 15%, showcasing its robustness and superior performance in complex retrieval scenarios. | ['Natural Language Processing'] | N/A | N/A |
| [More Thinking, Less Seeing? Assessing Amplified Hallucination in
  Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523) | James Zou, Juncheng Wu, Qingyue Wei, Zhongxing Xu, Chengzhi Liu | - This paper introduces RH-AUC, a new metric to evaluate the balance between reasoning ability and hallucination in multimodal reasoning models, and RH-Bench, a diagnostic benchmark dataset. - The authors find that longer reasoning chains can lead to increased hallucination, as models shift focus away from visual inputs and rely more on language priors. - Larger models generally exhibit a better balance between reasoning and perception than smaller models. - The performance of the model is dependent on the types and domains of the training data rather than the volume of training data. - The authors investigate the impact of reasoning length on the hallucination-reasoning balance and propose methods to control reasoning length. | ['Multimodal', 'Visual Question Answering'] | N/A | N/A |
