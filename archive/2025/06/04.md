

## Papers for 2025-06-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [UniWorld: High-Resolution Semantic Encoders for Unified Visual
  Understanding and Generation](https://arxiv.org/abs/2506.03147) | Yuwei Niu, Xinhua Cheng, Zongjian Li, BestWishYsh, LanguageBind | - UniWorld is a new unified generative framework for image perception and manipulation tasks, which uses high-resolution contrastive semantic encoders instead of VAEs. - The model architecture consists of pre-trained multi-modal large models for auto-regressive understanding and high-resolution contrastive semantic encoders for visual feature extraction. - UniWorld outperforms BAGEL on image editing benchmarks using only 1% of BAGEL's training data and achieves competitive performance on image understanding and generation tasks. - The paper also includes empirical observations on GPT-40-Image, inferring that it uses semantic encoders rather than VAEs for visual feature extraction. - UniWorld's code, model weights, training and evaluation scripts, and datasets are fully open-sourced. | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | [Link](https://github.com/PKU-YuanGroup/UniWorld-V1) | [Link](https://huggingface.co/LanguageBind/UniWorld-V1), [Link](https://huggingface.co/datasets/LanguageBind/UniWorld-V1) |
| [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in
  Multi-Agent Environments](https://arxiv.org/abs/2506.02387) | Xinlei Chen, Xiangmin Yi, Zhexuan Xu, HuiningYuan, zelaix | The paper introduces VS-BENCH, a new multimodal benchmark for evaluating Vision-Language Models (VLMs) in multi-agent environments.  VS-BENCH comprises eight vision-grounded environments covering cooperative, competitive, and mixed-motive interactions.  The benchmark uses two complementary evaluation dimensions: offline evaluation of strategic reasoning via next-action prediction accuracy and online evaluation of decision-making via normalized episode return. Experiments on fourteen leading VLMs reveal a significant gap between current models and optimal performance, highlighting areas for future research.  The code and data are publicly available. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://vs-bench.github.io) | [Link](https://huggingface.co/datasets/zelaix/VS-Bench) |
| [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for
  Vision Language Models](https://arxiv.org/abs/2506.03135) | Xinqiang Yu, Wenyao Zhang, Shaochen Zhang, Mengdi Jia, qizekun | This paper introduces OmniSpatial, a comprehensive benchmark for evaluating spatial reasoning capabilities in vision-language models.  It comprises over 1.5K question-answer pairs covering four categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking. The benchmark demonstrates significant limitations of current models in comprehensive spatial understanding, especially in complex logic and perspective-taking tasks. The paper also proposes enhancing spatial reasoning by incorporating auxiliary models such as point-graph and spatial chain-of-thought. The results reveal that while large language models show promising results, there is still a significant gap compared to human-level performance. | ['Visual Question Answering', 'Multimodal', 'Computer Vision', 'Robotics'] | [Link](https://github.com/thu-ml/omnispatial) | N/A |
| [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096) | Hang Yan, Zichen Liu, Xiangyan Liu, Jinjie Ni, Jakumetsu | - SynthRL is a novel method for scaling visual reasoning in reinforcement learning by synthesizing additional training data. - SynthRL comprises three key stages: seed question selection, targeted synthesis, and verification. - Experiments on the MMK12 dataset show that SynthRL synthesizes over 3.3K additional verifiable questions, leading to consistent gains across five out-of-domain visual math reasoning benchmarks. - Detailed analysis reveals that SynthRL is particularly effective in eliciting deeper reasoning on challenging evaluation samples. - The proposed approach is scalable and guarantees near-perfect correctness, making it suitable for large-scale data augmentation in visual reasoning tasks. | ['Reinforcement Learning', 'Visual Question Answering', 'Multimodal'] | [Link](github.com/NUS-TRAIL/SynthRL) | [Link](hf.co/collections/Jakumetsu/SynthRL) |
| [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143) | Jianwei Yang, vyokky, Ray2333, cckevinn, qianhuiwu | - GUI-Actor is a novel coordinate-free visual grounding method for visual grounding in GUI agents that uses an attention-based action head and an <ACTOR> token to identify relevant visual regions for action execution. - The model outperforms previous state-of-the-art methods on multiple GUI action grounding benchmarks, demonstrating improved generalization to unseen screen resolutions and layouts. - GUI-Actor-7B achieves scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL, outperforming UI-TARS-72B (38.1) on ScreenSpot-Pro. - A grounding verifier is designed to evaluate and select the most plausible action region from candidates proposed for action execution, further enhancing model performance. - The authors demonstrate that fine-tuning only the newly introduced action head while keeping the VLM backbone frozen achieves comparable performance to previous state-of-the-art models. | ['Multimodal', 'Image-to-Text', 'Reinforcement Learning'] | N/A | N/A |
| [Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in
  Robotics](https://arxiv.org/abs/2506.00070) | Jaehyung Kim, Jinwoo Shin, Huiwon Jang, Sumin Park, Dongyoung Kim | - This paper introduces ROBOT-R1, a novel framework that uses reinforcement learning to enhance embodied reasoning in robotics, addressing limitations of supervised fine-tuning (SFT). - ROBOT-R1 predicts the next keypoint state for task completion, conditioned on the scene image and environment metadata from expert demonstrations. - The model is trained using a multiple-choice question-answering (MCQA) approach, which converts the continuous action space into a discrete one. - Experiments show that ROBOT-R1 outperforms SFT methods and even surpasses GPT-40 on embodied reasoning tasks, particularly in low-level action control. - The paper also introduces a new benchmark, ROBOT-R1 Bench, to rigorously evaluate the diverse embodied reasoning capabilities required for robotic tasks. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136) | Mengdi Wang, Ke Shen, Ye Tian, Ling Yang, Yinjie Wang | The paper introduces CURE, a novel reinforcement learning framework that co-evolves LLM coder and unit tester capabilities without ground-truth code supervision.  CURE uses a dedicated reward design based on interaction outcomes, enabling flexible and scalable training. The ReasonFlux-Coder models (7B and 14B) derived from this framework improve code generation accuracy and Best-of-N accuracy, outperforming existing models of similar size.  Furthermore, the framework extends to downstream tasks, showing improvements in test-time scaling and agentic coding. Finally, the trained unit tester serves as an effective reward model for reinforcement learning on base models. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/Gen-Verse/CURE) | [Link](https://huggingface.co/Gen-Verse/ReasonFlux-Coder) |
| [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061) | Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Debangshu Banerjee, Tarun Suresh | This paper introduces DINGO, a novel constrained decoding algorithm designed for diffusion language models.  DINGO uses dynamic programming to ensure that generated outputs adhere to user-specified regular expressions while preserving the output distribution. It achieves up to a 68% improvement over unconstrained inference on benchmark tasks like symbolic math and JSON generation. The method provably guarantees the correctness and optimality of the generated output. It addresses the challenges of applying constrained decoding to the parallel nature of diffusion LLMs, outperforming previous methods.  DINGO can handle any user-specified regular expression. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation](https://arxiv.org/abs/2505.24714) | Jinsheng Huang, Xiao Luo, chunfenri, alan1027, luojunyu | - This paper introduces FINMME, a new benchmark dataset for evaluating multimodal large language models (MLLMs) in the financial domain.  - FINMME contains over 11,000 high-quality samples covering 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes.  - A novel evaluation system, FinScore, is proposed to address challenges in financial data, such as hallucination and varying domain complexities.  - Experiments show that state-of-the-art models like GPT-40 underperform on FINMME, highlighting its challenging nature and the need for further research in financial MLLMs.  - The dataset and evaluation protocol are publicly available. | ['Multimodal'] | [Link](https://github.com/luo-junyu/FinMME) | [Link](https://huggingface.co/datasets/luojunyu/FinMME) |
| [PCoreSet: Effective Active Learning through Knowledge Distillation from
  Vision-Language Models](https://arxiv.org/abs/2506.00910) | Sung Ju Hwang, Dongseop Kim, Hyungjoon Jang, Dong Bok Lee, Seongjae Kang |  - This paper introduces ActiveKD, a novel framework that integrates active learning (AL) with knowledge distillation (KD) using vision-language models (VLMs).  - ActiveKD leverages the zero- and few-shot capabilities of VLMs to overcome the limitations of traditional KD in data-scarce scenarios.  - The framework proposes a new sample selection strategy, Probabilistic CoreSet (PCoreSet), which maximizes coverage in the probability space rather than the feature space.  - Evaluations on 11 datasets demonstrate that ActiveKD with PCoreSet consistently outperforms existing active learning methods, achieving significant improvements in final-round accuracy.  - This work advances the intersection of AL and KD by effectively transferring knowledge from VLMs to compact, task-specific student models. | ['Image Classification', 'Zero-Shot Image Classification', 'Multimodal'] | N/A | N/A |
| [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for
  Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397) | Changwang Zhang, Jiawei Chen, Junjie Wu, jwanglux, Cynthia-1628 | - OThink-R1 is a novel method that dynamically switches between fast and slow thinking modes to mitigate over-reasoning in large reasoning models (LRMs). - The model classifies reasoning trajectories as either redundant or essential, pruning redundant steps while preserving logical validity. - OThink-R1 reduces reasoning redundancy by approximately 23% without sacrificing accuracy, as demonstrated by experiments on mathematical and question-answering tasks. - A dual-reference KL-divergence loss function is used to fine-tune the LRM, further enhancing its ability to switch between fast and slow thinking modes. - The approach is inspired by human cognitive processes and provides practical guidelines for creating efficient and accurate reasoning models. | ['Question Answering'] | [Link](https://github.com/AgenticIR-Lab/OThink-R1) | N/A |
| [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and
  Accountability](https://arxiv.org/abs/2506.01789) | David Anugraha, Genta Indra Winata, cryptexcode, seungone, patrickamadeus | This paper introduces DATARUBRICS, a structured framework for assessing the quality of datasets.  It addresses the lack of standardized, measurable metrics for evaluating data quality in existing tools like datasheets.  DATARUBRICS uses rubric-based evaluation metrics and explores cost-effective methods for synthetic data generation, including LLMs. It offers a reproducible solution for dataset quality assessment, beneficial for both authors and reviewers. The framework covers 10 dimensions of data quality, including data sources, annotations, novelty, and utility. | ['Natural Language Processing'] | [Link](https://github.com/datarubrics/datarubrics) | N/A |
| [Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.24726) | Kiran Kamble, Christopher Bryant, Umar Jamil, Shelly Bensal, melisa | - This paper introduces a novel methodology for improving large language models (LLMs) by training them to generate better self-reflections when they make mistakes. - The framework operates in two stages: first, upon failing a task, the model generates a self-reflective commentary; second, the model retries the task with the self-reflection in context.  If successful, the self-reflection tokens are rewarded using Group Relative Policy Optimization (GRPO). - The method only requires binary success/failure feedback and is effective across various model architectures, showing substantial performance gains (up to 34.7% improvement in math equation writing and 18.1% in function calling). - Smaller fine-tuned models (1.5 billion to 7 billion parameters) outperformed larger models (10 times larger), demonstrating efficiency gains. - The approach effectively reduces the need for extensive external feedback data, while also addressing the issue of catastrophic forgetting. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/abs/2506.03096) | Matthias Hein, Nicolas Flammarion, Francesco Croce, chs20 | - FuseLIP is a novel multimodal embedding model that uses early fusion of discrete image and text tokens processed by a single transformer encoder. - It outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks. - The model leverages recent progress in discrete image tokenizers, enabling interaction between modalities at each depth of encoding. - FuseLIP is trained with a contrastive loss and a masked multimodal modeling (MMM) loss, consistently enhancing performance across various zero-shot tasks. - Novel datasets for multimodal pre-training and evaluation were collected, including challenging tasks designed to assess modality interactions. | ['Multimodal'] | [Link](https://github.com/chs20/fuselip) | N/A |
| [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports
  From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454) | Xingyu Liu, Yiyao Wang, Han Wang, Bo Pan, Zhaorui Yang | This paper introduces Multimodal DeepResearcher, a novel agentic framework for generating text-chart interleaved reports from scratch.  It leverages a structured textual representation of charts (FDV) to enable LLMs to generate diverse, high-quality visualizations.  Multimodal DeepResearcher outperforms baseline methods, achieving an 82% win rate over DataNarrative using the same Claude 3.7 Sonnet model across various evaluation metrics. The framework decomposes the task into researching, exemplar report textualization, planning, and multimodal report generation.  A new benchmark, MultimodalReportBench, was developed to evaluate the generated reports. | ['Multimodal'] | [Link](https://rickyang1114.github.io/multimodal-deepresearcher/) | N/A |
| [One Missing Piece for Open-Source Reasoning Models: A Dataset to
  Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338) | Sunghyun Park, Beong-woo Kwak, Jihyuk Kim, Dongjin Kang, hyungjoochae | - This paper introduces the Long CoT Collection, a new dataset designed to mitigate the cold-start problem in reinforcement learning for short chain-of-thought (CoT) large language models (LLMs). - The dataset contains 100,000 long CoT rationales generated using existing short CoT LLMs, guided by a smaller seed dataset of 1,000 examples from a state-of-the-art LLM (R1). - The authors demonstrate that training LLMs on this dataset significantly improves their reasoning capabilities, leading to 2-3x larger gains in reinforcement learning performance compared to models trained without it. - Experiments show the dataset achieves comparable or slightly lower quality compared to R1, demonstrating its effectiveness as a strong foundation for reinforcement learning. - The Long CoT Collection is made publicly available, promoting further research and development in open-source reasoning models. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413) | Aditya Grover, Guy Van den Broeck, danielmisrael | - This paper introduces Adaptive Parallel Decoding (APD), a novel decoding method for diffusion large language models (dLLMs) that dynamically adjusts the number of tokens sampled in parallel to improve generation speed without sacrificing quality. - APD works by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model, enabling flexible trade-offs between throughput and quality. - The method is optimized by enabling KV caching and limiting the size of the masked input, resulting in three tunable parameters for balancing speed and quality. - Experiments show that APD achieves significantly higher throughput than autoregressive models and other dLLM decoding methods with minimal quality degradation. - The authors demonstrate that APD outperforms existing methods on several benchmark tasks, achieving a Pareto-optimal performance in terms of both speed and quality. | ['Text Generation'] | [Link](None) | [Link](None) |
| [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition
  Query](https://arxiv.org/abs/2506.03144) | Qi Xu, Xian Wang, Linfeng Li, Yuan Gao, WeiChow | This paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval.  It identifies limitations of existing models: neglecting conditional elements while focusing on global semantics, failing to extract attributes, and misinterpreting visual content.  A novel fine-tuning framework, CORAL, adapts pre-trained MLLMs using embedding reconstruction for conditional elements and contrastive learning for global semantics.  Experiments demonstrate CORAL achieves a 45.9% performance improvement over conventional methods on MERIT and strong generalization across eight benchmarks. | ['Multimodal'] | [Link](MERIT-2025.github.io) | N/A |
| [M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial
  Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510) | Lifan Guo, Xiandong Li, Yalong Wen, Junhui Li, amazingj | This research introduces M³FinMeeting, a novel multilingual, multi-sector, and multi-task dataset designed for evaluating financial meeting understanding in large language models (LLMs).  It addresses limitations of existing datasets by focusing on real-world financial meeting transcriptions across various industry sectors (defined by GICS) and three tasks: summarization, question-answer pair extraction, and question answering.  Experiments on seven popular LLMs show that even advanced models struggle with the tasks, demonstrating the effectiveness of M³FinMeeting. The dataset includes English, Chinese, and Japanese and has over 600 financial meetings. | ['Document Question Answering', 'Question Answering', 'Summarization'] | [Link](https://github.com/aliyun/qwen-dianjin) | N/A |
| [Knowing Before Saying: LLM Representations Encode Information About
  Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362) | Florian Matthes, yziser, galchechik, anumafzal94 | - This paper introduces a novel method for predicting the success of Chain-of-Thought (CoT) prompting in large language models (LLMs) before the generation process is complete. - The method uses a probing classifier trained on LLM internal representations to predict CoT success, outperforming a strong BERT-based baseline that relies only on generated tokens. - Experiments show that the classifier achieves high accuracy even before any tokens are generated, suggesting that crucial information about the reasoning process is encoded early in the LLM's internal representations. - Early stopping experiments demonstrate that truncating CoT reasoning can still improve performance compared to not using CoT, but there is a remaining gap compared to full reasoning. - The findings suggest that optimizing CoT's efficiency may be possible by leveraging the classifier's guidance to identify when early stopping is most effective. | ['Natural Language Processing', 'Text Classification', 'Question Answering', 'Zero-Shot Classification'] | [Link](https://github.com/anum94/CoTpred) | N/A |
| [Revisiting LRP: Positional Attribution as the Missing Ingredient for
  Transformer Explainability](https://arxiv.org/abs/2506.02138) | Lior Wolf, Hila Chefer, Itamar Zimerman, Yarden Bakish | - This paper introduces Positional-Aware Layer-wise Relevance Propagation (PA-LRP), a novel technique for improving transformer explainability by incorporating positional encoding (PE) information into the attribution process. - PA-LRP significantly outperforms existing LRP-based methods for transformer explainability on both NLP and vision tasks, as demonstrated by extensive experiments with fine-tuned classifiers and zero-shot foundation models. - The method reformulates the input space for transformer explainability to include position-token pairs, enabling the propagation of attributions across various positional encoding schemes. - PA-LRP introduces novel, theoretically grounded LRP rules designed to handle PE layers, including learnable, rotary, and absolute PEs. - The authors provide both quantitative and qualitative results that highlight the effectiveness of PA-LRP in achieving more faithful and comprehensive explanations. | ['Natural Language Processing', 'Computer Vision', 'Zero-Shot Classification'] | [Link](https://github.com/YardenBakish/PE-AWARE-LRP) | N/A |
| [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural
  Understanding and Transcreation](https://arxiv.org/abs/2506.01565) | Wenyan Li, Shaohuan Cheng, Dongchu Xie, Lutong Yu, Li Zhou | This paper introduces Hanfu-Bench, a novel multimodal benchmark dataset focusing on cross-temporal cultural understanding and transcreation using Hanfu (traditional Chinese clothing).  The benchmark includes two core tasks: cultural visual understanding (CVU) using multiple-choice visual question answering and cultural image transcreation (CIT) which involves transforming traditional Hanfu images into modern designs.  Evaluation on both tasks reveals significant challenges for existing vision-language models, highlighting the need for further advancements in temporal cultural understanding.  The dataset and evaluation tools are publicly available on HuggingFace. | ['Multimodal', 'Visual Question Answering', 'Image-to-Image', 'Text-to-Image'] | [Link](https://github.com/lizhou21/Hanfu-Bench) | [Link](https://huggingface.co/lizhou21/Hanfu-Bench), [Link](https://huggingface.co/lizhou21/TemporalCulture) |
