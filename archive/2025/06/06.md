

## Papers for 2025-06-06

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow
  Development](https://arxiv.org/abs/2506.05010) | Zijiao Wu, Qingli Hu, Yiyu Wang, Xue Yang, imryanxu | - ComfyUI-Copilot is a large language model-powered plugin for ComfyUI, an open-source platform for AI-driven art creation, designed to enhance usability and efficiency. - It addresses challenges such as limited documentation, model misconfigurations, and workflow complexity by offering intelligent node and model recommendations, and automated workflow construction. - The system uses a hierarchical multi-agent framework with a central assistant agent and specialized worker agents, supported by curated ComfyUI knowledge bases. - Evaluations show that ComfyUI-Copilot accurately recommends nodes and accelerates workflow development, lowering barriers for beginners and enhancing efficiency for experienced users. -  The system has been shown to achieve high recall rates for workflows and nodes (both exceeding 88.5%), demonstrating practical efficacy. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/AIDC-AI/ComfyUI-Copilot) | N/A |
| [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865) | Emilien Bir√©, Breno Baldas Skuk, Mathieu Andreux, tonywu71, hamza-hcompany | - This paper introduces Surfer-H, a cost-efficient web agent that leverages Vision-Language Models (VLMs) to perform user-defined tasks on the web.  The agent integrates three modules: a policy, a localizer, and a validator. -  The key innovation is Holol, a new open-weight collection of VLMs trained on curated data sources, including open-access web content, synthetic examples, and self-produced data. Holol outperforms existing methods in UI benchmarks and surpasses state-of-the-art performance on WebVoyager. - Surfer-H achieves a 92.2% success rate on the WebVoyager benchmark, demonstrating Pareto optimality by achieving this high accuracy while being cost-efficient. - The researchers also introduce WebClick, a new benchmark specifically designed for web-based UI localization, to aid in evaluating the performance of similar web agents. - To accelerate research, the WebClick evaluation dataset and the Holol model weights are open-sourced. | ['Reinforcement Learning', 'Multimodal'] | N/A | [Link](https://huggingface.co/collections/Hcompany/holo1-683dd1eece7eb077b96d0cbd), [Link](https://huggingface.co/datasets/Hcompany/WebClick) |
| [Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers
  for Long Contexts](https://arxiv.org/abs/2506.05229) | Ivan Oseledets, Yuri Kuratov, Gleb Kuzmin, Ivan Rodkin, Danil Sivtsov | - This paper introduces Diagonal Batching, a novel scheduling scheme that enhances the parallelism of Recurrent Memory Transformers (RMTs) for long-context inference. - Diagonal Batching reorders computations, enabling efficient GPU inference even with single long-context inputs, eliminating the need for complex batching and pipelining. - When applied to a LLaMA-1B ARMT model, Diagonal Batching achieves a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation. - The method preserves the exact recurrence of RMTs and requires no model retraining, making it adaptable to existing models. - Experimental results demonstrate reduced inference costs and latency, which makes RMTs a practical solution for real-world applications with long contexts. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/svtdanny/diagonal-batching) | N/A |
| [Qwen3 Embedding: Advancing Text Embedding and Reranking Through
  Foundation Models](https://arxiv.org/abs/2506.05176) | Huan Lin, Mingxin Li, Yanzhao Zhang, izhx, thenlper | - This paper introduces the Qwen3 Embedding series, a significant advancement in text embedding and reranking capabilities built upon the Qwen3 foundation models. - The model architecture utilizes LLMs with causal attention for embeddings and employs LLMs for point-wise reranking. - A multi-stage training pipeline is used, combining large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. - The Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks, excelling in multilingual evaluation and various retrieval tasks. - The models are publicly available under the Apache 2.0 license to facilitate reproducibility and community-driven research. | ['Natural Language Processing', 'Sentence Similarity', 'Text Generation', 'Feature Extraction'] | [Link](https://github.com/QwenLM/Qwen3-Embedding) | [Link](https://huggingface.co/Qwen) |
| [Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual
  Simulations](https://arxiv.org/abs/2506.04633) | Yinuo Yang, Zixian Ma, Mahtab Bigverdi, Linjie Li, kuvvi | This paper introduces STARE, a benchmark dataset for evaluating multimodal large language models on spatial reasoning tasks.  STARE contains approximately 4,000 tasks spanning foundational geometric transformations, integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning).  Evaluation reveals that current models struggle with complex visual simulations, performing near random chance on 3D tasks whereas humans achieve near-perfect accuracy.  The authors highlight the importance of visual simulation capabilities for robust spatial reasoning and provide extensive analysis of model performance across various task complexities and evaluation settings.  STARE aims to advance research on multimodal spatial reasoning and inspire further development of AI models capable of human-level visual simulation. | ['Multimodal'] | [Link](https://github.com/STARE-bench/STARE) | N/A |
| [SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs](https://arxiv.org/abs/2506.05344) | Jiwen Lu, Yongming Rao, Jiahui Wang, Zuyan | - This paper introduces SparseMM, a novel KV-Cache optimization strategy that leverages the sparsity of visual heads in Multimodal Large Language Models (MLLMs) to accelerate inference. - SparseMM identifies visual heads through a training-free framework that quantifies head-level visual relevance using OCR. - The method allocates asymmetric computation budgets to heads based on their visual scores, prioritizing visual semantics during decoding. - Extensive evaluations demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs compared to existing KV-Cache acceleration methods, delivering 1.38x real-time acceleration and 52% memory reduction. - SparseMM is shown to generalize across diverse LLM architectures and vision-language tasks. | ['Multimodal'] | [Link](https://github.com/CR400AF-A/SparseMM) | N/A |
| [AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual
  Counting for MLLMs](https://arxiv.org/abs/2506.05328) | Tong Lu, Yicheng Liu, Zhiqi Li, cg1177, lulidong | - This paper introduces CG-AV-Counting, a new benchmark dataset for evaluating clue-grounded audio-visual counting in long videos, addressing limitations of existing benchmarks. - It proposes AV-Reasoner, a model trained with GRPO and curriculum learning, which outperforms existing models on multiple benchmarks, demonstrating the effectiveness of reinforcement learning. - AV-Reasoner achieves state-of-the-art results across multiple benchmarks in Audio-Visual Question Answering, and other related tasks. - Experiments reveal that reasoning in the language space does not improve performance on out-of-domain benchmarks. - The dataset includes 1,027 multimodal questions and 5,845 annotated clues over 497 long videos, supporting both black-box and white-box evaluation. | ['Video-Text-to-Text', 'Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://av-reasoner.github.io) | [Link](https://huggingface.co/datasets/Video-R1/DVD-counting) |
| [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence
  Training of LLMs](https://arxiv.org/abs/2506.03077) | Xiao Li, Lei Zhao, Qijun Luo, Kullpar | - StreamBP is a novel memory-efficient exact backpropagation method for training large language models (LLMs) on long sequences. - It linearly decomposes the chain rule along the sequence dimension, significantly reducing the memory cost of activation values and logits. - StreamBP is applicable to common training objectives such as supervised fine-tuning (SFT), group relative policy optimization (GRPO), and direct preference optimization (DPO). - Experimental results demonstrate that StreamBP scales up the maximum sequence length by 2.8-5.5 times compared to gradient checkpointing, while using comparable or even less BP time. - A communication-efficient distributed StreamBP is also developed to support multi-GPU training. | ['Natural Language Processing'] | [Link](https://github.com/Ledzy/StreamBP) | N/A |
| [MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical
  Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.05331) | Shilin Yan, Aojun Zhou, Renrui Zhang, CaraJ, xy06 | - The paper introduces MINT-CoT, a novel method that incorporates visual tokens into the chain-of-thought (CoT) reasoning process for solving mathematical problems.- MINT-CoT utilizes an Interleave Token to dynamically select relevant visual regions within mathematical figures, addressing limitations of existing methods that rely on coarse-grained regions or external visual modifications.- The proposed method is evaluated on three benchmark datasets (MathVista, GeoQA, and MMStar), demonstrating significant improvements (+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar) over baseline models.- A new dataset, MINT-CoT, is constructed containing 54K mathematical problems with token-level alignment between reasoning steps and visual regions, facilitating the training of the MINT-CoT model.-  The model is trained using a three-stage strategy combining text-only CoT, interleaved CoT supervised fine-tuning, and interleaved CoT reinforcement learning, enhancing the model's ability to effectively reason with interleaved visual information. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/xinyan-cxy/MINT-CoT) | N/A |
| [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345) | Edoardo M. Ponti, Piotr Nawrot, Konrad Staniszewski, Adrian ≈Åa≈Ñcucki | This paper introduces Dynamic Memory Sparsification (DMS), a novel method for compressing key-value caches in Transformer LLMs during inference to improve efficiency.  DMS achieves 8x compression with only 1K training steps, outperforming training-free sparse attention methods. The method is shown to boost accuracy for comparable inference runtime and memory load across multiple families of LLMs on reasoning tasks.  Experiments show that DMS consistently outperforms baselines, particularly at higher compression ratios. The work also validates the effectiveness of efficient attention, unlocked by DMS, for inference-time scaling, improving reasoning capabilities. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | N/A |
| [EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an
  Egocentric World?](https://arxiv.org/abs/2506.05287) | Dian Jiao, Wentong Li, Long Li, Ronghao Dang, CircleRadon |  - The paper introduces EOC-Bench, a novel benchmark for evaluating multimodal large language models' (MLLMs) object-centric embodied cognition in dynamic egocentric scenarios.  - EOC-Bench features 3,277 meticulously annotated QA pairs categorized into past, present, and future temporal categories, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types.  - The benchmark is designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios, addressing the gap in existing benchmarks which primarily focus on static scene exploration.  - A mixed-format human-in-the-loop annotation framework is used for comprehensive evaluation, incorporating four question types and a novel multi-scale temporal accuracy metric.  - The comprehensive evaluation on EOC-Bench reveals clear deficiencies in object-level temporal perception for mainstream MLLMs, highlighting the challenges and importance of advancing these capabilities. | ['Video Classification', 'Visual Question Answering', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Language-Image Alignment with Fixed Text Encoders](https://arxiv.org/abs/2506.04209) | Yi Ma, Yue Zhao, robinwuzy, JingfengY | - This paper introduces LIFT, a novel framework for language-image alignment that uses a fixed, pre-trained large language model (LLM) as the text encoder and trains only the image encoder. - LIFT outperforms CLIP in most scenarios involving compositional understanding and long captions, demonstrating significant improvements in computational efficiency. - The authors conduct comprehensive benchmarking and ablation studies to evaluate LIFT's performance against CLIP, showcasing superior accuracy in several compositional understanding tasks and LLaVA downstream tasks. - They also investigate several design choices for LLM-based text encoders, finding that contrastive fine-tuning is necessary for optimal performance. - LIFT simplifies the design choices in mainstream contrastive language-image alignment approaches, enabling the use of a simpler cosine similarity loss while achieving comparable performance. | ['Multimodal', 'Image-to-Text', 'Zero-Shot Image Classification', 'Image Feature Extraction'] | [Link](https://github.com/Jingfeng0705/LIFT) | N/A |
| [FlexPainter: Flexible and Multi-View Consistent Texture Generation](https://arxiv.org/abs/2506.02620) | Luozhou Wang, Jiantao Lin, Leyi Wu, yingcongchen, StarYDY | - FlexPainter is a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. - It leverages a shared conditional embedding space to perform flexible aggregation between different input modalities, achieving reference image-based stylization via an image-based CFG method. - FlexPainter generates multi-view images simultaneously using a grid representation to enhance global understanding and uses a view synchronization and adaptive weighting module during diffusion sampling to ensure local consistency. - A 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. - The experimental results demonstrate that FlexPainter significantly outperforms state-of-the-art methods in both flexibility and generation quality. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | N/A | N/A |
| [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly
  Licensed Text](https://arxiv.org/abs/2506.05209) | Stella Biderman, Colin Raffel, Brian Lester, Nikhil Kandpal, storytracer |  - The paper introduces Common Pile v0.1, an 8TB dataset of openly licensed text for large language model (LLM) pre-training.  - It addresses the limitations of prior datasets which were too small or low quality to produce performant LLMs.  - The dataset comprises content from 30 diverse sources, spanning research papers, code, books, encyclopedias, and more.  - The authors validate the dataset by training two 7-billion parameter LLMs, achieving competitive performance to those trained on unlicensed text with similar computational budgets.  -  The Common Pile v0.1, associated code, training mixture, and checkpoints for the trained LLMs are publicly released. | ['Natural Language Processing'] | [Link](https://github.com/commonpile/commonpile) | [Link](https://huggingface.co/datasets/commonpile/commonpile) |
| [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at
  Scale](https://arxiv.org/abs/2506.04405) | Yue Yu, Yishan Zhong, Yuchen Zhuang, Ran Xu, wshi83 | MedAgentGym is introduced as the first publicly available training environment designed to improve the coding-based medical reasoning capabilities of large language models (LLMs).  It contains 72,413 task instances across 129 categories from 12 real-world biomedical scenarios.  Benchmarking over 25 LLMs reveals a significant performance gap between commercial API-based models and open-source LLMs. Med-Copilot-7B achieves substantial performance gains via fine-tuning and reinforcement learning, providing a cost-effective and privacy-preserving alternative.  MedAgentGym offers a unified platform for developing LLM-based coding assistants for biomedical research and practice. | ['Reinforcement Learning', 'Natural Language Processing', 'Tabular', 'Question Answering'] | [Link](https://github.com/wshi83/MedAgentGym) | [Link](https://huggingface.co/MedAgentGym) |
| [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning
  Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734) | Xiaoqi Jian, Yongfu Zhu, Jinzhu Wu, Weihong Lin, lincharliesun | - This paper reveals that the benchmark evaluation results of reasoning models, specifically the Deepseek-R1-Distill series, are susceptible to significant fluctuations due to subtle variations in evaluation conditions. - The study highlights the impact of various factors, including evaluation dataset versions, instruction positions, option bias, and tensor parallelism settings, on the model's performance. - The authors advocate for establishing a more rigorous paradigm for model performance evaluation, emphasizing transparency and stability. - They propose a methodology to estimate the required number of independent inferences for stable performance evaluation. - This research emphasizes the importance of standardized and well-documented evaluation procedures to prevent misrepresentation of model capabilities and ensure reproducibility. | ['Question Answering'] | N/A | N/A |
| [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245) | Janardhan Kulkarni, Huseyin A. Inan, wulu, sahar-abdelnabi, Eric-Lan | - This paper introduces a novel reinforcement learning (RL) framework to enhance the contextual integrity (CI) of large language models (LLMs). - The framework leverages chain-of-thought (CoT) prompting to guide LLMs to reason explicitly about CI norms before generating responses. - A synthetic dataset of approximately 700 automatically generated examples with diverse contexts and information disclosure norms was created to train and evaluate the model. - The proposed method significantly reduces inappropriate information disclosure while maintaining task performance across multiple LLMs. - Improvements transfer from the synthetic dataset to established CI benchmarks, demonstrating the effectiveness and generalizability of the framework. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Micro-Act: Mitigate Knowledge Conflict in Question Answering via
  Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278) | Xiaolong Li, Ge Qu, Bowen Qin, Jinyang Li, NanHUO | MICRO-ACT is a novel framework that dynamically decomposes each knowledge source in a retrieval-augmented generation (RAG) system into a sequence of fine-grained comparisons to mitigate knowledge conflicts.  It achieves significant improvements in question-answering (QA) accuracy across five benchmark datasets, outperforming state-of-the-art baselines, especially in temporal and semantic conflict scenarios.  The hierarchical action space in MICRO-ACT allows for reasoning beyond superficial context.  The framework also exhibits robust performance on non-conflict questions.  The code is available on GitHub. | ['Question Answering'] | [Link](https://github.com/Nan-Huo/Micro-Act) | N/A |
| [Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric
  Approach](https://arxiv.org/abs/2506.03238) | Weidi Xie, Yanfeng Wang, Ya Zhang, Lisong Dai, zzh99 | This paper introduces OminiAbnorm-CT, a novel system for abnormality-centric whole-body CT image interpretation.  It includes a hierarchical classification system with 404 abnormal findings, a new dataset (OminiAbnorm-CT-14K) with over 14.5K CT images and annotations for 19K abnormalities, and a new model (OminiAbnorm-CT) that outperforms existing methods in three clinically relevant evaluation tasks. OminiAbnorm-CT's architecture leverages a multi-modal language model and a segmentation module for grounded abnormality description, enabling flexible interaction through both text and visual prompts. The model shows significant improvement over existing baselines in all three tasks. | ['Image-to-Text', 'Multimodal'] | [Link](https://github.com/zhaoziheng/OminiAbnorm-CT) | N/A |
