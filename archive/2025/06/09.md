

## Papers for 2025-06-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Will It Still Be True Tomorrow? Multilingual Evergreen Question
  Classification to Improve Trustworthy QA](https://arxiv.org/abs/2505.21115) | VityaVitalich, nakrayko, VirVen, zlatamaria, memyprokotow | - This paper introduces EverGreenQA, the first multilingual question answering dataset with labels indicating whether questions are evergreen (answers remain stable over time) or mutable (answers change). - The dataset supports both evaluation and training of models for evergreen question classification. - They benchmark 12 modern LLMs on EverGreenQA to assess their ability to encode question temporality and train EG-E5, a lightweight multilingual classifier that achieves state-of-the-art performance on this task. - They demonstrate the practical utility of evergreen question classification in three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4's retrieval behavior. - The dataset and trained model are publicly released to facilitate further research. | ['Question Answering'] | N/A | [Link](https://huggingface.co/collections/s-nlp/evergreen-683465909575cb89d6b904fe) |
| [FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal
  Contextual Fusion](https://arxiv.org/abs/2506.01111) | Owen Lee, Liyan Zhao, Zheshu Chen, Shunian Chen, SatsukiVie | - This paper introduces FusionAudio-1.2M, a novel large-scale dataset comprising 1.2 million detailed audio captions and 6 million QA pairs. - A two-stage automated pipeline for fine-grained audio caption generation is proposed. This pipeline first uses pretrained models to extract contextual cues (speech, music, general sounds, and visual information) and then synthesizes these cues using an LLM to generate detailed captions. - The proposed method demonstrates enhanced accuracy and detail by leveraging visual and comprehensive auditory cues, outperforming baselines in terms of caption detail and accuracy according to a manual evaluation. - FusionAudio-1.2M improves audio-text alignment and instruction following, achieving better performance in audio-text retrieval tasks compared to existing datasets. - Ablation studies confirm the contributions of diverse modalities and the effectiveness of multimodal contextual fusion. | ['Audio'] | [Link](https://github.com/satsuki2486441738/FusionAudio) | N/A |
| [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872) | Yu Su, Muhao Chen, Kai Zhang, DarthZhu | - This paper investigates the effects of modality extension, a common technique for training multimodal models, on achieving true omni-modality. - The researchers analyze the trade-offs between extending modalities and preserving core language abilities, exploring model merging as an alternative approach. - Three key research questions are addressed: whether modality extension compromises core language abilities, whether model merging effectively integrates independently fine-tuned modality-specific models, and whether omni-modality extension leads to better generalization. - Experiments reveal that modality extension can enhance certain capabilities but may compromise others; weighted model merging outperforms standard average merging by preserving more crucial attributes. - The findings suggest that omni-modality fine-tuning is less efficient than modality-specific fine-tuning for specific tasks. | ['Multimodal', 'Any-to-Any', 'Natural Language Processing'] | [Link](https://github.com/DarthZhu/lm-extend) | N/A |
| [Audio-Aware Large Language Models as Judges for Speaking Styles](https://arxiv.org/abs/2506.05984) | Linjie Li, Kevin Lin, Chung-Ching Lin, xiaofei-wang, dcml0714 | - This paper explores using audio-aware large language models (ALLMs) as automatic judges for evaluating the speaking styles of speeches generated by spoken language models (SLMs). - Two tasks, voice style instruction following and role-playing, were designed to evaluate the SLMs' ability to control speaking styles, including emotion, volume, pace, and emphasis. - Human evaluations and ALLM evaluations were compared, showing that the Gemini ALLM's agreement with human judges was comparable to the agreement between human evaluators. - The results demonstrated that ALLMs can serve as effective automatic judges for speaking styles, while also highlighting areas where current SLMs need improvement in speaking style control. - The paper contributes two new evaluation tasks for SLMs and demonstrates the feasibility and effectiveness of using ALLMs as automatic judges for speaking styles. | ['Audio', 'Text-to-Speech'] | N/A | N/A |
| [Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs](https://arxiv.org/abs/2506.05629) | sambaran, abhi1nandy2, ananthmuppidi | - This paper introduces a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) for parameter-efficient fine-tuning of large language models. - ID-SPAM generates soft prompts based on input tokens, attending to different tokens with varying importance, and keeping the number of trainable parameters small. - The proposed approach is compared to state-of-the-art techniques on various tasks, demonstrating improved zero-shot domain transfer capability. - ID-SPAM outperforms several baselines on multiple benchmarks, including GLUE and SuperGLUE, showcasing better performance with fewer parameters. - Ablation studies highlight the importance of the self-attention mechanism in ID-SPAM, indicating improved efficiency and effectiveness. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence
  with Egocentric-Exocentric Vision](https://arxiv.org/abs/2506.06253) | Baoqi Pei, Lidong Lu, Yifei Huang, Yuping He, cg1177 | This survey paper does not introduce any new models or datasets.  - It provides a comprehensive overview of video understanding techniques from both egocentric and exocentric viewpoints. - It systematically analyzes recent advancements in three main research directions: leveraging egocentric data to enhance exocentric understanding; utilizing exocentric data to improve egocentric analysis; and joint learning frameworks that unify both perspectives. - The paper also discusses benchmark datasets and proposes promising future research directions.  - A key contribution is the identification and organization of key research challenges and opportunities for future development in cross-view collaborative intelligence. | ['Computer Vision', 'Video Classification', 'Multimodal'] | [Link](https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision) | N/A |
| [HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource
  Utilization](https://arxiv.org/abs/2506.04255) | Harshil Patel, helloparthshah, guineapig | - HASHIRU is a novel multi-agent system framework designed to enhance flexibility, resource efficiency, and adaptability in handling complex tasks. - It features a hierarchical structure with a "CEO" agent dynamically managing specialized "employee" agents, leveraging a hybrid intelligence model that prioritizes smaller, cost-effective local LLMs while flexibly integrating external APIs and larger models when necessary. - The system incorporates an economic model with hiring/firing costs, promoting efficient resource allocation and team stability, and includes autonomous API tool creation. - Evaluations demonstrate HASHIRU's capabilities in academic paper review, safety assessments, and complex reasoning tasks, often outperforming existing models. - Through case studies, it showcases self-improvement via autonomous cost model generation, tool integration, and budget management. | ['Natural Language Processing', 'Question Answering', 'Text Generation', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/HASHIRU-AI/HASHIRU) | N/A |
| [Truth in the Few: High-Value Data Selection for Efficient Multi-Modal
  Reasoning](https://arxiv.org/abs/2506.04755) | Chong Peng, Hao Yang, Lei Wang, Kaiyuan Deng, Shenshen Li | - This paper introduces a novel data selection paradigm, Reasoning Activation Potential (RAP), to improve the efficiency of multi-modal reasoning in large language models (LLMs). - RAP identifies high-value cognitive samples by using two estimators: Causal Discrepancy Estimator (CDE) and Attention Confidence Estimator (ACE), which remove samples that overly rely on language priors and those with attention on irrelevant tokens, respectively. - The Difficulty-aware Replacement Module (DRM) replaces easy samples with more challenging ones to ensure complexity in the training process. - Experimental results show RAP achieves superior performance using only 9.3% of the training data and reduces computational costs by over 43%, outperforming existing methods on six datasets. - The effectiveness of RAP is demonstrated across different model architectures and RL algorithms, showcasing its robustness and broad applicability. | ['Multimodal'] | [Link](https://github.com/Leo-ssl/RAP) | N/A |
| [GuideX: Guided Synthetic Data Generation for Zero-Shot Information
  Extraction](https://arxiv.org/abs/2506.00649) | Eneko Agirre, Iker Garc√≠a-Ferrero, OSainz, neildlf | - This paper introduces GUIDEX, a novel method for automatically defining domain-specific schemas, inferring guidelines, and generating synthetically labeled instances for improved zero-shot information extraction. - GUIDEX achieves state-of-the-art results across seven zero-shot Named Entity Recognition benchmarks, surpassing previous methods by up to 7 F1 points without human-labeled data and nearly 2 F1 points higher when combined with it. - The method involves four steps: document summarization, structured representation, guideline generation, and instance extraction, all leveraging LLMs. - GUIDEX demonstrates enhanced comprehension of complex, domain-specific annotation schemas and produces high-quality synthetic data by using executable Python code for validation and ensuring consistency. - The generated synthetic dataset exhibits diverse labels across domains like medicine, economics, history, music, and education, showcasing its versatility and broad applicability. | ['Natural Language Processing', 'Zero-Shot Classification', 'Feature Extraction', 'Text Generation'] | [Link](https://neilus03.github.io/guidex.com) | N/A |
