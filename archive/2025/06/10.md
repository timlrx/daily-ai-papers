

## Papers for 2025-06-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007) | Tianzhu Ye, Qingxiu Dong, frontierai, YaoTang23, unilm | - This paper introduces Reinforcement Pre-Training (RPT), a novel scaling paradigm that reframes next-token prediction as a reasoning task trained using reinforcement learning with verifiable rewards. - RPT offers a scalable and general-purpose method to leverage vast amounts of text data for general-purpose reinforcement learning, unlike previous approaches that relied on domain-specific annotated data. - Experimental results demonstrate that RPT significantly improves the accuracy of next-token prediction and provides a stronger pre-trained foundation for subsequent reinforcement fine-tuning, achieving state-of-the-art results on various downstream tasks. - The scaling curves show that RPT's performance improves consistently with increased training compute across different data difficulty levels. - This work positions RPT as a promising scaling paradigm to advance language model pre-training. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical
  Understanding and Reasoning](https://arxiv.org/abs/2506.07044) | 26hzhang, gowitheflow, Jianyu, kenchan0226, xww033 | This paper introduces LINGSHU, a new multimodal large language model (MLLM) specialized for medical applications.  LINGSHU's architecture is based on the Qwen2.5-VL model and undergoes multi-stage training, incorporating medical image-text pairs, medical texts, and general-domain data.  The model is evaluated on various benchmarks, demonstrating superior performance over existing open-source models in most cases.  Further, the authors introduce MEDEVALKIT, a unified evaluation framework for medical multimodal models. The use of reinforcement learning with verifiable rewards is also explored to improve the model's reasoning capabilities. | ['Multimodal', 'Visual Question Answering', 'Question Answering', 'Text Generation', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/lingshu-medical-mllm) |
| [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900) | Yuxuan Li, MiniCPM Team, BigDong, guojunshaoyao, xcjthu |  - This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed for end-side devices.  - MiniCPM4 achieves efficiency through innovations in model architecture (InfLLM v2, a trainable sparse attention mechanism), training data (UltraClean data filtering and UltraChat v2 fine-tuning dataset), training algorithms (ModelTunnel v2 and chunk-wise rollout for RL), and inference systems (CPM.cu). - MiniCPM4 outperforms similar-sized open-source models on multiple benchmarks, demonstrating significant speed improvements over Qwen3-8B for long sequences. - The model is available in 0.5B and 8B parameter versions, suitable for diverse on-device applications. - MiniCPM4 successfully powers diverse applications, showcasing its broad usability through its application in generating surveys and tool use with model context protocols. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Summarization', 'Question Answering'] | [Link](https://github.com/openbmb/minicpm) | [Link](https://huggingface.co/openbmb/MiniCPM4-8B), [Link](https://huggingface.co/openbmb/MiniCPM4-0.5B) |
| [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety
  Assurance](https://arxiv.org/abs/2506.06444) | Hanghang Tong, Jingrui He, Tianxin Wei, Gaotang Li, Ruizhong Qiu | - This paper introduces SAFFRON, a novel inference scaling paradigm for enhancing the safety of LLMs. - SAFFRON employs a multifurcation reward model (MRM) to significantly reduce the number of reward model calls during inference, addressing the exploration-efficiency dilemma. - The MRM is trained using a partial supervision objective and incorporates a conservative exploration constraint to prevent out-of-distribution explorations. - Experiments demonstrate that SAFFRON outperforms existing advanced inference scaling methods on challenging jailbreaking attacks, achieving considerably lower attack success rates with lower computational costs. - The authors release the trained MRM (SAFFRON-1) and a token-level safety reward dataset (Safety4M) to promote future research in LLM safety. | ['Natural Language Processing'] | [Link](https://github.com/q-rz/saffron) | N/A |
| [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491) | Rui Tang, Chuan Fang, Junhao Zhong, bertjiazheng, ysmao | - This paper introduces SPATIALLM, a large language model designed for processing 3D point cloud data and generating structured 3D scene understanding outputs, including architectural elements and object bounding boxes. - The model uses a standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs, unlike previous methods that rely on task-specific network designs. - SPATIALLM is trained on a large-scale, high-quality synthetic dataset of indoor scenes with 3D annotations. - The paper shows that SPATIALLM achieves state-of-the-art performance in layout estimation and competitive results in 3D object detection on public benchmarks. - The results demonstrate a feasible path for enhancing the spatial understanding capabilities of modern LLMs for augmented reality and embodied robotics. | ['Text-to-3D', 'Object Detection', 'Image-to-3D', 'Multimodal'] | [Link](https://manycore-research.github.io/SpatialLM) | N/A |
| [Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal
  Learning](https://arxiv.org/abs/2506.06205) | Yansheng Wang, Ziyang Liu, Jiaxin Hu, Peiyu He, sc-bd | - This paper introduces Astra, a novel dual-model architecture for mobile robot navigation that uses a multimodal large language model (MLLM) for high-level tasks and a multitask network for low-level tasks. - Astra-Global, the MLLM, handles goal and self-localization using a hybrid topological-semantic graph as a global map and outperforms traditional visual place recognition methods. - Astra-Local, the multitask network, manages local path planning and odometry estimation using a 4D spatial-temporal encoder and a novel masked ESDF loss to minimize collisions. - When deployed on real robots, Astra achieves a high end-to-end mission success rate across diverse indoor environments. - The experiments demonstrate that Astra outperforms existing methods in terms of goal and self-localization accuracy and path planning efficiency. | ['Robotics', 'Multimodal', 'Reinforcement Learning'] | [Link](https://astra-mobility.github.io/) | N/A |
| [Through the Valley: Path to Effective Long CoT Training for Small
  Language Models](https://arxiv.org/abs/2506.07712) | Wei Lu, Jiaxi Li, Albus-Chen, RogerLos | - This paper introduces the concept of "Long CoT Degradation", a phenomenon where small language models (SLMs) trained on extensive long chain-of-thought (CoT) data experience significant performance drops. - The authors demonstrate that this degradation is prevalent across various SLMs and is attributed to error accumulation in longer reasoning traces. - They propose two hypotheses to explain this phenomenon: early adoption of surface-level reasoning patterns leading to verbose outputs and error accumulation in longer responses. - Their empirical findings show that sufficiently scaled supervised fine-tuning (SFT) can alleviate Long CoT Degradation and improve the efficiency of subsequent reinforcement learning (RL). - The research provides practical guidance for building more effective small-scale reasoning models by carefully considering the scale of long CoT data during training. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298) | Jennifer J. Sun, Yahya Satter, Zhaolin Gao, sarahdean, DaiYijia |  - This paper demonstrates that pre-trained large language models (LLMs) can effectively model data generated by hidden Markov models (HMMs) through in-context learning.  - Using a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum.   - Novel scaling trends are identified, influenced by HMM properties, and theoretical conjectures are proposed to explain these trends.   - Practical guidelines are offered for using in-context learning as a diagnostic tool, with real-world applications in animal decision-making achieving performance comparable to models built by human experts.  - This is the first demonstration that in-context learning in LLMs can learn and predict HMM-generated sequences. | ['Natural Language Processing'] | [Link](https://github.com/DaiYijia02/icl-hmm) | N/A |
| [The Illusion of Thinking: Understanding the Strengths and Limitations of
  Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941) | Samy Bengio, Maxwell Horton, Keivan Alizadeh, Iman Mirzadeh, parshinsh | - This paper investigates the strengths and limitations of Large Reasoning Models (LRMs) using controlled puzzle environments. - The study reveals three performance regimes: standard LLMs outperform LRMs at low complexity, LRMs excel at moderate complexity, and both collapse at high complexity. - The findings show that LRMs exhibit counterintuitive scaling limits: their reasoning effort increases up to a point then declines despite having an adequate token budget. - The authors analyze reasoning traces in detail, finding that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. - The research highlights both strengths and limitations of existing LRMs, raising crucial questions about their true reasoning capabilities. | ['Natural Language Processing'] | N/A | N/A |
| [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large
  Language Models](https://arxiv.org/abs/2506.07463) | Yang Yu, Jijie Li, Yonghua, ldwang, ZacLiu | - This paper introduces CCI4.0, a large-scale bilingual (Chinese and English) pre-training dataset designed to enhance reasoning in large language models. - CCI4.0 comprises two sub-datasets: CCI4.0-M2-Base (a 35TB corpus combining various web corpora, mathematical, wiki, and code data) and CCI4.0-M2-CoT (4.5 billion chain-of-thought templates). - A novel data processing pipeline was developed, including deduplication, quality scoring, and domain-aware fluency filtering, to ensure high data quality. - Experiments show CCI4.0 consistently outperforms existing datasets across various downstream tasks, especially in mathematics and code-related tasks. - The results highlight the importance of high-quality, diverse, and reasoning-focused data in improving LLM performance. | ['Natural Language Processing'] | N/A | N/A |
| [Well Begun is Half Done: Low-resource Preference Alignment by
  Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434) | Tianyu Liu, Yuxuan Fan, Wen Luo, SylvainWei, songff | - The paper introduces a novel framework called Weak-to-Strong Decoding (WSD) for low-resource preference alignment in large language models (LLMs). - WSD uses a small, pre-trained model to generate well-aligned response beginnings, which are then extended by a larger base LLM. - A new dataset, GenerAlign, was created to fine-tune the small draft model, enhancing its alignment capabilities. - Experiments demonstrated that WSD outperforms several baseline methods on various benchmarks without degrading performance on downstream tasks. - The effectiveness of WSD is further analyzed through ablation studies focusing on the impact of hyperparameters such as window size, threshold, and maximum draft length. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection
  Behavior](https://arxiv.org/abs/2506.08012) | Lewei Lu, Jiaheng Yu, Bo Wang, Shengnan Ma, Penghao Wu | This paper introduces GUI-Reflection, a novel framework that enhances multimodal GUI models by integrating self-reflection and error correction capabilities.  GUI-Reflection leverages three dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning, and online reflection tuning.  The framework incorporates a GUI-Reflection Task Suite to explicitly train reflection-oriented abilities and automatically generates reflection data from successful trajectories.  Experiments show that GUI-Reflection significantly improves the model's ability to recover from errors and adapt to challenging tasks, achieving a 34.72% success rate on level-2 tasks in online evaluation. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://penghao-wu.github.io/GUI_Reflection/) | N/A |
| [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309) | Alicia Sun, Vera Yan, Kai Sun, Yifan Ethan Xu, MaggieHuang | - This paper introduces ConfQA, a novel fine-tuning strategy for Large Language Models (LLMs) designed to reduce hallucination rates. - ConfQA achieves this by training the LLM to respond with the answer only when it is highly confident; otherwise, it admits uncertainty. - The method incorporates a dampening prompt ("Answer only if you are confident") and uses simple factual statements from knowledge graphs for training, leading to robust generalization. - Experiments demonstrate a significant reduction in hallucination rate (from 20-40% to under 5%) across multiple benchmarks. - ConfQA is further integrated into a Dual Neural Knowledge framework that combines internal and external knowledge sources to improve accuracy while reducing latency. | ['Question Answering'] | N/A | N/A |
| [Dreamland: Controllable World Creation with Simulator and Generative
  Models](https://arxiv.org/abs/2506.08006) | Honglin He, Weizhen Wang, Leon Liu, Ziyang Leng, Sicheng Mo | - Dreamland is a novel hybrid world generation framework that combines a physics-based simulator with large-scale pre-trained generative models to create realistic and controllable visual worlds. - It employs a layered world abstraction (LWA) to align the simulator and generative model, enabling fine-grained control and minimizing adaptation costs. - Dreamland outperforms existing baselines by 50.8% in image quality and 17.9% in controllability, demonstrating improved image quality and stronger controllability. - A new dataset, D3Sim, is introduced to facilitate the training and evaluation of hybrid generation pipelines. - Dreamland shows great potential in enhancing embodied agent training, as demonstrated by improvements in downstream tasks like visual question answering. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Video', 'Video Classification', 'Computer Vision', 'Image Segmentation', 'Depth Estimation', 'Reinforcement Learning'] | [Link](https://metadriverse.github.io/dreamland/) | N/A |
| [Bootstrapping World Models from Dynamics Models in Multimodal Foundation
  Models](https://arxiv.org/abs/2506.06006) | Shay B. Cohen, Anna Korhonen, Yftah Ziser, ducdauge, yfqiu-nlp | - This paper introduces a novel method for bootstrapping world models from dynamics models in multimodal foundation models. - The proposed method uses two main strategies: 1) weakly supervised learning from synthetic data generated by the dynamics model and 2) inference-time verification using the dynamics model to score world model candidates. - The authors evaluate their approach on the AURORA-BENCH dataset, demonstrating that their best model achieves performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets. - The findings suggest that acquiring a dynamics model through supervision is significantly easier than acquiring a world model, and dynamics models can effectively bootstrap world models. - This work has implications for improving the capabilities of multimodal foundation models and creating more realistic and robust AI agents. | ['Image-to-Image', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/yfqiu-nlp/vlm-world-model) | N/A |
| [Learning What Reinforcement Learning Can't: Interleaved Online
  Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527) | Xiaochen Ma, Lexiang Tang, Meiyi Qiang, Hao Liang, RoadQAQ | - This paper introduces ReLIFT, a novel training approach that interleaves reinforcement learning (RL) with online fine-tuning (SFT) to improve the reasoning abilities of large language models (LLMs). - ReLIFT addresses the limitations of RL by incorporating SFT to learn what RL cannot, enabling the acquisition of new information and reasoning patterns. - The model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning. - ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. - The experiments demonstrate that ReLIFT outperforms both RL and SFT while using only 13% of the detailed demonstration data, highlighting its scalability and efficiency. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/TheRoadQaQ/ReLIFT) | [Link](https://github.com/huggingface/Math-Verify) |
| [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path
  Lengths in LLMs](https://arxiv.org/abs/2506.07240) | Lior Wolf, Itamar Zimerman, royeis | - This paper introduces a novel method to monitor and control the reasoning process of large language models (LLMs) by manipulating internal progress representations. - The authors introduce an interactive progress bar visualization to make the reasoning process more transparent and easier for users to understand. - They demonstrate that manipulating these internal representations can effectively mitigate overthinking and improve answer accuracy and efficiency. - Their empirical results show that the proposed "overclocking" method outperforms baseline methods in terms of both accuracy and efficiency on several mathematical reasoning benchmarks. - The code for this method is publicly available on GitHub. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/royeisen/reasoning_loading_bar) | [Link](None) |
| [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive
  Policy Optimization](https://arxiv.org/abs/2506.07160) | Qipeng Guo, Zimian Peng, Dianyi Wang, Yibin Wang, LibraTree | - This paper introduces GeometryZero, a family of geometric reasoning models that utilizes Group Contrastive Policy Optimization (GCPO) to improve the performance of LLMs in solving geometry problems. - GCPO addresses the limitations of existing reinforcement learning methods by incorporating two key innovations: Group Contrastive Masking and Length Reward, which help to judiciously determine when to use auxiliary construction. - GeometryZero models consistently outperform baselines on popular geometric benchmarks (Geometry3K, MathVista), achieving an average improvement of 4.29% across all benchmarks. - The paper provides an in-depth ablation study demonstrating the effectiveness of each component in GCPO and detailed analysis of training dynamics. - The results highlight the benefits of using a conditional reward mechanism for auxiliary construction, showing that a flexible approach is crucial for effective geometric reasoning. | ['Reinforcement Learning', 'Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/abs/2506.03690) | Xingyu Lu, Zhibo Zhu, Jiancan Wu, Junkang Wu, Sunshine279 | - This paper introduces γ-PO, a novel dynamic target margin preference optimization algorithm that enhances the robustness of Direct Preference Optimization (DPO). - γ-PO dynamically adjusts reward margins at the pair-wise level, prioritizing high-confidence pairs while suppressing noise from ambiguous pairs. - The proposed method is compatible with existing DPO variants and achieves an average 4.4% improvement over other baselines on AlpacaEval2 and Arena-Hard. - γ-PO requires minimal code changes and has a negligible impact on training efficiency. - Experimental results demonstrate the effectiveness of γ-PO in enhancing LLM alignment and robustness. | ['Natural Language Processing'] | [Link](https://github.com/sunjie279/gammaPO) | N/A |
| [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011) | Junfei Xiao, Alan Yuille, Shiyi Lan, Yinsong Ma, Yunfei Xie | This paper introduces Visual Game Learning (ViGaL), a novel post-training paradigm that enhances multimodal large language models (MLLMs) reasoning abilities by training them to play arcade-like games using reinforcement learning. ViGaL achieves out-of-domain generalization on downstream multimodal reasoning tasks without using in-domain data during training, outperforming models trained on multimodal reasoning data. The authors demonstrate that gameplay post-training enables the capture of transferable reasoning skills, unlocking generalizable multimodal reasoning abilities in MLLMs. ViGaL's effectiveness stems from the use of simple arcade games as controllable and scalable pre-text tasks. Finally, this method preserves the base model's performance on general visual benchmarks. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/yunfeixie233/ViGaL) | N/A |
| [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833) | Dacheng Tao, Jiaxing Huang, Xikun Zhang, michaelchenkj | - This paper introduces Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that improves large language models (LLMs) by enabling the learning of sequences spanning multiple tokens. - CAFT addresses the limitation of existing next-token prediction paradigms which hinder the formation of coherent, high-level concepts. - The method is evaluated on diverse tasks, including text summarization, code generation, and molecular design, demonstrating significant improvements over conventional next-token fine-tuning methods. - CAFT's effectiveness suggests that models do not sufficiently learn and plan beyond the next immediate token, and that an explicit multi-token objective is more effective. - The authors provide open-source code and data to facilitate broader adoption and further research in this area. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/michaelchen-lab/caft-llm) | [Link](null) |
| [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645) | Karolina Seweryn, llmAttack, mchraba | - This paper introduces a novel framework for evaluating the robustness of Large Language Models (LLMs) in less-resourced languages. - The framework utilizes proxy models and attribution methods to identify and perturb the most important words in a sentence, generating human-understandable perturbed examples. - The proposed methodology is validated on Polish, a low-resource language, demonstrating the potential vulnerabilities of LLMs to character and word-level attacks. - The authors find that surprisingly strong attacks can be cheaply created by altering just a few characters and words, drastically altering the predictions of different LLMs. - The created datasets and code are publicly released to facilitate further research in this area. | ['Natural Language Processing'] | N/A | N/A |
| [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and
  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473) | Chong Teng, Fei Li, Xin Zhang, Xiaofeng Mao, Xiaorui Wu | - This paper introduces EVOREFUSE, a novel prompt optimization algorithm that leverages evolutionary search to generate diverse pseudo-malicious instructions that consistently elicit confident refusals across various LLMs. - EVOREFUSE outperforms existing methods by achieving a 140.41% higher average refusal triggering rate, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores. - Two novel datasets are created using EVOREFUSE: EVOREFUSE-TEST (582 pseudo-malicious instructions) and EVOREFUSE-ALIGN (3,000 instructions with responses for supervised and preference-based alignment training). - Fine-tuning LLAMA3.1-8B-INSTRUCT on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset without compromising safety. - The analysis reveals that models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. | ['Natural Language Processing'] | N/A | N/A |
