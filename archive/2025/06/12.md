

## Papers for 2025-06-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Autoregressive Adversarial Post-Training for Real-Time Interactive Video
  Generation](https://arxiv.org/abs/2506.09350) | Yuxi Ren, Jianwen Jiang, Hao He, Ceyuan Yang, Shanchuan Lin | - This paper introduces Autoregressive Adversarial Post-Training (AAPT), a novel method that transforms pre-trained latent video diffusion models into real-time interactive video generators. - AAPT employs a single neural function evaluation (1NFE) to autoregressively generate latent frames, enabling real-time streaming and interactive control. - The model architecture utilizes a causal transformer with block causal attention and a KV cache for efficient one-step generation. - Experimental results demonstrate that the proposed 8B parameter model achieves real-time 24fps video generation at 736x416 resolution on a single H100 GPU, outperforming existing state-of-the-art methods. - The effectiveness of the adversarial training paradigm and long-video training is showcased through experiments on pose-conditioned virtual human generation and camera-controlled world exploration. | ['Image-to-Video', 'Text-to-Video', 'Multimodal'] | N/A | N/A |
| [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790) | Weihua Luo, Longyue Wang, Xue Yang, Yiyu Wang, Zhenran Xu | - ComfyUI-R1 is a large reasoning model for automated workflow generation in ComfyUI, achieving a 97% format validity rate and surpassing previous state-of-the-art methods. - The model uses a two-stage training framework: (1) cold-start CoT fine-tuning and (2) reinforcement learning with a rule-metric hybrid reward function. - ComfyUI-R1 outperforms existing methods such as GPT-40 and Claude series in terms of format validity rate, node-level and graph-level F1 scores and pass rate on ComfyBench. - The model leverages a code-based workflow representation for superior performance compared to a JSON-based representation. - Qualitative comparison demonstrates ComfyUI-R1's capability in handling complex workflows and ensuring alignment with user instructions. | ['Image-to-Image', 'Text-to-Image', 'Multimodal'] | [Link](https://github.com/AIDC-AI/ComfyUI-Copilot) | N/A |
| [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889) | Yu Cheng, Yuqing Xia, Shijie Cao, Shuming Guo, Yizhao Gao | - SeerAttention-R is a novel sparse attention framework designed to enhance the efficiency of long decoding in reasoning models. - It extends SeerAttention by removing query pooling and introducing modifications to support autoregressive decoding, improving efficiency. - The model uses a lightweight plug-in gating mechanism that can be easily integrated into existing pretrained models without modifying original parameters. - Experiments demonstrate that SeerAttention-R achieves near-lossless accuracy with large sparse attention blocks (64/128), outperforming existing methods. - An optimized sparse decoding kernel achieves near-theoretical speedups, demonstrating the efficiency and scalability of the proposed framework. | ['Natural Language Processing'] | [Link](https://github.com/microsoft/SeerAttention) | [Link](string) |
| [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003) | Mouxiang Chen, Jian Yang, Min Yang, Jiaxi Yang, Lei Zhang | SWE-Flow is a novel data synthesis framework that leverages Test-Driven Development (TDD) to automatically generate software engineering data.  The core of SWE-Flow is the construction of a Runtime Dependency Graph (RDG) which captures function interactions, enabling the generation of a structured development schedule. At each step, SWE-Flow produces a partial codebase, corresponding unit tests, and code modifications, resulting in fully verifiable TDD tasks.  SWE-Flow generated 16,061 training and 2,020 test instances from real-world projects, creating the SWE-Flow-Bench benchmark.  Experiments showed fine-tuning on this dataset significantly improved performance in TDD-based coding tasks. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Hambaobao/SWE-Flow) | N/A |
| [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio
  Conditions](https://arxiv.org/abs/2506.09984) | Gaojie Lin, Chao Liang, Jianwen Jiang, Jiaqi Yang, Zhenzhi Wang | - InterActHuman is a novel framework for multi-concept human animation that uses layout-aligned audio conditions. - It addresses the limitations of existing methods by introducing an attention module to explicitly predict spatial locations of concepts and bind audio conditions to corresponding regions. - The model is trained using a large dataset of human-centric videos with over two million video-entity pairs. - InterActHuman outperforms existing methods in terms of lip synchronization, motion diversity, and overall video quality, as demonstrated in both qualitative and quantitative evaluations. - The framework introduces a novel iterative mask prediction strategy to achieve accurate spatial alignment of multi-modal conditions during inference. | ['Text-to-Video', 'Multimodal', 'Audio'] | N/A | N/A |
| [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001) | Bernhard Schölkopf, Maximilian Dax, Tim Z. Xiao, Simon Buchholz, Zeju Qiu |  - This paper introduces POET, a novel reparameterized training algorithm for LLMs that uses orthogonal equivalence transformation to optimize neurons.  - POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix, which preserves the spectral properties of weight matrices.  - The proposed method demonstrates improved generalization and stability compared to standard AdamW optimization, achieving state-of-the-art validation perplexity results in several large-scale LLaMA model experiments.  - POET introduces two levels of approximations for efficient training: stochastic primitive optimization and approximate orthogonality via Cayley-Neumann parameterization.  - Extensive experiments demonstrate POET's effectiveness and scalability in training LLMs. | ['Natural Language Processing'] | [Link](https://github.com/jiaweizzhao/GaLore) | [Link](https://huggingface.co/) |
| [MIRAGE: Multimodal foundation model and benchmark for comprehensive
  retinal OCT image analysis](https://arxiv.org/abs/2506.08900) | Taha Emre, Ronald Fecso, Emese Sükei, Botond Fazekas, José Morano |  - The paper introduces MIRAGE, a novel multimodal foundation model for retinal OCT/SLO image analysis, trained using a paired multimodal Masked Autoencoder (MAE) approach.  - MIRAGE significantly outperforms state-of-the-art foundation models on both classification and segmentation tasks, showcasing its robustness and generalization capabilities.  - A new comprehensive evaluation benchmark is also proposed for validating foundation models in retinal OCT/SLO analysis, including classification and segmentation tasks across multiple datasets.  - The MIRAGE model is based on a Vision Transformer encoder with modality-specific linear projection layers and Transformer decoders, effectively utilizing complementary information from OCT and SLO images.  - The superior performance of MIRAGE across multiple datasets highlights its suitability for developing robust AI systems for retinal image analysis, making it a valuable tool for clinicians. | ['Image Segmentation', 'Image Classification', 'Multimodal'] | [Link](https://github.com/j-morano/MIRAGE) | N/A |
