

## Papers for 2025-06-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600) | Guy Uziel, Matan Vetzler, Koren Lazar, George Kour, Itay Nakash | - This paper introduces CRAFT, a novel multi-agent red-teaming system designed to evaluate the robustness of policy-adherent Large Language Model (LLM)-based agents against adversarial users. - CRAFT leverages policy-aware persuasive strategies and outperforms conventional methods such as DAN prompts in bypassing safety policies. - The authors introduce T-break, a complementary benchmark built upon T-bench to rigorously assess agent resilience against manipulation. - Several straightforward defense strategies are evaluated, highlighting the need for stronger research-driven safeguards. - The findings reveal critical vulnerabilities in policy-adherent agents and underscore the importance of developing robust defenses against adversarial attacks. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [The Diffusion Duality](https://arxiv.org/abs/2506.10892) | Justin Chiu, Guanghan Wang, Aaron Gokaslan, Justin Deschenaux, Subham Sekhar Sahoo | This paper introduces Duo, a novel framework that leverages the duality between Gaussian and Uniform-state discrete diffusion models to improve text generation.  Duo employs curriculum learning guided by the underlying Gaussian process to accelerate training and surpass autoregressive models in zero-shot perplexity on several benchmarks.  A new distillation technique, Discrete Consistency Distillation, is introduced to enhance sampling speed, achieving a two-order magnitude improvement over previous methods.  The framework transfers advanced techniques from Gaussian diffusion to improve both training and sampling efficiency in USDMs.  The code and model checkpoints are available on the project page. | ['Text Generation'] | [Link](https://github.com/s-sahoo/duo) | N/A |
| [LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive
  Programming?](https://arxiv.org/abs/2506.11928) | Kaiyuan Liu, Shang Zhou, Zeyu Shen, Zerui Cheng, Zihan Zheng | This paper introduces LiveCodeBench Pro, a benchmark for evaluating LLMs in competitive programming. It uses problems from Codeforces, ICPC, and IOI, annotated by Olympiad medalists.  The benchmark reveals that frontier models have significant limitations in algorithmic reasoning and complex case analysis, achieving only 53% pass@1 on medium-difficulty problems and 0% on hard problems. High performance is largely driven by implementation precision, not superior reasoning.  LiveCodeBench Pro highlights the gap between LLMs and human grandmasters, providing diagnostics to improve code-centric LLM reasoning. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [A High-Quality Dataset and Reliable Evaluation for Interleaved
  Image-Text Generation](https://arxiv.org/abs/2506.09427) | kpzhang, ZhangShenglin, fanrui00, cyrilli, finyorko |  - This paper introduces InterSyn, a large-scale multimodal dataset for instruction-following, multi-turn image-text generation, and SynJudge, an automatic evaluation model assessing multimodal outputs along four dimensions. - The Self-Evaluation with Iterative Refinement (SEIR) method used to build InterSyn yielded substantially higher dataset quality compared to an identical process without refinement, as shown via experimental studies. - SynJudge, the automatic evaluation model, aligns well with human judgment and provides interpretable quantitative feedback, facilitating more effective model training. - LMMs trained on InterSyn demonstrated uniform performance gains across all evaluation metrics, confirming its utility for advancing multimodal systems. - InterSyn contains approximately 1.8 million single-turn samples and 50k multi-turn dialogues, offering a robust foundation for training unified multimodal models. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Visual Question Answering', 'Text Generation'] | N/A | N/A |
| [Detecting Harmful Memes with Decoupled Understanding and Guided CoT
  Reasoning](https://arxiv.org/abs/2506.08477) | Anh Tuan Luu, Fengjun Pan, bobxwu | This paper introduces U-CoT+, a novel framework for harmful meme detection that addresses limitations of existing methods in resource efficiency, flexibility, and explainability.  The framework decouples meme interpretation from classification using a meme-to-text pipeline, enabling resource-efficient detection with LLMs.  Targeted, human-crafted guidelines are incorporated to guide CoT prompting, allowing easy adaptation to different criteria.  Experiments on seven benchmark datasets demonstrate U-CoT+'s effectiveness, highlighting its potential for low-resource settings and achieving comparable performance to state-of-the-art, fully supervised methods. | ['Zero-Shot Classification', 'Text Classification', 'Multimodal'] | [Link](https://anonymous.4open.science/r/HMC-AF2B/README.md) | N/A |
| [Beyond Homogeneous Attention: Memory-Efficient LLMs via
  Fourier-Approximated KV Cache](https://arxiv.org/abs/2506.11886) | Yuerong Song, Ruixiao Li, Qiqi Wang, Siyang He, Xiaoran Liu | - This paper introduces FourierAttention, a novel training-free framework for memory-efficient LLMs that exploits the heterogeneous roles of transformer head dimensions. - FourierAttention prioritizes local context using lower dimensions and captures long-range dependencies using upper dimensions projected onto orthogonal Fourier bases, approximating their temporal evolution with fixed-length spectral coefficients. - Evaluation on LLaMA models demonstrates that FourierAttention achieves state-of-the-art long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). - The proposed method is implemented using a custom Triton kernel, FlashFourierAttention, for efficient memory management and deployment without performance compromise. - Experimental results show that FourierAttention outperforms existing training-free KV cache compression methods on LongBench and NIAH, achieving the best long-context accuracy while maintaining lower memory consumption. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Configurable Preference Tuning with Rubric-Guided Synthetic Data](https://arxiv.org/abs/2506.11702) | vicgalle | - This paper introduces Configurable Preference Tuning (CPT), a novel framework that allows language models to dynamically adjust their behavior based on human-interpretable directives, without retraining. - CPT leverages synthetically generated preference data conditioned on system prompts derived from structured, fine-grained rubrics, which define desired attributes such as writing style. - The model is fine-tuned using a DPO-style objective with rubric-guided preference pairs, enabling it to modulate its outputs at inference time in response to system prompts. - Experiments demonstrate that CPT significantly improves the models' ability to adhere to system-prompted configurations, achieving higher accuracy and stronger rank correlations compared to baseline models. - CPT is shown to enhance other techniques, such as Best-of-N sampling, by improving generation efficiency and quality. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/vicgalle/configurable-preference-tuning) | [Link](https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences) |
| [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual
  Perception in VLMs](https://arxiv.org/abs/2506.10128) | Yuhang Zhou, Yongyuan Liang, Chao Feng, Zhengyuan Yang, Xiyao Wang | - ViCrit is a novel reinforcement learning proxy task designed to enhance visual perception in vision-language models (VLMs) by training them to identify subtle, synthetic visual hallucinations in image captions. - The task involves injecting minor visual errors into detailed image captions and training the VLM to pinpoint the erroneous span using a binary reward system, which is computationally efficient and unambiguous. - ViCrit training consistently improves performance on various vision-language benchmarks, including those involving abstract image reasoning and visual mathematics. - ViCrit-Bench, a new benchmark dataset with diverse image domains and fine-grained hallucination categories, is also introduced to evaluate and diagnose VLMs' visual perception abilities.  The results on ViCrit-Bench strongly correlate with overall VLM performance on general tasks. - The experiments demonstrate that ViCrit effectively enhances the fine-grained visual perception of VLMs, leading to improved performance across various downstream tasks. | ['Reinforcement Learning', 'Multimodal', 'Image-to-Text', 'Visual Question Answering'] | [Link](https://github.com/si0wang/ViCrit) | [Link](https://huggingface.co/collections/russwang/ViCrit), [Link](https://huggingface.co/datasets/zyang39/ViCrit-Train), [Link](https://huggingface.co/datasets/russwang/ViCrit-Bench) |
| [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity
  Dilemma of Embeddings](https://arxiv.org/abs/2506.08592) | Fandong Meng, Jiangnan Li, Mo Yu, Zhenlin Su, lxucs | - This paper introduces CapRetrieval, a new Chinese evaluation dataset for dense retrieval, focusing on the challenge of fine-grained semantic matching in image captions. - The dataset consists of image captions as passages and short phrases as queries, requiring fine-grained semantic understanding for accurate retrieval. - Zero-shot evaluation on CapRetrieval reveals limitations of existing text encoders in handling fine-grained details, regardless of model size or training data. - The authors propose data generation strategies using LLMs to enhance encoder training and address the identified granularity dilemma. - Finetuning with the proposed strategies improves performance on CapRetrieval, surpassing even large language models in zero-shot settings. | ['Question Answering'] | [Link](https://github.com/lxucs/CapRetrieval) | N/A |
