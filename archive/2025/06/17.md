

## Papers for 2025-06-17

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning
  Attention](https://arxiv.org/abs/2506.13585) | ManTle, windlx, LINMUJIE-judy, enochzhang, sheep33333 | - MiniMax-M1 is introduced, a novel open-weight, large-scale hybrid-attention reasoning model featuring a hybrid Mixture-of-Experts (MoE) architecture and a lightning attention mechanism.  - The model boasts 456 billion parameters and supports a context length of 1 million tokens, outperforming existing models in terms of context size. -  MiniMax-M1 demonstrates superior test-time compute efficiency compared to DeepSeek R1, consuming only 25% of the FLOPs at a generation length of 100K tokens. - A novel reinforcement learning algorithm, CISPO, is proposed to further enhance training efficiency, enabling full RL training within three weeks on 512 H800 GPUs.  - Experiments show MiniMax-M1 achieves competitive or superior results to other leading models across various benchmarks, particularly excelling in complex software engineering, tool utilization, and long-context tasks. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/MiniMax-AI/MiniMax-M1) | N/A |
| [Scientists' First Exam: Probing Cognitive Abilities of MLLM via
  Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521) | Ruoyao Xiao, Xuming He, Yiheng Wang, Yuhao Zhou, WilsonHwang |  - This paper introduces a new benchmark, Scientists' First Exam (SFE), designed to evaluate the scientific cognitive abilities of Multimodal Large Language Models (MLLMs).  - The benchmark comprises 830 expert-verified VQA pairs across three cognitive levels (perception, understanding, and reasoning), covering 66 multimodal tasks across five high-value disciplines.  - Experiments show that state-of-the-art models like GPT-03 and InternVL-3 achieve low scores (34.08% and 26.52% respectively), highlighting significant room for improvement.  - SFE categorizes scientific tasks by cognitive capacity, introducing a three-level taxonomy, and releases bilingual (English & Chinese) tasks constructed from native scientific data formats.  - The benchmark reveals performance gaps across disciplines and model types, indicating potential shifts in MLLM capabilities from knowledge understanding to high-order reasoning. | ['Visual Question Answering', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/PrismaX/SFE) |
| [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves
  Reasoning Efficiency](https://arxiv.org/abs/2506.08343) | Ranjay Krishna, Zhaoyang Chu, Dongping Chen, Yuanning Feng, Chenlong Wang | - This paper introduces NOWAIT, a training-free method that improves the efficiency of large reasoning models (LRMs) by suppressing self-reflection tokens like "Wait" and "Hmm" during inference. - NOWAIT reduces the chain-of-thought (CoT) trajectory length by 27%-51% across ten benchmarks spanning textual, visual, and video reasoning tasks without compromising model utility. - The method demonstrates consistent improvements across five R1-style model series and various reasoning tasks, showcasing its generalizability. - Experimental results show that NOWAIT outperforms existing training-free approaches such as Token-Budget while achieving comparable or even better accuracy than training-based methods like 01-Pruner. - The authors conclude that explicit self-reflection is not essential for advanced reasoning and that NOWAIT provides a plug-and-play solution for efficient and utility-preserving multimodal reasoning. | ['Natural Language Processing', 'Question Answering', 'Multimodal'] | N/A | N/A |
| [Discrete Diffusion in Large Language and Multimodal Models: A Survey](https://arxiv.org/abs/2506.13759) | Xinchao Wang, Qi Li, Runpeng Yu | This survey paper provides a comprehensive overview of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).  It systematically analyzes the mathematical foundations, model architectures, training strategies, and inference techniques used in this emerging field.  The authors categorize representative models, highlighting their capabilities and limitations compared to autoregressive approaches.  Finally, the paper concludes by discussing future research directions and potential applications across various domains. | ['Natural Language Processing', 'Text Generation', 'Multimodal'] | [Link](https://github.com/LiQiiiii/DLLM-Survey) | N/A |
| [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055) | Weizhen Li, Weichen Sun, Qianben Chen, Jingyi Cao, Dingfeng Shi | - This paper introduces TaskCraft, an automated workflow for generating agentic tasks (tasks requiring multi-step problem-solving with autonomy, tool use, and adaptive reasoning) with execution trajectories. - TaskCraft addresses the limitations of existing instruction data by automatically generating difficulty-scalable, multi-tool, and verifiable tasks. - The workflow uses depth-based and width-based extensions to create structurally and hierarchically complex tasks, improving prompt optimization and supervised fine-tuning of agentic foundation models. - Experiments demonstrate that the generated tasks improve performance on various benchmarks, including HotpotQA, Musique, and Bamboogle. - TaskCraft provides a large-scale synthetic dataset (approximately 36,000 tasks) with varying difficulty to support future research. | ['Natural Language Processing', 'Text Generation', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/OPPO-PersonalAI/TaskCraft) | N/A |
| [VGR: Visual Grounded Reasoning](https://arxiv.org/abs/2506.11991) | Haiyong Jiang, Haochen Wang, Zijiang Kang, bongbohong, stormthunder | - This paper introduces VGR, a novel multimodal large language model (MLLM) designed for visual grounded reasoning. - VGR enhances fine-grained visual perception by first detecting relevant image regions and then incorporating them into the reasoning process using a selective visual replay mechanism. - The model was trained on a new large-scale dataset called VGR-SFT containing reasoning data with mixed vision grounding and language deduction. - Experimental results on various multimodal benchmarks demonstrate that VGR outperforms the baseline LLaVA-NeXT-7B by significant margins while using only 30% of the image tokens. - VGR's self-driven selective visual replay method improves both accuracy and interpretability of multi-modal reasoning. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | [Link](https://huggingface.co/BytedanceDouyinContent/VGR) |
| [PersonaFeedback: A Large-scale Human-annotated Benchmark For
  Personalization](https://arxiv.org/abs/2506.12915) | Yuchen Eleanor Jiang, Tiannan Wang, Dongyi Ding, Chenghao Zhu, Meiling Tao | - This paper introduces PERSONAFEEDBACK, a new large-scale human-annotated benchmark dataset for evaluating the personalization capabilities of large language models (LLMs). - The dataset consists of 8298 human-annotated test cases categorized into easy, medium, and hard tiers based on the complexity of user personas and the difficulty of distinguishing between personalized responses. - PERSONAFEEDBACK decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas, unlike existing benchmarks. - Empirical results show that even state-of-the-art LLMs struggle with the harder tasks in PERSONAFEEDBACK, highlighting the challenges in LLM personalization. - All benchmark data, annotation protocols, and evaluation pipeline are publicly available to facilitate future research on LLM personalization. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/PersonalAILab/PersonaFeedback) |
| [From Real to Synthetic: Synthesizing Millions of Diversified and
  Complicated User Instructions with Attributed Grounding](https://arxiv.org/abs/2506.03968) | Zhendong Mao, Xiaorui Wang, Benfeng Xu, IgnoraZ | - This paper introduces a novel framework for synthesizing a large-scale dataset of diverse and complex user instructions with attributed grounding. - The framework utilizes a top-down attribution process, grounding real instructions to users and motivations, and a bottom-up synthesis process, leveraging web documents to generate instructions. - A dataset of 1 million instructions, called SYNTHQUESTIONS, is constructed using this framework. - Models trained on SYNTHQUESTIONS achieve state-of-the-art performance on several common benchmarks, outperforming models trained on datasets 10 times larger. - The improvements continually scale with the addition of more web corpora. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Ignoramus0817/SynthQuestions) | N/A |
| [Language Surgery in Multilingual Large Language Models](https://arxiv.org/abs/2506.12450) | Muhammad Ilham Ghozali, samuel-cahyawijaya, tackhwa, muhammadravi251001, joanitolopo | This paper introduces Inference-Time Language Control (ITLC), a novel method for enhancing cross-lingual performance in large language models (LLMs). ITLC leverages the naturally emerging representation alignment in the middle layers of LLMs to disentangle language-specific and language-agnostic information, enabling precise language control without semantic degradation. Experiments demonstrate ITLC's effectiveness in zero-shot cross-lingual language generation and mitigating language confusion. The approach outperforms existing methods in both tasks, showcasing its potential for improving cross-lingual capabilities of LLMs. ITLC is a practical solution for enhancing cross-lingual performance in LLMs and provides a better understanding of representation alignment. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/SEACrowd/itlc) | N/A |
| [AI Agent Behavioral Science](https://arxiv.org/abs/2506.06366) | Honglin Zhang, Haoye Chai, Yunke Zhang, Lin Chen, JJ-TMT |  - This paper introduces a novel research paradigm: AI Agent Behavioral Science, which studies how AI agents act, adapt, and interact.  - It proposes a framework for understanding individual AI agent behavior, emphasizing intrinsic attributes, environmental constraints, and behavioral feedback. - The paper examines emergent behaviors in multi-agent interactions, categorizing them into cooperative, competitive, and open-ended dynamics. - It discusses the behavioral roles of AI agents in human-agent interaction, such as companion, catalyst, and contender in cooperative settings and manipulator in competitive settings. - Finally, it addresses the importance of AI Agent Behavioral Science for responsible AI and proposes six promising research directions to further this new field. | ['Reinforcement Learning', 'Natural Language Processing', 'Multimodal'] | N/A | N/A |
| [Supernova Event Dataset: Interpreting Large Language Model's Personality
  through Critical Event Analysis](https://arxiv.org/abs/2506.12189) | Ioana Ciucă, pranavAL2109 |  - This paper introduces the Supernova Event Dataset, a new dataset for evaluating LLMs' ability to extract and rank critical events from various text types.  - The dataset consists of Wikipedia articles on biographies, news events, historical events, and scientific discoveries.  - A novel framework is proposed where one LLM acts as a judge, evaluating other LLMs' personality based on their event selection and ranking.  - The authors find distinct personality traits across different LLMs, showing how models prioritize events based on their underlying reasoning styles.  - This work helps improve LLM interpretability and provides a valuable tool for further research on LLM personality and alignment. | ['Natural Language Processing'] | N/A | N/A |
| [MS4UI: A Dataset for Multi-modal Summarization of User Interface
  Instructional Videos](https://arxiv.org/abs/2506.12623) | Jiuxiang Gu, Seunghyun Yoon, Hao Tan, Yuan Zang, Franck-Dernoncourt | - This paper introduces MS4UI, a new dataset for multi-modal summarization of user interface (UI) instructional videos. - The dataset contains 2,413 videos totaling 167 hours, manually annotated for video segmentation, text summarization, and video summarization. - Three core tasks are proposed: video segmentation, text summarization, and video summarization, focusing on generating concise and executable step-by-step instructions and illustrations. - Experiments show that existing multi-modal summarization methods struggle with UI video summarization, highlighting the need for new methods tailored to this domain. - The dataset includes key frame annotations to illustrate actions and ensure executability of the generated summaries. | ['Video-Text-to-Text', 'Summarization', 'Multimodal'] | N/A | N/A |
| [Profiling News Media for Factuality and Bias Using LLMs and the
  Fact-Checking Methodology of Human Experts](https://arxiv.org/abs/2506.12552) | Preslav Nakov, Maha Tufail Agro, Dilshod Azizov, Zain Muhammad Mujahid | - This paper introduces a novel methodology for assessing the factuality and political bias of news media outlets using Large Language Models (LLMs). - The methodology leverages various prompts designed to emulate the criteria used by human fact-checkers, eliciting responses from LLMs and aggregating them to make predictions. - Experiments demonstrate significant improvements over strong baselines, showcasing the effectiveness of the proposed approach in predicting both factuality and political bias. - An in-depth error analysis reveals the impact of media popularity and region on model performance, highlighting biases towards popular and U.S.-based outlets. - The study also includes an ablation study to identify the crucial components of the dataset that contribute to the improved performance. | ['Text Classification', 'Zero-Shot Classification'] | [Link](https://github.com/mbzuai-nlp/llm-media-profiling) | N/A |
| [Incorporating Domain Knowledge into Materials Tokenization](https://arxiv.org/abs/2506.11115) | SangKeun Lee, SungHo Kim, Junho Kim, Jun-Hyung Park, yerim0210 | - This paper introduces MATTER, a novel tokenization framework that integrates domain knowledge into the tokenization process for materials science. - MATTER uses MatDetector, a material concept identifier trained on a corpus of material knowledge, to score material concepts and prioritize them during token merging. - Experimental results demonstrate that MATTER outperforms existing methods, achieving an average performance gain of 4% in generation tasks and 2% in classification tasks. - The key contributions include a novel domain-specific tokenization framework, a novel scheme for materials tokenization based on MatDetector, and a demonstration of MATTER's superior performance. - The results highlight the importance of incorporating domain knowledge into tokenization strategies for scientific text processing, particularly in specialized domains like materials science. | ['Natural Language Processing', 'Token Classification', 'Text Generation'] | [Link](https://github.com/yerimoh/MATTER) | N/A |
| [Steering LLM Thinking with Budget Guidance](https://arxiv.org/abs/2506.13752) | Chuang Gan, Yang Zhang, Wenshuo Zhao, Junyan Li | - This paper introduces Budget Guidance, a novel method for controlling the reasoning length of LLMs without fine-tuning. - Budget Guidance employs a lightweight predictor that models a Gamma distribution over the remaining thinking length during token generation, guiding the LLM towards the target budget. - The method demonstrates significant improvements in token efficiency and accuracy on challenging math benchmarks compared to baseline methods, achieving up to a 26% accuracy gain under tight budgets. - Budget Guidance exhibits emergent capabilities such as estimating question difficulty and generalizes well to broader tasks. - The source code is available on GitHub. | ['Natural Language Processing'] | [Link](https://github.com/UMass-Embodied-AGI/BudgetGuidance) | N/A |
| [Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging
  Unsubstantiated Claims and Ambiguous Pronouns](https://arxiv.org/abs/2506.13172) | PChemGuy | - This paper introduces a novel methodology for using Large Language Models (LLMs) to perform high-level semantic and linguistic analysis of scholarly manuscripts. - The method employs structured workflow prompts to guide LLMs in identifying unsubstantiated claims and ambiguous pronoun references in abstracts and conclusions. - The proposed approach is evaluated on two state-of-the-art LLMs, Gemini Pro 2.5 Pro and ChatGPT Plus 03, under varied context conditions. - Results show significant divergence in model performance depending on the task and context, highlighting the need for rigorous, model-specific testing. - The findings suggest that structured prompting is a viable methodology for complex textual analysis, but model performance is highly dependent on the interplay between the model, task, and context. | ['Natural Language Processing'] | N/A | N/A |
| [QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299) | Yunho Maeng, Soo Yong Kim, Hyoungseo Cho, Jeonghwa Yoo, Taegyeong Lee | - QGuard is a novel zero-shot safety guard method for multi-modal LLMs that uses question prompting to effectively block harmful prompts. - The method defends against both text-based and multi-modal harmful prompts without requiring any fine-tuning, making it robust against the latest harmful prompts. - Experimental results show that QGuard performs competitively on both text-only and multi-modal harmful datasets, outperforming various baselines. - QGuard employs a white-box analysis of user inputs by analyzing the logits of question prompting, providing valuable insights into the decision-making process. - The proposed method is simple yet effective, making it suitable for real-world applications in mitigating security risks associated with harmful prompts. | ['Multimodal', 'Zero-Shot Classification'] | N/A | N/A |
| [Hatevolution: What Static Benchmarks Don't Tell Us](https://arxiv.org/abs/2506.12148) | Albert Meroño-Peñuela, Yulan He, Barbara McGillivray, Chiara Di Bonaventura | - This paper investigates the robustness of language models in the context of evolving hate speech. - The authors empirically evaluate 20 language models on two hate speech experiments that simulate the temporal dynamics of hate speech. - The findings reveal a significant temporal misalignment between static and time-sensitive evaluations, emphasizing the limitations of using static benchmarks for assessing language model safety. - The study advocates for incorporating time-sensitive linguistic benchmarks in the evaluation of hate speech models to ensure accurate and reliable assessments. - The paper's main contribution lies in demonstrating the limitations of static hate speech benchmarks and advocating for the development of more dynamic and time-sensitive evaluation methods. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/ChiaraDiBonaventura/hatevolution/tree/main) | N/A |
