

## Papers for 2025-06-20

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain
  Perspective](https://arxiv.org/abs/2506.14965) | Yutao Xie, Fan Zhou, Tianyang Liu, Shibo Hao, Zhoujun Cheng | This paper introduces GURU, a large-scale, curated reinforcement learning dataset for LLM reasoning, spanning six diverse domains.  The authors systematically investigate the effectiveness of reinforcement learning across these domains, revealing a nuanced relationship between pretraining exposure and RL performance gains.  Two new models, GURU-7B and GURU-32B, achieve state-of-the-art performance on a unified evaluation suite among open models. The dataset and models are publicly available, and the code is open source. | ['Reinforcement Learning', 'Question Answering', 'Table Question Answering'] | [Link](https://github.com/LLM360/Reasoning360) | N/A |
| [SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](https://arxiv.org/abs/2506.15154) | Dorien Herremans, Abhinaba Roy, Anuradha Chopra | - The paper introduces SonicVerse, a novel multi-task music captioning model that integrates caption generation with auxiliary music feature detection tasks. - SonicVerse uses a projection-based architecture that transforms audio input into language tokens while simultaneously detecting music features through dedicated auxiliary heads, improving caption quality and detail. - The model is trained on an extended MusicBench dataset annotated with music features using MIRFLEX, achieving state-of-the-art performance on several NLP metrics such as BLEU, ROUGE, and BERT. - An LLM-chaining mechanism is used to generate temporally-aware captions for longer music pieces by concatenating short segment captions. - The model and its weights are open-sourced to promote reproducibility and future research. | ['Audio Classification', 'Text Generation', 'Multimodal'] | [Link](https://github.com/AMAAI-Lab/sonicverse) | [Link](https://huggingface.co/m-a-p/MERT-VO) |
| [Improved Iterative Refinement for Chart-to-Code Generation via
  Structured Instruction](https://arxiv.org/abs/2506.14837) | Weiran Huang, Lichao Sun, Yuyang Wang, Chengzhi Xu, WaltonFuture | - This paper introduces ChartIR, a training-free iterative refinement method for chart-to-code generation that uses structured instructions. - ChartIR distinguishes between visual understanding and code translation, using description and difference instructions to transform visual features into language representations. - The method decomposes chart generation into two stages: initial code generation and iterative refinement, enabling progressive enhancement. - Experimental results on Plot2Code and ChartMimic benchmarks show that ChartIR outperforms existing methods (METAL) and direct generation on both open-source (Qwen2-VL) and closed-source (GPT-40) models. - Ablation studies confirm the importance of both structured description and iterative refinement for superior performance. | ['Image-to-Text', 'Text-to-Image', 'Multimodal'] | N/A | N/A |
