

## Papers for 2025-06-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406) | Xuanlei Zhao, Yuhao Zhou, Dongwen Tang, Zhiyuan Liang, VictorKai1996NUS | - This paper introduces Drag-and-Drop LLMs (DnD), a novel prompt-conditioned parameter generator that eliminates the need for per-task training in large language models (LLMs). - DnD maps unlabeled task prompts directly to LoRA weight updates, achieving task-specific parameter generation in seconds, significantly reducing adaptation overhead compared to traditional methods. - The model architecture consists of a lightweight text encoder that distills prompt batches into embeddings, and a cascaded hyper-convolutional decoder that transforms these embeddings into LoRA matrices. - Experiments demonstrate that DnD achieves up to 12,000Ã— lower overhead and average performance gains of up to 30% over the strongest training LoRAs on unseen datasets across various benchmarks. - DnD shows robust cross-domain generalization, highlighting the potential of prompt-conditioned parameter generation as a viable alternative to gradient-based adaptation for rapidly specializing LLMs. | ['Natural Language Processing', 'Text Generation', 'Zero-Shot Classification'] | [Link](https://jerryliang24.github.io/DnD) | N/A |
| [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal
  Document Understanding](https://arxiv.org/abs/2506.16035) | Biddwan Ahmed, Indraneel Das, Tanmay Odapally, udayallu, vishesh-t27 | - This paper introduces a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents, enhancing Retrieval Augmented Generation (RAG) systems. - The approach addresses limitations of traditional text-based methods by preserving semantic coherence and structural integrity across page boundaries, even when handling complex layouts, tables, figures, and visual elements. - It processes documents in configurable page batches with cross-batch context preservation, improving the accuracy of downstream RAG performance and achieving better quantitative results compared to traditional RAG systems. - The method's effectiveness is demonstrated on an internal benchmark dataset of diverse PDF documents, showing better preservation of document structure and semantic coherence. - The paper also contributes a new benchmark dataset and a detailed analysis of chunk quality, addressing limitations of traditional chunking approaches. | ['Document Question Answering', 'Multimodal'] | N/A | N/A |
| [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement
  Learning](https://arxiv.org/abs/2506.09049) | Jie Yang, Yiran Qin, Heng Zhou, Xiufeng Song, FACEONG | - This paper introduces VIKI-Bench, a hierarchical benchmark designed for evaluating visual reasoning in embodied multi-agent cooperation, and VIKI-R, a two-stage framework that leverages vision-language models for this task.  - VIKI-Bench incorporates three levels of visual reasoning tasks: agent activation, task planning, and trajectory perception, using diverse robot embodiments and multi-view observations.  - VIKI-R utilizes a two-stage approach: first, supervised fine-tuning with Chain-of-Thought annotations; then, reinforcement learning with hierarchical reward signals.  - Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels, showing the effectiveness of the proposed approach.  - The work highlights the emergence of compositional cooperation patterns among heterogeneous agents, which is facilitated by reinforcement learning. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://faceong.github.io/VIKI-R/) | N/A |
| [UniFork: Exploring Modality Alignment for Unified Multimodal
  Understanding and Generation](https://arxiv.org/abs/2506.17202) | Xizhou Zhu, Hao Li, Lirui Zhao, Quanfeng Lu, Teng Li | - UniFork is a novel Y-shaped architecture for unified image understanding and generation that addresses the limitations of fully shared Transformer backbones. - The model consists of shared shallow layers for cross-task representation learning and task-specific branches in deeper layers to avoid interference. - UniFork consistently outperforms conventional fully shared Transformer architectures and achieves performance on par with or better than task-specific models on various benchmarks. - Extensive ablation studies demonstrate the effectiveness of the proposed architecture, highlighting the importance of balancing shared learning and task specialization. - The analysis of modality alignment patterns reveals that understanding tasks benefit from progressively increasing alignment across network depth, while generation tasks require a rise-then-fall pattern. | ['Multimodal', 'Text-to-Image', 'Image-to-Text'] | [Link](https://github.com/tliby/UniFork) | N/A |
| [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925) | Kathleen McKeown, Nicholas Deas, narutatsuri |  - This paper introduces a novel reranking-based generation method for creating unbiased perspective summaries.  - The method is evaluated using a new test set designed for evaluating perspective summary quality and newly defined metrics, showing that reranking significantly outperforms zero-shot inference and prompting-based techniques.  - Human evaluations and automatic evaluations using LLM-based metrics (LLM-Coverage and ALIGNSCORE) further support the superiority of the reranking method.  - Preference tuning with synthetic data improves both coverage and faithfulness of the summaries.  - The findings contribute to the reliable development and evaluation of perspective summarization methods. | ['Summarization'] | [Link](https://github.com/narutatsuri/Unbiased-Perspective-Summarization) | N/A |
