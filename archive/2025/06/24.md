

## Papers for 2025-06-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871) | yzwang, sienna223, Shitao, Ruiran, wcyno23 | - OmniGen2 is a versatile, open-source multimodal generative model designed for diverse generation tasks including text-to-image, image editing, and in-context generation. - It features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer, enabling it to leverage existing multimodal understanding models without needing to re-adapt VAE inputs. - OmniGen2 introduces a reflection mechanism for image generation, and a new benchmark named OmniContext to evaluate in-context generation, achieving state-of-the-art performance among open-source models in terms of consistency. - The model is trained using a comprehensive dataset created through data construction pipelines encompassing image editing and in-context generation data. - The authors release the model, training code, datasets, and data construction pipeline to support future research. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Text'] | [Link](https://github.com/VectorSpaceLab/OmniGen2) | [Link](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), [Link](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) |
| [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement
  Learning](https://arxiv.org/abs/2506.18841) | Juanzi Li, Roy Ka-Wei Lee, Yushi Bai, Yuhao Wu, Zhiqiang007 | - LongWriter-Zero is a novel approach for ultra-long text generation in LLMs that leverages reinforcement learning (RL) without relying on synthetic data. - It employs specialized reward models to guide the LLM towards improved length control, writing quality, and structural formatting. - LongWriter-Zero, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods and achieves state-of-the-art results on WritingBench and Arena-Write. - The model surpasses even 100B+ models like DeepSeek R1 and Qwen3-235B. - The data and model checkpoints are open-sourced. | ['Reinforcement Learning', 'Text Generation'] | N/A | [Link](https://huggingface.co/THU-KEG/) |
| [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought
  Reasoning in LLMs](https://arxiv.org/abs/2506.18896) | Ke Shen, Jiahao Qiu, Jingwen Gu, Ling Yang, Jiaru Zou |  - ReasonFlux-PRM is a novel trajectory-aware Process Reward Model (PRM) designed to enhance long chain-of-thought reasoning in LLMs by incorporating both step-level and trajectory-level supervision.  - It addresses the limitations of existing PRMs, which primarily focus on final responses, by explicitly evaluating intermediate reasoning steps.  - ReasonFlux-PRM demonstrates consistent performance improvements across various downstream benchmarks (AIME, MATH500, GPQA-Diamond), surpassing existing PRMs and human-curated baselines in supervised fine-tuning, reinforcement learning, and test-time scaling.  - The model's architecture involves a joint training objective that balances step-level and trajectory-level rewards, thereby enabling effective supervision of trajectory-response data.  - The paper also explores offline data selection and online reward modeling applications of ReasonFlux-PRM, showcasing its versatility in diverse reasoning scenarios. | ['Natural Language Processing', 'Question Answering', 'Reinforcement Learning', 'Text Generation'] | [Link](https://github.com/Gen-Verse/ReasonFlux) | N/A |
| [Vision as a Dialect: Unifying Visual Understanding and Generation via
  Text-Aligned Representations](https://arxiv.org/abs/2506.18898) | Qi Zhao, Yang Zhao, Hao Chen, hywang66, csuhan | - This paper introduces Tar, a multimodal large language model (MLLM) that unifies visual understanding and generation using a shared discrete semantic representation. - The core of Tar is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from an LLM vocabulary. - Tar utilizes two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model, to handle diverse decoding needs and achieve high-fidelity visual outputs. - Experiments demonstrate that Tar matches or surpasses existing multimodal LLM methods in terms of speed and training efficiency across benchmarks for both visual understanding and generation tasks. - The authors also investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation capabilities. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image'] | [Link](https://tar.csuhan.com) | N/A |
| [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741) | Yeyi Guan, Heyuan Huang, He Zhu, kangz, tianyue818 | - This paper introduces OAgents, a modular open-source framework for building effective language agents. - OAgents achieves state-of-the-art performance on the GAIA benchmark, outperforming existing open-source and closed-source methods. - The authors conduct a systematic empirical study on popular design choices in agent components, revealing which designs are crucial for effective agents. - OAgents includes a modular design for various components, enabling future research in agentic AI. - The framework offers a more robust evaluation protocol to stabilize comparisons and improve reproducibility. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/OPPO-PersonalAI/OAgents) | N/A |
| [LettinGo: Explore User Profile Generation for Recommendation System](https://arxiv.org/abs/2506.18309) | Jianfeng Liu, Pu Zhao, Fangkai Yang, Di Zhang, Lu Wang | - This paper introduces LETTINGO, a novel framework for generating diverse and adaptive user profiles for recommendation systems, which significantly enhances recommendation accuracy, flexibility, and contextual awareness. - LETTINGO leverages the expressive power of LLMs and incorporates direct feedback from downstream recommendation tasks, avoiding the rigid constraints of supervised fine-tuning. - It operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. - Experimental results demonstrate that LETTINGO significantly outperforms traditional embedding-based profiles and other state-of-the-art methods. - The framework's flexibility allows it to capture the full diversity of user behaviors and adapt to various downstream tasks and recommendation needs. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Reinforcement Learning', 'Summarization', 'Feature Extraction', 'Other'] | N/A | N/A |
| [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123) | Potsawee Manakul, Panop Pitchayarthorn, Warit Sirichotedumrong, pittawat, natnitaract | - FinCoT is a novel prompting framework that integrates expert financial workflows into structured chain-of-thought prompting to improve the accuracy of large language models on financial reasoning tasks. - FinCoT significantly improves performance over standard prompting methods and even outperforms a domain-specific fine-tuned model (Fin-R1) on various financial reasoning benchmarks. - The framework incorporates Mermaid blueprints that encode expert financial workflows and enhances interpretability of LLM reasoning. - FinCoT achieves accuracy improvements of up to +17.3 percentage points and reduces the number of generated tokens by 8 times compared to other prompting techniques. - The study presents a comprehensive investigation of prompting styles in FinNLP and releases nine blueprint templates for various financial domains. | ['Question Answering', 'Zero-Shot Classification'] | N/A | [Link](https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1) |
| [Auto-Regressively Generating Multi-View Consistent Images](https://arxiv.org/abs/2506.18527) | Chen Zhao, Jinbo Wu, Jialun Liu, Yuxiao Yang, JiaKui Hu | - The paper introduces a novel Multi-View Auto-Regressive (MV-AR) model for generating multi-view consistent images from various prompts (text, images, shapes). - The model utilizes an autoregressive approach, progressively generating each view conditioned on previously generated views, enhancing consistency across different viewpoints. - A unified model architecture is designed to handle multiple input modalities simultaneously. - To mitigate the issue of limited high-quality training data, a "Shuffle View" data augmentation technique is proposed. - Experimental results demonstrate that MV-AR achieves comparable or better performance compared to leading diffusion-based methods in terms of image quality and consistency. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://github.com/MILab-PKU/MVAR) | N/A |
| [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming
  and Distillation](https://arxiv.org/abs/2506.18349) | Young Jin Kim, Ilgee Hong, Zixuan Zhang, Chen Liang, Pearush | - This paper introduces SlimMoE, a novel multi-stage compression framework designed to efficiently compress large Mixture-of-Experts (MoE) language models. - SlimMoE systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, mitigating the performance degradation often associated with one-shot pruning. - The framework compresses Phi-3.5-MoE (41.9B total / 6.6B activated parameters) into two smaller models: Phi-mini-MoE (7.6B total / 2.4B activated) and Phi-tiny-MoE (3.8B total / 1.1B activated) using less than 10% of the original training data. - The compressed models demonstrate strong performance, exceeding or matching the capabilities of models with similar sizes and remaining competitive with larger models. - The authors release their models on HuggingFace for broader adoption of MoE architectures across computational environments. | ['Natural Language Processing', 'Text Generation'] | N/A | [Link](https://huggingface.co/microsoft/Phi-mini-MoE-instruct), [Link](https://huggingface.co/microsoft/Phi-tiny-MoE-instruct) |
| [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962) | Wenjie Li, Yujie Zhang, Wenjie Lou, Yankai Jiang, manglu3935 |  - This paper introduces Mentor-Intern Collaborative Search (MICS), a novel method for generating high-quality chain-of-thought (CoT) data for medical visual question answering.  - MICS uses mentor models to initialize reasoning paths and intern models to evaluate and refine them, selecting optimal paths based on an MICS-score.  - The paper also introduces MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-01, a new medical MLLM trained using MICS and curriculum learning.  - Experiments show that Chiron-01 achieves state-of-the-art performance on various medical visual question answering and reasoning benchmarks.  - The code for the model will be available at the provided GitHub repository. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/manglu097/Chiron-o1) | N/A |
| [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879) | Tianle Cai, Talha Chafekar, Muhammad Yusuf Hassan, Yang Zhang, Junyan Li | - CommVQ, a novel method for compressing Key-Value (KV) caches in large language models (LLMs), is proposed to address the memory bottleneck caused by long context lengths. - CommVQ employs additive vector quantization with a lightweight encoder and codebook that is commutative with Rotary Position Embedding (RoPE), allowing efficient integration into the self-attention mechanism. - Experiments on LongBench, InfiniteBench, and GSM8K demonstrate that CommVQ reduces FP16 KV cache size by 87.5% with 2-bit quantization and enables 1-bit quantization with minimal accuracy loss. - The proposed method outperforms state-of-the-art KV cache quantization methods and allows a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. - CommVQ achieves superior trade-offs between memory savings and accuracy by combining additive quantization and RoPE-commutative codebook. | ['Natural Language Processing'] | [Link](https://github.com/UMass-Embodied-AGI/CommVQ) | N/A |
| [FaithfulSAE: Towards Capturing Faithful Features with Sparse
  Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673) | Andrew Bermingham, Luis Eduardo Rodrigues Vieira, Donghyun Lee, Harryn Oh, seonglae | - This paper introduces FaithfulSAE, a novel method for training Sparse Autoencoders (SAEs) that avoids the instability issues associated with using external datasets. - The proposed method trains SAEs on a synthetic dataset generated by the model itself, eliminating the reliance on external datasets which can introduce out-of-distribution (OOD) data. - Experimental results show that FaithfulSAEs outperform SAEs trained on web-based datasets in terms of stability across different initialization seeds and exhibit a lower Fake Feature Ratio. - The findings support the hypothesis that using OOD data in SAE training is the primary cause of their instability and that training on model-internal data leads to more stable and faithful feature representations. - This method improves the faithfulness and stability of SAEs, advancing the interpretability of large language models. | ['Natural Language Processing', 'Feature Extraction'] | [Link](https://github.com/seonglae/FaithfulSAE) | [Link](https://huggingface.co/collections/seonglae/faithful-saes-67f3b25ff21a185017879b33), [Link](https://huggingface.co/collections/seonglae/faithful-dataset-67f3b21ff8fca56b87e5370f) |
| [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code
  Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323) | Bertalan Borsos, Nils Gruschka, Richard A. Dubniczky, Tamas Bisztray, Neo111x | - This paper introduces CodeT5-Authorship, a novel model for LLM authorship attribution of C programs, which uses only the encoder layers from the original CodeT5 model. - The model achieves high accuracy (97.56% in binary classification and 95.40% in multi-class attribution) in distinguishing between C programs generated by different LLMs. - A new benchmark dataset, LLM-AuthorBench, containing 32,000 compilable C programs generated by eight state-of-the-art LLMs, is introduced to evaluate the proposed model and other methods. - The CodeT5-Authorship model outperforms several traditional ML classifiers and other transformer models in the authorship attribution task, demonstrating the effectiveness of the proposed approach. - The CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant Google Colab scripts are publicly available on GitHub. | ['Text Classification'] | [Link](https://github.com/LLMauthorbench/) | N/A |
| [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597) | Daoyuan Wu, Zongjie Li, Wenxuan Wang, Zhenlan Ji, Xunguang Wang | This paper presents the first comprehensive taxonomy for categorizing jailbreak guardrails for LLMs along six key dimensions.  A novel Security-Efficiency-Utility evaluation framework is introduced to assess guardrail effectiveness.  Extensive analysis and experiments identify the strengths and limitations of existing guardrails and provide insights for optimizing defense combinations.  A leaderboard is presented, ranking guardrails based on their performance across various metrics.  The findings guide the principled advancement and deployment of robust LLM guardrails. | ['Natural Language Processing'] | [Link](https://github.com/xunguangwang/SoK4JailbreakGuardrails) | N/A |
