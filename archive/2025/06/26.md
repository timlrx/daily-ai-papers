

## Papers for 2025-06-26

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image
  Generation](https://arxiv.org/abs/2506.18095) | Ke Ji, Shunian Chen, Zhenyang Cai, Junying Chen, cppppppc | - This paper introduces ShareGPT-40-Image, the first publicly available dataset containing 45K text-to-image and 46K image-to-image pairs generated using GPT-40's image generation capabilities. - Leveraging this dataset, a new multimodal large language model called Janus-40 is developed, demonstrating significant improvements in text-to-image and text-and-image-to-image generation. - Janus-40 achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8xA800 GPU machine. - The experiments show that Janus-40 outperforms other open-source models on several benchmarks, including improvements in image quality and instruction following. - This work contributes to the democratization of advanced image generation techniques by providing a large-scale, high-quality dataset and a powerful open-source model. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/FreedomIntelligence/ShareGPT-40-Image) | N/A |
| [Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large
  Language Models](https://arxiv.org/abs/2506.19697) | Jaewoo Kang, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, affjljoo3581 |  - This paper introduces Outlier-Safe Pre-training (OSP), a novel method for preventing outlier formation during the pre-training of large language models (LLMs).  - OSP combines three key innovations: the Muon optimizer, Single-Scale RMSNorm, and a learnable embedding projection.   -  The proposed method achieves a 35.7 average score across 10 benchmarks under aggressive 4-bit quantization, significantly outperforming an Adam-trained model (26.5).   -  OSP models exhibit near-zero excess kurtosis (0.04), compared to extreme values (1818.56) in standard models, demonstrating a fundamental alteration of LLM quantization behavior.   - The study trained a 1.4B-parameter model on 1 trillion tokens without outliers, showcasing the scalability and effectiveness of the proposed method. | ['Natural Language Processing'] | [Link](https://github.com/dmis-lab/Outlier-Safe-Pre-Training) | N/A |
| [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315) | Jing Shao, Zhe Zhang, Lehan He, lsheng2024, zx55 | - The paper introduces Property-Generated Solver (PGS), a novel framework that leverages Property-Based Testing (PBT) to validate LLM-generated code. - PGS uses two LLM-based agents: a Generator for code generation and refinement, and a Tester for managing the PBT lifecycle and providing feedback. - Experimental results on multiple code generation benchmarks demonstrate that PGS achieves substantial improvements (23.1% to 37.3% relative gains) over established TDD methods. - The framework addresses the limitations of traditional TDD approaches by focusing on high-level program properties instead of relying on specific input-output examples. - PGS provides a robust mechanism for steering LLMs towards more correct and generalizable code by effectively decoupling code generation from its validation. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Is There a Case for Conversation Optimized Tokenizers in Large Language
  Models?](https://arxiv.org/abs/2506.18674) | Pedro Reviriego, Gonzalo Mart√≠nez, Javier Conde, Raquel Ferrando | - This paper investigates the potential benefits of optimizing tokenizers specifically for conversational applications in large language models (LLMs). - The authors retrain several popular tokenizers using a chatbot conversation dataset and compare their performance to the original tokenizers on both the conversation dataset and the original LLM training dataset. - Results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, leading to energy savings in the range of 5% to 10%, with minimal or positive impact on tokenization efficiency for the training corpus. - Although further research is needed to confirm the results and evaluate the impact on training costs, the findings suggest that customizing tokenizers for conversation can improve LLM efficiency. - The study highlights the importance of considering tokenization efficiency, often overlooked, in optimizing LLMs for real-world applications. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/RaquelFerrando/conversational_tokenizers.git) | N/A |
| [When Life Gives You Samples: The Benefits of Scaling up Inference
  Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544) | Sara Hooker, Julia Kreutzer, Ye Shen, Daniel D'souza, ammar-cohere | This paper introduces novel sampling and selection strategies for improving the inference-time compute of multilingual LLMs. The proposed methods significantly improve performance across multiple languages and tasks, particularly in underrepresented languages. Compared to existing methods, the proposed approach yields notable gains in win-rates on various benchmarks, even against strong commercial models. These strategies involve adapting sampling and selection to account for diversity in domains and languages, leading to an average +6.8 jump in win-rates for 8B models and +9.0 for 111B models.  The researchers also provide a detailed experimental setup and extensive analysis of existing methods, highlighting the need for language- and task-aware approaches. Overall, the findings underscore the potential for democratizing performance improvements across various languages and tasks through efficient inference-time compute strategies. | ['Natural Language Processing'] | N/A | N/A |
| [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403) | Carlos C. N. Kuhn, adnaan525 | - This paper introduces the Debugging Decay Index (DDI), a novel metric to quantify the effectiveness of debugging in large language models (LLMs) for code generation. - The DDI is based on an exponential decay model that captures the diminishing returns of successive debugging attempts. - The authors propose a "fresh start" strategy to mitigate debugging decay by strategically restarting the code generation process when the effectiveness drops below a certain threshold. - Empirical results across multiple models demonstrate that this fresh start strategy can improve overall code generation accuracy compared to continuing debugging. - The DDI framework offers a multi-dimensional evaluation of debugging effectiveness, providing insights into initial performance, decay rate, optimal intervention points, and model fit quality. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining
  and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331) | Eric de la Clergerie, Nathan Godey, rntc | - This paper introduces Biomed-Enriched, a biomedical dataset created through a two-stage annotation process using LLMs. - The dataset contains 2 million clinical case paragraphs, including 450K high-quality ones from articles with commercial-use licenses, addressing the scarcity of publicly available clinical text. - Experiments show that strategically combining quality filtering and domain upsampling significantly improves data efficiency and targeted model performance. - The curated subsets in Biomed-Enriched enabled targeted improvements in continual pretraining experiments, with clinical upsampling boosting performance by 5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by 1%. - The results demonstrate the potential for more efficient and effective biomedical pretraining strategies. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility
  Applications](https://arxiv.org/abs/2506.19502) | Paul Laban, Matt Laing, AleksandrAlgazinov | - The paper introduces MATE, a multimodal accessibility MAS that performs modality conversions based on user needs, outperforming other LLMs and statistical models in experiments. - MATE supports multiple model types, from LLM API calling to custom ML classifiers, and runs locally to ensure privacy and security. - It includes ModCon-Task-Identifier, a model that extracts modality conversion tasks from user input, showing consistent improvement over other LLMs and models. - The system is adaptable to various needs and compatible with a wide range of hardware, designed to assist individuals with disabilities in interacting with digital environments. - The code and data for MATE are publicly available, promoting further research and development in the field of AI for accessibility. | ['Multimodal', 'Any-to-Any', 'Text-to-Speech', 'Text-to-Image', 'Image-to-Text', 'Image-to-Video', 'Text-to-Audio', 'Image-to-3D', 'Text-to-Video', 'Audio-to-Audio', 'Automatic Speech Recognition', 'Natural Language Processing', 'Text Classification'] | [Link](https://github.com/AlgazinovAleksandr/Multi-Agent-MATE) | N/A |
