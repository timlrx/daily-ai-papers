

## Papers for 2025-06-27

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670) | Bo You, Yiding Liu, Wei Li, Zihao Deng, kimingng |  - This paper introduces MMSearch-R1, a novel reinforcement learning framework that enables large multimodal models (LMMs) to perform on-demand, multi-turn search in real-world internet environments. - MMSearch-R1 integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. - The model outperforms RAG-based baselines of the same model size, and matches the performance of larger RAG-based models while reducing search calls by over 30% on various VQA tasks. -  A multimodal search VQA dataset was created with a semi-automated pipeline to support training. - The framework was shown to improve models' ability to recognize the boundaries of their knowledge and perform on-demand search, which enhanced their ability to utilize internal knowledge. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1) | N/A |
| [Where to find Grokking in LLM Pretraining? Monitor
  Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551) | Ziyue Li, zhoutianyi, Fcr09 | - This paper introduces two novel metrics for monitoring generalization during LLM pretraining without relying on test sets or finetuning.  - The study verifies that grokking, the phenomenon where test performance improves long after training loss converges, also occurs during the pretraining of large-scale LLMs.  - It is shown that, unlike previous findings, grokking in LLMs is local and asynchronous across different data domains.  - The proposed metrics quantify the complexity and similarity of routing pathways within a Mixture-of-Experts (MoE) LLM, providing mechanistic insights into the memorization-to-generalization transition.  - The findings are grounded in a theoretical analysis showing that more structured pathways reduce model complexity and improve the generalization bound. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SAM4D: Segment Anything in Camera and LiDAR Streams](https://arxiv.org/abs/2506.21547) | Sheng Yang, Chunyong Hu, Ziqian Ni, Jianyun Xu, songw-zju | - SAM4D is a novel multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. - It utilizes Unified Multi-modal Positional Encoding (UMPE) to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. - Motion-aware Cross-modal Memory Attention (MCMA) enhances temporal consistency and long-horizon feature retrieval. - SAM4D is trained on the constructed Waymo-4DSeg dataset, which contains over 300k camera-LiDAR associated masklets. - Extensive experiments demonstrate SAM4D's superior performance and generalizability in promptable multi-modal segmentation. | ['Image Segmentation', 'Multimodal'] | [Link](https://github.com/open-mmlab/mmsegmentation) | N/A |
| [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552) | Trevor Darrell, Yann LeCun, Amir Bar, dans123, Emma02 | - This paper introduces PEVA, a novel model for predicting egocentric videos conditioned on detailed 3D human motion. - The model architecture is based on an autoregressive conditional diffusion transformer, which leverages a structured action representation that captures both global body dynamics and fine-grained joint articulations. - PEVA is trained on the Nymeria dataset, a large-scale dataset of real-world egocentric video and body pose capture, and includes random timeskips to handle delayed visual consequences of actions. - The authors conduct a comprehensive evaluation demonstrating that PEVA outperforms baseline methods in terms of video quality, semantic consistency, and action control abilities. - The results showcase PEVA's ability to predict long-term visual consequences and even perform planning tasks by simulating action candidates and selecting the one with highest perceptual similarity to the goal. | ['Image-to-Video', 'Video Classification', 'Multimodal'] | N/A | N/A |
| [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655) | Adil Hafeez, Co Tran, nehcgs, parachas | - This paper introduces Arch-Router, a 1.5B parameter language model for preference-aligned LLM routing. - Arch-Router maps queries to user-defined domains or action types to guide model selection, aligning with subjective human preferences. - The framework allows for adding new models without retraining or architectural changes, enhancing flexibility and adaptability. - Experiments show that Arch-Router outperforms existing top proprietary LLMs by 7.71% on average in matching queries with human preferences. - The approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/katanemo/Arch-Router-1.5B) |
| [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430) | Pengcheng Qiu, Xiaoman Zhang, Yanjie Fan, Chaoyi Wu, Weike Zhao | DeepRare is an agentic system for rare disease diagnosis that uses a large language model (LLM) to process heterogeneous clinical inputs, including free text, structured data, and genomic data.  It generates ranked diagnostic hypotheses with traceable reasoning. DeepRare outperforms 15 other methods in HPO-based evaluations, achieving a Recall@1 score of 57.18%.  In multi-modal input scenarios, it achieves 70.6% Recall@1 compared to Exomiser's 53.2%. Clinical experts verified the system's reasoning chains with 95.4% agreement, indicating high reliability.  The system has been implemented as a user-friendly web application. | ['Natural Language Processing', 'Multimodal', 'Question Answering', 'Zero-Shot Classification'] | [Link](None) | [Link](https://huggingface.co/datasets/Angelakeke/DeepRare) |
| [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103) | Laurence Aitchison, tim-lawson | - This paper introduces a novel Transformer architecture that dynamically skips a variable number of middle layers based on a learned gating mechanism.  - The architecture aims to improve efficiency by reducing computation for simpler tokens and potentially fostering a multi-level representational hierarchy. - A gated attention mechanism prevents tokens from attending to skipped positions, and adaptive regularization is used to control gate sparsity. - The proposed approach does not demonstrate improvements in validation cross-entropy and estimated FLOPs compared to dense baselines at the scales investigated. - Future research directions include scaling the model to potentially realize the benefits of the architecture. | ['Natural Language Processing'] | [Link](https://github.com/tim-lawson/skip-middle) | N/A |
| [MuseControlLite: Multifunctional Music Generation with Lightweight
  Conditioners](https://arxiv.org/abs/2506.18729) | Bo-Rui Chen, Sheng-Ping Yang, Weijaw Lee, Shih-Lun Wu, fundwotsai2001 | - MuseControlLite is a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. - It utilizes positional embeddings in the decoupled cross-attention layers, which increases control accuracy while requiring fewer trainable parameters compared to existing methods. - The model demonstrates improved controllability over MusicGen-Large and Stable Audio Open ControlNet in musical attribute control, audio inpainting, and audio outpainting tasks. - Experiments show that MuseControlLite achieves superior performance in melody control, with a 4.5% improvement in melody accuracy compared to existing ControlNet-based approaches. - The model is efficient, using only 85M trainable parameters and demonstrating that positional embeddings are critical for time-varying conditions. | ['Audio-to-Audio', 'Text-to-Audio'] | [Link](https://MuseControlLite.github.io/web/) | N/A |
