

## Papers for 2025-06-30

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing](https://arxiv.org/abs/2506.17450) | Sanghyun Woo, Saining Xie, Xuhui Jia, Ramin Mehran, cccjc | - BlenderFusion is a novel generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background using a layering-editing-compositing pipeline. - It extends a pre-trained diffusion model to process both original and edited scenes in parallel, fine-tuned on video frames with two key training strategies (source masking and simulated object jittering). - BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks, as shown through quantitative and qualitative evaluations on three video datasets. - The model integrates 3D-grounded editing capabilities of Blender with the strong synthesis abilities of diffusion models, enabling flexible, disentangled, and 3D-aware manipulation of objects, camera, and background. - The proposed framework leverages a layering-editing-compositing process that includes object-centric layering, Blender-guided editing, and generative compositing. | ['Image-to-3D', 'Image-to-Image', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://blenderfusion.github.io/) | N/A |
| [ShotBench: Expert-Level Cinematic Understanding in Vision-Language
  Models](https://arxiv.org/abs/2506.21356) | Yuhao Dong, Dian Zheng, Yi Jin, Jingwen He, Hongbo Liu | This paper introduces ShotBench, a benchmark dataset for evaluating vision-language models' (VLMs) understanding of cinematic language.  ShotBench comprises over 3.5k expert-annotated QA pairs from images and video clips across 200 films, spanning eight key cinematography dimensions.  Evaluation of 24 leading VLMs shows substantial limitations, with even the top-performing model achieving less than 60% accuracy.  To address this, a large-scale multimodal dataset called ShotQA (70k cinematic QA pairs) and ShotVL, a novel VLM significantly outperforming existing models on ShotBench are introduced. The authors make their models, data, and code open-source. | ['Multimodal', 'Visual Question Answering', 'Video-Text-to-Text', 'Computer Vision', 'Video Classification'] | [Link](https://vchitect.github.io/ShotBench-project/) | N/A |
| [MiCo: Multi-image Contrast for Reinforcement Visual Reasoning](https://arxiv.org/abs/2506.22434) | Xiaogang Xu, Xiaoyang Wu, Shaoteng Liu, Mingkang Zhu, Xi Chen | - This paper introduces MiCo, a novel method for multi-image visual reasoning that leverages inherent image constraints as supervision, reducing reliance on manual annotations. - MiCo constructs image triplets comprising two augmented views of the same image and a different but similar image, prompting the model to compare them and generate a reasoning process. - The model is trained using rule-based reinforcement learning, optimizing its ability to attend to subtle visual changes and perform logical reasoning. - Experiments demonstrate MiCo's effectiveness on multi-image reasoning benchmarks, achieving significant improvements without relying on human-annotated question-answer pairs. - The method also shows strong performance on general vision tasks, indicating the learned reasoning ability generalizes effectively to a wide range of questions. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | N/A | N/A |
| [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656) | Xiaofeng Zhang, Xu Cao, Jingyuan Zhu, Yuanzhe Liu, Yifan Shen |  - This paper introduces SpatialReasoner-R1, a novel vision-language model designed for spatial reasoning that uses Long Chain-of-Thought (LongCoT) reasoning.  - The model employs a novel fine-grained Direct Preference Optimization (fDPO) method which applies differentiated learning updates tailored to two semantically distinct components: descriptive grounding and logical reasoning.  - A Multi-Model Monte Carlo Tree Search (M3CTS) method is proposed to generate high-quality supervision for spatial reasoning.  - SpatialReasoner-R1 achieves state-of-the-art performance on the SPATIALRGPT-BENCH benchmark, outperforming the strongest baseline by 9.8% in average accuracy.  - The fDPO method achieves an average improvement of 4.1% over standard DPO across spatial quality tasks and a 9.0% gain in spatial quantity tasks. | ['Multimodal', 'Visual Question Answering', 'Text2Text Generation'] | [Link](https://plan-lab.github.io/spatialreasoner/) | N/A |
| [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411) | Wei Guo, Xiaosong Li, MightyCrane, Fangcheng2, tangyehui | *- The paper introduces a novel Mixture of Grouped Experts (MoGE) model architecture designed to address expert load imbalance in large language models (LLMs). - MoGE groups experts and constrains tokens to activate an equal number of experts within each predefined group, improving the workload balance compared to conventional MoE. - The paper presents Pangu Pro MoE, a 72-billion parameter LLM based on MoGE, optimized for Ascend NPUs, achieving 1148 tokens/s per card inference speed. - Experiments show that Pangu Pro MoE outperforms comparable dense models (32B and 72B) and other open-source models (GLM-Z1-32B and Qwen3-32B) on various benchmarks. - The model's efficiency stems from a hierarchical parallelism strategy and optimized inference techniques for Ascend NPUs, including fused operators and a hierarchical hybrid parallelism strategy. | ['Natural Language Processing', 'Text Generation'] | [Link](https://gitcode.com/ascend-tribe/pangu-pro-moe) | N/A |
| [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with
  Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594) | Amr Fawzy, Mostafa Samy, Ahmed M. Adly | - Gazal-R1, a 32-billion parameter language model, achieves state-of-the-art performance in medical reasoning by utilizing a two-stage training pipeline. - The first stage involves supervised fine-tuning on a dataset of 107,033 synthetic medical reasoning examples, enhanced by parameter-efficient techniques like DoRA and rsLORA. - The second stage employs reinforcement learning with GRPO, refining accuracy and reasoning quality through a multi-component reward system. - Gazal-R1 surpasses larger models (up to 12 times larger) on medical benchmarks like MedQA (87.1%), MMLU Pro (Medical) (81.6%), and PubMedQA (79.6%). - The study provides insights into the challenges of training reasoning models in specialized domains, such as reward hacking and training instability. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/TachyHealth/Gazal-R1-32B-GRPO-preview), [Link](https://huggingface.co/datasets/TachyHealth/structured_medical), [Link](https://huggingface.co/datasets/TachyHealth/medical_grpo) |
| [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for
  Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330) | Yitao Duan, Jiachen Wang, Qiao Cheng, Na Cai, nomadlx | - This paper introduces Confucius3-Math, a 14B parameter open-source large language model designed for Chinese K-12 mathematics learning. - The model is lightweight, achieving state-of-the-art performance on various mathematical reasoning tasks while running efficiently on a single consumer-grade GPU. - Three novel techniques are introduced to improve the model's performance and training stability: Targeted Entropy Regularization, Recent Sample Recovery, and Policy-Specific Hardness Weighting. - The model outperforms many larger models on various benchmarks, demonstrating the feasibility of building high-performance reasoning models at low cost. - The model's code and weights are made publicly available on GitHub. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/netease-youdao/Confucius3-Math) | N/A |
| [RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation
  Models](https://arxiv.org/abs/2506.22149) | Hrvoje Bogunović, Ursula Schmidt-Erfurth, José Morano, Ronald Fecso | - RetFiner is a novel vision-language refinement scheme designed to enhance the representational capabilities of existing retinal foundation models (FMs). - The model architecture employs a Vision Transformer (ViT) vision encoder and a Transformer text encoder, utilizing cross-attention layers for multimodal encoding and generation. - RetFiner incorporates multiple training objectives (ITC, ITM, MLM, GM) to effectively leverage textual data for improving visual representations, leading to significant performance gains. - Experimental results on seven diverse OCT classification tasks demonstrate that RetFiner substantially improves the performance of three state-of-the-art retinal FMs (RETFound, UrFound, and VisionFM), achieving average gains of up to 5.8 percentage points in balanced accuracy. - The method is efficient, requiring less than ten epochs to refine a model and exhibits strong adaptability to specific populations, making it a practical solution for adapting retinal FMs to various applications and datasets. | ['Multimodal', 'Image Classification', 'Image Feature Extraction'] | [Link](https://github.com/ronnief1/RetFiner) | N/A |
