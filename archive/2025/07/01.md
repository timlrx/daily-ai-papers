

## Papers for 2025-07-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044) | Pengxin Zhan, Liangfu Cao, Xinjie Zhang, Shanshan Zhao, Flourish | - The paper introduces Ovis-U1, a 3-billion parameter unified multimodal model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. - Ovis-U1 uses a diffusion-based visual decoder and a bidirectional token refiner, enabling image generation and editing tasks comparable to leading models such as GPT-4. - The model achieves state-of-the-art results on several benchmarks, including a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models. - Ovis-U1's unified training approach, starting from a language model, yields better performance than training solely on understanding or generation tasks. - The model pushes the boundaries of multimodal understanding, generation, and editing, demonstrating the enhancement achieved by integrating these two tasks. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image', 'Image-to-Video', 'Visual Question Answering'] | [Link](https://github.com/AIDC-AI/Ovis-U1) | [Link](https://huggingface.co/AIDC-AI/Ovis-U1-3B) |
| [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832) | Anton Gusarov, Andrey Galichin, Li Pengyi, barracuda049, alexgambashidze | - This paper introduces a novel listener-augmented reinforcement learning framework for training vision-language models (VLMs) to reason about human visual preferences. - The key contribution is a listener-shaped reward mechanism that leverages an independent, frozen VLM to evaluate the reasoner's chain-of-thought, providing a dense and calibrated confidence score to shape the RL reward signal.  - Experimental results demonstrate that the proposed method achieves state-of-the-art accuracy on the ImageReward benchmark (67.4%) and significantly improves out-of-distribution performance on a large-scale human preference dataset. - The method also reduces reasoning contradictions compared to strong GRPO and SFT baselines, showcasing its effectiveness in aligning VLMs with nuanced human preferences. - The authors will release their reasoning model on Hugging Face. | ['Reinforcement Learning', 'Text-to-Image', 'Multimodal'] | N/A | [Link](https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner) |
| [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930) | LidongBing, Zhiqiang007, Jianyu |  - This paper introduces a novel prompt design paradigm for large language models (LLMs) that involves pruning random demonstrations to improve performance. - It challenges the conventional wisdom of using well-crafted instructions and demonstrations for in-context learning. - The proposed method, PROMPTQUINE, is an evolutionary search framework that automatically discovers effective pruning strategies. - Experiments show that PROMPTQUINE consistently outperforms state-of-the-art automatic prompt optimization techniques across various tasks and LLMs.  - The findings provide insights into the mechanisms of in-context learning and call for more open-ended search algorithms for improved LLM prompting. | ['Natural Language Processing'] | [Link](https://github.com/jianyu-cs/PromptQuine/) | N/A |
| [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in
  Inference-time Scaling?](https://arxiv.org/abs/2506.17417) | Kaizhuo Yan, Jize Jiang, Jingcheng Yang, Meitang Li, Mingyuan1997 | - This paper investigates whether inference-time scaling techniques, successful in LLMs, extend to VLMs, focusing on self-verification capabilities. - The study reveals that generation-based methods outperform verification-based methods in VLMs, indicating a deficiency in self-verification. - Experiments show that RL-trained VLMs do not significantly benefit from self-correction behaviors like "aha moments". - The authors found that the self-verification performance of VLMs is counterintuitively better without visual input, highlighting the models' limited use of visual information for verification. - This research emphasizes the need for improved self-verification capabilities in VLMs to fully leverage the potential of inference-time scaling. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500) | Ligeng Zhu, Junxian Guo, Xiuyu Li, zhijianliu, Skhaki | - This paper introduces SparseLoRA, a novel method that accelerates Large Language Model (LLM) fine-tuning through contextual sparsity. - SparseLoRA employs a lightweight, training-free Singular Value Decomposition (SVD) sparsity estimator to dynamically select a sparse subset of weights for computation. - Experimental results demonstrate that SparseLoRA reduces computational cost by up to 2.2x and achieves a speedup of up to 1.6x while maintaining accuracy across various downstream tasks. - The method is shown to outperform existing parameter-efficient fine-tuning methods such as LoRA and DoRA in terms of both speed and computational efficiency. - SparseLoRA incorporates techniques such as layer sensitivity analysis, token sensitivity analysis, and progressive sparse fine-tuning to further enhance its performance and efficiency. | ['Natural Language Processing'] | N/A | N/A |
| [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992) | Maria BrbiÄ‡, Yekun Chai, mdmoor, yljblues | - This paper introduces MARBLE, a challenging multimodal spatial reasoning benchmark designed to evaluate the step-by-step reasoning abilities of Multimodal Language Models (MLLMs). - MARBLE consists of two tasks: M-PORTAL, which involves solving complex spatial reasoning and planning problems inspired by the game Portal 2, and M-CUBE, which requires assembling 3D cubes from six jigsaw pieces. - Current MLLMs perform poorly on MARBLE, achieving near-random performance on M-PORTAL and 0% accuracy on M-CUBE, highlighting the challenges of complex multimodal reasoning. - The benchmark emphasizes reasoning trajectories and plans, providing gold-standard rationales and mechanisms for evaluating intermediate step fidelity, unlike prior datasets that overemphasize final answer accuracy. - MARBLE aims to spur the development of the next generation of models with stronger capabilities in multi-step multimodal reasoning and planning. | ['Multimodal'] | [Link](https://marble-benchmark.github.io) | N/A |
| [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394) | s-emanuilov | - This paper introduces TUCAN, a novel methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. - The approach involves continued training of the BgGPT model series on a bilingual dataset of 10,035 function-calling examples. - TUCAN achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding. - The models, evaluation framework, and dataset are released open-source to enable replication for other languages. - This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems. | ['Natural Language Processing'] | [Link](https://github.com/llm-bg/Tucan-Eval), [Link](https://github.com/insait-institute/lm-evaluation-harness-bg) | [Link](https://huggingface.co/datasets/llm-bg/Tucan-BG-v1.0), [Link](https://huggingface.co/datasets/llm-bg/Tucan-BG-Eval-v1.0) |
| [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence
  with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219) | Yong Li, Yanxin Xi, Tianhui Liu, Shengyuan Wang, JJ-TMT |  - UrbanLLaVA is a novel multi-modal large language model designed for urban intelligence that leverages multiple modalities (e.g. images, geospatial data, text) to process various tasks.  - It introduces a new urban instruction dataset (UData) spanning different views (location, trajectory, global) of the urban environment and a multi-stage training framework to enhance spatial reasoning and domain knowledge.  - UrbanLLaVA outperforms existing open-source and proprietary models in single and cross-modal urban tasks across various cities, demonstrating its robustness and adaptability.  - The model achieves superior performance on an enhanced benchmark (UBench) composed of 12 tasks related to various urban data types, showcasing its capacity for complex urban understanding.  - The research contributes a systematic multi-view dataset, a novel training methodology, and enhanced evaluation benchmarks, thus advancing urban intelligence research. | ['Multimodal'] | [Link](https://github.com/tsinghua-fib-lab/UrbanLLaVA) | N/A |
| [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694) | Yifan Zao, Junyoung Park, Mukul Gagrani, Sudhanshu Agrawal, Raghavv Goel | - This paper introduces VOCABTRIM, a training-free technique to enhance the efficiency of drafter-based speculative decoding in large language models (LLMs). - VOCABTRIM improves speed by reducing the drafter's vocabulary size to only the most frequently used tokens, thus decreasing the inference overhead. - Experiments on Llama-3 models using Spec-Bench show a 16% memory-bound speed-up for Llama-3.2-3B-Instruct and a significant speed increase on other tasks with minor drops in block efficiency. - The method is compatible with existing SpD techniques, requiring minimal modifications. - VOCABTRIM offers a novel approach to optimizing the LM head of the drafter, a previously under-explored component in SpD research. | ['Text Generation'] | N/A | N/A |
| [ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language
  Models for Audio Generation and Editing](https://arxiv.org/abs/2506.21448) | Qian Chen, Wen Wang, Kaicheng Luo, Jialei Wang, Huadai Liu | - ThinkSound is a novel framework for audio generation and editing that leverages chain-of-thought (CoT) reasoning in multimodal large language models (MLLMs). - The model uses a three-stage process: foundational foley generation, interactive object-centric refinement, and targeted audio editing, each guided by CoT reasoning. - ThinkSound incorporates a unified audio foundation model based on flow matching, enabling high-fidelity audio synthesis from various input modalities. - The paper introduces AudioCoT, a new dataset with structured reasoning annotations to support the training of the model. - Experimental results demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation, outperforming existing baselines on both audio and CoT metrics. | ['Audio', 'Text-to-Audio', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Tower+: Bridging Generality and Translation Specialization in
  Multilingual LLMs](https://arxiv.org/abs/2506.17080) | Pedro Teixeirinha, JoÃ£o Alves, JosÃ© Pombal, Nuno M. Guerreiro, RicardoRei |  - TOWER+, a new suite of multilingual LLMs, is introduced to address the trade-off between translation specialization and general-purpose capabilities. - The model achieves a Pareto frontier between translation specialization and multilingual general-purpose capabilities by using a novel training recipe comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. - TOWER+ models outperform larger general-purpose open-weight and proprietary LLMs (e.g., LLAMA 3.3 70B, GPT-40) on various benchmarks including translation and instruction following. - The findings highlight the possibility of rivaling frontier models in general capabilities while optimizing for specific business domains such as translation and localization. - IF-MT, a new benchmark evaluating both translation and instruction-following is introduced and the models are made available on Huggingface. | ['Translation', 'Text Generation'] | N/A | [Link](https://huggingface.co/infly/INF-ORM-Llama3.1-70B), [Link](Huggingface) |
