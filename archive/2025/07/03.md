

## Papers for 2025-07-03

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949) | huxiao09, yw95, TinaGao, hjy, caojiangxia |  - This paper introduces Kwai Keye-VL, an 8-billion parameter multimodal foundation model designed for short-video understanding and general-purpose vision-language tasks. - The model architecture uses a Qwen3-8B language model with a vision encoder initialized from SigLIP, supporting native dynamic resolution and 3D ROPE for unified processing of text, image, and video information. - Keye-VL was trained using a four-stage pre-training process and a two-phase post-training process, achieving state-of-the-art results on public video benchmarks and remaining highly competitive on general image-based tasks. - The authors also introduce KC-MMBench, a new benchmark for real-world short-video scenarios, where Keye-VL shows a significant advantage. - The training methodology, data construction, and evaluation results are detailed in the paper, providing insights for building the next generation of MLLMs for the video era. | ['Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/Kwai-Keye/Keye) | [Link](https://huggingface.co/Kwai-Keye) |
| [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/abs/2506.23552) | Youngjung Uh, Jaesik Park, Jaeseok Jung, Mingi Kwon, alex4727 | - JAM-Flow is a novel unified framework for simultaneously synthesizing and conditioning on both facial motion and speech, addressing talking head generation and text-to-speech synthesis as a single task. - The model leverages flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) architecture with specialized Motion-DiT and Audio-DiT modules coupled via selective joint attention layers. - It incorporates key architectural choices such as temporally aligned positional embeddings and localized joint attention masking for effective cross-modal interaction while preserving modality-specific strengths. - JAM-Flow supports various conditioning inputs, including text, reference audio, and reference motion, facilitating tasks like synchronized talking head generation from text and audio-driven animation. - Experimental results on HDTF and CelebV-Dub datasets demonstrate that JAM-Flow significantly outperforms existing methods on talking head generation and automated video dubbing tasks. | ['Multimodal', 'Text-to-Speech', 'Text-to-Video', 'Image-to-Video', 'Audio-to-Audio'] | N/A | N/A |
