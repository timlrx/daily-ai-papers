

## Papers for 2025-07-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with
  TriMap Video Diffusion](https://arxiv.org/abs/2507.02813) | Minghui Yang, Jiawei Chi, Hao Li, Fangfu Liu, hanyang-21 | - LangScene-X is a novel generative framework that reconstructs generalizable 3D language-embedded scenes from sparse views (as few as two images) using a TriMap video diffusion model and a Language Quantized Compressor (LQC). - The TriMap video diffusion model generates 3D-consistent RGB images, normal maps, and semantic maps, while the LQC efficiently encodes language embeddings for cross-scene generalization. - LangScene-X unifies and generates 3D consistent multi-modality information for reconstruction and understanding, supporting open-ended language queries. - Experimental results demonstrate that LangScene-X outperforms state-of-the-art methods in terms of quality and generalizability on real-world datasets. - The model uses a progressive multi-task training strategy that integrates knowledge from diverse domains to ensure 3D consistency and high-quality generation. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://liuff19.github.io/LangScene-X/) | N/A |
| [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592) | Liwen Zhang, Huifeng Yin, Zhongwang Zhang, Kuan Li, xxwu | - This paper introduces WebSailor, a post-training methodology designed to enhance the reasoning capabilities of large language models (LLMs) for complex web-based information-seeking tasks. - WebSailor significantly outperforms all open-source agents and matches the performance of proprietary agents on complex benchmarks like BrowseComp, by systematically reducing uncertainty when navigating vast information landscapes. - The approach involves generating high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm called DUPO. - WebSailor's performance improvements stem from its training data synthesis, which generates complex, emergent structures that compel the model to develop advanced reasoning patterns. - The experimental results demonstrate that WebSailor significantly outperforms existing open-source models and is on par with commercial, closed-source models. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/Alibaba-NLP/WebAgent) | N/A |
| [Thinking with Images for Multimodal Reasoning: Foundations, Methods, and
  Future Frontiers](https://arxiv.org/abs/2506.23918) | Zhenhua Liu, Hangyu Guo, Peng Xia, Zhaochen Su, Xiaoye08 |  - This paper introduces a novel three-stage framework for multimodal reasoning which transcends the limitations of existing text-centric approaches by enabling models to "think with images". - The framework categorizes the evolution of image utilization in AI models into three stages: tool-driven visual exploration, programmatic visual manipulation, and intrinsic visual imagination. - Each stage is characterized by specific methodologies (prompt-based, SFT-based, RL-based) and unique challenges (computational cost, information density, architectural divide, cross-task generalization). - The paper provides a comprehensive review of core methods and evaluation benchmarks for each stage and identifies significant challenges and promising future directions for research. -  The study aims to establish foundational principles for "Thinking with Images", offering a clear roadmap for future research towards more powerful and human-aligned multimodal AI. | ['Multimodal'] | [Link](https://github.com/zhaochen0110/Awesome_Think_With_Images) | N/A |
| [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for
  Deep Search](https://arxiv.org/abs/2507.02652) | Yutao Zhu, Yuyao Zhang, Guanting Dong, Xiaoxi Li, Jiajie Jin | - This paper introduces HiRA, a hierarchical reasoning framework for deep search that decouples strategic planning from specialized execution to improve efficiency and scalability. - HiRA decomposes complex search tasks into focused subtasks, assigns them to domain-specific agents with external tools, and coordinates results through a structured mechanism. - Experiments on four complex, cross-modal deep search benchmarks show that HiRA significantly outperforms state-of-the-art RAG and agent-based systems in both answer quality and system efficiency. - The separation of planning and execution in HiRA prevents execution details from disrupting high-level reasoning, enabling the system to leverage specialized expertise for different types of information processing. - HiRA's modular design allows for easy integration of new tools and capabilities, improving extensibility and adaptability to diverse search scenarios. | ['Question Answering'] | [Link](https://github.com/ignorejjj/HiRA) | N/A |
| [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754) | Jiecao Yu, Sijia Chen, Sai Surya Duvvuri, Timothy Chou, Aurko Roy | - This paper introduces the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions using an efficient Triton kernel. - The 2-simplicial Transformer achieves better token efficiency than standard Transformers; for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. - The authors demonstrate that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention. - The paper includes a detailed description of the 2-simplicial attention mechanism, including its implementation in Triton, and an analysis of its computational complexity. - Experimental results show that the 2-simplicial Transformer achieves significant improvements in downstream performance on reasoning-heavy tasks. | ['Natural Language Processing'] | N/A | N/A |
| [Can LLMs Identify Critical Limitations within Scientific Research? A
  Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694) | Arman Cohan, Lovekesh Vig, Manasi Patwardhan, Yilun Zhao, Zhijian Xu | This paper introduces LIMITGEN, a novel benchmark dataset designed to evaluate Large Language Models' (LLMs) ability to identify critical limitations within scientific research papers. The dataset is categorized into two subsets, one synthetic and one human-generated. The model uses a retrieval augmented generation (RAG) technique to ground LLM limitation generation and improve feedback quality. The study found that LLMs struggle to identify limitations, but RAG consistently improves performance. The researchers propose a taxonomy of limitation types, focusing on AI research, to guide evaluations. | ['Natural Language Processing'] | [Link](https://github.com/yale-nlp/LimitGen) | [Link](https://huggingface.co/datasets/yale-nlp/LimitGen) |
| [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092) | Peixuan Han, Md Mofijul Islam, Ganesh Nanduru, Alexi Gladstone, amanchadha | This paper introduces Energy-Based Transformers (EBTs), a new class of energy-based models that leverage a dynamic allocation of computation to improve model performance.  The EBT model architecture uses Transformers to enable efficient EBM training, outperforming the Transformer++ approach in scaling and demonstrating improvements with System 2 Thinking, or improved reasoning capabilities through additional computation.  Experiments across discrete and continuous modalities show EBTs scaling faster and achieving higher performance than Transformer++ and Diffusion Transformers on a variety of downstream tasks. EBTs also exhibit improved generalization to out-of-distribution data. | ['Natural Language Processing', 'Text Generation', 'Computer Vision', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/alexiglad/EBT) | N/A |
| [Selecting and Merging: Towards Adaptable and Scalable Named Entity
  Recognition with Large Language Models](https://arxiv.org/abs/2506.22813) | Wei Wei, Zhuojun Ding, Facico | - This paper introduces the SaM framework, a novel model merging strategy for Named Entity Recognition (NER) that dynamically selects and merges expert models at inference time. - SaM improves generalization across various domains without extra training by dynamically merging beneficial expert models, enhancing adaptability and scalability. - Extensive experiments demonstrate SaM's effectiveness, outperforming unified models by an average of 10% across multiple benchmarks and up to 20% in specific domains. - The framework's scalability is highlighted by the ability to conveniently add or remove experts, adapting to evolving needs. - SaM addresses the limitations of existing methods which struggle with adaptation and scalability due to the cost of data annotation and training domain-specific models. | ['Natural Language Processing'] | [Link](https://github.com/Ding-ZJ/SaM) | [Link](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) |
| [Self-Correction Bench: Revealing and Addressing the Self-Correction
  Blind Spot in LLMs](https://arxiv.org/abs/2507.02778) | Ken Tsui |  - This paper introduces Self-Correction Bench, a novel framework designed to systematically evaluate the self-correction capabilities of large language models (LLMs). - The framework injects controlled errors into LLM outputs, and it measures their ability to identify and correct these errors across three complexity levels. - Through experiments involving 14 models, it is observed that LLMs exhibit a significant "self-correction blind spot," failing to correct identical errors in their own outputs while succeeding on similar errors in user inputs. - This blind spot is linked to the composition of training data, where error-free responses are predominantly presented compared to error-correction sequences. -  A simple intervention, appending "Wait," remarkably improves self-correction performance, suggesting that the capability is present but underutilized. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear
  Attention](https://arxiv.org/abs/2507.01004) | Tianjian Li, Xinyi Wan, Ruijie Zhu, Zehao Liu, Yuhong Chou | - ZeCO is a novel sequence parallelism method for linear attention models that reformulates sequence parallelism by leveraging an All-Scan collective communication primitive, achieving the theoretically minimum communication volume.  - ZeCO's integrated approach enables efficient overlap of communication and computation, resulting in minimal extra computational and I/O overhead.  - Theoretical optimality of ZeCO is proven by showing that its time cost is minimal compared to other methods.  - Empirical results show that ZeCO achieves up to a 3.9x communication speedup and a 9.3x overall speedup compared to SOTA methods, demonstrating near-linear scalability from 8 to 256 devices.  - The method establishes a clear path towards efficiently training next-generation LLMs on previously intractable sequence lengths. | ['Natural Language Processing'] | N/A | N/A |
