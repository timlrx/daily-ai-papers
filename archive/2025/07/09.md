

## Papers for 2025-07-09

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203) | Tianhao Peng, jeshragh, chujiezheng, Jinfa, ridger | - This survey paper provides a comprehensive overview of latent reasoning in large language models (LLMs), focusing on how these models can perform complex reasoning tasks without explicitly generating intermediate reasoning steps. - It introduces a novel taxonomy for categorizing different latent reasoning approaches, including vertical and horizontal methods, and discusses the trade-offs between these approaches. - The authors analyze existing latent reasoning models and highlight key architectural design choices and training strategies. - They also explore the mechanistic interpretability of latent reasoning, investigating the role of different layers in the model's reasoning process. - Finally, the paper looks ahead to the future of latent reasoning, discussing the potential for infinite-depth reasoning and the development of more efficient and powerful reasoning systems. | ['Natural Language Processing'] | [Link](https://github.com/multimodal-art-projection/LatentCoT-Horizon/) | N/A |
| [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566) | Ron Kimmel, Daniel Bensaïd, David Bensaïd, royve, noamrot | - SingLoRA is a novel parameter-efficient fine-tuning method that reformulates low-rank adaptation using a single matrix instead of two, addressing the scale disparity issues that cause unstable training in existing methods like LoRA. - Unlike LoRA, SingLoRA inherently removes inter-matrix scale conflicts, ensuring stable optimization and roughly halving the parameter count. - Experiments on common sense reasoning (MNLI) show SingLoRA outperforming LoRA and LoRA+ while using only 60% of their parameter budget, and in image generation (DreamBooth), SingLoRA significantly improves image fidelity. - Theoretical analysis within the infinite-width neural network framework demonstrates that SingLoRA guarantees stable feature learning by construction. - The method is extended to non-square matrices and validated through comprehensive experiments across multiple modalities. | ['Natural Language Processing', 'Image-to-Image'] | N/A | N/A |
| [CriticLean: Critic-Guided Reinforcement Learning for Mathematical
  Formalization](https://arxiv.org/abs/2507.06181) | Yifan Yao, Zhongyuan Peng, zhangysk, zhouliang, yifAI | CriticLean is a novel critic-guided reinforcement learning framework for translating natural language mathematical statements into formal, executable code.  It introduces CriticLeanGPT, a model trained to assess the semantic fidelity of Lean 4 formalizations, and CriticLeanBench, a benchmark to measure models' ability to distinguish semantically correct from incorrect formalizations.  CriticLean significantly outperforms existing baselines on CriticLeanBench.  Furthermore, it constructs FineLeanCorpus, a dataset of over 285K problems exhibiting rich domain diversity and difficulty coverage.  The results highlight that optimizing the critic phase is essential for reliable formalization. | ['Reinforcement Learning', 'Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/multimodal-art-projection/CriticLean) | N/A |
| [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138) | Songyang Zhang, Maosong Cao, Taolin Zhang, jnanliu, MichaelErchi | - This paper introduces the "Code Triangle" framework, a novel approach to evaluating Large Language Models (LLMs) in code generation. - The framework assesses LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation, revealing inconsistencies in LLM cognition. - Experiments on competitive programming benchmarks show that while LLMs can form self-consistent systems, their solutions lack diversity and robustness compared to human programmers. - The study identifies a significant distribution shift between model cognition and human expertise, highlighting the impact of training data biases. - Incorporating human-generated data and leveraging model mixtures are shown to significantly enhance both performance and robustness. | ['Natural Language Processing'] | N/A | N/A |
| [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791) | Yuhao Yang, Yutong Dai, Dongxu Li, Yan Yang, Ziyang | - This paper introduces GTA1, a novel GUI Test-time Scaling Agent that addresses the challenges of ambiguity in task planning and accurate grounding of actions in complex interfaces. - GTA1 employs a test-time scaling method where multiple candidate action proposals are sampled at each step, and a judge model selects the most appropriate one. - The agent also uses a grounding model that predicts interaction coordinates directly, without relying on explicit reasoning. - Experiments on diverse benchmarks show that GTA1 achieves state-of-the-art performance in GUI grounding and task completion, outperforming existing methods across various metrics. - The code and models are open-sourced, allowing for further research and development in the field of GUI agents. | ['Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts](https://arxiv.org/abs/2507.04569) | Mohamed Anwar, Amr Mohamed, Ahmad Chamma, Hadi Abdine, guokan-shang | - This paper introduces Nile-Chat, a family of large language models (LLMs) for Egyptian Arabic that natively supports both Arabic and Latin scripts. - Nile-Chat-3x4B-A6B, a Mixture-of-Experts (MoE) model, leverages a novel language adaptation approach using the Branch-Train-MiX strategy to merge script-specialized experts. - The models significantly outperform existing multilingual and Arabic LLMs on new Egyptian evaluation benchmarks, achieving a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. - All models, data, and evaluation code are publicly available, making it a valuable resource for research on LLMs for underrepresented and dual-script languages. - The work introduces a comprehensive methodology for adapting LLMs to dual-script languages. | ['Translation', 'Text Generation', 'Natural Language Processing'] | [Link](https://github.com/MBZUAI-Paris/lm-evaluation-harness-nile-chat) | [Link](https://hf.co/MBZUAI-Paris/Nile-Chat-12B), [Link](https://hf.co/datasets/MBZUAI-Paris/Egyptian-SFT-Mixture), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianBench), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianMMLU), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianHellaSwag), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianPIQA), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianWinoGrande), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianOpenBookQA), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianRACE), [Link](https://hf.co/datasets/MBZUAI-Paris/EgyptianAlpacaEval) |
| [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223) | Yi Fang, Ting-ruen Wei, Zhiyuan Peng, yilunzhao, songtingyu | - This paper introduces new metrics, RPP and QPP, to evaluate the efficiency-effectiveness trade-off of LLM-based rerankers, addressing the limitations of existing proxy metrics. - It proposes a closed-form, interpretable formula for estimating the FLOPs of LLM-based rerankers and provides an open-source calculator. - The paper conducts a large-scale study comparing various LLM-based rerankers across different architectures and tasks using the proposed metrics. - Experimental results reveal that pointwise methods generally outperform pairwise and listwise methods in terms of both ranking metrics and FLOP efficiency. - The study highlights the importance of considering efficiency-effectiveness trade-offs when deploying LLM-based rerankers in practice. | ['Natural Language Processing'] | [Link](https://github.com/zhiyuanpeng/EER-FLOPs) | N/A |
| [LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation
  framework](https://arxiv.org/abs/2507.04723) | Ruoxi Sun, Baibei Ji, Haitian Wang, Zecheng Tang, QQTang1223 | - This paper introduces LOOM-Scope, a comprehensive and efficient framework for evaluating long-context models. - LOOM-Scope standardizes evaluation settings, supports efficient inference acceleration methods, and provides a holistic benchmark suite. - The framework addresses inconsistencies in existing benchmarks by standardizing evaluation settings and minimizing confounding factors. - LOOM-Scope supports 22 long-context benchmarks and over 140 tasks, covering diverse capabilities and context lengths. - Experiments demonstrate LOOM-Scope's efficiency, enabling comprehensive evaluations with significantly reduced computational costs compared to existing benchmarks. | ['Natural Language Processing'] | [Link](https://github.com/LCM-Lab/LOOM-Scope) | N/A |
| [Differential Mamba](https://arxiv.org/abs/2507.06204) | Eliya Nachmani, Itamar Zimerman, Nadav Schneider | - This paper introduces Diff-Mamba, a novel modification of the Mamba architecture that incorporates a differential mechanism to mitigate the issue of over-allocating attention to irrelevant context in sequence models. - Diff-Mamba achieves improved retrieval capabilities and superior performance compared to the vanilla Mamba architecture on language modeling benchmarks. - The proposed differential mechanism is empirically validated through extensive ablation studies and empirical analyses demonstrating improved performance on various language tasks. - The authors address the limitations of a naive adaptation of differential design to Mamba by introducing architectural modifications to effectively mitigate the over-allocation problem. - The code for Diff-Mamba is publicly available. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/NadavSc/Diff-Mamba) | N/A |
| [any4: Learned 4-bit Numeric Representation for LLMs](https://arxiv.org/abs/2507.04610) | Jeff Johnson, melhoushi | - This paper introduces any4, a novel 4-bit weight quantization technique for Large Language Models (LLMs). Unlike other methods, any4 does not require preprocessing of weights or activations.  - Any4 achieves higher accuracy than existing 4-bit numeric representation types (int4, fp4, nf4) across various model sizes and families (Llama 2, Llama 3, Mistral, Mixtral).  - It demonstrates competitiveness with orthogonal techniques (AWQ, GPTQ) that do require preprocessing.  - The method uses a single, curated diverse sample for calibration rather than hundreds, improving efficiency.  - Along with any4, the authors release tinygemm, a latency-optimized GPU matrix multiplication library. | ['Natural Language Processing'] | [Link](https://github.com/facebookresearch/any4) | N/A |
| [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based
  Reinforcement Learning](https://arxiv.org/abs/2507.05920) | Rui Feng, Bo Li, Weiwei Tian, Yuhao Dong, Xinyu Huang | - This paper introduces Multi-turn Grounding-based Policy Optimization (MGPO), a novel reinforcement learning framework that enables large multimodal models (LMMs) to iteratively focus on key visual regions by automatically cropping sub-images.  - MGPO effectively elicits stronger grounding capabilities compared to GRPO, leading to a 5.4% improvement on in-distribution MME-Realworld and a 5.2% improvement on the challenging out-of-distribution (OOD) V* Bench.  - MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses OpenAI's o1 and GPT-40 models on the OOD V* Bench.  - The proposed method overcomes the maximum pixel constraints of LMMs and does not require additional grounding annotations.  - MGPO achieves top-down and interpretable visual reasoning by providing outputs that indicate which image regions are attended to throughout the reasoning process. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/EvolvingLMMs-Lab/MGPO) | N/A |
| [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and
  Mitigation](https://arxiv.org/abs/2507.05578) | Dawn Song, Aneesh Pappu, Xuandong Zhao, Alexander Xiong | - This paper investigates the landscape of memorization in large language models (LLMs), examining its mechanisms, measurement, and mitigation. - It explores key drivers of memorization, including data duplication, training dynamics, and fine-tuning procedures. - The paper examines methodologies for detecting and measuring memorized content, such as prefix-based extraction and membership inference. - It discusses mitigation strategies, including data cleaning, differential privacy, and post-training unlearning. - Finally, the paper identifies critical directions for future research on LLM memorization, highlighting open challenges in balancing the minimization of harmful memorization with utility. | ['Natural Language Processing'] | N/A | N/A |
| [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411) | Hanzhi Zhou, John Peebles, Chang Lan, Tom Gunter, Mark Lee | - AXLearn is a production-ready deep learning system designed for scalable and high-performance training of large deep learning models. - It prioritizes modularity and supports heterogeneous hardware infrastructure, allowing for rapid model development and experimentation across various platforms. - AXLearn uses a novel method to quantify modularity via Lines-of-Code (LoC)-complexity, demonstrating its constant complexity even as components are added, unlike other systems with linear or quadratic complexity. - It achieves equivalent performance compared to state-of-the-art systems. - The paper provides details on the development and operation experiences of AXLearn. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/apple/axlearn) | N/A |
