

## Papers for 2025-07-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448) | Hongru Wang, Sofia Stoica, Xuehang Guo, Zhenhailong Wang, xhyandwyy | - The paper introduces PAPO, a novel reinforcement learning algorithm that improves multimodal reasoning by incorporating an Implicit Perception Loss. - PAPO encourages models to learn to perceive visual inputs while learning to reason, using internal supervision signals without relying on external reward models or additional data. - Experiments demonstrate that PAPO significantly improves performance on diverse multimodal reasoning benchmarks, with gains of up to 8% on tasks with high vision dependency. - A comprehensive error analysis reveals that PAPO substantially reduces perception errors, indicating improved perceptual capabilities. - The authors rigorously analyze and mitigate a unique loss hacking issue, proposing a Double Entropy Loss to prevent model collapse. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://mikewangwzhl.github.io/PAPO) | [Link](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), [Link](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) |
| [Rethinking Verification for LLM Code Generation: From Generation to
  Testing](https://arxiv.org/abs/2507.06920) | Minnan Luo, Wenwei Zhang, Maosong Cao, Taolin Zhang, MichaelErchi |  - This paper introduces SAGA, a novel human-LLM collaborative framework for Test Case Generation (TCG), which significantly improves the quality and diversity of generated test cases compared to existing methods.  - The effectiveness of SAGA is demonstrated through experiments on TCGBench, achieving a 90.62% detection rate and 32.58% verifier accuracy.  - SAGA's superior performance is further validated by its application to enhance the LiveCodeBench-v6 benchmark, demonstrating a 10.78% increase in verifier accuracy.  - The proposed multi-dimensional evaluation metrics rigorously quantify test suite thoroughness and provide a valuable framework for assessing the quality of code verifiers.  - Finally, the work includes the development of TCGBench, a benchmark dataset for facilitating future research into TCG. | ['Text Generation'] | [Link](https://github.com/open-compass/SAGA) | N/A |
| [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457) | Taylor Kergan, Yong Shan, Steven Abreu, Dustin Wang, ridger | - This paper presents a systematic analysis of hybrid linear attention models for long sequences, addressing the quadratic complexity limitations of traditional Transformers. - It comprehensively evaluates various linear attention models across generations, both standalone and hybridized, training and open-sourcing 72 models with varying parameters and training data. - The study reveals that superior standalone linear models do not necessarily translate to superior hybrid performance, and recall significantly improves with increased full attention layers. - It highlights the importance of selective gating, hierarchical recurrence, and controlled forgetting in achieving efficient hybrid models, recommending architectures like HGRN-2 or GatedDeltaNet with a specific linear-to-full attention ratio. - The paper offers practical guidelines for building memory-efficient long-context language models based on these findings. | ['Natural Language Processing'] | N/A | N/A |
| [Towards Solving More Challenging IMO Problems via Decoupled Reasoning
  and Proving](https://arxiv.org/abs/2507.06804) | Feng Zhang, Tao Yang, Yang Li, Linfeng Song, Zhenwen Liang | This paper introduces a novel framework for automated theorem proving that decouples high-level reasoning from low-level proof generation.  The framework uses two specialized models: a general-purpose Reasoner (LLM) to generate strategic lemmas and an efficient Prover to verify them.  This approach successfully solves 5 challenging post-2000 IMO problems, significantly outperforming existing methods.  The authors release a dataset of generated and verified lemmas to facilitate further research. The framework's decoupled design allows the Reasoner and Prover to leverage their respective strengths, enhancing overall problem-solving capabilities. | ['Natural Language Processing'] | [Link](https://tencent-imo.github.io/) | [Link](https://tencent-imo.github.io/) |
| [ModelCitizens: Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455) | Karolina Naranjo, notaphonologist, hamidpalangi, christinachance, Ashima | - This paper introduces MODELCITIZENS, a new dataset comprising 6.8K social media posts and 40K toxicity annotations across diverse identity groups, addressing the limitations of existing datasets which often collapse diverse annotator perspectives into a single ground truth. - To capture the conversational context typical of social media posts, the dataset includes LLM-generated conversational scenarios. - State-of-the-art toxicity detection tools underperform on MODELCITIZENS, highlighting the importance of community-informed annotation and modeling. - Two new models, LLAMACITIZEN-8B and GEMMACITIZEN-12B, are presented, which outperform existing models by 5.5% on in-distribution evaluations. - The findings underscore the significance of community-informed annotation and modeling for inclusive content moderation. | ['Text Classification'] | [Link](https://github.com/asuvarna31/modelcitizens) | N/A |
| [Evaluating the Critical Risks of Amazon's Nova Premier under the
  Frontier Model Safety Framework](https://arxiv.org/abs/2507.06260) | Vincent Ponzo, Matteo Memelli, Abhinav Mohanty, Ninareh Mehrabi, Satyapriya Krishna | - This paper presents a comprehensive evaluation of Amazon's Nova Premier, a multimodal foundation model, using the Frontier Model Safety Framework. - The evaluation focuses on three high-risk domains: Chemical, Biological, Radiological & Nuclear (CBRN) weapons proliferation, Offensive Cyber Operations, and Automated AI R&D. - The methodology combines automated benchmarks, expert red-teaming, and uplift studies to assess whether the model exceeds release thresholds. - The findings indicate that Nova Premier is safe for public release, according to the commitments made at the 2025 Paris AI Safety Summit. - This work provides a template for future cross-organisational safety audits of frontier models. | ['Multimodal'] | N/A | N/A |
| [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large
  Language Models on Harmfulness](https://arxiv.org/abs/2507.01702) | Zhen Ye, Ziyang Luo, Kaixin Li, Hongzhan Lin, Zixin Chen | - This paper introduces AdamMeme, a novel evaluation framework for assessing the reasoning capabilities of multimodal large language models (mLLMs) on meme harmfulness. - AdamMeme uses a multi-agent system with a harmfulness mining agent, model scoring agent, and iterative refinement agent to dynamically assess the mLLMs. - The framework addresses the limitations of existing static benchmarks by iteratively generating challenging meme samples, revealing model-specific weaknesses. - Extensive experiments demonstrated that AdamMeme effectively reveals varying performance and model-specific vulnerabilities of different mLLMs on harmfulness analysis. - The proposed framework is adaptable to the evolving nature of memes, promoting more comprehensive and diverse evaluation. | ['Multimodal'] | [Link](https://github.com/Lbotirx/AdamMeme) | N/A |
