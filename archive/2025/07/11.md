

## Papers for 2025-07-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966) | Hanrong Ye, Qinghao Hu, Baifeng Shi, Wei Huang, Yukang Chen | - The paper introduces LongVILA-R1, a framework that scales reasoning in vision-language models to long videos, using reinforcement learning. - LongVILA-R1 incorporates a large-scale dataset (Long Video-Reason), a two-stage training pipeline (CoT-SFT and RL), and a novel training infrastructure (MR-SP) for efficient long video RL training. - Experiments show LongVILA-R1 achieves strong performance on long video QA benchmarks, outperforming existing methods and matching the performance of Gemini-1.5-Pro on temporal, goal, spatial, and plot reasoning tasks. - The MR-SP system achieves up to 2.1x speedup on long video RL training, enabling training on hour-long videos on a single A100 node. - The authors release their training system, supporting RL training on various modalities (video, text, and audio), various models, and even image and video generation models. | ['Video-Text-to-Text', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/NVlabs/Long-RL) | N/A |
| [Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and
  Methodology](https://arxiv.org/abs/2507.07999) | Zilong Huang, garlicisnotmyfavor, stormthunder, LXT, HaochenWang |  - This paper introduces TreeBench, a new benchmark for evaluating visual grounded reasoning in large multimodal models.  TreeBench emphasizes three key aspects: focused visual perception, traceable evidence, and vision-centric second-order reasoning. - TreeBench contains 405 challenging visual question-answering pairs with accurate bounding boxes of target instances.  Existing models achieve less than 60% accuracy, highlighting its difficulty. - The authors also propose TreeVGR, a training paradigm using reinforcement learning with traceable evidence. TreeVGR enhances the accuracy of localization and the explainability of reasoning pathways. - Experiments on TreeBench and other benchmarks show that TreeVGR improves performance compared to existing methods, demonstrating the effectiveness of its approach. - TreeBench and TreeVGR are publicly available and are expected to advance the field of multimodal reasoning assessment. | ['Visual Question Answering', 'Multimodal'] | [Link](https://github.com/Haochen-Wang409/TreeVGR) | N/A |
| [OST-Bench: Evaluating the Capabilities of MLLMs in Online
  Spatio-temporal Scene Understanding](https://arxiv.org/abs/2507.07984) | Xihui Liu, Xiaohan Mao, Runsen Xu, Chenming Zhu, JingLi Lin |  - OST-Bench is a new benchmark for evaluating online spatio-temporal scene understanding in multi-modal large language models (MLLMs).  - It consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes, focusing on online and spatio-temporal understanding.  - OST-Bench evaluates several leading MLLMs and finds that they fall short on tasks requiring complex spatio-temporal reasoning, with accuracy declining as the exploration horizon extends.  - Common error patterns across models are identified, highlighting the challenges of complex clue-based spatial reasoning and long-term memory retrieval.  - The benchmark is publicly available to foster further research and development in the field. | ['Visual Question Answering', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/rbler1234/OST-Bench) | N/A |
| [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998) | Qilong Wu, Ming Li, Shaoheng Lin, haoquan03, stzhao | - PyVision is a novel multi-turn framework that enables large language models (LLMs) to autonomously generate, execute, and refine Python-based tools for visual reasoning tasks. - It introduces dynamic tooling, allowing LLMs to create custom Python code snippets tailored to the specific problem, going beyond using pre-defined toolsets. - PyVision achieves consistent performance gains across various benchmarks, including a +7.8% improvement on V* with GPT-4.1 and a +31.1% improvement on VLMsAreBlind-mini with Claude-4.0-Sonnet. - The framework uses a multi-turn interaction loop between the LLM and a Python interpreter, allowing for iterative refinement of code and reasoning based on visual feedback. - PyVision's dynamic tooling enables adaptive, grounded, verifiable visual reasoning by creating highly task-specific tools, demonstrating its potential in more agentic visual reasoning scenarios. | ['Multimodal'] | N/A | N/A |
| [Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs](https://arxiv.org/abs/2507.07996) | Yang Li, Ziyue Li, zhoutianyi | - This paper introduces Chain-of-Layers (CoLa), a novel technique that adapts the architecture of pretrained LLMs at test time without any further training. - CoLa dynamically skips, repeats, or reorders layers from the pretrained model to create a customized architecture (CoLa) for each input, allowing for flexible, dynamic architectures. - The authors develop a Monte Carlo Tree Search (MCTS) algorithm to efficiently explore the CoLa search space and identify the optimal architecture for each sample. - Extensive experiments on math and commonsense reasoning benchmarks demonstrate that CoLa consistently improves prediction accuracy and efficiency compared to using the original LLM's fixed architecture. - CoLa's ability to dynamically adapt depth and architecture suggests a significant potential for improving the efficiency and generalization of LLMs on diverse downstream tasks. | ['Natural Language Processing'] | N/A | N/A |
| [Machine Bullshit: Characterizing the Emergent Disregard for Truth in
  Large Language Models](https://arxiv.org/abs/2507.07484) | Thomas L. Griffiths, Dawn Song, Xuandong Zhao, Haimin Hu, Kaiqu Liang |  - This paper introduces a novel metric, the Bullshit Index, to quantify large language models' (LLMs) indifference to truth.   - It proposes a taxonomy classifying four qualitative forms of machine bullshit: empty rhetoric, paltering, weasel words, and unverified claims.   - Empirical evaluations on several benchmarks demonstrate that reinforcement learning from human feedback (RLHF) significantly exacerbates LLM bullshit.   - The study also reveals that chain-of-thought prompting amplifies specific forms of bullshit.   - Findings highlight systematic challenges in AI alignment and offer new insights toward enhancing truthfulness in LLMs. | ['Natural Language Processing'] | [Link](https://machine-bullshit.github.io) | N/A |
| [Beyond the Linear Separability Ceiling](https://arxiv.org/abs/2507.07574) | Mohit Vaishnav, Tanel Tammet, envomp | This paper introduces a novel diagnostic framework to analyze the limitations of Visual-Language Models (VLMs) on abstract reasoning tasks. The framework identifies a "linear reasoning bottleneck" where VLM performance is limited by the linear separability of their visual embeddings.  The authors demonstrate that this bottleneck is not due to poor visual perception, but rather a failure in the reasoning pathways.  The study proposes two pathways to improve VLM performance: (1) refining embeddings for improved linear separability, (2) enabling non-linear reasoning through targeted alignment.  Finally, the framework helps identify intervention points for efficient fine-tuning to enhance VLM performance.  | ['Multimodal'] | N/A | N/A |
