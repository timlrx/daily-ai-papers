

## Papers for 2025-07-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951) | Jie Gao, Mengting Xing, Xiaorui Wang, Yuxin Wang, Zixiao Wang | - This paper introduces MetaStone-S1, a novel reflective generative model that achieves comparable performance to OpenAI's 03-mini series for various reasoning tasks. - MetaStone-S1 employs a new Reflective Generative Form, which features a unified interface for policy and process reward models, eliminating the reliance on process-level annotations. - The model architecture utilizes a shared backbone network for both reasoning trajectory prediction and scoring, reducing the number of extra parameters needed for trajectory scoring. - Experiments demonstrate that MetaStone-S1 achieves comparable performance to OpenAI 03-mini with only 32B parameters, outperforming several other open-source and closed-source models in various benchmarks. - The authors open-sourced MetaStone-S1 to support further research and development within the community. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/MetaStone-AI/MetaStone-S1) | N/A |
| [NeuralOS: Towards Simulating Operating Systems via Neural Generative
  Models](https://arxiv.org/abs/2507.08800) | Yuntian Deng, Wenhu Chen, Hongyu Guo, Sun Sun, Luke Rivard | - NeuralOS is a novel neural framework that simulates graphical user interfaces (GUIs) by directly predicting screen frames based on user inputs (mouse movements, clicks, keyboard events). - The model architecture consists of a recurrent neural network (RNN) that tracks the computer's state and a diffusion-based neural renderer that generates screen images. - NeuralOS was trained on a large-scale dataset of Ubuntu XFCE recordings, combining randomly generated and realistic interactions from AI agents. - Experimental results demonstrate that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions. - While precise keyboard interaction modeling is challenging, NeuralOS is a significant step toward creating fully adaptive and generative neural interfaces for human-computer interaction systems. | ['Multimodal', 'Image-to-Image', 'Image-to-Video', 'Computer Vision', 'Reinforcement Learning'] | [Link](https://github.com/google-research/google-research/tree/master/neural_os) | N/A |
| [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799) | Cees G. M. Snoek, M. Jehanzeb Mirza, Michael Dorkenwald, Dawid J. Kopiczko, Max Belitsky | - This paper introduces cache steering, a novel method for implicitly guiding language models' reasoning behavior by directly modifying the key-value cache. - The method uses GPT-4-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without requiring fine-tuning or prompt modifications. - Experimental evaluations on several reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. - Compared to activation steering, cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration. - The proposed method is lightweight and compatible with standard inference APIs, making it a more practical and robust solution for controlled generation. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/MaxBelitsky/cache-steering) | N/A |
| [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397) | Zilong Ye, Wangbo Zhao, Xiaopeng Peng, Jie Xia, Pengfei Zhou | - LoongX, a novel hands-free image editing approach driven by multimodal neurophysiological signals (EEG, fNIRS, PPG, and head motion) is proposed. - LoongX integrates two key modules: a cross-scale state space (CS3) module and a dynamic gated fusion (DGF) module to effectively address the heterogeneity of multimodal signals. - LoongX achieves performance comparable to text-driven methods and outperforms them when neural signals are combined with speech, highlighting the promise of neural-driven generative models. - The model is trained on a comprehensive dataset of 23,928 image editing pairs with synchronized multimodal neural signals. - Datasets and code will be released to support future work and foster progress in this emerging area. | ['Image-to-Image', 'Multimodal'] | [Link](https://loongx1.github.io) | N/A |
| [Lumos-1: On Autoregressive Video Generation from a Unified Model
  Perspective](https://arxiv.org/abs/2507.08801) | Jingyun Liang, Hu Yu, Jun Cen, Weihua Chen, Hangjie Yuan | - Lumos-1 is a novel autoregressive video generation model that leverages the LLM architecture with minimal modifications, addressing limitations of existing methods. - It incorporates a modified rotary position embedding (MM-ROPE) scheme to effectively capture spatiotemporal correlations in video data, improving upon previous 3D RoPE techniques by addressing frequency spectrum imbalances. - To mitigate frame-wise loss imbalance caused by spatial information redundancy, Lumos-1 introduces Autoregressive Discrete Diffusion Forcing (AR-DF), a training and inference masking strategy that maintains temporal causality. - The model achieves performance comparable to state-of-the-art methods on multiple benchmarks (GenEval, COSMOS-Video2World, OpenSoraPlan), despite being pre-trained on a relatively smaller scale (48 GPUs). - The authors introduce several memory-efficient training techniques, demonstrating the potential for effective autoregressive video generation within a unified model framework. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | [Link](https://github.com/alibaba-damo-academy/Lumos) | N/A |
| [Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for
  Visual Reasoning](https://arxiv.org/abs/2507.05255) | Jisheng Yin, Kangheng Lin, Jianjian Sun, Liang Zhao, Yana Wei |  - This paper introduces Open-Vision-Reasoner (OVR), a novel multimodal reasoning model that leverages a two-stage training paradigm: linguistic cold-start fine-tuning followed by multimodal reinforcement learning. - OVR achieves state-of-the-art performance on various reasoning benchmarks, including MATH500 (95.3%), MathVision (51.8%), and MathVerse (54.6%), surpassing previous open-source efforts. - The model's superior performance is attributed to the transfer of linguistic cognitive behaviors to visual reasoning, a phenomenon observed early in the training process. -  OVR exhibits high utility behaviors such as visual reflection, effectively leveraging visual cognitive behaviors in complex visual tasks. - The authors contribute a comprehensive analysis of the model's training dynamics and visual cognitive behavior, releasing the model, data, and training dynamics to foster further research. | ['Multimodal'] | N/A | N/A |
| [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,
  Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261) | Noveen Sachdeva, Ice Pasupat, Mike Schaekermann, Eric Bieber, Gheorghe Comanici |  - The paper introduces the Gemini 2.X family of models, including Gemini 2.5 Pro and Gemini 2.5 Flash, which are multimodal models with advanced reasoning and tool-use capabilities.  - Gemini 2.5 Pro is a sparse mixture-of-experts (MoE) transformer that achieves state-of-the-art performance on various benchmarks, including coding, reasoning, and multimodal understanding.  - The models support long context inputs of over 1 million tokens and have native tool use support, enabling them to comprehend vast datasets and handle complex problems from various sources.  - Different models in the series are designed to span the Pareto frontier of model capability versus cost, offering users the ability to explore complex agentic problem-solving scenarios.  - The paper showcases several example applications of Gemini 2.5 Pro, including Gemini Plays Pok√©mon, where the model successfully completed the game autonomously. | ['Multimodal', 'Video-Text-to-Text', 'Reinforcement Learning', 'Robotics'] | N/A | N/A |
| [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with
  Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771) | Yingfa Chen, Chaojun Xiao, Xu Han, Weilin Zhao, Chenyang Song | - This paper introduces BlockFFN, a novel Mixture-of-Experts (MoE) architecture designed for efficient large language model (LLM) inference on resource-constrained devices. - BlockFFN integrates a ReLU-based differentiable and flexible routing mechanism, which is more efficient than existing MoE routers, and uses CLS-aware training objectives to improve chunk-level sparsity, making it friendlier to acceleration. - The proposed model achieves over 80% token-level sparsity and 70% 8-token chunk-level sparsity. - BlockFFN is evaluated on various tasks and demonstrates significant improvements in terms of perplexity and speed compared to existing MoE models. - Efficient acceleration kernels are implemented that combine activation sparsity and speculative decoding, resulting in up to 3.67x speedup on end-side devices compared to dense models. | ['Natural Language Processing'] | [Link](https://github.com/thunlp/BlockFFN) | N/A |
