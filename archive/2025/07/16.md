

## Papers for 2025-07-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and
  Reasoning Modes](https://arxiv.org/abs/2507.11407) | Stanley Jungkyu Choi, Kibong Choi, Eunbi Choi, Kyunghoon Bae, LG AI Research |  - This technical report introduces EXAONE 4.0, a unified large language model that integrates both non-reasoning and reasoning modes, improving upon the capabilities of its predecessors, EXAONE 3.5 and EXAONE Deep. - The model architecture employs a hybrid attention mechanism combining local and global attention, enhancing long-context processing and reducing computational costs.  It also features a re-positioning of layer normalization for improved performance. - EXAONE 4.0 supports three languages: English, Korean, and Spanish, with multilingual capabilities expanded.  The model series includes two sizes: a 32B parameter model optimized for performance, and a 1.2B parameter model for on-device applications. - Compared to other models in its class, EXAONE 4.0 demonstrates superior performance, maintaining competitiveness even against frontier-class models and excelling in areas such as world knowledge, mathematical and coding reasoning tasks. - The models are publicly available for research purposes and are designed to pave the way for the agentic AI era by incorporating agentic tool use. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/LGAI-EXAONE) |
| [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404) | Enrico Fini, David Grangier, Dan Busbridge, Louis Bethune, Mustafa Shukor | - This paper introduces a novel systematic method for determining the optimal data mixture for large foundation models, using scaling laws.- The approach accurately predicts model loss as a function of model size, training tokens, and domain weights, validated across LLMs, NMMs, and LVMs.- Scaling laws are shown to extrapolate well to new data mixtures and scales, allowing for the estimation of optimal domain weights using only a few small-scale training runs.- The proposed method offers a principled alternative to the costly trial-and-error approach typically used for data mixture selection in large-scale model training.- Results demonstrate that the scaling laws accurately predict performance at larger scales and new domain weights, and that the derived optimal domain weights lead to improved performance compared to other methods. | ['Multimodal'] | N/A | N/A |
| [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via
  Self-Critique](https://arxiv.org/abs/2507.09075) | Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek, Wasi Uddin Ahmad, smajumdar94 | - This paper introduces OPENCODEREASONING-II, a new dataset with 2.5M question-solution-critique triples, nearly double the size of existing datasets. -  It presents a two-stage supervised fine-tuning strategy for code generation and critique, achieving performance exceeding or equal to prior open-weight distilled models. - The integration of code generation and critique models significantly improves competitive coding performance. - An extension of the LiveCodeBench benchmark is introduced to specifically support the C++ programming language. - The paper evaluates a simple test-time scaling approach using self-critique for selecting the best code solutions from multiple generations. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | [Link](https://huggingface.co/datasets/nvidia/OpenCodeReasoning-2), [Link](https://huggingface.co/datasets/nvidia/LiveCodeBench-CPP) |
| [Planted in Pretraining, Swayed by Finetuning: A Case Study on the
  Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186) | Gabriel Stanovsky, Yonatan Belinkov, itay1itzhak | - This paper introduces a two-step causal experimental approach to investigate the origins of cognitive biases in large language models (LLMs). - The first step assesses the impact of training randomness by repeatedly finetuning the same pretrained models with different random seeds. - The second step uses a novel "cross-tuning" approach that swaps instruction datasets between models with different bias patterns to isolate bias sources. - Results reveal that pretraining is the main factor shaping cognitive biases in LLMs, while training randomness and finetuning data have smaller effects. - The findings highlight the importance of considering pretraining origins when evaluating and mitigating biases in LLMs. | ['Natural Language Processing'] | [Link](https://github.com/itaylitzhak/planted-in-pretraining) | N/A |
