

## Papers for 2025-07-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334) | ShowerMaker, LImax72, YuyaoGe, Theodyy, Chevalier | This survey introduces Context Engineering as a formal discipline for optimizing information payloads for LLMs, going beyond simple prompt design.  It presents a comprehensive taxonomy that decomposes Context Engineering into foundational components (Context Retrieval and Generation, Context Processing, Context Management) and system implementations (RAG, Memory Systems, Tool-Integrated Reasoning, Multi-Agent Systems).  The survey analyzes over 1400 research papers, revealing a critical research gap: current models, while proficient in understanding complex contexts, struggle to generate equally sophisticated outputs.  Finally, the survey provides a unified framework for researchers and engineers in the field. | ['Natural Language Processing'] | [Link](https://github.com/Meirtz/Awesome-Context-Engineering) | [Link](null) |
| [VisionThink: Smart and Efficient Vision Language Model via Reinforcement
  Learning](https://arxiv.org/abs/2507.13348) | Hengshuang Zhao, Bei Yu, Xin Lai, Junyi Li, Senqiao Yang |  - VisionThink is a novel efficient vision language model (VLM) that uses reinforcement learning to dynamically adjust image resolution based on task complexity.  - The model starts with a downsampled image and requests higher resolution only when needed, saving computational resources without sacrificing accuracy.  - VisionThink is shown to outperform existing methods on various benchmarks, particularly those with strong OCR-related tasks, as demonstrated by achieving 102% performance on an OCR-related benchmark (compared to baseline 100%).  - A key innovation is the LLM-as-Judge strategy for reinforcement learning, which uses an external LLM to assess the accuracy of VLM responses, enabling training on diverse general VQA tasks.  - Extensive experiments demonstrate superior efficiency and effectiveness compared to other efficient VLMs and state-of-the-art models. | ['Visual Question Answering', 'Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/dvlab-research/VisionThink) | [Link](null) |
| [The Imitation Game: Turing Machine Imitator is Length Generalizable
  Reasoner](https://arxiv.org/abs/2507.13332) | Songyang Gao, Chengqi Lyu, Wenwei Zhang, vanilla1116, ZhouqiHUA | - This paper introduces TAIL (Turing Machine Imitation Learning), a novel method to enhance the length generalization ability of large language models (LLMs). - TAIL synthesizes chain-of-thought (CoT) data that mimics the execution process of a Turing Machine, addressing the challenge of length generalization in LLMs. - The proposed method uses three key structures in the synthesized CoT data: Linear Transition, Atomic State, and Memory Fetcher, which emulate the core properties of Turing Machine execution. - Experiments on a challenging synthetic dataset with 8 algorithm classes and 18 tasks show that TAIL significantly improves length generalization ability compared to previous methods and DeepSeek-R1. - Ablation studies demonstrate the necessity of each core module of TAIL for effective length generalization. | ['Natural Language Processing'] | N/A | N/A |
| [AnyCap Project: A Unified Framework, Dataset, and Benchmark for
  Controllable Omni-modal Captioning](https://arxiv.org/abs/2507.12841) | Gao Meng, Yu Li, Zhiqiang Lin, Yiming Ren, Ruihang |  - This paper introduces the AnyCap Project, a unified framework, dataset, and benchmark for controllable omni-modal captioning.  - The AnyCapModel (ACM) is a lightweight, plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning.  - The AnyCapDataset (ACD) is a large-scale omni-modal dataset covering 3 modalities and 28 types of user instructions, with 300k high-quality data entries. - The AnyCapEval is a novel benchmark that offers more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. - Experiments show ACM significantly improves caption quality across diverse base models and outperforms state-of-the-art methods on widely-used benchmarks. | ['Multimodal'] | [Link](https://github.com/qishisuren123/AnyCap) | N/A |
| [MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://arxiv.org/abs/2507.12508) | Reuben Tan, Siyuan Zhou, Zheyuan Zhang, Jiageng Liu, yyuncong | - MindJourney is a test-time scaling framework that enhances Vision-Language Models (VLMs) for spatial reasoning by integrating a controllable world model based on video diffusion. - The VLM iteratively generates camera trajectories, while the world model synthesizes corresponding views, enabling reasoning over multi-view evidence without fine-tuning. - On the SAT benchmark, MindJourney achieves an average 8% performance improvement, surpassing the performance of test-time inference VLMs trained with reinforcement learning. - The proposed method is model-agnostic, enhancing multiple VLMs with different world models. - MindJourney offers a simple, plug-and-play route for robust 3D reasoning by leveraging the strengths of both VLMs and world models. | ['Visual Question Answering', 'Multimodal', 'Video-Text-to-Text'] | [Link](https://umass-embodied-agi.github.io/MindJourney/) | N/A |
| [AbGen: Evaluating Large Language Models in Ablation Study Design and
  Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300) | Yixin Liu, Manasi Patwardhan, Zhijian Xu, Weiyuan Chen, Yilun Zhao | - ABGEN, a benchmark for evaluating LLMs in designing ablation studies for scientific research, is introduced. It comprises 1,500 expert-annotated examples derived from 807 NLP papers. - LLMs are tasked with generating detailed ablation study designs based on given research contexts, and leading LLMs are evaluated on their performance. - A significant performance gap is found between LLMs and human experts regarding the importance, faithfulness, and soundness of generated designs. - ABGEN-EVAL, a meta-evaluation benchmark, is developed to assess the reliability of commonly used automated evaluation systems. - The study investigates various LLM-as-Judge systems on ABGEN-EVAL, providing insights for future research on developing reliable LLM-based evaluation systems for scientific tasks. | ['Natural Language Processing'] | [Link](https://github.com/yale-nlp/AbGen) | [Link](https://huggingface.co/yale-nlp/AbGen) |
| [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720) | Sachin Kumar, Orevaoghene Ahia, Abraham Toluase Owodunni | - This paper introduces FLEXITOKENS, a novel training objective for byte-level language models that enables flexible and adaptive tokenization. - The model architecture incorporates a submodule that learns to predict boundaries between byte sequences, dynamically adapting to new data distributions during finetuning. - FLEXITOKENS consistently outperforms existing subword and gradient-based tokenization methods across diverse multilingual benchmarks and morphologically rich tasks, showing up to 10% improvement in downstream task performance. - The proposed method reduces token over-fragmentation, resulting in improved efficiency and better generalization to unseen languages and domains. - The code and data for the experiments are publicly available at https://github.com/owos/flexitokens | ['Natural Language Processing'] | [Link](https://github.com/owos/flexitokens) | N/A |
| [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255) | Nay Oo, Tri Cao, Ziwen Xu, Mengru Wang, Lyucheng Wu | - AutoSteer, a novel inference-time intervention technique, is introduced to mitigate safety risks in multimodal large language models (MLLMs) without requiring model retraining. - The method incorporates three core components: a Safety Awareness Score (SAS) to identify safety-relevant layers, an adaptive safety prober to estimate toxicity, and a lightweight Refusal Head to intervene when necessary. - Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the attack success rate (ASR) for various threats while preserving general abilities. - AutoSteer is shown to be practical, interpretable, and effective, offering a modular and model-agnostic framework for safer MLLM deployment. - The findings highlight AutoSteer's potential as a practical solution for improving the safety of multimodal AI systems in real-world applications. | ['Multimodal'] | N/A | N/A |
| [Voxtral](https://arxiv.org/abs/2507.13264) | Corentin Barreau, Cl√©ment Denoix, Andy Lo, Andy Ehrenberg, Alexander H. Liu | - This paper introduces Voxtral Mini and Voxtral Small, two open-source multimodal audio chat models that achieve state-of-the-art performance on various audio benchmarks while maintaining strong text capabilities. - Voxtral models utilize a Transformer architecture with an audio encoder (Whisper large-v3), an adapter layer for downsampling, and a language decoder (Mistral). - The models are trained in three phases: pretraining (with audio-text repetition and cross-modal continuation patterns), supervised finetuning on speech understanding tasks, and preference alignment using Direct Preference Optimization. - Voxtral Small outperforms several closed-source models, demonstrating its effectiveness in speech transcription, translation, and question answering. - The authors also introduce three new benchmarks for evaluating speech understanding models on knowledge and trivia, contributing to the field's evaluation ecosystem. | ['Audio', 'Automatic Speech Recognition', 'Multimodal', 'Question Answering'] | N/A | [Link](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507), [Link](https://huggingface.co/mistralai/Voxtral-Small-24B-2507), [Link](https://huggingface.co/collections/mistralai/speech-evals-6875e9b26c78be4a081050f4) |
