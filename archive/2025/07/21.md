

## Papers for 2025-07-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges
  in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563) | Mikhail Gorodnichev, Maxim Maslov, Vasiliy Kudryavtsev, Nikita Vasiliev, Kirill Borodin | - The paper introduces Balalaika, a new Russian speech dataset containing over 2000 hours of high-quality speech with comprehensive annotations, including punctuation and stress markings. -  The dataset significantly improves the performance of speech synthesis and enhancement models compared to existing datasets, as demonstrated by experimental results. - The authors detail the dataset construction pipeline, including data collection, audio cutting, separation, transcription, punctuation, and stress placement, along with the various models used in these steps. -  The paper also presents a comparative analysis of various speech restoration and synthesis models trained on Balalaika and other datasets using different metrics, highlighting Balalaika's superior performance. - The results show improvements in NISQA, MOS, and CER metrics indicating higher quality speech synthesis when using the Balalaika dataset.  This is attributed to improved annotation quality and inclusion of prosodic features. | ['Text-to-Speech'] | [Link](https://github.com/mtuciru/balalaika) | [Link](https://huggingface.co/RUPunct), [Link](https://huggingface.co/salute-developers/GigaAM) |
| [The Devil behind the mask: An emergent safety vulnerability of Diffusion
  LLMs](https://arxiv.org/abs/2507.11097) | Ruixi Wu, Zhiyuan Liu, Dongrui Liu, Zichen Wen, Joshua999 | - This paper introduces DIJA, a novel jailbreak attack framework that exploits the unique safety vulnerabilities of diffusion-based large language models (dLLMs). - DIJA constructs adversarial interleaved mask-text prompts that leverage the bidirectional context modeling and parallel decoding mechanisms of dLLMs to generate harmful outputs. - Through comprehensive experiments, DIJA significantly outperforms existing jailbreak methods across multiple dLLMs and benchmarks, achieving up to 100% keyword-based attack success rate. - The findings underscore the urgent need for rethinking safety alignment in dLLMs and highlight the critical gaps in current alignment strategies. - DIJA's effectiveness is demonstrated on several publicly available dLLMs across multiple benchmarks, showing its robustness against existing defense mechanisms. | ['Natural Language Processing', 'Text Generation', 'Fill-Mask'] | [Link](https://github.com/ZichenWen1/DIJA) | N/A |
| [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal
  Large Language Models](https://arxiv.org/abs/2507.12566) | Xue Yang, Wenhao Li, Wenhan Dou, Gen Luo, wzk1015 | This paper introduces Mono-InternVL-1.5, a monolithic multimodal large language model (MLLM) that integrates visual encoding and language decoding into a single model.  The model architecture uses a multimodal mixture-of-experts to incorporate visual experts and addresses the challenge of catastrophic forgetting through delta tuning.  Mono-InternVL-1.5 significantly improves training and inference efficiency compared to its predecessor, Mono-InternVL, and outperforms existing monolithic MLLMs on 12 out of 15 benchmarks.  The authors also introduce an improved Endogenous Visual Pre-training (EViP++) method and a fused CUDA kernel for faster inference. | ['Multimodal'] | [Link](https://github.com/OpenGVLab/Mono-InternVL) | N/A |
| [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking
  Services](https://arxiv.org/abs/2507.10605) | Ziyan Liu, Zheyong Xie, Yue Wang, Chonggang Lu, Hiiamein | - RedOne is a domain-specific large language model (LLM) for social networking services (SNS), trained using a three-stage approach (continued pre-training, supervised fine-tuning, and preference optimization). - It achieves an average improvement of up to 14.02% across eight major SNS tasks and 7.56% in an SNS bilingual evaluation benchmark compared to baseline models. - RedOne demonstrates strong generalization capabilities across various tasks and shows promising applicability in real-world scenarios, such as reducing harmful content exposure and improving click-through rates. - The model's performance is evaluated through extensive experiments on various benchmarks, both general and SNS-specific. - Ablation studies confirm the effectiveness of each training stage, highlighting the importance of a comprehensive approach for optimal performance. | ['Natural Language Processing'] | N/A | N/A |
| [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455) | Zhuotao Tian, Li Jiang, Senqiao Yang, Shangpin Peng | This paper introduces SENTINEL, a novel framework to reduce object hallucinations in multimodal large language models (MLLMs).  SENTINEL leverages an in-domain preference learning approach, mitigating hallucinations at early generation stages without using external models.  Experimental results show that SENTINEL reduces object hallucinations by over 90% compared to previous state-of-the-art methods and consistently improves generalization performance across various benchmark tasks.  The framework is model-agnostic and computationally efficient, addressing limitations of existing methods. | ['Multimodal'] | [Link](https://github.com/pspdada/SENTINEL) | N/A |
| [The Generative Energy Arena (GEA): Incorporating Energy Awareness in
  Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302) | Pedro Reviriego, Javier Conde, Eneko Sendin, Gonzalo Mart√≠nez, Carlos Arriaga | - The paper introduces the Generative Energy Arena (GEA), a platform for evaluating large language models (LLMs) that incorporates information on energy consumption into the human evaluation process. - GEA presents energy consumption information to users after they have made their initial model choice based on response quality, mitigating bias toward smaller models. - Preliminary results demonstrate that users tend to favor smaller, more energy-efficient LLMs when energy consumption is considered, suggesting that energy awareness can influence decision-making in LLM evaluation. -  The study employs a two-step evaluation method to collect user preferences with and without energy information. - Further investigation is needed to examine energy consumption's effect across diverse LLM models, question types, and languages. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/) |
