

## Papers for 2025-07-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via
  Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683) | Yao Xiao, LidongBing, ZonglinY, binwang, veggiebird | - This paper introduces MiroMind-M1, a series of fully open-source reasoning language models (RLMs) that match or exceed the performance of existing open-source RLMs. - MiroMind-M1 models are trained using a two-stage process: supervised fine-tuning (SFT) on a curated corpus of 719K math-reasoning problems and reinforcement learning with verifiable reward (RLVR) on 62K challenging problems. - The RLVR process employs a novel Context-Aware Multi-Stage Policy Optimization (CAMPO) algorithm to enhance robustness and efficiency. -  MiroMind-M1 achieves state-of-the-art or competitive performance among Qwen-2.5-based open-source models on the AIME24, AIME25, and MATH benchmarks. - The complete stack, including models, datasets, and training configurations, is released to foster reproducibility and further research. | ['Reinforcement Learning', 'Natural Language Processing', 'Question Answering'] | [Link](https://github.com/MiroMindAsia/MiroMind-M1) | [Link](https://huggingface.co/miromind-ai/MiroMind-M1-RL-7B), [Link](https://huggingface.co/datasets/miromind-ai/MiroMind-M1-RL-62K) |
| [WebShaper: Agentically Data Synthesizing via Information-Seeking
  Formalization](https://arxiv.org/abs/2507.15061) | Baixuan Li, Junkai Zhang, Wenbiao Yin, Jialong Wu, Zhengwei Tao | - This paper introduces WebShaper, a novel framework for synthesizing high-quality training data for information-seeking agents. - WebShaper employs a formalization-driven approach, systematically formalizing information-seeking tasks using set-theoretic constructs, unlike existing information-driven methods. - Central to WebShaper is the concept of Knowledge Projections (KP), enabling precise control over reasoning structures. - The framework includes an agentic Expander module, which autonomously expands formal questions with retrieval and validation tools. - Experimental results demonstrate that WebShaper achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks. | ['Question Answering'] | [Link](https://github.com/Alibaba-NLP/WebAgent) | [Link](https://huggingface.co/datasets/Alibaba-NLP/WebShaper) |
| [SeC: Advancing Complex Video Object Segmentation via Progressive Concept
  Construction](https://arxiv.org/abs/2507.15852) | Jianfan Lin, Songxin He, Xiaoyi Dong, Shuangrui Ding, rookiexiong | - The paper introduces Segment Concept (SeC), a novel concept-driven framework for video object segmentation that leverages Large Vision-Language Models (LVLMs) to construct and utilize high-level, object-centric representations. - SeC shifts from conventional feature matching to the progressive construction and utilization of robust conceptual priors, leading to improved robustness against drastic visual variations, occlusions, and complex scene changes. - A new benchmark, Semantic Complex Scenarios Video Object Segmentation (SeCVOS), is introduced to rigorously evaluate VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding; SeCVOS comprises 160 manually annotated multi-scenario videos. - Empirical evaluations on SeCVOS and standard VOS benchmarks demonstrate that SeC substantially outperforms state-of-the-art approaches, achieving an 11.8-point improvement over SAM 2.1 on SeCVOS. - SeC employs a scene-adaptive activation strategy, dynamically adjusting computational efforts based on scene complexity, making it computationally efficient. | ['Video Classification', 'Image Segmentation', 'Multimodal'] | [Link](https://github.com/OpenIXCLab/SeC) | N/A |
| [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417) | Jacob Goldman-Wetzler, Andy Arditi, Runjin Chen, Alexander HÃ¤gele, Aryo Pradipta Gema | This paper reveals inverse scaling in large reasoning models (LRMs), where increasing reasoning length reduces accuracy.  They identify five distinct failure modes: distraction by irrelevant information, overfitting to problem framings, reliance on spurious correlations, difficulty maintaining focus on complex tasks, and amplification of concerning behaviors.  Evaluation spans simple counting, regression, and deduction tasks.  Findings highlight the importance of evaluating models across diverse reasoning lengths to mitigate these failure modes. | ['Natural Language Processing'] | [Link](https://github.com/safety-research/inverse-scaling-ttc) | [Link](https://huggingface.co/datasets/inverse-scaling-ttc/inverse-scaling-ttc-main) |
| [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for
  Spoken Language Models](https://arxiv.org/abs/2507.15375) | Kevin Lin, Chung-Ching Lin, Linjie Li, xiaofei-wang, dcml0714 | - This paper introduces STITCH, a novel method for spoken language model generation that alternates between generating unspoken reasoning chunks and spoken response chunks.  - STITCH addresses the issue of latency in spoken language models by using the remaining time between spoken response chunks to generate reasoning tokens, effectively enabling simultaneous thinking and talking. - Experimental results demonstrate that STITCH matches the latency of baseline models while outperforming them by 15% on math reasoning datasets and performing equally well on non-reasoning datasets. - The authors introduce two variants of STITCH: STITCH-R (reasoning first) and STITCH-S (speaking first), both of which exhibit improved performance over baselines. - The method is validated using several benchmark datasets, including math reasoning datasets and non-reasoning datasets, showing consistent improvements over baseline models. | ['Audio', 'Text-to-Speech'] | [Link](https://d223302.github.io/STITCH) | N/A |
