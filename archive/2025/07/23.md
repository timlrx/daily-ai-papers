

## Papers for 2025-07-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning](https://arxiv.org/abs/2507.16784) | Tina Li, Nathaniel Morgan, Hongyin Luo, thejackobrien, drkylj | - This paper introduces TIM, a novel family of large language models (LLMs) designed for recursive and decompositional problem-solving, and TIMRUN, an inference runtime that enables long-horizon reasoning. - TIM models the reasoning process as a tree structure measured by both length and depth, overcoming output limits and positional-embedding constraints of traditional LLMs that model language as linear sequences. - TIMRUN maintains a working memory that retains only key/value states of relevant context tokens, which enables reuse of positional embeddings and GPU memory pages throughout the reasoning process. - Experimental results show that TIMRUN sustains high inference throughput and delivers accurate reasoning on mathematical and information retrieval tasks that require long-horizon reasoning. - This approach allows for virtually unlimited working memory and multi-hop tool calls within a single language model inference, significantly improving efficiency and accuracy compared to traditional LLMs. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | [Link](https://github.com/subconscious-systems/TIMRUN) | N/A |
| [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632) | Chao Yan, Boyong Wu, Insects, SmailAA, petronny | - Step-Audio 2 is a new end-to-end multi-modal large language model designed for high-quality audio understanding and speech conversation.  It integrates a latent audio encoder and reasoning-centric reinforcement learning. - The model incorporates the generation of discrete audio tokens into language modeling, improving responsiveness to paralinguistic information (speaking style and emotion). - Step-Audio 2 utilizes retrieval-augmented generation (RAG) and external tools (web search and audio search) to mitigate hallucinations and improve audio search. - Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. - The model was trained on 680 billion tokens of text data and 8 million hours of real and synthesized audio data. | ['Audio', 'Automatic Speech Recognition', 'Text-to-Speech', 'Audio-to-Audio', 'Multimodal'] | [Link](https://github.com/stepfun-ai/Step-Audio2) | N/A |
| [MegaScience: Pushing the Frontiers of Post-Training Datasets for Science
  Reasoning](https://arxiv.org/abs/2507.16812) | Pengfei Liu, SinclairWang, Vfrz | - This paper introduces MegaScience, a post-training dataset designed to improve the scientific reasoning capabilities of large language models (LLMs). - MegaScience comprises 25 million high-quality question-answer pairs covering diverse scientific disciplines, exhibiting superior quality and scale compared to existing datasets. - The dataset's curation process involves meticulous steps such as textbook sourcing, question-answer pair extraction, refinement, and difficulty level assessment, along with comprehensive data decontamination to enhance reliability. - Empirical evaluations demonstrate significant performance gains on various scientific reasoning benchmarks when LLMs are fine-tuned using MegaScience, surpassing the performance of models trained on other existing datasets. - The authors publicly release MegaScience to foster future research in scientific reasoning and advance the development of more capable LLMs. | ['Question Answering'] | [Link](https://github.com/GAIR-NLP/MegaScience) | N/A |
| [Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking
  Reasoning](https://arxiv.org/abs/2507.16814) | Songyang Gao, Harold-lkk, vanilla1116, haitengzhao, shenjunhao | - This paper introduces SOPHIA, a novel semi-off-policy reinforcement learning framework designed to enhance vision-language slow-thinking reasoning. - SOPHIA combines an off-policy visual understanding module with a policy model for reasoning, effectively leveraging both visual and textual information. - The framework is evaluated on multiple benchmarks, consistently outperforming existing baselines and achieving state-of-the-art results on several challenging datasets. - Ablation studies demonstrate the effectiveness of various components within SOPHIA, including the semi-off-policy learning strategy and the reward design. - SOPHIA's superior performance highlights the potential of reinforcement learning techniques in improving the capabilities of large language models for complex visual reasoning tasks. | ['Reinforcement Learning', 'Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent
  Planning](https://arxiv.org/abs/2507.16815) | Fu-En Yang, Yu-Chiang Frank Wang, Yueh-Hua Wu, cmhungsteve, jasper0314-huang | - ThinkAct is a novel dual-system framework for vision-language-action reasoning that uses a multimodal large language model (LLM) for high-level reasoning and a downstream action model for low-level action execution. - The model architecture involves a multimodal LLM that generates embodied reasoning plans, which are compressed into a visual plan latent that conditions a downstream action model. - ThinkAct utilizes reinforcement learning with action-aligned visual rewards to guide the reasoning process, enabling few-shot adaptation, long-horizon planning, and self-correction. - Experimental results on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct outperforms existing methods in terms of success rate and generalization ability. - The model demonstrates few-shot adaptation and self-correction capabilities through structured reasoning and visual feedback. | ['Robotics', 'Reinforcement Learning', 'Multimodal'] | [Link](https://jasper0314-huang.github.io/thinkact-vla/) | N/A |
| [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.16746) | Zikui Cai, Kaiyu Yue, deqing, charleslwang, leonli66 | - This paper introduces ZEBRA-COT, a new large-scale dataset (182,384 samples) designed for interleaved vision-language reasoning. - The dataset focuses on four categories of tasks: scientific questions, 2D visual reasoning, 3D visual reasoning, and visual logic and strategic games. - Fine-tuning the Anole-7B model on ZEBRA-COT improves test-set accuracy by 12% and yields up to a 13% performance gain on standard VLM benchmark evaluations. - Fine-tuning Bagel-7B on ZEBRA-COT produces a model capable of generating high-quality interleaved visual reasoning chains. - The dataset and models are open-sourced to facilitate further research and evaluation of visual chain-of-thought reasoning. | ['Multimodal'] | [Link](https://github.com/multimodal-reasoning-lab/Bagel-Zebra-CoT) | N/A |
| [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement
  Feedback](https://arxiv.org/abs/2507.15024) | Hongyu Lin, Bowen Yu, Le Yu, Hao Xiang, Qiaoyu Tang | - RefCritic, a novel long chain-of-thought critic model, is proposed to enhance the critique abilities of LLMs. - RefCritic uses reinforcement learning with dual rule-based rewards focusing on instance-level correctness and refinement accuracies. - The model consistently outperforms existing methods across multiple benchmarks, achieving gains of up to 7.2% on AIME25 and 9.9% on Olympiad. - Under majority voting, policy models filtered by RefCritic demonstrate superior scaling with increased voting numbers. - RefCritic achieves strong performance on ProcessBench, surpassing step-level supervised approaches. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | N/A | N/A |
