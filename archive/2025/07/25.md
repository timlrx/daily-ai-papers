

## Papers for 2025-07-25

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958) | Jian Zhang, Yifei Li, Rongman Xu, Fangzhi Xu, Hang Yan |  - This paper introduces MUR, a novel method for improving the reasoning efficiency of Large Language Models (LLMs) without additional training. - MUR dynamically allocates computational resources to critical reasoning steps by tracking and aggregating step-wise uncertainty using a momentum-based approach. - The method introduces a y-control mechanism to flexibly control the reasoning budget and performance via a single hyperparameter. - Comprehensive evaluations across four challenging benchmarks demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%. - Theoretical analysis supports the superiority of MUR in terms of stability and convergence. | ['Natural Language Processing'] | [Link](https://github.com/yayayacc/MUR) | N/A |
| [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844) | Xingyu Wu, Linjuan Wu, tricktreat, yanyc, paradox122 | - This paper introduces Hierarchical Budget Policy Optimization (HBPO), a novel reinforcement learning framework that enhances the efficiency of large reasoning models without sacrificing accuracy. - HBPO addresses the challenge of exploration space collapse by partitioning rollout samples into multiple subgroups with distinct token budgets, enabling efficient resource allocation. - The framework introduces differentiated reward mechanisms that incentivize budget-aware behavior, allowing models to automatically adjust reasoning depth based on problem complexity. - Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. - Unlike existing methods that rely on external constraints or discrete mode selection, HBPO exhibits emergent adaptive behavior, allowing models to automatically adjust reasoning depth based on problem complexity. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/zju-real/hbpo) | N/A |
| [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013) | Yu Zhao, Chao Wang, Yitong Yao, Xinzhang Liu, Zihan Wang | This paper introduces three new large language models: TeleChat2, TeleChat2.5, and T1.  These models utilize enhanced training strategies, including a 10-trillion token pre-training phase and techniques like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).  TeleChat2.5 prioritizes speed, while T1 excels in complex reasoning tasks.  Benchmark results indicate that T1-115B outperforms existing models like OpenAI's 01-mini and GPT-40. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Tele-AI/TeleChat2), [Link](https://github.com/Tele-AI/TeleChat2.5), [Link](https://github.com/Tele-AI/T1) | [Link](https://modelscope.cn/models/TeleAI/TeleChat2-35B), [Link](https://modelscope.cn/models/TeleAI/TeleChat2-115B), [Link](https://modelscope.cn/models/TeleAI/TeleChat2.5-35B), [Link](https://modelscope.cn/models/TeleAI/TeleChat2.5-115B), [Link](https://modelscope.cn/models/TeleAI/T1-35B), [Link](https://modelscope.cn/models/TeleAI/T1-115B) |
| [A New Pair of GloVes](https://arxiv.org/abs/2507.18103) | Christopher D. Manning, John Bauer, Riley Carlson | - This paper introduces updated 2024 English GloVe word embedding models, addressing limitations of the 2014 models by incorporating newer data and improved vocabulary selection techniques. - The new models were trained using Wikipedia, Gigaword, and a subset of the Dolma corpus, demonstrating improved coverage of contemporary language. - Evaluations on word analogy and similarity tasks show comparable performance to the 2014 models, while NER evaluations highlight improved performance on recent, temporally dependent datasets. - The updated lexicon includes numerous new words and expressions reflecting current linguistic and cultural trends, enriching the semantic expressiveness of the embeddings. - Overall, these improved embeddings are valuable for low-resource settings, computationally efficient models, and interpretability-focused applications. | ['Natural Language Processing'] | N/A | N/A |
| [DMOSpeech 2: Reinforcement Learning for Duration Prediction in
  Metric-Optimized Speech Synthesis](https://arxiv.org/abs/2507.14988) | Kaifeng Xu, Cheng Niu, Fei Tao, Xilin Jiang, Yinghao Aaron Li | - DMOSpeech 2 is a novel model that extends metric optimization to the duration predictor in text-to-speech synthesis using a reinforcement learning approach. - It uses a novel duration policy framework with group relative preference optimization (GRPO), speaker similarity, and word error rate as reward signals. - The model introduces teacher-guided sampling, a hybrid approach that leverages a teacher model for initial denoising steps, improving output diversity and efficiency. - DMOSpeech 2 demonstrates superior performance across all metrics compared to previous systems, reducing sampling steps by half without quality degradation. - The audio samples, code, and pre-trained models are publicly available. | ['Text-to-Speech'] | [Link](https://dmospeech2.github.io/) | N/A |
| [GLiNER2: An Efficient Multi-Task Information Extraction System with
  Schema-Driven Interface](https://arxiv.org/abs/2507.18546) | Ash Lewis, George Hurn-Maloney, Oliver Boyd, Gil Pasternak, Urchade Zaratiana | - This paper introduces GLiNER2, a unified multi-task information extraction framework that improves upon the original GLINER architecture. - GLiNER2 is designed for CPU efficiency and supports named entity recognition, text classification, and hierarchical structured data extraction within a single model. - The model utilizes a schema-driven interface for intuitive task composition and is implemented as an open-source Python library. - Experimental results demonstrate competitive performance across various benchmarks, with substantial improvements in accessibility compared to LLM-based alternatives. - The authors highlight GLiNER2's enhanced efficiency and CPU performance as key advantages for practical applications, particularly in resource-constrained environments. | ['Natural Language Processing', 'Text Classification', 'Token Classification', 'Zero-Shot Classification', 'Feature Extraction'] | [Link](https://github.com/fastino-ai/GLiNER2) | [Link](https://huggingface.co/knowledgator/GLiClass) |
| [Agentar-Fin-R1: Enhancing Financial Intelligence through Domain
  Expertise, Training Efficiency, and Advanced Reasoning](https://arxiv.org/abs/2507.16802) | Zhaowen Zhou, Xiaoke Zhao, Longfei Liao, Xiyang Du, Yanjun Zheng | - Agentar-Fin-R1, a family of 8B and 32B parameter financial LLMs, is introduced, enhancing financial intelligence through domain expertise, training efficiency, and advanced reasoning. - The models are based on the Qwen3 foundation model and utilize a high-quality, systematic financial task label system with a multi-layered trustworthiness assurance framework. -  Agentar-Fin-R1 achieves state-of-the-art performance on mainstream financial benchmarks (Fineva, FinEval, and FinanceIQ) and general reasoning datasets (MATH-500 and GPQA-diamond). - A novel evaluation benchmark, Finova, is proposed to assess real-world deployment capabilities, focusing on agent-level financial reasoning and compliance verification. - Experimental results demonstrate that Agentar-Fin-R1 exhibits exceptional general reasoning capabilities and is a trustworthy solution for high-stakes financial applications. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/antgroup/Finova) | N/A |
