

## Papers for 2025-07-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane
  Algorithm](https://arxiv.org/abs/2507.18553) | Dan Alistarh, Torsten Hoefler, softmax | - This paper presents a novel geometric interpretation of the GPTQ algorithm for post-training quantization of large language models. - It establishes a mathematical equivalence between GPTQ and Babai's nearest plane algorithm for the closest vector problem (CVP) in lattice theory. - This equivalence provides a geometric interpretation of GPTQ's error propagation step and allows for the inheritance of Babai's algorithm's error bound under specific conditions. - The findings are used to develop a new quantization order heuristic, the "min-pivot" order, which aims to improve the accuracy of the algorithm. - The paper's theoretical contributions provide a firmer foundation for the understanding and improvement of post-training quantization methods for large language models. | ['Natural Language Processing'] | N/A | N/A |
| [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075) | Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yanfei Chen, Rujun Han | - This paper introduces Test-Time Diffusion Deep Researcher (TTD-DR), a novel framework that conceptualizes research report generation as a diffusion process, inspired by the iterative nature of human research. - TTD-DR enhances report quality through two mechanisms: (1) Report-Level Refinement via Denoising with Retrieval, and (2) Component-wise Optimization via Self-Evolution. - The framework significantly outperforms existing deep research agents on various benchmarks requiring intensive search and multi-hop reasoning, as evidenced by the superior results achieved in Table 1 (Win Rate, Correctness). - The model's performance is further enhanced by a Self-Evolutionary Algorithm, which is implemented in parallel, with several stages including Initialization, Feedback, Revision, and Crossover. - Despite its notable achievements, the current work primarily focuses on search tools and may not be applicable to other scenarios due to the lack of integration with browsing or coding tools.  | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation', 'Question Answering'] | N/A | N/A |
| [Specification Self-Correction: Mitigating In-Context Reward Hacking
  Through Test-Time Refinement](https://arxiv.org/abs/2507.18742) | vicgalle | - This paper introduces Specification Self-Correction (SSC), a novel test-time framework that allows language models to identify and correct flaws in their own guiding specifications. - SSC employs a multi-step inference process: initial response generation, self-critique, specification revision, and final response generation. - Experiments across creative writing and agentic coding tasks demonstrate that SSC reduces the vulnerability to reward hacking by over 90%. - The method requires no weight modification and operates at inference time, leading to more robustly aligned model behavior. - The authors empirically demonstrate the effectiveness of SSC across diverse domains and language models. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/vicgalle/specification-self-correction) | N/A |
