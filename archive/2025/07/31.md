

## Papers for 2025-07-31

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ScreenCoder: Advancing Visual-to-Code Generation for Front-End
  Automation via Modular Multimodal Agents](https://arxiv.org/abs/2507.22827) | Qunzhong Wang, Yuxuan Wan, Yaozhi Zheng, Yilei Jiang, csuhan | - This paper introduces ScreenCoder, a modular multi-agent framework for UI-to-code generation that addresses limitations of existing end-to-end methods by decomposing the task into grounding, planning, and generation stages. - The framework consists of three agents: a grounding agent (detects and labels UI components), a planning agent (constructs a hierarchical layout), and a generation agent (produces HTML/CSS code via adaptive prompt-based synthesis). - ScreenCoder achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness, outperforming existing methods on various evaluation metrics. - The framework also functions as a scalable data engine that automatically generates large-scale image-code pairs for training and improving vision-language models. - Extensive experiments demonstrate that ScreenCoder achieves significant improvements in both inference-time performance and the quality of generated code. | ['Multimodal', 'Image-to-Text', 'Text-to-Image', 'Text Generation'] | [Link](https://github.com/leigest519/ScreenCoder) | N/A |
| [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency
  and Performance](https://arxiv.org/abs/2507.22448) | Maksim Velikanov, Iheb-Chaabane, ifarhat1993, ybelkada, JingweiZuo | The paper introduces Falcon-H1, a new series of large language models featuring a parallel hybrid architecture combining Transformer attention with State Space Models (SSMs).  The models are released in various sizes (0.5B, 1.5B, 3B, 7B, and 34B parameters) and are optimized for high performance and efficiency.  Falcon-H1-34B-Instruct outperforms leading models up to 70B parameters on various benchmarks despite being approximately half the size and trained on a fraction of the data.  All Falcon-H1 models are open-source, supporting up to 256K tokens and 18 languages. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/tiiuae/falcon-h1) | [Link](https://huggingface.co/tiiuae) |
| [BANG: Dividing 3D Assets via Generative Exploded Dynamics](https://arxiv.org/abs/2507.21493) | Wei Yang, Yinuo Bai, Haoran Jiang, Qixuan Zhang, ZarkLngeW |  - This paper introduces BANG, a novel generative framework for dividing 3D assets into interpretable parts using Generative Exploded Dynamics.  The model uses a pre-trained latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter and temporal attention module, enabling smooth transitions and consistency.  - BANG incorporates spatial prompts (bounding boxes and surface regions) for precise control over the decomposition process.  Furthermore, it leverages multimodal models (like GPT-4) for intuitive 2D-to-3D manipulations.  - The framework enhances control over object decomposition, enabling users to specify which parts to decompose and how.  - Experimental results demonstrate the effectiveness of BANG in generating high-quality exploded views with smooth transitions, which outperforms existing methods by preserving semantic and geometric coherence.  - The authors show the application of BANG for 3D printing, part-level editing, interactive dialogues for part-level 3D analysis and creation, and 3D asset generation workflows. | ['Text-to-3D', 'Image-to-3D', 'Image Segmentation', 'Multimodal'] | N/A | N/A |
| [VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced
  Multimodal Reasoning](https://arxiv.org/abs/2507.22607) | Sicong Leng, Chenghao Xiao, Ruifeng Yuan, 26hzhang, kenchan0226 | - This paper introduces VL-Cogito, a multimodal large language model (MLLM) trained using a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. - PCuRL incorporates an online difficulty soft weighting mechanism and a dynamic length reward mechanism to improve the model's reasoning abilities and efficiency. - VL-Cogito outperforms existing reasoning-oriented models on various multimodal benchmarks, achieving state-of-the-art or competitive performance across mathematics, science, logic, and general understanding domains. - Ablation studies confirm the effectiveness of PCuRL's components, demonstrating that the progressive curriculum strategy and the dynamic reward mechanism contribute significantly to VL-Cogito's superior performance. - The paper provides extensive experimental results and detailed visualizations of the training process, validating the effectiveness and efficiency of the proposed approach. | ['Multimodal', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/alibaba-damo-academy/VL-Cogito) | N/A |
| [Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual
  Segmentation](https://arxiv.org/abs/2507.22886) | Yu-Gang Jiang, Guanquan Jie, Henghui Ding, Kaining Ying | - This paper introduces OmniAVS, a new benchmark dataset for omnimodal referring audio-visual segmentation, containing 2,104 videos and 61,095 multimodal referring expressions. - OmniAVS supports diverse multimodal referring expressions combining text, speech, sound, and images, emphasizing audio content understanding and complex reasoning. - The paper proposes Omnimodal Instructed Segmentation Assistant (OISA), a baseline model using a Multimodal Large Language Model (MLLM) for omnimodal referring audio-visual segmentation. - OISA incorporates Audio-Visual Interleaving for temporal alignment and query propagation for efficient segmentation, outperforming existing methods on OmniAVS and achieving competitive results on related tasks. - The experimental results demonstrate OISA's effectiveness on OmniAVS and other datasets, highlighting its capabilities in handling complex multimodal expressions and reasoning. | ['Image Segmentation', 'Multimodal', 'Audio Classification', 'Video Classification'] | N/A | N/A |
| [Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement
  Learning](https://arxiv.org/abs/2507.22565) | Gilbert Fridgen, Ramin Bahmani, Igor Tchappi, Amir Sartipi, akhadangi |  - The paper introduces RLDP, a novel framework for differentially private fine-tuning of LLMs using reinforcement learning.  - RLDP dynamically learns per-adapter gradient clipping thresholds and noise levels, outperforming seven strong baselines in terms of perplexity reduction (1.3-30.5%, mean 5.4%) and downstream utility gain (5.6%). - The framework achieves this by casting the DP optimization as a closed-loop control problem and using a soft actor-critic (SAC) hyper-policy. - RLDP also significantly reduces training steps (71% fewer on average) while maintaining the same privacy guarantees. - The approach has been validated on four model families (GPT2-small, Llama-3.2-1B/3B, and Mistral-7B) and is shown to be resistant to membership inference and canary extraction attacks. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/) | [Link](https://huggingface.co/) |
