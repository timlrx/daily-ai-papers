

## Papers for 2025-08-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Phi-Ground Tech Report: Advancing Perception in GUI Grounding](https://arxiv.org/abs/2507.23779) | Kai Qiu, Qi Dai, Jialiang Zhu, Ziqiang Xu, Miaosen Zhang | [- The paper introduces Phi-Ground, a family of models achieving state-of-the-art performance across five GUI grounding benchmarks for models under 10B parameters in agent settings. - The models adopt a two-stage implementation: a large language model generates detailed reference expressions, and a smaller model outputs coordinates. - The study explores various data augmentation techniques, coordinate representations, and loss functions, highlighting the impact of input order and computational cost. - Phi-Ground demonstrates strong generalization capabilities across diverse benchmarks and improved in-domain performance through post-training. - The authors also discuss the challenges and ethical considerations surrounding the use of Computer Use Agents (CUAs).] | ['Multimodal'] | [Link](https://zhangmiaosen2000.github.io/Phi-Ground/) | N/A |
| [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring
  Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968) | Yiwen Guo, Wei Tao, Chengqian Ma | - This paper introduces C3, a new bilingual benchmark dataset for evaluating spoken dialogue models (SDMs) focusing on complex conversational phenomena like ambiguity and context-dependency. - The dataset contains 1079 instances in English and Chinese, encompassing five key challenges: phonological ambiguity, semantic ambiguity, omission, coreference, and multi-turn interactions. - C3 employs an LLM-based evaluation method aligning well with human judgment, facilitating a comprehensive analysis of SDM performance. - The empirical study highlights the distinct difficulties posed by the five phenomena, along with language-specific challenges. - The findings reveal that SDMs exhibit significant performance variations across different tasks and languages, emphasizing the need for improved robustness and cross-lingual capabilities in SDM development. | ['Audio'] | N/A | N/A |
| [RecGPT Technical Report](https://arxiv.org/abs/2507.22879) | Jian Wu, Jiakai Tang, Gaoyang Guo, Dian Chen, Chao Yi |  - RecGPT is a novel framework for recommender systems that utilizes large language models (LLMs) to model user intent and generate personalized explanations. - RecGPT is deployed on Taobao's homepage, demonstrating consistent performance improvements for users, merchants, and the platform. Online experiments show significant gains across various metrics like Click Through Rate (CTR), Daily Click Active Users (DCAU), and user Dwell Time (DT). - The framework addresses the limitations of existing log-fitting approaches by placing user intent at the center of the recommendation pipeline. This involves integrating LLMs into stages like user interest mining, item retrieval, and explanation generation. - RecGPT employs a multi-stage training paradigm that integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. This ensures the effective alignment of general-purpose LLMs to domain-specific recommendation tasks. - The study validates the effectiveness of RecGPT through online A/B tests, human evaluation experiments, and case studies, demonstrating its ability to foster a more sustainable and mutually beneficial recommendation ecosystem. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation', 'Text2Text Generation', 'Other'] | N/A | N/A |
| [Persona Vectors: Monitoring and Controlling Character Traits in Language
  Models](https://arxiv.org/abs/2507.21509) | Jack Lindsey, Owain Evans, Henry Sleight, Andy Arditi, Runjin Chen | This paper introduces persona vectors, which are directions in a language model's activation space that correspond to specific personality traits.  These vectors can monitor personality changes during deployment and training.  A new automated pipeline extracts persona vectors from natural language descriptions.  Experiments show a strong correlation between persona vector shifts and personality changes, both intended and unintended.  A preventative steering method mitigates unwanted persona drift during finetuning. | ['Natural Language Processing'] | [Link](https://github.com/safety-research/persona_vectors) | N/A |
| [On the Expressiveness of Softmax Attention: A Recurrent Neural Network
  Perspective](https://arxiv.org/abs/2507.23632) | Eric C. Larson, Gabriel Mongaras | - This paper introduces a recurrent neural network (RNN) formulation of softmax attention, which helps explain its superior performance compared to linear attention. - The RNN formulation reveals that softmax attention is a structured process with interpretable, sequential dynamics, not merely a heuristic construction. - Linear attention is derived as a first-order approximation of softmax attention in the RNN framework. - Ablation studies demonstrate that higher-order terms in the Taylor expansion contribute to the expressiveness of softmax attention. - The paper shows that replacing the softmax denominator with a vector norm maintains performance while using a gate leads to slight performance degradation. | ['Natural Language Processing'] | [Link](https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective) | N/A |
| [TARS: MinMax Token-Adaptive Preference Strategy for Hallucination
  Reduction in MLLMs](https://arxiv.org/abs/2507.21584) | Jiasheng Tang, Chang Liu, Zhiming Luo, Keda Tao, Kejia Zhang | - This paper introduces TARS, a novel token-adaptive preference strategy to mitigate hallucinations in large multimodal language models (MLLMs). - TARS reformulates direct preference optimization (DPO) as a min-max optimization problem, maximizing token-level distributional shifts under semantic constraints and minimizing expected preference loss. - The proposed method outperforms existing DPO baselines and matches GPT-40 in hallucination reduction, achieving a 13.2% hallucination rate on the AMBER benchmark. - Experiments show that TARS effectively reduces hallucinations without sacrificing factual grounding and improves visual grounding. - The method is data-efficient, requiring only 4.8k preference samples without expert feedback. | ['Multimodal'] | [Link](https://kejiazhang-robust.github.io/tars_web) | N/A |
| [Enhanced Arabic Text Retrieval with Attentive Relevance Scoring](https://arxiv.org/abs/2507.23404) | Abdenour Hadid, Fadi Dornaika, Yazid Bounab, Azeddine Benlamoudi, Salah Eddine Bekhouche | - The paper introduces APR, a novel Arabic Dense Passage Retrieval (DPR) framework that incorporates Attentive Relevance Scoring (ARS) for enhanced semantic matching. - APR uses a dual-encoder architecture with a lightweight Arabic-specific encoder (MiniBERT) and integrates ARS to improve retrieval performance. - The ARS module replaces standard interaction mechanisms with an adaptive scoring function, which more effectively models semantic relevance between questions and passages. - Experimental results on the ArabicaQA dataset show that APR outperforms existing state-of-the-art methods, achieving absolute gains of +0.91% in Top-1, +4.77% in Top-10, and +1.53% in Top-100 accuracy. - The code is made publicly available on GitHub. | ['Question Answering'] | [Link](https://github.com/Bekhouche/APR) | N/A |
