

## Papers for 2025-08-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language
  Models](https://arxiv.org/abs/2508.00819) | Jiaqi Wang, Yuhang Cao, Yuhang Zang, Xiaoyi Dong, Jinsong Li | - This paper introduces DAEDAL, a novel training-free denoising strategy for Diffusion Large Language Models (DLLMs) that addresses the limitation of statically predefined generation lengths. - DAEDAL operates in two phases: 1) Initial Length Adjustment, where it iteratively expands the generation length guided by a sequence completion metric, and 2) Iterative Mask Insertion, where it dynamically expands insufficient generation regions. - Experiments on various benchmarks demonstrate that DAEDAL achieves performance comparable to, and in some cases superior to, meticulously tuned fixed-length baselines, while simultaneously improving computational efficiency. - By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging the gap with autoregressive counterparts. - The method dynamically adjusts the length based on the task's complexity, resulting in improved performance and computational efficiency compared to fixed-length baselines. | ['Text Generation'] | [Link](https://github.com/Li-Jinsong/DAEDAL) | N/A |
| [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265) | Zuxuan Wu, Chang Liu, Shuting He, Song Tang, Henghui Ding | This survey paper provides a comprehensive overview of multimodal referring segmentation across various visual scenes (images, videos, and 3D scenes) and modalities (text, audio, and multi-modal cues).  The authors introduce a unified meta-architecture for referring segmentation, followed by a detailed review of representative methods for each scene and modality.  The survey also covers more recent trends such as Generalized Referring Expression (GREx) methods and their applications. Finally, the authors analyze existing benchmarks and their limitations. | ['Image Segmentation', 'Video Classification', 'Multimodal'] | [Link](https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation) | N/A |
| [3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding](https://arxiv.org/abs/2507.23478) | Hao Tang, Zeyu Zhang, Ting Huang |  - 3D-R1 is a foundation model that enhances the reasoning capabilities of 3D Vision-Language Models (VLMs) by leveraging reinforcement learning and a high-quality synthetic dataset, Scene-30K.  - The model architecture employs a multi-modal approach, integrating text, multi-view images, 3D point clouds, and depth maps to perform comprehensive 3D tasks as autoregressive sequence prediction. - 3D-R1 incorporates three reward functions within a GRPO-based RLHF policy to enhance reasoning capabilities, including perception, semantic similarity, and format rewards. - Experiments demonstrate that 3D-R1 significantly outperforms existing methods on various 3D scene understanding benchmarks, achieving an average improvement of 10%. - A dynamic view selection strategy is introduced to improve model efficiency and reduce reliance on pre-defined viewpoints. | ['Multimodal', 'Text-to-3D', 'Image-to-3D', 'Visual Question Answering', 'Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/AIGeeksGroup/3D-R1) | N/A |
| [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348) | Heng Lian, Xiaodong Gu, Shaoxin Lin, Yuling Shi, Han Li | - This paper introduces SWE-Debate, a novel competitive multi-agent debate framework for resolving software issues. - SWE-Debate leverages fault propagation traces generated from code dependency graphs and structured three-round debates among specialized agents to achieve more consolidated issue localization. - Experimental results on the SWE-bench benchmark demonstrate that SWE-Debate outperforms existing baselines by a large margin, achieving state-of-the-art results in open-source agent frameworks. - Ablation studies highlight the significant contributions of multiple chain generation, multi-agent debate, and the MCTS-based code modification agent to the overall performance. - The proposed method addresses the limitations of existing agent-based approaches by incorporating diverse reasoning paths and structured competitive analysis. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering', 'Graph Machine Learning'] | [Link](https://github.com/YerbaPage/SWE-Debate) | N/A |
| [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454) | Chengfei Lv, Zhiwen Chen, Yunfeng Wang, Kehua Feng, Yuqi Tang | - This paper introduces MTDEval, a novel multi-turn dialogue evaluator that efficiently aggregates preference knowledge from multiple Large Language Model (LLM) judges into a single model. - The model architecture consists of a text-embedding model with specialized scoring heads, trained using a learning-to-rank strategy. - MTDEval outperforms existing baselines across various multi-turn dialogue evaluation benchmarks, demonstrating its effectiveness and robustness in single rating, pairwise comparison, and multi-dimensional comparison tasks. - The efficiency of MTDEval is highlighted by its significantly reduced computational cost during inference compared to traditional multi-judge methods. - A large-scale pairwise preference dataset (P2-MTD) and a high-quality human-annotated evaluation dataset (Daily-MTD) are constructed and released to facilitate future research. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/James-TYQ/MTDEval) | N/A |
| [Investigating Hallucination in Conversations for Low Resource Languages](https://arxiv.org/abs/2507.22720) | Fatemeh Jamshidi, Zheng Zhang, Souvika Sarkar, Md. Najib Hasan, Amit Das | - This paper investigates hallucination in conversational data across three low-resource languages (Hindi, Farsi, and Mandarin) using six different LLMs. - The main contribution is a comprehensive analysis of hallucination tendencies in these languages, considering both factual and linguistic errors. - The findings show significantly higher hallucination rates in Hindi and Farsi compared to Mandarin, highlighting the impact of data availability on LLM performance. - The study also analyzes the performance of different LLMs across these languages, showing that some models are more robust to hallucination than others. - The authors suggest several mitigation techniques for addressing hallucination, such as incorporating retrieval-augmented generation and using models specifically pretrained for the target languages. | ['Natural Language Processing', 'Text Generation', 'Translation'] | [Link](https://github.com/AmitDasRup123/LLM-Hallucination-Low-Resource-Languages/) | N/A |
| [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware
  Video Generation](https://arxiv.org/abs/2508.00782) | Long Chen, Qifeng Chen, Yazhou Xing, Yingqing He, Kien T. Pham | - SpA2V is a novel framework that leverages spatial auditory cues from audio to generate realistic videos with accurate semantic and spatial alignment. - The model uses a two-stage approach: Audio-guided Video Planning and Layout-grounded Video Generation. - Audio-guided Video Planning uses a Multimodal Large Language Model (MLLM) to construct Video Scene Layouts (VSLs) that capture spatial and semantic information from the audio. - Layout-grounded Video Generation uses a pre-trained diffusion model to generate videos based on the VSLs, achieving training-free video generation. - Experiments on a new benchmark, AVLBench, demonstrate that SpA2V outperforms state-of-the-art methods in generating videos with high semantic and spatial correspondence to input audios. | ['Audio-to-Audio', 'Text-to-Video', 'Audio Classification', 'Video Classification', 'Multimodal'] | [Link](https://github.com/tkpham3105/SpA2V) | N/A |
