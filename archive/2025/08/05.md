

## Papers for 2025-08-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324) | Kaiyuan Gao, Junyang Lin, Jingren Zhou, Jiahao Li, Chenfei Wu | The paper introduces Qwen-Image, a multimodal image generation foundation model that achieves significant improvements in complex text rendering and precise image editing.  The model utilizes a dual-encoding mechanism incorporating Qwen2.5-VL and a VAE encoder to balance semantic consistency and visual fidelity during editing.  Evaluated on multiple benchmarks, Qwen-Image demonstrates state-of-the-art performance, particularly excelling in text rendering tasks, especially in Chinese.  The model architecture employs an improved multi-task training paradigm and a curriculum learning strategy for enhanced capabilities. | ['Text-to-Image', 'Image-to-Image', 'Image-to-Text', 'Multimodal'] | [Link](https://github.com/QwenLM/Qwen-Image) | [Link](https://huggingface.co/Qwen/Qwen-Image) |
| [SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic
  Association and Long Story Comprehension](https://arxiv.org/abs/2508.01959) | Liyan Xu, Lemao Liu, Yuqing Li, Jiangnan Li, Junjie Wu | - This paper introduces SitEmb, a novel situated embedding model for improved context-aware dense retrieval, addressing limitations of existing methods in handling long documents and complex semantic associations. - The model incorporates a broader context window into short chunk embeddings, enhancing retrieval performance without straining model capacity. - SitEmb significantly outperforms state-of-the-art embedding models on a curated book-plot retrieval dataset, showcasing superior performance with fewer parameters. - The model demonstrates strong results across diverse downstream applications, including long story comprehension tasks and semantic association tasks. - A residual learning framework is used to enhance situated context usage, enabling the model to focus on additional contextual information beyond shallow heuristics. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/SituatedEmbedding) |
| [Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report](https://arxiv.org/abs/2508.01059) | Anu Vellore, Baturay Saglam, Blaine Nelson, Paul Kassianik, Sajana Weerawardhena | - This paper introduces Foundation-Sec-8B-Instruct, a cybersecurity-specialized large language model (LLM) built upon Llama 3.1. - The model undergoes instruction tuning and post-training on cybersecurity-focused datasets to enhance its cybersecurity knowledge and instruction-following capabilities. - Evaluation on various benchmarks demonstrates that Foundation-Sec-8B-Instruct achieves state-of-the-art performance on cybersecurity-specific tasks and competitive results on general post-training benchmarks. - The study analyzes the model's cybersecurity knowledge distribution across various topics and identifies potential biases and limitations, suggesting areas for further improvement. - A safety analysis is performed to evaluate the model's vulnerability to malicious prompts and toxic outputs, proposing safety mechanisms to mitigate risks. | ['Natural Language Processing', 'Text Classification', 'Text Generation'] | N/A | [Link](https://huggingface.co/Foundation-models/Sec-8B-Instruct) |
| [InstructVLA: Vision-Language-Action Instruction Tuning from
  Understanding to Manipulation](https://arxiv.org/abs/2507.17520) | Yang Tian, Bin Wang, Yilun Chen, Hao Li, Shuai Yang |  - InstructVLA is a novel end-to-end vision-language-action (VLA) model that integrates multimodal reasoning with precise action generation, bridging the gap between understanding and manipulation.  - It introduces Vision-Language-Action Instruction Tuning (VLA-IT), a new training paradigm that jointly optimizes textual reasoning and action generation using a Mixture-of-Experts adaptation strategy.  - On SimplerEnv tasks, InstructVLA outperforms existing methods such as SpatialVLA by 30.5% and achieves a 92% improvement over OpenVLA on SimplerEnv-Instruct.  - InstructVLA also demonstrates superior performance on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning.  - The model shows strong potential for bridging intuitive and steerable human-robot interaction with efficient policy learning. | ['Robotics', 'Multimodal', 'Reinforcement Learning'] | N/A | N/A |
