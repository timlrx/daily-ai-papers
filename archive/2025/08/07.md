

## Papers for 2025-08-07

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191) | Zhen Tan, Bohan, wjldw, ympc08, chengshuaizhao | - This paper introduces DATAALCHEMY, a controlled environment for training LLMs from scratch to investigate the effects of distributional shifts on chain-of-thought (CoT) reasoning. - The authors propose a data distribution lens for analyzing CoT reasoning, arguing that its effectiveness is fundamentally limited by the discrepancy between training and test data distributions. - Their findings reveal that CoT reasoning is a fragile phenomenon, easily breaking down under moderate distributional shifts. - The study examines CoT reasoning across three dimensions: task, length, and format, showing that its performance significantly degrades even with modest changes in these dimensions. - The work underscores the need to develop models with genuine and generalizable reasoning capabilities, moving beyond surface-level pattern matching. | ['Natural Language Processing'] | [Link](https://github.com/ChengshuaiZhao0/DataAlchemy) | [Link](None) |
| [LaTCoder: Converting Webpage Design to Code with Layout-as-Thought](https://arxiv.org/abs/2508.03560) | Tianpeng Lv, Guohao Wang, Zhongyi Zhang, Zhen Li, starmage520 | - The paper introduces LaTCoder, a novel approach for converting webpage designs into code that leverages Layout-as-Thought (LaT) to improve layout preservation during code generation. - LaTCoder divides the webpage design into image blocks, uses a chain-of-thought prompt to generate code for each block using an MLLM, and then assembles the code using either absolute positioning or MLLM-based assembly, dynamically choosing the better method. - LaTCoder outperforms existing design-to-code methods on both a public dataset (Design2Code-Hard) and a newly introduced dataset (CC-HARD) based on human preference evaluation and metrics. - The CC-HARD dataset is introduced as a more challenging dataset for evaluating design-to-code models, containing webpages with more complex layouts than existing datasets. - The study evaluates the effectiveness of LaTCoder using multiple backbone models (DeepSeek-VL2, Gemini, and GPT-4) on both datasets, demonstrating consistent improvement across all models. | ['Multimodal', 'Image-to-Text'] | [Link](https://github.com/CGCL-codes/naturalcc/tree/main/examples/latcoder) | [Link](https://huggingface.co/datasets/xcodemind/CC-HARD) |
| [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web
  Agents](https://arxiv.org/abs/2508.01858) | Xinyu Yang, Hongliang He, Aiwen Sun, Cong Guo, Gnonymous | - This paper introduces Web-CogReasoner, a novel multimodal large-scale model for web agents that leverages a knowledge-driven Chain-of-Thought (CoT) reasoning framework. - The model's architecture incorporates three knowledge domains: Factual, Conceptual, and Procedural, which are systematically instilled using the Web-CogDataset and Web-CogBench. - Extensive experiments demonstrate that Web-CogReasoner significantly outperforms existing state-of-the-art models on various benchmarks, particularly in generalization to unseen tasks. - The Web-CogDataset comprises 12 fine-grained tasks meticulously designed to incrementally build the agent's knowledge, cognition, and reasoning abilities. - The Web-CogBench is a novel evaluation suite that comprehensively assesses agent performance across delineated knowledge domains and cognitive capabilities. | ['Multimodal'] | [Link](https://github.com/Gnonymous/Web-CogReasoner) | N/A |
| [LeanK: Learnable K Cache Channel Pruning for Efficient Decoding](https://arxiv.org/abs/2508.02215) | Yuqing Yang, Chengruidong Zhang, Huiqiang Jiang, hzy46, zhangyik21 | - LeanK is a novel learning-based method for pruning unimportant key (K) cache channels in large language models (LLMs) to improve decoding efficiency.  - It employs a two-stage training process to learn a channel-wise static mask that satisfies specific sparsity ratios and hardware alignment requirements.  - Experiments show that LeanK achieves up to 70% K cache and 16%-18% V cache memory reduction, resulting in a 1.3x speedup for attention computation.  - The method maintains accuracy while reducing memory usage and enhancing decoding speed.  - LeanK is compatible with existing KV cache optimization techniques and can be combined for further acceleration. | ['Natural Language Processing'] | [Link](https://aka.ms/LeanK) | [Link](None) |
| [Sculptor: Empowering LLMs with Cognitive Agency via Active Context
  Management](https://arxiv.org/abs/2508.04664) | Yunxin Liu, Ting Cao, Qitai Tan, L. H. Xu, Mor-Li | - This paper introduces Sculptor, a novel framework that enhances LLMs by enabling them to actively manage their internal working memory. - Sculptor equips LLMs with tools for context fragmentation, summarization, hiding/restoring information, and intelligent search, allowing them to selectively focus on relevant information and filter out distractions. - Experimental evaluations on PI-LLM and NeedleBench Multi-Needle Reasoning benchmarks demonstrate that Sculptor significantly improves LLM performance on long-context tasks, even without specific training. - The key advantage of Sculptor lies in its ability to mitigate proactive interference, which is a significant challenge in processing long contexts, by enabling LLMs to selectively manage the context instead of simply increasing the context window. - Sculptor's active context management strategy provides a more cognitive approach to handling long contexts than simply enlarging the context window, leading to greater reliability and robustness at scale. | ['Natural Language Processing'] | N/A | N/A |
| [DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a
  Stage-Wise Diffusion Transformer Framework](https://arxiv.org/abs/2508.02807) | Chao Liang, Ente Lin, Shuliang Ning, Zaiyu Huang, Tongchun Zuo | - DreamVVT is a novel two-stage framework for high-fidelity video virtual try-on, leveraging diffusion transformers and addressing limitations of existing methods by utilizing unpaired data and pretrained models.  - The first stage generates high-fidelity try-on images for keyframes, using a multi-frame try-on model integrated with a vision-language model for semantic consistency.  - The second stage synthesizes a coherent try-on video using a pretrained video generation model enhanced with LoRA adapters, guided by keyframe try-on images, motion information, and textual descriptions.  - Experimental results on the ViViD and Wild-TryOn datasets demonstrate that DreamVVT surpasses existing methods in terms of garment detail preservation, temporal consistency, and generalization to unseen scenarios.  - Ablation studies confirm the effectiveness of the two-stage design and the use of LoRA adapters for efficient fine-tuning. | ['Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Enhancing Vision-Language Model Training with Reinforcement Learning in
  Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280) | Ruslan Rakhimov, Viacheslav Sinii, Stanislav Dereka, kefirski, GeorgeBredis | - This paper introduces Vision-Language Decoupled Actor-Critic (VL-DAC), a novel reinforcement learning algorithm for training vision-language models (VLMs). - VL-DAC decouples policy and value updates, resulting in faster, more stable training compared to previous methods like RL4VLM and LOOP. - The algorithm is evaluated across multiple lightweight simulators and is shown to transfer learned skills to real-world benchmarks with measurable improvements in agentic control, spatial planning, and web navigation. - Experiments demonstrate that VL-DAC achieves +50% relative improvement on BALROG, +5% on VSI-Bench, and +2% on VisualWebBench. - The authors contribute a simple, hyperparameter-free RL algorithm that enables effective transfer learning from synthetic environments to complex real-world tasks. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/corl-team/VL-DAC) | N/A |
| [A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding](https://arxiv.org/abs/2508.01197) | Jianke Zhu, Junbo Chen, Zhan Shi, songw-zju | - This paper introduces Talk2Occ, a novel benchmark dataset for 3D occupancy grounding in autonomous driving, and GroundingOcc, a new end-to-end model for this task. - GroundingOcc combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine, incorporating a multimodal encoder, an occupancy head, and a grounding head. - The model architecture includes a 2D grounding module and a depth estimation module to enhance geometric understanding, improving model performance. - Extensive experiments on Talk2Occ demonstrate that GroundingOcc outperforms existing baselines on 3D occupancy grounding, achieving a significant improvement in localization accuracy. - The dataset and code are publicly available on GitHub, enabling further research and development in this area. | ['Multimodal', 'Text-to-3D', 'Image-to-3D', 'Object Detection', 'Depth Estimation', 'Computer Vision'] | [Link](https://github.com/RONINGOD/GroundingOcc) | N/A |
| [Reasoning Language Models for Root Cause Analysis in 5G Wireless
  Networks](https://arxiv.org/abs/2507.21974) | Haozhe Zhang, Yibin Kang, Antonio De Domenico, Mohamed Sana, nicopi | - This paper introduces a novel lightweight framework for Root Cause Analysis (RCA) in 5G wireless networks using Large Language Models (LLMs). - A new curated dataset, TeleLogs, is introduced to benchmark RCA capabilities and evaluate the performance of LLMs in this task. - The authors propose a two-stage training methodology that leverages supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs for RCA. - The proposed approach significantly outperforms existing open-source reasoning LLMs on the TeleLogs dataset, achieving accuracy gains of up to 7x in some cases. - Extensive experiments demonstrate the effectiveness of the proposed method, highlighting the potential of domain-adapted reasoning-enhanced LLMs for practical and explainable RCA in network operation and management. | ['Natural Language Processing', 'Reinforcement Learning', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/netop/TeleLogs) |
| [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with
  Verifiable Rewards](https://arxiv.org/abs/2508.04632) | Ling-I Wu, Xiaogui Yang, Tong Jian, Tianyi Liang, Xu Guo |  - This paper introduces IFDecorator, a framework that enhances Reinforcement Learning with Verifiable Rewards (RLVR) for instruction following in large language models (LLMs).  - IFDecorator addresses two key limitations of RLVR4IF: training inefficiency due to inadequate difficulty assessment and over-optimization (reward hacking).  - It consists of three components: a cooperative-adversarial data flywheel, IntentCheck (a bypass module), and trip wires (a diagnostic mechanism).  - IFDecorator significantly improves instruction-following performance on multiple benchmarks, outperforming larger proprietary models while preserving general capabilities.  - The trip wires effectively reduce reward hacking rates, demonstrating the robustness of the framework. | ['Reinforcement Learning', 'Natural Language Processing', 'Text Generation'] | [Link](https://github.com/guox18/IFDecorator) | [Link](https://huggingface.co/datasets/guox18/IFDecorator) |
| [SonicMaster: Towards Controllable All-in-One Music Restoration and
  Mastering](https://arxiv.org/abs/2508.03448) | Ambuj Mehrish, Jan Melechovsky, dorienh | - This paper introduces SonicMaster, a unified generative model for music restoration and mastering that addresses various audio artifacts with text-based control. - The model architecture uses a flow-matching generative training paradigm and combines a VAE codec with multimodal and dual-stream DiT blocks to process audio and text inputs and produce high-fidelity enhanced outputs. - SonicMaster is trained on a large dataset of paired degraded and high-quality music tracks, created by simulating common degradation types with nineteen degradation functions. - Objective and subjective evaluations demonstrate that SonicMaster significantly improves sound quality across all artifact categories compared to baselines and that listeners prefer SonicMaster's outputs over original degraded audio. - The model, code, and dataset are available through the provided GitHub link. | ['Audio-to-Audio'] | [Link](https://amaai-lab.github.io/SonicMaster/) | N/A |
| [MiDashengLM: Efficient Audio Understanding with General Audio Captions](https://arxiv.org/abs/2508.03983) | Yadong Niu, Jian Luan, Jizhong Liu, Gang Li, Heinrich Dinkel | - MiDashengLM is a novel open audio-language model designed for efficient and comprehensive audio understanding using general audio captions. - The model integrates Dasheng, an open-source audio encoder, to process diverse auditory information effectively, unlike previous works which primarily focus on ASR-based audio-text alignment. - MiDashengLM is trained using publicly available datasets, ensuring full transparency and reproducibility. - It achieves a 4x speedup in time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. - Experiments show MiDashengLM outperforms baseline models on various benchmarks including audio captioning, question answering, and audio classification tasks. | ['Audio'] | [Link](https://github.com/xiaomi-research/dasheng-lm) | N/A |
| [Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and
  Self-Checking for Complex Instruction Following](https://arxiv.org/abs/2508.03178) | Liang Xu, Xiangzheng Zhang, Shousheng Jia, Liang Wen, Chenyang Wang | - The paper introduces Light-IF, a novel framework that enhances LLMs' generalizable reasoning abilities for complex instruction following through preview and self-checking mechanisms. - Light-IF addresses the issue of "lazy reasoning" in LLMs by employing an entropy-preserving supervised fine-tuning strategy coupled with a token-wise entropy-adaptive reinforcement learning approach. - The framework involves generating a dataset of complex instructions and uses rejection sampling to curate a high-quality subset for training. - Experimental results on various instruction-following benchmarks demonstrate that Light-IF significantly outperforms existing LLMs, achieving state-of-the-art performance across various model scales. - Light-IF-32B surpasses both large open-source models like DeepSeek-R1 and closed-source models like Doubao-1.6. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | [Link](https://huggingface.co/qihoo360/Light-IF-32B) |
