

## Papers for 2025-08-08

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning](https://arxiv.org/abs/2508.05405) | Ziming Wang, BÃ¶rje F. Karlsson, Ye Wang, Pi Bu, Xinrun Xu |  - DeepPHY is a novel benchmark framework designed to systematically evaluate Vision-Language Models' (VLMs) understanding of fundamental physical principles through challenging simulated environments.  - It integrates multiple physical reasoning environments of varying difficulty and incorporates fine-grained evaluation metrics, finding that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise control.  - DeepPHY systematically integrates six challenging physics-based simulation environments: PHYRE, I-PHYRE, Kinetix, Pooltool, Angry Birds, and Cut the Rope.  -  The benchmark utilizes a unified framework and standardized metrics to transform diverse physics simulators into a rigorous and accessible testbed, evaluating VLMs and collecting interaction data.  - DeepPHY reveals the boundaries and core shortcomings of current VLMs, highlighting their limitations in complex physical interaction, long-horizon planning, and dynamic adaptation. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | [Link](https://github.com/XinrunXu/DeepPHY) | N/A |
| [Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity](https://arxiv.org/abs/2508.05609) | Zhibing Li, Tong Wu, Ziyang Chu, Long Zhuo, Yuhan Zhang |  - Hi3DEval is a new hierarchical evaluation framework for 3D generation that assesses object-level and part-level quality, along with material evaluation via reflectance cues.  - It introduces a large-scale benchmark (Hi3DBench) with diverse 3D generative models and human-aligned annotations generated via a multi-agent, multi-modal pipeline.  - Hi3DEval uses a hybrid automated scoring system integrating video-based and naive 3D-based representations to enhance evaluations of 3D structure.  - Experiments demonstrate that Hi3DEval outperforms existing image-based metrics in modeling 3D characteristics and shows superior alignment with human preference.  - This framework provides a scalable alternative to manual evaluations and detailed diagnostic analysis capabilities. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | N/A | [Link](https://huggingface.co/datasets/anonymous-mY2nG5/H3DBench) |
| [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990) | Huan Liu, Chengshuai Zhao, Zhen Tan, Dawei Li, Bohan Jiang | - This paper introduces a novel large-scale dataset containing 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. - A principle-guided LLM-as-a-judge evaluation framework is proposed, employing dual judges to assess explanation quality, which aligns well with human evaluations. - Fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) significantly improves the quality of generated explanations. - The study reveals that explanation quality varies significantly across models, audiences, and categories, with DPO- and SFT-finetuned models outperforming their larger counterparts. - This research pioneers a large-scale, systematic investigation of LLMs' capabilities in explaining well-being concepts and provides insights into the strengths and weaknesses of current LLMs in this specific task. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Can Large Multimodal Models Actively Recognize Faulty Inputs? A
  Systematic Evaluation Framework of Their Input Scrutiny Ability](https://arxiv.org/abs/2508.04017) | Yuan Wu, Yi Chang, Gengxu Li, Jinzhe Li, Haiqi Yang | - This paper introduces ISEval, a novel framework for evaluating the input scrutiny ability of Large Multimodal Models (LMMs). - ISEval uses seven categories of flawed premises and three evaluation metrics to assess LMMs' ability to proactively identify and report errors in inputs. - The evaluation of ten advanced LMMs reveals that most struggle to autonomously detect flawed inputs, relying heavily on explicit prompts. - Error type significantly impacts performance, with models excelling at logical fallacies but struggling with surface-level linguistic errors. - Cross-modal inconsistencies reveal modality preferences, with most models favoring visual input when conflicts arise. | ['Multimodal'] | [Link](https://github.com/MLGroupJLU/LMM_ISEval) | N/A |
| [InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs
  to Enhance Reasoning Capabilities](https://arxiv.org/abs/2508.05496) | Zhijie Sang, Kejing Yang, Qi Zhou, Su Lu, Shuo Cai | - InfiAlign is a novel post-training framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) in a scalable and sample-efficient manner. - It integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) and a robust data selection pipeline that automatically curates high-quality alignment data using multi-dimensional quality metrics. - When evaluated on the Qwen-2.5-Math-7B-Base model, InfiAlign achieves performance on par with DeepSeek-R1-Distill-Qwen-7B while using only approximately 12% of the training data. - Further improvements are achieved through DPO, resulting in an average improvement of 3.89% on AIME 24/25 benchmarks. - The results highlight the effectiveness of combining principled data selection with a multi-stage post-training approach for aligning LLMs to enhance reasoning capabilities. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT) |
| [Evaluating, Synthesizing, and Enhancing for Customer Support
  Conversation](https://arxiv.org/abs/2508.04423) | Feng Chen, Lifan Guo, Junhui Li, Huaixia Dou, Jie Zhu | This paper introduces the task of Customer Support Conversation (CSC) and proposes a structured framework grounded in COPC guidelines, defining five conversational stages and twelve strategies.  A new evaluation dataset, CSConv (1,855 real-world conversations), and a training dataset, RoleCS (synthetic data using LLMs), are constructed. Experiments demonstrate that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses. Human evaluations further confirm gains in problem resolution.  The contribution of this work lies in the novel framework and datasets for a previously underexplored task in NLP. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/aliyun/qwen-dianjin) | N/A |
| [Don't Overthink It: A Survey of Efficient R1-style Large Reasoning
  Models](https://arxiv.org/abs/2508.02120) | Fangzhou Yao, Weibo Gao, Yizhi Wang, Yichao Du, Linan Yue |  - This paper surveys efficient reasoning methods for R1-style Large Reasoning Models (LRMs), categorizing existing works into single-model optimization and model collaboration.  -  The authors propose a novel taxonomy to organize existing efficient reasoning methods for R1-style LRMs. -  The survey covers various efficient reasoning techniques including early exit, CoT compression, adaptive reasoning, representation engineering, and model collaboration strategies.  -  The paper also discusses future research directions such as multimodal reasoning, tool-integrated reasoning, multi-agent systems and truthful and efficient reasoning. - A public GitHub repository is maintained to track the latest progress in efficient reasoning methods. | ['Natural Language Processing'] | [Link](https://github.com/yuelinan/Awesome-Efficient-R1-style-LRMS) | N/A |
| [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923) | Taiwei Shi, Jieyu Zhang, Viraj Prabhu, Yutong Dai, Linxin Song | - CoAct-1 is a novel multi-agent system that uses coding as an enhanced action for computer-using agents, combining GUI-based control with direct programmatic execution. - It features an Orchestrator that dynamically delegates subtasks to a GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. - This hybrid approach significantly outperforms prior methods on the OSWorld benchmark, achieving a new state-of-the-art success rate of 60.76% and reducing the average number of steps required to complete a task to 10.15. - The system's efficiency is attributed to its ability to strategically bypass inefficient GUI action sequences by using code for tasks like file management and data processing. - The results demonstrate the effectiveness and scalability of integrating coding as a core action for generalized computer automation. | ['Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/salesforce/CoAct-1) | N/A |
| [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038) | Qingjuan Li, Haoqin Sun, Xuanfan Ni, Chenyang Lyu, Fengping Tian | - This paper introduces Marco-Voice, a novel multifunctional speech synthesis system that integrates voice cloning and emotion control within a unified framework. - The model architecture employs a speaker-emotion disentanglement mechanism with in-batch contrastive learning and a rotational emotion embedding integration method for smooth emotion control. - Marco-Voice achieves substantial improvements in both objective and subjective metrics compared to existing systems, showcasing competitive performance in terms of speech clarity and emotional richness. - The authors contribute CSEMOTIONS, a high-quality Mandarin emotional speech dataset with 10 hours of speech from six professional speakers across seven emotional categories, supporting comprehensive training and evaluation. - The code and dataset are publicly available, furthering research and development in the field of expressive neural speech synthesis. | ['Text-to-Speech'] | [Link](https://github.com/AIDC-AI/Marco-Voice) | [Link](https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS) |
| [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during
  Multi-Hop Analysis](https://arxiv.org/abs/2508.04699) | Reshmi Ghosh, Yashwanth Babu, Srujana Pillarichety, Isha Nalawade, Anushka Yadav | - This paper introduces a novel diagnostic framework for analyzing reasoning failures in multi-hop question answering models. - The framework categorizes errors across three dimensions: hops, coverage, and overthinking, providing a nuanced understanding of model limitations. - The authors manually annotated model traces from six language models across three datasets, revealing common reasoning issues such as incomplete reasoning and unnecessary steps. - An LLM-as-a-Judge framework was developed to scale the analysis, demonstrating strong agreement with human annotations. - Findings highlight the prevalence of overthinking and its negative impact on answer accuracy, especially in complex datasets. | ['Question Answering'] | N/A | N/A |
| [PRvL: Quantifying the Capabilities and Risks of Large Language Models
  for PII Redaction](https://arxiv.org/abs/2508.05545) | Prajit Das, Lavanya Elluri, Aritran Piplai, Anantaa Kotal, Leon Garza | - This paper introduces PRvL, an open-source suite of fine-tuned models and evaluation tools for general-purpose PII redaction, built on open-source LLMs and supporting multiple inference settings. - PRvL addresses the limitations of rule-based and NER-based methods by leveraging the contextual understanding capabilities of LLMs for accurate and privacy-aware PII redaction across domains and languages. - The research evaluates a range of LLM architectures and training strategies, including fine-tuning, instruction-tuning, and retrieval-augmented generation, measuring redaction performance, semantic preservation, and PII leakage. - The empirical results provide practical guidance for configuring LLM-based redactors and demonstrate the effectiveness of instruction-tuning for high-accuracy, generalizable PII redaction. - The open-source nature of PRvL, along with its flexibility and compliance features, enables data owners to perform redactions within their own secure environments without relying on third-party services. | ['Natural Language Processing', 'Token Classification', 'Text2Text Generation'] | [Link](https://anonymous.4open.science/r/PRvL-C1BF) | [Link](https://huggingface.co/datasets/ai4privacy/pii-masking-300k) |
| [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating
  Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939) | Chirag Shah, Aman Chadha, Tanya Roosta, Julia Kharchenko |  - This paper introduces a new benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths, which are subtle linguistic markers that can reveal demographic attributes.  - The benchmark uses 100 validated question-response pairs in simulated interview scenarios to measure how LLMs systematically penalize certain linguistic patterns.  -  The paper demonstrates that hedged responses receive significantly lower ratings than confident responses despite equivalent content quality.  - The benchmark's effectiveness is validated by identifying model-specific biases in various LLMs.  - The proposed framework provides a foundation for detecting and measuring linguistic discrimination in AI systems. | ['Natural Language Processing'] | N/A | N/A |
| [I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal
  Entity Linking](https://arxiv.org/abs/2508.02243) | Chao Wang, Tong Ruan, Kaiwen Li, Junwen Li, Ziyan Liu | - This paper introduces I2CR, a novel framework for multimodal entity linking that prioritizes text information and uses visual clues iteratively when necessary. - The I2CR framework employs a multi-round iterative strategy, integrating key visual features from various aspects of the image to support reasoning and enhance accuracy. - Experiments on three widely used public datasets demonstrate that I2CR outperforms existing state-of-the-art methods, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. - The framework addresses the challenges of unnecessary image data incorporation and reliance on one-time visual feature extraction present in previous methods. - The core of the framework is a fine-tuned Large Language Model that makes initial entity selections, and then employs intra- and inter-modal consistency checks and visual feedback loops before returning a final result. | ['Multimodal'] | [Link](https://github.com/ziyan-xiaoyu/I2CR/) | N/A |
