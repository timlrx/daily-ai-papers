

## Papers for 2025-08-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471) | GLM-4. 5 Team, zixuanlimit, ZAHNGYUXUAN, LiquidAmmonia, Stanislas |  - This paper introduces GLM-4.5, a 355B parameter Mixture-of-Experts (MoE) large language model with a hybrid reasoning method.  - GLM-4.5 achieves strong performance on agentic, reasoning, and coding benchmarks, outperforming several competitors with fewer parameters.  - The model is trained using a multi-stage approach including pre-training, mid-training, and post-training with expert model iteration and reinforcement learning.  - A smaller version, GLM-4.5-Air (106B parameters), is also released to facilitate research.  - Both models are open-sourced along with evaluation tools. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | [Link](https://github.com/zai-org/GLM-4.5) | [Link](https://huggingface.co/zai-org/GLM-4.5) |
| [Pruning the Unsurprising: Efficient Code Reasoning via First-Token
  Surprisal](https://arxiv.org/abs/2508.05988) | Chengcheng Wan, Chao Hu, Yaoning Wang, Wenhao Zeng, YerbaPage | - This paper introduces ASAP, a novel framework for compressing Chain-of-Thought (CoT) reasoning in large language models (LLMs) by using an anchor-guided pruning method combined with a surprisal-based refining method.  - The ASAP framework first prunes redundant parts of the CoT using an anchor-guided approach and then refines the pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric.  - Experimental results demonstrate that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs.  - On the LiveCodeBench v4_v5 benchmark, ASAP reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline while achieving competitive accuracy.  - The authors also conduct ablation studies to validate the contribution of each component in ASAP and show the effectiveness of the two-stage pruning method. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/Zengwh02/ASAP) | N/A |
| [MELLA: Bridging Linguistic Capability and Cultural Groundedness for
  Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502) | Guohang Yan, Ruirui Chen, Nuo Chen, Jiaying Fei, Yufei Gao | - This paper introduces MELLA, a novel multimodal multilingual dataset designed to improve the performance of Multimodal Large Language Models (MLLMs) in low-resource languages. - MELLA addresses the dual objectives of enhancing linguistic capabilities and cultural groundedness by employing a dual-source data strategy. - The dataset consists of 6.82 million image-text pairs across eight low-resource languages, sourced from native web alt-text and MLLM-generated captions. - Experiments demonstrate that fine-tuning on MELLA leads to significant performance improvements across various MLLM backbones, showcasing the effectiveness of the dual-objective and dual-source strategy. - The authors also provide extensive qualitative analysis to further illustrate MELLA's improvement in bridging the linguistic capability and cultural groundedness gap. | ['Multimodal', 'Image-to-Text'] | N/A | N/A |
