

## Papers for 2025-08-12

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050) | Yuchen Li, Yutao Zhu, Weiwei Sun, Xinyu Ma, Wenhan Liu |  - ReasonRank is a novel reasoning-intensive passage reranker that significantly improves the performance of passage ranking tasks. - It employs a two-stage training framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance reasoning ability. - ReasonRank uses a multi-view ranking reward that considers both single-turn and multi-turn ranking rewards for effective training. - Experimental results on the BRIGHT and R2MED benchmarks show that ReasonRank outperforms existing state-of-the-art methods. - Ablation studies demonstrate the effectiveness of each component of the proposed framework. | ['Natural Language Processing', 'Question Answering'] | [Link](https://github.com/8421BCD/ReasonRank) | N/A |
| [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings
  and Speaks in Tokens](https://arxiv.org/abs/2508.05305) | Anton Razzhigaev, Andrey Kuznetsov, Elizaveta Goncharova, Temurbek Rahmatullaev, Nikita Dragunov | - This paper introduces SONAR-LLM, a decoder-only transformer that generates text by predicting a sequence of sentence embeddings and using a token-level cross-entropy objective propagated through a frozen SONAR decoder. - The model architecture combines the semantic abstraction of Large Concept Models (LCMs) with the stability of likelihood-based training, eliminating the need for diffusion samplers. - SONAR-LLM achieves competitive generation quality across different model sizes (39M to 1.3B parameters) and exhibits strong scaling properties. - Experimental results on summarization tasks (XSum and CNN/DM) demonstrate that SONAR-LLM matches or exceeds the performance of other sentence-level approaches. - The model demonstrates superior inference efficiency on long sequences compared to standard LLMs, resulting from its operation on compressed sentence embeddings. | ['Text Generation'] | [Link](https://github.com/FusionBrainLab/SONAR-LLM/tree/main) | N/A |
| [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm
  Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407) | Xinhao Yi, Yingxu Wang, Xi Zhang, Yanwen Peng, Jinyuan Fang |  - This paper introduces a novel conceptual framework for understanding and comparing self-evolving AI agents, which highlights four key components: System inputs, Agent System, Environment, and Optimizers.  - It provides a systematic review of existing techniques for self-evolving agentic systems, focusing on foundation models, agent prompts, memory, tools, workflows, and communication mechanisms.  - The paper also investigates domain-specific evolution strategies developed for specialized fields such as biomedicine, programming, and finance.  - A dedicated discussion on evaluation, safety, and ethical considerations for self-evolving agentic systems is included.  - The authors propose a set of guiding principles for safe and effective self-evolution of AI agents, inspired by Isaac Asimov's Three Laws of Robotics. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents) | N/A |
| [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785) | Tieyuan Chen, Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Haoyuan Wu | - This paper introduces Grove MoE, a novel Mixture-of-Experts (MoE) architecture for large language models (LLMs) that uses experts of varying sizes and a dynamic activation mechanism. - The Grove MoE architecture is inspired by the big.LITTLE CPU architecture and features adjugate experts that are shared among groups of experts, improving computational efficiency. - GroveMoE-Base and GroveMoE-Inst are two 33B-parameter LLMs built using the Grove MoE architecture by applying an upcycling strategy to the Qwen-30B-A3B-Base model. - Experiments show that GroveMoE models achieve performance comparable to state-of-the-art open-source LLMs of similar or even larger scales on various benchmarks. - The authors discuss the limitations of the current Grove MoE architecture, including the scarcity of long-CoT data and the exclusive reliance on rejection sampling, and suggest future research directions. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/inclusionAI/GroveMoE) | N/A |
| [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via
  Past-Future](https://arxiv.org/abs/2508.06026) | Qiufeng Wang, Junfeng Fang, Cunxiang Wang, Xin Wang, Yidong Wang | - This paper introduces Temporal Self-Rewarding Language Models, a novel method to address the diminishing preference signals in existing self-rewarding language models. - The core idea is to decouple the chosen and rejected responses by using past model outputs for rejected responses and next-generation model predictions for chosen responses, thus maintaining a clear quality gap. - The proposed two-phase framework (Anchored Rejection and Future-Guided Chosen) is evaluated on three model families (Llama, Qwen, Mistral) and various model sizes. - Experimental results across multiple benchmarks (AlpacaEval 2.0, Arena-Hard-v0.1, MT-Bench) demonstrate significant improvements over existing self-rewarding methods, achieving higher win rates and better scores, even with fewer iterations. - The approach also shows superior out-of-distribution generalization across mathematical reasoning, knowledge-based QA, and code generation tasks. | ['Natural Language Processing', 'Text Generation', 'Reinforcement Learning'] | N/A | N/A |
| [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189) | Qingwei Meng, Kevin Qinghong Lin, Joya Chen, Chen Gao, Weijia Wu | This survey paper provides a comprehensive overview of recent advances in reinforcement learning (RL) applied to vision.  It categorizes over 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models.  The survey examines algorithmic design, reward engineering, and benchmark progress in each pillar.   Key challenges and promising future directions in visual RL are identified, including sample efficiency, generalization, and safe deployment.  Finally, the paper offers a structured overview of visual reinforcement learning to support future research and practical deployment. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning) | N/A |
| [Less Is More: Training-Free Sparse Attention with Global Locality for
  Efficient Reasoning](https://arxiv.org/abs/2508.07101) | Baihong Yuan, Shijie Cao, Arti Jain, Zhihao Zhang, Lijie Yang | - LessIsMore is a novel training-free sparse attention mechanism that improves efficiency and accuracy in reasoning tasks by leveraging global attention patterns and recency locality. - It addresses the limitations of existing sparse attention methods that suffer from accuracy degradation due to accumulated errors during long-generation reasoning by unifying token selection across attention heads. - The model aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. - Evaluations across diverse reasoning tasks and benchmarks show that LessIsMore achieves a 1.1× average decoding speed-up compared to full attention, attends to 2× fewer tokens without accuracy loss and achieves a 1.13× end-to-end speed-up compared to existing sparse attention methods. - LessIsMore consistently outperforms existing sparse attention methods on challenging reasoning benchmarks while maintaining or even improving accuracy. | ['Natural Language Processing', 'Question Answering', 'Text Generation'] | [Link](https://github.com/DerrickYLJ/LessIsMore) | N/A |
| [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation
  for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493) | Tong Yu, Chenguang Wang, Jihyung Kil, Ming Li, Jian Chen | - This paper introduces VisR-Bench, a multilingual benchmark for question-driven multimodal retrieval in long documents, containing over 35K QA pairs across 1.2K documents spanning sixteen languages. - VisR-Bench enables fine-grained evaluation of multimodal retrieval with three question types (figures, text, and tables), and queries without explicit answers to prevent superficial keyword matching. - The evaluation of various retrieval models, including text-based methods, multimodal encoders, and LLMs, shows that while LLMs significantly outperform other models, they still struggle with structured tables and low-resource languages. - VisR-Bench addresses limitations of existing benchmarks by focusing on QA relevance rather than text-image similarity and including multi-page multilingual documents. - This work highlights key challenges in multilingual visual retrieval and provides insights for improving LLMs. | ['Document Question Answering', 'Multimodal', 'Visual Question Answering', 'Table Question Answering'] | [Link](https://github.com/puar-playground/VisR-Bench) | N/A |
| [MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://arxiv.org/abs/2508.05257) | Jianguo Li, Jing Zhang, Zhenzhong Lan, Mingming Ha, Xiaodong Chen | - This paper introduces MoBE, a novel Mixture-of-Basis-Experts method for compressing MoE-based LLMs. - MoBE factorizes each expert's weight matrix using rank decomposition (W = AB), where matrix A is unique to each expert and matrix B is shared across experts as a linear combination of basis matrices. - Experiments show that MoBE achieves significantly lower accuracy drops compared to previous methods, reducing parameter counts by 24%-30% with only 1%-2% accuracy drop. - MoBE outperforms existing MoE compression methods (MoLAE and D2-MoE) across various benchmarks, demonstrating its effectiveness in compressing large MoE-based LLMs. - The code is open-sourced to encourage further research in efficient MoE architectures. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/inclusionAI/MOBE) | N/A |
| [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662) | Alexander Yavorskyi, Oleksandr Lukashov, Dmytro Vodianytskyi, Mykhailo Shtopko, Ihor Stepanov | - This paper introduces GLiClass, a novel sequence classification model based on the GLiNER uni-encoder architecture, designed for efficient and accurate text classification. - GLiClass addresses the limitations of existing methods by combining the accuracy of advanced architectures with the efficiency of embedding-based methods, achieving comparable or superior performance to cross-encoder baselines. - The model is designed to perform multi-label classification in a single forward pass and achieve non-linear scaling with the number of classes, enabling efficient handling of multiple categories and large-scale applications. - GLiClass utilizes proximal policy optimization (PPO) for multi-label text classification, allowing training in data-sparse conditions or with human feedback. - The experimental results demonstrate that GLiClass achieves state-of-the-art results on standard text classification benchmarks, outperforming strong cross-encoder baselines in terms of both accuracy and efficiency. | ['Text Classification'] | [Link](https://github.com/Knowledgator/GLiClass) | [Link](https://huggingface.co/collections/knowledgator/gliclass-v3-687a2d211b89659da1e3f34a) |
| [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
  Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601) | Robert Kirk, Tomek Korbak, Quentin Anthony, Stephen Casper, Kyle O'Brien | - This paper introduces a multi-stage data filtering pipeline for large language models (LLMs) to enhance their tamper resistance and reduce vulnerabilities related to proxy knowledge.- The pipeline involves filtering by keywords, using classifiers, and combining these techniques to mitigate different types of attacks.- Experiments show that the filtering approach achieves state-of-the-art tamper resistance, defending against fine-tuning attacks up to 10k steps and 500M tokens, and latent-space attacks.- The study also explores challenges with synthetic document training and introduces techniques to address these challenges, further improving the robustness of the LLMs.-The approach improves robustness to latent space attacks and other adversarial attacks, which makes the model more resilient, ultimately providing a significant step towards building more secure and reliable LLMs. | ['Natural Language Processing'] | N/A | N/A |
| [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059) | Reynold Cheng, Dacheng Wen, Bin Benjamin Zhu, Yupeng Li, Haorui He | - This paper introduces FACT2FICTION, the first poisoning attack framework targeting agentic fact-checking systems. - FACT2FICTION mirrors the decomposition strategy of agentic systems and leverages system-generated justifications to craft targeted malicious evidence that compromises sub-claim verification. - Experiments show that FACT2FICTION achieves 8.9%-21.2% higher attack success rates than state-of-the-art attacks across various poisoning budgets. - FACT2FICTION exposes security vulnerabilities in current fact-checking systems and highlights the need for defensive countermeasures. - The framework consists of two LLM-based agents: a Planner and an Executor, which collaboratively create and inject malicious evidence. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with
  Benign Inputs](https://arxiv.org/abs/2508.03365) | Dasol Choi, Taeyoun Kwon, Hiskias Dingeto, Bodam Kim, oneonlee | - This paper introduces WHISPERINJECT, a novel two-stage framework for launching adversarial attacks against audio-language models (ALMs). - Stage 1, Native Target Discovery, uses reinforcement learning with projected gradient descent (RL-PGD) to identify model-native harmful responses, which are then used as targets for Stage 2. - Stage 2, Payload Injection, employs projected gradient descent (PGD) to embed these payloads into benign audio carriers, making the attacks stealthy and effective. - Experiments demonstrate a success rate exceeding 86% across various state-of-the-art ALMs, including Qwen2.5-Omni, and Phi-4-Multimodal. - The work highlights a new class of practical, audio-native threats and emphasizes the need for more robust safety mechanisms in ALMs. | ['Audio', 'Audio Classification', 'Reinforcement Learning'] | [Link](https://github.com/AIM-Intelligence/WhisperInject) | N/A |
| [Compressing Chain-of-Thought in LLMs via Step Entropy](https://arxiv.org/abs/2508.03346) | Zhijian Xu, Xiangyu Wen, Ziyang Zheng, Jianyuan Zhong, Zeju Li | - This paper introduces a novel Chain-of-Thought (CoT) compression framework for Large Language Models (LLMs) based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps. - The proposed method identifies and prunes redundant steps with low entropy, achieving significant compression with minimal accuracy loss. Experiments show up to 80% of low-entropy steps can be pruned across multiple LLMs and benchmarks. - A two-stage training strategy, combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO), is proposed to enable LLMs to autonomously generate compressed CoTs. - The results demonstrate that LLMs can learn to generate compressed CoTs, further improving inference efficiency without significant accuracy degradation. - The findings offer profound implications for practical LLM deployment and a deeper understanding of reasoning structures. | ['Natural Language Processing'] | [Link](https://github.com/staymylove/COT_Compresstion_via_Step_entropy) | N/A |
| [Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations
  and Sentences](https://arxiv.org/abs/2508.03542) | Matvey Skripkin, Elvir Karimov, Artyom Iudin, Dmitrii Tarasov, Dmitrii Korzh | - This paper introduces a novel large-scale, open-source dataset (S2L) for spoken mathematical expressions and sentences, consisting of approximately 66,000 human-annotated and 571,000 synthetic audio samples. - It proposes several speech-to-LaTeX (S2L) methods, combining state-of-the-art ASR models with post-processing via fine-tuned language models (LMs) and end-to-end approaches based on Audio-LLMs. - The best models achieve an equation character error rate (CER) between 27.7% and 30.0% on English data, and a text CER up to 9.6% on mathematical sentences. - The results show that the proposed models outperform the existing MathSpeech model by a substantial margin, particularly on a newly proposed S2L-equations benchmark (27% vs. 64%). - This work establishes the first benchmark for mathematical sentence recognition and lays the groundwork for future advances in multimodal AI focused on mathematical content recognition. | ['Automatic Speech Recognition', 'Text2Text Generation', 'Multimodal'] | N/A | [Link](https://hf.co/datasets/marsianin500/Speech2Latex) |
