

## Papers for 2025-08-13

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent](https://arxiv.org/abs/2508.05748) | zhaoyd, callanwu, zhzhen23, richardxp888, Ornamentt | - This paper introduces WebWatcher, a novel multimodal deep research agent designed for complex information-seeking tasks that leverage both visual and textual data. - WebWatcher is equipped with enhanced visual-language reasoning capabilities and utilizes various tools for deep reasoning, including web search, image search, and code execution. - The model is trained using high-quality synthetic multimodal trajectories for efficient cold-start training and further enhanced via reinforcement learning. - To evaluate WebWatcher, the authors propose BrowseComp-VL, a challenging benchmark requiring complex information retrieval involving visual and textual data. - Experimental results on four benchmarks demonstrate that WebWatcher significantly outperforms existing baselines, including RAG workflow and open-source agents. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/Alibaba-NLP/WebAgent) | N/A |
| [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086) | Yuqi Li, Wenhang Ge, Zhongqi Yang, kangfei, dearamy | - The paper introduces Matrix-3D, a novel framework for generating omnidirectional explorable 3D worlds from single images or text prompts.  It leverages a trajectory-guided panoramic video diffusion model and a panoramic 3D reconstruction module for high-quality and wide-coverage 3D world generation. - Matrix-3D uses a two-stage training strategy for 3D reconstruction: an optimization-based pipeline for detailed reconstruction and a feed-forward method for faster generation. The model integrates scene mesh renders as condition for trajectory guidance to address the issue of geometric inconsistencies in existing approaches. - The Matrix-Pano dataset, a large-scale synthetic dataset with 116K high-quality static panoramic video sequences and depth and trajectory annotations is introduced to facilitate effective training and evaluation.  - Extensive experiments demonstrate that Matrix-3D outperforms state-of-the-art methods in panoramic video generation and 3D world reconstruction in terms of visual quality, camera controllability, and reconstruction speed, according to quantitative and qualitative evaluations. - The paper also conducts ablation studies to demonstrate the effectiveness of specific design choices, such as using scene mesh renders, and the two-stage training strategy. | ['Image-to-3D', 'Text-to-3D', 'Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://matrix-3d.github.io) | N/A |
| [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale
  Asynchronous RL](https://arxiv.org/abs/2508.07976) | Chuyi He, Shusheng Xu, Minyang Xie, Wei Fu, Jiaxuan Gao | - This paper introduces ASearcher, an open-source project for large-scale reinforcement learning of search agents that achieves expert-level search intelligence. - ASearcher uses a fully asynchronous RL training method to enable long-horizon search (over 40 turns and 150k output tokens), surpassing existing open-source 32B agents. - A key contribution is a prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset for training. - ASearcher achieves substantial improvements on various benchmarks (46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively) compared to existing methods. - The agent design is simple, using only a search engine and a web browser as tools, without relying on any external LLMs. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/inclusionAI/ASearcher) | N/A |
| [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409) | Fei Shen, Yanhong Zeng, Wenran Liu, LiJiaxing, Gaojunyao | - CharacterShot is a novel framework for controllable and consistent 4D character animation, generating dynamic 3D characters from a single reference image and a 2D pose sequence. - The model architecture uses a DiT-based image-to-video model enhanced with a dual-attention module and camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency, followed by neighbor-constrained 4D Gaussian splatting for optimization. - CharacterShot outperforms existing state-of-the-art methods on the newly constructed CharacterBench benchmark, demonstrating superior performance in generating high-quality and consistent 4D character animations. - A large-scale dataset, Character4D, containing 13,115 unique characters with diverse appearances and motions, was also constructed to facilitate training and evaluation. - The framework is designed to be accessible to individual creators, democratizing the creation of high-quality 4D character animations without specialized hardware or expertise. | ['Image-to-Video', 'Text-to-Video', 'Computer Vision', 'Multimodal'] | [Link](https://github.com/Jeoyal/CharacterShot) | N/A |
| [Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language
  Models](https://arxiv.org/abs/2508.09138) | Chenchen Jing, Bozhen Fang, Wen Wang, qiuyuu, tricktreat | - This paper introduces two novel methods to improve the performance of diffusion language models (DLLMs) by leveraging their temporal dynamics. - The first method is Temporal Self-Consistency Voting, a training-free method that aggregates predictions across multiple denoising steps to select the most consistent output. - The second method is Temporal Consistency Reinforcement, a post-training method that uses a reward signal based on the Temporal Semantic Entropy (TSE) to encourage more stable generations. - Experimental results on multiple benchmarks show that these methods significantly improve the performance of DLLMs, with an average improvement of 24.7% on the Countdown dataset. - The findings underscore the importance of temporal dynamics in DLLMs and provide simple yet effective tools to harness them. | ['Text Generation'] | [Link](https://aim-uofa.github.io/dLLM-MidTruth) | N/A |
| [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating
  Local and Web Searches](https://arxiv.org/abs/2508.08088) | Qiang Ju, Jiehan Cheng, Yan Yu, Zhicheng Dou, zstanjj | - This paper introduces HierSearch, a hierarchical agentic deep search framework that integrates both local and web searches to enhance the retrieval of information. - The framework uses hierarchical reinforcement learning (HRL) to train three agents: a local deep search agent, a web deep search agent, and a planner agent, which coordinates the low-level agents. - To prevent hallucinations and irrelevant information propagation, a knowledge refiner is incorporated to filter the output from the low-level agents, providing a more accurate final answer. - Experiments across six benchmarks show that HierSearch outperforms flat RL and various deep search baselines, demonstrating the effectiveness of its hierarchical architecture and the knowledge refiner. - The superiority of the method is evidenced by improved performance on six benchmark datasets and more effective utilization of both local and web knowledge sources. | ['Question Answering'] | [Link](https://github.com/plageon/HierSearch) | N/A |
| [Aryabhata: An exam-focused language model for JEE Math](https://arxiv.org/abs/2508.08665) | Sandeep Varma, Sachin Dharashivkar, RitvikPW | - Aryabhata 1.0 is a 7B parameter language model designed for solving mathematical problems, specifically focusing on the Indian Joint Entrance Examination (JEE). - The model is built by merging three strong open-weight reasoning models and then fine-tuned using supervised fine-tuning (SFT) with curriculum learning and reinforcement learning with verifiable rewards (RLVR). - Aryabhata outperforms existing models in accuracy and efficiency on both in-distribution (JEE Main 2025) and out-of-distribution benchmarks (MATH, GSM8K), while also providing pedagogically useful step-by-step reasoning. - The model is released as a foundation model to promote the development of exam-centric, open-source small language models. - Future work includes expanding coverage to Physics and Chemistry, scaling to the full JEE syllabus, and developing a family of exam-centric SLMs. | ['Question Answering'] | N/A | [Link](https://huggingface.co/) |
| [Train Long, Think Short: Curriculum Learning for Efficient Reasoning](https://arxiv.org/abs/2508.08940) | Marzyeh Ghassemi, Elie Bou-Zeid, Abed Hammoud, Kumail Alhamoud, Hasan Abed Al Kader Hammoud | - This paper introduces curriculum learning for length-controlled reasoning in large language models (LLMs) to improve efficiency. - It proposes a method that starts with generous token budgets and gradually tightens them over training using Group Relative Policy Optimization (GRPO), allowing the model to discover effective strategies and distill them into concise solutions. - The method is evaluated on several benchmark datasets (GSM8K, MATH500, SVAMP, College Math, and GSM+), demonstrating consistent outperformance of fixed-budget baselines in both accuracy and token efficiency. - Ablation studies show the impact of reward weighting and decay schedule design, highlighting the effectiveness of progressive constraint as an inductive bias. - The code and checkpoints are publicly available on GitHub. | ['Reinforcement Learning', 'Question Answering'] | [Link](https://github.com/hammoudhasan/curriculum_grpo) | N/A |
| [OpenCUA: Open Foundations for Computer-Use Agents](https://arxiv.org/abs/2508.09123) | Tianbao Xie, Junlin Yang, Dunjie Lu, Bowen Wang, xywang626 | - OpenCUA is a comprehensive open-source framework for Computer-Use Agents (CUAs) that addresses critical gaps in existing research by providing diverse datasets, effective training recipes, and efficient evaluation benchmarks. - The framework introduces novel modules, including reflective reasoning and context encoding, to improve the reasoning capabilities of CUAs. - OpenCUA achieves state-of-the-art performance on multiple benchmarks, surpassing proprietary models in various aspects, including success rate and efficiency. - The framework's data processing pipeline and novel training recipes significantly enhance the performance and generalizability of CUAs, enabling the community to conduct more rigorous research. - Comprehensive evaluation benchmarks ensure the models' robustness and reliability across diverse tasks and platforms. | ['Reinforcement Learning', 'Multimodal'] | [Link](https://github.com/OpenCUA/OpenCUA) | N/A |
| [AutoCodeBench: Large Language Models are Automatic Code Benchmark
  Generators](https://arxiv.org/abs/2508.09101) | Tao Zhang, Zhiying Zeng, Yuchi Deng, Ao Liu, Jason Chou | - AutoCodeBench, a novel multilingual code generation benchmark, is introduced to evaluate the capabilities of large language models (LLMs) in generating code across multiple programming languages. - The benchmark features a fully automated workflow built on an LLM-based sandbox, ensuring high quality, diversity, and practicality for evaluating code generation performance. - AutoCodeBench comprises a large-scale, high-difficulty multilingual benchmark covering 20 programming languages and various programming domains, outperforming existing benchmarks. - Experimental results demonstrate that even the most advanced LLMs still struggle with complex and diverse multilingual code generation problems. - AutoCodeBench provides valuable insights for the future development of code generation benchmarks and helps evaluate the capabilities of LLMs in generating code across multiple programming languages. | ['Text Generation'] | N/A | N/A |
| [BiasGym: Fantastic Biases and How to Find (and Remove) Them](https://arxiv.org/abs/2508.08855) | Arnav Arora, Haeun Yu, Siddhesh Milind Pawar, Nadav Borenstein, sekhcopenlu | - This paper introduces BiasGym, a novel framework for identifying and mitigating biases in large language models (LLMs). - BiasGym uses a two-module framework: the first module identifies biased attention heads, and the second module uses a novel fine-tuning approach to mitigate those biases. - The proposed approach outperforms existing methods across several LLMs and bias types, achieving higher performance in both bias mitigation and downstream task performance. - The framework is shown to generalize well across diverse models and bias types. - BiasGym provides a controlled setup for in-depth analysis of bias mitigation techniques in LLMs, supporting future research on fairness and safety in AI. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via
  General Samples Replay](https://arxiv.org/abs/2508.04676) | Yang Fan, Yuefeng Li, Mengchen Zhao, Shuoran Jiang, Yunan Zhang | - This paper introduces GeRe, a novel framework for efficient anti-forgetting in continual learning of LLMs via general samples replay. - GeRe leverages a set of general samples replayed using a threshold-based margin loss to mitigate catastrophic forgetting in the downstream tasks. - The proposed method demonstrates superior performance compared to existing continual learning methods, achieving better generalization capabilities and robustness. - GeRe shows significant improvements in both MMU and average accuracy across multiple downstream tasks compared to baseline methods. - The authors provide a comprehensive analysis of their method, investigating the effect of various parameters and showcasing the method's effectiveness in various continual learning scenarios. | ['Natural Language Processing'] | [Link](https://github.com/Qznan/GeRe) | N/A |
| [NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech
  Modeling with Paralinguistic Vocalizations](https://arxiv.org/abs/2508.04195) | Haoyue Zhan, Yiheng Lu, Yuancheng Wang, Qinke Ni, Huan Liao | - This paper introduces NVSpeech, an integrated and scalable pipeline for human-like speech modeling that incorporates paralinguistic vocalizations (like laughter, breathing, and interjections). - NVSpeech includes a new, manually annotated dataset of 48,430 utterances with 18 word-level paralinguistic categories and an automatically annotated corpus of 174,179 utterances (573 hours). - The pipeline develops a paralinguistic-aware ASR model that jointly transcribes lexical and non-verbal content. - NVSpeech also uses a fine-tuned zero-shot TTS model to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for human-like speech synthesis. - Experimental results demonstrate that NVSpeech outperforms baseline models on paralinguistic tagging, ASR, and zero-shot TTS tasks, showing the effectiveness of incorporating paralinguistic vocalizations for more natural and expressive speech synthesis. | ['Audio', 'Automatic Speech Recognition', 'Text-to-Speech'] | [Link](https://github.com/nvspeech170k/nvspeech) | N/A |
