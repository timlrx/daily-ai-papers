

## Papers for 2025-08-14

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery](https://arxiv.org/abs/2508.08401) | Di Zhang, Junxian Li, Qinggang Zhang, Weida Wang, Jiatong Li | - Mol-R1 is a novel framework designed to enhance the explainability and reasoning performance of explicit long chain-of-thought (CoT) reasoning LLMs in text-based molecule generation. - It introduces Prior Regulation via In-context Distillation (PRID), a distillation strategy to generate paired reasoning traces guided by prior regulations. - Mol-R1 also uses Molecular Iterative Adaptation (MoIA), a training strategy combining supervised fine-tuning (SFT) with reinforced policy optimization (RPO) to improve reasoning performance. - Experiments show Mol-R1 outperforms existing baselines in text-based molecule reasoning generation, achieving a BLEU score that is 354% higher than QWQ-32B and an exact match score 1.5 times better than DeepSeek-R1. - The framework significantly enhances the interpretability and trustworthiness of generated molecules through high-quality reasoning traces. | ['Text Generation'] | N/A | N/A |
| [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust
  GAIA Problem Solving](https://arxiv.org/abs/2508.09889) | Jinjie Gu, Chenyi Zhuang, Chengyue Yu, Qintong Wu, Zhitian Xie | - This paper introduces AWorld, a dynamic multi-agent system (MAS) for robust GAIA problem solving that uses dynamic supervision and maneuvering mechanisms to improve the stability and reliability of agent-based systems. - The MAS architecture consists of an Execution Agent and a Guard Agent; the Guard Agent verifies and corrects the reasoning process of the Execution Agent, reducing errors and improving robustness. - Experiments on the GAIA benchmark show that AWorld significantly outperforms single-agent systems and standard tool-augmented systems, achieving first place among open-source projects. - The dynamic maneuvering mechanism in AWorld improves both the effectiveness and stability of solutions, highlighting the practical value of collaborative agent roles in developing more reliable intelligent systems. - This approach is inspired by principles in control theory, particularly from marine vessel navigation, where continuous and adaptive adjustments are essential to ensure a vessel converges to a desired trajectory. | ['Question Answering'] | [Link](https://github.com/inclusionAI/AWorld) | N/A |
| [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion
  Forcing](https://arxiv.org/abs/2508.09192) | Hao Zhang, Jiachun Jin, Yijie Jin, Chenkai Xu, Xu Wang | - This paper introduces Discrete Diffusion Forcing (D2F), a novel training paradigm for Diffusion Large Language Models (dLLMs) that achieves faster-than-autoregressive (AR) inference speeds. - D2F combines block-wise autoregressive generation with inter-block parallel decoding, enabling efficient KV cache utilization and significantly increased throughput. - The proposed method utilizes an asymmetric distillation process to train D2F dLLMs, leveraging pre-trained dLLMs to avoid the high cost of training from scratch. - Empirical results demonstrate that D2F dLLMs achieve more than 2.5x inference speed compared to similarly sized AR baselines on multiple benchmarks. - The acceleration provided by D2F is shown to be more than 50x faster than other dLLMs while maintaining comparable output quality. | ['Text Generation'] | [Link](https://github.com/zhijie-group/Discrete-Diffusion-Forcing) | N/A |
| [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with
  Long-Term Memory](https://arxiv.org/abs/2508.09736) | Yuan Lin, Yiyuan Pan, Wentao Ye, Yichen He, Lin Long | This paper introduces M3-Agent, a novel multimodal agent framework equipped with long-term memory, which processes real-time visual and auditory inputs to build and update its memory.  M3-Agent outperforms existing methods by achieving 6.7%, 7.7%, and 5.3% higher accuracy on three benchmarks (M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively).  A new benchmark, M3-Bench, is also introduced for evaluating multimodal agent capabilities, including long-video question answering.  M3-Agent's design incorporates both episodic and semantic memory organized in an entity-centric format. | ['Robotics', 'Reinforcement Learning', 'Multimodal', 'Video Classification', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/bytedance-seed/m3-agent) | N/A |
| [Learning to Align, Aligning to Learn: A Unified Approach for
  Self-Optimized Alignment](https://arxiv.org/abs/2508.07750) | Lei Fan, Shuowen Zhang, Zhiling Ye, Yun Yue, Haowen Wang | - This paper introduces GRAO (Group Relative Alignment Optimization), a novel unified framework that synergizes supervised fine-tuning (SFT) and reinforcement learning (RL) for self-optimized language model alignment. - GRAO addresses the limitations of SFT (offline policy trajectory) and RL (low sample efficiency, dependency on high-quality base models) through a multi-sample generation strategy, a novel Group Direct Alignment Loss, and reference-aware parameter updates. - Theoretical analysis demonstrates GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. - Comprehensive evaluations across complex human alignment tasks show GRAO achieves significant relative improvements (57.70%, 17.65%, 7.95%, and 5.18%) over SFT, DPO, PPO, and GRPO baselines respectively. - The results indicate that GRAO enhances both the reasoning ability and alignment of language models by dynamically adjusting imitation learning and exploratory learning. | ['Natural Language Processing'] | N/A | N/A |
| [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math
  Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009) | Zhihan Zhou, Yue Guo, Zhentao Zhang, Zixin Wang, junfeng0288 |  - This paper introduces MATHREAL, a new benchmark dataset containing 2000 real-world mathematical questions with images captured by handheld devices.   -  The dataset is categorized into three primary challenges: image quality degradation, perspective variation, and irrelevant content interference, which are further divided into 14 subcategories.   -  MATHREAL spans five core knowledge and ability categories, three question types, and three difficulty levels, making it comprehensive and nuanced.   - Experiments on MATHREAL reveal a significant performance gap between existing MLLMs' abilities on real-world versus clean or processed images.   - The analysis of error patterns provides insights for future model improvements in multimodal mathematical reasoning. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://github.com/junfeng0288/MathReal) | N/A |
| [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456) | Di Zhang, Beining Xu, Junxian Li | - This paper introduces a novel input-aware backdoor attack method, IAG, for manipulating the grounding behavior of Vision-Language Models (VLMs). - IAG forces the model to ground a specific target object regardless of the user's query by embedding semantic information of the attack target's description into the original image using a text-conditional U-Net. - The attack's stealthiness is ensured by utilizing a reconstruction loss to minimize visual discrepancies between poisoned and clean images. - IAG achieves over 65% ASR@0.5 on InternVL-2.5-8B and shows promising results on Ferret-7B and LlaVA-1.5-7B, demonstrating its effectiveness and robustness. - Extensive experiments validate IAG's effectiveness across multiple VLMs and datasets, highlighting its potential threat to the security of VLMs in visual grounding tasks. | ['Multimodal'] | N/A | N/A |
| [VisCodex: Unified Multimodal Code Generation via Merging Vision and
  Coding Models](https://arxiv.org/abs/2508.09945) | Dongdong Zhang, Yixia Li, Xun Wu, Shaohan Huang, Lingjie Jiang | - VisCodex is a unified framework that merges vision and coding language models to achieve strong multimodal code generation capabilities.  The model architecture uses a task vector-based model merging technique to integrate a state-of-the-art coding LLM into a vision-language backbone. - The Multimodal Coding Dataset (MCD), a large-scale and diverse dataset of 598k samples, was introduced to support training and evaluation. MCD includes various modalities such as HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. - InfiBench-V, a challenging benchmark designed to evaluate visually rich, real-world programming questions, was also introduced.  - VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4, highlighting the effectiveness of the model merging strategy and new datasets. - Extensive experiments demonstrate VisCodex's superior performance on various benchmarks, outperforming other open-source models and approaching the performance of GPT-4. | ['Multimodal', 'Image-Text-to-Text', 'Text2Text Generation', 'Text Generation'] | [Link](https://github.com/JackLingjie/VisCodex) | N/A |
| [Can LLM-Generated Textual Explanations Enhance Model Classification
  Performance? An Empirical Study](https://arxiv.org/abs/2508.09776) | Gjergji Kasneci, Zineb Attaoui, Ege Erdogan, Juraj Vladika, Mahdi Dhaini | - This paper introduces a novel LLM-based framework for automatically generating textual explanations for natural language inference (NLI) tasks. - The framework leverages multiple state-of-the-art LLMs to generate high-quality textual explanations, rigorously evaluated using a suite of natural language generation (NLG) metrics. - Experiments demonstrate that these automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance across various pre-trained language models (PLMs) and LLMs. - The findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance. - The research addresses the lack of definitive ground-truth explanations in Explainable NLP and the limitations of traditional human annotation approaches. | ['Natural Language Processing'] | N/A | N/A |
| [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal
  Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944) | Yong Li, Jie Feng, Lixuan He | This paper introduces AMFT, a novel single-stage algorithm for aligning large language model (LLM) reasoners by learning the optimal balance between imitation and exploration.  AMFT uses a meta-gradient adaptive weight controller to dynamically adjust the balance between supervised fine-tuning (SFT) and reinforcement learning (RL).  The proposed method consistently outperforms state-of-the-art methods on various benchmark tasks such as mathematical reasoning, visual reasoning, and vision-language navigation.  AMFT's adaptive controller learns a dynamic training curriculum without the need for manually tuning hyperparameters.  Ablation studies confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance. | ['Reinforcement Learning', 'Natural Language Processing', 'Multimodal'] | [Link](https://github.com/hlxtsyj/AMFT) | N/A |
