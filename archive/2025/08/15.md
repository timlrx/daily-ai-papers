

## Papers for 2025-08-15

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual
  Mathematical Reasoning](https://arxiv.org/abs/2508.10433) | Xiaowan Wang, Yanzi Wang, Peiqing Yang, Qiuna Tan, Runqi Qiao |  - This paper introduces WE-MATH 2.0, a unified system that enhances the mathematical reasoning abilities of Multimodal Large Language Models (MLLMs). - WE-MATH 2.0 integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm. - The system includes three key components: MathBook Knowledge System, MathBook-Standard and Pro, and MathBookEval benchmark covering 491 knowledge points and 1819 fundamental principles. - The proposed MathBook-RL training framework uses a two-stage RL approach (Cold-Start Fine-tuning and Progressive Alignment RL) and outperforms existing baselines on four widely used benchmarks. - The comprehensive MathBookEval benchmark demonstrates that MathBook-RL performs well and shows promising generalization in mathematical reasoning. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://we-math2.github.io/) | N/A |
| [ToonComposer: Streamlining Cartoon Production with Generative
  Post-Keyframing](https://arxiv.org/abs/2508.10881) | Xiaoyu Li, Yaowei Li, Zhaoyang Zhang, Guangzhi Wang, Lingen Li | - ToonComposer is a novel generative model that unifies inbetweening and colorization into a single post-keyframing stage, reducing manual effort in cartoon production. - It uses a diffusion transformer (DiT) architecture with a spatial low-rank adapter (SLRA) to adapt a pre-trained video foundation model to the cartoon domain while preserving temporal consistency. - The model incorporates a sparse sketch injection mechanism, allowing for precise control with as few as a single keyframe sketch and a colored reference frame. - Experiments on synthetic and real-world benchmarks demonstrate that ToonComposer outperforms existing methods in visual quality, motion coherence, and production efficiency. - ToonComposer introduces region-wise control, enabling flexible motion generation without the need for fully drawn sketches, further reducing the workload of artists. | ['Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://lg-li.github.io/project/tooncomposer) | N/A |
| [UI-Venus Technical Report: Building High-performance UI Agents with RFT](https://arxiv.org/abs/2508.10833) | Shuheng Shen, Xingran Zhou, Zhenyu Xu, Zhengwen Zeng, Zhangxuan Gu |  *  This paper introduces UI-Venus, a novel UI agent that leverages a multimodal large language model and reinforcement learning to achieve state-of-the-art performance in UI grounding and navigation tasks.  *  The model uses only screenshots as input and is trained on a relatively small dataset of several hundred thousand samples.  *  UI-Venus outperforms existing baselines on standard grounding benchmarks (Screenspot-V2/Pro) and online UI navigation arenas (AndroidWorld), demonstrating its effectiveness.  *  The authors propose Self-Evolving Trajectory History Alignment & Sparse Action Enhancement, a novel technique that enhances the navigation performance of UI agents.  *  The code and evaluation codes are publicly available on Github, promoting further research and development in the community. | ['Reinforcement Learning', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/antgroup/UI-Venus) | N/A |
| [PRELUDE: A Benchmark Designed to Require Global Comprehension and
  Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848) | Rui Lu, Tong Li, Chulun Zhou, Tsz Ting Chung, Mo Yu | - This paper introduces PRELUDE, a new benchmark designed to evaluate long-context understanding and reasoning capabilities in large language models (LLMs). - The benchmark focuses on the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book, requiring global comprehension and deep reasoning. -  Experimental results demonstrate that state-of-the-art LLMs, even with advanced techniques like retrieval-augmented generation (RAG) and in-domain training, lag significantly behind human performance. - The substantial performance gap highlights the considerable room for improvement in LLMs' long-context understanding and reasoning abilities. -  PRELUDE provides a novel and challenging task for evaluating LLMs, pushing the boundaries of current long-context understanding research. | ['Natural Language Processing'] | [Link](https://gorov.github.io/prelude) | N/A |
| [HumanSense: From Multimodal Perception to Empathetic Context-Aware
  Responses through Reasoning MLLMs](https://arxiv.org/abs/2508.10576) | Yi Yuan, Tianqi Li, Yabing Wang, Ruobing Zheng, Zheng Qin | - The paper introduces HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of Multimodal Large Language Models (MLLMs). - HumanSense focuses on deep understanding of extended multimodal contexts and the formulation of rational feedback, covering 15 progressively challenging tests and 3882 questions. - The evaluation reveals considerable room for improvement in leading MLLMs, particularly for advanced interaction-oriented tasks; supplementing visual input with audio and text yields substantial improvements; and Omni-modal models show advantages. - A multi-stage, modality-progressive reinforcement learning approach enhances the reasoning abilities of an Omni-modal model, achieving substantial gains on evaluation results; successful reasoning processes exhibit highly consistent thought patterns. - The training-free manner prompts further enhance the performance of non-reasoning models. | ['Multimodal', 'Video-Text-to-Text', 'Visual Question Answering', 'Question Answering', 'Reinforcement Learning'] | [Link](https://digital-avatar.github.io/ai/HumanSense/) | N/A |
| [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875) | Zhiqiang Shen, Bowei Guo, Mingda Chen, Tianyi Li | This survey paper provides a comprehensive overview of Diffusion Language Models (DLMs), highlighting their advantages over autoregressive models in terms of speed and bidirectional context.  It details the evolution of DLMs, covering both foundational principles and state-of-the-art models, along with pre-training strategies and post-training methods.  The survey also includes a thorough analysis of DLM inference strategies and their optimization, in addition to the latest approaches in multimodal extensions. Finally, it addresses the limitations and challenges of DLMs, while outlining future research directions. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/VILA-Lab/Awesome-DLMs) | N/A |
| [When Explainability Meets Privacy: An Investigation at the Intersection
  of Post-hoc Explainability and Differential Privacy in the Context of Natural
  Language Processing](https://arxiv.org/abs/2508.10482) | Gjergji Kasneci, Florian Matthes, Ege Erdogan, Stephen Meisenbacher, Mahdi Dhaini | - This paper investigates the interplay between post-hoc explainability and differential privacy in natural language processing (NLP). - The authors empirically analyze the privacy-explainability trade-off using differentially private text rewriting methods and various post-hoc explainability techniques. - Their findings reveal a complex relationship between privacy and explainability, highlighting the influence of downstream tasks and method choices. - They also demonstrate that privacy and explainability can coexist synergistically under certain configurations. - The authors propose practical recommendations for researchers and practitioners working at the intersection of privacy and explainability in NLP. | ['Natural Language Processing', 'Text Classification'] | [Link](https://github.com/dmah10/xpnlp) | N/A |
