

## Papers for 2025-08-18

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630) | Wei Chen, Chaoyou Fu, Shukang Yin, Xingyu Lu, Yi-Fan Zhang | This paper introduces Thyme, a novel multimodal large language model that transcends existing "think with images" approaches by autonomously generating and executing image processing and computational operations via executable code.  Thyme uses a two-stage training strategy: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), incorporating the GRPO-ATS algorithm to balance reasoning exploration with code execution precision.  Evaluations on nearly 20 benchmarks demonstrate significant and consistent performance gains, especially in challenging high-resolution perception and complex reasoning tasks.  The datasets, sandbox, and code are publicly available. | ['Multimodal', 'Visual Question Answering', 'Reinforcement Learning'] | [Link](https://github.com/yfzhang114/Thyme) | [Link](https://huggingface.co/Kwai-Keye/Thyme-RL) |
| [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization](https://arxiv.org/abs/2508.10395) | Rishabh Tiwari, Haocheng Xi, Minjae Lee, Coleman Hooper, Aditya Tomar | - This paper introduces XQuant, a novel technique for reducing the memory footprint of large language model (LLM) inference by quantizing and caching layer input activations instead of standard key-value (KV) caching. - XQuant achieves up to 7.7x memory savings with less than 0.1 perplexity degradation compared to the FP16 baseline. - The authors further introduce XQuant-CL, which exploits cross-layer similarity in activations to achieve up to 10x memory savings with minimal perplexity degradation. - XQuant-CL outperforms state-of-the-art KV cache quantization methods despite using standard uniform quantization. - The results demonstrate that XQuant and XQuant-CL effectively alleviate the memory bottleneck in LLM inference while maintaining near FP16 accuracy across a wide range of models. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for
  Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255) | Mu Xu, Fan Jiang, MengChao Wang, wangqiang9 | - This paper introduces Talking-Critic, a multimodal reward model, and Talking-NSQ, a large-scale multi-dimensional human preference dataset containing 410K preference pairs, to address the limitations of existing audio-driven portrait animation methods. - A novel framework called Timestep-Layer adaptive multi-expert Preference Optimization (TLPO) is proposed to align diffusion-based portrait animation models with fine-grained, multidimensional preferences. - TLPO decouples preferences into specialized expert modules and fuses them across timesteps and network layers, enabling comprehensive, fine-grained enhancement across all dimensions. - Experiments demonstrate that Talking-Critic significantly outperforms existing methods in aligning with human preference ratings, and TLPO achieves substantial improvements over baseline models in lip-sync accuracy, motion naturalness, and visual quality. - The proposed methods exhibit superior performance in both qualitative and quantitative evaluations, showcasing the effectiveness of the proposed approach for generating high-fidelity, human-aligned audio-driven portrait animations. | ['Audio-to-Audio', 'Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://fantasyamap.github.io/fantasy-talking2/) | N/A |
| [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616) | Michal Drozdzal, Adriana Romero-Soriano, Koustuv Sinha, Pierluca D'Oro, oscmansan | - This paper introduces a novel method for reward-guided decoding of Multimodal Large Language Models (MLLMs) to improve visual grounding. - The method involves building two separate reward models to independently control object precision and recall, enabling on-the-fly controllability of an MLLM's inference process. - The proposed method allows users to dynamically trade off between object precision and recall, as well as between compute and visual grounding quality. - Evaluation on standard object hallucination benchmarks demonstrates that the method provides significant controllability over MLLM inference while consistently outperforming existing hallucination mitigation methods. - The method is shown to be effective on various MLLMs and robust to changes in the reward model's quality. | ['Multimodal'] | N/A | N/A |
