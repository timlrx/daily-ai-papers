

## Papers for 2025-08-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737) | Yang Li, cqgwin, Suikong, xxyyy123, runninglsy |  - Ovis2.5 is a multimodal large language model designed for native resolution visual perception and strong multimodal reasoning. It integrates a native resolution vision transformer and is trained with a five-phase curriculum.  - To enhance reasoning, it performs reflection, including self-checking and revision, exposed as an optional "thinking mode".  - Ovis2.5-9B achieves state-of-the-art results on the OpenCompass multimodal leaderboard, averaging 78.3, and Ovis2.5-2B achieves state-of-the-art results for its size, scoring 73.9.  -  The model shows strong capabilities in grounding and video tasks, and achieves leading results on STEM benchmarks and complex chart analysis.  - Two open-source models, Ovis2.5-9B and Ovis2.5-2B, are released. | ['Multimodal'] | [Link](https://github.com/AIDC-AI/Ovis) | [Link](https://huggingface.co/AIDC-AI/Ovis2.5-9B) |
| [Speed Always Wins: A Survey on Efficient Architectures for Large
  Language Models](https://arxiv.org/abs/2508.09834) | Jusen Du, Yucheng Zhou, Jiaxi Hu, Weigao Sun, landisen | This paper surveys recent advancements in efficient architectures for large language models (LLMs). - The authors systematically examine innovative LLM architectures that address the inherent limitations of transformers and boost efficiency. - The survey covers linear and sparse sequence modeling, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures, and emerging diffusion LLMs. - The authors provide a blueprint of modern efficient LLM architectures and discuss applications of efficient techniques to other modalities. - The study categorizes the reviewed studies for efficient LLM architecture, and hopes to help researchers develop scalable resource-aware foundation models. | ['Natural Language Processing'] | [Link](https://github.com/weigao266/Awesome-Efficient-Arch) | [Link](string) |
| [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142) | Ruisi Wang, Qingping Sun, Yubo Wang, yl-1993, caizhongang | This paper empirically evaluates the spatial intelligence capabilities of GPT-5 and other state-of-the-art multi-modal language models (MLLMs).  A comprehensive taxonomy of spatial tasks is proposed to unify existing benchmarks, ensuring fair evaluations.  GPT-5 demonstrates significant advancements in spatial intelligence, surpassing other models across various benchmarks. However, GPT-5 still falls short of human performance on many challenging spatial reasoning tasks, highlighting ongoing limitations in MLLMs.  The paper identifies areas where further research is needed to advance MLLMs towards achieving true spatial intelligence. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | N/A | N/A |
| [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness
  Methods for LLMs](https://arxiv.org/abs/2508.11383) | Elena Tutubalina, Gleb Ershov, Mikhail Chaichuk, apanc, myyycroft | - This paper presents the first systematic evaluation of five methods for improving prompt robustness in large language models (LLMs). - The evaluation is conducted on eight models from Llama, Qwen, and Gemma families across 52 tasks from the Natural Instructions dataset, covering various robustness methods and learning paradigms. - The study analyzes the generalization of these methods against multiple types of distribution shifts and extends the analysis to GPT-4.1 and DeepSeek V3. - The findings offer insights into the effectiveness of different robustness methods, enabling practitioners to make informed decisions. - The code for the study is publicly available on GitHub. | ['Natural Language Processing'] | [Link](https://github.com/AIRI-Institute/when-punctuation-matters) | N/A |
| [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive
  World Model](https://arxiv.org/abs/2508.13009) | Yifan Zhang, Boyang Wang, Zexiang Liu, Chunli Peng, Xianglong He | - Matrix-Game 2.0 is a novel interactive world model that generates high-quality, minute-level videos at 25 FPS using a few-step autoregressive diffusion approach, addressing limitations of existing models that rely on bidirectional attention and lengthy inference steps. - The model architecture consists of three key components: a scalable data production pipeline using Unreal Engine and GTA5, an action injection module for frame-level user inputs, and a few-step distillation technique. - Matrix-Game 2.0 outperforms existing methods in terms of speed and video quality, achieving real-time performance on a single H100 GPU.  Quantitative evaluations show improvements in visual quality, temporal coherence, and action controllability compared to baselines such as Oasis and YUME. - The model's strong generalization capability is demonstrated through experiments on diverse scenes including Minecraft, GTA5 driving scenarios, and TempleRun game environments. - The model weights and codebase are open-sourced to advance research in interactive world modeling. | ['Text-to-Video', 'Image-to-Video', 'Video-Text-to-Text', 'Multimodal'] | N/A | N/A |
| [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598) | Daniel L. K. Yamins, Evelina Fedorenko, Greta Tuckute, klemenk | - AuriStream, a novel biologically-inspired model for speech representation, is introduced. It uses a two-stage framework: WavCoch transforms audio into a time-frequency representation based on the human cochlea, and AuriStream, an autoregressive model, predicts cochlear tokens. - AuriStream demonstrates competitive performance on various speech tasks, including phoneme and word decoding, and achieves state-of-the-art results on lexical semantics. - Unlike other models, AuriStream generates audio continuations which are interpretable in a cochleagram space, providing insights into the model's internal representations and predictions.  - The results show that the model efficiently learns short and long-range speech statistics without ground-truth phoneme, word, or task labels. - The proposed framework combines autoregressive prediction with biologically plausible inputs, contributing to advancements in human-like speech models. | ['Audio', 'Automatic Speech Recognition', 'Audio Classification', 'Text-to-Speech'] | [Link](https://tukoresearch.github.io/auristream-speech/) | N/A |
| [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision
  Mapping](https://arxiv.org/abs/2508.12466) | Tyler Derr, xuhuizhan5 | - This paper introduces Inverse-LLaVA, a novel multimodal learning approach that eliminates the need for expensive alignment pre-training by inverting the traditional mapping direction. - Unlike existing methods that project visual features into discrete text token spaces, Inverse-LLaVA maps text embeddings into continuous visual representation space and performs fusion within transformer intermediate layers. - Inverse-LLaVA achieves notable improvements on reasoning-intensive and cognitive tasks while showing expected decreases in perception tasks requiring memorized visual-text associations, providing empirical evidence that alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning tasks. - The proposed method reduces computational requirements by 45% compared to traditional methods. - Inverse-LLaVA's performance is validated across nine multimodal benchmarks demonstrating nuanced performance trade-offs across different tasks. | ['Multimodal'] | [Link](https://inverse-llava.github.io) | N/A |
| [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning
  Models to Ask for Information](https://arxiv.org/abs/2508.11252) | Xi Yang, Duanyu Feng, Chen Huang, Bowen Qin, YouchengHuang |  - This paper introduces CRITIC-math, a new benchmark dataset for evaluating the ability of Large Reasoning Models (LRMs) to ask for information when presented with incomplete mathematical problems.  - The dataset consists of two types of incomplete problems: missing goals and missing premises, with diverse contexts and challenging mathematical problems.  - Experiments on several state-of-the-art LRMs reveal their inability in proactively asking for information, highlighting behaviors related to overthinking and hallucination.  - Supervised fine-tuning (SFT) is explored as a potential solution for improving LRMs' ability to ask for information, though it shows a dilemma between solving problems and asking questions.  - CRITIC-math offers new insights into the development of LRMs with genuine intelligence, going beyond simple problem-solving abilities. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/YouchengHuang/CRITIC-math), [Link](https://huggingface.co/datasets/YouchengHuang/CRITIC-math-sft) |
