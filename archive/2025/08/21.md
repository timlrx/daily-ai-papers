

## Papers for 2025-08-21

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating
  Financial Large Language Models](https://arxiv.org/abs/2508.13491) | Ziyan Kuang, Effoula, QianqianXie1994, hugai101, 2083L | - This paper introduces FinCDM, the first cognitive diagnosis framework designed for evaluating financial LLMs at the knowledge-skill level, moving beyond traditional score-level evaluations. - FinCDM leverages CPA-QKA, a new cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, rigorously annotated by domain experts. - Experiments on 30 LLMs demonstrate FinCDM's ability to reveal hidden knowledge gaps, identify under-tested areas, and uncover behavioral clusters among models, supporting more trustworthy and targeted model development. - The proposed framework enables interpretable, skill-aware diagnosis, offering a more nuanced understanding of LLM capabilities compared to existing aggregate-score benchmarks. - All datasets and evaluation scripts are publicly released to foster further research in financial LLM evaluation. | ['Question Answering'] | [Link](https://github.com/WHUNextGen/FinCDM) | N/A |
| [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference
  Optimization](https://arxiv.org/abs/2508.14460) | Yu Lu, Yu Bao, Shanbo, ShujianHuang, kevinpro | - The paper introduces DuPO, a dual learning-based preference optimization framework for reliable LLM self-verification, addressing the limitations of RLVR's reliance on costly labels and traditional dual learning's restriction to strictly dual task pairs. - DuPO decomposes a primal task's input into known and unknown components, constructing a dual task to reconstruct the unknown part using the primal output and known information, thus broadening applicability to non-invertible tasks. - The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, achieving substantial gains across diverse tasks such as translation and mathematical reasoning. - Empirical results demonstrate that DuPO enhances translation quality by an average of 2.13 COMET points and boosts mathematical reasoning accuracy by an average of 6.4 points. - DuPO is presented as a scalable, general, and annotation-free paradigm for LLM optimization, showcasing its effectiveness in both training and inference-time reranking. | ['Reinforcement Learning', 'Natural Language Processing', 'Translation', 'Text Generation', 'Question Answering'] | [Link](https://github.com/ByteDance-Seed/Seed-X-7B/tree/main/challenge_set) | N/A |
| [Quantization Meets dLLMs: A Systematic Study of Post-training
  Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896) | Haobo Xu, cityug7353, ZiyuG, chriswyc, Felix1023 | - This paper presents the first systematic study on post-training quantization (PTQ) for diffusion large language models (dLLMs). - The authors identify activation outliers as a key challenge to low-bit quantization in dLLMs and implement state-of-the-art PTQ methods. - Their comprehensive evaluation across various task types and model variants offers practical insights into the quantization behavior of dLLMs. - The results show that 4-bit is the most effective configuration for weight-only quantization, while 8-bit is recommended for weight-activation quantization. - GPTQ consistently outperforms AWQ, and rotation-based methods like DuQuant demonstrate clear advantages over SmoothQuant for weight-activation quantization. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160) | jiangpinliu, CausalLi, maoyunxuan, CircleRadon, RH-Dang |  - RynnEC, a novel video multimodal large language model (MLLM), is introduced for embodied cognition, incorporating a region encoder and a mask decoder for flexible region-level video interaction.  - The model achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning.  - To address the scarcity of annotated 3D datasets, an egocentric video-based pipeline is proposed for embodied cognition data generation.  - A new benchmark, RynnEC-Bench, is introduced for evaluating embodied cognitive capabilities, covering 22 tasks in object and spatial cognition.  - Extensive experiments demonstrate RynnEC's superior performance in embodied cognitive abilities compared to existing general and task-specific MLLMs. | ['Robotics', 'Video Classification', 'Visual Question Answering', 'Multimodal', 'Mask Generation'] | [Link](https://github.com/alibaba-damo-academy/RynnEC) | N/A |
| [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid
  Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444) | abercovich, aditya-malte, adirendu, aklife97, apaithan |  - The paper introduces Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed for reasoning workloads. - The model architecture combines Mamba-2 and self-attention layers, resulting in improved inference speed. - Nemotron-Nano-9B-v2 outperforms similarly-sized models (e.g., Qwen3-8B) by achieving on-par accuracy and up to 6x higher throughput on reasoning benchmarks. - The model was trained using a novel FP8 training recipe on 20 trillion tokens and further aligned with various post-training methods like SFT and RLHF. - The authors are releasing the model, along with the majority of their pre-training and post-training datasets, on Hugging Face. | ['Natural Language Processing', 'Text Generation', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1) |
| [MCP-Universe: Benchmarking Large Language Models with Real-World Model
  Context Protocol Servers](https://arxiv.org/abs/2508.14704) | Prathyusha Jwalapuram, Zirui Zhao, Wenzhuo Yang, Zhiqi Shen, Ziyang Luo | This paper introduces MCP-Universe, a comprehensive benchmark designed for evaluating large language models (LLMs) interacting with real-world Model Context Protocol (MCP) servers. - MCP-Universe includes six core domains spanning eleven different MCP servers, evaluating LLMs on realistic, challenging tasks. - The benchmark employs execution-based evaluators (format, static, and dynamic) for rigorous evaluation, overcoming limitations of simpler benchmarks. - Extensive evaluation reveals performance limitations of leading LLMs, highlighting challenges such as long-context handling and unfamiliar tools. - The benchmark is open-sourced to facilitate innovation and seamless integration of new agents and MCP servers. | ['Natural Language Processing'] | [Link](https://github.com/SalesforceAIResearch/MCP-Universe) | N/A |
| [ViExam: Are Vision Language Models Better than Humans on Vietnamese
  Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680) | Daeyoung Kim, Duc Dm, Quang Tau, anvo25, tuongvy2603 |  - This paper introduces ViExam, a benchmark dataset consisting of 2,548 multimodal Vietnamese exam questions across 7 academic domains. - ViExam is the first comprehensive multimodal benchmark for evaluating Vision Language Models (VLMs) on Vietnamese educational assessments. - State-of-the-art (SOTA) VLMs achieve only 57.74% mean accuracy on ViExam, while open-source models achieve 27.70%, underperforming human test-takers (66.54%). - The findings reveal that multimodal reasoning and cross-lingual prompting pose significant challenges for VLMs on low-resource languages, and that human-in-the-loop collaboration is effective in improving VLM performance. - The dataset includes a variety of visual elements (charts, diagrams, illustrations, tables) which present additional challenges for VLMs. | ['Visual Question Answering', 'Multimodal'] | N/A | N/A |
| [mSCoRe: a Multilingual and Scalable Benchmark for Skill-based
  Commonsense Reasoning](https://arxiv.org/abs/2508.10137) | anoperson, Franck-Dernoncourt, ntnghia1811 |  - mSCoRe, a novel multilingual and scalable benchmark for skill-based commonsense reasoning, is introduced.  The benchmark systematically evaluates large language models (LLMs) using three key components: a taxonomy of reasoning skills, a data synthesis pipeline, and a complexity scaling framework.  - The benchmark incorporates general and cultural commonsense knowledge across multiple languages (English, German, French, Chinese, and Japanese).  - Extensive experiments on eight state-of-the-art LLMs reveal that mSCoRe remains challenging for current models, particularly at higher complexity levels, demonstrating the limitations of current reasoning-reinforced models in handling nuanced multilingual commonsense.   - The results reveal the limitations of current models in handling nuanced multilingual commonsense and suggest future directions for improving these capabilities.   - The paper provides a detailed analysis of the models' reasoning processes, suggesting future directions for improving multilingual commonsense reasoning capabilities. | ['Question Answering'] | N/A | N/A |
