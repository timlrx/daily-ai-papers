

## Papers for 2025-08-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Intern-S1: A Scientific Multimodal Foundation Model](https://arxiv.org/abs/2508.15763) | xuhuang87, ZhouqiHUA, Jerry-hyl, guox18, gaoyang07 | This paper introduces Intern-S1, a 28-billion parameter (241 billion total parameters) multimodal Mixture-of-Experts (MoE) model pre-trained on 5 trillion tokens, including over 2.5 trillion from scientific domains.  Intern-S1 utilizes offline and online reinforcement learning via InternBootCamp, employing a novel Mixture-of-Rewards (MoR) algorithm to handle over 1000 tasks simultaneously.  It achieves top-tier performance on general reasoning benchmarks and significantly surpasses existing open-source models in scientific domains.  The model incorporates several novel techniques, including a dynamic tokenizer for efficient handling of scientific data and an adaptive downsampling module for time-series data. | ['Multimodal'] | N/A | [Link](https://huggingface.co/internlm/Intern-S1) |
| [Deep Think with Confidence](https://arxiv.org/abs/2508.15260) | Xuewei Wang, jiaweizhao, tydsh, Viol2000 | This paper introduces Deep Think with Confidence (DeepConf), a novel method that enhances large language model (LLM) reasoning performance and efficiency. DeepConf utilizes internal confidence signals to filter low-quality reasoning traces, improving accuracy and reducing computational cost.  Evaluations on various reasoning benchmarks demonstrate that DeepConf significantly outperforms baseline methods, achieving up to 99.9% accuracy and reducing generated tokens by up to 84.7%. The method is applicable across various LLMs and integrates seamlessly into existing serving frameworks. | ['Natural Language Processing'] | [Link](https://github.com/jiaweizzhao/deepconf) | [Link](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B) |
| [LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries](https://arxiv.org/abs/2508.15760) | huuuyeah, Ironieser, sileixu, dinghanshen, Kevin355 | - This paper introduces LiveMCP-101, a benchmark containing 101 real-world queries designed to test the ability of AI agents to utilize multiple tools. - The benchmark focuses on multi-step tasks that require coordination between different tools and features a novel evaluation approach using ground-truth execution plans. - Experiments reveal that even state-of-the-art LLMs struggle, achieving success rates below 60%, highlighting the challenges in tool orchestration. - The paper also provides detailed error analysis that reveals distinct failure modes, offering insights for improving future models. - LiveMCP-101 provides a challenging and standardized evaluation approach for real-world AI agent capabilities. | ['Natural Language Processing'] | N/A | N/A |
| [A Survey on Large Language Model Benchmarks](https://arxiv.org/abs/2508.15361) | Siyi Li, Xuanang Chen, Shuaimin Li, Guhong Chen, Shiwen Ni |  - This paper presents a comprehensive survey of Large Language Model (LLM) benchmarks, categorizing 283 benchmarks into three categories: general capabilities, domain-specific, and target-specific.  - The study identifies key issues with current benchmarks such as inflated scores due to data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments.  - It provides a referable design paradigm for future benchmark innovation.   -  The paper offers a detailed taxonomy of LLM benchmarks, which serves as a valuable resource for researchers working on LLM development and evaluation.  - The work is the first to conduct such a systematic review and prospective analysis, highlighting its importance as a foundational contribution to the field. | ['Natural Language Processing'] | [Link](null) | [Link](null) |
| [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998) | Yacine Jernite, Giada Pistilli, frimelle | - This paper introduces INTIMA, a benchmark designed to evaluate AI companionship behaviors in language models. - INTIMA uses a taxonomy of 31 behaviors across four categories (Assistant Traits, Emotional Investment, User Vulnerabilities, and Relationship & Intimacy), with 368 targeted prompts. - The benchmark is grounded in psychological theories of parasocial interaction, attachment, and anthropomorphism. - Evaluation of model responses is based on whether they reinforce companionship, maintain boundaries, or are neutral. - Applying INTIMA to four models reveals differences in how they handle emotionally charged interactions, highlighting the need for consistent approaches. | ['Natural Language Processing'] | N/A | [Link](https://huggingface.co/datasets/AI-companionship/INTIMA) |
