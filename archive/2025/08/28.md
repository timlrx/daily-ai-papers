

## Papers for 2025-08-28

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652) | Zhenwen Liang, Rui Liu, Wenhao Yu, Zongxia Li, ChengsongHuang | - This paper introduces Vision-SR1, a novel self-rewarding method for improving visual reasoning in Vision-Language Models (VLMs) without external supervision. - Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning, using a self-contained visual perception to compute reward. - The model is first prompted to generate a self-contained visual perception, and then it's re-prompted to perform language reasoning using only the generated perception. - Experiments demonstrate that Vision-SR1 significantly improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks, outperforming existing methods. - The approach uses a balanced training signal that strengthens both visual perception and language reasoning, addressing the limitations of prior methods that rely on external supervision or simple answer matching. | ['Visual Question Answering', 'Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/zli12321/Vision-SR1) | N/A |
| [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228) | Alham Fikri Aji, Erland, zaydzuhri | - This paper introduces Token Order Prediction (TOP), a novel auxiliary training objective for language models that aims to improve next-token prediction (NTP) performance. - TOP trains models to predict the order of upcoming tokens based on their proximity, using a learning-to-rank loss. This is in contrast to Multi-Token Prediction (MTP), which tries to predict the exact future tokens. - TOP only requires a single additional unembedding layer, compared to MTP's multiple transformer layers, making it more efficient and scalable. - Experiments on eight standard NLP benchmarks showed that TOP outperforms both NTP and MTP across different model sizes (340M, 1.8B, and 7B parameters). - The results suggest that TOP is a more effective auxiliary objective than MTP for improving general language modeling performance. | ['Natural Language Processing'] | [Link](https://github.com/zaydzuhri/token-order-prediction) | N/A |
| [Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health
  Biomarkers Estimation](https://arxiv.org/abs/2508.17924) | Anton Ivaschenko, Galina Zubkova, Stepan Botman, Konstantin Egorov, blinoff | - The paper introduces MCD-rPPG, a comprehensive multi-view video dataset for rPPG and health biomarker estimation, comprising 3600 synchronized video recordings from 600 subjects under varied conditions. - It includes a fast baseline model for rPPG and health biomarker estimation, utilizing a 1D fully convolutional feature pyramid network. - The model achieves a speed improvement of up to 13% over existing models, while maintaining competitive accuracy in cross-dataset scenarios. - The dataset includes extended health metrics such as ECG, blood pressure, and stress levels, addressing limitations of existing datasets. - Public release of the dataset and model should accelerate progress in AI medical assistants. | ['Computer Vision', 'Video Classification', 'Multimodal'] | [Link](https://github.com/ksyegorov/mcd_rppg) | [Link](https://huggingface.co/datasets/kyegorov/mcd_rppg) |
| [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982) | Shilin Yan, Lu Yin, Dilxat Muhtar, Yefan Zhou, Pengxiang Li | - This paper introduces Prophet, a training-free fast decoding paradigm for diffusion language models (DLMs) that leverages the observation of early answer convergence. - Prophet dynamically decides whether to continue refinement or decode all remaining tokens at once, using the confidence gap between the top-2 prediction candidates as a criterion. - Empirical evaluations on LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. - The method integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. - Prophet recasts DLM decoding as a problem of when to stop sampling, demonstrating that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference. | ['Natural Language Processing', 'Text Generation'] | [Link](https://github.com/pixeli99/Prophet) | N/A |
| [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered
  Smartphone Agents](https://arxiv.org/abs/2508.19493) | Yue Yao, Yibo Shi, Shidong Pan, Zhixin Lin, Jungang | - This paper introduces SAPA-Bench, the first large-scale benchmark for evaluating the privacy awareness of smartphone agents, comprising 7,138 real-world scenarios. - The benchmark includes annotations for privacy presence, leakage modality, privacy category, risk severity, and expected risk prompt, enabling a comprehensive evaluation of agents' privacy capabilities. - Five specialized privacy metrics (PRR, PLR, PLAR, PCAR, RA) are defined to quantitatively evaluate agents' privacy understanding and response capabilities. - The evaluation of seven mainstream smartphone agents reveals that almost all agents exhibit unsatisfactory privacy awareness, with performance below 60% even with explicit hints. - Closed-source agents generally outperform open-source ones, highlighting the need for further research and development to enhance the privacy awareness of these agents. | ['Multimodal'] | [Link](https://zhixin-l.github.io/SAPA-Bench) | N/A |
| [AudioStory: Generating Long-Form Narrative Audio with Large Language
  Models](https://arxiv.org/abs/2508.20088) | Yixiao Ge, Shijie Ma, Yuying Ge, Yuxin Guo, wybertwang | - AudioStory is a novel framework that integrates LLMs with TTA systems to generate long-form narrative audio. - It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks, enabling coherent scene transitions and emotional tone consistency. - AudioStory introduces a decoupled bridging mechanism and an end-to-end training approach, improving both audio fidelity and temporal consistency. - Experiments on the AudioStory-10k benchmark show that AudioStory surpasses prior TTA baselines in instruction-following ability and audio fidelity. - AudioStory demonstrates strong performance on both single-audio generation and narrative audio generation tasks. | ['Text-to-Audio', 'Audio-to-Audio', 'Multimodal'] | [Link](https://github.com/TencentARC/AudioStory) | N/A |
| [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229) | Olga Golovneva, Weizhe Yuan, Wenting Zhao, Wei Xiong, sainbar |  - This paper introduces STEPWISER, a novel approach to training stepwise generative judges for large language models (LLMs) that uses reinforcement learning to improve the accuracy and generalization of intermediate reasoning steps.  - STEPWISER addresses the limitations of existing process reward models by reframing the task as a reasoning task, employing a meta-reasoning process that reasons about the policy model's reasoning steps before delivering a final verdict. - The model demonstrates improved judgment accuracy on intermediate steps, facilitates better policy model training, and enhances inference-time search. - STEPWISER is evaluated on ProcessBench, showing significant improvements over existing baselines across multiple dimensions including accuracy on intermediate steps and overall task performance. - Experimental results highlight the effectiveness of the proposed meta-reasoning process, reinforcement learning, and self-segmentation techniques in improving LLM reasoning capabilities. | ['Reinforcement Learning', 'Natural Language Processing'] | N/A | N/A |
| [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and
  Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559) | Chunlei Han, Sida Zhao, Zefang Chu, Ruogu Du, Rongzhi Li | - This paper introduces HeteroScale, a coordinated autoscaling framework designed to address challenges in serving large language models (LLMs) using modern Prefill-Decode (P/D) disaggregated architectures. - HeteroScale combines a topology-aware scheduler that accounts for heterogeneous hardware and network constraints with a novel metric-driven scaling policy. - The core of HeteroScale's policy is based on the finding that Decode Tokens-Per-Second (TPS) is a robust metric for jointly scaling prefill and decode pools, maintaining architectural balance. - When deployed on tens of thousands of GPUs in a production environment, HeteroScale demonstrated significant improvements, including a 26.6 percentage point increase in average GPU utilization and considerable cost savings. - The HeteroScale system's effectiveness is validated through comprehensive experiments across various LLM service types and workloads, demonstrating its robust performance and stability in real-world production settings. | ['Natural Language Processing'] | N/A | N/A |
| [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for
  Generative Research Synthesis](https://arxiv.org/abs/2508.20033) | Ion Stoica, Ankita Sundar, Harshit Gupta, Negar Arabzadeh, Liana Patel |  * The main contribution of this paper is DeepScholar-Bench, a live benchmark and automated evaluation framework for generative research synthesis.  * It addresses the challenge of evaluating generative research synthesis systems by using queries from recent, high-quality ArXiv papers and focusing on the related work section generation task.  * The framework assesses performance across three dimensions: knowledge synthesis, retrieval quality, and verifiability, using metrics that show strong agreement with human judgments.  * DeepScholar-base, a reference pipeline, is introduced and evaluated, showing competitive or higher performance than existing open-source systems and commercial models.  * The benchmark is far from saturated, highlighting the importance and difficulty of generative research synthesis. | ['Natural Language Processing', 'Question Answering', 'Text2Text Generation', 'Summarization'] | [Link](https://github.com/guestrin-lab/deepscholar-bench) | N/A |
