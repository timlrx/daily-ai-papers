

## Papers for 2025-08-29

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [USO: Unified Style and Subject-Driven Generation via Disentangled and
  Reward Learning](https://arxiv.org/abs/2508.18966) | Jiahe Tian, Mengqi Huang, wuwx, cb1cyf, fenfan | - This paper introduces USO, a unified model for style-driven and subject-driven image generation that uses a cross-task co-disentanglement paradigm and reward learning. - The model architecture consists of two stages: style alignment training and content-style disentanglement training, both supervised by a style reward. - USO achieves state-of-the-art performance on both subject and style consistency across multiple benchmarks, outperforming existing methods. - A new benchmark dataset, USO-Bench, is introduced for unified evaluation of style and subject driven generation, comprising style-driven, subject-driven, and combined style-subject generation tasks. - The results of extensive experiments demonstrate that USO significantly outperforms existing methods in terms of subject consistency and style similarity. | ['Text-to-Image', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/bytedance/USO) | N/A |
| [TCIA: A Task-Centric Instruction Augmentation Method for Instruction
  Finetuning](https://arxiv.org/abs/2508.20374) | Simin Ma, kqsong, songwang41, huuuyeah, shujian2025 | - This paper introduces TCIA, a novel task-centric instruction augmentation method designed to enhance instruction finetuning for large language models (LLMs). - TCIA addresses the limitations of existing methods by systematically expanding instructions while maintaining both diversity and task alignment, represented in a discrete query-constraints space. - Experiments demonstrate that TCIA improves the performance of open-source LLMs by an average of 8.7% across four real-world, task-specific applications, surpassing even some leading closed-source models. - The method's effectiveness is attributed to its ability to maintain high instruction diversity across multiple augmentation steps while preserving task relevance. - TCIA's scalability and efficiency make it a promising solution for adapting LLMs to real-world, task-focused applications. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World
  Tasks via MCP Servers](https://arxiv.org/abs/2508.20453) | Shashank Biju, Hemani Patel, Qi Chang, Zhenting Wang, ankits0052 | This paper introduces MCP-BENCH, a benchmark for evaluating large language models (LLMs) on complex, multi-step tasks requiring tool use and cross-tool coordination.  MCP-BENCH connects LLMs to 28 real-world MCP servers with 250 tools across diverse domains.  It evaluates agents on tool schema understanding, multi-hop planning, and cross-domain workflow orchestration, capabilities not fully captured in prior benchmarks.  Experiments on 20 advanced LLMs reveal persistent challenges in these areas, highlighting the need for further research. | ['Natural Language Processing'] | [Link](https://github.com/Accenture/mcp-bench) | N/A |
| [Turning the Spell Around: Lightweight Alignment Amplification via
  Rank-One Safety Injection](https://arxiv.org/abs/2508.20766) | Bernard Ghanem, George Turkiyyah, Hasan Abed Al Kader Hammoud, Harethah Abu Shairah | - This paper introduces RANK-One SafetY INJECTION (ROSI), a novel method for enhancing the safety alignment of Large Language Models (LLMs). - ROSI operates by injecting a safety direction into the model's weights, amplifying the model's tendency to refuse unsafe requests, without requiring any fine-tuning. - The safety direction is derived from a small set of harmful and harmless instruction pairs, making ROSI computationally efficient. - Empirical evaluation demonstrates that ROSI consistently increases safety refusal rates across various LLMs, while preserving model utility on standard benchmarks. - ROSI can also re-align "uncensored" models by amplifying their own latent safety directions, showcasing its effectiveness as a last-mile safety procedure. | ['Natural Language Processing'] | N/A | N/A |
| [Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability
  in Knowledge and Safety with DuET-PD](https://arxiv.org/abs/2508.17450) | Roy Ka-Wei Lee, Nancy F. Chen, Zhengyuan Liu, Daniel Wai Kit Chin, Incomple | This research introduces DuET-PD, a novel framework for evaluating Large Language Models' (LLMs) stance-change dynamics in persuasive dialogues.  DuET-PD assesses LLMs across knowledge and safety domains using a dual-evaluation approach and identifies a concerning trend of increasing sycophancy in newer open-source models.  The study proposes Holistic DPO, a training method which improves LLM robustness to misinformation and receptiveness to corrections, significantly enhancing performance compared to baselines.  The results highlight a critical capability-adaptability trade-off, with larger models exhibiting greater robustness but reduced adaptability, underscoring the need for balanced training approaches. | ['Natural Language Processing'] | [Link](https://github.com/Social-AI-Studio/DuET-PD) | N/A |
| [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn
  Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061) | Alex Endert, Eunyee Koh, Shunan Guo, Adam Coscia | OnGoal is a novel chat interface designed to enhance user experience in multi-turn dialogues with Large Language Models (LLMs) by tracking and visualizing conversational goals.  It addresses challenges in managing evolving goals by providing real-time feedback on goal alignment and visualizing goal progress over time.  The system uses a three-stage pipeline to infer, merge, and evaluate goals against LLM responses, incorporating explanations and examples for evaluation results.  A user study showed that OnGoal improved goal evaluation and review compared to a baseline chat interface, suggesting the value of visualizing goals in multi-turn LLM dialogues. | ['Natural Language Processing'] | N/A | N/A |
| [Provable Benefits of In-Tool Learning for Large Language Models](https://arxiv.org/abs/2508.20755) | Vivien Cabannes, Charles Arnal, Ambroise Odonnat, Sam Houliston | - This paper introduces a novel method for improving large language models (LLMs) by using external tools for factual recall, which it calls in-tool learning. - The authors prove that the number of facts an LLM can memorize in its weights is fundamentally limited by its parameter count, whereas tool use enables unbounded factual recall. - They present a formal circuit construction to support this claim and validate it with controlled experiments where tool-using models consistently outperform memorizing models. - Furthermore, they demonstrate that teaching LLMs to use tools is more effective than fine-tuning facts into memory. - The study offers both theoretical and empirical evidence showing that tool-augmented workflows are not only practical but also provably more scalable. | ['Question Answering'] | [Link](https://github.com/ambroiseodt/itl) | N/A |
