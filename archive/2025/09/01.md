

## Papers for 2025-09-01

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs
  via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113) | Han Hu, Shiming Xiang, Bolin Ni, Qi Yang, Jie Jiang | - This paper introduces R-4B, a novel multimodal large language model (MLLM) that can adaptively decide whether to engage in step-by-step reasoning based on problem complexity. - The model architecture uses a bi-mode annealing approach, training on both thinking and non-thinking examples to enable efficient and flexible reasoning. - Bi-mode Policy Optimization (BPO) is introduced to further incentivize auto-thinking by optimizing the model's decision-making process. - R-4B outperforms existing state-of-the-art models on 25 challenging benchmarks, showing significant improvement in reasoning-intensive tasks while maintaining efficiency. - The model is open-sourced to promote further advancement in MLLMs. | ['Multimodal', 'Question Answering'] | [Link](https://github.com/yannqi/R-4B) | [Link](https://huggingface.co/YannQi/R-4B) |
| [A.S.E: A Repository-Level Benchmark for Evaluating Security in
  AI-Generated Code](https://arxiv.org/abs/2508.18106) | Libo Chen, Lei Zhang, Bin Wang, wanng, KekeLian | - This paper introduces A.S.E., a novel repository-level benchmark designed for evaluating the security of AI-generated code.  Unlike existing benchmarks, A.S.E focuses on real-world repositories with documented CVEs, providing a more comprehensive and realistic evaluation. - The benchmark constructs tasks from real-world repositories, preserving the full context including build systems and cross-file dependencies, ensuring reproducibility and auditability. - A.S.E uses expert-defined rules and a containerized evaluation framework to provide stable assessments of security, build quality, and generation stability, leading to more reliable evaluation results. - The evaluation of several leading LLMs on A.S.E reveals Claude-3.7-Sonnet as the top performer overall, with Qwen3-235B-A22B-Instruct achieving the best security score.  The study highlights a narrow gap between proprietary and open-source models. - The findings also indicate that concise, fast-thinking decoding strategies significantly outperform complex, slow-thinking reasoning methods for security patching, offering valuable insights into efficient code generation strategies. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470) | Qi Jia, Liang Jin, Runze Zhang, Guoguang Du, lixiaochuan |  - This paper introduces Droplet3D, a novel 3D generative model that leverages commonsense priors from videos to enhance 3D generation capabilities.  - The model architecture is based on a video backbone model (DropletVideo) fine-tuned with a large-scale video dataset (Droplet3D-4M) containing multi-view annotations and detailed text descriptions.  - Droplet3D supports both image and dense text input, generating spatially consistent and semantically plausible 3D content.  - Experimental results demonstrate Droplet3D's superior performance compared to existing baselines, showcasing its potential for scene-level applications.  - The authors open-sourced the dataset, code, framework, and model weights. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://dropletx.github.io/) | N/A |
| [TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head
  Synthesis](https://arxiv.org/abs/2508.13618) | Pengcheng Chen, Zihan Ye, Yexin Liu, Hejin Huang, Shunian Chen | - This paper introduces TalkVid, a large-scale, high-quality, and diverse dataset for audio-driven talking head synthesis, containing 1244 hours of video from 7729 unique speakers. - TalkVid addresses the generalization gap of existing datasets by providing data with diversity in ethnicity, language, and age groups, rigorously filtered for motion stability, aesthetic quality, and facial detail. - The authors also introduce TalkVid-Bench, a stratified evaluation set of 500 clips for assessing model fairness and generalization, revealing performance disparities across subgroups. - Experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. - The dataset and code are publicly available at https://github.com/FreedomIntelligence/TalkVid. | ['Video-Text-to-Text', 'Audio-to-Audio', 'Computer Vision'] | [Link](https://github.com/FreedomIntelligence/TalkVid) | N/A |
| [UItron: Foundational GUI Agent with Advanced Perception and Planning](https://arxiv.org/abs/2508.21767) | Yufeng Zhong, Wenkang Han, Liming Zheng, Jing Huang, Zhixiong Zeng | - This paper introduces Ultron, an open-source foundational model for GUI agents that excels in perception, grounding, and planning. - Ultron incorporates advanced data engineering strategies, including data unification, trajectory distillation, and manual annotation to enhance training effectiveness. - The model employs a three-stage training strategy: supervised fine-tuning for perception and planning, followed by curriculum reinforcement learning for complex reasoning and online adaptation. - Ultron significantly surpasses existing methods in benchmark evaluations, including Chinese language scenarios. - The interactive infrastructure developed for Ultron supports scalable data collection, dynamic training, and online evaluation. | ['Reinforcement Learning', 'Multimodal', 'Computer Vision'] | [Link](https://github.com/UITron-hub/UItron) | N/A |
| [TiKMiX: Take Data Influence into Dynamic Mixture for Language Model
  Pre-training](https://arxiv.org/abs/2508.17677) | Jiyao Deng, Yuanfan Guo, Fengze Liu, Binbin Liu, Yifan Wang | - This paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture during language model pre-training according to the model's evolving preferences. - TiKMiX uses a new metric called Group Influence to efficiently evaluate the impact of data domains on the model, enabling dynamic adjustment of the data mixture. - The method is evaluated using two approaches: TiKMiX-D for direct optimization and TiKMiX-M which utilizes a regression model to predict a superior mixture. - Experiments show that TiKMiX-D outperforms state-of-the-art methods like REGMIX while using significantly fewer computational resources, and TiKMiX-M achieves an average performance gain of 2% across multiple benchmarks. - The results demonstrate that dynamically adjusting the data mixture based on evolving preferences significantly improves performance. | ['Natural Language Processing'] | N/A | N/A |
| [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290) | Han Xiao, Scott Martens, Michael Günther, Saba Sturua, dariakryvosheieva | - This paper introduces *jina-code-embeddings*, a novel code embedding model suite designed for code retrieval from natural language queries, question answering, and identifying semantically similar code snippets across programming languages. - The model suite leverages an autoregressive decoder backbone pre-trained on text and code, generating embeddings via last-token pooling, and incorporates several key innovations including task-specific training strategies. - *jina-code-embeddings* demonstrates state-of-the-art performance despite its relatively small size (0.5B and 1.5B parameters), outperforming other models of comparable size and some larger alternatives. - The models are evaluated on various benchmarks including MTEB-COIR, showing significant improvements across different code retrieval tasks. - The authors explore different pooling techniques and training procedures to optimize the model's performance, providing insights into effective strategies for code embedding model construction. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering', 'Feature Extraction'] | N/A | N/A |
| [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456) | Amy Pavel, Jeffrey P. Bigham, Dingzeyu Li, Yi-Hao Peng |  - This paper introduces Morae, a novel accessible UI agent designed to proactively pause automation at critical decision points, empowering blind and low-vision users to actively participate in the process.  - Morae leverages a large multimodal model to interpret user queries and analyze UI representations, dynamically identifying ambiguous choices.  - The system uses a dynamic verification of ambiguous choices mechanism to pause when needed and generates interactive UIs to facilitate user input and clarification.  - In a user study, Morae outperformed existing agents such as OpenAI Operator, significantly improving task success rates and empowering users to make more informed and preference-aligned choices.  - Results from a technical evaluation demonstrated that Morae’s approach of dynamically verifying ambiguities surpasses baselines in terms of both task completion and pausing performance. | ['Multimodal'] | N/A | N/A |
| [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376) | Siwei Yang, Zijun Wang, Chi Heem Wong, Haoqin Tu, Tony Lee | This paper introduces AHELM, a holistic benchmark for evaluating audio-language models.  AHELM addresses the limitations of existing benchmarks by aggregating diverse datasets and evaluating models across ten key aspects (audio perception, knowledge, reasoning, etc.).  Two new datasets, PARADE and CoRe-Bench, are introduced to evaluate specific capabilities. Results demonstrate that while Gemini 2.5 Pro performs well overall, there is no single dominant model. The AHELM framework and data are publicly available for transparency and future development. | ['Audio', 'Automatic Speech Recognition', 'Audio Classification'] | [Link](https://github.com/stanford-crfm/helm) | [Link](https://huggingface.co/datasets/UCSC-VLAA/PARADE_audio), [Link](https://huggingface.co/datasets/stanford-crfm/CoReBench_v1) |
| [CLIPSym: Delving into Symmetry Detection with CLIP](https://arxiv.org/abs/2508.14197) | Raymond A. Yeh, Md Ashiqur Rahman, Tinghan Yang | - The paper introduces CLIPSym, a novel framework for reflection and rotation symmetry detection that leverages the pre-trained CLIP model. - CLIPSym utilizes CLIP's image and text encoders and incorporates a rotation-equivariant decoder based on Transformers and G-Convolution. - A novel prompting technique, Semantic-Aware Prompt Grouping (SAPG), is introduced to enhance the model's understanding of symmetry by aggregating diverse object-based prompts. - Empirical evaluation on three standard symmetry detection datasets demonstrates that CLIPSym outperforms the current state-of-the-art. - Ablation studies validate the contributions of CLIP's pre-training, the equivariant decoder, and the SAPG technique. | ['Computer Vision', 'Object Detection', 'Image-to-Image', 'Multimodal'] | [Link](https://github.com/timyoung2333/CLIPSym) | N/A |
