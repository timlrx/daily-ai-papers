

## Papers for 2025-09-04

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Open Data Synthesis For Deep Research](https://arxiv.org/abs/2509.00375) | Zheng Liu, Hongjin Qian, Kun Luo, ZiyiXia | - This paper introduces InfoSeek, a novel framework for synthesizing complex deep research tasks, which are formalized as Hierarchical Constraint Satisfaction Problems (HCSPs). - InfoSeek uses a dual-agent system to recursively build research trees from large-scale webpages, converting these trees into natural language questions. - Experiments show that models trained on InfoSeek consistently outperform strong baselines, including surpassing much larger models and lightweight commercial APIs on the BrowseComp-Plus benchmark. - InfoSeek addresses the limitations of existing benchmarks by generating datasets with controllable complexity, preventing shortcut reasoning, and ensuring verifiable answers. - The framework is open-source and includes over 50K training examples and a curated test set. | ['Question Answering'] | [Link](https://github.com/VectorSpaceLab/InfoSeek) | N/A |
| [LMEnt: A Suite for Analyzing Knowledge in Language Models from
  Pretraining Data to Representations](https://arxiv.org/abs/2509.03405) | Yoav Gur-Arieh, Ido Cohen, Alon Gilae-Dotan, Daniela Gottesman, mega |  - This paper introduces LMEnt, a suite of resources designed to analyze knowledge acquisition in Language Models (LMs). - LMEnt comprises a knowledge-rich pretraining corpus (annotated with entity mentions from English Wikipedia), an efficient entity-based retrieval method, and twelve pretrained models (with varying parameter counts and intermediate checkpoints). - The entity-based retrieval method significantly outperforms previous string-based approaches, achieving up to 80.4% improvement in relevant document retrieval. - Through experiments using LMEnt, the authors demonstrate that fact frequency is a key factor in knowledge acquisition but does not fully explain learning dynamics. - LMEnt is publicly available via GitHub and Hugging Face, enabling further research on knowledge representation, plasticity, editing, attribution, and learning dynamics in LMs. | ['Question Answering'] | [Link](https://github.com/LMEnt) | [Link](huggingface.co/LMEnt) |
| [Mixture of Global and Local Experts with Diffusion Transformer for
  Controllable Face Generation](https://arxiv.org/abs/2509.00428) | Kai Li, Yue Li, Xing Fu, Shun Zhang, Xuechao Zou | - This paper introduces Face-MoGLE, a novel framework for high-quality and controllable face generation that uses a Mixture of Global and Local Experts (MoGLE) with a Diffusion Transformer. - The model architecture consists of semantic-decoupled latent modeling through mask-conditioned space factorization, a mixture of global and local experts, and a dynamic gating network. - Face-MoGLE supports text-to-face, mask-to-face, and multimodal face generation, achieving state-of-the-art results on several benchmark datasets. - The authors demonstrate Face-MoGLE's superior performance on high-quality and controllable face generation compared to existing methods. - The model also exhibits robust zero-shot generalization capabilities, making it a flexible and powerful tool for various applications. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Mask Generation'] | [Link](https://github.com/XavierJiezou/Face-MoGLE) | N/A |
