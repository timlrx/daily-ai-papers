

## Papers for 2025-09-05

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth](https://arxiv.org/abs/2509.03867) | Chi-Li Chen, Zi Yan Chang, Chia-Yi Hsiao, Chenghao Xiao, Yang Wang | - This paper introduces Drivelology, a novel linguistic phenomenon characterized by utterances that are syntactically coherent but pragmatically paradoxical, and proposes a benchmark dataset (DRIVELHUB) to evaluate LLMs' ability to understand such nuanced language. - DRIVELHUB contains over 1200 examples of Drivelological text in multiple languages (English, Mandarin, Spanish, French, Japanese, Korean), each meticulously curated and annotated with implicit meaning. - The authors evaluate several LLMs on four tasks: Drivelology Detection, Drivelology Tagging, Implicit Narrative Writing, and Narrative Selection, revealing clear limitations in LLMs' understanding of nuanced semantics. - Their findings challenge the assumption that statistical fluency in LLMs implies true cognitive comprehension, highlighting the importance of deeper pragmatic understanding. - The DRIVELHUB dataset and code are publicly available to facilitate further research on modelling linguistic depth beyond surface-level coherence. | ['Natural Language Processing'] | [Link](https://github.com/ExtraordinaryLab/drivelology) | [Link](https://huggingface.co/datasets/extraordinarylab/drivel-hub) |
| [Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow
  Real Instructions?](https://arxiv.org/abs/2509.04292) | Yu Fu, Ruijie Miao, Xinping Lei, Qinyan Zhang, zhangysk | - This paper introduces Inverse IFEval, a new benchmark designed to evaluate Large Language Models' (LLMs) ability to overcome ingrained training biases and follow counterintuitive instructions. - Inverse IFEval comprises eight categories of challenging instructions, including question correction, intentional textual flaws, and code without comments, systematically inverting conventional training paradigms. - The benchmark uses a human-in-the-loop pipeline to create a dataset of 1012 high-quality questions in Chinese and English across 23 diverse domains. - Experiments on leading LLMs demonstrate the benchmark's effectiveness in identifying models' limitations in handling unconventional instructions. - The authors hope that Inverse IFEval serves as a diagnostic tool and encourages the development of methods to mitigate cognitive inertia and enhance LLMs' instruction-following reliability. | ['Natural Language Processing', 'Question Answering'] | N/A | [Link](https://huggingface.co/datasets/m-a-p/Inverse_IFEval) |
| [DeepResearch Arena: The First Exam of LLMs' Research Abilities via
  Seminar-Grounded Tasks](https://arxiv.org/abs/2509.01396) | Jiaxuan Lu, Meiqi Tu, Junchi Yu, Chen Yang, haiyuanwan | - This paper introduces DeepResearch Arena, a novel benchmark designed to evaluate the research abilities of large language models (LLMs) using seminar-grounded tasks. - The benchmark is constructed using a Multi-Agent Hierarchical Task Generation (MAHTG) system, which extracts research-worthy inspirations from seminar transcripts and translates them into high-quality research tasks. - DeepResearch Arena includes over 10,000 high-quality research tasks from over 200 academic seminars, spanning 12 disciplines. - The evaluation results show that DeepResearch Arena presents substantial challenges for current state-of-the-art LLMs, with clear performance gaps observed across different models. - The benchmark is designed to reduce data leakage and better reflect real-world research environments. | ['Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware
  Embeddings](https://arxiv.org/abs/2509.04011) | Oren Glickman, Yoav Goldberg, Uri Katz, Or Shachar | - This paper introduces NER Retriever, a novel zero-shot retrieval framework for ad-hoc Named Entity Retrieval (NER). - The model leverages internal representations from large language models (LLMs) to embed both entity mentions and user-provided type descriptions into a shared semantic space. - It employs a lightweight contrastive projection network to refine these representations, aligning type-compatible entities while separating unrelated types. - NER Retriever significantly outperforms lexical and dense sentence-level retrieval baselines across three benchmarks, demonstrating the effectiveness of representation selection within LLMs and providing a practical solution for scalable, schema-free entity retrieval. - The proposed method generalizes to novel entity types without requiring task-specific LLM fine-tuning. | ['Natural Language Processing', 'Zero-Shot Classification'] | [Link](https://github.com/ShacharOr100/ner_retriever) | [Link](https://huggingface.co/CascadeNER/models_for_CascadeNER) |
| [Delta Activations: A Representation for Finetuned Large Language Models](https://arxiv.org/abs/2509.04442) | Ser-Nam Lim, Mayur Naik, Amish Sethi, OscarXZQ | - This paper introduces Delta Activations, a novel method for representing fine-tuned large language models (LLMs) as vector embeddings. - Delta Activations measures the shifts in internal activations of a fine-tuned model relative to a base model, enabling effective clustering by domain and task. - The method demonstrates desirable properties, such as robustness across different finetuning settings and an additive property when combining datasets. - Delta Activations can be used for model selection and merging, and it is shown to outperform baseline methods in clustering quality based on silhouette score. - The code for Delta Activations is publicly available on GitHub. | ['Natural Language Processing'] | [Link](https://github.com/OscarXZQ/delta_activations) | N/A |
| [False Sense of Security: Why Probing-based Malicious Input Detection
  Fails to Generalize](https://arxiv.org/abs/2509.03888) | Muhao Chen, Qin Liu, Zeming Wei, Cheng Wang | - This paper reveals that probing-based methods for detecting malicious inputs in large language models (LLMs) are unreliable and fail to generalize to out-of-distribution data. - The authors demonstrate that these probing classifiers learn superficial patterns, such as n-grams and trigger words, rather than capturing the underlying semantic harmfulness of the input. - Controlled experiments using semantically cleaned datasets and paraphrased inputs confirm that probing classifiers over-rely on surface-level features and fail to generalize. -  A comparison with simple n-gram based methods shows similar performance, further highlighting the limitations of probing-based approaches. - The study concludes that a redesign of both models and evaluation protocols is necessary to enhance the safety and reliability of LLM safety detection. | ['Text Classification'] | [Link](https://github.com/WangCheng0116/Why-Probe-Fails) | N/A |
