

## Papers for 2025-09-10

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual
  Search](https://arxiv.org/abs/2509.07969) | Tianjian Li, Tao Liu, Wei Li, Junyi Li, Xin Lai | - Mini-o3 is a novel system designed to overcome limitations in existing open-source visual search approaches by scaling up tool-based interactions and enabling deep, multi-turn reasoning. - It achieves state-of-the-art performance on challenging visual search tasks by employing a multi-stage training process that includes constructing a new Visual Probe Dataset, developing an iterative data collection pipeline, and using an over-turn masking strategy to balance training efficiency and test-time scalability.  - The model generates trajectories with tens of interaction turns, showcasing improved accuracy as the number of turns increases, despite being trained with a limited budget of six turns.  - Experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems and outperforming existing methods on various benchmarks. - The over-turn masking technique is crucial for enabling the scalability of the interaction turns during inference without compromising test-time performance. | ['Multimodal', 'Reinforcement Learning', 'Visual Question Answering'] | [Link](https://github.com/Mini-o3/Mini-o3) | N/A |
| [Visual Representation Alignment for Multimodal Large Language Models](https://arxiv.org/abs/2509.07979) | Heeseong Shin, Hyungyu Choi, Junwan Kim, Jaewoo Jung, Heeji Yoon | - This paper introduces VIRAL, a novel regularization strategy that aligns the internal visual representations of Multimodal Large Language Models (MLLMs) with those of pre-trained Vision Foundation Models (VFMs). - VIRAL addresses the limitation of existing MLLMs in vision-centric tasks by explicitly enforcing the alignment, allowing the model to retain critical visual details and enhance its reasoning capabilities over complex visual inputs. - The experimental results demonstrate consistent improvements across various vision-centric and general multimodal benchmarks, showcasing the effectiveness of VIRAL. - Ablation studies validate the key design choices of VIRAL, including the selection of VFMs, alignment target layers, and alignment objectives. - The authors suggest that this simple regularization strategy opens up a new direction for the effective integration of visual information in training MLLMs. | ['Multimodal'] | [Link](https://cvlab-kaist.github.io/VIRAL) | N/A |
| [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295) | XuDong Wang, Luke Zettlemoyer, Trevor Darrell, Ji Xie | This paper introduces Reconstruction Alignment (RecA), a resource-efficient post-training method that enhances image generation and editing in Unified Multimodal Models (UMMs). RecA leverages visual understanding encoder embeddings as dense prompts, providing rich supervision without captions. The method conditions a UMM on its visual understanding embeddings and optimizes it using a self-supervised reconstruction loss.  Experiments show that RecA improves image generation and editing performance across various UMM architectures, outperforming larger open-source models on several benchmarks including GenEval and DPGBench.  RecA is broadly applicable to autoregressive, masked autoregressive, and diffusion-based UMMs. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Text'] | [Link](https://reconstruction-alignment.github.io/) | N/A |
| [Curia: A Multi-Modal Foundation Model for Radiology](https://arxiv.org/abs/2509.06830) | Elodie Ferreres, Helene Philippe, Antoine Saporta, Julien Khlaut, Corentin Dancette |  - The paper introduces Curia, a multi-modal foundation model for radiology trained on a large dataset of CT and MRI images using the DINOv2 algorithm.  - Curia uses a Vision Transformer architecture and demonstrates strong performance on a benchmark of 19 radiology tasks, surpassing previous foundation models and often matching or exceeding radiologist performance.  - The model exhibits strong cross-modal generalization abilities, effectively transferring knowledge between CT and MRI modalities.  - The model achieves high accuracy in low-data regimes, showing data efficiency in few-shot learning scenarios.  - The authors release the base model's weights to accelerate research progress in radiology. | ['Image-to-Image', 'Image Classification', 'Image Segmentation', 'Image Feature Extraction', 'Zero-Shot Image Classification', 'Multimodal'] | N/A | [Link](https://huggingface.co/raidium/curia) |
| [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414) | Vijai Mohan, Yuandong Tian, Qi Ma, Mengting Gu, Jakub Grudzien Kuba | - This paper introduces Language Self-Play (LSP), a reinforcement learning approach that allows large language models (LLMs) to improve without additional training data. - LSP leverages a game-theoretic framework of self-play, where an LLM acts as both the challenger (generating increasingly difficult prompts) and the solver (answering the prompts). - Experiments using Llama-3.2-3B-Instruct on instruction-following benchmarks demonstrated that LSP can enhance model performance, sometimes exceeding data-driven baselines. - The LSP algorithm iteratively improves both the LLM and the distribution of examples it learns from, without requiring an adversarial expert. - This data-free training method is particularly useful for overcoming the data bottleneck in LLM training and improving models on challenging tasks. | ['Natural Language Processing', 'Reinforcement Learning', 'Text Generation'] | N/A | N/A |
| [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301) | Quanquan Gu, Huizhuo Yuan, Peng Sun, Zhuoqing Song |  - This paper introduces CASTLE, a novel attention mechanism that incorporates information from future tokens into the attention mechanism while maintaining the autoregressive property. - CASTLE continually updates each token's keys as the context unfolds, allowing the model to access information from subsequent tokens. - The authors derive a mathematical equivalence that enables efficient parallel training, avoiding the explicit materialization of lookahead keys. - Experimental results on various language modeling benchmarks show that CASTLE consistently outperforms standard causal attention across different model scales, reducing validation perplexity and improving performance on downstream tasks. - The improved performance is attributed to CASTLE's ability to capture global context more effectively than standard causal attention. | ['Natural Language Processing', 'Text Generation'] | N/A | N/A |
| [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric
  Knowledge](https://arxiv.org/abs/2509.07968) | Dipanjan Das, Sasha Goldshtein, Giovanni D'Antonio, Gal Yona, Lukas Haas | - This paper introduces SimpleQA Verified, a new benchmark dataset designed to evaluate the factuality of large language models (LLMs) in short-answer questions. - It addresses limitations of the original SimpleQA benchmark, such as noisy labels, biases, and question redundancy, by applying a rigorous multi-stage filtering process. - SimpleQA Verified achieves a state-of-the-art F1-score of 55.6 with Gemini 2.5 Pro, outperforming other leading models such as GPT-5. - The dataset, evaluation code, and leaderboard are publicly available to foster progress toward more reliable AI systems. - This work provides a higher-fidelity tool to track progress in parametric model factuality and mitigate hallucinations. | ['Question Answering'] | N/A | N/A |
