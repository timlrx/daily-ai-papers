

## Papers for 2025-09-11

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [<think> So let's replace this phrase with insult... </think> Lessons
  learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358) | Alexander Panchenko, Daniil Moskovskiy, Sergey Pletenev | - This paper investigates the use of large language models (LLMs) to generate synthetic toxic data for training text detoxification models. - Experiments using Llama 3 and Qwen activation-patched models show that models trained on synthetic data perform worse than those trained on human-annotated data, with up to a 30% drop in performance. - The key limitation identified is the lack of lexical diversity in LLM-generated toxic text, which uses repetitive vocabulary compared to the nuanced language of human-generated toxic data. - These results highlight the current limitations of LLMs in generating diverse and nuanced toxic text for training detoxification models. - The paper emphasizes the continued importance of using human-annotated data for building robust and effective detoxification systems. | ['Text Generation'] | [Link](https://github.com/AlexRey/Lessons-from-Generating-Toxic-Texts) | N/A |
| [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI
  Assistants](https://arxiv.org/abs/2509.08494) | Jacy Reese Anthis, Jacob Haimes, Daniel Samuelson, Benjamin Sturgeon |  - This paper introduces HUMANAGENCYBENCH (HAB), a novel benchmark for evaluating human agency support in AI assistants.  - HAB is scalable and adaptive, using large language models (LLMs) to simulate user queries and evaluate AI responses across six dimensions of human agency.  - The six dimensions measure AI's tendency to ask clarifying questions, avoid value manipulation, correct misinformation, defer important decisions, encourage learning, and maintain social boundaries.  - HAB reveals low-to-moderate agency support in contemporary LLMs, with substantial variation across developers and dimensions.  - The authors encourage future research and the development of more robust safety and alignment targets for LLMs. | ['Natural Language Processing'] | [Link](https://github.com/BenSturgeon/HumanAgencyBench/) | [Link](string) |
