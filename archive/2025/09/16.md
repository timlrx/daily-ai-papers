

## Papers for 2025-09-16

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](https://arxiv.org/abs/2509.12201) | Yang Zhou, MingyuLiu, Xxxy13, lizizun, ZhouTimeMachine | - This paper introduces OmniWorld, a large-scale, multi-domain, and multi-modal dataset designed for 4D world modeling.  - The dataset surpasses existing public synthetic datasets in both modal diversity and data scale.  - The paper proposes a new benchmark for 3D geometric foundation models and camera control video generation, revealing limitations in current state-of-the-art (SOTA) approaches.  - Fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld's value as a training resource.  - The dataset includes over 96K video clips and more than 18M frames with multiple modalities. | ['Multimodal', 'Computer Vision', 'Depth Estimation', 'Image-to-Video', 'Video Classification', 'Text-to-Video', 'Mask Generation'] | N/A | N/A |
| [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986) | Ivan VuliÄ‡, Caiqi Zhang, Chengzu Li, Raphael Tang, lyan62 | - This paper introduces two novel methods to quantify information loss in vision-language model (VLM) connectors: k-nearest neighbors overlap ratio and patch-level embedding reconstruction. - The k-nearest neighbors overlap ratio measures how well the local geometry of visual representations is preserved before and after projection through the connector, while patch-level embedding reconstruction allows for the localization of information loss at the image patch level. - Experiments across six datasets reveal that connectors substantially distort the local geometry of visual representations and cause significant information loss, which correlates with performance degradation on downstream tasks. - The patch-level analysis provides insights into model behavior by identifying areas of high information loss that reliably predict instances where models struggle. - The authors suggest that future work could explore designing dynamic projection layers or better visual feature selection mechanisms for modality fusion. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/lyan62/vlm-info-loss) | N/A |
| [CognitiveSky: Scalable Sentiment and Narrative Analysis for
  Decentralized Social Media](https://arxiv.org/abs/2509.11444) | Subasish Das, Anandi Dutta, gauravfs-14 | - CognitiveSky is an open-source, scalable framework designed for real-time sentiment, emotion, and narrative analysis of decentralized social media platforms, specifically Bluesky. - It leverages transformer-based models (ROBERTa and DistilRoBERTa) to annotate user-generated content and produces structured JSON summaries, visualized via a dynamic dashboard. -  The system is built on free-tier infrastructure (Oracle Cloud Virtual Machine, Supabase, Turso), achieving both low operational cost and high accessibility. -  CognitiveSky demonstrates capabilities in detecting evolving patterns in emotion, activity, and conversation topics, shown through a case study on mental health discourse. - Its modular design allows for adaptation across various domains including disinformation detection, crisis response, and civic sentiment analysis. | ['Natural Language Processing', 'Text Classification', 'Summarization', 'Text Generation'] | [Link](https://github.com/gauravfs-14/CognitiveSky) | N/A |
| [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language
  Models](https://arxiv.org/abs/2509.12132) | Shuo Ren, Chen Wang, Wei Sun, Junhong Wu, Pu Jian | - This paper introduces Reflection-V, a novel visual reasoning model that enhances visual reflection capabilities by leveraging a two-stage training strategy. - The first stage involves constructing vision-centered reasoning data using an agent that interacts between Vision-Language Models (VLMs) and Large Language Models (LLMs), enabling cold-start learning of visual reflection patterns. - The second stage employs a visual attention-based reward model during reinforcement learning (RL) to encourage reasoning based on visual information. - Experiments on multiple visual reasoning benchmarks demonstrate that Reflection-V achieves significant improvements and maintains a stronger, more consistent reliance on visual information during reasoning. - The results indicate that Reflection-V effectively enhances visual reflection capabilities compared to existing methods. | ['Multimodal', 'Visual Question Answering'] | [Link](https://github.com/jian0805/Reflection-V) | N/A |
| [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648) | UVSKKR | - This paper introduces EthicsMH, a pilot benchmark dataset designed to evaluate AI systems' ethical reasoning capabilities in mental health contexts. - EthicsMH comprises 125 scenarios with structured annotations encompassing multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. - The dataset addresses ethical challenges unique to mental health, such as confidentiality, autonomy, beneficence, and bias, which are often neglected in existing benchmarks. - EthicsMH's structured schema allows for a multi-faceted evaluation of AI systems, going beyond simple accuracy assessments to encompass explanation quality and alignment with professional norms. - The authors aim for EthicsMH to serve as a seed resource that can be expanded upon through community and expert contributions, fostering the development of more responsible AI systems in mental health. | ['Natural Language Processing', 'Text Classification'] | N/A | [Link](https://huggingface.co/datasets/EthicsMH) |
| [PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits](https://arxiv.org/abs/2509.11362) | Zhenhao Chen, Guangyi Chen, Minghao Fu, Wong Yu Kang, Loka Li |  - This paper introduces PersonaX, a novel multimodal dataset designed for analyzing human behavior traits.  - PersonaX combines LLM-inferred behavioral traits (Big Five personality dimensions) with visual (facial images) and biographical features. - The dataset includes two subsets: CelebPersona (9444 public figures) and AthlePersona (4181 athletes).  - The paper proposes a two-level analysis framework, including structured statistical analysis and a novel causal representation learning method for uncovering relationships between modalities.  - Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed approach. | ['Multimodal', 'Image-to-Text', 'Text2Text Generation', 'Natural Language Processing', 'Feature Extraction'] | N/A | [Link](huggingface.co/datasets/Persona-X/celebpersona), [Link](huggingface.co/datasets/Persona-X/athlepersona) |
| [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844) | Yixuan Tang, yixuantt | - This paper introduces GAPrune, a novel pruning framework for domain-aware embedding models that prioritizes both domain-specific importance and the preservation of general linguistic capabilities. - GAPrune utilizes Fisher Information to assess parameter importance for domain-specific tasks and gradient cosine similarity to evaluate the alignment of parameters across general and domain-specific objectives. - The framework incorporates a Domain Alignment Importance (DAI) scoring mechanism to balance these two aspects, enabling principled trade-offs between model compression and domain expertise preservation. - Experimental results demonstrate that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity while outperforming all baselines. - After retraining, GAPrune achieves significant performance improvements (+4.51% on FinMTEB and +1.73% on ChemTEB), showcasing that the pruning strategy not only preserves but also enhances domain-specific capabilities. | ['Natural Language Processing', 'Text Classification', 'Sentence Similarity'] | [Link](https://github.com/yixuantt/GAPrune) | N/A |
