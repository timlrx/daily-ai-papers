

## Papers for 2025-09-19

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform
  Data](https://arxiv.org/abs/2509.15221) | Zehao Li, QiushiSun, heroding77, ownerEli, zyliu | This paper introduces ScaleCUA, a large-scale dataset of computer use agent (CUA) interactions across six operating systems and three task domains. The dataset is created using a closed-loop pipeline combining automated agents and human experts.  ScaleCUA models, trained on this dataset, significantly outperform existing baselines on various GUI-centric benchmarks.  The authors achieve state-of-the-art results on several benchmarks, demonstrating the effectiveness of their cross-platform data and models. The ScaleCUA dataset, models, and code are publicly released. | ['Multimodal'] | [Link](https://github.com/OpenGVLab/ScaleCUA) | N/A |
| [Reasoning over Boundaries: Enhancing Specification Alignment via
  Test-time Delibration](https://arxiv.org/abs/2509.14760) | Zhilin Wang, Dongrui Liu, Xuyang Hu, Yafu Li, zzzhr97 |  - This paper introduces ALIGN3, a lightweight method for enhancing specification alignment in LLMs using Test-Time Deliberation (TTD).  - ALIGN3 employs hierarchical reflection and revision to reason over specification boundaries, improving both safety and helpfulness.  - The paper also introduces SPECBENCH, a new benchmark for evaluating specification alignment across five diverse scenarios, 103 specifications, and 1500 prompts.  - Experimental results demonstrate that ALIGN3 outperforms existing TTD methods, achieving a significant improvement in specification alignment with minimal overhead.  - The findings highlight the effectiveness of test-time deliberation for improving specification alignment and provide a valuable resource for future research in this area. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | N/A | N/A |
| [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial
  Search and Reasoning](https://arxiv.org/abs/2509.13160) | Jiashuo Liu, Jianpeng Jiao, Liang Hu, WenhaoHuang, zhangysk |  - FinSearchComp, a new benchmark for evaluating financial search and reasoning capabilities of LLM-based agents, is introduced.  - It comprises three tasks mirroring real-world analyst workflows: Time-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation.  - 70 professional financial experts contributed to annotation and a rigorous multi-stage quality-assurance pipeline.  - Results show that Grok 4 (web) outperforms other models on a global subset while Doubao (web) leads on the Greater China subset.  - The study highlights the significance of equipping agents with web search and financial plugins, emphasizing the importance of end-to-end evaluation for complex financial search and reasoning. | ['Question Answering'] | [Link](https://randomtutu.github.io/FinSearchComp/) | [Link](https://huggingface.co/ByteSeedXpert/FinSearchComp/) |
| [AToken: A Unified Tokenizer for Vision](https://arxiv.org/abs/2509.14476) | Mingze Xu, Liangchen Song, afshin525, byeongjooahn, Jiasenlu |  - ATOKEN is a novel unified visual tokenizer that processes images, videos, and 3D assets within a shared 4D latent space, enabling both high-fidelity reconstruction and semantic understanding across diverse visual modalities. - It employs a pure transformer architecture with 4D rotary position embeddings, addressing format discrepancies across modalities and achieving state-of-the-art reconstruction quality through an adversarial-free training objective combining perceptual and Gram matrix losses. - The model utilizes a progressive training curriculum, starting from single images and gradually expanding to videos and 3D data, and supports both continuous and discrete latent tokens. - In downstream applications, ATOKEN enables both visual generation tasks (image generation, text-to-video generation, image-to-3D synthesis) and understanding tasks (multimodal LLMs), achieving competitive performance on various benchmarks. - This unified approach addresses limitations of existing tokenizers that specialize in either reconstruction or understanding for single modalities, and the superior results demonstrate the potential of unified visual tokenization for next-generation multimodal AI systems. | ['Multimodal'] | N/A | N/A |
| [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model
  via Training-Free Guidance](https://arxiv.org/abs/2509.15130) | Ruibo Li, Tong Zhao, ChiZhang, 2hiTee, ChenxiSong | - WorldForge is a novel training-free framework for controllable 3D/4D video generation that leverages pre-trained video diffusion models. - It consists of three tightly coupled modules: Intra-Step Recursive Refinement, Flow-Gated Latent Fusion, and Dual-Path Self-Corrective Guidance, which enable precise trajectory injection and high-quality outputs without retraining. - WorldForge achieves superior performance compared to state-of-the-art methods in both 3D scene generation and dynamic 4D scene re-rendering tasks, as demonstrated by extensive experiments. - The framework is model-agnostic, readily adapting to different video diffusion models and achieving consistent improvements across multiple datasets and benchmarks. - The approach offers a new plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence. | ['Image-to-Video', 'Text-to-Video', 'Video Classification', 'Multimodal'] | [Link](https://worldforge-agi.github.io) | N/A |
| [Unleashing the Potential of Multimodal LLMs for Zero-Shot
  Spatio-Temporal Video Grounding](https://arxiv.org/abs/2509.15178) | Rynson W. H. Lau, Gerhard Hancke, yuhaoliu, zaiquan | - This paper proposes a novel zero-shot framework for spatio-temporal video grounding (STVG) that leverages multimodal large language models (MLLMs). - The framework introduces two novel strategies: decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS). - DSTH decomposes the query into attribute and action sub-queries to improve the reasoning capabilities of MLLMs, while TAS enhances the temporal consistency of the spatial grounding. - The proposed method outperforms state-of-the-art (SOTA) methods on three common STVG benchmarks (HCSTVG v1 & v2, and VidSTG). - The code for the proposed method will be available on Github. | ['Video-Text-to-Text', 'Zero-Shot Object Detection', 'Multimodal'] | [Link](https://github.com/zaiquanyang/LLaVA_Next_STVG) | N/A |
| [Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question
  Answering with LLMs](https://arxiv.org/abs/2509.15020) | Katharina von der Wense, MinhDucBui, mario-sanz | - This paper investigates the impact of tokenization choices on the performance of large language models (LLMs) in multiple-choice question answering (MCQA). - The authors find that the seemingly trivial decision of whether to tokenize the space after the colon in the prompt "Answer:" significantly impacts accuracy, with a difference of up to 11% observed in experiments. - The recommended strategy is to tokenize the space together with the answer letter, which consistently improves performance across various LLMs and datasets. - This finding underscores the importance of careful evaluation design, highlighting the need for standardized and transparent evaluation protocols. - The study also shows that this tokenization strategy improves model calibration, making the model's confidence estimates more reliable. | ['Question Answering'] | N/A | N/A |
| [EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal
  Ultrasound Intelligence](https://arxiv.org/abs/2509.14977) | Qinghua Huang, WeiWang, lidachen, Ruimed, chaoyinshe | EchoVLM is a novel vision-language model designed for universal ultrasound intelligence.  It utilizes a Mixture-of-Experts (MoE) architecture trained on a large-scale, multi-organ ultrasound dataset.  EchoVLM achieves significant performance improvements compared to existing methods in multiple tasks such as report generation and visual question answering, as demonstrated by experimental results showing substantial gains in BLEU-1 and ROUGE-1 scores. The model's effectiveness stems from its dynamic routing mechanism and task-specific expert subnetworks. The source code and model weights are publicly available. | ['Multimodal', 'Image-Text-to-Text', 'Visual Question Answering'] | [Link](https://github.com/Asunatan/EchoVLM) | N/A |
