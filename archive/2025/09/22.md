

## Papers for 2025-09-22

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [RPG: A Repository Planning Graph for Unified and Scalable Codebase
  Generation](https://arxiv.org/abs/2509.16198) | Steven Liu, Xin Zhang, Kyleraha, Cipherxzc, Luo2003 | This paper introduces a novel approach to codebase generation using a Repository Planning Graph (RPG). The RPG unifies proposal and implementation levels of planning, enabling scalable and unified repository generation.  Experiments on a benchmark dataset (RepoCraft) show that the proposed method (ZeroRepo), outperforms existing baselines, generating larger and more functionally complete repositories.  ZeroRepo also demonstrates near-linear scaling in functionality and code size, further highlighting the efficiency of the RPG.  Finally, the RPG improves LLMs' understanding of repositories, improving agent localization. | ['Text Generation'] | [Link](null) | [Link](null) |
| [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid
  Vision Tokenizer](https://arxiv.org/abs/2509.16197) | jialingt, haosoul122, haotiz, bpan, FrozzZen | - Manzano is a novel unified multimodal large language model (MLLM) that integrates both understanding and generation capabilities using an autoregressive approach. - The model architecture consists of a hybrid vision tokenizer that produces both continuous and discrete visual representations, a unified LLM decoder that predicts the next discrete image or text tokens, and an image decoder that renders pixels. - Manzano significantly mitigates the task conflict often present in unified models through the use of a shared visual encoder and two lightweight specialized adapters (continuous for understanding, discrete for generation) that originate from the same encoder. - The model achieves state-of-the-art results among unified models on various image understanding and generation benchmarks, showing competitive performance against specialist models, particularly on text-rich evaluations. - The authors demonstrate consistent gains from scaling the model size, validating the design choice of a hybrid tokenizer and highlighting the minimal task conflicts under the joint training recipe. | ['Multimodal', 'Text-to-Image', 'Image-to-Text', 'Image-to-Image'] | N/A | N/A |
| [Latent Zoning Network: A Unified Principle for Generative Modeling,
  Representation Learning, and Classification](https://arxiv.org/abs/2509.15591) | Wenyu Wang, Junyi Zhu, Xuefei Ning, Enshu Liu, fjxmlzn |  - The paper introduces a novel model, Latent Zoning Network (LZN), designed to unify generative modeling, representation learning, and classification using a shared Gaussian latent space.  - LZN utilizes separate encoder-decoder pairs for each data type (images, text, labels), mapping samples to disjoint latent zones and enabling various tasks through different encoder-decoder combinations.  - Experiments on image generation show LZN improves the Fr√©chet Inception Distance (FID) score on CIFAR10 by enhancing the state-of-the-art rectified flow model, while results on representation learning outperform MoCo and SimCLR methods on ImageNet.  - On CIFAR10, LZN achieves improved FID and state-of-the-art classification accuracy when jointly solving both image generation and classification tasks.   - The code and trained models are publicly available. | ['Multimodal', 'Unconditional Image Generation', 'Image Feature Extraction', 'Image Classification'] | [Link](https://github.com/microsoft/latent-zoning-networks) | N/A |
| [BaseReward: A Strong Baseline for Multimodal Reward Model](https://arxiv.org/abs/2509.16127) | jianfeipan, xuwang, KaiWu123, achernarcursa, yifanzhang114 | - This paper introduces BaseReward, a strong baseline for multimodal reward models, which achieves state-of-the-art performance on major benchmarks. - BaseReward uses a simple yet effective architecture built upon a Qwen2.5-VL backbone with an optimized two-layer reward head. - The model is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. - BaseReward outperforms previous open-source and proprietary models on MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench. -  The work also provides empirical guidance for developing robust reward models for future multimodal large language models. | ['Reinforcement Learning', 'Multimodal'] | N/A | N/A |
| [SPATIALGEN: Layout-guided 3D Indoor Scene Generation](https://arxiv.org/abs/2509.14981) | Yongsen Mao, Yixun Liang, Heng Li, Chuan Fang, bertjiazheng | - This paper introduces SpatialGen, a novel framework for high-fidelity 3D indoor scene generation using a multi-view multi-modal diffusion model. - The model takes as input a 3D semantic layout and either a textual description or reference image, generating realistic and semantically consistent 3D scenes from arbitrary viewpoints. - SpatialGen is trained on a new large-scale synthetic dataset featuring 12,328 structured scenes, 57,440 rooms, and 4.7M photorealistic renderings, outperforming prior methods. - The model employs an iterative dense view generation strategy to ensure complete scene coverage and uses a layout-guided attention mechanism to improve multi-view consistency and cross-modal alignment. - SpatialGen demonstrates superior performance compared to existing score distillation and panorama-as-proxy methods across different evaluation metrics. | ['Text-to-3D', 'Image-to-3D', 'Multimodal'] | [Link](https://manycore-research.github.io/SpatialGen) | N/A |
| [A Vision-Language-Action-Critic Model for Robotic Real-World
  Reinforcement Learning](https://arxiv.org/abs/2509.15937) | Jiangmiao, simonlin123, andyzsz123, haoranzhang, fuxian | - This paper introduces VLAC, a Vision-Language-Action-Critic model for robotic real-world reinforcement learning that addresses the limitations of sparse, handcrafted rewards and inefficient exploration in existing methods. - VLAC is a general process reward model built upon InternVL and trained on large-scale heterogeneous datasets, providing dense progress rewards and supporting one-shot in-context transfer to unseen tasks. - The model uses a single VLAC model to alternately generate reward and action tokens, unifying critic and policy, and is deployed inside an asynchronous real-world RL loop with a graded human-in-the-loop protocol to accelerate exploration and stabilize early learning. - Experiments on four real-world manipulation tasks show that VLAC improves success rates from about 30% to about 90% within 200 real-world interaction episodes, with human-in-the-loop interventions yielding a further 50% improvement in sample efficiency. - The VLAC model demonstrates strong generalization capabilities to unseen environments and tasks, showcasing its potential for real-world robotic applications. | ['Reinforcement Learning', 'Robotics', 'Multimodal'] | N/A | N/A |
| [Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in
  Instruction-Guided Expressive Text-To-Speech Systems](https://arxiv.org/abs/2509.13989) | Hung-yi Lee, Kuan-Yu Chen, Tzu-Chieh Wei, Huang-Cheng Chou, Yi-Cheng Lin | - This paper introduces a novel evaluation framework for Instruction-Guided Text-to-Speech (ITTS) systems, focusing on fine-grained control of expressiveness. - The framework incorporates human perceptual ratings on expressive dimensions (adverbs of degree, graded emotion intensity, speaker age, and word-level emphasis). - It presents the Expressive VOice Control (E-VOC) corpus, a large-scale dataset of human evaluations for ITTS controllability. - The results reveal a significant instruction-perception gap, where most ITTS systems struggle with fine-grained control, particularly in generating child or elderly voices and handling word-level emphasis. - The study shows that gpt-40-mini-tts is the most reliable ITTS model, demonstrating better alignment between instructions and generated utterances across various dimensions. | ['Text-to-Speech'] | N/A | N/A |
