

## Papers for 2025-09-23

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [LIMI: Less is More for Agency](https://arxiv.org/abs/2509.17567) | happyZYM, evanlin2570, weizhihao1, mhjiang0408, YangXiao-nlp | This paper introduces LIMI (Less Is More for Intelligent Agency), a novel approach to cultivating machine agency that prioritizes strategic data curation over large-scale data accumulation.  LIMI achieves superior performance on the AgencyBench benchmark, surpassing state-of-the-art models despite using significantly fewer training samples.  The findings support the Agency Efficiency Principle, demonstrating that high-quality agentic demonstrations are more effective than data abundance.  This paradigm shift may enable more sustainable and resource-efficient development of truly agentic AI systems. | ['Natural Language Processing', 'Text2Text Generation', 'Question Answering'] | [Link](https://github.com/GAIR-NLP/AgencyBench), [Link](https://github.com/GAIR-NLP/SII-CLI) | [Link](https://huggingface.co/datasets/YangXiao-nlp/DynToM) |
| [OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion
  Transformer Models](https://arxiv.org/abs/2509.17627) | Pengze Zhang, Tianxiang Ma, Xu Bai, Xinghui Li, Jinshu Chen | - This paper introduces Omnilnsert, a novel unified framework for mask-free video insertion, capable of seamlessly inserting both single and multiple reference subjects into videos. - Omnilnsert utilizes a Condition-Specific Feature Injection mechanism to inject multi-source conditions and a Progressive Training strategy to maintain subject-scene equilibrium. - To improve insertion harmonization, Omnilnsert employs an Insertive Preference Optimization methodology and a Context-Aware Rephraser module. - The proposed InsertPipe data pipeline constructs diverse cross-pair data automatically, addressing data scarcity issues. - Experiments on the introduced InsertBench benchmark demonstrate that OmniInsert outperforms state-of-the-art closed-source commercial solutions. | ['Image-to-Video', 'Text-to-Video', 'Multimodal'] | [Link](https://phantom-video.github.io/OmniInsert/) | N/A |
| [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765) | Lhma-aslp, Cyanbox, jinzheng-he, faychu, ZhifangGuo |  - Qwen3-Omni is a novel multimodal model that achieves state-of-the-art performance across text, image, audio, and video without performance degradation compared to its single-modality counterparts.  - It uses a Thinker-Talker Mixture-of-Experts (MoE) architecture to unify perception and generation across different modalities, supporting various tasks like voice dialogue, video reasoning, and video dialogue.  - Qwen3-Omni demonstrates strong performance on various benchmarks, achieving open-source state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22 out of 36 audio and audio-visual benchmarks.  - The model employs a multi-codebook scheme and lightweight causal ConvNet for streaming speech synthesis, resulting in a first-packet latency of 234ms.  - Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license. | ['Multimodal', 'Any-to-Any', 'Automatic Speech Recognition', 'Text-to-Speech', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/QwenLM/Qwen3-Omni) | [Link](https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo) |
| [OnePiece: Bringing Context Engineering and Reasoning to Industrial
  Cascade Ranking System](https://arxiv.org/abs/2509.18091) | Jiahua Wu, Ethan7, vicowang, TangJiakai5704, KID-22 |  - OnePiece is a novel unified framework that integrates LLM-style context engineering and reasoning into both retrieval and ranking models within industrial cascaded pipelines.  - The model architecture is built upon a pure Transformer backbone and further introduces structured context engineering, block-wise latent reasoning, and progressive multi-task training to improve performance.  - Offline experiments demonstrate that OnePiece achieves higher sample efficiency and surpasses strong baselines with fewer days of logs, consistently improving with more data.  - Online A/B testing on Shopee's main personalized search shows consistent gains across key business metrics, including over +2% GMV/UU and a +2.90% increase in advertising revenue.  - OnePiece delivers consistent online business gains and demonstrates its ability to both subsume existing recall routes and provide substantial novel impressions and clicks. | ['Natural Language Processing'] | N/A | N/A |
| [GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric
  Reasoning](https://arxiv.org/abs/2509.17437) | Hou Pong Chan, Weiwen Xu, Swrooy, 26hzhang, Guizhen | - This paper introduces GeoPQA, a benchmark dataset designed to evaluate the visual perception capabilities of multimodal large language models (MLLMs) in geometric reasoning tasks.  - The authors identify a perceptual bottleneck in MLLMs that limits the effectiveness of reinforcement learning (RL) for improving reasoning abilities.  - To address this, they propose a two-stage RL training framework that first enhances visual perception and then fosters reasoning capabilities.  - Experiments on Qwen-2.5-VL-3B-Instruct demonstrate that the two-stage training significantly improves geometric reasoning and problem-solving compared to a direct reasoning training approach.  - The findings highlight the importance of addressing perceptual limitations in MLLMs for achieving better performance in vision-intensive tasks. | ['Visual Question Answering', 'Multimodal', 'Reinforcement Learning'] | [Link](https://github.com/DAMO-NLP-SG/GeoPQA) | N/A |
| [EpiCache: Episodic KV Cache Management for Long Conversational Question
  Answering](https://arxiv.org/abs/2509.17396) | Minsik Cho, Richa Dixit, Han-Byul Kim, Arnav Kundu, minsoo2333 |  - EPICACHE is a novel training-free framework for managing Key-Value (KV) caches in long conversational question answering (LongConvQA).  - It addresses limitations of existing methods by bounding cache growth through block-wise prefill and preserving topic-relevant context via episodic KV compression.  - EPICACHE improves accuracy by up to 40% over recent baselines on three LongConvQA benchmarks.  - It maintains near-full KV accuracy under 4-6x compression and reduces latency and memory by up to 2.4x and 3.5x respectively.  - The framework uses an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget accordingly. | ['Question Answering'] | N/A | N/A |
| [SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering
  Tasks?](https://arxiv.org/abs/2509.16941) | Yannis Yiming He, Edwin Pan, Jeff Da, Xiang Deng, nlauffer | - This paper introduces SWE-BENCH PRO, a substantially more challenging benchmark for evaluating AI agents' ability to solve long-horizon software engineering tasks. - SWE-BENCH PRO features 1865 problems sourced from 41 actively maintained repositories, including a public set, a held-out set, and a commercial set. - The benchmark includes long-horizon tasks requiring hours or days to complete, often involving patches across multiple files. - Evaluation of widely used coding models reveals performance below 25% (Pass@1), with GPT-5 achieving the highest score at 23.3%. - The authors provide a detailed analysis of failure modes to better understand the limitations of current models and suggest potential future research directions. | ['Natural Language Processing', 'Text2Text Generation'] | [Link](https://github.com/scaleapi/SWE-bench_Pro-os) | [Link](https://huggingface.co/datasets/ScaleAI/SWE-bench_Pro) |
| [FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning
  Models on Automatically Verifiable Textual and Visual Questions](https://arxiv.org/abs/2509.17177) | tengdai722, stephaniezhou, xuanricheng, miguelhuchen, lilaczheng | This paper introduces ROME, a new benchmark for evaluating large reasoning models (LRMs) on automatically verifiable textual and visual questions.  The evaluation focuses on identifying problematic behaviors in LRMs and compares their performance to non-reasoning models.  Results show that many top-tier models exhibit issues like misaligned thinking and hallucination of tool usage.  While inference-time scaling shows some benefits for certain tasks, visual reasoning remains a major challenge for current models. | ['Multimodal', 'Visual Question Answering', 'Question Answering'] | [Link](https://flageval-baai.github.io/LRM-Eval/) | N/A |
| [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158) | Matteo Bettini, Gerard Moreno-Torres Bertran, Amine Benhalloum, Pierre Andrews, HugoLaurencon |  * This paper introduces Meta Agents Research Environments (ARE), a platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations.  * ARE provides simple abstractions for building complex environments and proposes Gaia2, a benchmark measuring general agent capabilities beyond search and execution.   * Gaia2 involves handling ambiguities, adapting to dynamic environments, collaboration, and temporal constraints, addressing limitations of previous benchmarks.   * Experiments show no system dominates across intelligence spectrums and budget scaling curves plateau, highlighting the need for new architectures.  * ARE abstractions enable continuous Gaia2 extension to other environments, facilitating community contributions and rapid benchmark creation tailored to specific domains. | ['Reinforcement Learning', 'Natural Language Processing'] | [Link](https://github.com/facebookresearch/meta-agents-research-environments) | N/A |
| [Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from
  Token and Parameter Levels](https://arxiv.org/abs/2509.16596) | Qi Zhang, Shuo Li, Yang Nan, Umean, Junjie-Ye |  - This paper analyzes the effects of supervised fine-tuning (SFT) on large language models (LLMs) from token and parameter levels.  -  The authors surprisingly find that models fine-tuned on larger datasets can perform worse than those fine-tuned on smaller datasets, with performance fluctuations exceeding 12%.  - They conduct analyses to investigate these effects through the lens of token-level Kullback-Leibler divergence and selective parameter restoration.   - Their analysis shows that up to 90% of parameter updates during SFT do not improve model knowledge.  - By restoring some of these unnecessary updates, they show improvements in model performance, offering practical guidance for more effective fine-tuning strategies. | ['Question Answering'] | N/A | N/A |
| [Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG
  Applications](https://arxiv.org/abs/2509.17671) | Fatma Betül Terzioğlu, Reyhan Bayraktar, ozayezerceli, MElHuseyni, selvatas | - This paper introduces Turk-LettuceDetect, the first suite of hallucination detection models specifically designed for Turkish RAG applications. - It formulates hallucination detection as a token-level classification task and fine-tunes three distinct encoder architectures: a Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. - The ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, outperforming baseline multilingual approaches. - The models maintain computational efficiency while supporting long contexts up to 8,192 tokens. - This work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages. | ['Token Classification', 'Natural Language Processing', 'Question Answering'] | N/A | N/A |
| [QWHA: Quantization-Aware Walsh-Hadamard Adaptation for
  Parameter-Efficient Fine-Tuning on Large Language Models](https://arxiv.org/abs/2509.17428) | Jae-Joon Kim, Yulhwa Kim, Beomseok Kang, Seojune Lee, Hyesung Jeon |  - This paper introduces QWHA, a novel quantization-aware parameter-efficient fine-tuning (QA-PEFT) framework for large language models (LLMs).  - QWHA integrates a Walsh-Hadamard Transform (WHT)-based adapter (WHA) to mitigate quantization errors.  - A novel parameter initialization scheme, AdaAlloc, is proposed to improve the effectiveness of the WHA adapter.  - Experimental results demonstrate that QWHA outperforms existing low-rank and FT-based adapters in low-bit quantization accuracy and training speed.  - The code is available at https://github.com/vantaa89/qwha | ['Natural Language Processing'] | [Link](https://github.com/vantaa89/qwha) | N/A |
| [Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248) | Emmanuel Candès, Tatsunori Hashimoto, Hong Liu, Aonan Zhang, Zitong Yang | This paper introduces Synthetic Bootstrapped Pretraining (SBP), a novel pretraining method for language models.  SBP first trains a model to learn inter-document relationships from a pretraining dataset and then leverages this model to synthesize a large new corpus for joint training.  Experiments on a 3B parameter model demonstrate SBP's consistent improvement over strong baselines.  Qualitative analysis reveals that the synthesized documents go beyond simple paraphrasing, suggesting SBP's ability to abstract core concepts from seed documents.  Finally, a Bayesian interpretation of SBP is provided, explaining its improvement in capturing latent concepts shared between related documents. | ['Natural Language Processing'] | N/A | N/A |
| [MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late
  Interaction](https://arxiv.org/abs/2509.18095) | Xintao Chen, Chun-cheng Jason Chen, Mengting Gu, Qi Ma, MrZilinXiao | - This paper introduces MetaEmbed, a new framework for multimodal retrieval that uses a small number of learnable Meta Tokens appended to input sequences to create compact, expressive multi-vector embeddings. - At test time, the last-layer contextualized representations of these Meta Tokens serve as multi-vector embeddings, allowing users to balance retrieval quality and efficiency by adjusting the number of tokens. - MetaEmbed employs Matryoshka Multi-Vector Retrieval training, enabling it to learn to organize information by granularity across multiple vectors. -  Evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) show that MetaEmbed achieves state-of-the-art retrieval performance and scales robustly to large models (32B parameters). - The flexible test-time scaling allows for a trade-off between retrieval accuracy and computational cost, making MetaEmbed suitable for various scenarios. | ['Multimodal'] | N/A | N/A |
| [DIWALI - Diversity and Inclusivity aWare cuLture specific Items for
  India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian
  Context](https://arxiv.org/abs/2509.17399) | Maunendra Sankar Desarkar, mrajbrahma, pramitsahoo | This paper introduces DIWALI, a new dataset containing 8,817 culture-specific items from 17 facets across 36 Indian subregions, designed to evaluate large language models' (LLMs) cultural text adaptation capabilities.  The dataset is shown to improve LLM performance on cultural adaptation tasks compared to existing datasets like CANDLE and DOSA, demonstrating superior regional coverage and cultural nuance.  LLMs are assessed using automated and human evaluations, revealing a bias towards surface-level adaptations and a challenge to achieve deeper, emotionally resonant cultural adaptations.  The DIWALI dataset and associated code are publicly available. | ['Natural Language Processing', 'Text Generation', 'Text2Text Generation'] | [Link](https://github.com/pramitsahoo/culture-evaluation) | [Link](https://huggingface.co/datasets/nlip/DIWALI) |
| [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End
  Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856) | Hang Yu, Zihan Liao, Xunjin Zheng, Hanyang Guo, Geralt-Targaryen | - This paper introduces CodeFuse-CR-Bench, a new benchmark for evaluating code review models that addresses the limitations of existing benchmarks by focusing on the comprehensiveness of real-world code review. - The benchmark comprises 601 high-quality instances from 70 Python projects and includes rich contextual information such as associated issues, PR details, commit history, and complete code changes. - A novel evaluation framework that combines rule-based checks and model-based judgments of review quality was developed and used to assess the performance of state-of-the-art LLMs. - Gemini 2.5 Pro achieved the highest comprehensive performance among the evaluated LLMs. - Experimental results highlighted the necessity of holistic, multi-dimensional evaluation in advancing truly intelligent and practical code review assistants. | ['Natural Language Processing', 'Text Generation', 'Summarization'] | N/A | N/A |
| [SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward
  Learning](https://arxiv.org/abs/2509.16548) | Zhaopeng Tu, Xiaobo Liang, Juntao Li, Xinyu Shi, dyyyyyyyy | - This paper introduces SCAN, a self-denoising Monte Carlo annotation method for robust process reward learning that addresses the high noise ratio in synthetic data generated from Monte Carlo estimation.- SCAN utilizes a self-confidence metric to assess annotation reliability, mitigating underestimation and overestimation of step correctness.- Experimental results show SCAN achieves superior performance to existing methods on ProcessBench, surpassing even strong baselines trained on large-scale human-annotated datasets such as PRM800K.- SCAN enables the use of lightweight models for high-quality annotation and is efficient in data synthesis.- The paper also conducts a thorough analysis of noise distribution in Monte Carlo estimation and investigates the effects of different model parameters on the effectiveness of SCAN. | ['Natural Language Processing', 'Reinforcement Learning'] | [Link](https://scan-prm.github.io) | N/A |
