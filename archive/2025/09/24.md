

## Papers for 2025-09-24

| Title | Authors | Summary | Classification | GitHub URLs | HuggingFace URLs |
|-------|---------|---------|----------------|-------------|-----------------|
| [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and
  Training Recipe](https://arxiv.org/abs/2509.18154) | Wenshuo Ma, Fuwei Huang, Chongyi Wang, Zefan Wang, Tianyu Yu | MiniCPM-V 4.5 is an 8B parameter multimodal large language model that prioritizes efficiency.  The model uses a unified 3D-Resampler architecture for compact image and video encoding, a unified learning paradigm for document knowledge and text recognition, and a hybrid reinforcement learning strategy for improved reasoning.  Experiments show MiniCPM-V 4.5 outperforms existing models such as GPT-4 and Qwen2.5-VL 72B in several benchmarks, achieving state-of-the-art results on VideoMME with significantly reduced memory and inference time. The model is open-sourced on Github. | ['Multimodal'] | [Link](https://github.com/OpenBMB/MiniCPM-V) | N/A |
| [Hyper-Bagel: A Unified Acceleration Framework for Multimodal
  Understanding and Generation](https://arxiv.org/abs/2509.18824) | Jianbin Zheng, Huafeng Kuang, Manlin Zhang, Xin Xia, Yanzuo Lu | - The paper introduces Hyper-Bagel, a unified acceleration framework designed to enhance both multimodal understanding and generation tasks. - The framework employs a divide-and-conquer strategy, using speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. - Hyper-Bagel achieves over a 2x speedup in multimodal understanding and significant speedups in text-to-image generation (16.67x) and image editing (22x), while maintaining high-quality outputs. - A highly efficient 1-NFE model is also presented, enabling near real-time interactive editing and generation. - The model is trained using a combination of advanced distillation techniques and human feedback learning, resulting in seamless and instantaneous multimodal interactions. | ['Multimodal', 'Text-to-Image', 'Image-to-Image', 'Image-to-Text'] | [Link](https://hyper-bagel.github.io/) | N/A |
| [What Characterizes Effective Reasoning? Revisiting Length, Review, and
  Structure of CoT](https://arxiv.org/abs/2509.19284) | Anthony Hartshorn, Parag Jain, Cheng Zhang, Julia Kempe, Yunzhen Feng | - This paper introduces the Failed-Step Fraction (FSF), a novel metric for evaluating the effectiveness of chain-of-thought (CoT) reasoning in large language models (LLMs). - The study demonstrates that FSF consistently outperforms existing metrics like length and review ratio in predicting the accuracy of LLM reasoning. - Two causal experiments, including test-time selection and controlled CoT editing, are conducted to validate the findings, providing compelling evidence that FSF is a key factor influencing reasoning performance. - The results suggest that indiscriminately generating long CoTs is not always beneficial and that a focus on structural quality, minimizing failed reasoning branches, is a more effective approach to enhancing reasoning accuracy. - The study provides a valuable contribution to the field of LLM reasoning by advancing our understanding of effective reasoning strategies and offering a new perspective on test-time scaling. | ['Natural Language Processing'] | N/A | N/A |
| [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835) | Katharina von der Wense, Anne Lauscher, Valentin Hofmann, Carolin Holtermann, Minh Duc Bui | This paper investigates whether Large Language Models (LLMs) exhibit biases against speakers of German dialects.  The study uses two tasks: an association task and a decision task to assess dialect naming bias and dialect usage bias. The results show that all evaluated LLMs exhibit significant biases, reflecting negative stereotypes.  Explicitly labeling linguistic demographics amplifies this bias more than implicit cues.  Larger LLMs exhibit stronger biases than smaller ones. | ['Natural Language Processing'] | [Link](https://github.com/UhhDS/German-Dialect-Bias) | [Link](https://huggingface.co/LeoLM/leo-hessianai-70b) |
| [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal
  Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087) | Genady Beryozkin, Maxim Neumann, Dahun Kim, Yotam Gigi, Ganesh Mallya | - This paper proposes a training-free approach for incorporating multi-spectral data into generalist multimodal models for remote sensing applications. - The method leverages the model's understanding of visual space and injects domain-specific information as instructions, enabling zero-shot performance gains. - Using the Gemini 2.5 model, the approach achieves strong zero-shot performance improvements on popular remote sensing benchmarks like BigEarthNet and EuroSAT for land cover and land use classification. - The results demonstrate the adaptability of Gemini 2.5 to new inputs without retraining, highlighting its potential for geospatial professionals. - The work showcases the potential of generalist multimodal models for handling non-standard inputs and specialized tasks in remote sensing. | ['Zero-Shot Image Classification', 'Multimodal'] | N/A | N/A |
