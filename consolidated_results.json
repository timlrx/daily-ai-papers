[
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": " - This research introduces MaskLLM, a novel learnable pruning method that generates semi-structured (N:M) sparsity in Large Language Models (LLMs) for enhanced inference efficiency.\n- MaskLLM distinguishes itself from previous methods by directly learning the distribution of N:M sparsity patterns using Gumbel Softmax sampling, enabling end-to-end training on large datasets and addressing limitations of hand-crafted importance criteria.\n-  Empirical evaluations on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, demonstrate that MaskLLM surpasses state-of-the-art techniques, achieving a perplexity of 6.72 on LLaMA2-7B compared to SparseGPT's 10.42.\n- The research underscores the efficacy of learning sparsity patterns directly from data, leading to more accurate and efficient compression of LLMs without compromising performance.\n- The adaptability of MaskLLM to downstream tasks and its ability to achieve lossless compression in certain scenarios highlight its potential for practical applications.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "This research proposes LLaVA-3D, a novel framework for building 3D-aware Large Multimodal Models (LMMs) by adapting the existing 2D LLaVA model.  LLaVA-3D introduces the concept of \"3D Patches,\" which inject 3D positional embeddings into 2D image features, enhancing the model's spatial understanding without complex 3D processing pipelines.  Evaluations demonstrate LLaVA-3D's state-of-the-art performance on various 3D tasks, including question answering, dense captioning, and visual grounding, surpassing existing 3D LMMs while maintaining comparable 2D image understanding capabilities to its 2D counterpart. The research highlights the advantages of leveraging pre-trained 2D LMMs for 3D scene understanding and the benefits of integrating 3D spatial information into 2D visual features.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Image-to-Text",
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "This paper introduces EMOVA, a novel end-to-end multimodal large language model capable of perceiving and generating images, text, and speech with emotional expressiveness. EMOVA utilizes a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for end-to-end speech processing.  The model achieves state-of-the-art performance on both vision-language and speech benchmarks, outperforming models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks and surpassing the speech LLM Mini-Omni in ASR tasks. EMOVA also enables emotional spoken dialogue by explicitly predicting speech style labels (emotions and pitches) and leveraging a lightweight style module for controllable speech synthesis. This is achieved through a novel text-centric multimodal alignment approach, which leverages publicly available bimodal data and eliminates the reliance on scarce trimodal data.",
        "classification": [
            "Multimodal",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": "\u2022 This paper introduces Lotus, a novel diffusion-based visual foundation model designed for high-quality dense prediction tasks, specifically focusing on depth and surface normal estimation.\n\u2022 Lotus leverages the rich visual priors acquired from pre-trained text-to-image diffusion models, enabling strong zero-shot generalization capabilities.\n\u2022 The authors systematically analyze the diffusion formulation and identify that the standard parameterization type and multi-step diffusion process are not optimal for dense prediction, proposing a more effective adaptation protocol.\n\u2022 Through extensive experiments, Lotus demonstrates state-of-the-art performance on various benchmark datasets for zero-shot depth and normal estimation with significantly less training data compared to previous methods. \n\u2022 Notably, Lotus achieves superior efficiency due to its single-step diffusion formulation, making it hundreds of times faster than many existing diffusion-based methods.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": "This research paper introduces GemFilter, a novel approach to reduce the computational cost and latency of processing long context inputs with Large Language Models (LLMs). GemFilter leverages the ability of early LLM layers to identify relevant tokens and compresses the input sequence by a factor of 1000x for subsequent processing by the full model. Empirical evaluations show that GemFilter achieves a 2.4x speedup and 30% reduction in GPU memory consumption compared to state-of-the-art methods, while maintaining comparable performance on benchmarks like LongBench and outperforming them on the Needle in a Haystack task. GemFilter is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting the selected input sequence.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": " - This paper proposes a novel post-training objective function for Latent Diffusion Models (LDMs) by incorporating a pixel-space loss alongside the standard latent-space loss during fine-tuning.  \n- This method enhances the generation of high-frequency details and reduces visual flaws in generated images.\n- Human evaluation demonstrates that this approach significantly improves both supervised fine-tuning and preference-based post-training on DiT and U-Net based LDMs. \n- For example, it boosts visual appeal win rate by 12.9% in DiT and 5.9% in Emu during preference-based fine-tuning,  and achieves a 32.8% vs 9.3% win rate on visual flaws with supervised fine-tuning on DiT. \n- The paper also shows the benefit of using SimPO over DPO for reward modeling in diffusion models.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": " - This research paper investigates implicit instruction tuning, demonstrating that instruction following can emerge without explicit instruction-response training. \n- The authors show that training solely on responses (response tuning) and on narrow-domain data (single-task finetuning) leads to broad instruction-following abilities in language models. \n- For instance, response-tuned models achieve a 43% win rate against explicitly instruction-tuned models in head-to-head evaluations. \n- Furthermore, they introduce a simple rule-based language model that, when combined with a pretrained model, exhibits instruction-following behavior. \n- These findings highlight that adaptation methods not explicitly designed for instruction following can implicitly induce such capabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This research paper introduces a novel technique named \"TOKEN POOLING\" for enhancing the efficiency of multi-vector retrieval models, especially focusing on ColBERT, without significantly affecting performance.  The method uses clustering techniques to group together similar token representations and then applies mean pooling to create a single, representative vector, effectively reducing the overall storage footprint. Experiments show that this approach reduces the required vector count by 50% without compromising accuracy, and a 66% reduction still yields strong performance. The paper also demonstrates that TOKEN POOLING can be effectively combined with existing quantization methods, leading to even more significant compression rates while maintaining reasonable retrieval performance. ",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": " - This paper introduces Disco4D, a novel Gaussian Splatting framework that generates animatable 3D clothed human avatars from a single image. \n- Disco4D disentangles clothing from the human body, representing the former with Gaussian models and the latter with the SMPL-X model, enhancing generation detail and flexibility.\n- Disco4D utilizes diffusion models to refine textures during 3D generation and extrapolate unseen views during 4D animation, surpassing existing methods in fidelity and view consistency.\n- Disco4D enables fine-grained editing and animation of generated avatars, including direct manipulation of clothing Gaussians and pose-driven animation.\n- The authors highlight Disco4D's limitations, such as reliance on robust SMPL-X estimations and limitations in multi-layered clothing modeling, pointing to future research directions.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": " - This paper presents a comprehensive review of the emerging field of Conversation Analysis (CA), a process designed to extract critical information from conversational data and leverage it for system optimization and decision-making.  - The paper systematically defines CA as a four-step procedure: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, and discusses the challenges and trends within each step. - The paper further argues that while previous CA efforts focused on atomic tasks with limited business impact, the rise of Large Language Models (LLMs) enables deeper, more insightful analysis and strategic decision-making from conversations.  - The authors compile and categorize existing benchmark datasets for CA but highlight a significant gap in comprehensive benchmarks containing fine-grained conversation elements and long-context modeling capabilities.  - The paper concludes by outlining future research directions, including the development of LLM-based conversation simulators, fine-grained CA benchmarks, long-context conversation modeling, in-depth attribution analysis, and advanced goal-directed optimization and evaluation methods.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging Knowledge Graphs (KGs) and graph-based architectures. The authors demonstrate the effectiveness of their framework by applying it to the SoccerNet dataset, a large dataset of soccer videos. Their findings show that Structured-GraphRAG significantly improves query processing efficiency, reduces response times, and enhances accuracy compared to traditional RAG methods. The structured nature of KGs reduces hallucinations in LLMs, making the responses more consistent and reliable. The authors highlight that their framework can be applied to a broad range of applications due to its flexible design.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": " - This paper introduces Robot See Robot Do (RSRD), a novel approach for enabling robots to imitate articulated object manipulation from a single monocular RGB human demonstration. \n- Given a static multi-view object scan and a monocular human interaction video, RSRD reconstructs the 3D motion trajectories of object parts using a method called 4D Differentiable Part Models (4D-DPM). \n- RSRD achieves an average success rate of 87% for each phase (object pose registration, trajectory planning, grasping, and motion execution) and an end-to-end success rate of 60% across 90 trials involving 9 different objects. \n- The method leverages DINO feature fields and an analysis-by-synthesis paradigm for robust 3D motion recovery from monocular video. \n- RSRD demonstrates the potential of object-centric learning from human demonstrations for enabling robots to acquire new manipulation skills in a user-friendly and efficient manner.",
        "classification": [
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": " - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.\n - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.\n -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.\n -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.\n -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Any-to-Any"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": "This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence.",
        "classification": [
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": " - This paper introduces Lotus, a novel diffusion-based visual foundation model specifically designed for high-quality dense prediction tasks, including depth and normal estimation.\n - By employing a novel adaptation protocol that includes direct annotation prediction, a single-step diffusion formulation, and a detail preserver, Lotus effectively leverages visual priors from pre-trained text-to-image diffusion models for enhanced zero-shot generalization.\n - Lotus outperforms existing methods in zero-shot depth and normal estimation benchmarks, achieving state-of-the-art results with minimal training data, as evidenced by its superior performance on datasets such as NYUv2, KITTI, ETH3D for depth and NYUv2, ScanNet, iBims-1, and Sintel for normal estimation.\n - Moreover, Lotus exhibits significant efficiency improvements compared to previous diffusion-based methods, being hundreds of times faster for inference, as demonstrated in a comparison of inference times at different image resolutions.\n - The model's adaptability and performance make it suitable for various downstream applications like 3D reconstruction and scene understanding.",
        "classification": [
            "Computer Vision",
            "Depth Estimation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": "- This paper introduces a novel post-training method for Latent Diffusion Models (LDMs) that enhances image generation quality by incorporating a pixel-space loss function alongside the conventional latent-space loss.\n- The authors demonstrate that this approach significantly improves both visual appeal and reduces visual flaws, as evidenced by a 32.8% vs 9.3% win rate in visual flaws and a 34.8% vs 16.6% win rate in visual appeal during supervised fine-tuning of a DiT model.\n-  Furthermore, the method proves effective for both supervised fine-tuning and preference-based post-training, showing consistent improvements across different LDM architectures like DiT and U-Net.\n- The paper also explores variations in reward modeling during preference-based fine-tuning, finding that combining SimPO with the proposed pixel-space loss yields the most substantial enhancements.\n- The authors highlight the simplicity and general applicability of their method, allowing for seamless integration into existing LDM pipelines and potentially benefiting future post-training techniques.",
        "classification": [
            "Computer Vision",
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": "This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that \"response tuning,\" which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that \"single-task finetuning,\"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": " - This paper introduces Disco4D, a novel framework that generates and animates 3D clothed human avatars from a single image using disentangled Gaussian representations.\n - Disco4D utilizes SMPL-X for body representation and separate Gaussian models for clothing and accessories, leading to improved detail and flexibility compared to existing methods that merge body and clothing into a single mesh. \n - The paper demonstrates Disco4D's superior performance in fidelity and geometry through quantitative comparisons on standard benchmarks (Synbody and CloSe), outperforming competing methods like DreamGaussian, LGM, and SHERF.\n -  Beyond static 3D generation, Disco4D enables animation through SMPL-X pose sequences and learns clothing dynamics from monocular videos, achieving more realistic and nuanced clothing movements.\n - Disco4D's disentanglement approach allows for fine-grained, localized editing of individual clothing items, enabling color changes, object removal, and swapping clothing items without impacting other parts of the avatar.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": "\u2022 This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field.\n\u2022 The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA.\n\u2022 The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses.\n\u2022 The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach.\n\u2022 The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs).",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": "- This paper presents Robot See Robot Do (RSRD), a novel approach to teach robots how to manipulate articulated objects with moving parts through a single monocular human demonstration. \n- RSRD constructs a 4D Differentiable Part Model (4D-DPM) from a multi-view static object scan and uses it to track object part motion from the monocular video. \n- RSRD outperforms photometric-based tracking approaches, achieving a mean average point distance of 7.5mm on tracking manipulated object part poses. \n- The robot then leverages the recovered part motions to plan its own motion to achieve the same object configuration change, demonstrating a 60% end-to-end success rate across various objects and re-orientations. \n- The proposed method is particularly notable for its zero-shot capability, requiring no task-specific training data or annotations.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": "\n- MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference.\n- Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end.\n- Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models.\n- MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks.\n- The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "-\nLLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings.\n- This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks.\n- LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets.\n- The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks.\n- Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Visual Question Answering",
            "Image-to-Text",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "- EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities.\n- It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control.\n- The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data.\n- EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models.\n- A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue.",
        "classification": [
            "Multimodal",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Text-to-Audio",
            "Audio-to-Audio",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": " - Lotus, a diffusion-based visual foundation model, is introduced for high-quality dense prediction, specializing in depth and normal map estimation.\n- It utilizes a novel single-step diffusion process with direct annotation prediction for improved performance and a detail preserver to enhance predictions in intricate areas.\n- Evaluation on standard datasets like NYUv2, KITTI, ETH3D, ScanNet, iBims-1, and Sintel shows that Lotus achieves state-of-the-art zero-shot results, outperforming competitors trained on much larger datasets, especially Marigold trained with 74K vs 59K images used for training Lotus, achieving an avg. rank between 1.0 - 7.0 vs 1.5 - 2.5 on Lotus across all datasets and metrics.\n- Lotus offers significant efficiency gains, being hundreds of times faster than existing diffusion-based methods.\n- The efficiency and quality enable various applications like joint estimation and 3D reconstruction.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": " - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs. \n- It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference. \n- This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV. \n- Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilter\u2019s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O. \n- Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [
            "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
            "https://huggingface.co/mistralai/Mistral-Nemo-Base-2407",
            "https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
        ],
        "date": "2024-09-29"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": " - This research proposes a novel pixel-space post-training method for Latent Diffusion Models (LDMs) to enhance the generation of high-frequency details and complex compositions, which are often imperfect in LDMs.\n- This method addresses the limitations of latent space training by adding pixel-space supervision during post-training, thereby preserving details lost in the compression of the latent space. \n- Human evaluations on a DiT transformer model demonstrate a significant improvement of 18.2% in visual appeal and 23.5% in reduction of visual flaws with supervised fine-tuning, and 17.8% and 11.3% with preference-based fine-tuning using this method compared to a latent-space baseline. \n- This improvement is also validated on U-Net diffusion models, showing a 32.8% improvement on visual flaws with the same fine-tuning dataset. \n- This simple method can be easily integrated into any existing LDM, offering advancements in both supervised and preference-based post-training.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": "\n- This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose. \n- Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data). \n- Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models. \n- Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain. \n- A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/john-hewitt/implicit-ins"
        ],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": " - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era.\n- It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations.\n- The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs.\n- It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs.\n-  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security.",
        "classification": [
            "Natural Language Processing",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations.\n- Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets.\n- It can achieve even further reduction of vector count by 66% with less than 3% performance degradation.\n- This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models.\n- The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors.\n- The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/colbert-ir/colbertv2.0"
        ],
        "date": "2024-09-29"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "\n- This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG).\n- It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness.\n- Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time.\n- The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs.\n- While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": "-\"Robot See Robot Do (RSRD)\" is introduced, a method for robots to imitate articulated object manipulation from a single monocular RGB human demonstration, given a static multi-view object scan.\n- RSRD uses 4D Differentiable Part Models (4D-DPM) to recover 3D part motion from monocular video using part-centric feature fields and iterative optimization with geometric regularizers.\n- The robot replicates demonstrated object trajectories by planning bimanual arm motions inducing the same part motion, focusing on the intended behavior rather than mimicking human hand motions.\n- RSRD achieves an average 87% success rate in each phase (registration, planning, grasping, execution), resulting in a 60% total end-to-end success rate across 90 trials with 9 objects.\n- This is achieved using feature fields from pre-trained vision models without task-specific training, fine-tuning, data collection, or annotation.",
        "classification": [
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": "- Disco4D is a novel Gaussian Splatting framework that generates and animates 4D clothed humans from a single image, outperforming existing methods by disentangling clothing from the human body.\n- Disco4D uses the SMPL-X model for body representation and Gaussian models for clothing, allowing for detailed generation and flexibility.\n- It leverages diffusion models to enhance 3D generation, particularly for occluded parts, and includes an identity encoding for each clothing Gaussian for asset separation.\n- Disco4D supports 4D human animation with vivid dynamics, enabling virtual try-on and avatar customization.\n- User studies confirm that Disco4D generates higher-fidelity outputs and aligns better with original image content compared to competing methods.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "MIO: A Foundation Model on Multimodal Tokens",
        "authors": "Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang",
        "link": "https://arxiv.org/abs/2409.17692",
        "github_repo": null,
        "summary": " - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video.\n- It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning.\n- Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines.\n- It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning.\n- MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs.",
        "classification": [
            "Multimodal",
            "Any-to-Any",
            "Text-to-Image",
            "Image-to-Text",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
        "authors": "Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92",
        "link": "https://arxiv.org/abs/2409.17066",
        "github_repo": null,
        "summary": " - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization.\n- VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization.\n- The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization.\n- They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks.\n- VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/microsoft/VPTQ"
        ],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult",
        "authors": "fetong",
        "link": "https://arxiv.org/abs/2409.17545",
        "github_repo": null,
        "summary": "This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs).\n- MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning.\n- Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models.\n- On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B.\n- MIPO simplifies hyperparameter tuning by using only a single parameter, \u03b2, exhibiting robustness across different model architectures and datasets within a specific range.\n- MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback"
        ],
        "date": "2024-09-30"
    },
    {
        "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
        "authors": "Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu",
        "link": "https://arxiv.org/abs/2409.16686",
        "github_repo": null,
        "summary": "- The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales.\n- MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights.\n- Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment.\n- The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios.\n- MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents.",
        "classification": [
            "Robotics",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
        "authors": "nm-w, pdufter, zhegan27, fly6464, haotiz",
        "link": "https://arxiv.org/abs/2409.20566",
        "github_repo": null,
        "summary": " - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning.\n- MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training.\n- It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks.\n- MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI).\n-  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "DiaSynth -- Synthetic Dialogue Generation Framework",
        "authors": "Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl",
        "link": "https://arxiv.org/abs/2409.19020",
        "github_repo": null,
        "summary": " - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning.\n- It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues.\n- Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks.\n- The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data.\n- DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Hyper-Connections",
        "authors": "banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder",
        "link": "https://arxiv.org/abs/2409.19606",
        "github_repo": null,
        "summary": "This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse.\n- Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters.\n- Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections.\n- Hyper-connections are also effective in vision tasks.\n- Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections.\n- The authors anticipate this method's broad applicability across various AI problems.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision",
            "Image Classification",
            "Text Generation",
            "Image-to-Text",
            "Unconditional Image Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
        "authors": "yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li",
        "link": "https://arxiv.org/abs/2409.18943",
        "github_repo": "https://github.com/Geaming2002/Ruler",
        "summary": "- RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs).\n- Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation.\n- RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs.\n- Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments. \n- RULER maintains overall performance on various other benchmarks without affecting non-length based generation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Geaming2002/Ruler"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Cottention: Linear Transformers With Cosine Attention",
        "authors": "Eric C. Larson, TrevorDohm, gmongaras",
        "link": "https://arxiv.org/abs/2409.18747",
        "github_repo": null,
        "summary": "This study introduces \"Cottention,\" a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/gmongaras/Cottention_Transformer"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Image Copy Detection for Diffusion Models",
        "authors": "Yi Yang, Zhentao Tan, Yifan Sun, WenhaoWang",
        "link": "https://arxiv.org/abs/2409.19952",
        "github_repo": null,
        "summary": "-\"ICDiff\", a new Image Copy Detection (ICD) model specialized for diffusion-generated replicas, is introduced, addressing the challenge of content originality in AI-generated images.\n- A novel Diffusion-Replication (D-Rep) dataset comprising 40,000 image-replica pairs, annotated with six replication levels, is created using Stable Diffusion V1.5 and LAION-Aesthetics V2.\n- A new PDF-Embedding method transforms replication levels into probability density functions (PDFs) for supervision, improving performance by leveraging the continuous and smooth nature of replication level probabilities.\n- Experimental results demonstrate PDF-Embedding outperforms protocol-driven and non-PDF methods on D-Rep, highlighting its effectiveness in detecting diffusion-based replication.\n- Analysis reveals replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%, indicating a significant prevalence of content replication in AI-generated images.",
        "classification": [
            "Computer Vision",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://icdiff.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Can Models Learn Skill Composition from Examples?",
        "authors": "Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu",
        "link": "https://arxiv.org/abs/2409.19808",
        "github_repo": null,
        "summary": "This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4.\n- Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills.\n- Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill.\n- The study shows that incorporating skill-rich synthetic text improves compositional capabilities.\n- Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency.\n- Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
        "authors": "Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae",
        "link": "https://arxiv.org/abs/2409.19715",
        "github_repo": null,
        "summary": "COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym"
        ],
        "date": "2024-10-01"
    },
    {
        "title": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding",
        "authors": "Jianzong Wang, Jing Xiao, zhangxulong, Pechola",
        "link": "https://arxiv.org/abs/2409.19627",
        "github_repo": null,
        "summary": "-\nIDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization.\n- It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating.\n- A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability.\n- IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods.\n- Experimental results show its ability to withstand various attacks while maintaining good imperceptibility.",
        "classification": [
            "Audio",
            "Audio-to-Audio"
        ],
        "github_urls": [
            "https://github.com/PecholaL/IDEAW"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
        "authors": "xwhan, ruihou16, xwwang, astonzhang, MingZhong",
        "link": "https://arxiv.org/abs/2409.19951",
        "github_repo": "https://github.com/facebookresearch/llm-cross-capabilities",
        "summary": " - This research paper explores the intersection of multiple abilities, termed \"cross capabilities,\" in Large Language Models (LLMs), which are essential for real-world tasks but often overlooked in current evaluations that focus on individual capabilities.\n- It introduces CROSSEVAL, a benchmark with 1,400 human-annotated prompts and 8,400 human ratings, designed to evaluate both individual and cross capabilities, revealing that current LLMs underperform in cross-capability tasks.\n- The study finds that LLM cross-capability performance adheres to the \"Law of the Weakest Link,\" being significantly limited by the weakest individual capability, regardless of improvements in other areas.\n- The results highlight that tool use is a major challenge for LLMs and suggest that prioritizing the enhancement of weaker capabilities is more crucial for improving overall performance than focusing on already strong ones.\n-  The work emphasizes the importance of shifting focus towards cross-capability evaluation and development to improve LLM effectiveness in complex, real-world scenarios rather than just on individual capabilities.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/llm-cross-capabilities"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices",
        "authors": "Hongfang Yu, Mohsen Guizani, Jiaoshen, LIKirin",
        "link": "https://arxiv.org/abs/2410.00531",
        "github_repo": "https://github.com/Lizonghang/TPI-LLM",
        "summary": "TPI-LLM is a tensor parallel inference system designed for serving 70B-scale LLMs efficiently on low-resource edge devices.\n- It addresses memory limitations by introducing a sliding window memory scheduler that dynamically manages layer weights during inference, overlapping disk I/O with computation and communication.\n- TPI-LLM prioritizes tensor parallelism over pipeline parallelism for single-user scenarios on edge devices and implements a star-based allreduce algorithm to minimize link latency.\n- Experimental results show significant reductions in time-to-first-token, token latency, and peak memory footprint compared to benchmarks like Transformers, Accelerate, and Galaxy.\n- TPI-LLM successfully runs Llama 2-70B with a peak memory footprint of 3.1GB across 8 low-resource devices, enabling larger models to run on edge devices while preserving user privacy.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Lizonghang/TPI-LLM"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect",
        "authors": "imomayiz, amr-mohamed, khoubrane-yousef, habdine, guokan-shang",
        "link": "https://arxiv.org/abs/2409.17912",
        "github_repo": null,
        "summary": "Atlas-Chat introduces the first Large Language Models (LLMs) for Moroccan Arabic, a low-resource dialectal Arabic (DA) variant also known as Darija.\n- A new instruction dataset, Darija-SFT-Mixture, was created by combining existing and new manually and synthetically created Darija resources, as well as translated English instructions.\n- Atlas-Chat-9B and 2B models, fine-tuned on this dataset, outperform existing LLMs, including Arabic-specific and state-of-the-art models like LLaMa, Jais, and AceGPT, achieving a 13% improvement over a 13B model on a new Darija benchmark.\n- A new evaluation suite, including DarijaMMLU, DarijaHellaSwag, and DarijaBench, was developed for comprehensive LLM assessment in Darija, focusing on discriminative and generative tasks. \n- An experimental analysis was conducted on fine-tuning strategies and base model choices, finding that instruction-tuned Gemma 2 models with LoRA performed optimally.",
        "classification": [
            "Natural Language Processing",
            "Translation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/MBZUAI-Paris/Atlas-Chat-9B",
            "https://hf.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
        "authors": "sebgao, wangpichao, meihaiyang, tonghe, ZechenBai",
        "link": "https://arxiv.org/abs/2409.19603",
        "github_repo": "https://github.com/showlab/VideoLISA",
        "summary": "-\nVideoLISA, a video-based multimodal large language model, is introduced for language-instructed reasoning segmentation in videos. \n- It leverages the reasoning capabilities of large language models and the Segment Anything Model to generate temporally consistent segmentation masks.\n- VideoLISA employs a Sparse Dense Sampling strategy, balancing temporal context and spatial detail, and a One-Token-Seg-All approach using a <TRK> token for object tracking.\n- Evaluations on various benchmarks, including the newly introduced ReasonVOS, demonstrate its superior performance in video object segmentation with complex reasoning.\n- While optimized for videos, it generalizes well to image segmentation, showing potential as a foundation model for language-instructed object segmentation.",
        "classification": [
            "Computer Vision",
            "Image Segmentation",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/showlab/VideoLISA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Illustrious: an Open Advanced Illustration Model",
        "authors": "Junha Lee, leehg57, mhy9910, solbon1212, andyp-nvidia",
        "link": "https://arxiv.org/abs/2409.19946",
        "github_repo": null,
        "summary": "\n- Illustrious, a state-of-the-art, open-source anime image generation model, leverages a large dataset and detailed prompt guidance to generate high-resolution, dynamic images with anatomical integrity.\n- The model focuses on three key improvements: batch size and dropout control for faster concept activation learning, increased training resolution for accurate anatomy depiction, and refined multi-level captions covering tags and natural language for enhanced model development.\n- Illustrious outperforms existing models in animation style and allows for easier customization, as shown in Figure 17, which demonstrates diverse image generation using various prompts.\n- Evaluation using Elo Rating, TrueSkill 2, and Character-wise Image Comparison (CCIP) demonstrates Illustrious's superior performance compared to other models.\n- The model addresses limitations of existing datasets and text encoders within the illustration/animation domain by employing techniques like No Dropout Token and Quasi-Register Tokens.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0",
            "https://huggingface.co/datasets/nyanko7/danbooru2023"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation",
        "authors": "Filippos Kokkinos, Andrea Vedaldi, philiptorr, JianyuanWang, Junlinh",
        "link": "https://arxiv.org/abs/2410.00890",
        "github_repo": null,
        "summary": "Flex3D is a novel two-stage framework for generating high-quality 3D content from text, single images, or sparse view images.\n- The first stage generates a pool of candidate views using fine-tuned multi-view image and video diffusion models, followed by a selection process to filter these views based on quality and consistency.\n- The second stage introduces FlexRM (Flexible Reconstruction Model), a transformer-based architecture that reconstructs detailed 3D Gaussian points from the selected views using a tri-plane representation.\n- Flex3D also employs a novel training strategy simulating imperfect input views by injecting noise into the generated 3D Gaussian points to enhance robustness for generation tasks. \n- Experimental results show that Flex3D achieves state-of-the-art performance in both 3D generation and reconstruction tasks, with a user study win rate exceeding 92% in generation tasks. \n- FlexRM outperforms other baselines in 3D reconstruction across various input view settings, demonstrating its flexibility and efficiency.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer",
        "authors": "Jingren, chenweix7, chaojiemao, jingfengzhang, jiangzeyinzi",
        "link": "https://arxiv.org/abs/2410.00086",
        "github_repo": null,
        "summary": " - ACE, a unified framework based on a Diffusion Transformer, supports a wide range of visual generation and editing tasks through natural language instructions, including text-guided generation, low-level visual analysis, controllable generation, semantic editing, element editing, repainting, layer editing, and reference generation.\n- ACE introduces the Long-context Condition Unit (LCU) to incorporate historical information from previous generation rounds, enabling multi-turn and long-context generation.\n- A meticulous data collection workflow is established to construct a 0.7 billion-scale dataset covering various generation and editing tasks.\n- Evaluation on benchmarks such as MagicBrush and a user study on a manually curated benchmark demonstrates ACE\u2019s superior performance in various visual generation tasks.\n- ACE can be easily integrated into a multimodal chat system to streamline image creation and editing, avoiding cumbersome pipelines typically employed in visual agents.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/runwayml/stable-diffusion-v1-5",
            "https://huggingface.co/runwayml/stable-diffusion-inpainting"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models",
        "authors": "Xiaolong Wang, Xuxin Cheng, Zipeng Fu, Qi Wu, cbfinn",
        "link": "https://arxiv.org/abs/2410.00231",
        "github_repo": null,
        "summary": "-\"Helpful DoggyBot\" is introduced, a quadrupedal robot system for open-world object fetching in indoor environments, integrating a 1-DoF gripper, a learned whole-body controller, and vision-language models (VLMs).\n-The system uses a two-phase training process for the controller, focusing on whole-body control and agility, leveraging privileged information in simulation and distilling it to a deployable policy using egocentric depth.\n-VLMs are used for zero-shot generalization to unseen environments and objects, enabling open-vocabulary object detection, efficient navigation, and precise grasping.\n-Real-world experiments show that the system achieves a 60% first-attempt success rate in fetching objects from beds and sofas, outperforming baselines and approaching teleoperation performance in terms of average time to completion.\n-The system demonstrates the potential of integrating learned controllers and VLMs for complex mobile manipulation tasks in unstructured indoor settings.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Object Detection"
        ],
        "github_urls": [
            "https://helpful-doggybot.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
        "authors": "Shubham Tulsiani, Donglai Xiang, Jeff Tan, gengshan-y, devakramanan",
        "link": "https://arxiv.org/abs/2409.20563",
        "github_repo": null,
        "summary": "DressRecon reconstructs temporally consistent 4D human body models from monocular RGB videos, focusing on challenging scenarios with loose clothing and object interactions.\n- It leverages a hierarchical \"bag-of-bones\" motion model, disentangling body and clothing deformations as separate layers.\n- The method uses image-based priors such as human body pose, surface normals, and optical flow to optimize the model and improve reconstruction quality.\n- The resulting neural fields can be converted to meshes or further optimized as explicit 3D Gaussians for interactive rendering.\n- On datasets with challenging clothing deformations, DressRecon shows superior performance over prior art in terms of reconstruction fidelity and rendering quality.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Visual Context Window Extension: A New Perspective for Long Video Understanding",
        "authors": "Zhenzhong Chen, hcwei",
        "link": "https://arxiv.org/abs/2409.20018",
        "github_repo": null,
        "summary": "This research paper proposes a novel approach to enhance long video understanding by extending the visual context window of Large Multimodal Models (LMMs).\n- It redefines the context window in LMMs as two distinct windows: visual and language, addressing the discrepancies between these modalities.\n- The study introduces a method to extend positional embeddings within the visual context window, enabling LMMs to handle lengthy videos without retraining on large video-text datasets.\n- A progressive pooling strategy is implemented to reduce memory consumption by selectively adjusting the spatial resolution of frame embeddings.\n- Experimental results on benchmarks like MLVU, VideoMME, and LongVideoBench demonstrate consistent performance improvements with increasing video frames, outperforming models like GPT-40 and achieving memory savings of approximately 45%.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs",
        "authors": "Qing Lian, Xu Yan, Yingjie Cai, Weichao Qiu, Leheng Li",
        "link": "https://arxiv.org/abs/2410.00337",
        "github_repo": null,
        "summary": "-\nSyntheOcc, a novel image generation framework, achieves fine-grained 3D geometric control by conditioning on 3D occupancy labels, enabling applications like 3D editing, dataset generation, and long-tailed scene generation.\n- It leverages 3D semantic Multi-Plane Images (MPIs) as conditional input, offering precise spatial alignment with generated images.\n- An MPI encoder and reweighing strategies enhance image quality and recognizability.\n- SyntheOcc outperforms existing methods in generating realistic and controllable street view images, as demonstrated by its superior performance on the nuScenes dataset.\n- The synthetic data generated by SyntheOcc effectively augments perception models for 3D occupancy prediction.",
        "classification": [
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration",
        "authors": "Michael Elad, Michato, ohayonguy",
        "link": "https://arxiv.org/abs/2410.00418",
        "github_repo": null,
        "summary": "This paper introduces Posterior-Mean Rectified Flow (PMRF), a new algorithm for photo-realistic image restoration.\n- PMRF aims to minimize the mean squared error (MSE) under the constraint of a perfect perceptual index, unlike methods that sample from the posterior or optimize a weighted sum of distortion and perceptual losses.\n- PMRF first predicts the posterior mean and then uses a rectified flow model to transport the result to a high-quality image distribution, approximating the optimal estimator for minimal MSE under perfect perceptual index.\n- PMRF consistently outperforms existing methods on various image restoration tasks, including blind face restoration, as demonstrated by improved FID, KID, PSNR, and SSIM scores on the CelebA-Test benchmark and lower IndRMSE on real-world datasets, while maintaining competitive performance on other perceptual and distortion metrics.\n- PMRF's effectiveness is attributed to its novel framework, which directly targets the optimal MSE estimator under a perfect perceptual index constraint, as shown by its superior performance compared to alternative flow-based methods in controlled experiments.\n- The codes are available at https://github.com/ohayonguy/PMRF.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/ohayonguy/PMRF"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging",
        "authors": "Xiaodong Gu, Chengcheng Wan, Songsong Wang, YerbaPage",
        "link": "https://arxiv.org/abs/2410.01215",
        "github_repo": "https://github.com/YerbaPage/MGDebugger",
        "summary": " - MGDebugger, a hierarchical code debugger, is introduced to improve the pass rate of LLM-generated code by addressing bugs at multiple levels of granularity. \n - MGDebugger decomposes code into subfunctions, debugs them iteratively in a bottom-up manner, and uses an LLM-simulated Python executor to track variable states for precise error identification. \n - Experiments show that MGDebugger significantly outperforms existing debugging systems, achieving an 18.9% accuracy improvement over seed generations in HumanEval and a 97.6% repair success rate in HumanEval-Fix. \n- Ablation studies confirm the effectiveness of hierarchical debugging, and further analysis highlights the robustness of MGDebugger across diverse bug types, code lengths, and debugging attempts. \n- MGDebugger leverages pretrained LLMs for debugging, eliminating task-specific retraining for a lightweight and scalable solution.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/YerbaPage/MGDebugger"
        ],
        "date": "2024-10-03"
    },
    {
        "title": "Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis",
        "authors": "nunonmg, PierreColombo, CelineH, emmanuelmalherbe, hgissbkh",
        "link": "https://arxiv.org/abs/2409.20059",
        "github_repo": null,
        "summary": "This paper conducts an empirical analysis of preference-based alignment techniques for enhancing large language model (LLM)-based translation, focusing on Contrastive Preference Optimization (CPO).\n- CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data regarding alignment metrics, like xCOMET-QE.\n- Preference-based alignment is highly sensitive to the choice of candidate translation systems used for generating preference data, affecting both the alignment metric and downstream metric consistency.\n- Aligning a model using its own translations achieves performance comparable to employing multiple external systems, ensuring better metric consistency. \n- The paper also finds that preference-based lexical alignment using the gold reference as the preferred translation performs poorly. \n- Optimizing preference data in a mono-system setting, specifically setting the quality of the chosen and rejected translations, allows the model to match the performance of multi-system settings.",
        "classification": [
            "Natural Language Processing",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/collections/artefactory/translation-alignment-analysis"
        ],
        "date": "2024-10-03"
    },
    {
        "title": "LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks",
        "authors": "Zhihan Zhang, Tianqing Fang, Mengzhao Jia, kaixinm, wyu1",
        "link": "https://arxiv.org/abs/2410.01744",
        "github_repo": "https://github.com/Jill0001/Leopard",
        "summary": "-\nLEOPARD, a Multimodal Large Language Model (MLLM), specializes in handling text-rich, multi-image tasks, addressing the limitations of existing MLLMs in this area by focusing on high-quality instruction tuning data and image resolution.\n- A new dataset, LEOPARD-INSTRUCT, comprising 925K samples, including 739K designed for text-rich, multi-image scenarios, is introduced to train the model. The dataset focuses on real-world domains like multi-page documents, multi-charts, and webpage snapshots.\n- An adaptive, high-resolution, multi-image encoding module dynamically optimizes the visual sequence length based on image dimensions using pixel shuffling for compression, enabling processing of multiple high-resolution images without information loss.\n- Experiments conducted on 13 benchmarks demonstrate LEOPARD's superior performance in text-rich multi-image benchmarks with a +9.61 point improvement over other open-source MLLMs.\n- The model remains competitive on single image and general-domain tasks, highlighting the benefits of training on high-quality, tailored multi-image datasets",
        "classification": [
            "Multimodal",
            "Document Question Answering",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/Jill0001/Leopard"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation",
        "authors": "galchechik, cohenor, yuvalalaluf, adihaviv, rinong",
        "link": "https://arxiv.org/abs/2410.01731",
        "github_repo": null,
        "summary": "ComfyGen is introduced, which is a large language model (LLM) capable of creating prompt-specific text-to-image workflows to enhance image quality and prompt alignment.\n- It leverages ComfyUI, which stores workflows as JSON files, for easier parsing.\n- It collects 500 user prompts to generate images, scores them using ensemble aesthetic predictors and human preference estimators.\n- It uses two approaches: tuning-based (ComfyGen-FT), learning from user-preference data, and training-free (ComfyGen-IC), using an LLM to select existing flows.\n- The methods shows improved image quality and alignment compared to single models and fixed workflows.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Not All LLM Reasoners Are Created Equal",
        "authors": "Aaron Courville, Daniel Toyama, Alessandro Sordoni, agarwl, arianhosseini",
        "link": "https://arxiv.org/abs/2410.01748",
        "github_repo": null,
        "summary": " - This paper investigates Large Language Models' (LLMs) reasoning abilities on grade-school math (GSM) problems, specifically focusing on compositional GSM problems, where the answer to the first question is a variable in the second question.\n- The study reveals a significant reasoning gap in most LLMs, indicated by a performance difference between solving compositional question pairs and solving each question independently.\n- This gap is more pronounced in smaller, more cost-efficient, and math-specialized models, suggesting potential limitations in reasoning abilities.\n-  Instruction-tuning, code generation, and finetuning have varying effects across LLMs, while finetuning can lead to overfitting.\n- Large reasoning gaps stem from distraction from additional context and poor second-hop reasoning, rather than dataset leakage, impacting performance despite high scores on standard GSM benchmarks.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection",
        "authors": "Dan Xu, Yuanliang, YangCaoCS",
        "link": "https://arxiv.org/abs/2410.01647",
        "github_repo": "https://github.com/yangcaoai/3DGS-DET",
        "summary": "-\n3DGS-DET is introduced, integrating 3D Gaussian Splatting (3DGS) into 3D Object Detection, marking the first such integration.\n- This approach addresses inherent 3DGS limitations by improving spatial differentiation between objects and background and minimizing noisy background blobs.\n- Boundary Guidance leverages 2D boundary information to optimize 3D Gaussian blob distribution for clearer differentiation between objects and background in 3D space, effectively enhancing detection. \n- Box-Focused Sampling employs 2D bounding box projections to construct 3D probability spaces, allowing object-focused sampling of Gaussian blobs for better preservation of object details.\n- Experiments show a significant performance boost of +5.6 mAP@0.25 and +3.7 mAP@0.5 over baseline, notably outperforming the state-of-the-art NeRF-Det by +6.6 mAP@0.25 and +8.1 mAP@0.5 on ScanNet, and +31.5 mAP@0.25 on ARKitScenes.",
        "classification": [
            "Object Detection",
            "Computer Vision",
            "Image-to-3D"
        ],
        "github_urls": [
            "https://github.com/yangcaoai/3DGS-DET"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "HelpSteer2-Preference: Complementing Ratings with Preferences",
        "authors": "okuchaiev, gshennvm, trias702, odelalleau, alexwb",
        "link": "https://arxiv.org/abs/2410.01257",
        "github_repo": null,
        "summary": " \n- This paper introduces HelpSteer2-Preference, a novel dataset of preference annotations designed to complement the existing ratings in the HelpSteer2 dataset, enabling a head-to-head comparison of Bradley-Terry and Regression style reward models.\n- The authors propose a novel approach combining Bradley-Terry and Regression reward modeling, leading to a Llama 3.1 70B Instruct model that achieved a state-of-the-art 94.1 score on RewardBench as of October 1, 2024.\n- The preference annotations are accompanied by human-written justifications, enhancing data interpretability and providing insights into annotator decision-making.\n- The research demonstrates that data format (regression vs. preference) is less critical than the model's ability to capture annotation information, with preference magnitude being key for Bradley-Terry models. \n- The combined reward model effectively aligns language models to follow instructions using online Reinforcement Learning from Human Feedback (RLHF), particularly with the REINFORCE algorithm.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/nvidia/HelpSteer2",
            "https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward"
        ],
        "date": "2024-10-03"
    },
    {
        "title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning",
        "authors": "Guoxuan Wang, danyaljj, ChuyuLiu, ylu610, Dongwei",
        "link": "https://arxiv.org/abs/2410.01044",
        "github_repo": "https://github.com/JHU-CLSP/Rationalyst",
        "summary": "- RATIONALYST, a model pre-trained on implicit rationales extracted from unlabeled text and existing reasoning datasets, is introduced for process-supervision of reasoning.\n- RATIONALYST leverages these implicit rationales during inference to guide the reasoning process of large language models, enhancing both interpretability and performance.\n- It consistently generalizes across various reasoning tasks, demonstrating an average 3.9% accuracy improvement on 7 representative reasoning benchmarks when fine-tuned from LLaMa-3-8B.\n- RATIONALYST outperforms both stronger general-purpose verifiers like GPT-4 and similarly sized models trained on matching datasets, showcasing the efficacy of its process supervision approach.\n- An ablation study shows that rationales from web-scale data enhance performance, while implicit supervision proves more robust than explicit supervision due to tolerance for imperfect rationales.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/JHU-CLSP/Rationalyst"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Quantifying Generalization Complexity for Large Language Models",
        "authors": "maxtiktok, Nrain, zhuokai, Xulianghuang, luohy",
        "link": "https://arxiv.org/abs/2410.01769",
        "github_repo": null,
        "summary": "This paper introduces SCYLLA, a dynamic evaluation framework designed to measure the generalization ability of Large Language Models (LLMs) and disentangle it from memorization.\n- SCYLLA evaluates LLMs across 20 tasks and 5 complexity levels, generating in-distribution and out-of-distribution data to assess generalization.\n- The study reveals a \"generalization valley,\" where the performance gap between in-distribution and out-of-distribution data is non-monotonic with task complexity.\n- The peak of this valley, the \"critical complexity,\" represents the upper bound of an LLM's generalization and shifts to higher complexity levels with increasing model size.\n- The benchmark results covering 28 LLMs show that closed-source models generally exhibit stronger generalization abilities and higher critical complexity than their open-sourced counterparts.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/zhentingqi/scylla"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
        "authors": "George Kopanas, Alexander Mai, xharlie, dorverbin, phedman",
        "link": "https://arxiv.org/abs/2410.01804",
        "github_repo": null,
        "summary": "This paper introduces EVER (Exact Volumetric Ellipsoid Rendering), a new real-time differentiable emission-only volume rendering method.\n- EVER uses constant-density ellipsoids as primitives for scene representation, allowing for exact volume rendering without numerical quadrature, unlike the approximate alpha compositing used in methods like 3D Gaussian Splatting (3DGS).\n- This approach addresses issues like popping artifacts and view-dependent density that are common in 3DGS while maintaining real-time frame rates of ~30 FPS at 720p on an NVIDIA RTX4090.\n- The method achieves sharper results on large-scale scenes from the Zip-NeRF dataset compared to other real-time techniques.\n-  EVER is built upon ray tracing, which enables it to handle effects like radial distortion lensing (fisheye, defocus blur), which is difficult with rasterization-based methods.\n-  EVER\u2019s performance and quality benefits come from its exact rendering of ellipsoid primitives, ensuring 3D consistency by design and resolving blending issues that plague previous techniques like 3DGS.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding",
        "authors": "Ying Shan, Yang Wu, Zhongang Qi, Zongyang Ma, Ye Liu",
        "link": "https://arxiv.org/abs/2409.18111",
        "github_repo": null,
        "summary": "-\"E.T. Bench\", a large-scale benchmark designed for open-ended, event-level video understanding.\n- The benchmark comprises 7.3K samples across 12 tasks, spanning 8 domains and featuring 7K videos totaling 251.4 hours.\n-A novel Video-LLM called \"E.T. Chat\" is introduced, which excels in event-level understanding by treating timestamp prediction as an embedding matching problem.\n- A dedicated instruction-tuning dataset, \"E.T. Instruct 164K\", tailored for multi-event, time-sensitive videos is created.\n- State-of-the-art models on existing video question answering benchmarks struggle with this new benchmark indicating that current methods struggle with fine-grained time-sensitive video understanding.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling",
        "authors": "Jiazhong Yu, Cao Sheng, Fei Li, feifeiobama, ljh0104",
        "link": "https://arxiv.org/abs/2410.01440",
        "github_repo": "https://github.com/Singularity0104/equilibrium-planner",
        "summary": " - This paper introduces equilibrium sequence modeling, a novel method for training large language models (LLMs) to perform long-horizon robotic planning by iteratively refining plans based on environmental feedback through a self-refinement process.\n- The approach formulates self-refinement as a fixed-point problem, allowing for end-to-end supervised training without needing external verifiers or reward models, simplifying training compared to reinforcement learning methods.\n- A nested equilibrium sequence modeling procedure enables efficient closed-loop planning, leveraging feedback from the environment (or a world model) and accelerating plan refinement by reusing previously computed equilibrium solutions.\n- Evaluations on VirtualHome-Env benchmark demonstrate state-of-the-art performance in most metrics, especially when incorporating environmental feedback, and show advantageous scaling of performance with increased inference computation.\n- Ablation studies highlight the effectiveness of equilibrium sequence modeling, reuse of previous solutions, and dynamic computation allocation in improving plan quality and computational efficiency.",
        "classification": [
            "Robotics",
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/Singularity0104/equilibrium-planner"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration",
        "authors": "Xinjie Zhang, Jing Liu, Ruihao Gong, Zining Wang, Yushi Huang",
        "link": "https://arxiv.org/abs/2410.01723",
        "github_repo": null,
        "summary": "This paper introduces HarmoniCa, a novel framework to improve the training and inference processes of Diffusion Transformers by leveraging a feature cache. HarmoniCa features a Step-wise Denoising Training (SDT) to improve consistency between the training and inference processes. It also utilizes an Image Error Proxy-Guided Objective (IEPO) to balance image quality and cache utilization. Experimental results on various datasets and models show that HarmoniCa achieves a 1.5x speedup over PixArt and a 1.24 FID decrease for DiT-XL/2 256x256 with a higher speedup ratio, demonstrating HarmoniCa's superior speedup and quality. Finally, the approach boasts higher training efficiency with no image data and shorter training times compared to similar caching methods.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Selective Aggregation for Low-Rank Adaptation in Federated Learning",
        "authors": "Huijie Fan, Liangqiong-QU, yanranw1, stevezs, gpx333",
        "link": "https://arxiv.org/abs/2410.01463",
        "github_repo": null,
        "summary": " - This research paper introduces FedSA-LoRA, a new method for federated learning that selectively aggregates learned A and B matrices from LoRA.\n- It asserts that A matrices learn general knowledge while B matrices capture client-specific information, leading to only sharing A matrices for aggregation.\n- Experimental validation across language understanding and generation tasks on benchmarks like GLUE and GSM8K demonstrates FedSA-LoRA outperforms other methods. \n- The authors extend this approach to other LoRA variants (rsLoRA and VeRA), creating FedSA-rsLoRA and FedSA-VeRA, and show consistent improvements.\n- The findings provide insights into LoRA in federated settings and a general framework for using future LoRA adaptations.",
        "classification": [
            "Natural Language Processing",
            "Text Classification",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
        "authors": "Chen Chen, Vasileios Saveris, haotiz, Hong-You, jefflai",
        "link": "https://arxiv.org/abs/2410.02740",
        "github_repo": null,
        "summary": "This paper investigates the role of large-scale image-caption data in pre-training multimodal foundation models, particularly focusing on the interplay between synthetic captions and original AltText.\n- It proposes a controllable and scalable captioning pipeline capable of generating diverse caption formats (short, descriptive, dense, AltText-fused).\n- Experiments across CLIP, multimodal LLMs, and diffusion models reveal that a hybrid approach, combining synthetic captions and AltText, often outperforms using synthetic captions alone. \n- Different model types exhibit preferences for specific caption formats: shorter captions for CLIP, descriptive for multimodal LLMs and diffusion models.\n- Combining AltText with synthetic captions enhances performance, likely due to improved image-text alignment from synthetic captions and increased data diversity from AltText.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Video Instruction Tuning With Synthetic Data",
        "authors": "Wei Li, Chunyuan24, liuziwei7, kimingng, ZhangYuanhan",
        "link": "https://arxiv.org/abs/2410.02713",
        "github_repo": null,
        "summary": " - This paper introduces LLaVA-Video, a large multimodal model for video understanding, and LLaVA-Video-178K, a synthetic dataset created for video instruction following.\n- LLaVA-Video-178K consists of 178,510 videos with 1.3 million instruction samples including detailed captions generated with a recurrent, multi-level approach, along with open-ended and multiple-choice question answering generated using GPT-4.\n- The model leverages a SlowFast video representation technique to optimize the balance between frame count and limited GPU memory, enabling processing of three times more frames than traditional methods.\n- LLaVA-Video achieves state-of-the-art results on various video benchmarks, outperforming existing open-source models and demonstrating the effectiveness of the proposed synthetic dataset and training approach.\n- The dataset, codebase, model checkpoints, and a visual chat demo are publicly released to foster development of general-purpose visual assistants.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/lmms-lab/VideoDetailCaption"
        ],
        "date": "2024-10-04"
    },
    {
        "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models",
        "authors": "Tianwei Xiong, XihuiLiu, bykang, Ikuinen, Epiphqny",
        "link": "https://arxiv.org/abs/2410.02757",
        "github_repo": null,
        "summary": " - Loong is a novel autoregressive LLM-based video generator that produces minute-level long videos. \n- It addresses the challenges of imbalanced loss and error accumulation during long video generation by using a progressive short-to-long training strategy with loss re-weighting and video token re-encoding. \n- Loong models text and video tokens as a unified sequence, trained from scratch on both image and video data, unlike prior approaches which use pretrained models. \n- The model utilizes a low-resolution video and later upscales the output to enhance visual quality. \n- User studies show that Loong outperforms StreamingT2V in terms of content consistency and visual text matching.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [
            "https://epiphqny.github.io/Loong-video"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
        "authors": "Chunyuan24, henghuang, thughost, russwang, txiong23",
        "link": "https://arxiv.org/abs/2410.02712",
        "github_repo": null,
        "summary": "**-** LLaVA-Critic is the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess the performance of other multimodal models across various tasks. \n**-** It leverages a new high-quality critic instruction-following dataset incorporating diverse evaluation criteria and scenarios, including pointwise scoring and pairwise ranking. \n**-** The model shows strong performance as an LMM-as-a-Judge, generating evaluation scores and rankings comparable to commercial GPT models. \n**-** In preference learning, LLaVA-Critic generates effective reward signals for iterative Direct Preference Optimization (DPO), surpassing rewards from human feedback as seen in LLaVA-RLHF. \n**-** LLaVA-Critic is open-sourced, including its data, code, checkpoints, and demo.",
        "classification": [
            "Multimodal",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Contrastive Localized Language-Image Pre-Training",
        "authors": "Marcin Eichner, Xinze Wang, haotiz, jefflai, Hong-You",
        "link": "https://arxiv.org/abs/2410.02746",
        "github_repo": null,
        "summary": "-\nCLOC is a new pre-training framework for vision encoders with enhanced localization capabilities.\n- It augments the CLIP loss with a region-text contrastive loss and a lightweight prompter module that extracts region embeddings from the image embedding given spatial hints.\n- A visually-enriched and spatially-localized captioning pipeline is designed to generate region-text pseudo-labels at scale, resulting in a two-billion image-text dataset with fine-grained region-text annotations.\n- CLOC consistently outperforms CLIP on 31 evaluation tasks, including standard image-text tasks, newly constructed region-text tasks, and downstream evaluations with MLLMs, particularly on referring and grounding tasks.\n- The enhanced localization capabilities of CLOC enable it to be a drop-in replacement of CLIP to enhance MLLMs.",
        "classification": [
            "Multimodal",
            "Image Classification",
            "Image Feature Extraction",
            "Visual Question Answering",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/zzliang/GRIT"
        ],
        "date": "2024-10-04"
    },
    {
        "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
        "authors": "Hugo Germain, Aleksei Bochkovskii, srrichter, msantoso98, amael-apple",
        "link": "https://arxiv.org/abs/2410.02073",
        "github_repo": "https://github.com/apple/ml-depth-pro",
        "summary": "\u2022 Depth Pro is a foundation model for zero-shot metric monocular depth estimation that uses a multi-scale Vision Transformer (ViT) architecture.\n\u2022 It synthesizes high-resolution (2.25-megapixel) depth maps in 0.3 seconds on a standard GPU, achieving both speed and accuracy, outperforming previous state-of-the-art methods in boundary tracing and metric depth accuracy (demonstrated by a higher \u03b4\u2081 score and faster inference time compared to baselines).\n\u2022 Depth Pro estimates metric depth with absolute scale without requiring camera intrinsics or metadata, and introduces new evaluation metrics leveraging matting datasets for boundary accuracy assessment.\n\u2022 A two-stage training curriculum combining real and synthetic datasets contributes to enhanced performance, and the inclusion of zero-shot focal length estimation further improves accuracy.\n\u2022 Depth Pro is designed for broader applicability and efficiency in tasks like novel view synthesis and is evaluated on diverse datasets not seen during training to demonstrate its generalization capabilities.",
        "classification": [
            "Depth Estimation",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/apple/ml-depth-pro"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Large Language Models as Markov Chains",
        "authors": "Abdelhakim Benechehab, Oussama Zekri, ievred, NBoulle, ambroiseodt",
        "link": "https://arxiv.org/abs/2410.02724",
        "github_repo": null,
        "summary": " - This paper draws an equivalence between large language models (LLMs) and Markov chains, offering a new theoretical framework to analyze LLM inference.\n - By representing LLMs with vocabulary size *T* and context window *K* as Markov chains on a state space of size O(*T*<sup>*K*</sup>), the authors derive findings on stationary distribution, convergence speed, and temperature influence.\n - The paper derives generalization bounds for pre-training and in-context learning under minimal assumptions, using concentration inequalities for dependent random variables and leveraging insights from the Markov chain equivalence.\n - The theoretical analysis predicts in-context scaling laws that are experimentally validated on recent LLMs (2023-2024), showing that LLMs outperform minimax optimal frequentist Markov chain learning.\n - Experimental results on various Markov chains and dynamical systems further support the theoretical findings and demonstrate the practical implications of the proposed framework.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
        "authors": "Yu Cheng, Jihai Zhang, Spico, Xiaoye08",
        "link": "https://arxiv.org/abs/2409.19291",
        "github_repo": null,
        "summary": "-\n\nThis paper introduces Diversified Multiplet Upcycling (DMU), a novel method for enhancing the Contrastive Language-Image Pre-training (CLIP) model by integrating it with a Mixture of Experts (MoE) architecture. DMU fine-tunes multiple CLIP models from a pre-trained checkpoint using Multistage Contrastive Learning (MCL) to capture diverse feature distributions. These fine-tuned models, sharing parameters except for the Feed-Forward Network, are then used to initialize a CLIP-MoE. The approach significantly improves CLIP's performance on various zero-shot tasks, including retrieval and image classification, as well as in downstream Multimodal Large Language Model (MLLM) benchmarks when serving as a vision encoder. Notably, CLIP-MoE surpasses the base OpenAI CLIP model by approximately 20% on retrieval tasks and exhibits minimal additional training overhead, using only 2% of the computational resources required to train a CLIP from scratch. This method provides a model-agnostic and computationally efficient way to scale CLIP and enhance its ability to capture rich, fine-grained information for improved performance in various multimodal applications.",
        "classification": [
            "Multimodal",
            "Image Feature Extraction",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [
            "https://github.com/OpenSparseLLMS/CLIP-MOE"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
        "authors": "Otmar Hilliges, RMW, msadat97",
        "link": "https://arxiv.org/abs/2410.02416",
        "github_repo": null,
        "summary": "\u2022 Proposes Adaptive Projected Guidance (APG), a method to mitigate oversaturation and artifacts in classifier-free guidance (CFG) at high guidance scales in diffusion models.\n\u2022 Decomposes the CFG update into parallel and orthogonal components, down-weighting the parallel component responsible for oversaturation, while preserving the orthogonal component that enhances image quality.\n\u2022 Introduces rescaling and reverse momentum inspired by gradient ascent to regulate update impact and refine sampling trajectories.\n\u2022 Demonstrates through experiments on various diffusion models that APG improves FID, recall, and saturation scores while maintaining precision comparable to CFG, even with higher guidance scales.\n\u2022 Shows APG compatibility with various conditional diffusion models, samplers, and distilled models, making it a superior plug-and-play alternative to CFG.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
        "authors": "Jun Zhu, Pengle Zhang, Jia wei, Jintao Zhang, surfingtomchen",
        "link": "https://arxiv.org/abs/2410.02367",
        "github_repo": null,
        "summary": "-\nSageAttention, a novel post-training quantization method designed to accelerate attention in Transformer models by quantizing tensors to 8-bit integers.\n- It overcomes the challenges of accuracy degradation in existing methods by smoothing the K matrix to mitigate outlier effects and employing a low-precision FP16 accumulator for the PV matrix multiplication.\n- It integrates effective kernel fusion with ROPE and an online softmax inspired by FlashAttention.\n- Comprehensive experiments demonstrate a 2.1x speed improvement over FlashAttention2 and 2.7x over xFormers on an RTX 4090.\n- It maintains comparable end-to-end metrics across diverse applications, including language, image, and video generation models.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Text2Text Generation",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/thu-ml/SageAttention"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
        "authors": "Xin Yu, Yida Wang, xiaobiaodu",
        "link": "https://arxiv.org/abs/2410.02103",
        "github_repo": null,
        "summary": " - MVGS, a new optimization method for 3D Gaussian Splatting (3DGS)-based novel view synthesis, addresses overfitting issues and enhances 3D geometry and appearance accuracy. \n- The method replaces single-view training with multi-view regulated learning, enabling joint optimization across multiple views and incorporating a cross-intrinsic guidance scheme for coarse-to-fine training.\n- A cross-ray densification strategy increases Gaussian kernels in crucial overlapped 3D regions and a multi-view augmented densification strategy intensifies this process when perspectives differ significantly.\n- MVGS improves novel view synthesis by approximately 1 dB PSNR across various Gaussian-based explicit representation methods and tasks, including general/reflective object and dynamic 4D scene reconstruction.\n- Experiments demonstrate consistent improvements in PSNR, SSIM, and LPIPS across diverse datasets, showcasing MVGS's effectiveness in challenging scenes with reflections, fine details, and lighting variations.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
        "authors": "Jianye Hou, Baibei Ji, Juntao Li, Keyan Zhou, ZetangForward",
        "link": "https://arxiv.org/abs/2410.02115",
        "github_repo": null,
        "summary": "\u2022 L-CiteEval, a new multi-task benchmark for evaluating long-context understanding with citations in large language models (LLMs) is introduced.\n\u2022 The benchmark comprises 11 diverse tasks with context lengths ranging from 8K to 48K tokens and employs automatic evaluation metrics for reproducibility.\n\u2022 Evaluation of 11 LLMs reveals that open-source models lag significantly behind closed-source counterparts in citation accuracy, suggesting reliance on inherent knowledge rather than provided context.\n\u2022 Retrieval-Augmented Generation (RAG) improves faithfulness in open-source LLMs but slightly diminishes generation quality.\n\u2022 A strong correlation is observed between LLMs' attention mechanisms and citation generation process, offering insight into LLM evaluation and development.",
        "classification": [
            "Question Answering",
            "Summarization",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/ZetangForward/L-CITEEVAL.git"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
        "authors": "Rob Fergus, lerrel, upiter",
        "link": "https://arxiv.org/abs/2410.02749",
        "github_repo": null,
        "summary": "-\nLintSeq, a synthetic data generation algorithm, refactors existing code into edit sequences to improve code synthesis in large language models (LLMs).\n- LLMs trained on this data produce more diverse programs, resulting in better inference-time scaling for benchmark pass rate.\n- Tiny (150M parameter) edit sequence LMs achieve state-of-the-art performance for their model class, matching or outperforming models twice their size.\n- Repeated sampling from smaller edit sequence finetuned LLMs achieves HumanEval coverage competitive with GPT-4 at similar cumulative inference cost to single samples from large open-source LLMs.\n- Ablating linter guidance from LintSeq degrades downstream performance.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/upiterbarg/lintseq"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
        "authors": "Michael Ryan, Ella Li, zyanzhe, missblanchett, WillHeld",
        "link": "https://arxiv.org/abs/2410.02678",
        "github_repo": null,
        "summary": "**Summary of \"Distilling an End-to-End Voice Assistant Without Instruction Training Data\"**\n\n- This paper introduces DiVA, a new speech large language model (LLM) trained through knowledge distillation from a text-based LLM, eliminating the need for explicit instruction-following data. DiVA utilizes a novel cross-modal context distillation method, which uses a frozen text-based LLM to guide the audio model's training by matching the output distribution from text transcripts of the audio. The audio input is processed using Whisper for feature extraction and a Q-Former initialized from Whisper's decoder to achieve audio-text feature alignment.\n- DiVA generalizes well to various spoken language tasks such as Spoken Question Answering, Classification (emotion, humor, and sarcasm detection), and Translation, using only ASR data for training.\n- In evaluation benchmarks, DiVA outperforms other open-access Speech and Audio LLMs on question answering by a significant margin despite using substantially less compute for training.\n- DiVA excels in following text-based instructions provided through prompts and user's speech, addressing the \"forgetting\" issue observed in other models trained using supervised fine-tuning. \n- In a user study, DiVA received a 72% preference rate compared to Qwen 2 Audio, demonstrating its effectiveness in real-world scenarios despite some limitations like inheriting the base LLM's bias.",
        "classification": [
            "Multimodal",
            "Audio",
            "Automatic Speech Recognition",
            "Question Answering",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation",
        "authors": "Amir Shmuel, Janine Mendola, amanchadha, gurucharan-marthi",
        "link": "https://arxiv.org/abs/2410.02458",
        "github_repo": null,
        "summary": "This study introduces MedVisionLlama, a novel approach for enhancing medical image segmentation by integrating pre-trained Large Language Model (LLM) transformer blocks into a Vision Transformer (ViT) architecture.\n- The architecture inserts a frozen LLM transformer block into the encoder of a ViT and uses residual connections between the LLM and ViT components, where the LLM block acts as a visual encoder. \n- It proposes a Hybrid Attention Mechanism that balances global and local feature learning and a Multi-Scale Fusion Block to aggregate features across different scales.\n- Experimental results across ten medical imaging modalities from the Medical Segmentation Decathlon (MSD) demonstrate significant performance gains, including an average Dice score increase from 0.74 to 0.79. \n- Ablation studies further validate the effectiveness of incorporating frozen LLM transformer blocks and the proposed hybrid attention mechanism.",
        "classification": [
            "Image Segmentation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
        "authors": "Jianrui Zhang, yjlee0222, mucai",
        "link": "https://arxiv.org/abs/2410.02763",
        "github_repo": null,
        "summary": "\n- This paper introduces Vinoground, a novel temporal counterfactual benchmark for evaluating Large Multimodal Models (LMMs) on dense temporal reasoning in short videos.\n- Vinoground contains 1000 short video and caption pairs with captions containing the same words but in different orders to create temporal counterfactuals.\n- The benchmark evaluates an LMM\u2019s ability to distinguish temporal differences between actions and object transformations (e.g., \"water turning into ice\u201d vs. \u201cice turning into water\u201d).\n- Experimental results show that even state-of-the-art LMMs struggle with temporal reasoning, with the best model (GPT-40) achieving only 54% accuracy on text score and much worse on other metrics, while human performance is around 90%.\n- All open-source models and CLIP-based models perform much worse, suggesting that existing methods struggle at fully understanding video temporality.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal"
        ],
        "github_urls": [
            "https://vinoground.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data",
        "authors": "manocha, ctnzr, rafaelvalle, ZhifengKong, SreyanG-NVIDIA",
        "link": "https://arxiv.org/abs/2410.02056",
        "github_repo": "https://github.com/Sreyan88/Synthio",
        "summary": "Synthio is a novel approach to augment small-scale audio classification datasets using synthetic data generated from text-to-audio (T2A) diffusion models, aligning the generated data with the target dataset's acoustic characteristics through preference optimization.\n- It addresses the challenge of creating diverse synthetic augmentations by introducing MixCap, a technique that leverages Large Language Models (LLMs) to generate and refine meaningful audio captions used for prompting the T2A model.\n- Synthio's evaluation across ten datasets and four limited-data settings demonstrates consistent outperformance of existing baselines, improving classification accuracy by 0.1% to 39% using a T2A model trained solely on weakly-captioned AudioSet.\n- Ablation studies show the vital role of preference optimization and MixCap in achieving optimal results.\n- Additional analysis demonstrates effectiveness of Synthio in enhancing captioning tasks and addressing long-tail categories.",
        "classification": [
            "Audio",
            "Audio Classification",
            "Text-to-Audio"
        ],
        "github_urls": [
            "https://github.com/Sreyan88/Synthio"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Addition is All You Need for Energy-efficient Language Models",
        "authors": "Wei Sun, luohy",
        "link": "https://arxiv.org/abs/2410.00907",
        "github_repo": null,
        "summary": "-\nThe paper proposes a novel linear-complexity multiplication (L-Mul) algorithm to approximate floating-point multiplication with integer addition, aiming to reduce energy consumption in large language models (LLMs).\n-\nL-Mul replaces expensive floating-point multiplications with less energy-intensive integer additions and introduces an offset to maintain accuracy.\n-\nThe authors claim L-Mul achieves higher precision and requires less computation compared to 8-bit floating-point multiplications and 80% energy reduction for dot products.\n-\nExperiments on various LLMs and tasks (MMLU, BBH, GSM8k, visual question answering) showed that L-Mul in attention layers maintained or even slightly improved performance compared to standard multiplication and outperformed float8 with training free setting.\n-\nFine-tuning models with all multiplications replaced by 3-bit L-Mul achieved comparable results to models using float8_e4m3 accumulation, showcasing its potential for efficient LLM training and deployment.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "NL-Eye: Abductive NLI for Images",
        "authors": "Zorik Gekhman, yonatanbitton, nitay, tokeron, MorVentura",
        "link": "https://arxiv.org/abs/2410.02613",
        "github_repo": null,
        "summary": "\n- NL-EYE, a benchmark designed to evaluate the visual abductive reasoning skills of Visual Language Models (VLMs), is introduced.\n- NL-EYE tasks models with evaluating the plausibility of hypothesis images given a premise image, requiring explanations for their choices and consisting of 350 image triplets across six reasoning categories: physical, functional, logical, emotional, cultural, and social.\n- Results show that while humans perform well, VLMs struggle, often failing to surpass random baselines in plausibility prediction.\n- Even with correct predictions, VLM explanations are frequently unhelpful, indicating weaknesses in visual interpretation and accurate representation generation for reasoning.\n- Further analysis suggests that VLMs face challenges with temporal reasoning, absolute judgments, and non-correlational tasks, particularly emotional reasoning.",
        "classification": [
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Selective Attention Improves Transformer",
        "authors": "Yossi Matias, Matan Kalman, yanivle",
        "link": "https://arxiv.org/abs/2410.02703",
        "github_repo": null,
        "summary": "-\"Selective Attention\" is introduced; a parameter-free adjustment to the standard attention mechanism in Transformers, enabling a token to deem another as no longer relevant for future tokens and masking it, improving language modelling performance across various model sizes and context lengths.\n-It allows for reduction in the attention context buffer size without quality loss, resulting in significant memory and compute savings during inference, achieving up to 16X, 25X, and 47X memory reduction for context sizes of 512, 1024, and 2048 respectively with a 100M parameter model trained on C4.\n-Selective attention transformers often outperform standard transformers with ~2X more parameters and heads in their attention module.\n-Visualizations show selective attention exhibiting dynamic context pruning behavior; masking previous assignments to the same variable in variable assignment, masking ambiguous inputs until ambiguity resolution, and retaining only necessary elements in tasks like Parity and Copy.\n-Evaluation on C4 dataset shows consistent perplexity improvements across different model sizes and context lengths; further improvements via explicit loss to encourage masking, and HellaSwag benchmark reveals consistent accuracy gains across various model sizes using selective attention.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
        "authors": "Susanna Loeb, ddemszky, carlycodes, Analu, rose-e-wang",
        "link": "https://arxiv.org/abs/2410.03017",
        "github_repo": null,
        "summary": " - This paper introduces Tutor CoPilot, a Human-AI system designed to enhance real-time tutoring in K-12 education by providing expert-like guidance to tutors as they interact with students. \n- Tutor CoPilot leverages the Bridge method, which captures expert decision-making patterns and adapts Large Language Models (LLMs) to generate contextually relevant suggestions for tutors during live sessions. \n- A randomized controlled trial involving 900 tutors and 1,800 K-12 students demonstrates that Tutor CoPilot significantly improves student learning outcomes, particularly for students with lower-rated tutors. \n- Analysis of over 550,000 chat messages reveals that tutors using Tutor CoPilot are more likely to employ high-quality pedagogical strategies that foster student understanding and less likely to simply provide answers. \n- Tutor CoPilot offers a scalable and cost-effective solution (\n$20 per tutor annually) for enhancing tutoring quality, especially in under-served communities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models",
        "authors": "Jeonga Wi, Junyoung Choi, Jiun, DK9, longshiine",
        "link": "https://arxiv.org/abs/2409.19989",
        "github_repo": null,
        "summary": "RoCoTex is a novel diffusion-based method for high-quality, consistent texture synthesis on 3D meshes.\n- The method employs a symmetrical view synthesis strategy with regional prompts, which leverage a 2D diffusion prior (SDXL) along with multiple ControlNets, to enhance view consistency and texture alignment with the underlying 3D geometry.\n-  A confidence-based texture blending technique and a Differential Diffusion-based soft-inpainting method minimize seam artifacts and inconsistencies between different views.\n- Quantitative results demonstrate that RoCoTex achieves state-of-the-art performance in terms of image quality, diversity and user preference compared to existing methods.\n- User studies confirm that RoCoTex generates textures with superior quality, consistency, and alignment compared to baseline approaches such as TEXTure, Text2Tex and Paint3D.",
        "classification": [
            "Text-to-Image",
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Erasing Conceptual Knowledge from Language Models",
        "authors": "David Bau, Samuel Marks, sfeucht, RohitGandikota",
        "link": "https://arxiv.org/abs/2410.02760",
        "github_repo": null,
        "summary": "-\nThis research introduces Erasure of Language Memory (ELM), a novel method for removing specific concepts from large language models (LLMs) while preserving fluency and general knowledge.\n-\nELM employs a multi-objective fine-tuning approach with targeted low-rank updates (LoRA).\n-\nThe method optimizes for erasure of the target concept, retention of unrelated information, and generation fluency when prompted with the erased concept.\n-\nExperiments on biosecurity, cybersecurity, and literary domains demonstrate ELM\u2019s efficacy in achieving near-random performance on erased topics while maintaining high scores on general knowledge benchmarks and generating more fluent text than baseline methods.\n-\nELM also exhibits robustness against adversarial attacks, further highlighting its potential for safe and controlled LLM editing.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/rohitgandikota/erasing-llm"
        ],
        "huggingface_urls": [
            "https://huggingface.co/cais/Zephyr_RMU"
        ],
        "date": "2024-10-07"
    },
    {
        "title": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond",
        "authors": "gduggal, Man1kandan, Madddy, HARI45SH, shubhii0712",
        "link": "https://arxiv.org/abs/2410.02362",
        "github_repo": null,
        "summary": " - This survey paper explores Mamba architectures, a type of State Space Model (SSM), for medical image analysis.\n- Mamba offers linear time complexity, efficient processing of long sequences and strong performance in multimodal data merging, making it suitable for complex medical image analysis tasks.\n- The paper discusses core SSM concepts, various Mamba architecture designs (pure, U-Net variants, and hybrid models), optimization techniques, adaptations for different learning paradigms (weakly, semi-, and self-supervised learning), and diverse applications in medical image segmentation, classification, restoration, registration, and other miscellaneous tasks.\n-  Experimental results demonstrate that Mamba models outperform or are comparable to attention and transformer-based methods on benchmark medical datasets, like BraTS2023, ISIC2017, and ACDC.\n- The paper also discusses Mamba's limitations, including spatial information loss and parameter initialization challenges, along with emerging research areas like Mamba 2 and xLSTM.",
        "classification": [
            "Image Segmentation",
            "Image Classification",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
        "authors": "wpiioos, Unmanned-YuBeen, lastdefiance20, PurpleSand, MilkClouds",
        "link": "https://arxiv.org/abs/2410.01273",
        "github_repo": null,
        "summary": " - CANVAS, a novel framework for intuitive human-robot interaction, is introduced for commonsense-aware navigation. It combines visual and linguistic instructions to generate robot actions, leveraging pre-trained vision-language models (VLMs) to achieve this.\n- A new dataset called COMMAND, containing 48 hours of driving data over 219 kilometers with human-annotated instructions and navigation outcomes across office, street and orchard simulated environments, was collected to train and test the model.\n- Experimental results show that CANVAS consistently outperforms the rule-based ROS NavStack in all environments, especially in challenging scenarios like uneven terrain or misleading instructions, with higher success and lower collision rates.\n- CANVAS achieves successful Sim2Real transfer with a 69% success rate in a real-world office setting, demonstrating its robustness beyond simulated data.\n- Ablation study confirms that using pre-trained VLM weights improves performance considerably, indicating the usefulness of existing knowledge for navigation tasks.",
        "classification": [
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction",
        "authors": "Heming Weng, Genesis Wang, yh1567, zjy2001",
        "link": "https://arxiv.org/abs/2410.02241",
        "github_repo": null,
        "summary": "-\nMIGA, a Mixture of Experts with Group Aggregation framework, is proposed for stock market prediction.\n- MIGA employs a two-stage design: an expert router that encodes stock data and assigns weights to experts, and an expert group aggregation stage that facilitates information sharing among experts within groups.\n- MIGA outperforms existing end-to-end models on three Chinese Stock Index benchmarks (CSI300, CSI500, and CSI1000), with MIGA-Conv achieving a 24% excess annual return on CSI300, surpassing the previous state-of-the-art by 8%.\n- A comprehensive analysis reveals the specialization of MIGA's experts for different types of stocks and market conditions.\n- The paper explores the impact of expert aggregation size and inner group attention on model performance, demonstrating their effectiveness in enhancing prediction accuracy and stability.",
        "classification": [
            "Time Series Forecasting"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "NRGBoost: Energy-Based Generative Boosted Trees",
        "authors": "joaobravo",
        "link": "https://arxiv.org/abs/2410.03535",
        "github_repo": null,
        "summary": "-\nNRGBoost, a novel energy-based generative boosting model, is introduced. The model is trained to maximize a local second-order approximation to the likelihood at each boosting round, analogous to XGBoost.\n- An amortized sampling approach is proposed to reduce the training time, which is often dominated by sampling in energy-based models.\n- The algorithm achieves similar discriminative performance to XGBoost on several real-world tabular datasets while remaining competitive with other generative models for sampling.\n- It also outperforms alternative generative approaches on smaller datasets and produces visually similar samples to real data on MNIST and California Housing datasets.\n- The work also explores bagged ensembles of Density Estimation Trees (DET) with feature subsampling as a generative counterpart to Random Forests.",
        "classification": [
            "Tabular",
            "Tabular Classification",
            "Tabular Regression"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Differential Transformer",
        "authors": "Li Dong, thegenerality, sunyt32, yuqxia, ytz20",
        "link": "https://arxiv.org/abs/2410.05258",
        "github_repo": null,
        "summary": "\u2022 This paper introduces the Differential Transformer (DIFF Transformer), a novel architecture for large language models (LLMs) designed to improve attention to relevant context and mitigate noise.\n\u2022 The core innovation is the differential attention mechanism, which calculates attention scores as the difference between two separate softmax attention maps, thus canceling noise and promoting sparse attention patterns.\n\u2022 Experimental results on language modeling demonstrate that DIFF Transformer outperforms standard Transformer models in various scaling settings, requiring only about 65% of the model size or training tokens to achieve comparable performance.\n\u2022 The model also exhibits advantages in downstream tasks such as long-context modeling, key information retrieval, hallucination mitigation, and in-context learning.\n\u2022 Additionally, DIFF Transformer demonstrates increased robustness to order permutation in in-context learning and a reduction in activation outliers, which presents opportunities for model quantization.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations",
        "authors": "Roi Reichart, Zorik Gekhman, belinkov, tokeron, hadasor",
        "link": "https://arxiv.org/abs/2410.02707",
        "github_repo": null,
        "summary": " \n- This paper investigates the internal representations of large language models (LLMs) and their connection to the phenomenon of hallucinations.\n- The research finds that truthfulness information is highly localized within exact answer tokens, leading to improved error detection when probing these specific tokens.\n- The study demonstrates that while error detection is enhanced by focusing on these tokens, probing classifiers trained on one dataset often fail to generalize effectively to others, indicating that truthfulness mechanisms are skill-specific.\n- The authors further categorize LLM errors based on repeated sampling, showing that error types are predictable from internal representations.\n- Finally, they highlight a discrepancy between LLM internal encoding and external behavior, revealing that models may internally identify the correct answer but consistently generate an incorrect one, suggesting the potential for harnessing this existing knowledge to reduce errors.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/technion-cs-nlp/LLMsKnow"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "FAN: Fourier Analysis Networks",
        "authors": "Yongding Tao, Ge Li, Jingjingxu, zkcpku, dongyh",
        "link": "https://arxiv.org/abs/2410.02675",
        "github_repo": null,
        "summary": "-\nThis paper introduces the Fourier Analysis Network (FAN), a novel neural network architecture designed to effectively model and reason about periodic phenomena by incorporating Fourier Series into its structure and computational process.\n-\nFAN aims to address the limitations of existing neural networks, such as MLPs and Transformers, which often struggle to generalize periodic functions beyond the training data domain.\n-\nThe architecture consists of stacking FAN layers where each layer outputs a concatenation of cosine, sine transformations, and an activation function applied to a linear transformation of the input.\n-\nExperimental results demonstrate FAN's superior performance compared to MLP, KAN, and Transformer on various tasks, including symbolic formula representation, time series forecasting, and language modeling tasks.\n-\nBy seamlessly replacing MLP layers with FAN layers, models achieve improved generalization while reducing parameters and FLOPs.",
        "classification": [
            "Time Series Forecasting",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/YihongDong/FAN"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
        "authors": "Jong Chul Ye, geonyoung-park, bryanswkim, DHCAI",
        "link": "https://arxiv.org/abs/2410.04364",
        "github_repo": null,
        "summary": " - VideoGuide is a novel framework that enhances the temporal consistency of pre-trained text-to-video diffusion models without requiring any additional training or fine-tuning.\n- It leverages any pre-trained video diffusion model or itself as a guide during the initial steps of inference, improving temporal quality by interpolating the guide model's denoised samples into the sampling model's denoising process.\n- VideoGuide significantly improves both subject and background consistency without sacrificing image quality or motion smoothness. \n- This method demonstrates prior distillation, where the base model's text coherence is enhanced by leveraging the superior data prior of the guiding model. \n- By applying VideoGuide, underperforming video diffusion models achieve state-of-the-art quality.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
        "authors": "Jonah Casebeer, Ge Zhu, Njb, tberg12, ZacharyNovack",
        "link": "https://arxiv.org/abs/2410.05167",
        "github_repo": null,
        "summary": "\n- Presto! is a new dual-faceted distillation approach for accelerating score-based diffusion transformers by reducing sampling steps and the cost per step.\n- Presto includes score-based distribution-matching distillation for continuous-time diffusion (EDM) using a GAN, improved conditional layer distillation with better-preserved hidden-state variance, and combined layer-step distillation.\n- For step distillation, Presto-S achieves best-in-class performance among step distillation techniques and matches the original model quality with 4-step inference.\n- When combined with the novel layer distillation Presto-L, which independently outperforms SOTA layer dropping and base diffusion sampling, the resulting Presto-LS approach accelerates the model by 10-18x, generating 32-second mono audio in 230ms and stereo audio in 435ms on an A100 40GB GPU, outperforming Stable Audio Open by 15x.",
        "classification": [
            "Audio",
            "Text-to-Audio"
        ],
        "github_urls": [
            "https://presto-music.github.io/web/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Named Clinical Entity Recognition Benchmark",
        "authors": "Cl\u00e9ment Christophe, Tathagata Raha, Muhammad Umar Salman, Marco AF Pimentel, Wadood M Abdul",
        "link": "https://arxiv.org/abs/2410.05046",
        "github_repo": null,
        "summary": "-\nThis paper introduces a Named Clinical Entity Recognition (NER) benchmark designed for evaluating language models in healthcare.\n- This benchmark encompasses a curated selection of publicly accessible medical datasets with standardized entities adhering to the Observational Medical Outcomes Partnership (OMOP) Common Data Model.\n- The leaderboard accommodates various language model architectures, including encoder, decoder, and GLiNER models, and employs standardized evaluation metrics, predominantly the F1-score, to ensure consistent performance comparisons.\n- Initial findings from the leaderboard indicate superior performance by GLiNER-based models over decoder-only architectures, commonly used in Large Language Models (LLMs).\n- The choice of evaluation strategy, token-based or span-based, has been found to influence model ranking.",
        "classification": [
            "Natural Language Processing",
            "Token Classification"
        ],
        "github_urls": [
            "https://github.com/WadoodAbdul/clinical_ner_benchmark"
        ],
        "huggingface_urls": [
            "https://huggingface.co/m42-health/clinical_ner_leaderboard",
            "https://huggingface.co/spaces/m42-health/clinical_ner_leaderboard"
        ],
        "date": "2024-10-08"
    },
    {
        "title": "OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction",
        "authors": "Xu Yan, Weichao Qiu, bingbl, Evenc, lilelife",
        "link": "https://arxiv.org/abs/2410.04932",
        "github_repo": null,
        "summary": "OmniBooth is a novel image generation framework that enables spatial control with instance-level multi-modal customization using text or image guidance.  It leverages latent control signals, a high-dimensional spatial feature, to seamlessly integrate spatial, textual, and image conditions.  The method expands the capabilities of text-to-image generation, providing enhanced performance in image synthesis fidelity and alignment.  Experimental results demonstrate improved performance compared to existing methods on instance segmentation tasks and image quality metrics.  OmniBooth is a unified framework for text and image conditioned generation offering improved flexibility and control compared to existing methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://len-li.github.io/omnibooth-web"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models",
        "authors": "Rui Wang, Tong Xiao, tbpangolin, pzzhang, deqing",
        "link": "https://arxiv.org/abs/2410.04734",
        "github_repo": null,
        "summary": " - This paper introduces TLDR, a novel token-level reward model designed to improve the performance and interpretability of large vision-language models (VLMs).\n- The TLDR model assigns rewards to individual tokens rather than entire sequences, enabling finer-grained feedback and more precise identification of errors, like hallucinations.\n- A perturbation-based method is used to generate synthetic hard negatives for training TLDR, enhancing its robustness.\n- Experiments demonstrate that TLDR significantly improves VLM performance in various tasks and reduces human annotation time by approximately threefold.\n- The study shows that the proposed model speeds up human annotation by 3 times in acquiring high-quality vision-language data.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "UniMuMo: Unified Text, Music and Motion Generation",
        "authors": "Yutong Zhang, Kun Su, Han Yang, auspicious3000, Jiaben",
        "link": "https://arxiv.org/abs/2410.04534",
        "github_repo": null,
        "summary": "  - UniMuMo is a unified multimodal model that uses a transformer-based encoder-decoder architecture to generate music, motion, and text from any combination of the three modalities as input.\n- The model bridges the modalities through a unified encoder-decoder architecture after converting inputs to a token-based representation and addresses the lack of time-synchronized data by aligning unpaired music and motion data based on rhythmic patterns and using existing large-scale datasets of single modalities. \n- It utilizes a music codebook to encode motion and introduces a music-motion parallel generation scheme.\n- This design unifies all music and motion generation tasks into a single transformer decoder architecture with one training task of music-motion joint generation and can be efficiently achieved by fine-tuning existing pre-trained single-modality models.\n- Extensive evaluations shows that UniMuMo achieves competitive results across all unidirectional generation benchmarks including text-to-music, music-to-motion, motion-to-music, music captioning and motion captioning.",
        "classification": [
            "Multimodal",
            "Text-to-Audio",
            "Text-to-Video",
            "Audio-to-Audio",
            "Audio-to-Audio",
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://hanyangclarence.github.io/unimumo_demo/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
        "authors": "Tong Che, Jingdi Lei, schrodingers-tiger, jwu323, qq8933",
        "link": "https://arxiv.org/abs/2410.02884",
        "github_repo": null,
        "summary": "LLaMA-Berry is a new framework for enhancing the mathematical reasoning ability of Large Language Models (LLMs) by combining Monte Carlo Tree Search (MCTS) with iterative Self-Refine and a pairwise reward model.\n- The framework uses Self-Refine applied to MCTS (SR-MCTS) to optimize the reasoning path by leveraging the self-critic and rewriting capabilities of LLMs.\n- A Pairwise Preference Reward Model (PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is used to evaluate different reasoning paths globally.\n- An Enhanced Borda Count (EBC) method synthesizes pairwise preferences between solutions into a global ranking score to identify better answers.\n- Experimental results on benchmarks like GSM8K, MATH, AIME24, AMC23, and GPQA Diamond demonstrate that LLaMA-Berry significantly improves the performance of LLaMA-3.1-8B, achieving results competitive with GPT-4 Turbo without additional training.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs",
        "authors": "cxiong, lunshi, hendrydong, yuhuixu, demolei",
        "link": "https://arxiv.org/abs/2410.04698",
        "github_repo": null,
        "summary": "**- MATHHAY: An automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs.**\n**- Unlike previous benchmarks, MATHHAY requires both information retrieval and complex mathematical reasoning, focusing on real-world scenarios within a specified time period.**\n**- Includes questions of varying difficulty levels across different input lengths (32K, 64K, 128K) and utilizes a combination of rule-based exact matching and LLM-based judgment for evaluation.**\n**- Experimental results reveal that even top-performing LLMs like Gemini struggle with long contexts in mathematical reasoning, indicating room for improvement.**\n**- Open-source models significantly underperform compared to closed-source models.**",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles",
        "authors": "siminniu, fan2goa1, WinfredShi, Ki-Seki, Duguce",
        "link": "https://arxiv.org/abs/2410.05262",
        "github_repo": null,
        "summary": " - TurtleBench is a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) using real user guesses from an online Turtle Soup Puzzle game.\n- This dynamic approach creates a bilingual dataset (Chinese and English) with 1532 annotated user guesses, which are then used to test the reasoning abilities of the LLMs. \n- The benchmark emphasizes reasoning ability and minimizes reliance on memorization and background knowledge. \n- Nine advanced LLMs, including open and closed-source models, were tested on TurtleBench. \n- The results show that Claude-3.5-Sonnet and GPT-4 performed best but that OpenAI's o1 series models performed sub-optimally.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/mazzzystar/TurtleBench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion",
        "authors": "fcole, trevordarrell, hurjunhwa, irwinherrmann, Junyi42",
        "link": "https://arxiv.org/abs/2410.03825",
        "github_repo": null,
        "summary": "-\nMotion DUSt3R (MonST3R) is a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes, effectively adapting DUSt3R's representation for dynamic scenes by estimating a pointmap for each timestep.\n- The key insight is that by estimating pointmaps per timestep and aligning them in the same camera coordinate frame it is possible to handle the dynamics of the scene without explicitly modelling motion.\n- This approach addresses the challenge of scarce training data by posing the problem as a fine-tuning task and strategically training the model on limited dynamic, posed videos with depth labels.\n- MonST3R achieves strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency, and demonstrates promising results for 4D reconstruction.\n- It improves performance on Sintel dataset when finetuned with PointOdyssey, TartanAir, Spring and Waymo datasets.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Video Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Autonomous Character-Scene Interaction Synthesis from Text Instruction",
        "authors": "thuhsy, YixinChen, awfuact, milleret, jnnan",
        "link": "https://arxiv.org/abs/2410.03187",
        "github_repo": null,
        "summary": "\u2022 This paper introduces a new framework for synthesizing multi-stage, scene-aware human motion in 3D environments from text instructions and goal locations. \n\u2022 It uses an auto-regressive diffusion model to generate realistic and coherent motion sequences, along with an autonomous scheduler for stage transitions. \n\u2022 A dual voxel scene encoder captures both current and imminent scene contexts for enhanced realism and collision avoidance. \n\u2022 The method integrates frame embeddings with language input for precise semantic guidance, and a stage-specific goal encoder conditions motion generation relative to current interaction goals. \n\u2022 Results show the model's ability to generate high-quality motions closely aligned with text instructions and scene constraints, showcasing an improvement over existing methods for locomotion, object reaching, and interaction motion synthesis.",
        "classification": [
            "Computer Vision",
            "Text-to-Video",
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Grounding Language in Multi-Perspective Referential Communication",
        "authors": "alsuhr, mao1207, ZinengTang",
        "link": "https://arxiv.org/abs/2410.03959",
        "github_repo": null,
        "summary": "This paper introduces a new task and dataset for evaluating referring expression generation and comprehension in multi-agent embodied environments. The dataset, comprising 2,970 human-written referring expressions, requires agents to consider each other's perspective when generating and understanding references to objects.  The authors find that model performance lags behind that of human agents in both generation and comprehension tasks.  A speaker model fine-tuned using communicative success significantly improves performance, surpassing even a strong proprietary model (GPT-40). The contributions include a novel platform for generating 3D scenes, a new dataset, and analysis of language strategies in embodied referential communication.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/zinengtang/MulAgentRef"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "authors": "Peijie Dong, wenxinsiju, xuminghui, Dominic789654",
        "link": "https://arxiv.org/abs/2410.04199",
        "github_repo": null,
        "summary": "-\nLongGenBench, a synthetic benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs), focusing on consistency and logical flow.\n-\nIt redesigns question formats, requiring LLMs to provide single, cohesive long-context answers encompassing multiple questions within a single query.\n-\nEvaluation on LongGenBench reveals performance degradation across both API-accessed and open-source LLMs in long-context scenarios, ranging from 1.2% to 47.1%.\n-\nDifferent LLM series show varying degradation trends, with Gemini-1.5-FLASH exhibiting minimal degradation among API-accessed models, and QWEN2 series showing minimal degradation among open-source models.\n-\nModel size influences performance decline, with larger models within a series generally demonstrating less degradation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization",
        "authors": "Francois Charton, Justin Wang, shizhuo2",
        "link": "https://arxiv.org/abs/2410.04717",
        "github_repo": null,
        "summary": "-\nThis paper investigates the impact of instruction diversity on the generalization ability of Large Language Models (LLMs), focusing solely on instruction-following capabilities and isolating them from reasoning and knowledge retrieval.\n- Through controlled string rewriting experiments inspired by the Turing-complete Markov algorithm and mathematical deduction tasks, the study demonstrates that generalization to unseen instructions emerges only when training data is sufficiently diverse across semantic domains.\n- Findings reveal that diversifying data within limited domains does not guarantee robust generalization, while cross-domain diversification significantly enhances adaptability to new instructions.\n- The research further shows that increasing the diversity of training data can lead to performance improvements in real-world scenarios, including code generation and reasoning tasks with both specialized and generalist models. \n- The results underscore the importance of strategic data diversification over simply increasing data size, offering guidelines for improving instruction-tuning datasets and enhancing model performance across various domains.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
        "authors": "lifengshang, YuxinJiang, Tiezheng, yufeiwang201217a, DonJoey",
        "link": "https://arxiv.org/abs/2410.05193",
        "github_repo": null,
        "summary": "\u2022 REVISEVAL, a novel evaluation paradigm, leverages the revision capabilities of Large Language Models (LLMs) to generate response-adapted references for evaluating text generation quality. \n\u2022 It revises the generated response based on the given instruction and evaluation rubric, then uses the revised text as a reference for subsequent evaluation by either LLM-as-a-Judge or classic text evaluation metrics.\n\u2022 REVISEVAL outperforms reference-free and reference-based evaluation methods across various NLG and instruction-following tasks using both open-source and proprietary LLMs. \n\u2022 Response-adapted references enhance the performance of classic metrics, sometimes even rivaling LLM-as-a-Judge. \n\u2022 REVISEVAL effectively reduces bias in evaluation, such as verbosity and positional biases, and its effectiveness is linked to the relevance of the generated references.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
        "authors": "Sinan Tan, Jinze, JustinLin610, ZefanCai, leonardPKU",
        "link": "https://arxiv.org/abs/2410.01912",
        "github_repo": "https://github.com/chenllliang/DnD-Transformer",
        "summary": "-\nThe paper introduces the 2-Dimensional Autoregression (DnD) Transformer, a novel autoregressive model for image generation that addresses the information loss bottleneck of vector quantization (VQ) by predicting more codes for an image through a new autoregression direction (depth) alongside the traditional sequence length.\n- The DnD-Transformer inserts multiple prediction heads into the backbone transformer decoder to predict depth codes within a single forward pass, enhancing image quality without increasing model size or sequence length.\n- On ImageNet 256x256 generation, DnD-Transformer achieves up to 1.54 FID and 82.6 IS improvements without increased model size or sequence length, surpassing the larger LlamaGen model.\n- It demonstrates an emergent vision-language intelligence by generating images with rich text and graphical elements in a self-supervised manner, solely trained on images, even outperforming diffusion models on rich-text image datasets.\n- It opens up a new optimization perspective in autoregressive image generation by introducing a new autoregression direction that reduces information loss and efficiently reconstructs images.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/chenllliang/DnD-Transformer"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
        "authors": "Haocheng Shen, Peize Sun, Shoufa Chen, Tianheng Cheng, Zongming Li",
        "link": "https://arxiv.org/abs/2410.02705",
        "github_repo": "https://github.com/hustvl/ControlAR",
        "summary": " - ControlAR, a new framework, allows autoregressive models to perform controllable image generation from spatial control inputs like edges, depth maps, and segmentation masks.\n- ControlAR uses a lightweight control encoder, based on a Vision Transformer, to transform control images into sequential control tokens.\n- It employs a conditional decoding strategy, where the next image token is predicted based on both previous image tokens and the corresponding control token, demonstrating better performance and efficiency than pre-filling methods.\n- ControlAR extends autoregressive models to arbitrary-resolution image generation by conditioning on control token inputs of varying sizes.\n- Experiments show that ControlAR achieves highly competitive performance with state-of-the-art diffusion-based methods and strong control capability across various tasks, even surpassing ControlNet++ in some cases.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/hustvl/ControlAR"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
        "authors": "Yu Sun, Shuohuan Wang, Huang Fang, Haoran Sun, Yekun Chai",
        "link": "https://arxiv.org/abs/2410.02743",
        "github_repo": "https://github.com/ernie-research/MA-RLHF",
        "summary": "\n- MA-RLHF, a new Reinforcement Learning from Human Feedback (RLHF) framework, is introduced to improve large language model alignment with human preferences.  It leverages \"macro actions\" which are sequences of tokens or higher-level language constructs. \n- This approach reduces the temporal distance between actions and rewards, addressing the credit assignment problem in token-level RLHF, and facilitates faster and more accurate credit assignment. \n- The model achieves substantial performance improvements across various tasks, including up to a 30% gain in summarization, an 18% gain in dialogue, and an 8% gain in question answering, while demonstrating a 1.7x-2x faster convergence compared to standard RLHF. \n- MA-RLHF's robustness is highlighted through experiments conducted with different model sizes (2B to 27B) on various tasks, such as text summarization with the TL;DR dataset and dialogue generation with the HH-RLHF dataset. \n- Further analysis explores termination strategies for macro actions, demonstrating the effectiveness of n-gram and parsing-based approaches in improving model performance.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Summarization",
            "Text2Text Generation",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/ernie-research/MA-RLHF"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Dahoas/full-hh-rlhf"
        ],
        "date": "2024-10-09"
    },
    {
        "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
        "authors": "Yufan Zhou, Shizhe Diao, Yu Cheng, Zhiyang Xu, WHB139426",
        "link": "https://arxiv.org/abs/2410.03290",
        "github_repo": null,
        "summary": "**-** This paper introduces Grounded-VideoLLM, a novel Video Large Language Model (Video-LLM) designed for fine-grained temporal grounding in videos. \n**-** Grounded-VideoLLM uses a two-stream architecture, encoding spatial information from keyframes and temporal dynamics from multiple frames using a video encoder, to create a temporally-aware video representation.\n**-**  It introduces discrete temporal tokens into the LLM's vocabulary for representing timestamps efficiently, avoiding tokenization of numerical text and integrating time representations directly into the LLM.  \n**-** A multi-stage training approach is employed, progressing from video-caption alignment to temporal token alignment and finally multi-task instruction tuning on datasets incorporating temporal grounding tasks.\n**-** Experimental results demonstrate that Grounded-VideoLLM achieves state-of-the-art performance on various fine-grained temporal grounding tasks including Temporal Sentence Grounding, Dense Video Captioning and Grounded VideoQA, as well as general video understanding benchmarks.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/WHB139426/Grounded-Video-LLM"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
        "authors": "yuyijiong",
        "link": "https://arxiv.org/abs/2410.04422",
        "github_repo": null,
        "summary": " - This paper investigates the underlying reasons why Long Context Language Models (LCLMs) struggle with complex tasks, despite their ability to handle extensive text. \n- Through experiments with synthetic datasets, the study identifies \"multi-matching retrieval\" (retrieving multiple items simultaneously) and \"logic-based retrieval\" (using logic within retrieval criteria) as the core challenges, and further defines them as \"hyper-multi-step\" problems.\n- \"Hyper-multi-step\" implies that these seemingly simple tasks actually comprise a large number of indivisible sub-steps, which increases with context length and exceeds the processing capacity of current LCLMs. \n- The paper provides empirical evidence through linear probing of hidden states and analysis of attention weights, demonstrating that these problems are more akin to complex arithmetic tasks, rather than traditional retrieval, and are therefore not adequately addressed by existing techniques such as Retrieval-Augmented Generation (RAG) or Chain-of-Thought (CoT) prompting. \n- The study concludes that simply increasing the context window size of LCLMs may not suffice; instead, future research should focus on addressing the numerous steps involved and explore alternative solutions, such as using external tools.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    }
]