[
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": " - This research introduces MaskLLM, a novel learnable pruning method that generates semi-structured (N:M) sparsity in Large Language Models (LLMs) for enhanced inference efficiency.\n- MaskLLM distinguishes itself from previous methods by directly learning the distribution of N:M sparsity patterns using Gumbel Softmax sampling, enabling end-to-end training on large datasets and addressing limitations of hand-crafted importance criteria.\n-  Empirical evaluations on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, demonstrate that MaskLLM surpasses state-of-the-art techniques, achieving a perplexity of 6.72 on LLaMA2-7B compared to SparseGPT's 10.42.\n- The research underscores the efficacy of learning sparsity patterns directly from data, leading to more accurate and efficient compression of LLMs without compromising performance.\n- The adaptability of MaskLLM to downstream tasks and its ability to achieve lossless compression in certain scenarios highlight its potential for practical applications.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "This research proposes LLaVA-3D, a novel framework for building 3D-aware Large Multimodal Models (LMMs) by adapting the existing 2D LLaVA model.  LLaVA-3D introduces the concept of \"3D Patches,\" which inject 3D positional embeddings into 2D image features, enhancing the model's spatial understanding without complex 3D processing pipelines.  Evaluations demonstrate LLaVA-3D's state-of-the-art performance on various 3D tasks, including question answering, dense captioning, and visual grounding, surpassing existing 3D LMMs while maintaining comparable 2D image understanding capabilities to its 2D counterpart. The research highlights the advantages of leveraging pre-trained 2D LMMs for 3D scene understanding and the benefits of integrating 3D spatial information into 2D visual features.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Image-to-Text",
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "This paper introduces EMOVA, a novel end-to-end multimodal large language model capable of perceiving and generating images, text, and speech with emotional expressiveness. EMOVA utilizes a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for end-to-end speech processing.  The model achieves state-of-the-art performance on both vision-language and speech benchmarks, outperforming models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks and surpassing the speech LLM Mini-Omni in ASR tasks. EMOVA also enables emotional spoken dialogue by explicitly predicting speech style labels (emotions and pitches) and leveraging a lightweight style module for controllable speech synthesis. This is achieved through a novel text-centric multimodal alignment approach, which leverages publicly available bimodal data and eliminates the reliance on scarce trimodal data.",
        "classification": [
            "Multimodal",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": "\u2022 This paper introduces Lotus, a novel diffusion-based visual foundation model designed for high-quality dense prediction tasks, specifically focusing on depth and surface normal estimation.\n\u2022 Lotus leverages the rich visual priors acquired from pre-trained text-to-image diffusion models, enabling strong zero-shot generalization capabilities.\n\u2022 The authors systematically analyze the diffusion formulation and identify that the standard parameterization type and multi-step diffusion process are not optimal for dense prediction, proposing a more effective adaptation protocol.\n\u2022 Through extensive experiments, Lotus demonstrates state-of-the-art performance on various benchmark datasets for zero-shot depth and normal estimation with significantly less training data compared to previous methods. \n\u2022 Notably, Lotus achieves superior efficiency due to its single-step diffusion formulation, making it hundreds of times faster than many existing diffusion-based methods.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": "This research paper introduces GemFilter, a novel approach to reduce the computational cost and latency of processing long context inputs with Large Language Models (LLMs). GemFilter leverages the ability of early LLM layers to identify relevant tokens and compresses the input sequence by a factor of 1000x for subsequent processing by the full model. Empirical evaluations show that GemFilter achieves a 2.4x speedup and 30% reduction in GPU memory consumption compared to state-of-the-art methods, while maintaining comparable performance on benchmarks like LongBench and outperforming them on the Needle in a Haystack task. GemFilter is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting the selected input sequence.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": " - This paper proposes a novel post-training objective function for Latent Diffusion Models (LDMs) by incorporating a pixel-space loss alongside the standard latent-space loss during fine-tuning.  \n- This method enhances the generation of high-frequency details and reduces visual flaws in generated images.\n- Human evaluation demonstrates that this approach significantly improves both supervised fine-tuning and preference-based post-training on DiT and U-Net based LDMs. \n- For example, it boosts visual appeal win rate by 12.9% in DiT and 5.9% in Emu during preference-based fine-tuning,  and achieves a 32.8% vs 9.3% win rate on visual flaws with supervised fine-tuning on DiT. \n- The paper also shows the benefit of using SimPO over DPO for reward modeling in diffusion models.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": " - This research paper investigates implicit instruction tuning, demonstrating that instruction following can emerge without explicit instruction-response training. \n- The authors show that training solely on responses (response tuning) and on narrow-domain data (single-task finetuning) leads to broad instruction-following abilities in language models. \n- For instance, response-tuned models achieve a 43% win rate against explicitly instruction-tuned models in head-to-head evaluations. \n- Furthermore, they introduce a simple rule-based language model that, when combined with a pretrained model, exhibits instruction-following behavior. \n- These findings highlight that adaptation methods not explicitly designed for instruction following can implicitly induce such capabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This research paper introduces a novel technique named \"TOKEN POOLING\" for enhancing the efficiency of multi-vector retrieval models, especially focusing on ColBERT, without significantly affecting performance.  The method uses clustering techniques to group together similar token representations and then applies mean pooling to create a single, representative vector, effectively reducing the overall storage footprint. Experiments show that this approach reduces the required vector count by 50% without compromising accuracy, and a 66% reduction still yields strong performance. The paper also demonstrates that TOKEN POOLING can be effectively combined with existing quantization methods, leading to even more significant compression rates while maintaining reasonable retrieval performance. ",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": " - This paper introduces Disco4D, a novel Gaussian Splatting framework that generates animatable 3D clothed human avatars from a single image. \n- Disco4D disentangles clothing from the human body, representing the former with Gaussian models and the latter with the SMPL-X model, enhancing generation detail and flexibility.\n- Disco4D utilizes diffusion models to refine textures during 3D generation and extrapolate unseen views during 4D animation, surpassing existing methods in fidelity and view consistency.\n- Disco4D enables fine-grained editing and animation of generated avatars, including direct manipulation of clothing Gaussians and pose-driven animation.\n- The authors highlight Disco4D's limitations, such as reliance on robust SMPL-X estimations and limitations in multi-layered clothing modeling, pointing to future research directions.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": " - This paper presents a comprehensive review of the emerging field of Conversation Analysis (CA), a process designed to extract critical information from conversational data and leverage it for system optimization and decision-making.  - The paper systematically defines CA as a four-step procedure: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, and discusses the challenges and trends within each step. - The paper further argues that while previous CA efforts focused on atomic tasks with limited business impact, the rise of Large Language Models (LLMs) enables deeper, more insightful analysis and strategic decision-making from conversations.  - The authors compile and categorize existing benchmark datasets for CA but highlight a significant gap in comprehensive benchmarks containing fine-grained conversation elements and long-context modeling capabilities.  - The paper concludes by outlining future research directions, including the development of LLM-based conversation simulators, fine-grained CA benchmarks, long-context conversation modeling, in-depth attribution analysis, and advanced goal-directed optimization and evaluation methods.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging Knowledge Graphs (KGs) and graph-based architectures. The authors demonstrate the effectiveness of their framework by applying it to the SoccerNet dataset, a large dataset of soccer videos. Their findings show that Structured-GraphRAG significantly improves query processing efficiency, reduces response times, and enhances accuracy compared to traditional RAG methods. The structured nature of KGs reduces hallucinations in LLMs, making the responses more consistent and reliable. The authors highlight that their framework can be applied to a broad range of applications due to its flexible design.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": " - This paper introduces Robot See Robot Do (RSRD), a novel approach for enabling robots to imitate articulated object manipulation from a single monocular RGB human demonstration. \n- Given a static multi-view object scan and a monocular human interaction video, RSRD reconstructs the 3D motion trajectories of object parts using a method called 4D Differentiable Part Models (4D-DPM). \n- RSRD achieves an average success rate of 87% for each phase (object pose registration, trajectory planning, grasping, and motion execution) and an end-to-end success rate of 60% across 90 trials involving 9 different objects. \n- The method leverages DINO feature fields and an analysis-by-synthesis paradigm for robust 3D motion recovery from monocular video. \n- RSRD demonstrates the potential of object-centric learning from human demonstrations for enabling robots to acquire new manipulation skills in a user-friendly and efficient manner.",
        "classification": [
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": " - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.\n - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.\n -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.\n -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.\n -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Any-to-Any"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": "This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence.",
        "classification": [
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": " - This paper introduces Lotus, a novel diffusion-based visual foundation model specifically designed for high-quality dense prediction tasks, including depth and normal estimation.\n - By employing a novel adaptation protocol that includes direct annotation prediction, a single-step diffusion formulation, and a detail preserver, Lotus effectively leverages visual priors from pre-trained text-to-image diffusion models for enhanced zero-shot generalization.\n - Lotus outperforms existing methods in zero-shot depth and normal estimation benchmarks, achieving state-of-the-art results with minimal training data, as evidenced by its superior performance on datasets such as NYUv2, KITTI, ETH3D for depth and NYUv2, ScanNet, iBims-1, and Sintel for normal estimation.\n - Moreover, Lotus exhibits significant efficiency improvements compared to previous diffusion-based methods, being hundreds of times faster for inference, as demonstrated in a comparison of inference times at different image resolutions.\n - The model's adaptability and performance make it suitable for various downstream applications like 3D reconstruction and scene understanding.",
        "classification": [
            "Computer Vision",
            "Depth Estimation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": "- This paper introduces a novel post-training method for Latent Diffusion Models (LDMs) that enhances image generation quality by incorporating a pixel-space loss function alongside the conventional latent-space loss.\n- The authors demonstrate that this approach significantly improves both visual appeal and reduces visual flaws, as evidenced by a 32.8% vs 9.3% win rate in visual flaws and a 34.8% vs 16.6% win rate in visual appeal during supervised fine-tuning of a DiT model.\n-  Furthermore, the method proves effective for both supervised fine-tuning and preference-based post-training, showing consistent improvements across different LDM architectures like DiT and U-Net.\n- The paper also explores variations in reward modeling during preference-based fine-tuning, finding that combining SimPO with the proposed pixel-space loss yields the most substantial enhancements.\n- The authors highlight the simplicity and general applicability of their method, allowing for seamless integration into existing LDM pipelines and potentially benefiting future post-training techniques.",
        "classification": [
            "Computer Vision",
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": "This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that \"response tuning,\" which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that \"single-task finetuning,\"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": " - This paper introduces Disco4D, a novel framework that generates and animates 3D clothed human avatars from a single image using disentangled Gaussian representations.\n - Disco4D utilizes SMPL-X for body representation and separate Gaussian models for clothing and accessories, leading to improved detail and flexibility compared to existing methods that merge body and clothing into a single mesh. \n - The paper demonstrates Disco4D's superior performance in fidelity and geometry through quantitative comparisons on standard benchmarks (Synbody and CloSe), outperforming competing methods like DreamGaussian, LGM, and SHERF.\n -  Beyond static 3D generation, Disco4D enables animation through SMPL-X pose sequences and learns clothing dynamics from monocular videos, achieving more realistic and nuanced clothing movements.\n - Disco4D's disentanglement approach allows for fine-grained, localized editing of individual clothing items, enabling color changes, object removal, and swapping clothing items without impacting other parts of the avatar.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": "\u2022 This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field.\n\u2022 The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA.\n\u2022 The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses.\n\u2022 The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach.\n\u2022 The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs).",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": "- This paper presents Robot See Robot Do (RSRD), a novel approach to teach robots how to manipulate articulated objects with moving parts through a single monocular human demonstration. \n- RSRD constructs a 4D Differentiable Part Model (4D-DPM) from a multi-view static object scan and uses it to track object part motion from the monocular video. \n- RSRD outperforms photometric-based tracking approaches, achieving a mean average point distance of 7.5mm on tracking manipulated object part poses. \n- The robot then leverages the recovered part motions to plan its own motion to achieve the same object configuration change, demonstrating a 60% end-to-end success rate across various objects and re-orientations. \n- The proposed method is particularly notable for its zero-shot capability, requiring no task-specific training data or annotations.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": "\n- MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference.\n- Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end.\n- Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models.\n- MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks.\n- The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "-\nLLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings.\n- This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks.\n- LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets.\n- The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks.\n- Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Visual Question Answering",
            "Image-to-Text",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "- EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities.\n- It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control.\n- The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data.\n- EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models.\n- A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue.",
        "classification": [
            "Multimodal",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Text-to-Audio",
            "Audio-to-Audio",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": " - Lotus, a diffusion-based visual foundation model, is introduced for high-quality dense prediction, specializing in depth and normal map estimation.\n- It utilizes a novel single-step diffusion process with direct annotation prediction for improved performance and a detail preserver to enhance predictions in intricate areas.\n- Evaluation on standard datasets like NYUv2, KITTI, ETH3D, ScanNet, iBims-1, and Sintel shows that Lotus achieves state-of-the-art zero-shot results, outperforming competitors trained on much larger datasets, especially Marigold trained with 74K vs 59K images used for training Lotus, achieving an avg. rank between 1.0 - 7.0 vs 1.5 - 2.5 on Lotus across all datasets and metrics.\n- Lotus offers significant efficiency gains, being hundreds of times faster than existing diffusion-based methods.\n- The efficiency and quality enable various applications like joint estimation and 3D reconstruction.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": " - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs. \n- It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference. \n- This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV. \n- Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilter\u2019s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O. \n- Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [
            "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
            "https://huggingface.co/mistralai/Mistral-Nemo-Base-2407",
            "https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
        ],
        "date": "2024-09-29"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": " - This research proposes a novel pixel-space post-training method for Latent Diffusion Models (LDMs) to enhance the generation of high-frequency details and complex compositions, which are often imperfect in LDMs.\n- This method addresses the limitations of latent space training by adding pixel-space supervision during post-training, thereby preserving details lost in the compression of the latent space. \n- Human evaluations on a DiT transformer model demonstrate a significant improvement of 18.2% in visual appeal and 23.5% in reduction of visual flaws with supervised fine-tuning, and 17.8% and 11.3% with preference-based fine-tuning using this method compared to a latent-space baseline. \n- This improvement is also validated on U-Net diffusion models, showing a 32.8% improvement on visual flaws with the same fine-tuning dataset. \n- This simple method can be easily integrated into any existing LDM, offering advancements in both supervised and preference-based post-training.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": "\n- This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose. \n- Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data). \n- Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models. \n- Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain. \n- A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/john-hewitt/implicit-ins"
        ],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": " - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era.\n- It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations.\n- The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs.\n- It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs.\n-  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security.",
        "classification": [
            "Natural Language Processing",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations.\n- Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets.\n- It can achieve even further reduction of vector count by 66% with less than 3% performance degradation.\n- This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models.\n- The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors.\n- The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/colbert-ir/colbertv2.0"
        ],
        "date": "2024-09-29"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "\n- This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG).\n- It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness.\n- Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time.\n- The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs.\n- While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": "-\"Robot See Robot Do (RSRD)\" is introduced, a method for robots to imitate articulated object manipulation from a single monocular RGB human demonstration, given a static multi-view object scan.\n- RSRD uses 4D Differentiable Part Models (4D-DPM) to recover 3D part motion from monocular video using part-centric feature fields and iterative optimization with geometric regularizers.\n- The robot replicates demonstrated object trajectories by planning bimanual arm motions inducing the same part motion, focusing on the intended behavior rather than mimicking human hand motions.\n- RSRD achieves an average 87% success rate in each phase (registration, planning, grasping, execution), resulting in a 60% total end-to-end success rate across 90 trials with 9 objects.\n- This is achieved using feature fields from pre-trained vision models without task-specific training, fine-tuning, data collection, or annotation.",
        "classification": [
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": "- Disco4D is a novel Gaussian Splatting framework that generates and animates 4D clothed humans from a single image, outperforming existing methods by disentangling clothing from the human body.\n- Disco4D uses the SMPL-X model for body representation and Gaussian models for clothing, allowing for detailed generation and flexibility.\n- It leverages diffusion models to enhance 3D generation, particularly for occluded parts, and includes an identity encoding for each clothing Gaussian for asset separation.\n- Disco4D supports 4D human animation with vivid dynamics, enabling virtual try-on and avatar customization.\n- User studies confirm that Disco4D generates higher-fidelity outputs and aligns better with original image content compared to competing methods.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "MIO: A Foundation Model on Multimodal Tokens",
        "authors": "Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang",
        "link": "https://arxiv.org/abs/2409.17692",
        "github_repo": null,
        "summary": " - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video.\n- It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning.\n- Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines.\n- It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning.\n- MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs.",
        "classification": [
            "Multimodal",
            "Any-to-Any",
            "Text-to-Image",
            "Image-to-Text",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
        "authors": "Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92",
        "link": "https://arxiv.org/abs/2409.17066",
        "github_repo": null,
        "summary": " - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization.\n- VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization.\n- The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization.\n- They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks.\n- VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/microsoft/VPTQ"
        ],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult",
        "authors": "fetong",
        "link": "https://arxiv.org/abs/2409.17545",
        "github_repo": null,
        "summary": "This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs).\n- MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning.\n- Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models.\n- On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B.\n- MIPO simplifies hyperparameter tuning by using only a single parameter, \u03b2, exhibiting robustness across different model architectures and datasets within a specific range.\n- MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback"
        ],
        "date": "2024-09-30"
    },
    {
        "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
        "authors": "Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu",
        "link": "https://arxiv.org/abs/2409.16686",
        "github_repo": null,
        "summary": "- The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales.\n- MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights.\n- Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment.\n- The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios.\n- MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents.",
        "classification": [
            "Robotics",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
        "authors": "nm-w, pdufter, zhegan27, fly6464, haotiz",
        "link": "https://arxiv.org/abs/2409.20566",
        "github_repo": null,
        "summary": " - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning.\n- MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training.\n- It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks.\n- MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI).\n-  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "DiaSynth -- Synthetic Dialogue Generation Framework",
        "authors": "Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl",
        "link": "https://arxiv.org/abs/2409.19020",
        "github_repo": null,
        "summary": " - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning.\n- It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues.\n- Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks.\n- The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data.\n- DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Hyper-Connections",
        "authors": "banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder",
        "link": "https://arxiv.org/abs/2409.19606",
        "github_repo": null,
        "summary": "This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse.\n- Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters.\n- Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections.\n- Hyper-connections are also effective in vision tasks.\n- Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections.\n- The authors anticipate this method's broad applicability across various AI problems.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision",
            "Image Classification",
            "Text Generation",
            "Image-to-Text",
            "Unconditional Image Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
        "authors": "yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li",
        "link": "https://arxiv.org/abs/2409.18943",
        "github_repo": "https://github.com/Geaming2002/Ruler",
        "summary": "- RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs).\n- Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation.\n- RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs.\n- Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments. \n- RULER maintains overall performance on various other benchmarks without affecting non-length based generation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Geaming2002/Ruler"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Cottention: Linear Transformers With Cosine Attention",
        "authors": "Eric C. Larson, TrevorDohm, gmongaras",
        "link": "https://arxiv.org/abs/2409.18747",
        "github_repo": null,
        "summary": "This study introduces \"Cottention,\" a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/gmongaras/Cottention_Transformer"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Image Copy Detection for Diffusion Models",
        "authors": "Yi Yang, Zhentao Tan, Yifan Sun, WenhaoWang",
        "link": "https://arxiv.org/abs/2409.19952",
        "github_repo": null,
        "summary": "-\"ICDiff\", a new Image Copy Detection (ICD) model specialized for diffusion-generated replicas, is introduced, addressing the challenge of content originality in AI-generated images.\n- A novel Diffusion-Replication (D-Rep) dataset comprising 40,000 image-replica pairs, annotated with six replication levels, is created using Stable Diffusion V1.5 and LAION-Aesthetics V2.\n- A new PDF-Embedding method transforms replication levels into probability density functions (PDFs) for supervision, improving performance by leveraging the continuous and smooth nature of replication level probabilities.\n- Experimental results demonstrate PDF-Embedding outperforms protocol-driven and non-PDF methods on D-Rep, highlighting its effectiveness in detecting diffusion-based replication.\n- Analysis reveals replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%, indicating a significant prevalence of content replication in AI-generated images.",
        "classification": [
            "Computer Vision",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://icdiff.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Can Models Learn Skill Composition from Examples?",
        "authors": "Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu",
        "link": "https://arxiv.org/abs/2409.19808",
        "github_repo": null,
        "summary": "This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4.\n- Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills.\n- Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill.\n- The study shows that incorporating skill-rich synthetic text improves compositional capabilities.\n- Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency.\n- Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
        "authors": "Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae",
        "link": "https://arxiv.org/abs/2409.19715",
        "github_repo": null,
        "summary": "COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym"
        ],
        "date": "2024-10-01"
    },
    {
        "title": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding",
        "authors": "Jianzong Wang, Jing Xiao, zhangxulong, Pechola",
        "link": "https://arxiv.org/abs/2409.19627",
        "github_repo": null,
        "summary": "-\nIDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization.\n- It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating.\n- A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability.\n- IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods.\n- Experimental results show its ability to withstand various attacks while maintaining good imperceptibility.",
        "classification": [
            "Audio",
            "Audio-to-Audio"
        ],
        "github_urls": [
            "https://github.com/PecholaL/IDEAW"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
        "authors": "xwhan, ruihou16, xwwang, astonzhang, MingZhong",
        "link": "https://arxiv.org/abs/2409.19951",
        "github_repo": "https://github.com/facebookresearch/llm-cross-capabilities",
        "summary": " - This research paper explores the intersection of multiple abilities, termed \"cross capabilities,\" in Large Language Models (LLMs), which are essential for real-world tasks but often overlooked in current evaluations that focus on individual capabilities.\n- It introduces CROSSEVAL, a benchmark with 1,400 human-annotated prompts and 8,400 human ratings, designed to evaluate both individual and cross capabilities, revealing that current LLMs underperform in cross-capability tasks.\n- The study finds that LLM cross-capability performance adheres to the \"Law of the Weakest Link,\" being significantly limited by the weakest individual capability, regardless of improvements in other areas.\n- The results highlight that tool use is a major challenge for LLMs and suggest that prioritizing the enhancement of weaker capabilities is more crucial for improving overall performance than focusing on already strong ones.\n-  The work emphasizes the importance of shifting focus towards cross-capability evaluation and development to improve LLM effectiveness in complex, real-world scenarios rather than just on individual capabilities.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/llm-cross-capabilities"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices",
        "authors": "Hongfang Yu, Mohsen Guizani, Jiaoshen, LIKirin",
        "link": "https://arxiv.org/abs/2410.00531",
        "github_repo": "https://github.com/Lizonghang/TPI-LLM",
        "summary": "TPI-LLM is a tensor parallel inference system designed for serving 70B-scale LLMs efficiently on low-resource edge devices.\n- It addresses memory limitations by introducing a sliding window memory scheduler that dynamically manages layer weights during inference, overlapping disk I/O with computation and communication.\n- TPI-LLM prioritizes tensor parallelism over pipeline parallelism for single-user scenarios on edge devices and implements a star-based allreduce algorithm to minimize link latency.\n- Experimental results show significant reductions in time-to-first-token, token latency, and peak memory footprint compared to benchmarks like Transformers, Accelerate, and Galaxy.\n- TPI-LLM successfully runs Llama 2-70B with a peak memory footprint of 3.1GB across 8 low-resource devices, enabling larger models to run on edge devices while preserving user privacy.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Lizonghang/TPI-LLM"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect",
        "authors": "imomayiz, amr-mohamed, khoubrane-yousef, habdine, guokan-shang",
        "link": "https://arxiv.org/abs/2409.17912",
        "github_repo": null,
        "summary": "Atlas-Chat introduces the first Large Language Models (LLMs) for Moroccan Arabic, a low-resource dialectal Arabic (DA) variant also known as Darija.\n- A new instruction dataset, Darija-SFT-Mixture, was created by combining existing and new manually and synthetically created Darija resources, as well as translated English instructions.\n- Atlas-Chat-9B and 2B models, fine-tuned on this dataset, outperform existing LLMs, including Arabic-specific and state-of-the-art models like LLaMa, Jais, and AceGPT, achieving a 13% improvement over a 13B model on a new Darija benchmark.\n- A new evaluation suite, including DarijaMMLU, DarijaHellaSwag, and DarijaBench, was developed for comprehensive LLM assessment in Darija, focusing on discriminative and generative tasks. \n- An experimental analysis was conducted on fine-tuning strategies and base model choices, finding that instruction-tuned Gemma 2 models with LoRA performed optimally.",
        "classification": [
            "Natural Language Processing",
            "Translation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/MBZUAI-Paris/Atlas-Chat-9B",
            "https://hf.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
        "authors": "sebgao, wangpichao, meihaiyang, tonghe, ZechenBai",
        "link": "https://arxiv.org/abs/2409.19603",
        "github_repo": "https://github.com/showlab/VideoLISA",
        "summary": "-\nVideoLISA, a video-based multimodal large language model, is introduced for language-instructed reasoning segmentation in videos. \n- It leverages the reasoning capabilities of large language models and the Segment Anything Model to generate temporally consistent segmentation masks.\n- VideoLISA employs a Sparse Dense Sampling strategy, balancing temporal context and spatial detail, and a One-Token-Seg-All approach using a <TRK> token for object tracking.\n- Evaluations on various benchmarks, including the newly introduced ReasonVOS, demonstrate its superior performance in video object segmentation with complex reasoning.\n- While optimized for videos, it generalizes well to image segmentation, showing potential as a foundation model for language-instructed object segmentation.",
        "classification": [
            "Computer Vision",
            "Image Segmentation",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/showlab/VideoLISA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Illustrious: an Open Advanced Illustration Model",
        "authors": "Junha Lee, leehg57, mhy9910, solbon1212, andyp-nvidia",
        "link": "https://arxiv.org/abs/2409.19946",
        "github_repo": null,
        "summary": "\n- Illustrious, a state-of-the-art, open-source anime image generation model, leverages a large dataset and detailed prompt guidance to generate high-resolution, dynamic images with anatomical integrity.\n- The model focuses on three key improvements: batch size and dropout control for faster concept activation learning, increased training resolution for accurate anatomy depiction, and refined multi-level captions covering tags and natural language for enhanced model development.\n- Illustrious outperforms existing models in animation style and allows for easier customization, as shown in Figure 17, which demonstrates diverse image generation using various prompts.\n- Evaluation using Elo Rating, TrueSkill 2, and Character-wise Image Comparison (CCIP) demonstrates Illustrious's superior performance compared to other models.\n- The model addresses limitations of existing datasets and text encoders within the illustration/animation domain by employing techniques like No Dropout Token and Quasi-Register Tokens.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0",
            "https://huggingface.co/datasets/nyanko7/danbooru2023"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation",
        "authors": "Filippos Kokkinos, Andrea Vedaldi, philiptorr, JianyuanWang, Junlinh",
        "link": "https://arxiv.org/abs/2410.00890",
        "github_repo": null,
        "summary": "Flex3D is a novel two-stage framework for generating high-quality 3D content from text, single images, or sparse view images.\n- The first stage generates a pool of candidate views using fine-tuned multi-view image and video diffusion models, followed by a selection process to filter these views based on quality and consistency.\n- The second stage introduces FlexRM (Flexible Reconstruction Model), a transformer-based architecture that reconstructs detailed 3D Gaussian points from the selected views using a tri-plane representation.\n- Flex3D also employs a novel training strategy simulating imperfect input views by injecting noise into the generated 3D Gaussian points to enhance robustness for generation tasks. \n- Experimental results show that Flex3D achieves state-of-the-art performance in both 3D generation and reconstruction tasks, with a user study win rate exceeding 92% in generation tasks. \n- FlexRM outperforms other baselines in 3D reconstruction across various input view settings, demonstrating its flexibility and efficiency.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer",
        "authors": "Jingren, chenweix7, chaojiemao, jingfengzhang, jiangzeyinzi",
        "link": "https://arxiv.org/abs/2410.00086",
        "github_repo": null,
        "summary": " - ACE, a unified framework based on a Diffusion Transformer, supports a wide range of visual generation and editing tasks through natural language instructions, including text-guided generation, low-level visual analysis, controllable generation, semantic editing, element editing, repainting, layer editing, and reference generation.\n- ACE introduces the Long-context Condition Unit (LCU) to incorporate historical information from previous generation rounds, enabling multi-turn and long-context generation.\n- A meticulous data collection workflow is established to construct a 0.7 billion-scale dataset covering various generation and editing tasks.\n- Evaluation on benchmarks such as MagicBrush and a user study on a manually curated benchmark demonstrates ACE\u2019s superior performance in various visual generation tasks.\n- ACE can be easily integrated into a multimodal chat system to streamline image creation and editing, avoiding cumbersome pipelines typically employed in visual agents.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/runwayml/stable-diffusion-v1-5",
            "https://huggingface.co/runwayml/stable-diffusion-inpainting"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models",
        "authors": "Xiaolong Wang, Xuxin Cheng, Zipeng Fu, Qi Wu, cbfinn",
        "link": "https://arxiv.org/abs/2410.00231",
        "github_repo": null,
        "summary": "-\"Helpful DoggyBot\" is introduced, a quadrupedal robot system for open-world object fetching in indoor environments, integrating a 1-DoF gripper, a learned whole-body controller, and vision-language models (VLMs).\n-The system uses a two-phase training process for the controller, focusing on whole-body control and agility, leveraging privileged information in simulation and distilling it to a deployable policy using egocentric depth.\n-VLMs are used for zero-shot generalization to unseen environments and objects, enabling open-vocabulary object detection, efficient navigation, and precise grasping.\n-Real-world experiments show that the system achieves a 60% first-attempt success rate in fetching objects from beds and sofas, outperforming baselines and approaching teleoperation performance in terms of average time to completion.\n-The system demonstrates the potential of integrating learned controllers and VLMs for complex mobile manipulation tasks in unstructured indoor settings.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Object Detection"
        ],
        "github_urls": [
            "https://helpful-doggybot.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
        "authors": "Shubham Tulsiani, Donglai Xiang, Jeff Tan, gengshan-y, devakramanan",
        "link": "https://arxiv.org/abs/2409.20563",
        "github_repo": null,
        "summary": "DressRecon reconstructs temporally consistent 4D human body models from monocular RGB videos, focusing on challenging scenarios with loose clothing and object interactions.\n- It leverages a hierarchical \"bag-of-bones\" motion model, disentangling body and clothing deformations as separate layers.\n- The method uses image-based priors such as human body pose, surface normals, and optical flow to optimize the model and improve reconstruction quality.\n- The resulting neural fields can be converted to meshes or further optimized as explicit 3D Gaussians for interactive rendering.\n- On datasets with challenging clothing deformations, DressRecon shows superior performance over prior art in terms of reconstruction fidelity and rendering quality.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Visual Context Window Extension: A New Perspective for Long Video Understanding",
        "authors": "Zhenzhong Chen, hcwei",
        "link": "https://arxiv.org/abs/2409.20018",
        "github_repo": null,
        "summary": "This research paper proposes a novel approach to enhance long video understanding by extending the visual context window of Large Multimodal Models (LMMs).\n- It redefines the context window in LMMs as two distinct windows: visual and language, addressing the discrepancies between these modalities.\n- The study introduces a method to extend positional embeddings within the visual context window, enabling LMMs to handle lengthy videos without retraining on large video-text datasets.\n- A progressive pooling strategy is implemented to reduce memory consumption by selectively adjusting the spatial resolution of frame embeddings.\n- Experimental results on benchmarks like MLVU, VideoMME, and LongVideoBench demonstrate consistent performance improvements with increasing video frames, outperforming models like GPT-40 and achieving memory savings of approximately 45%.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs",
        "authors": "Qing Lian, Xu Yan, Yingjie Cai, Weichao Qiu, Leheng Li",
        "link": "https://arxiv.org/abs/2410.00337",
        "github_repo": null,
        "summary": "-\nSyntheOcc, a novel image generation framework, achieves fine-grained 3D geometric control by conditioning on 3D occupancy labels, enabling applications like 3D editing, dataset generation, and long-tailed scene generation.\n- It leverages 3D semantic Multi-Plane Images (MPIs) as conditional input, offering precise spatial alignment with generated images.\n- An MPI encoder and reweighing strategies enhance image quality and recognizability.\n- SyntheOcc outperforms existing methods in generating realistic and controllable street view images, as demonstrated by its superior performance on the nuScenes dataset.\n- The synthetic data generated by SyntheOcc effectively augments perception models for 3D occupancy prediction.",
        "classification": [
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration",
        "authors": "Michael Elad, Michato, ohayonguy",
        "link": "https://arxiv.org/abs/2410.00418",
        "github_repo": null,
        "summary": "This paper introduces Posterior-Mean Rectified Flow (PMRF), a new algorithm for photo-realistic image restoration.\n- PMRF aims to minimize the mean squared error (MSE) under the constraint of a perfect perceptual index, unlike methods that sample from the posterior or optimize a weighted sum of distortion and perceptual losses.\n- PMRF first predicts the posterior mean and then uses a rectified flow model to transport the result to a high-quality image distribution, approximating the optimal estimator for minimal MSE under perfect perceptual index.\n- PMRF consistently outperforms existing methods on various image restoration tasks, including blind face restoration, as demonstrated by improved FID, KID, PSNR, and SSIM scores on the CelebA-Test benchmark and lower IndRMSE on real-world datasets, while maintaining competitive performance on other perceptual and distortion metrics.\n- PMRF's effectiveness is attributed to its novel framework, which directly targets the optimal MSE estimator under a perfect perceptual index constraint, as shown by its superior performance compared to alternative flow-based methods in controlled experiments.\n- The codes are available at https://github.com/ohayonguy/PMRF.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/ohayonguy/PMRF"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging",
        "authors": "Xiaodong Gu, Chengcheng Wan, Songsong Wang, YerbaPage",
        "link": "https://arxiv.org/abs/2410.01215",
        "github_repo": "https://github.com/YerbaPage/MGDebugger",
        "summary": " - MGDebugger, a hierarchical code debugger, is introduced to improve the pass rate of LLM-generated code by addressing bugs at multiple levels of granularity. \n - MGDebugger decomposes code into subfunctions, debugs them iteratively in a bottom-up manner, and uses an LLM-simulated Python executor to track variable states for precise error identification. \n - Experiments show that MGDebugger significantly outperforms existing debugging systems, achieving an 18.9% accuracy improvement over seed generations in HumanEval and a 97.6% repair success rate in HumanEval-Fix. \n- Ablation studies confirm the effectiveness of hierarchical debugging, and further analysis highlights the robustness of MGDebugger across diverse bug types, code lengths, and debugging attempts. \n- MGDebugger leverages pretrained LLMs for debugging, eliminating task-specific retraining for a lightweight and scalable solution.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/YerbaPage/MGDebugger"
        ],
        "date": "2024-10-03"
    },
    {
        "title": "Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis",
        "authors": "nunonmg, PierreColombo, CelineH, emmanuelmalherbe, hgissbkh",
        "link": "https://arxiv.org/abs/2409.20059",
        "github_repo": null,
        "summary": "This paper conducts an empirical analysis of preference-based alignment techniques for enhancing large language model (LLM)-based translation, focusing on Contrastive Preference Optimization (CPO).\n- CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data regarding alignment metrics, like xCOMET-QE.\n- Preference-based alignment is highly sensitive to the choice of candidate translation systems used for generating preference data, affecting both the alignment metric and downstream metric consistency.\n- Aligning a model using its own translations achieves performance comparable to employing multiple external systems, ensuring better metric consistency. \n- The paper also finds that preference-based lexical alignment using the gold reference as the preferred translation performs poorly. \n- Optimizing preference data in a mono-system setting, specifically setting the quality of the chosen and rejected translations, allows the model to match the performance of multi-system settings.",
        "classification": [
            "Natural Language Processing",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/collections/artefactory/translation-alignment-analysis"
        ],
        "date": "2024-10-03"
    },
    {
        "title": "LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks",
        "authors": "Zhihan Zhang, Tianqing Fang, Mengzhao Jia, kaixinm, wyu1",
        "link": "https://arxiv.org/abs/2410.01744",
        "github_repo": "https://github.com/Jill0001/Leopard",
        "summary": "-\nLEOPARD, a Multimodal Large Language Model (MLLM), specializes in handling text-rich, multi-image tasks, addressing the limitations of existing MLLMs in this area by focusing on high-quality instruction tuning data and image resolution.\n- A new dataset, LEOPARD-INSTRUCT, comprising 925K samples, including 739K designed for text-rich, multi-image scenarios, is introduced to train the model. The dataset focuses on real-world domains like multi-page documents, multi-charts, and webpage snapshots.\n- An adaptive, high-resolution, multi-image encoding module dynamically optimizes the visual sequence length based on image dimensions using pixel shuffling for compression, enabling processing of multiple high-resolution images without information loss.\n- Experiments conducted on 13 benchmarks demonstrate LEOPARD's superior performance in text-rich multi-image benchmarks with a +9.61 point improvement over other open-source MLLMs.\n- The model remains competitive on single image and general-domain tasks, highlighting the benefits of training on high-quality, tailored multi-image datasets",
        "classification": [
            "Multimodal",
            "Document Question Answering",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/Jill0001/Leopard"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation",
        "authors": "galchechik, cohenor, yuvalalaluf, adihaviv, rinong",
        "link": "https://arxiv.org/abs/2410.01731",
        "github_repo": null,
        "summary": "ComfyGen is introduced, which is a large language model (LLM) capable of creating prompt-specific text-to-image workflows to enhance image quality and prompt alignment.\n- It leverages ComfyUI, which stores workflows as JSON files, for easier parsing.\n- It collects 500 user prompts to generate images, scores them using ensemble aesthetic predictors and human preference estimators.\n- It uses two approaches: tuning-based (ComfyGen-FT), learning from user-preference data, and training-free (ComfyGen-IC), using an LLM to select existing flows.\n- The methods shows improved image quality and alignment compared to single models and fixed workflows.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Not All LLM Reasoners Are Created Equal",
        "authors": "Aaron Courville, Daniel Toyama, Alessandro Sordoni, agarwl, arianhosseini",
        "link": "https://arxiv.org/abs/2410.01748",
        "github_repo": null,
        "summary": " - This paper investigates Large Language Models' (LLMs) reasoning abilities on grade-school math (GSM) problems, specifically focusing on compositional GSM problems, where the answer to the first question is a variable in the second question.\n- The study reveals a significant reasoning gap in most LLMs, indicated by a performance difference between solving compositional question pairs and solving each question independently.\n- This gap is more pronounced in smaller, more cost-efficient, and math-specialized models, suggesting potential limitations in reasoning abilities.\n-  Instruction-tuning, code generation, and finetuning have varying effects across LLMs, while finetuning can lead to overfitting.\n- Large reasoning gaps stem from distraction from additional context and poor second-hop reasoning, rather than dataset leakage, impacting performance despite high scores on standard GSM benchmarks.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection",
        "authors": "Dan Xu, Yuanliang, YangCaoCS",
        "link": "https://arxiv.org/abs/2410.01647",
        "github_repo": "https://github.com/yangcaoai/3DGS-DET",
        "summary": "-\n3DGS-DET is introduced, integrating 3D Gaussian Splatting (3DGS) into 3D Object Detection, marking the first such integration.\n- This approach addresses inherent 3DGS limitations by improving spatial differentiation between objects and background and minimizing noisy background blobs.\n- Boundary Guidance leverages 2D boundary information to optimize 3D Gaussian blob distribution for clearer differentiation between objects and background in 3D space, effectively enhancing detection. \n- Box-Focused Sampling employs 2D bounding box projections to construct 3D probability spaces, allowing object-focused sampling of Gaussian blobs for better preservation of object details.\n- Experiments show a significant performance boost of +5.6 mAP@0.25 and +3.7 mAP@0.5 over baseline, notably outperforming the state-of-the-art NeRF-Det by +6.6 mAP@0.25 and +8.1 mAP@0.5 on ScanNet, and +31.5 mAP@0.25 on ARKitScenes.",
        "classification": [
            "Object Detection",
            "Computer Vision",
            "Image-to-3D"
        ],
        "github_urls": [
            "https://github.com/yangcaoai/3DGS-DET"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "HelpSteer2-Preference: Complementing Ratings with Preferences",
        "authors": "okuchaiev, gshennvm, trias702, odelalleau, alexwb",
        "link": "https://arxiv.org/abs/2410.01257",
        "github_repo": null,
        "summary": " \n- This paper introduces HelpSteer2-Preference, a novel dataset of preference annotations designed to complement the existing ratings in the HelpSteer2 dataset, enabling a head-to-head comparison of Bradley-Terry and Regression style reward models.\n- The authors propose a novel approach combining Bradley-Terry and Regression reward modeling, leading to a Llama 3.1 70B Instruct model that achieved a state-of-the-art 94.1 score on RewardBench as of October 1, 2024.\n- The preference annotations are accompanied by human-written justifications, enhancing data interpretability and providing insights into annotator decision-making.\n- The research demonstrates that data format (regression vs. preference) is less critical than the model's ability to capture annotation information, with preference magnitude being key for Bradley-Terry models. \n- The combined reward model effectively aligns language models to follow instructions using online Reinforcement Learning from Human Feedback (RLHF), particularly with the REINFORCE algorithm.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/nvidia/HelpSteer2",
            "https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward"
        ],
        "date": "2024-10-03"
    },
    {
        "title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning",
        "authors": "Guoxuan Wang, danyaljj, ChuyuLiu, ylu610, Dongwei",
        "link": "https://arxiv.org/abs/2410.01044",
        "github_repo": "https://github.com/JHU-CLSP/Rationalyst",
        "summary": "- RATIONALYST, a model pre-trained on implicit rationales extracted from unlabeled text and existing reasoning datasets, is introduced for process-supervision of reasoning.\n- RATIONALYST leverages these implicit rationales during inference to guide the reasoning process of large language models, enhancing both interpretability and performance.\n- It consistently generalizes across various reasoning tasks, demonstrating an average 3.9% accuracy improvement on 7 representative reasoning benchmarks when fine-tuned from LLaMa-3-8B.\n- RATIONALYST outperforms both stronger general-purpose verifiers like GPT-4 and similarly sized models trained on matching datasets, showcasing the efficacy of its process supervision approach.\n- An ablation study shows that rationales from web-scale data enhance performance, while implicit supervision proves more robust than explicit supervision due to tolerance for imperfect rationales.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/JHU-CLSP/Rationalyst"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Quantifying Generalization Complexity for Large Language Models",
        "authors": "maxtiktok, Nrain, zhuokai, Xulianghuang, luohy",
        "link": "https://arxiv.org/abs/2410.01769",
        "github_repo": null,
        "summary": "This paper introduces SCYLLA, a dynamic evaluation framework designed to measure the generalization ability of Large Language Models (LLMs) and disentangle it from memorization.\n- SCYLLA evaluates LLMs across 20 tasks and 5 complexity levels, generating in-distribution and out-of-distribution data to assess generalization.\n- The study reveals a \"generalization valley,\" where the performance gap between in-distribution and out-of-distribution data is non-monotonic with task complexity.\n- The peak of this valley, the \"critical complexity,\" represents the upper bound of an LLM's generalization and shifts to higher complexity levels with increasing model size.\n- The benchmark results covering 28 LLMs show that closed-source models generally exhibit stronger generalization abilities and higher critical complexity than their open-sourced counterparts.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/zhentingqi/scylla"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
        "authors": "George Kopanas, Alexander Mai, xharlie, dorverbin, phedman",
        "link": "https://arxiv.org/abs/2410.01804",
        "github_repo": null,
        "summary": "This paper introduces EVER (Exact Volumetric Ellipsoid Rendering), a new real-time differentiable emission-only volume rendering method.\n- EVER uses constant-density ellipsoids as primitives for scene representation, allowing for exact volume rendering without numerical quadrature, unlike the approximate alpha compositing used in methods like 3D Gaussian Splatting (3DGS).\n- This approach addresses issues like popping artifacts and view-dependent density that are common in 3DGS while maintaining real-time frame rates of ~30 FPS at 720p on an NVIDIA RTX4090.\n- The method achieves sharper results on large-scale scenes from the Zip-NeRF dataset compared to other real-time techniques.\n-  EVER is built upon ray tracing, which enables it to handle effects like radial distortion lensing (fisheye, defocus blur), which is difficult with rasterization-based methods.\n-  EVER\u2019s performance and quality benefits come from its exact rendering of ellipsoid primitives, ensuring 3D consistency by design and resolving blending issues that plague previous techniques like 3DGS.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding",
        "authors": "Ying Shan, Yang Wu, Zhongang Qi, Zongyang Ma, Ye Liu",
        "link": "https://arxiv.org/abs/2409.18111",
        "github_repo": null,
        "summary": "-\"E.T. Bench\", a large-scale benchmark designed for open-ended, event-level video understanding.\n- The benchmark comprises 7.3K samples across 12 tasks, spanning 8 domains and featuring 7K videos totaling 251.4 hours.\n-A novel Video-LLM called \"E.T. Chat\" is introduced, which excels in event-level understanding by treating timestamp prediction as an embedding matching problem.\n- A dedicated instruction-tuning dataset, \"E.T. Instruct 164K\", tailored for multi-event, time-sensitive videos is created.\n- State-of-the-art models on existing video question answering benchmarks struggle with this new benchmark indicating that current methods struggle with fine-grained time-sensitive video understanding.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling",
        "authors": "Jiazhong Yu, Cao Sheng, Fei Li, feifeiobama, ljh0104",
        "link": "https://arxiv.org/abs/2410.01440",
        "github_repo": "https://github.com/Singularity0104/equilibrium-planner",
        "summary": " - This paper introduces equilibrium sequence modeling, a novel method for training large language models (LLMs) to perform long-horizon robotic planning by iteratively refining plans based on environmental feedback through a self-refinement process.\n- The approach formulates self-refinement as a fixed-point problem, allowing for end-to-end supervised training without needing external verifiers or reward models, simplifying training compared to reinforcement learning methods.\n- A nested equilibrium sequence modeling procedure enables efficient closed-loop planning, leveraging feedback from the environment (or a world model) and accelerating plan refinement by reusing previously computed equilibrium solutions.\n- Evaluations on VirtualHome-Env benchmark demonstrate state-of-the-art performance in most metrics, especially when incorporating environmental feedback, and show advantageous scaling of performance with increased inference computation.\n- Ablation studies highlight the effectiveness of equilibrium sequence modeling, reuse of previous solutions, and dynamic computation allocation in improving plan quality and computational efficiency.",
        "classification": [
            "Robotics",
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/Singularity0104/equilibrium-planner"
        ],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration",
        "authors": "Xinjie Zhang, Jing Liu, Ruihao Gong, Zining Wang, Yushi Huang",
        "link": "https://arxiv.org/abs/2410.01723",
        "github_repo": null,
        "summary": "This paper introduces HarmoniCa, a novel framework to improve the training and inference processes of Diffusion Transformers by leveraging a feature cache. HarmoniCa features a Step-wise Denoising Training (SDT) to improve consistency between the training and inference processes. It also utilizes an Image Error Proxy-Guided Objective (IEPO) to balance image quality and cache utilization. Experimental results on various datasets and models show that HarmoniCa achieves a 1.5x speedup over PixArt and a 1.24 FID decrease for DiT-XL/2 256x256 with a higher speedup ratio, demonstrating HarmoniCa's superior speedup and quality. Finally, the approach boasts higher training efficiency with no image data and shorter training times compared to similar caching methods.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Selective Aggregation for Low-Rank Adaptation in Federated Learning",
        "authors": "Huijie Fan, Liangqiong-QU, yanranw1, stevezs, gpx333",
        "link": "https://arxiv.org/abs/2410.01463",
        "github_repo": null,
        "summary": " - This research paper introduces FedSA-LoRA, a new method for federated learning that selectively aggregates learned A and B matrices from LoRA.\n- It asserts that A matrices learn general knowledge while B matrices capture client-specific information, leading to only sharing A matrices for aggregation.\n- Experimental validation across language understanding and generation tasks on benchmarks like GLUE and GSM8K demonstrates FedSA-LoRA outperforms other methods. \n- The authors extend this approach to other LoRA variants (rsLoRA and VeRA), creating FedSA-rsLoRA and FedSA-VeRA, and show consistent improvements.\n- The findings provide insights into LoRA in federated settings and a general framework for using future LoRA adaptations.",
        "classification": [
            "Natural Language Processing",
            "Text Classification",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-03"
    },
    {
        "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
        "authors": "Chen Chen, Vasileios Saveris, haotiz, Hong-You, jefflai",
        "link": "https://arxiv.org/abs/2410.02740",
        "github_repo": null,
        "summary": "This paper investigates the role of large-scale image-caption data in pre-training multimodal foundation models, particularly focusing on the interplay between synthetic captions and original AltText.\n- It proposes a controllable and scalable captioning pipeline capable of generating diverse caption formats (short, descriptive, dense, AltText-fused).\n- Experiments across CLIP, multimodal LLMs, and diffusion models reveal that a hybrid approach, combining synthetic captions and AltText, often outperforms using synthetic captions alone. \n- Different model types exhibit preferences for specific caption formats: shorter captions for CLIP, descriptive for multimodal LLMs and diffusion models.\n- Combining AltText with synthetic captions enhances performance, likely due to improved image-text alignment from synthetic captions and increased data diversity from AltText.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Video Instruction Tuning With Synthetic Data",
        "authors": "Wei Li, Chunyuan24, liuziwei7, kimingng, ZhangYuanhan",
        "link": "https://arxiv.org/abs/2410.02713",
        "github_repo": null,
        "summary": " - This paper introduces LLaVA-Video, a large multimodal model for video understanding, and LLaVA-Video-178K, a synthetic dataset created for video instruction following.\n- LLaVA-Video-178K consists of 178,510 videos with 1.3 million instruction samples including detailed captions generated with a recurrent, multi-level approach, along with open-ended and multiple-choice question answering generated using GPT-4.\n- The model leverages a SlowFast video representation technique to optimize the balance between frame count and limited GPU memory, enabling processing of three times more frames than traditional methods.\n- LLaVA-Video achieves state-of-the-art results on various video benchmarks, outperforming existing open-source models and demonstrating the effectiveness of the proposed synthetic dataset and training approach.\n- The dataset, codebase, model checkpoints, and a visual chat demo are publicly released to foster development of general-purpose visual assistants.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/lmms-lab/VideoDetailCaption"
        ],
        "date": "2024-10-04"
    },
    {
        "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models",
        "authors": "Tianwei Xiong, XihuiLiu, bykang, Ikuinen, Epiphqny",
        "link": "https://arxiv.org/abs/2410.02757",
        "github_repo": null,
        "summary": " - Loong is a novel autoregressive LLM-based video generator that produces minute-level long videos. \n- It addresses the challenges of imbalanced loss and error accumulation during long video generation by using a progressive short-to-long training strategy with loss re-weighting and video token re-encoding. \n- Loong models text and video tokens as a unified sequence, trained from scratch on both image and video data, unlike prior approaches which use pretrained models. \n- The model utilizes a low-resolution video and later upscales the output to enhance visual quality. \n- User studies show that Loong outperforms StreamingT2V in terms of content consistency and visual text matching.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [
            "https://epiphqny.github.io/Loong-video"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
        "authors": "Chunyuan24, henghuang, thughost, russwang, txiong23",
        "link": "https://arxiv.org/abs/2410.02712",
        "github_repo": null,
        "summary": "**-** LLaVA-Critic is the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess the performance of other multimodal models across various tasks. \n**-** It leverages a new high-quality critic instruction-following dataset incorporating diverse evaluation criteria and scenarios, including pointwise scoring and pairwise ranking. \n**-** The model shows strong performance as an LMM-as-a-Judge, generating evaluation scores and rankings comparable to commercial GPT models. \n**-** In preference learning, LLaVA-Critic generates effective reward signals for iterative Direct Preference Optimization (DPO), surpassing rewards from human feedback as seen in LLaVA-RLHF. \n**-** LLaVA-Critic is open-sourced, including its data, code, checkpoints, and demo.",
        "classification": [
            "Multimodal",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Contrastive Localized Language-Image Pre-Training",
        "authors": "Marcin Eichner, Xinze Wang, haotiz, jefflai, Hong-You",
        "link": "https://arxiv.org/abs/2410.02746",
        "github_repo": null,
        "summary": "-\nCLOC is a new pre-training framework for vision encoders with enhanced localization capabilities.\n- It augments the CLIP loss with a region-text contrastive loss and a lightweight prompter module that extracts region embeddings from the image embedding given spatial hints.\n- A visually-enriched and spatially-localized captioning pipeline is designed to generate region-text pseudo-labels at scale, resulting in a two-billion image-text dataset with fine-grained region-text annotations.\n- CLOC consistently outperforms CLIP on 31 evaluation tasks, including standard image-text tasks, newly constructed region-text tasks, and downstream evaluations with MLLMs, particularly on referring and grounding tasks.\n- The enhanced localization capabilities of CLOC enable it to be a drop-in replacement of CLIP to enhance MLLMs.",
        "classification": [
            "Multimodal",
            "Image Classification",
            "Image Feature Extraction",
            "Visual Question Answering",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/zzliang/GRIT"
        ],
        "date": "2024-10-04"
    },
    {
        "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
        "authors": "Hugo Germain, Aleksei Bochkovskii, srrichter, msantoso98, amael-apple",
        "link": "https://arxiv.org/abs/2410.02073",
        "github_repo": "https://github.com/apple/ml-depth-pro",
        "summary": "\u2022 Depth Pro is a foundation model for zero-shot metric monocular depth estimation that uses a multi-scale Vision Transformer (ViT) architecture.\n\u2022 It synthesizes high-resolution (2.25-megapixel) depth maps in 0.3 seconds on a standard GPU, achieving both speed and accuracy, outperforming previous state-of-the-art methods in boundary tracing and metric depth accuracy (demonstrated by a higher \u03b4\u2081 score and faster inference time compared to baselines).\n\u2022 Depth Pro estimates metric depth with absolute scale without requiring camera intrinsics or metadata, and introduces new evaluation metrics leveraging matting datasets for boundary accuracy assessment.\n\u2022 A two-stage training curriculum combining real and synthetic datasets contributes to enhanced performance, and the inclusion of zero-shot focal length estimation further improves accuracy.\n\u2022 Depth Pro is designed for broader applicability and efficiency in tasks like novel view synthesis and is evaluated on diverse datasets not seen during training to demonstrate its generalization capabilities.",
        "classification": [
            "Depth Estimation",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/apple/ml-depth-pro"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Large Language Models as Markov Chains",
        "authors": "Abdelhakim Benechehab, Oussama Zekri, ievred, NBoulle, ambroiseodt",
        "link": "https://arxiv.org/abs/2410.02724",
        "github_repo": null,
        "summary": " - This paper draws an equivalence between large language models (LLMs) and Markov chains, offering a new theoretical framework to analyze LLM inference.\n - By representing LLMs with vocabulary size *T* and context window *K* as Markov chains on a state space of size O(*T*<sup>*K*</sup>), the authors derive findings on stationary distribution, convergence speed, and temperature influence.\n - The paper derives generalization bounds for pre-training and in-context learning under minimal assumptions, using concentration inequalities for dependent random variables and leveraging insights from the Markov chain equivalence.\n - The theoretical analysis predicts in-context scaling laws that are experimentally validated on recent LLMs (2023-2024), showing that LLMs outperform minimax optimal frequentist Markov chain learning.\n - Experimental results on various Markov chains and dynamical systems further support the theoretical findings and demonstrate the practical implications of the proposed framework.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
        "authors": "Yu Cheng, Jihai Zhang, Spico, Xiaoye08",
        "link": "https://arxiv.org/abs/2409.19291",
        "github_repo": null,
        "summary": "-\n\nThis paper introduces Diversified Multiplet Upcycling (DMU), a novel method for enhancing the Contrastive Language-Image Pre-training (CLIP) model by integrating it with a Mixture of Experts (MoE) architecture. DMU fine-tunes multiple CLIP models from a pre-trained checkpoint using Multistage Contrastive Learning (MCL) to capture diverse feature distributions. These fine-tuned models, sharing parameters except for the Feed-Forward Network, are then used to initialize a CLIP-MoE. The approach significantly improves CLIP's performance on various zero-shot tasks, including retrieval and image classification, as well as in downstream Multimodal Large Language Model (MLLM) benchmarks when serving as a vision encoder. Notably, CLIP-MoE surpasses the base OpenAI CLIP model by approximately 20% on retrieval tasks and exhibits minimal additional training overhead, using only 2% of the computational resources required to train a CLIP from scratch. This method provides a model-agnostic and computationally efficient way to scale CLIP and enhance its ability to capture rich, fine-grained information for improved performance in various multimodal applications.",
        "classification": [
            "Multimodal",
            "Image Feature Extraction",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [
            "https://github.com/OpenSparseLLMS/CLIP-MOE"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
        "authors": "Otmar Hilliges, RMW, msadat97",
        "link": "https://arxiv.org/abs/2410.02416",
        "github_repo": null,
        "summary": "\u2022 Proposes Adaptive Projected Guidance (APG), a method to mitigate oversaturation and artifacts in classifier-free guidance (CFG) at high guidance scales in diffusion models.\n\u2022 Decomposes the CFG update into parallel and orthogonal components, down-weighting the parallel component responsible for oversaturation, while preserving the orthogonal component that enhances image quality.\n\u2022 Introduces rescaling and reverse momentum inspired by gradient ascent to regulate update impact and refine sampling trajectories.\n\u2022 Demonstrates through experiments on various diffusion models that APG improves FID, recall, and saturation scores while maintaining precision comparable to CFG, even with higher guidance scales.\n\u2022 Shows APG compatibility with various conditional diffusion models, samplers, and distilled models, making it a superior plug-and-play alternative to CFG.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
        "authors": "Jun Zhu, Pengle Zhang, Jia wei, Jintao Zhang, surfingtomchen",
        "link": "https://arxiv.org/abs/2410.02367",
        "github_repo": null,
        "summary": "-\nSageAttention, a novel post-training quantization method designed to accelerate attention in Transformer models by quantizing tensors to 8-bit integers.\n- It overcomes the challenges of accuracy degradation in existing methods by smoothing the K matrix to mitigate outlier effects and employing a low-precision FP16 accumulator for the PV matrix multiplication.\n- It integrates effective kernel fusion with ROPE and an online softmax inspired by FlashAttention.\n- Comprehensive experiments demonstrate a 2.1x speed improvement over FlashAttention2 and 2.7x over xFormers on an RTX 4090.\n- It maintains comparable end-to-end metrics across diverse applications, including language, image, and video generation models.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Text2Text Generation",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/thu-ml/SageAttention"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
        "authors": "Xin Yu, Yida Wang, xiaobiaodu",
        "link": "https://arxiv.org/abs/2410.02103",
        "github_repo": null,
        "summary": " - MVGS, a new optimization method for 3D Gaussian Splatting (3DGS)-based novel view synthesis, addresses overfitting issues and enhances 3D geometry and appearance accuracy. \n- The method replaces single-view training with multi-view regulated learning, enabling joint optimization across multiple views and incorporating a cross-intrinsic guidance scheme for coarse-to-fine training.\n- A cross-ray densification strategy increases Gaussian kernels in crucial overlapped 3D regions and a multi-view augmented densification strategy intensifies this process when perspectives differ significantly.\n- MVGS improves novel view synthesis by approximately 1 dB PSNR across various Gaussian-based explicit representation methods and tasks, including general/reflective object and dynamic 4D scene reconstruction.\n- Experiments demonstrate consistent improvements in PSNR, SSIM, and LPIPS across diverse datasets, showcasing MVGS's effectiveness in challenging scenes with reflections, fine details, and lighting variations.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
        "authors": "Jianye Hou, Baibei Ji, Juntao Li, Keyan Zhou, ZetangForward",
        "link": "https://arxiv.org/abs/2410.02115",
        "github_repo": null,
        "summary": "\u2022 L-CiteEval, a new multi-task benchmark for evaluating long-context understanding with citations in large language models (LLMs) is introduced.\n\u2022 The benchmark comprises 11 diverse tasks with context lengths ranging from 8K to 48K tokens and employs automatic evaluation metrics for reproducibility.\n\u2022 Evaluation of 11 LLMs reveals that open-source models lag significantly behind closed-source counterparts in citation accuracy, suggesting reliance on inherent knowledge rather than provided context.\n\u2022 Retrieval-Augmented Generation (RAG) improves faithfulness in open-source LLMs but slightly diminishes generation quality.\n\u2022 A strong correlation is observed between LLMs' attention mechanisms and citation generation process, offering insight into LLM evaluation and development.",
        "classification": [
            "Question Answering",
            "Summarization",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/ZetangForward/L-CITEEVAL.git"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
        "authors": "Rob Fergus, lerrel, upiter",
        "link": "https://arxiv.org/abs/2410.02749",
        "github_repo": null,
        "summary": "-\nLintSeq, a synthetic data generation algorithm, refactors existing code into edit sequences to improve code synthesis in large language models (LLMs).\n- LLMs trained on this data produce more diverse programs, resulting in better inference-time scaling for benchmark pass rate.\n- Tiny (150M parameter) edit sequence LMs achieve state-of-the-art performance for their model class, matching or outperforming models twice their size.\n- Repeated sampling from smaller edit sequence finetuned LLMs achieves HumanEval coverage competitive with GPT-4 at similar cumulative inference cost to single samples from large open-source LLMs.\n- Ablating linter guidance from LintSeq degrades downstream performance.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/upiterbarg/lintseq"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
        "authors": "Michael Ryan, Ella Li, zyanzhe, missblanchett, WillHeld",
        "link": "https://arxiv.org/abs/2410.02678",
        "github_repo": null,
        "summary": "**Summary of \"Distilling an End-to-End Voice Assistant Without Instruction Training Data\"**\n\n- This paper introduces DiVA, a new speech large language model (LLM) trained through knowledge distillation from a text-based LLM, eliminating the need for explicit instruction-following data. DiVA utilizes a novel cross-modal context distillation method, which uses a frozen text-based LLM to guide the audio model's training by matching the output distribution from text transcripts of the audio. The audio input is processed using Whisper for feature extraction and a Q-Former initialized from Whisper's decoder to achieve audio-text feature alignment.\n- DiVA generalizes well to various spoken language tasks such as Spoken Question Answering, Classification (emotion, humor, and sarcasm detection), and Translation, using only ASR data for training.\n- In evaluation benchmarks, DiVA outperforms other open-access Speech and Audio LLMs on question answering by a significant margin despite using substantially less compute for training.\n- DiVA excels in following text-based instructions provided through prompts and user's speech, addressing the \"forgetting\" issue observed in other models trained using supervised fine-tuning. \n- In a user study, DiVA received a 72% preference rate compared to Qwen 2 Audio, demonstrating its effectiveness in real-world scenarios despite some limitations like inheriting the base LLM's bias.",
        "classification": [
            "Multimodal",
            "Audio",
            "Automatic Speech Recognition",
            "Question Answering",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation",
        "authors": "Amir Shmuel, Janine Mendola, amanchadha, gurucharan-marthi",
        "link": "https://arxiv.org/abs/2410.02458",
        "github_repo": null,
        "summary": "This study introduces MedVisionLlama, a novel approach for enhancing medical image segmentation by integrating pre-trained Large Language Model (LLM) transformer blocks into a Vision Transformer (ViT) architecture.\n- The architecture inserts a frozen LLM transformer block into the encoder of a ViT and uses residual connections between the LLM and ViT components, where the LLM block acts as a visual encoder. \n- It proposes a Hybrid Attention Mechanism that balances global and local feature learning and a Multi-Scale Fusion Block to aggregate features across different scales.\n- Experimental results across ten medical imaging modalities from the Medical Segmentation Decathlon (MSD) demonstrate significant performance gains, including an average Dice score increase from 0.74 to 0.79. \n- Ablation studies further validate the effectiveness of incorporating frozen LLM transformer blocks and the proposed hybrid attention mechanism.",
        "classification": [
            "Image Segmentation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
        "authors": "Jianrui Zhang, yjlee0222, mucai",
        "link": "https://arxiv.org/abs/2410.02763",
        "github_repo": null,
        "summary": "\n- This paper introduces Vinoground, a novel temporal counterfactual benchmark for evaluating Large Multimodal Models (LMMs) on dense temporal reasoning in short videos.\n- Vinoground contains 1000 short video and caption pairs with captions containing the same words but in different orders to create temporal counterfactuals.\n- The benchmark evaluates an LMM\u2019s ability to distinguish temporal differences between actions and object transformations (e.g., \"water turning into ice\u201d vs. \u201cice turning into water\u201d).\n- Experimental results show that even state-of-the-art LMMs struggle with temporal reasoning, with the best model (GPT-40) achieving only 54% accuracy on text score and much worse on other metrics, while human performance is around 90%.\n- All open-source models and CLIP-based models perform much worse, suggesting that existing methods struggle at fully understanding video temporality.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal"
        ],
        "github_urls": [
            "https://vinoground.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data",
        "authors": "manocha, ctnzr, rafaelvalle, ZhifengKong, SreyanG-NVIDIA",
        "link": "https://arxiv.org/abs/2410.02056",
        "github_repo": "https://github.com/Sreyan88/Synthio",
        "summary": "Synthio is a novel approach to augment small-scale audio classification datasets using synthetic data generated from text-to-audio (T2A) diffusion models, aligning the generated data with the target dataset's acoustic characteristics through preference optimization.\n- It addresses the challenge of creating diverse synthetic augmentations by introducing MixCap, a technique that leverages Large Language Models (LLMs) to generate and refine meaningful audio captions used for prompting the T2A model.\n- Synthio's evaluation across ten datasets and four limited-data settings demonstrates consistent outperformance of existing baselines, improving classification accuracy by 0.1% to 39% using a T2A model trained solely on weakly-captioned AudioSet.\n- Ablation studies show the vital role of preference optimization and MixCap in achieving optimal results.\n- Additional analysis demonstrates effectiveness of Synthio in enhancing captioning tasks and addressing long-tail categories.",
        "classification": [
            "Audio",
            "Audio Classification",
            "Text-to-Audio"
        ],
        "github_urls": [
            "https://github.com/Sreyan88/Synthio"
        ],
        "huggingface_urls": [],
        "date": "2024-10-04"
    },
    {
        "title": "Addition is All You Need for Energy-efficient Language Models",
        "authors": "Wei Sun, luohy",
        "link": "https://arxiv.org/abs/2410.00907",
        "github_repo": null,
        "summary": "-\nThe paper proposes a novel linear-complexity multiplication (L-Mul) algorithm to approximate floating-point multiplication with integer addition, aiming to reduce energy consumption in large language models (LLMs).\n-\nL-Mul replaces expensive floating-point multiplications with less energy-intensive integer additions and introduces an offset to maintain accuracy.\n-\nThe authors claim L-Mul achieves higher precision and requires less computation compared to 8-bit floating-point multiplications and 80% energy reduction for dot products.\n-\nExperiments on various LLMs and tasks (MMLU, BBH, GSM8k, visual question answering) showed that L-Mul in attention layers maintained or even slightly improved performance compared to standard multiplication and outperformed float8 with training free setting.\n-\nFine-tuning models with all multiplications replaced by 3-bit L-Mul achieved comparable results to models using float8_e4m3 accumulation, showcasing its potential for efficient LLM training and deployment.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "NL-Eye: Abductive NLI for Images",
        "authors": "Zorik Gekhman, yonatanbitton, nitay, tokeron, MorVentura",
        "link": "https://arxiv.org/abs/2410.02613",
        "github_repo": null,
        "summary": "\n- NL-EYE, a benchmark designed to evaluate the visual abductive reasoning skills of Visual Language Models (VLMs), is introduced.\n- NL-EYE tasks models with evaluating the plausibility of hypothesis images given a premise image, requiring explanations for their choices and consisting of 350 image triplets across six reasoning categories: physical, functional, logical, emotional, cultural, and social.\n- Results show that while humans perform well, VLMs struggle, often failing to surpass random baselines in plausibility prediction.\n- Even with correct predictions, VLM explanations are frequently unhelpful, indicating weaknesses in visual interpretation and accurate representation generation for reasoning.\n- Further analysis suggests that VLMs face challenges with temporal reasoning, absolute judgments, and non-correlational tasks, particularly emotional reasoning.",
        "classification": [
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Selective Attention Improves Transformer",
        "authors": "Yossi Matias, Matan Kalman, yanivle",
        "link": "https://arxiv.org/abs/2410.02703",
        "github_repo": null,
        "summary": "-\"Selective Attention\" is introduced; a parameter-free adjustment to the standard attention mechanism in Transformers, enabling a token to deem another as no longer relevant for future tokens and masking it, improving language modelling performance across various model sizes and context lengths.\n-It allows for reduction in the attention context buffer size without quality loss, resulting in significant memory and compute savings during inference, achieving up to 16X, 25X, and 47X memory reduction for context sizes of 512, 1024, and 2048 respectively with a 100M parameter model trained on C4.\n-Selective attention transformers often outperform standard transformers with ~2X more parameters and heads in their attention module.\n-Visualizations show selective attention exhibiting dynamic context pruning behavior; masking previous assignments to the same variable in variable assignment, masking ambiguous inputs until ambiguity resolution, and retaining only necessary elements in tasks like Parity and Copy.\n-Evaluation on C4 dataset shows consistent perplexity improvements across different model sizes and context lengths; further improvements via explicit loss to encourage masking, and HellaSwag benchmark reveals consistent accuracy gains across various model sizes using selective attention.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
        "authors": "Susanna Loeb, ddemszky, carlycodes, Analu, rose-e-wang",
        "link": "https://arxiv.org/abs/2410.03017",
        "github_repo": null,
        "summary": " - This paper introduces Tutor CoPilot, a Human-AI system designed to enhance real-time tutoring in K-12 education by providing expert-like guidance to tutors as they interact with students. \n- Tutor CoPilot leverages the Bridge method, which captures expert decision-making patterns and adapts Large Language Models (LLMs) to generate contextually relevant suggestions for tutors during live sessions. \n- A randomized controlled trial involving 900 tutors and 1,800 K-12 students demonstrates that Tutor CoPilot significantly improves student learning outcomes, particularly for students with lower-rated tutors. \n- Analysis of over 550,000 chat messages reveals that tutors using Tutor CoPilot are more likely to employ high-quality pedagogical strategies that foster student understanding and less likely to simply provide answers. \n- Tutor CoPilot offers a scalable and cost-effective solution (\n$20 per tutor annually) for enhancing tutoring quality, especially in under-served communities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models",
        "authors": "Jeonga Wi, Junyoung Choi, Jiun, DK9, longshiine",
        "link": "https://arxiv.org/abs/2409.19989",
        "github_repo": null,
        "summary": "RoCoTex is a novel diffusion-based method for high-quality, consistent texture synthesis on 3D meshes.\n- The method employs a symmetrical view synthesis strategy with regional prompts, which leverage a 2D diffusion prior (SDXL) along with multiple ControlNets, to enhance view consistency and texture alignment with the underlying 3D geometry.\n-  A confidence-based texture blending technique and a Differential Diffusion-based soft-inpainting method minimize seam artifacts and inconsistencies between different views.\n- Quantitative results demonstrate that RoCoTex achieves state-of-the-art performance in terms of image quality, diversity and user preference compared to existing methods.\n- User studies confirm that RoCoTex generates textures with superior quality, consistency, and alignment compared to baseline approaches such as TEXTure, Text2Tex and Paint3D.",
        "classification": [
            "Text-to-Image",
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Erasing Conceptual Knowledge from Language Models",
        "authors": "David Bau, Samuel Marks, sfeucht, RohitGandikota",
        "link": "https://arxiv.org/abs/2410.02760",
        "github_repo": null,
        "summary": "-\nThis research introduces Erasure of Language Memory (ELM), a novel method for removing specific concepts from large language models (LLMs) while preserving fluency and general knowledge.\n-\nELM employs a multi-objective fine-tuning approach with targeted low-rank updates (LoRA).\n-\nThe method optimizes for erasure of the target concept, retention of unrelated information, and generation fluency when prompted with the erased concept.\n-\nExperiments on biosecurity, cybersecurity, and literary domains demonstrate ELM\u2019s efficacy in achieving near-random performance on erased topics while maintaining high scores on general knowledge benchmarks and generating more fluent text than baseline methods.\n-\nELM also exhibits robustness against adversarial attacks, further highlighting its potential for safe and controlled LLM editing.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/rohitgandikota/erasing-llm"
        ],
        "huggingface_urls": [
            "https://huggingface.co/cais/Zephyr_RMU"
        ],
        "date": "2024-10-07"
    },
    {
        "title": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond",
        "authors": "gduggal, Man1kandan, Madddy, HARI45SH, shubhii0712",
        "link": "https://arxiv.org/abs/2410.02362",
        "github_repo": null,
        "summary": " - This survey paper explores Mamba architectures, a type of State Space Model (SSM), for medical image analysis.\n- Mamba offers linear time complexity, efficient processing of long sequences and strong performance in multimodal data merging, making it suitable for complex medical image analysis tasks.\n- The paper discusses core SSM concepts, various Mamba architecture designs (pure, U-Net variants, and hybrid models), optimization techniques, adaptations for different learning paradigms (weakly, semi-, and self-supervised learning), and diverse applications in medical image segmentation, classification, restoration, registration, and other miscellaneous tasks.\n-  Experimental results demonstrate that Mamba models outperform or are comparable to attention and transformer-based methods on benchmark medical datasets, like BraTS2023, ISIC2017, and ACDC.\n- The paper also discusses Mamba's limitations, including spatial information loss and parameter initialization challenges, along with emerging research areas like Mamba 2 and xLSTM.",
        "classification": [
            "Image Segmentation",
            "Image Classification",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
        "authors": "wpiioos, Unmanned-YuBeen, lastdefiance20, PurpleSand, MilkClouds",
        "link": "https://arxiv.org/abs/2410.01273",
        "github_repo": null,
        "summary": " - CANVAS, a novel framework for intuitive human-robot interaction, is introduced for commonsense-aware navigation. It combines visual and linguistic instructions to generate robot actions, leveraging pre-trained vision-language models (VLMs) to achieve this.\n- A new dataset called COMMAND, containing 48 hours of driving data over 219 kilometers with human-annotated instructions and navigation outcomes across office, street and orchard simulated environments, was collected to train and test the model.\n- Experimental results show that CANVAS consistently outperforms the rule-based ROS NavStack in all environments, especially in challenging scenarios like uneven terrain or misleading instructions, with higher success and lower collision rates.\n- CANVAS achieves successful Sim2Real transfer with a 69% success rate in a real-world office setting, demonstrating its robustness beyond simulated data.\n- Ablation study confirms that using pre-trained VLM weights improves performance considerably, indicating the usefulness of existing knowledge for navigation tasks.",
        "classification": [
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction",
        "authors": "Heming Weng, Genesis Wang, yh1567, zjy2001",
        "link": "https://arxiv.org/abs/2410.02241",
        "github_repo": null,
        "summary": "-\nMIGA, a Mixture of Experts with Group Aggregation framework, is proposed for stock market prediction.\n- MIGA employs a two-stage design: an expert router that encodes stock data and assigns weights to experts, and an expert group aggregation stage that facilitates information sharing among experts within groups.\n- MIGA outperforms existing end-to-end models on three Chinese Stock Index benchmarks (CSI300, CSI500, and CSI1000), with MIGA-Conv achieving a 24% excess annual return on CSI300, surpassing the previous state-of-the-art by 8%.\n- A comprehensive analysis reveals the specialization of MIGA's experts for different types of stocks and market conditions.\n- The paper explores the impact of expert aggregation size and inner group attention on model performance, demonstrating their effectiveness in enhancing prediction accuracy and stability.",
        "classification": [
            "Time Series Forecasting"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "NRGBoost: Energy-Based Generative Boosted Trees",
        "authors": "joaobravo",
        "link": "https://arxiv.org/abs/2410.03535",
        "github_repo": null,
        "summary": "-\nNRGBoost, a novel energy-based generative boosting model, is introduced. The model is trained to maximize a local second-order approximation to the likelihood at each boosting round, analogous to XGBoost.\n- An amortized sampling approach is proposed to reduce the training time, which is often dominated by sampling in energy-based models.\n- The algorithm achieves similar discriminative performance to XGBoost on several real-world tabular datasets while remaining competitive with other generative models for sampling.\n- It also outperforms alternative generative approaches on smaller datasets and produces visually similar samples to real data on MNIST and California Housing datasets.\n- The work also explores bagged ensembles of Density Estimation Trees (DET) with feature subsampling as a generative counterpart to Random Forests.",
        "classification": [
            "Tabular",
            "Tabular Classification",
            "Tabular Regression"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-07"
    },
    {
        "title": "Differential Transformer",
        "authors": "Li Dong, thegenerality, sunyt32, yuqxia, ytz20",
        "link": "https://arxiv.org/abs/2410.05258",
        "github_repo": null,
        "summary": "\u2022 This paper introduces the Differential Transformer (DIFF Transformer), a novel architecture for large language models (LLMs) designed to improve attention to relevant context and mitigate noise.\n\u2022 The core innovation is the differential attention mechanism, which calculates attention scores as the difference between two separate softmax attention maps, thus canceling noise and promoting sparse attention patterns.\n\u2022 Experimental results on language modeling demonstrate that DIFF Transformer outperforms standard Transformer models in various scaling settings, requiring only about 65% of the model size or training tokens to achieve comparable performance.\n\u2022 The model also exhibits advantages in downstream tasks such as long-context modeling, key information retrieval, hallucination mitigation, and in-context learning.\n\u2022 Additionally, DIFF Transformer demonstrates increased robustness to order permutation in in-context learning and a reduction in activation outliers, which presents opportunities for model quantization.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations",
        "authors": "Roi Reichart, Zorik Gekhman, belinkov, tokeron, hadasor",
        "link": "https://arxiv.org/abs/2410.02707",
        "github_repo": null,
        "summary": " \n- This paper investigates the internal representations of large language models (LLMs) and their connection to the phenomenon of hallucinations.\n- The research finds that truthfulness information is highly localized within exact answer tokens, leading to improved error detection when probing these specific tokens.\n- The study demonstrates that while error detection is enhanced by focusing on these tokens, probing classifiers trained on one dataset often fail to generalize effectively to others, indicating that truthfulness mechanisms are skill-specific.\n- The authors further categorize LLM errors based on repeated sampling, showing that error types are predictable from internal representations.\n- Finally, they highlight a discrepancy between LLM internal encoding and external behavior, revealing that models may internally identify the correct answer but consistently generate an incorrect one, suggesting the potential for harnessing this existing knowledge to reduce errors.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/technion-cs-nlp/LLMsKnow"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "FAN: Fourier Analysis Networks",
        "authors": "Yongding Tao, Ge Li, Jingjingxu, zkcpku, dongyh",
        "link": "https://arxiv.org/abs/2410.02675",
        "github_repo": null,
        "summary": "-\nThis paper introduces the Fourier Analysis Network (FAN), a novel neural network architecture designed to effectively model and reason about periodic phenomena by incorporating Fourier Series into its structure and computational process.\n-\nFAN aims to address the limitations of existing neural networks, such as MLPs and Transformers, which often struggle to generalize periodic functions beyond the training data domain.\n-\nThe architecture consists of stacking FAN layers where each layer outputs a concatenation of cosine, sine transformations, and an activation function applied to a linear transformation of the input.\n-\nExperimental results demonstrate FAN's superior performance compared to MLP, KAN, and Transformer on various tasks, including symbolic formula representation, time series forecasting, and language modeling tasks.\n-\nBy seamlessly replacing MLP layers with FAN layers, models achieve improved generalization while reducing parameters and FLOPs.",
        "classification": [
            "Time Series Forecasting",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/YihongDong/FAN"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
        "authors": "Jong Chul Ye, geonyoung-park, bryanswkim, DHCAI",
        "link": "https://arxiv.org/abs/2410.04364",
        "github_repo": null,
        "summary": " - VideoGuide is a novel framework that enhances the temporal consistency of pre-trained text-to-video diffusion models without requiring any additional training or fine-tuning.\n- It leverages any pre-trained video diffusion model or itself as a guide during the initial steps of inference, improving temporal quality by interpolating the guide model's denoised samples into the sampling model's denoising process.\n- VideoGuide significantly improves both subject and background consistency without sacrificing image quality or motion smoothness. \n- This method demonstrates prior distillation, where the base model's text coherence is enhanced by leveraging the superior data prior of the guiding model. \n- By applying VideoGuide, underperforming video diffusion models achieve state-of-the-art quality.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
        "authors": "Jonah Casebeer, Ge Zhu, Njb, tberg12, ZacharyNovack",
        "link": "https://arxiv.org/abs/2410.05167",
        "github_repo": null,
        "summary": "\n- Presto! is a new dual-faceted distillation approach for accelerating score-based diffusion transformers by reducing sampling steps and the cost per step.\n- Presto includes score-based distribution-matching distillation for continuous-time diffusion (EDM) using a GAN, improved conditional layer distillation with better-preserved hidden-state variance, and combined layer-step distillation.\n- For step distillation, Presto-S achieves best-in-class performance among step distillation techniques and matches the original model quality with 4-step inference.\n- When combined with the novel layer distillation Presto-L, which independently outperforms SOTA layer dropping and base diffusion sampling, the resulting Presto-LS approach accelerates the model by 10-18x, generating 32-second mono audio in 230ms and stereo audio in 435ms on an A100 40GB GPU, outperforming Stable Audio Open by 15x.",
        "classification": [
            "Audio",
            "Text-to-Audio"
        ],
        "github_urls": [
            "https://presto-music.github.io/web/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Named Clinical Entity Recognition Benchmark",
        "authors": "Cl\u00e9ment Christophe, Tathagata Raha, Muhammad Umar Salman, Marco AF Pimentel, Wadood M Abdul",
        "link": "https://arxiv.org/abs/2410.05046",
        "github_repo": null,
        "summary": "-\nThis paper introduces a Named Clinical Entity Recognition (NER) benchmark designed for evaluating language models in healthcare.\n- This benchmark encompasses a curated selection of publicly accessible medical datasets with standardized entities adhering to the Observational Medical Outcomes Partnership (OMOP) Common Data Model.\n- The leaderboard accommodates various language model architectures, including encoder, decoder, and GLiNER models, and employs standardized evaluation metrics, predominantly the F1-score, to ensure consistent performance comparisons.\n- Initial findings from the leaderboard indicate superior performance by GLiNER-based models over decoder-only architectures, commonly used in Large Language Models (LLMs).\n- The choice of evaluation strategy, token-based or span-based, has been found to influence model ranking.",
        "classification": [
            "Natural Language Processing",
            "Token Classification"
        ],
        "github_urls": [
            "https://github.com/WadoodAbdul/clinical_ner_benchmark"
        ],
        "huggingface_urls": [
            "https://huggingface.co/m42-health/clinical_ner_leaderboard",
            "https://huggingface.co/spaces/m42-health/clinical_ner_leaderboard"
        ],
        "date": "2024-10-08"
    },
    {
        "title": "OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction",
        "authors": "Xu Yan, Weichao Qiu, bingbl, Evenc, lilelife",
        "link": "https://arxiv.org/abs/2410.04932",
        "github_repo": null,
        "summary": "OmniBooth is a novel image generation framework that enables spatial control with instance-level multi-modal customization using text or image guidance.  It leverages latent control signals, a high-dimensional spatial feature, to seamlessly integrate spatial, textual, and image conditions.  The method expands the capabilities of text-to-image generation, providing enhanced performance in image synthesis fidelity and alignment.  Experimental results demonstrate improved performance compared to existing methods on instance segmentation tasks and image quality metrics.  OmniBooth is a unified framework for text and image conditioned generation offering improved flexibility and control compared to existing methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://len-li.github.io/omnibooth-web"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models",
        "authors": "Rui Wang, Tong Xiao, tbpangolin, pzzhang, deqing",
        "link": "https://arxiv.org/abs/2410.04734",
        "github_repo": null,
        "summary": " - This paper introduces TLDR, a novel token-level reward model designed to improve the performance and interpretability of large vision-language models (VLMs).\n- The TLDR model assigns rewards to individual tokens rather than entire sequences, enabling finer-grained feedback and more precise identification of errors, like hallucinations.\n- A perturbation-based method is used to generate synthetic hard negatives for training TLDR, enhancing its robustness.\n- Experiments demonstrate that TLDR significantly improves VLM performance in various tasks and reduces human annotation time by approximately threefold.\n- The study shows that the proposed model speeds up human annotation by 3 times in acquiring high-quality vision-language data.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "UniMuMo: Unified Text, Music and Motion Generation",
        "authors": "Yutong Zhang, Kun Su, Han Yang, auspicious3000, Jiaben",
        "link": "https://arxiv.org/abs/2410.04534",
        "github_repo": null,
        "summary": "  - UniMuMo is a unified multimodal model that uses a transformer-based encoder-decoder architecture to generate music, motion, and text from any combination of the three modalities as input.\n- The model bridges the modalities through a unified encoder-decoder architecture after converting inputs to a token-based representation and addresses the lack of time-synchronized data by aligning unpaired music and motion data based on rhythmic patterns and using existing large-scale datasets of single modalities. \n- It utilizes a music codebook to encode motion and introduces a music-motion parallel generation scheme.\n- This design unifies all music and motion generation tasks into a single transformer decoder architecture with one training task of music-motion joint generation and can be efficiently achieved by fine-tuning existing pre-trained single-modality models.\n- Extensive evaluations shows that UniMuMo achieves competitive results across all unidirectional generation benchmarks including text-to-music, music-to-motion, motion-to-music, music captioning and motion captioning.",
        "classification": [
            "Multimodal",
            "Text-to-Audio",
            "Text-to-Video",
            "Audio-to-Audio",
            "Audio-to-Audio",
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://hanyangclarence.github.io/unimumo_demo/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
        "authors": "Tong Che, Jingdi Lei, schrodingers-tiger, jwu323, qq8933",
        "link": "https://arxiv.org/abs/2410.02884",
        "github_repo": null,
        "summary": "LLaMA-Berry is a new framework for enhancing the mathematical reasoning ability of Large Language Models (LLMs) by combining Monte Carlo Tree Search (MCTS) with iterative Self-Refine and a pairwise reward model.\n- The framework uses Self-Refine applied to MCTS (SR-MCTS) to optimize the reasoning path by leveraging the self-critic and rewriting capabilities of LLMs.\n- A Pairwise Preference Reward Model (PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is used to evaluate different reasoning paths globally.\n- An Enhanced Borda Count (EBC) method synthesizes pairwise preferences between solutions into a global ranking score to identify better answers.\n- Experimental results on benchmarks like GSM8K, MATH, AIME24, AMC23, and GPQA Diamond demonstrate that LLaMA-Berry significantly improves the performance of LLaMA-3.1-8B, achieving results competitive with GPT-4 Turbo without additional training.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs",
        "authors": "cxiong, lunshi, hendrydong, yuhuixu, demolei",
        "link": "https://arxiv.org/abs/2410.04698",
        "github_repo": null,
        "summary": "**- MATHHAY: An automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs.**\n**- Unlike previous benchmarks, MATHHAY requires both information retrieval and complex mathematical reasoning, focusing on real-world scenarios within a specified time period.**\n**- Includes questions of varying difficulty levels across different input lengths (32K, 64K, 128K) and utilizes a combination of rule-based exact matching and LLM-based judgment for evaluation.**\n**- Experimental results reveal that even top-performing LLMs like Gemini struggle with long contexts in mathematical reasoning, indicating room for improvement.**\n**- Open-source models significantly underperform compared to closed-source models.**",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles",
        "authors": "siminniu, fan2goa1, WinfredShi, Ki-Seki, Duguce",
        "link": "https://arxiv.org/abs/2410.05262",
        "github_repo": null,
        "summary": " - TurtleBench is a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) using real user guesses from an online Turtle Soup Puzzle game.\n- This dynamic approach creates a bilingual dataset (Chinese and English) with 1532 annotated user guesses, which are then used to test the reasoning abilities of the LLMs. \n- The benchmark emphasizes reasoning ability and minimizes reliance on memorization and background knowledge. \n- Nine advanced LLMs, including open and closed-source models, were tested on TurtleBench. \n- The results show that Claude-3.5-Sonnet and GPT-4 performed best but that OpenAI's o1 series models performed sub-optimally.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/mazzzystar/TurtleBench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion",
        "authors": "fcole, trevordarrell, hurjunhwa, irwinherrmann, Junyi42",
        "link": "https://arxiv.org/abs/2410.03825",
        "github_repo": null,
        "summary": "-\nMotion DUSt3R (MonST3R) is a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes, effectively adapting DUSt3R's representation for dynamic scenes by estimating a pointmap for each timestep.\n- The key insight is that by estimating pointmaps per timestep and aligning them in the same camera coordinate frame it is possible to handle the dynamics of the scene without explicitly modelling motion.\n- This approach addresses the challenge of scarce training data by posing the problem as a fine-tuning task and strategically training the model on limited dynamic, posed videos with depth labels.\n- MonST3R achieves strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency, and demonstrates promising results for 4D reconstruction.\n- It improves performance on Sintel dataset when finetuned with PointOdyssey, TartanAir, Spring and Waymo datasets.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Video Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Autonomous Character-Scene Interaction Synthesis from Text Instruction",
        "authors": "thuhsy, YixinChen, awfuact, milleret, jnnan",
        "link": "https://arxiv.org/abs/2410.03187",
        "github_repo": null,
        "summary": "\u2022 This paper introduces a new framework for synthesizing multi-stage, scene-aware human motion in 3D environments from text instructions and goal locations. \n\u2022 It uses an auto-regressive diffusion model to generate realistic and coherent motion sequences, along with an autonomous scheduler for stage transitions. \n\u2022 A dual voxel scene encoder captures both current and imminent scene contexts for enhanced realism and collision avoidance. \n\u2022 The method integrates frame embeddings with language input for precise semantic guidance, and a stage-specific goal encoder conditions motion generation relative to current interaction goals. \n\u2022 Results show the model's ability to generate high-quality motions closely aligned with text instructions and scene constraints, showcasing an improvement over existing methods for locomotion, object reaching, and interaction motion synthesis.",
        "classification": [
            "Computer Vision",
            "Text-to-Video",
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "Grounding Language in Multi-Perspective Referential Communication",
        "authors": "alsuhr, mao1207, ZinengTang",
        "link": "https://arxiv.org/abs/2410.03959",
        "github_repo": null,
        "summary": "This paper introduces a new task and dataset for evaluating referring expression generation and comprehension in multi-agent embodied environments. The dataset, comprising 2,970 human-written referring expressions, requires agents to consider each other's perspective when generating and understanding references to objects.  The authors find that model performance lags behind that of human agents in both generation and comprehension tasks.  A speaker model fine-tuned using communicative success significantly improves performance, surpassing even a strong proprietary model (GPT-40). The contributions include a novel platform for generating 3D scenes, a new dataset, and analysis of language strategies in embodied referential communication.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/zinengtang/MulAgentRef"
        ],
        "huggingface_urls": [],
        "date": "2024-10-08"
    },
    {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "authors": "Peijie Dong, wenxinsiju, xuminghui, Dominic789654",
        "link": "https://arxiv.org/abs/2410.04199",
        "github_repo": null,
        "summary": "-\nLongGenBench, a synthetic benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs), focusing on consistency and logical flow.\n-\nIt redesigns question formats, requiring LLMs to provide single, cohesive long-context answers encompassing multiple questions within a single query.\n-\nEvaluation on LongGenBench reveals performance degradation across both API-accessed and open-source LLMs in long-context scenarios, ranging from 1.2% to 47.1%.\n-\nDifferent LLM series show varying degradation trends, with Gemini-1.5-FLASH exhibiting minimal degradation among API-accessed models, and QWEN2 series showing minimal degradation among open-source models.\n-\nModel size influences performance decline, with larger models within a series generally demonstrating less degradation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization",
        "authors": "Francois Charton, Justin Wang, shizhuo2",
        "link": "https://arxiv.org/abs/2410.04717",
        "github_repo": null,
        "summary": "-\nThis paper investigates the impact of instruction diversity on the generalization ability of Large Language Models (LLMs), focusing solely on instruction-following capabilities and isolating them from reasoning and knowledge retrieval.\n- Through controlled string rewriting experiments inspired by the Turing-complete Markov algorithm and mathematical deduction tasks, the study demonstrates that generalization to unseen instructions emerges only when training data is sufficiently diverse across semantic domains.\n- Findings reveal that diversifying data within limited domains does not guarantee robust generalization, while cross-domain diversification significantly enhances adaptability to new instructions.\n- The research further shows that increasing the diversity of training data can lead to performance improvements in real-world scenarios, including code generation and reasoning tasks with both specialized and generalist models. \n- The results underscore the importance of strategic data diversification over simply increasing data size, offering guidelines for improving instruction-tuning datasets and enhancing model performance across various domains.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
        "authors": "lifengshang, YuxinJiang, Tiezheng, yufeiwang201217a, DonJoey",
        "link": "https://arxiv.org/abs/2410.05193",
        "github_repo": null,
        "summary": "\u2022 REVISEVAL, a novel evaluation paradigm, leverages the revision capabilities of Large Language Models (LLMs) to generate response-adapted references for evaluating text generation quality. \n\u2022 It revises the generated response based on the given instruction and evaluation rubric, then uses the revised text as a reference for subsequent evaluation by either LLM-as-a-Judge or classic text evaluation metrics.\n\u2022 REVISEVAL outperforms reference-free and reference-based evaluation methods across various NLG and instruction-following tasks using both open-source and proprietary LLMs. \n\u2022 Response-adapted references enhance the performance of classic metrics, sometimes even rivaling LLM-as-a-Judge. \n\u2022 REVISEVAL effectively reduces bias in evaluation, such as verbosity and positional biases, and its effectiveness is linked to the relevance of the generated references.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
        "authors": "Sinan Tan, Jinze, JustinLin610, ZefanCai, leonardPKU",
        "link": "https://arxiv.org/abs/2410.01912",
        "github_repo": "https://github.com/chenllliang/DnD-Transformer",
        "summary": "-\nThe paper introduces the 2-Dimensional Autoregression (DnD) Transformer, a novel autoregressive model for image generation that addresses the information loss bottleneck of vector quantization (VQ) by predicting more codes for an image through a new autoregression direction (depth) alongside the traditional sequence length.\n- The DnD-Transformer inserts multiple prediction heads into the backbone transformer decoder to predict depth codes within a single forward pass, enhancing image quality without increasing model size or sequence length.\n- On ImageNet 256x256 generation, DnD-Transformer achieves up to 1.54 FID and 82.6 IS improvements without increased model size or sequence length, surpassing the larger LlamaGen model.\n- It demonstrates an emergent vision-language intelligence by generating images with rich text and graphical elements in a self-supervised manner, solely trained on images, even outperforming diffusion models on rich-text image datasets.\n- It opens up a new optimization perspective in autoregressive image generation by introducing a new autoregression direction that reduces information loss and efficiently reconstructs images.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/chenllliang/DnD-Transformer"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
        "authors": "Haocheng Shen, Peize Sun, Shoufa Chen, Tianheng Cheng, Zongming Li",
        "link": "https://arxiv.org/abs/2410.02705",
        "github_repo": "https://github.com/hustvl/ControlAR",
        "summary": " - ControlAR, a new framework, allows autoregressive models to perform controllable image generation from spatial control inputs like edges, depth maps, and segmentation masks.\n- ControlAR uses a lightweight control encoder, based on a Vision Transformer, to transform control images into sequential control tokens.\n- It employs a conditional decoding strategy, where the next image token is predicted based on both previous image tokens and the corresponding control token, demonstrating better performance and efficiency than pre-filling methods.\n- ControlAR extends autoregressive models to arbitrary-resolution image generation by conditioning on control token inputs of varying sizes.\n- Experiments show that ControlAR achieves highly competitive performance with state-of-the-art diffusion-based methods and strong control capability across various tasks, even surpassing ControlNet++ in some cases.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/hustvl/ControlAR"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
        "authors": "Yu Sun, Shuohuan Wang, Huang Fang, Haoran Sun, Yekun Chai",
        "link": "https://arxiv.org/abs/2410.02743",
        "github_repo": "https://github.com/ernie-research/MA-RLHF",
        "summary": "\n- MA-RLHF, a new Reinforcement Learning from Human Feedback (RLHF) framework, is introduced to improve large language model alignment with human preferences.  It leverages \"macro actions\" which are sequences of tokens or higher-level language constructs. \n- This approach reduces the temporal distance between actions and rewards, addressing the credit assignment problem in token-level RLHF, and facilitates faster and more accurate credit assignment. \n- The model achieves substantial performance improvements across various tasks, including up to a 30% gain in summarization, an 18% gain in dialogue, and an 8% gain in question answering, while demonstrating a 1.7x-2x faster convergence compared to standard RLHF. \n- MA-RLHF's robustness is highlighted through experiments conducted with different model sizes (2B to 27B) on various tasks, such as text summarization with the TL;DR dataset and dialogue generation with the HH-RLHF dataset. \n- Further analysis explores termination strategies for macro actions, demonstrating the effectiveness of n-gram and parsing-based approaches in improving model performance.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Summarization",
            "Text2Text Generation",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/ernie-research/MA-RLHF"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Dahoas/full-hh-rlhf"
        ],
        "date": "2024-10-09"
    },
    {
        "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
        "authors": "Yufan Zhou, Shizhe Diao, Yu Cheng, Zhiyang Xu, WHB139426",
        "link": "https://arxiv.org/abs/2410.03290",
        "github_repo": null,
        "summary": "**-** This paper introduces Grounded-VideoLLM, a novel Video Large Language Model (Video-LLM) designed for fine-grained temporal grounding in videos. \n**-** Grounded-VideoLLM uses a two-stream architecture, encoding spatial information from keyframes and temporal dynamics from multiple frames using a video encoder, to create a temporally-aware video representation.\n**-**  It introduces discrete temporal tokens into the LLM's vocabulary for representing timestamps efficiently, avoiding tokenization of numerical text and integrating time representations directly into the LLM.  \n**-** A multi-stage training approach is employed, progressing from video-caption alignment to temporal token alignment and finally multi-task instruction tuning on datasets incorporating temporal grounding tasks.\n**-** Experimental results demonstrate that Grounded-VideoLLM achieves state-of-the-art performance on various fine-grained temporal grounding tasks including Temporal Sentence Grounding, Dense Video Captioning and Grounded VideoQA, as well as general video understanding benchmarks.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/WHB139426/Grounded-Video-LLM"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
        "authors": "yuyijiong",
        "link": "https://arxiv.org/abs/2410.04422",
        "github_repo": null,
        "summary": " - This paper investigates the underlying reasons why Long Context Language Models (LCLMs) struggle with complex tasks, despite their ability to handle extensive text. \n- Through experiments with synthetic datasets, the study identifies \"multi-matching retrieval\" (retrieving multiple items simultaneously) and \"logic-based retrieval\" (using logic within retrieval criteria) as the core challenges, and further defines them as \"hyper-multi-step\" problems.\n- \"Hyper-multi-step\" implies that these seemingly simple tasks actually comprise a large number of indivisible sub-steps, which increases with context length and exceeds the processing capacity of current LCLMs. \n- The paper provides empirical evidence through linear probing of hidden states and analysis of attention weights, demonstrating that these problems are more akin to complex arithmetic tasks, rather than traditional retrieval, and are therefore not adequately addressed by existing techniques such as Retrieval-Augmented Generation (RAG) or Chain-of-Thought (CoT) prompting. \n- The study concludes that simply increasing the context window size of LCLMs may not suffice; instead, future research should focus on addressing the numerous steps involved and explore alternative solutions, such as using external tools.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic Environments",
        "authors": "Roi Reichart, Samuel Joseph Amouyal, Omer Madmon, ireinman, EilamSha",
        "link": "https://arxiv.org/abs/2410.05254",
        "github_repo": null,
        "summary": "  - This paper introduces GLEE, a unified framework and benchmark for evaluating Large Language Models (LLMs) in language-based economic games like bargaining, negotiation, and persuasion.\n  - It parameterizes the space of these games, defines consistent evaluation metrics (self-gain, efficiency, and fairness), and provides an open-source framework for interaction simulation.\n  - A dataset of 7.15M LLM decisions across various game configurations and an additional human vs. LLM dataset are collected using four different LLMs.\n  - The framework facilitates controlled experiments across numerous game configurations and LLMs, enabling robust evaluation.\n  - Demonstrates the framework's utility in evaluating and comparing LLMs to human players and in quantifying the impact of economic environment parameters.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/eilamshapira/GLEE"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Personalized Visual Instruction Tuning",
        "authors": "Jipeng Zhang, Tianyang Han, research4pan, Sterzhang, renjiepi",
        "link": "https://arxiv.org/abs/2410.07113",
        "github_repo": null,
        "summary": " PVIT (Personalized Visual Instruction Tuning) is a new training paradigm designed to enable Multimodal Large Language Models (MLLMs) to engage in personalized conversations by identifying target individuals within an image.\n- The framework leverages in-context learning, utilizing a multimodal prefix of <personal image, personal introduction> and personalized wrapper tokens to eliminate ambiguity.\n- PVIT involves an automatic framework to create training data in three stages: visual concept curation, dual-level textual information extraction and fusion, and dataset generation using LLM reasoning.\n- A benchmark named P-Bench, with various question types, is introduced to evaluate the personalized capabilities of MLLMs. \n- Experimental results on P-Bench demonstrate that current MLLMs have limited ability for personalized conversations. P-LLaVA trained with PVIT significantly improves performance on both answerable and unanswerable question types across all input complexities, achieving an average accuracy of 96.69% for answerable questions and 99.72% for unanswerable questions on the multiple choice questions in P-Bench.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/sterzhang/PVIT"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Sterzhang/PVIT-3M"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "authors": "kpzhang, hflqf88888, wqshao126, ljq940913, FanqingM",
        "link": "https://arxiv.org/abs/2410.05363",
        "github_repo": "https://github.com/OpenGVLab/PhyGenBench",
        "summary": "PhyGenBench, a novel benchmark designed to evaluate Text-to-Video (T2V) models' understanding of physical commonsense.\n- It comprises 160 prompts across 27 distinct physical laws spanning four fundamental physical domains: mechanics, optics, thermal, and material properties. \n- A novel Physics Generation Evaluation framework, PhyGenEval, is introduced that combines GPT-40 for physical commonsense understanding and a hierarchical three-tier evaluation structure. \n- This structure uses vision-language models for single image, multiple image, and full video evaluations. \n- Experimental results on various T2V models indicate a general deficiency in generating physically plausible videos, with the best model, Gen-3, achieving only a 0.51 score.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/OpenGVLab/PhyGenBench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
        "authors": "Pan Zhang, Xiaoyi Dong, lindahua, yuhangzang, shikiw",
        "link": "https://arxiv.org/abs/2410.07167",
        "github_repo": "https://github.com/shikiw/Modality-Integration-Rate",
        "summary": "-\nThis paper introduces the Modality Integration Rate (MIR), a new metric for evaluating the cross-modal alignment quality during the pre-training phase of Large Vision-Language Models (LVLMs).\n-\nMIR quantifies the domain divergence between vision and language features across all layers of the LLM, thus, correlates strongly with the model's post-SFT multi-modal performance and exhibits convergence behavior during pre-training, offering insights for training optimization.\n-\nFurthermore, it is robust to variations in input type and training/evaluation datasets, and generalizes across different pre-training recipes, strategies, and module designs.\n-\nA lightweight and learnable calibration module called MoCa is proposed, improving alignment between visual and textual tokens and leading to performance gains when integrated into both pre-training and SFT stages.\n-\nExperiments show that MoCa yields a 1.5% average performance increase for LLaVA-1.5 and a 0.9% increase for Mini-Gemini.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/shikiw/Modality-Integration-Rate"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
        "authors": "Ling Yang, Thu-redrobot, kelisiya, yaqicc, comin",
        "link": "https://arxiv.org/abs/2410.07171",
        "github_repo": "https://github.com/YangLing0818/IterComp",
        "summary": "\u2022 IterComp, a novel framework, aggregates composition-aware model preferences from multiple diffusion models and uses iterative feedback learning to enhance text-to-image generation.\n\u2022 It curates a gallery of six powerful open-source diffusion models and evaluates their performance on attribute binding, spatial, and non-spatial relationships to build a composition-aware model preference dataset.\n\u2022 The framework trains reward models for each compositional metric and uses them for iterative feedback learning, enabling progressive self-refinement of both the base diffusion model and reward models.\n\u2022 IterComp shows significant improvements over existing methods like Omost and FLUX, especially in complex object compositions and semantic alignments, according to experiments.\n\u2022 The method demonstrates superior performance in both compositional accuracy and image realism.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/YangLing0818/IterComp"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Pixtral 12B",
        "authors": "saurabhgarg, devendrachaplot, EmmaBH, Simontwice, pragra",
        "link": "https://arxiv.org/abs/2410.07073",
        "github_repo": null,
        "summary": "\n- Pixtral 12B is a 12-billion parameter multimodal language model trained to understand both images and text.\n- It utilizes a novel vision encoder trained from scratch, allowing it to process images at native resolution, and a multimodal decoder based on Mistral Nemo 12B.\n- Pixtral 12B outperforms open models of similar size on multimodal benchmarks, such as Llama 3.2 11B and Qwen-2-VL 7B and even surpasses larger models like Llama 3.2 90B on certain tasks. \n- It also achieves strong performance on text-only tasks, demonstrating its capability as a general purpose language model. \n- The authors introduce MM-MT-Bench, an open-source benchmark to evaluate vision-language models in practical multi-turn scenarios.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering"
        ],
        "github_urls": [
            "https://github.com/mistralai/mistral-inference",
            "https://github.com/mistralai/mistral-evals"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/mistralai/MM-MT-Bench"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
        "authors": "JunnanLi, guoyinwang, sirius-ctrl, teowu, dxli1",
        "link": "https://arxiv.org/abs/2410.05993",
        "github_repo": null,
        "summary": "**Summary of Aria: An Open Multimodal Native Mixture-of-Experts Model:**\n- ARIA is an open-source, multimodal native, mixture-of-experts (MoE) model with 24.9B parameters, trained from scratch and designed for comprehensive understanding across diverse modalities.\n- With a visual encoder lightweight of only 438M parameters, ARIA's MoE decoder has 3.9B and 3.5B activated parameters per visual and text token, respectively, enabling efficient parameter utilization and leading to faster training and inference. It outperforms Pixtral-12B and Llama3.2-11B and is competitive with top proprietary models on various multimodal tasks.\n- Trained in a 4-stage pipeline, the model progressively develops capabilities in language understanding, multimodal understanding, long context (64k tokens), and instruction following. This pipeline design ensures that each stage enhances the model's capabilities while preserving the already acquired skills from the previous stages.\n- ARIA's training data includes 6.4T language tokens and 400B multimodal tokens, with a rigorous curation process employing a combination of rule-based and model-based filtering to maintain data quality.\n- Qualitative results showcases ARIA is able to integrate information across multiple modalities in complex reasoning tasks involving chart, table, text, and images understanding and show advanced coding, debugging, math, paper reading, video understanding abilities.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text",
            "Text2Text Generation",
            "Video-Text-to-Text",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning",
        "authors": "szli-0000, sunbaigui, SOTA-Owner, ZCLiu35, ZedongWangAI",
        "link": "https://arxiv.org/abs/2410.06373",
        "github_repo": null,
        "summary": " - This paper introduces the concept of Backbone-Optimizer Coupling Bias (BOCB), where the performance of a vision backbone is shown to be significantly influenced by the choice of optimizer. \n- It presents a benchmark evaluating 20 backbones and 20 optimizers on CIFAR-100, ImageNet, and COCO, demonstrating that classical CNNs favor SGD-family optimizers, while modern architectures like ViTs and ConvNeXt perform better with adaptive learning rate optimizers. \n- The paper investigates the influence of backbone macro design and token mixers on BOCB, finding that increased complexity in modern architectures necessitates adaptive optimization strategies. \n- It further analyzes hyperparameter robustness and parameter patterns to understand the underlying mechanisms of BOCB. \n- This analysis leads to recommendations for optimizer selection and insights for designing robust vision backbones, including suggestions for pre-training and transfer learning.",
        "classification": [
            "Image Classification",
            "Object Detection",
            "Keypoint Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
        "authors": "quzhe, Payne53, Ninggggy, feifeiobama, rain1011",
        "link": "https://arxiv.org/abs/2410.05954",
        "github_repo": null,
        "summary": "-\nThis paper introduces pyramidal flow matching, a novel video generation framework leveraging spatial and temporal pyramid representations for enhanced efficiency.\n-\nThe model reinterprets the denoising trajectory as a series of pyramid stages, with only the final stage operating at full resolution, thus minimizing redundant computations.\n- It utilizes a unified flow matching objective within a single Diffusion Transformer (DiT) for joint optimization of pyramid stages and streamlines knowledge sharing and decompression. The proposed temporal pyramid employs compressed, lower-resolution history for conditioning, improving training efficiency.\n-\nThe model supports generating high-quality videos at resolutions up to 768p and 24fps, requiring fewer computational resources and training time compared to full-sequence diffusion models.\n-\nEvaluation on VBench and EvalCrafter benchmarks demonstrates highly competitive performance against other open-source models, and in some aspects, even surpasses commercial models, particularly in motion smoothness and dynamic degree metrics.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://pyramid-flow.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
        "authors": "HaoxuanYou, FrozzZen, edaxberger, haotiz, leoye",
        "link": "https://arxiv.org/abs/2410.07177",
        "github_repo": null,
        "summary": "**Key Points:**\n- Introduces MM-Ego, a multimodal large language model (MLLM) designed for egocentric video understanding, featuring a novel \"Memory Pointer Prompting\" mechanism. This mechanism incorporates a global glimpse step, which extracts compressed visual embeddings from the entire video to gain an overarching understanding, and a fallback step, which uses higher-resolution key visual embeddings identified in the global glimpse stage to respond to questions.\n- Creates a 7M egocentric QA dataset, generated automatically from human-annotated video narrations from the Ego4D dataset, that ranges from 30 seconds to one hour, representing the largest egocentric QA dataset currently available.\n- Introduces EgoMemoria, a benchmark to evaluate egocentric video understanding capabilities with 7,026 multiple-choice questions across 629 videos ranging from 30 seconds to one hour in length, alongside a debiased metric to mitigate language bias.\n- In experiments, MM-Ego outperforms prior state-of-the-art models on the EgoMemoria benchmark and demonstrates competitive results on general video benchmarks like EgoSchema and Video-MME.\n- The Memory Pointer Prompting and data augmentation strategies show improvements even after the removal of language-biased questions, demonstrating their efficacy for the targeted task.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
        "authors": "Marc Peter Deisenroth, Benedikt Alkin, thomasschmied, sirluk, paischer101",
        "link": "https://arxiv.org/abs/2410.07170",
        "github_repo": "https://github.com/ml-jku/EVA",
        "summary": "\n- This paper introduces Explained Variance Adaptation (EVA), a novel data-driven initialization method for Low-Rank Adaptation (LoRA) used in fine-tuning large foundation models.\n- EVA computes the Singular Value Decomposition (SVD) on mini-batches of activation vectors derived from downstream data to initialize LoRA weights, maximizing explained variance and enabling adaptive rank allocation across model layers.\n- Experiments conducted on diverse tasks, including language generation, understanding, image classification, and reinforcement learning, demonstrate EVA's superior performance to existing initialization and rank adaptation techniques.\n- EVA achieves faster convergence than competitor models across multiple tasks, such as achieving higher average scores on commonsense reasoning with LLMs and even exceeding full fine-tuning performance when combined with DORA on reinforcement learning tasks.\n- Ablation studies confirm that both the directional components and scale obtained from SVD contribute to EVA's enhanced performance.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Image Classification",
            "Reinforcement Learning",
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/ml-jku/EVA",
            "https://github.com/BenediktAlkin/vtab1k-pytorch",
            "https://github.com/sirluk/peft/blob/main/examples/eva_finetuning/eva_finetuning.py"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Story-Adapter: A Training-free Iterative Framework for Long Story Visualization",
        "authors": "Yunfei Xie, RitaCoding, MudeHui, xk-huang, JohnWeck",
        "link": "https://arxiv.org/abs/2410.06244",
        "github_repo": null,
        "summary": "Story-Adapter is a training-free and computationally efficient framework designed to enhance long story visualization (up to 100 frames), which leverages an iterative paradigm that refines each generated image using both the text prompt and all generated images from the previous iteration.\n- A novel Global Reference Cross-Attention (GRCA) module aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings.\n- This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, leading to more precise and fine-grained interactions.\n- Story-Adapter outperforms existing methods for visualizing both regular and long stories, showing a 9.4% improvement in aCCS and a 21.71 reduction in aFID compared to StoryGen and achieving a 3.4% improvement in aCCS and an 8.14 reduction in aFID compared to StoryDiffusion.\n- The iterative paradigm enhances both semantic consistency and the quality of fine-grained interactions across iterations, and GRCA sustains global story semantics for long story visualization.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
        "authors": "Zhifang Sui, Li Dong, thegenerality, THU-CHUNXIA, Rsy24",
        "link": "https://arxiv.org/abs/2410.06961",
        "github_repo": null,
        "summary": "SynPO, a novel self-boosting paradigm, leverages synthetic preference data for Large Language Model (LLM) alignment, eliminating the need for extensive human preference data. It employs an iterative mechanism where a self-prompt generator creates diverse prompts, and a response improver refines model responses.  After four SynPO iterations, LLMs like Llama2-8B and Mistral-7B demonstrated significant improvements, achieving over 22.1% win rate improvements on benchmarks like AlpacaEval 2.0 and ArenaHard. Moreover, SynPO boosts the general LLM performance, as evidenced by a 3.2 to 5.0 average score increase on the Open LLM leaderboard.  SynPO's self-boosting mechanism dynamically guides LLMs to refine their own outputs, effectively integrating generative rewards for preference learning.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model",
        "authors": "Ilyas Chahed, Dhia Eddine Rhaiem, ybelkada, yellowvm, JingweiZuo",
        "link": "https://arxiv.org/abs/2410.05355",
        "github_repo": null,
        "summary": "-\nFalcon Mamba 7B is a new large language model based on the Mamba architecture, making it attention-free, trained on 5.8 trillion tokens.\n-\nIt outperforms other open-source 7B models like Mistral 7B and Llama 3.1 8B, as well as larger models such as Falcon2 11B in benchmarks like the Open LLM Leaderboard.\n-\nFalcon Mamba 7B has faster inference speeds and lower memory usage, especially beneficial for long sequence generation due to the Mamba architecture's linear memory scaling.\n-\nThe model uses an AdamW optimizer with a warmup-stable-decay learning rate schedule and is trained on a dataset mixture of web data, curated content, code, and math data.\n-\nFalcon Mamba 7B is available with a permissive license on Hugging Face, supporting functionalities such as inference, quantization, and fine-tuning.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/tiiuae/falcon-mamba-7b",
            "https://huggingface.co/tiiuae/falcon-mamba-7b-pre-decay"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "Temporal Reasoning Transfer from Text to Video",
        "authors": "Chancy, PY007, yaolily, lyx97, tobiaslee",
        "link": "https://arxiv.org/abs/2410.06166",
        "github_repo": null,
        "summary": "-\nT3 (Textual Temporal reasoning Transfer) is introduced, a method that enhances Video Large Language Models' (Video LLMs) temporal reasoning by transferring knowledge from the text domain. \n- T3 creates diverse temporal reasoning tasks in text format from existing image-text datasets, addressing the lack of video samples with complex temporal scenarios. \n- Without using any video data, T3 improves LongVA-7B's performance significantly, achieving a 5.3 absolute accuracy gain on TempCompass, exceeding ShareGPT4Video-8B (trained on 28,000 video samples).\n- The enhanced LongVA-7B achieves competitive performance on video benchmarks, e.g. 49.7 accuracy on Video-MME's Temporal Reasoning task, outperforming InternVL-Chat-V1.5-20B and VILA1.5-40B. \n-  Analysis reveals a strong correlation between textual and video temporal task performance (e.g., Pearson r=0.89 on TempCompass), validating the efficacy of T3.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Video Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
        "authors": "Xiaoying Tang, Mingda Li, Jingyu Liu, qingbinliu, Yongxin-Guo",
        "link": "https://arxiv.org/abs/2410.05643",
        "github_repo": "https://github.com/gyxxyg/TRACE",
        "summary": "\n- TRACE, a novel task-interleaved video Large Language Model (LLM), is introduced for Video Temporal Grounding (VTG). It addresses the limitations of current video LLMs that rely solely on natural language generation, which lack the clear structure and information presented in videos. \n- TRACE models videos as sequences of events, each with timestamps, salient scores, and captions, and leverages causal event modeling framework to represent the inherent structure of videos.\n- The TRACE architecture uses an interleaved sequence of task tokens for visual frames, timestamps, salient scores, and text, and employs separate encoders and decoding heads for each task.\n- The model also incorporates an adaptive head-switching mechanism for improved generation and achieves superior performance on various VTG tasks and datasets, outperforming current video LLMs.\n- TRACE improves zero-shot performance by 3.1% and 4.9% on Youcook2 (CIDEr and F1 Score), by 6.5% and 3.7% on Charades-STA (Recall with IOU=0.5 and IOU=0.7 respectively), and by 10.3% and 9.2% on QVHighlights (mAP and HIT@1).",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/gyxxyg/TRACE"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Data Selection via Optimal Control for Language Models",
        "authors": "Li Dong, thegenerality, Rsy24, howang, t1101675",
        "link": "https://arxiv.org/abs/2410.07064",
        "github_repo": "https://github.com/microsoft/LMOps/tree/main/data_selection",
        "summary": "This paper introduces PMP-based Data Selection (PDS), a framework for selecting high-quality pre-training data for language models (LMs).\nPDS formulates data selection as an Optimal Control problem and leverages Pontryagin's Maximum Principle (PMP) to derive necessary conditions for optimal data selection.\nExperiments show that PDS accelerates LM pre-training by 2x and improves performance across various model sizes and downstream tasks, even extrapolating to 400B models trained on 15T tokens.\nPDS also enhances data utilization in data-constrained settings, reducing pre-training data demand by 1.8 times.\nThis method offers a principled, theory-driven approach to data selection compared to existing heuristics, leading to more efficient and effective LM training.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/microsoft/LMOps/tree/main/data_selection"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "CursorCore: Assist Programming through Aligning Anything",
        "authors": "Shijin Wang, Rui Li, Qi Liu, Eviloder, TechxGenus",
        "link": "https://arxiv.org/abs/2410.07002",
        "github_repo": "https://github.com/TechxGenus/CursorCore",
        "summary": "\n- This paper introduces CursorCore, a new framework for AI-assisted programming that integrates various information sources such as coding history, current code, and user instructions for enhanced automation. \n- It also presents a new benchmark called APEval (Assist Programming Eval) to evaluate models on this task and a data generation pipeline, Programming-Instruct, to create synthetic training data from diverse sources. \n- This pipeline generated 219K samples to fine-tune the CursorCore models. \n- The CursorCore models reportedly outperforms other models of comparable size on the APEval benchmark. \n- This framework unifies applications like inline chat and automated editing, contributing to the advancement of coding assistants.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/TechxGenus/CursorCore"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation",
        "authors": "Jong Chul Ye, gkwon",
        "link": "https://arxiv.org/abs/2410.05591",
        "github_repo": "https://github.com/KwonGihyun/TweedieMix",
        "summary": "-\n\nTweedieMix is a novel method that enhances the fusion of multiple personalized concepts in diffusion-based image and video generation models during inference.\n\n-\n\nThe method involves a two-stage sampling process: multi-object-aware sampling with a novel resampling strategy and concept fusion sampling using object-wise region guidance and Tweedie's formula for combining custom concept samples in the denoised image space.\n\n-\n\nThis approach allows for seamless integration of multiple, even semantically related, concepts without blending issues and can handle more than two concepts effectively.\n\n-\n\nExperimental results demonstrate higher fidelity in generating multiple personalized concepts compared to existing methods, achieving better CLIP scores and user preference ratings.\n\n-\n\nThe framework extends to image-to-video diffusion models, enabling multi-concept video generation through a training-free strategy involving feature injection from the first frame to subsequent frames, outperforming fine-tuning-based methods.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Image-to-Video"
        ],
        "github_urls": [
            "https://github.com/KwonGihyun/TweedieMix"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Response Tuning: Aligning Large Language Models without Instruction",
        "authors": "Hyounghun Kim, seokhyun",
        "link": "https://arxiv.org/abs/2410.02465",
        "github_repo": null,
        "summary": "\n- Response Tuning (RT) is proposed, a novel fine-tuning method that omits the instruction-conditioning step of instruction tuning, instead focusing exclusively on the supervision of response space.\n- RT models, trained solely on responses, exhibit helpfulness and open-ended instruction following capabilities comparable to instruction-tuned models, demonstrating the potential of response space supervision in alignment.\n- Refining the structural attributes of training responses leads to significant improvements in user preference for RT models, while incorporating contextual refusals into the training data allows RT models to implicitly evaluate and reject unsafe queries. \n- These findings emphasize the importance of controlling response distribution in safety alignment and suggest that large language models inherently acquire many capabilities during pre-training.\n- In-context learning with response demonstrations only yields effective instruction-following and refusal behaviors, further strengthening the argument for the power of response supervision and highlighting the inherent potential of pretrained large language models.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/seokhyunan/response-tuning"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
        "authors": "Haoran Zhang, zhangysk, CheeryLJH, EZ-hwh, Rosiness",
        "link": "https://arxiv.org/abs/2410.06555",
        "github_repo": "https://github.com/Thisisus7/ING-VP.git",
        "summary": "\n- This research introduces ING-VP, a novel interactive game-based vision planning benchmark designed to evaluate the spatial imagination and multi-step reasoning capabilities of Multimodal Large Language Models (MLLMs).\n- ING-VP comprises six distinct games with varying complexity, offering 300 levels and six unique configurations per level, leading to over 60,000 interaction rounds for a single model.\n- The benchmark incorporates image-text and text-only input modalities, single and multi-step reasoning settings, and conditions with and without interaction history, facilitating a comprehensive evaluation of MLLM performance.\n- Initial evaluations using ING-VP demonstrate that current state-of-the-art MLLMs struggle with these seemingly simple game tasks. The highest performing model, Claude-3.5 Sonnet, only achieves an average accuracy of 3.37%, significantly below human performance.\n- This underscores the need for further research and development to enhance MLLMs' capacity for complex spatial reasoning and planning, a crucial aspect of achieving robust artificial general intelligence.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/Thisisus7/ING-VP.git"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Mixed-Session Conversation with Egocentric Memory",
        "authors": "Taeyoung Kim, khh3323, jihyoung",
        "link": "https://arxiv.org/abs/2410.02503",
        "github_repo": null,
        "summary": "\u2022 The paper introduces Mixed-Session Conversation, a new dialogue paradigm where a main speaker interacts with different partners across multiple sessions, promoting deeper layered interactions and complex dynamics. \n\u2022 MISC, a new dataset comprising 8.5K episodes with 6 sessions and 4 speakers per episode is presented, implementing Mixed-Session Conversation and managing memories across sessions and partners from the main speaker's perspective.  \n\u2022 EMMA (Egocentric Memory Enhanced Mixed-session Conversation Agent), a novel dialogue model trained on MISC, facilitates seamless conversation continuity using Egocentric Memory, and allows retention of all conversational contexts across sessions and partners.  \n\u2022 Human evaluations validate that dialogues in MISC demonstrate seamless conversational flow even with changing partners, with EMMA exhibiting high humanness, engagingness, and memorability. \n\u2022 EMMA's use of Egocentric memory retains high memorability without contradiction by connecting instances within and across sessions and tagging memory to each utterance.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://mixed-session.github.io/"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context RL",
        "authors": "Markus Hofmarcher, razp, vihangp, paischer101, thomasschmied",
        "link": "https://arxiv.org/abs/2410.07071",
        "github_repo": null,
        "summary": "-\nRetrieval-Augmented Decision Transformer (RA-DT) is introduced, a novel in-context reinforcement learning (ICL) method that addresses the limitations of current approaches requiring entire episodes in the agent's context by incorporating an external memory with sub-trajectory retrieval.\n-\nRA-DT uses a pre-trained embedding model to encode sub-trajectories and maximum inner product search to retrieve relevant past experiences, which are then fused with the current context in the decision transformer (DT) via cross-attention.\n-\nThis retrieval mechanism does not require training and can be domain-agnostic.\n-\nRA-DT significantly outperforms existing ICL baselines on grid-world environments with sparse rewards while requiring only a fraction of their context length.\n-\nA domain-agnostic embedding model utilizing a FrozenHopfield mechanism and BERT shows comparable retrieval performance to a domain-specific DT, and RA-DT demonstrates consistent improvement on hold-out tasks in complex environments like robotics simulations and procedurally-generated video games, though without achieving general in-context improvement.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/ml-jku/RA-DT"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "F\u00fcrElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance",
        "authors": "C. Karen Liu, Elizabeth Schumann, Haochen Shi, Pei Xu, rcwang",
        "link": "https://arxiv.org/abs/2410.05791",
        "github_repo": null,
        "summary": "-\n  This paper introduces F\u00fcrElise, a large-scale dataset of 3D hand motions and audio from 15 pianists playing 153 classical music pieces, captured using a markerless multi-view video setup and refined with MIDI data from a Disklavier piano.  \n- A new model is proposed to synthesize physically plausible piano playing motions from sheet music, combining a diffusion model for initial motion generation, a music-based motion retrieval method for enhancing accuracy, and reinforcement learning for physics-based bimanual control. \n- The diffusion model, trained on F\u00fcrElise, generates kinematic hand trajectories conditioned on sheet music, providing high-level guidance and fingering information. \n- Motion retrieval augments the diffusion model's output by retrieving similar motions from F\u00fcrElise based on musical similarity, improving the precision of key presses. \n- The reinforcement learning policy learns to control simulated hands interacting with a piano keyboard, optimizing a combination of imitation and goal-based rewards to achieve realistic and musically accurate performance.",
        "classification": [
            "Computer Vision",
            "Reinforcement Learning",
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs",
        "authors": "Edward Suh, huansun, someshjha, peiranli0930, ShletonLiu-N",
        "link": "https://arxiv.org/abs/2410.05295",
        "github_repo": "https://github.com/SaFoLab-WISC/AutoDAN-Turbo",
        "summary": "-\nAutoDAN-Turbo, a novel black-box jailbreak method for Large Language Models (LLMs), automatically discovers and combines diverse jailbreak strategies using a lifelong learning approach.\n-\nThis method leverages three core modules: an Attack Generation and Exploration Module, a Strategy Library Construction Module, and a Jailbreak Strategy Retrieval Module, allowing for continuous strategy discovery, evolution, and integration of human-designed strategies.\n-\nEvaluation on Harmbench and StrongREJECT benchmarks shows that AutoDAN-Turbo significantly outperforms existing methods, achieving a 74.3% higher average attack success rate and a 92.3% higher StrongREJECT score than the runner-up.\n-\nNotably, it demonstrates exceptional effectiveness on GPT-4-1106-turbo, reaching an 88.5% attack success rate, which further increases to 93.4% with the integration of human-designed strategies.\n-\nThe learned strategy library exhibits strong transferability across different target models and datasets, demonstrating its robustness and adaptability in various attack scenarios.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SaFoLab-WISC/AutoDAN-Turbo"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Multimodal Situational Safety",
        "authors": "xw-eric, dawnsong, acompalas, Xuandong, LCZZZZ",
        "link": "https://arxiv.org/abs/2410.06172",
        "github_repo": null,
        "summary": "\n- This paper introduces the novel problem of Multimodal Situational Safety, which focuses on evaluating a multimodal model's ability to consider safety aspects based on visual context.\n- A new benchmark called MSSBench is created to evaluate the situational safety performance of current Multimodal Large Language Models (MLLMs).\n- The benchmark comprises 1820 language query-image pairs across two scenarios: chat and embodied assistants, where half the images depict safe situations and the other half unsafe.\n- An evaluation framework analyzes key safety aspects, including explicit safety reasoning, visual understanding, and situational safety reasoning.\n- Results show current MLLMs struggle with recognizing unsafe situations, especially open-source models which frequently ignore safety clues. ",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Visual Question Answering",
            "Image-Text-to-Text"
        ],
        "github_urls": [
            "mssbench.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design",
        "authors": "wangwilliamyang, wenhu, rpiramuthu, xfgao, jiachenli-ucsb",
        "link": "https://arxiv.org/abs/2410.05677",
        "github_repo": null,
        "summary": "\u2022 T2V-Turbo-v2, a novel text-to-video (T2V) generation model, enhances post-training through incorporating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance into the consistency distillation process. \n\u2022 It eliminates the target network from T2V-Turbo for improved memory efficiency and enables full model training, rather than just LORA.\n\u2022 It leverages motion guidance from training videos to formulate an energy function that augments the ODE solver, improving motion quality. \n\u2022 Evaluated on VBench, T2V-Turbo-v2 achieves state-of-the-art performance with a Total Score of 85.13, surpassing proprietary systems such as Gen-3 and Kling. \n\u2022 Ablation studies confirm the benefits of curating specialized datasets, utilizing diverse reward models and employing motion guidance.",
        "classification": [
            "Text-to-Video",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/spaces/Vchitect/VBench_Leaderboard"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler",
        "authors": "Jong Chul Ye, Taesung Kwon, sr2851766",
        "link": "https://arxiv.org/abs/2410.05651",
        "github_repo": null,
        "summary": " - This paper introduces ViBiDSampler, a novel bidirectional diffusion sampling method for video interpolation, which addresses off-manifold issues common in existing time-reversal fusion methods.\n- ViBiDSampler sequentially samples along forward and backward paths conditioned on start and end frames, improving coherence and on-manifold generation of intermediate frames.\n- The method incorporates CFG++ and DDS guidance techniques to enhance interpolation performance and ensure proper alignment with keyframes.\n- Experimental results on DAVIS and Pexels datasets demonstrate state-of-the-art performance in terms of fidelity and perceptual quality, outperforming baselines like FILM, TRF, and Generative Inbetweening.\n- ViBiDSampler efficiently generates high-resolution (1024x576) 25-frame videos in 195 seconds on a single 3090 GPU without fine-tuning or multiple re-noising steps.",
        "classification": [
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://vibid.github.io/"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "Collective Critics for Creative Story Generation",
        "authors": "Hyounghun Kim, minwook",
        "link": "https://arxiv.org/abs/2410.02428",
        "github_repo": null,
        "summary": "CRITICS is a novel framework for long-form story generation that integrates a collaborative critique mechanism to enhance story creativity and expressiveness.\n- It consists of two stages: CRPLAN for refining story plans and CRTEXT for enhancing story expressiveness.\n- Multiple LLM critics and a leader collaborate to refine story plans and enhance story texts based on criteria for creativity.\n- Human evaluation shows that CRITICS significantly improves story creativity and reader engagement while maintaining coherence.\n- It supports interactive writing, where humans can participate as any player within the framework.",
        "classification": [
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/EMNLP-2024-CritiCS/Collective-Critics-for-Creative-Story-Generation"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Diversity-Rewarded CFG Distillation",
        "authors": "alexrame, Sper42, bachem, ferretj, aagostinelli86",
        "link": "https://arxiv.org/abs/2410.06084",
        "github_repo": null,
        "summary": "\n- This paper introduces diversity-rewarded CFG distillation, a novel finetuning strategy to enhance the quality-diversity trade-off in generative models, specifically for text-to-music generation.\n- It combines distillation and reinforcement learning (RL) to optimize two complementary objectives; a novel CFG distillation objective and an RL with diversity reward objective.\n- By interpolating between the weights of two models(quality-focused and diversity-focused model), the strategy controls the quality-diversity trade-off at deployment time, further boosting performance.\n- Experiments on MusicLM using human evaluation validate that the model generates more diverse music samples while maintaining high quality.\n- The finetuned-then-merged model outperforms CFG augmentation in terms of Pareto-optimal quality and diversity, generating high-quality samples with improved diversity.",
        "classification": [
            "Text-to-Audio",
            "Reinforcement Learning",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control",
        "authors": "Dante De Nigris, SlavaElizarov, CiaraRowles, bostadynamics, esx2ve",
        "link": "https://arxiv.org/abs/2410.06985",
        "github_repo": null,
        "summary": " - This paper introduces a novel end-to-end pipeline for generating multi-view consistent Physically Based Rendering (PBR) textures from a 3D mesh and a text prompt. \n- The approach leverages a pre-trained text-to-image diffusion model within a Collaborative Control framework, extending it to multi-view by incorporating cross-attention to a reference view, its DINOv2 features, and pixel-wise correspondences between views with occlusion awareness.\n- The generated multi-view PBR images, including albedo, roughness, metallic, and normal bump maps, are consistent enough for naive fusion into a single texture map using a tri-planar representation and a small neural decoder.\n- The method bypasses the need for inverse rendering, directly modeling the PBR material distribution.\n- Qualitative comparisons suggest that the proposed method generates cleaner PBR textures than FlashTex, exhibiting better multi-view consistency, and produces high-resolution bump maps, although the overall realism is slightly lower than that of MetaTextureGen in some cases.",
        "classification": [
            "Text-to-Image",
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
        "authors": "ggcristian",
        "link": "https://arxiv.org/abs/2410.07062",
        "github_repo": "https://github.com/ggcr/TinyEmo",
        "summary": "\n- TinyEmo, a family of small Multimodal Large Language Models (MM-LLMs), is introduced for enhanced emotional reasoning and classification, integrating a synthetic emotional instruction dataset, a Metric Projector for classification, and a conditional reasoning approach.\n- The architecture includes a vision encoder (CLIP ViT-L/14), two projectors for classification and reasoning respectively and different LLM backbones (OpenELM, TinyLlama, Phi-2) ranging from 0.7B to 3.21B parameters. The Metric Projector is trained separately with metric learning, detaching classification from the LLM to improve efficiency and performance.\n- TinyEmo-700M outperforms larger state-of-the-art models like EmoVIT (7.91B parameters) with only 700M parameters on emotion classification and achieves a Zero-Shot accuracy of 57.62% when trained with data augmentation, outperforming EmoVIT's 55.57%.\n- A Conditional Reasoning approach, where the predicted emotion label from the Metric Projector is inserted into the prompt, leads to more accurate reasoning compared to the standard approach.\n- A semi-automated framework is proposed which uses the Metric Projector for interpretability and bias detection by analyzing neuron activations and embedding space robustness, showing the potential for mitigating bias and improving understanding of model behavior.",
        "classification": [
            "Multimodal",
            "Image Classification",
            "Visual Question Answering",
            "Text Generation",
            "Zero-Shot Classification",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [
            "https://github.com/ggcr/TinyEmo"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
        "authors": "Zhikang Niu, kaiyu-hf, ChunHuiWangFN, D-Keqi, SWivid",
        "link": "https://arxiv.org/abs/2410.06885",
        "github_repo": null,
        "summary": "\u2022 F5-TTS is a fully non-autoregressive text-to-speech model based on flow matching with Diffusion Transformer (DiT) and ConvNeXt V2.\n\u2022 It simplifies the pipeline by removing the need for a duration model, text encoder, phoneme alignment, and semantically infused codec, using padded character sequences as input.\n\u2022 The model employs a novel Sway Sampling strategy during inference, improving performance and allowing for faster inference with fewer function evaluations.\n\u2022 Evaluation on LibriSpeech-PC, Seed-TTS test-en, and test-zh demonstrates that F5-TTS achieves state-of-the-art zero-shot performance with a real-time factor (RTF) of 0.15, outperforming existing methods in terms of both speed and quality.\n\u2022 Ablation studies highlight the robustness of F5-TTS, especially in handling challenging scenarios where the alignment between text and speech is crucial.",
        "classification": [
            "Text-to-Speech",
            "Audio"
        ],
        "github_urls": [
            "https://github.com/SWivid/F5-TTS"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders",
        "authors": "Chi Han, Qingyun Wang, May Fung, jindongwang, Cheng228",
        "link": "https://arxiv.org/abs/2410.06845",
        "github_repo": "https://github.com/Scarelette/MentalArena/tree/main",
        "summary": "-\nMentalArena is a novel self-play training framework for LLMs to improve their ability to diagnose and treat mental health disorders by generating personalized training data.\n- It consists of three modules: Symptom Encoder simulates realistic mental health patients, Symptom Decoder mitigates intent bias in patient-therapist dialogues, and Model Optimizer fine-tunes the LLM on the generated data.\n- The Symptom Encoder uses cognitive models and behavior principles of patients to produce realistic symptom descriptions.\n- The framework significantly outperformed several state-of-the-art and mental-health-specific LLMs, including GPT-4, on six benchmark datasets, demonstrating improvement over base models by 20.7% for GPT-3.5-turbo and 6.6% for Llama-3-8b.\n- Further analysis revealed a strong correlation between model performance and perplexity of the training data, and that maintaining data diversity above a certain threshold during training contributes to improved model performance.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Scarelette/MentalArena/tree/main"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "TextToon: Real-Time Text Toonify Head Avatar from Single Video",
        "authors": "Chenliang Xu, Lele Chen, Luchuan Song, pliu23, goddice",
        "link": "https://arxiv.org/abs/2410.07160",
        "github_repo": null,
        "summary": "**TextToon** is a real-time text-driven toonification model for generating stylized head avatars from single-view videos.\n* The model uses a conditional Tri-plane Gaussian deformation field to learn facial expressions and deformations in a canonical space, allowing for text-driven stylization.\n* The framework is first pre-trained on photo-realistic appearances and then fine-tuned on stylized images generated by a text-to-image (T2I) model. \n* A \"lazy factor\" is introduced to enhance the handling of shoulder movements. \n* Results show qualitative and quantitative improvements over existing methods in terms of style preservation, identity retention, and real-time animation, achieving up to 48 FPS on a GPU and 15-18 FPS on a mobile device. \n* Evaluation shows that fine-tuning is completed within five minutes, enabling fast stylization and adaptation.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://songluchuan.github.io/TextToon/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning",
        "authors": "Dongwoo Kim, Sangdon Park, Minjong, hi-sammy",
        "link": "https://arxiv.org/abs/2410.05664",
        "github_repo": null,
        "summary": "-\nThis paper introduces Holistic Unlearning Benchmark (HUB), a comprehensive evaluation framework for assessing the effectiveness of unlearning methods in text-to-image diffusion models.\n- HUB evaluates unlearning methods across five key aspects: effectiveness on target concepts, faithfulness of images, compliance with prompts, robustness on side effects, and consistency in downstream applications.\n- The benchmark was used to evaluate six state-of-the-art unlearning methods including ESD, UCE, AC, SA, SalUn, and Receler.\n- Empirical results reveal that current unlearning methods exhibit limitations, especially in handling complex prompts and downstream tasks. \n- The authors release their evaluation code and datasets to facilitate further research in unlearning methods for text-to-image diffusion models.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
        "authors": "Jie Chen, Wojciech Matusik, Michael Sun, Gang Liu, mjiang89",
        "link": "https://arxiv.org/abs/2410.04223",
        "github_repo": null,
        "summary": "This research paper presents Llamole, a multimodal large language model (MLLM) for controllable and synthesizable molecular generation and retrosynthetic planning. Llamole integrates a base LLM with a graph diffusion transformer, graph neural networks, and A* search, allowing for the seamless generation of text, molecules, and reactions. Benchmarks on 14 LLMs of various sizes reveal the limitations of existing models in controllable molecular design and synthetic planning. Llamole shows significant improvement, increasing success rates from 5.5% to 35% and enhancing controllability by up to 80.9% across various metrics.",
        "classification": [
            "Multimodal",
            "Graph Machine Learning",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way",
        "authors": "Pan Zhang, Pengyang Ling, Jiazi Bu, lindahua, yuhangzang",
        "link": "https://arxiv.org/abs/2410.06241",
        "github_repo": null,
        "summary": "BroadWay is a training-free method to improve the quality of text-to-video generation without introducing additional parameters or increasing memory and sampling time. It has two main components: Temporal Self-Guidance and Fourier-based Motion Enhancement. Temporal Self-Guidance improves structural plausibility and temporal consistency by reducing disparity between temporal attention maps across decoder blocks. Fourier-based Motion Enhancement amplifies motion magnitude and richness by scaling the high-frequency components of temporal attention maps. Experimental results on AnimateDiff and VideoCrafter2 backbones show significant improvement in generated video quality without any training or fine-tuning.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders",
        "authors": "fgmckee, dnoever",
        "link": "https://arxiv.org/abs/2410.06462",
        "github_repo": null,
        "summary": " - This research paper explores the adversarial potential of Large Language Models (LLMs) to recommend malicious code within popular code repositories.\n- It demonstrates that while LLMs have guardrails against harmful outputs, these can be bypassed using context-shifting techniques.\n- Empirical examples are presented showing LLMs suggesting compromised APIs, RSS feeds, GitHub repositories, and NPM packages.\n- The attack surface is amplified by the use of trojan-hosting repositories and content delivery networks.\n- This work highlights the vulnerability of software supply chains to LLM-generated recommendations and calls for further research to improve context-aware safety measures.",
        "classification": [
            "Other"
        ],
        "github_urls": [
            "None"
        ],
        "huggingface_urls": [
            "None"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
        "authors": "Minlie Huang, Yuan Yuan, Yuxuan Chen, XUANMINGZHANG",
        "link": "https://arxiv.org/abs/2410.06949",
        "github_repo": null,
        "summary": "\n- Seeker, a multi-agent framework leverages LLMs to enhance exception handling in code by addressing three key issues: insensitive detection of fragile code, inaccurate capture of exception types, and distorted handling solutions.\n- Seeker employs five agents\u2014Scanner, Detector, Predator, Ranker, and Handler\u2014inspired by expert developer strategies.\n-  A Common Exception Enumeration (CEE) document, built from trusted external experience and exception practices, is used to improve retrieval and handling.\n- A deep retrieval-augmented generation (Deep-RAG) algorithm is proposed to handle complex inheritance relationships between exception types.\n- Experimental results show that Seeker outperforms baselines on various metrics including code quality, coverage, accuracy, and edit similarity.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/XMZhangAI/Seeker"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
        "authors": "Jordan Boyd-Graber, Hal Daum\u00e9 III, zhoutianyi, mgor",
        "link": "https://arxiv.org/abs/2410.06524",
        "github_repo": null,
        "summary": "\n- This paper introduces CAIMIRA, a novel framework based on Item Response Theory (IRT) for evaluating and comparing the question-answering abilities of humans and AI systems.\n- CAIMIRA uses question text to infer characteristics, enabling generalization to new questions without needing prior responses and allowing for analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions.\n- The study finds that humans outperform AI in knowledge-grounded abductive and conceptual reasoning, while LLMs like GPT-4-TURBO excel at targeted information retrieval and fact-based reasoning.\n- The authors suggest future QA tasks focus on challenging higher-order reasoning, scientific thinking, nuanced linguistic interpretation, and cross-contextual knowledge application.\n- The implementation can be found at https://github.com/maharshi95/neural-irt",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/maharshi95/neural-irt"
        ],
        "huggingface_urls": [
            "mgor/protobowl-11-13"
        ],
        "date": "2024-10-10"
    },
    {
        "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
        "authors": "lilianweng, tejalp, thesofakillers, evanmays, nch0w",
        "link": "https://arxiv.org/abs/2410.07095",
        "github_repo": "http://github.com/openai/mle-bench/",
        "summary": "\n- This paper introduces MLE-bench, a new benchmark for evaluating how well AI agents can perform at machine learning engineering. \n- The benchmark consists of 75 diverse Kaggle competitions to reflect real-world ML engineering skills such as training models, preparing datasets, and running experiments. \n- The authors establish human baselines for each competition using Kaggle\u2019s publicly available leaderboards and evaluate several frontier language models. \n- They found that the best-performing model, OpenAI\u2019s ol-preview with AIDE scaffolding, achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. \n- The study further explores resource scaling for AI agents and the impact of contamination from pre-training data.",
        "classification": [
            "Other"
        ],
        "github_urls": [
            "https://github.com/openai/mle-bench/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "Does Spatial Cognition Emerge in Frontier Models?",
        "authors": "vkoltun, philkra, erikwijmans, sramakrishnan",
        "link": "https://arxiv.org/abs/2410.06468",
        "github_repo": null,
        "summary": " - The paper introduces SPACE, a benchmark for evaluating spatial cognition in large language models (LLMs) and large multimodal models. \n - SPACE evaluates large-scale mapping abilities and smaller-scale reasoning about object shapes and layouts. \n - The benchmark includes tasks from cognitive science, instantiated in parallel via text and images. \n - Results indicate that current frontier models fall short of animal spatial intelligence, performing near chance level on several classic tests. \n - The authors suggest that spatial cognition is a crucial form of intelligence, and its emergence in models is worthy of further investigation.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Question Answering",
            "Zero-Shot Image Classification",
            "Zero-Shot Object Detection",
            "Computer Vision",
            "Image Classification",
            "Image Segmentation",
            "Video Classification",
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-10"
    },
    {
        "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
        "authors": "juntingpan, shiwk20, Houxing, scikkk, AJZhou",
        "link": "https://arxiv.org/abs/2410.08196",
        "github_repo": "https://github.com/mathllm/MathCoder2",
        "summary": "\n- MathCoder2, a new family of models, enhances mathematical reasoning in Large Language Models (LLMs) through continued pretraining on a 19.2B token dataset named MathCode-Pile, which pairs mathematical code with corresponding natural language reasoning steps.\n- The MathCode-Pile dataset was constructed by filtering and combining various math-related data sources, including web data, synthetic data, code using math packages, textbooks, and model-translated mathematical code.\n- A novel method was introduced to extract reasoning steps (conditions, LaTeX expressions, and results) from text using Llama 3.1-70B Instruct, subsequently translated into executable Python snippets.\n- MathCoder2-Llama-3-8B, a model from the MathCoder2 family, achieves 4-shot accuracies of 38.4% on MATH and 69.9% on GSM8K, improving upon the baseline by 3.1% and 4.1% respectively.\n- The complete data processing and training code, along with the dataset, is open-sourced for transparency and reproducibility.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/mathllm/MathCoder2"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs",
        "authors": "Yi Bin, Jiahao Wang, Yi Liu, wqshao126, ChenMnZ",
        "link": "https://arxiv.org/abs/2410.05265",
        "github_repo": "https://github.com/ChenMnZ/PrefixQuant",
        "summary": "\u2022 PrefixQuant is a novel quantization technique for Large Language Models (LLMs) that leverages the observation that outlier tokens often appear at predictable locations or have low semantic value. \n\u2022 The technique involves offline identification and prefixing of these outlier tokens in the key-value cache to prevent their generation during inference, enabling the use of per-tensor static quantization. \n\u2022 This method enables static quantization to outperform the more computationally expensive per-token dynamic quantization. \n\u2022 The authors demonstrate PrefixQuant's efficacy on Llama-2, Llama-3, and other LLMs, achieving perplexity improvements and accuracy gains over existing methods like QuaRot while also improving inference speed. \n\u2022 For example, in a W4A4KV4 quantized Llama-3-8B model, PrefixQuant attains a 7.43 WikiText2 perplexity and 71.08% average accuracy on five common sense reasoning tasks, surpassing QuaRot by 0.98 perplexity and 5.98 accuracy points.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/ChenMnZ/PrefixQuant"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents",
        "authors": "Zongqing Lu, Xinru Xu, tellarin, yuejunpengpku",
        "link": "https://arxiv.org/abs/2410.03450",
        "github_repo": null,
        "summary": "-\nMART (MLLM As ReTriever) is a new approach for multimodal retrieval in embodied agents, using interactive learning to fine-tune an MLLM retriever to assess trajectory effectiveness.\n- It leverages interaction data and preference learning to prioritize trajectories that are most beneficial for unseen tasks, addressing limitations of current retrieval methods that focus on surface-level similarities.\n- It introduces Trajectory Abstraction, a mechanism using MLLMs' summarization capabilities to condense trajectories while preserving key information, improving comprehension and efficiency in long-horizon tasks.\n- Experimental results across various environments show that MART significantly improves task success rates in unseen scenes compared to baselines, often exceeding 10% improvement.\n- MART offers a new paradigm for multimodal retrieval, adapting general-purpose MLLMs as retrievers for embodied agents to consider the task-specific relevance of retrieved information.",
        "classification": [
            "Multimodal",
            "Robotics",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models",
        "authors": "akashsri, FelixXu, quandao10, ligongh, AristHe",
        "link": "https://arxiv.org/abs/2410.08207",
        "github_repo": null,
        "summary": "-\nDICE (Discrete Inversion for Controllable Editing) is introduced as the first method to enable precise inversion for discrete diffusion models such as VQ-Diffusion, Paella, and masked generative models such as RoBERTa.\n- DICE enhances the editability of these models by recording noise sequences or masking patterns in the reverse sampling process, allowing for accurate reconstruction and controlled editing without reliance on predefined masks or attention manipulations.\n- The method's effectiveness has been demonstrated in image and text modalities.  For image editing, the experimental results on PIE-Bench using Paella show that the proposed method achieves lower structure distance while preserving background as well as competitive CLIP similarity compared to baselines including DDIM inversion with Stable Diffusion v1.4 and masked inpainting.\n- For text editing, using RoBERTa as the language model, DICE shows the ability to adjust a sentence\u2019s sentiment without altering its original structure, outperforming masked generation by a large margin based on structure preservation and sentiment correctness evaluation using ChatGPT-4.\n- A novel text-editing dataset, Sentiment Editing, focusing on controlled sentiment adjustments in sentences while preserving their structure and theme, is presented",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Benchmarking Agentic Workflow Generation",
        "authors": "Ningyu, xiaoyuehanbin, consultantQ, Runnaning, GoooDte",
        "link": "https://arxiv.org/abs/2410.07869",
        "github_repo": "https://github.com/zjunlp/WorFBench",
        "summary": "WORFBENCH, a unified workflow generation benchmark featuring diverse scenarios and complex graph workflow structures, is introduced to evaluate Large Language Model (LLM) agents' ability to decompose problems into executable workflows.\n- WORFEVAL, a systematic evaluation protocol employing subsequence and subgraph matching algorithms, is presented to rigorously assess workflow generation capabilities.\n- Evaluations across different LLMs reveal performance gaps between sequence and graph planning, with GPT-4 showing a 15% gap.\n- Two open-source models are trained and evaluated, demonstrating improved but limited generalization on held-out tasks.\n- Generated workflows enhance downstream tasks by serving as Chain-of-Thought augmentation and prior knowledge, enabling superior performance with reduced inference time through parallel and shortened planning steps.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/zjunlp/WorFBench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow",
        "authors": "Ling Yang, hsli-cuhk, Edify-Kd2024, DrinkingCoder, wangfuyun",
        "link": "https://arxiv.org/abs/2410.07303",
        "github_repo": "https://github.com/G-U-N/Rectified-Diffusion",
        "summary": " - This paper introduces Rectified Diffusion, a novel method for enhancing the training of diffusion models by focusing on achieving a first-order approximate ODE path during training, rather than prioritizing straight ODE paths as in previous rectified flow methods.\n - The key insight is that using pre-trained diffusion models to collect matched pairs of noise and samples, coupled with retraining, significantly improves model performance.\n- Rectified Diffusion generalizes the design space and application scope to broader diffusion model variants and prediction types.\n- Experimental results on Stable Diffusion v1-5 and XL demonstrate faster training and superior performance compared to rectified flow-based methods like InstaFlow, particularly in low-step generation scenarios. \n - It achieves one-step FID scores of 27.26 and 16.88 for SD and SDXL, respectively.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/G-U-N/Rectified-Diffusion"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human",
        "authors": "Shuyu Gan, Saaket Agashe, xw-eric, jc-y42, Jiuzhouh",
        "link": "https://arxiv.org/abs/2410.08164",
        "github_repo": "https://github.com/simular-ai/Agent-S",
        "summary": "\n- Agent S is introduced as an open agentic framework designed for autonomous interaction with computers through a GUI, aiming to automate complex multi-step tasks.\n- The framework utilizes experience-augmented hierarchical planning, learning from both external web knowledge searches and internal experience retrieval to plan and execute subtasks efficiently.\n- It employs an Agent-Computer Interface (ACI) that improves grounding by using vision-augmented accessibility tree observations and restricts the agent's action space to enhance safety and control.\n- Evaluation on the OSWorld benchmark demonstrates a significant performance improvement, achieving a 9.37% higher success rate than the baseline and establishing a new state-of-the-art, with consistent improvement across five categories of computer tasks.\n- Further evaluation on the WindowsAgentArena benchmark reveals the framework's broad generalizability to different operating systems with an improvement from 13.3% to 18.2% on an equivalent setup without explicit adaption.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/simular-ai/Agent-S"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Intriguing Properties of Large Language and Vision Models",
        "authors": "Ho-Jin Choi, yechan99, mkmiracle, kobiso, passing2961",
        "link": "https://arxiv.org/abs/2410.04751",
        "github_repo": "https://github.com/passing2961/IP-LLVM",
        "summary": "\u2022 This paper investigates the intriguing properties of Large Language and Vision Models (LLVMs), focusing on how they perceive and process images. \n\u2022 The study evaluates the performance of LLaVA-series models across 10 diverse benchmarks, including visual question answering, OCR and mathematical reasoning tasks, revealing that LLVMs process images globally despite using localized visual tokens.\n\u2022 The experiments show that LLVMs can solve math problems even with missing numerical details from the image, and the lower layers of the model are crucial for visual understanding while higher layers focus on text interpretation.\n\u2022 The research highlights LLVMs' struggle to preserve initial visual understanding capabilities after alignment and visual instruction tuning. \n\u2022 It suggests that future work should focus on developing interactive evaluation benchmarks and new model architectures to improve cross-modal alignment and visual perception.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/passing2961/IP-LLVM"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning",
        "authors": "Ye Tian, haitaominlp, Pluie1503, freesunshine0316, russwang",
        "link": "https://arxiv.org/abs/2410.06508",
        "github_repo": null,
        "summary": "-\nALPHALLM-CPL, a novel pairwise training framework, enhances the reasoning capabilities of Large Language Models (LLMs) through Monte Carlo Tree Search (MCTS) behavior distillation.\n- It leverages stepwise trajectory pairs from child nodes in the search tree, providing step-level information for effective distillation.\n- Curriculum preference learning dynamically adjusts the training sequence, prioritizing critical learning steps and mitigating overfitting.\n- Experiments on mathematical reasoning tasks (GSM8K and MATH) show substantial improvements over existing MCTS distillation methods.\n- ALPHALLM-CPL boosts LLaMA2-7B's accuracy on GSM8K by 150%, Mistral-7B by 48.8%, and LLaMA3-8B by 17.4% on MATH, demonstrating its effectiveness in LLM self-improvement.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
        "authors": "Junmo Kim, In So Kweon, Dong-Jin Kim, Jae Won Cho, ytaek-oh",
        "link": "https://arxiv.org/abs/2410.05210",
        "github_repo": "https://github.com/ytaek-oh/fsc-clip",
        "summary": "-\nFSC-CLIP, a novel fine-tuning framework for Vision-Language Models (VLMs), enhances compositional reasoning without sacrificing performance in zero-shot multi-modal tasks.\n-\nIt integrates Local Hard Negative (LHN) Loss, which uses dense alignments between image patches and text tokens to compute loss, and Selective Calibrated Regularization (SCR) to regulate hard negative supervision.\n-\nExtensive evaluations on 11 compositionality benchmarks and 21 zero-shot classification tasks show that FSC-CLIP achieves comparable compositionality to state-of-the-art methods while better preserving multi-modal capabilities and exceeding pre-trained CLIP's zero-shot classification score by +0.5 points when fine-tuned on 100k LAION-COCO samples, a substantial improvement compared to a drop of -4.9 observed in existing methods.\n-\nAdditionally, FSC-CLIP demonstrates superior retrieval capabilities, particularly in counterfactual scenarios, showcasing a more nuanced understanding of compositional concepts, as evidenced by qualitative examples on COCO-Counterfactuals.\n-\nFSC-CLIP addresses the trade-off between compositionality and multi-modal task performance, common in existing fine-tuning approaches that use global hard negative losses and often lead to degraded performance in tasks like zero-shot classification and retrieval.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Image-to-Text",
            "Zero-Shot Classification"
        ],
        "github_urls": [
            "https://github.com/ytaek-oh/fsc-clip"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
        "authors": "Sanqiang Zhao, Marzyeh Ghassemi, wzhouad, szhang42, YuxinXiao",
        "link": "https://arxiv.org/abs/2410.05248",
        "github_repo": null,
        "summary": "-\nSFTMix is a novel Mixup-based recipe for Large Language Model (LLM) instruction tuning that aims to improve performance without relying on curated datasets. \n- SFTMix leverages training dynamics to identify and split the training dataset into confident and unconfident subsets based on the model's perplexity.\n- A Mixup-based regularization is then applied, interpolating examples between these subsets to mitigate overfitting on confident examples and propagate supervision to unconfident ones.\n- SFTMix significantly outperforms next-token prediction (NTP) across various instruction-following tasks and healthcare-related benchmarks using different LLMs and dataset sizes. \n- Ablation studies confirm the method's robustness and design choices, demonstrating its potential across NLP applications.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Progressive Autoregressive Video Diffusion Models",
        "authors": "Hao Tan, Zhan Xu, smebliu, YicongHong, desaix",
        "link": "https://arxiv.org/abs/2410.08151",
        "github_repo": null,
        "summary": "-\nProgressive Autoregressive Video Diffusion Models (PA-VDM) extends video diffusion models to generate long, high-quality videos by assigning progressively increasing noise levels to latent frames during denoising.\n-\nThis approach enables fine-grained conditioning among latent frames and large overlaps between attention windows, facilitating smoother temporal transitions and motion consistency.\n-\nPA-VDM can be implemented by adjusting noise scheduling and fine-tuning existing video diffusion model architectures, such as UNet and Diffusion Transformer-based models, without any major changes. \n-\nExperimental results on long video generation (60 seconds) demonstrate PA-VDM outperforms baselines on several metrics, including dynamic degree, aesthetic quality, and imaging quality, while maintaining competitive results for subject/background consistency and motion smoothness.\n-\nQualitative analysis showcases PA-VDM's ability to preserve frame fidelity and motion realism over extended durations. ",
        "classification": [
            "Text-to-Video",
            "Video Classification",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/desaixie/pa_vdm"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
        "authors": "aquila147, mdorkenw, paulgavrikov, sivand, kevinmzy",
        "link": "https://arxiv.org/abs/2410.06154",
        "github_repo": "https://github.com/jmiemirza/GLOV",
        "summary": "\u2022 GLOV is a novel method that utilizes LLMs as implicit optimizers for Vision-Language Models (VLMs), enhancing performance on downstream tasks by optimizing natural language prompts. \n\u2022 It uses a meta-prompt to guide iterative prompt generation, incorporating ranked in-context examples based on a few-shot training set and explicit guidance in the embedding space using offset vectors. \n\u2022 This guidance steers the LLM towards positive solutions, improving recognition performance by up to 15% and 57.5% (3.8% and 21.6% average) on dual-encoder and encoder-decoder VLMs. \n\u2022  Comprehensive evaluation on 16 diverse datasets using CLIP and LLaVa demonstrates GLOV's ability to consistently improve performance.\n\u2022 The method was shown to be effective even for challenging fine-grained recognition tasks using encoder-decoder models without requiring gradient-based learning.",
        "classification": [
            "Zero-Shot Image Classification",
            "Image Classification",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/jmiemirza/GLOV"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System",
        "authors": "Cheng Yang, Chen Qian, Jiarui Yuan, zibuyu9, weizechen",
        "link": "https://arxiv.org/abs/2410.08115",
        "github_repo": null,
        "summary": "\n- OPTIMA, a novel framework designed to optimize Large Language Model (LLM)-based Multi-Agent Systems (MAS) by enhancing both communication efficiency and task effectiveness through LLM training. \n- Employs an iterative \"generate, rank, select, and train\" paradigm and utilizes a reward function that balances task performance, token efficiency, and communication interpretability. \n- Integrates Monte Carlo Tree Search (MCTS)-inspired techniques for DPO data generation, to explore diverse interaction paths during conversations. \n- Evaluated on various multi-agent tasks, including information-asymmetric question answering and complex reasoning, OPTIMA consistently outperforms single-agent and vanilla LLM-based MAS baselines, showing significant improvements in token usage and task performance (up to 2.8x performance gain with <10% tokens). \n- The efficiency gains also contribute to improved inference-time scaling laws, enhancing the overall capabilities of LLM systems.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Emergent properties with repeated examples",
        "authors": "Fran\u00e7ois Charton, Knykny",
        "link": "https://arxiv.org/abs/2410.07041",
        "github_repo": null,
        "summary": "This study explores the impact of training example repetition on transformer performance using generated datasets for three mathematical tasks: greatest common divisor (GCD), modular multiplication, and matrix eigenvalues.\n- For a fixed number of training steps, models trained on smaller datasets with repeated examples outperform models trained on larger datasets with single-use examples.\n- This \"repetition helps\" phenomenon sometimes leads to the emergence of properties learned only by models trained on smaller, repeated datasets. \n- A \"two-set training\" approach, where a small random subset of examples is repeated more often alongside normal sampling on the rest of the training set, further improves learning speed and performance.  \n- The findings suggest that repetition's benefits can outweigh those of data diversity, challenging the common practice of minimizing example reuse.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations",
        "authors": "xyyue, DingXiaoH, Yiyuan",
        "link": "https://arxiv.org/abs/2410.08049",
        "github_repo": "https://github.com/AILab-CVC/UniRepLKNet",
        "summary": " \n- UniRepLKNet, a large-kernel Convolutional Neural Network (ConvNet) architecture, is proposed, challenging the dominance of Vision Transformers (ViTs) in multimodal tasks by demonstrating comparable performance with faster inference and reduced complexity.\n- The architecture employs a few strategically placed large kernels to efficiently capture global context, supplemented by small kernels for detailed spatial feature extraction, achieving a balance between receptive field coverage and computational efficiency.\n- Design principles for large-kernel ConvNets are introduced, including guidelines for kernel size selection based on task and layer depth, efficient implementation of large kernels using depth-wise convolutions, the vital role of identity shortcuts, and the use of dilated small kernels for re-parameterizing large kernels.\n- Experiments across diverse modalities like images, audio, video, point clouds, and time series demonstrate UniRepLKNet's superior performance. It achieves state-of-the-art results on ImageNet classification, ADE20K semantic segmentation, and a global weather forecasting task, surpassing both existing large-kernel ConvNets and recent transformer-based models.\n- When scaled to 1.4B parameters and pretrained on a massive dataset of 10B image-text pairs, UniRepLKNet exhibits exceptional zero-shot image recognition capabilities and competitive performance on large vision-language model benchmarks, showcasing its scalability and potential for broader applications in multimodal learning.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Object Detection",
            "Image Segmentation",
            "Zero-Shot Image Classification",
            "Audio Classification",
            "Time Series Forecasting",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/AILab-CVC/UniRepLKNet"
        ],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting",
        "authors": "ztz1989, jiahao97, Free1unch, Rosetta-Leong, RuijieZhu",
        "link": "https://arxiv.org/abs/2410.07707",
        "github_repo": null,
        "summary": " - MotionGS, a novel deformable 3D Gaussian Splatting framework, is proposed for dynamic scene reconstruction by explicitly modeling and constraining object motion.\n- The framework incorporates an optical flow decoupling module, which separates motion flow from optical flow priors to provide explicit supervision for 3D Gaussian deformation.\n- A camera pose refinement module alternately optimizes 3DGS and camera poses to enhance rendering quality and robustness.\n- Experimental results on NeRF-DS and HyperNeRF datasets demonstrate state-of-the-art performance, showcasing improvements in handling complex dynamic scenes with rapid movements and inaccurate camera poses. \n- The approach is agnostic to specific network designs and applicable to similar deformation-based 3DGS methods.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-11"
    },
    {
        "title": "Baichuan-Omni Technical Report",
        "authors": "kenshinn, dbv, dongguosheng, TJU-Tianpengli, lin5547",
        "link": "https://arxiv.org/abs/2410.08565",
        "github_repo": null,
        "summary": " - Baichuan-Omni is a 7B Multimodal Large Language Model (MLLM) capable of processing image, video, audio, and text modalities concurrently.\n- The model architecture involves separate encoders for each modality, projectors to map these encodings into the language model's embedding space, and a shared decoder.\n- The training process consists of two phases: multimodal alignment pre-training and multitask fine-tuning, using a diverse dataset of open-source, synthetic, and internally annotated data.\n- Evaluation across various benchmarks demonstrates that Baichuan-Omni outperforms existing open-source omni-modal models like VITA and achieves competitive results compared to closed-source models like GPT-4, particularly excelling in Chinese benchmarks and audio tasks.\n- Real-time interaction is facilitated by predicting audio input boundaries while concurrently processing visual data, enhancing dynamic attention calculation and streaming capabilities.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Video-Text-to-Text",
            "Any-to-Any",
            "Audio",
            "Automatic Speech Recognition",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/westlake-baichuan-mllm/bc-omni"
        ],
        "huggingface_urls": [],
        "date": "2024-10-14"
    },
    {
        "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
        "authors": "Runjia Li, Bohan Zeng, Junlin Han, Zixiang Zhang, Ling Yang",
        "link": "https://arxiv.org/abs/2410.09009",
        "github_repo": "https://github.com/YangLing0818/SemanticSDS-3D",
        "summary": "-\nSEMANTICSDS, a novel semantic-guided score distillation sampling approach, is introduced to enhance the expressiveness and precision of compositional text-to-3D generation.\n-\nThe approach integrates program-aided layout planning, derived from large language models (LLMs), to ensure accurate object placement and scene composition from textual descriptions.\n-\nNovel semantic embeddings maintain view consistency and distinguish different objects and parts within the 3D scene, rendered into a semantic map that guides a region-wise score distillation sampling (SDS) process for fine-grained control.\n-\nUsing 3D Gaussian Splatting (3DGS) and leveraging pre-trained 2D diffusion priors, SEMANTICSDS achieves superior quality in generating complex 3D objects and scenes with multiple attributes, as demonstrated by qualitative comparisons and a user study.\n-\nQuantitative results show improvements across prompt alignment, spatial arrangement, geometric fidelity, and overall scene quality, outperforming state-of-the-art baselines.",
        "classification": [
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/YangLing0818/SemanticSDS-3D"
        ],
        "huggingface_urls": [],
        "date": "2024-10-14"
    },
    {
        "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
        "authors": "Joseph E. Gonzalez, Minkai Xu, Tianjun Zhang, Zhaochen Yu, Ling Yang",
        "link": "https://arxiv.org/abs/2410.09008",
        "github_repo": "https://github.com/YangLing0818/SuperCorrect-llm",
        "summary": "SuperCorrect is a novel two-stage framework that leverages a large teacher model to supervise and correct the reasoning and reflection processes of a smaller student model, thereby improving mathematical reasoning and self-correction abilities.  The first stage uses hierarchical thought templates extracted from the teacher model to guide the student in generating more fine-grained reasoning thoughts.  The second stage employs cross-model collaborative direct preference optimization (DPO) to refine the student model's self-correction capabilities by following the teacher's correction traces during training.  Experimental results show that SuperCorrect achieves state-of-the-art performance among 7B models on MATH and GSM8K benchmarks, outperforming DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3%, respectively and surpasses models that are larger, such as Llama3-70B.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/YangLing0818/SuperCorrect-llm"
        ],
        "huggingface_urls": [],
        "date": "2024-10-14"
    },
    {
        "title": "Mechanistic Permutability: Match Features Across Layers",
        "authors": "Ian Maksimov, kefirski, elephantmipt",
        "link": "https://arxiv.org/abs/2410.07656",
        "github_repo": null,
        "summary": " - This paper introduces SAE Match, a data-free method for aligning Sparse Autoencoder (SAE) features across different layers of a neural network, addressing the challenge of understanding feature evolution and polysematicity in large language models (LLMs).\n- The method involves matching features by minimizing the mean squared error (MSE) between the \"folded\" parameters of SAEs, a technique that integrates activation thresholds into encoder and decoder weights to account for differences in feature scales.\n- Experiments on the Gemma 2 language model demonstrate improved feature matching quality compared to methods without parameter folding and provide insights into feature persistence and transformation across layers.\n- The approach also shows potential for approximating hidden states across layers, effectively skipping intermediate layers with minimal performance loss, especially in later layers where features are more monosemantic. \n-  Evaluation using external LLMs and matching scores shows that feature similarity gradually declines over several layers but remains significant for approximately five layers, while initial layers appear to exhibit higher polysematicity, making feature matching more challenging in these layers.",
        "classification": [
            "Natural Language Processing",
            "Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/loubnabnl/github-small-near-dedup"
        ],
        "date": "2024-10-14"
    },
    {
        "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
        "authors": "SKyii, monocrat23, nokomon",
        "link": "https://arxiv.org/abs/2410.09037",
        "github_repo": null,
        "summary": " - Mentor-KD is a novel reasoning distillation framework that improves the multi-step reasoning capabilities of small language models (LLMs) by addressing the limitations of insufficient distillation sets from large LLM teachers.\n- It introduces a mentor model, an intermediate-sized task-specific model, to augment the distillation sets by generating additional chain-of-thought (CoT) rationales and soft labels for the student model.\n- Through extensive experiments, Mentor-KD has shown to improve student performance and outperform existing reasoning distillation baselines on complex reasoning tasks, including commonsense, arithmetic, logical, and symbolic reasoning.\n- Notably, the student models trained with Mentor-KD sometimes even surpassed the performance of the LLM teacher (GPT-3.5) on certain tasks.\n- The framework also proved effective in low-resource scenarios, offering performance improvements even with limited distillation sets, which showcases its cost-efficiency.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/2hojae/mentor-kd"
        ],
        "huggingface_urls": [],
        "date": "2024-10-14"
    },
    {
        "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
        "authors": "Yiming Huang, lx865712528, bjEdward, FangyuLei, Jianwen2003",
        "link": "https://arxiv.org/abs/2410.07331",
        "github_repo": null,
        "summary": " - Introduces DA-Code, a benchmark designed to evaluate Large Language Models (LLMs) on agent-based data science tasks.\n - DA-Code features challenging tasks requiring advanced coding skills, diverse real-world data sources, and complex data science programming languages (Python, SQL, Bash).\n - A controllable and executable environment simulating real-world scenarios is provided, along with a meticulously designed evaluation suite and a DA-Agent baseline.\n - Experimental results show that even state-of-the-art LLMs achieve only 30.5% accuracy on DA-Code, indicating significant room for improvement in LLM-agent capabilities.\n - The benchmark and baseline are released to facilitate research in this area.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://da-code-bench.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-10-14"
    },
    {
        "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
        "authors": "WendellZwh, wangzhaoyang, StarThomas1002, Lillianwei, richardxp888",
        "link": "https://arxiv.org/abs/2410.10139",
        "github_repo": null,
        "summary": "\u2022 MMIE is a large-scale benchmark designed to evaluate the interleaved multimodal comprehension and generation capabilities of Large Vision-Language Models (LVLMs).\n\u2022 The benchmark comprises 20K meticulously curated multimodal queries across diverse fields, supporting both interleaved inputs and outputs in multiple-choice and open-ended formats.\n\u2022 An automated evaluation metric is proposed based on a fine-tuned InternVL-2-4B scoring model, which demonstrates strong alignment with human evaluation and mitigates potential biases. \n\u2022 Experimental results reveal that even state-of-the-art LVLMs and the combination of advanced LLMs with text-to-image models face significant challenges in MMIE, with most achieving moderate performance, indicating substantial room for improvement. \n\u2022 Error analysis categorizes key challenges into temporal understanding (cross-modality coherence, generation adaptability) and reasoning (multimodal information comprehension, complex reasoning) skills.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://mmie-bench.github.io/"
        ],
        "date": "2024-10-15"
    },
    {
        "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
        "authors": "Junan Zhang, Zilong Huang, beccabai, bczhou, Yejy53",
        "link": "https://arxiv.org/abs/2410.09732",
        "github_repo": null,
        "summary": "- LOKI, a new benchmark designed to evaluate large multimodal models (LMMs) on synthetic data detection across various modalities (video, image, 3D, text, and audio), has been introduced.\n- The benchmark includes 18K questions across 26 subcategories, with multi-level annotations including coarse-grained and multiple-choice questions, and fine-grained anomaly selection and explanation tasks.\n- An evaluation of 22 open-source and 6 closed-source LMMs on LOKI has revealed their potential as synthetic data detectors while also showing limitations such as model biases, a lack of expert domain knowledge, and unbalanced multimodal capabilities.\n- While LMMs exhibited moderate capabilities with some levels of explainability and generalization, they still lag behind human performance in synthetic data detection tasks.\n- Chain-of-thought prompting improved the performance of most LMMs, but not GPT-4, suggesting that GPT-4 already exhibits strong reasoning capabilities for this task.",
        "classification": [
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
        "authors": "Zhicheng Dou, Runqi Qiao, Yutao Zhu, Xiaoshuai Song, Guanting Dong",
        "link": "https://arxiv.org/abs/2410.09584",
        "github_repo": null,
        "summary": " \n- This paper introduces VIF-RAG, an automated, scalable, and verifiable data synthesis pipeline designed to improve instruction-following alignment in Retrieval-Augmented Generation (RAG) systems. \n- VIF-RAG begins with a small set of manually crafted atomic instructions and uses a combination of rule-based composition, supervised rewriting, and code-based verification to generate a large-scale dataset (VIF-RAG-QA) of instruction-following data for RAG. \n- It also presents FollowRAG, a new benchmark for evaluating complex instruction-following capabilities in RAG, composed of 2.8K samples covering 22 categories of general instruction constraints and 4 knowledge-intensive QA datasets. \n- In experiments, VIF-RAG significantly boosts performance across various LLMs and datasets, demonstrating a remarkable 44% improvement over the Llama3-base model in instruction-following within RAG scenarios. \n- The results further indicate that VIF-RAG not only enhances IF capability but also maintains stability in RAG performance across different model sizes and datasets, offering promise for real-world applications.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks",
        "authors": "wenhu, yuexiang96, DongfuJiang, yuanshengni, shermansiu",
        "link": "https://arxiv.org/abs/2410.10563",
        "github_repo": null,
        "summary": " \n- MEGA-BENCH is a multimodal evaluation benchmark comprising over 500 real-world tasks designed to assess the diverse capabilities of contemporary vision-language models. \n- The benchmark employs a taxonomy of multimodal tasks and incorporates diverse output formats, moving beyond standard multiple-choice questions to include numbers, phrases, code, LaTeX, and coordinates. \n-  A range of over 40 unique evaluation metrics, including rule-based and LLM-assisted options, is used to accommodate these diverse formats. \n-  In evaluations, MEGA-BENCH demonstrated GPT-4's superior performance over other flagship models, and Qwen2-VL's leading performance among open-source models. \n-  The benchmark facilitates fine-grained capability analysis by offering a breakdown of model performance across various dimensions such as input/output format and required skill. ",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Document Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "Animate-X: Universal Character Image Animation with Enhanced Motion Representation",
        "authors": "Dandan Zheng, Shiwei Zhang, Xiang Wang, Shuai Tan, BiaoGong",
        "link": "https://arxiv.org/abs/2410.10306",
        "github_repo": null,
        "summary": "\u2022 Animate-X is a universal image animation framework based on Latent Diffusion Models (LDM) that generates videos from a reference image and target pose sequence, applicable to various character types, including anthropomorphic characters. \n\u2022 It introduces a Pose Indicator with implicit and explicit components to enhance motion representation; the Implicit Pose Indicator (IPI) extracts comprehensive motion patterns from driving videos using CLIP visual features, while the Explicit Pose Indicator (EPI) strengthens LDM generalization by simulating misalignments between reference and pose during training.\n\u2022 Animate-X excels at animating anthropomorphic characters, addressing limitations of existing human-centric models by improving motion modeling and handling unique body structures.\n\u2022 A new Animated Anthropomorphic Benchmark (A2Bench) with 500 diverse characters and dance videos is introduced for evaluation.\n\u2022 Extensive experiments on A2Bench and existing human animation datasets demonstrate Animate-X's superior performance in preserving identity and motion consistency compared to state-of-the-art methods.",
        "classification": [
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content",
        "authors": "M. Jehanzeb Mirza, Sivan Doveh, Felipe Maia Polo, Nimrod Shabtay, wlin21at",
        "link": "https://arxiv.org/abs/2410.10783",
        "github_repo": null,
        "summary": "\n- LiveXiv is a novel, fully automated, multimodal live benchmark focusing on scientific domains, designed to address test set contamination and provide an updated evaluation of Large Multi-modal Models (LMMs).\n- It uses scientific papers from arXiv to generate Visual Question Answering (VQA) and Table Question Answering (TQA) pairs automatically, avoiding human bias and ensuring scalability.\n- An efficient evaluation pipeline based on Item Response Theory (IRT) allows for performance estimation on new benchmark versions by reevaluating only a small subset of models, significantly reducing computational costs.\n- The benchmark has been evaluated with 17 prominent open and proprietary LMMs, demonstrating its challenging nature and exposing model capabilities on less-contaminated data.\n- It provides the first version of the dataset including VQA and TQA pairs, alongside an efficient evaluation methodology and benchmark results, along with its limitations.",
        "classification": [
            "Visual Question Answering",
            "Table Question Answering",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/IBM/LiveXiv"
        ],
        "date": "2024-10-15"
    },
    {
        "title": "Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention",
        "authors": "Thorsten Gernoth, Liangchen Song, Chen Huang, Yifan Jiang, ir1d",
        "link": "https://arxiv.org/abs/2410.10774",
        "github_repo": null,
        "summary": "Cavia is a novel framework for generating multi-view consistent videos with precise camera control by converting an input image into multiple spatiotemporally consistent videos.\n- The framework extends spatial and temporal attention modules into view-integrated attention modules to enhance viewpoint and temporal consistency, enabling joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular videos.\n- Cavia allows users to specify camera motion while obtaining object motion.\n- Experiments demonstrate that Cavia surpasses state-of-the-art methods in geometric consistency and perceptual quality, showing its applicability in challenging indoor, outdoor, object-centric, and large-scene cases.\n- The framework allows for extrapolation to generate four views during inference and enables 3D reconstruction of generated frames.",
        "classification": [
            "Computer Vision",
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models",
        "authors": "Jianrui Zhang, Reuben Tan, Mu Cai, fengyao1909, BochengZou",
        "link": "https://arxiv.org/abs/2410.10818",
        "github_repo": null,
        "summary": " - TemporalBench, a novel video understanding benchmark, is introduced to evaluate the fine-grained temporal understanding abilities of multimodal video models.\n- The benchmark consists of ~10K video question-answer pairs derived from ~2K human-annotated captions with rich activity details, focusing on long-range dependencies and event progression.\n-  State-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, significantly lower than human performance (67.9%).\n- A critical pitfall in multi-choice QA is identified where LLMs can detect subtle changes in negative captions and find a \"centralized\" description as a cue for prediction.\n- Multiple Binary Accuracy (MBA) is proposed to correct such bias by decomposing multi-choice QA into multiple binary QAs.",
        "classification": [
            "Visual Question Answering",
            "Multimodal",
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://TemporalBench.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations",
        "authors": "Sanjay Shakkottai, Constantine Caramanis, Nataniel Ruiz, Yujia Chen, Litu Rout",
        "link": "https://arxiv.org/abs/2410.10792",
        "github_repo": null,
        "summary": "\u2022 This paper introduces a novel zero-shot method for inverting Rectified Flow (RF) models, particularly Flux, enabling image editing without additional training or optimization.\n\u2022 A controlled ODE is used for inversion, navigating between consistency with the input image and the true image distribution via a tunable controller guidance parameter.\n\u2022 It is theoretically shown that this controlled ODE corresponds to a rectified Stochastic Differential Equation (SDE).\n\u2022 Extensive qualitative results are demonstrated on tasks like stroke-to-image synthesis, cartoonization, and semantic image editing, with large-scale human evaluations indicating superior performance to existing methods.\n\u2022 For example, the method outperforms state-of-the-art by 89% in photorealism for stroke-to-image generation and shows 4.7% improvement in faithfulness on LSUN-bedroom compared to optimization-free methods.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "Tree of Problems: Improving structured problem solving with compositionality",
        "authors": "Rachel Bawden, Beno\u00eet Sagot, Armel Zebaze",
        "link": "https://arxiv.org/abs/2410.06634",
        "github_repo": "https://github.com/ArmelRandy/tree-of-problems",
        "summary": "-\nThis research paper proposes Tree of Problems (ToP), a novel prompting approach for enhancing the problem-solving abilities of Large Language Models (LLMs).\n-\nToP decomposes complex problems into a tree structure of simpler, analogous subproblems, leveraging compositionality for efficient problem-solving, and drawing inspiration from techniques like divide-and-conquer.\n-\nEmpirical results demonstrate that ToP outperforms existing methods like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of Thoughts (GoT) on structured tasks.\n-\nFurthermore, ToP excels in out-of-distribution generalization scenarios.\n-\nThe authors provide evidence of superior performance across various LLMs, including GPT-3.5, on difficult benchmark tasks such as Last Letter Concatenation and Navigate from BIG-Bench Hard.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/ArmelRandy/tree-of-problems"
        ],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies",
        "authors": "Xialin He, Tianyi Chen, Wenhao Wang, Zixuan Chen, Yanjie Ze",
        "link": "https://arxiv.org/abs/2410.10803",
        "github_repo": null,
        "summary": "\n- This paper introduces iDP3, an improved 3D diffusion policy for generalizable humanoid manipulation. \n- iDP3 leverages egocentric 3D visual representations, eliminating the need for camera calibration and point cloud segmentation, which are limitations in current methods that hinder deployment on mobile robots.\n- A whole-upper-body teleoperation system is developed to efficiently collect data from human demonstrations for training iDP3.\n- Experimental results show that iDP3 enables a full-sized humanoid robot to generalize contact-rich manipulation skills to a wide array of real-world scenarios, using only data collected in a single scene.\n- iDP3 demonstrates notable view invariance and object generalization capabilities.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "humanoid-manipulation.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
        "authors": "Kai-Wei Chang, Yuwei Zhang, Wenhao Yu, Hongwei Wang, xiaowu0162",
        "link": "https://arxiv.org/abs/2410.10813",
        "github_repo": null,
        "summary": "-\nLongMemEval, a comprehensive benchmark designed to evaluate the long-term memory capabilities of chat assistants. \n- It focuses on five core abilities: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention.\n- The benchmark consists of 500 meticulously curated questions embedded within freely scalable user-assistant chat histories.\n- A unified framework is presented that breaks down long-term memory design into four design choices across indexing, retrieval, and reading stages. \n- Several memory designs, including session decomposition, fact-augmented key expansion, and time-aware query expansion, are proposed and shown to greatly improve both memory recall and downstream question answering.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/xiaowu0162/LongMemEval"
        ],
        "huggingface_urls": [],
        "date": "2024-10-15"
    },
    {
        "title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation",
        "authors": "Haoming Xu, Bozhong Tian, Xiang Chen, Chenxi Wang, Ningyu",
        "link": "https://arxiv.org/abs/2410.11779",
        "github_repo": "https://github.com/zjunlp/DeCo",
        "summary": "This paper introduces DeCo, a novel dynamic correction decoding method for Multimodal Large Language Models (MLLMs) designed to mitigate hallucinations by leveraging information from preceding layers.\n- DeCo dynamically selects an appropriate preceding layer (\"anchor layer\") based on the probabilities of candidate tokens, and integrates its knowledge into the final layer to adjust the output logits, thereby correcting potential hallucinations.\n- The method is training-free and model-agnostic, compatible with various decoding strategies (greedy search, nucleus sampling, beam search) and applicable to different MLLMs.\n- Experimental results on benchmarks like CHAIR and POPE demonstrate that DeCo significantly reduces hallucination rates compared to baselines and existing methods like OPERA and VCD, with an average suppression rate of 10.8% on image captioning datasets.\n- Empirical analysis suggests that MLLMs can recognize visual objects in earlier layers, but this recognition is suppressed in later layers due to language model priors, leading to hallucinations. DeCo addresses this by correcting final-layer logits using more accurate information from preceding layers.\n- Further analysis shows DeCo is also effective in reducing snowballing hallucinations, where an initial hallucination leads to a cascade of errors.",
        "classification": [
            "Multimodal",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/zjunlp/DeCo"
        ],
        "huggingface_urls": [],
        "date": "2024-10-16"
    },
    {
        "title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models",
        "authors": "Xiaoshuai Song, Jiaheng Liu, Zekun Wang, Yanan Wu, Pei Wang",
        "link": "https://arxiv.org/abs/2410.11710",
        "github_repo": null,
        "summary": " - This paper introduces MTU-Bench, a multi-granularity tool-use benchmark designed to evaluate large language models' (LLMs) ability to interact with external tools. \n- MTU-Bench consists of two main components: MTU-Instruct, a diverse instruction tuning dataset for training LLMs on tool usage, and MTU-Eval, a comprehensive evaluation framework featuring fine-grained metrics that assess various tool-use scenarios without relying on GPT-based evaluation. \n- The authors propose a novel automated data synthesis pipeline based on existing task-oriented dialogue datasets to create MTU-Bench. \n- The benchmark covers five tool usage scenes: single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks. \n- Experiments demonstrate that fine-tuning LLaMA on MTU-Bench yields a robust model, MTU-LLaMA, with improved performance in various tool-use scenarios, outperforming the baseline model and demonstrating the efficacy of the MTU-Instruct dataset.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/MTU-Bench-Team/MTU-Bench.git"
        ],
        "huggingface_urls": [],
        "date": "2024-10-16"
    },
    {
        "title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI",
        "authors": "Wenbo Guo, Yuheng Tang, Zhun Wang, Yuzhou Nie, yuyangy",
        "link": "https://arxiv.org/abs/2410.11096",
        "github_repo": null,
        "summary": " - This paper introduces SECCODEPLT, a unified evaluation platform designed to assess the security risks of code generation AI models concerning insecure coding and cyberattack helpfulness. \n- For insecure coding, a two-stage data creation pipeline is employed, combining expert-crafted seed examples with LLM-based mutation and dynamic testing to ensure benchmark quality and scalability, covering 27 critical Python CWEs compared to existing benchmarks' 8. \n- For cyberattack helpfulness, a real-world attack environment with dynamic metrics is designed to evaluate models' capabilities across different attack stages based on MITRE ATT&CK. \n- Experimental results indicate SECCODEPLT outperforms CYBERSECEVAL in benchmark quality and reveals higher risks in SOTA models, including GPT-40 and Claude\u2019s capability to generate end-to-end attacks, also uncovering risks in the code agent Cursor where it failed on code injection, access control and data leakage prevention CWEs. \n- Additionally, providing security policy reminders significantly improves model performance in secure coding by 30%.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Virtue-AI-HUB/SecCodePLT"
        ],
        "date": "2024-10-16"
    },
    {
        "title": "LVD-2M: A Long-take Video Dataset with Temporally Dense Captions",
        "authors": "Zhijie Lin, Daquan Zhou, Yuqing Wang, XihuiLiu, YuuTennYi",
        "link": "https://arxiv.org/abs/2410.10816",
        "github_repo": "https://github.com/SilentView/LVD-2M",
        "summary": " - This paper introduces LVD-2M, a large-scale dataset of 2 million long-take videos with temporally dense captions, designed to address the limitations of existing datasets for training long video generation models.\n- The dataset creation involved an automatic pipeline with low-level filtering (scene cut detection, optical flow) and semantic-level filtering (video LLMs) to select high-quality videos, and a hierarchical captioning approach combining LLaVA and Claude3-Haiku to generate detailed descriptions of video content over time.\n- Human evaluations show LVD-2M surpasses other datasets in long-take consistency, dynamic degree, and caption quality.\n- Fine-tuning experiments with both diffusion-based and LM-based video generation models demonstrate that LVD-2M improves their ability to generate longer, more dynamic videos with smoother transitions and camera motions.\n- The authors argue that LVD-2M will significantly benefit future research in long video generation.",
        "classification": [
            "Text-to-Video",
            "Video-Text-to-Text",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/SilentView/LVD-2M"
        ],
        "huggingface_urls": [],
        "date": "2024-10-16"
    },
    {
        "title": "What Matters in Transformers? Not All Attention is Needed",
        "authors": "Zheyu Shen, Guoheng Sun, Shwai He, charleslipku",
        "link": "https://arxiv.org/abs/2406.15786",
        "github_repo": "https://github.com/Shwai-He/LLM-Drop",
        "summary": "-\"What Matters in Transformers? Not All Attention is Needed\" introduces a method called \"Joint Layer Drop\" to efficiently prune redundant attention and MLP layers in Transformer-based language models.\n-The method identifies these layers using a similarity-based metric, removing those with minimal transformation between input and output, and prioritizes dropping attention layers due to their observed higher redundancy.\n-Experiments demonstrate that removing a substantial portion of attention layers (e.g., 50% in Llama-2-70B) leads to minimal performance degradation while significantly improving inference speed (48.4% speedup with a 2.4% performance drop).\n-The redundancy in attention layers is found to be consistent throughout training, suggesting it's an inherent property, and Joint Layer Drop allows for even more aggressive pruning by targeting both attention and MLP layers for increased efficiency.\n-This work reveals that not all attention layers are equally important in transformers, leading to potential improvements in future network architecture designs and training techniques.",
        "classification": [
            "Natural Language Processing",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/Shwai-He/LLM-Drop"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"
        ],
        "date": "2024-10-16"
    },
    {
        "title": "GS^3: Efficient Relighting with Triple Gaussian Splatting",
        "authors": "Xiang Feng, Fan Pei, Yixin Zeng, Zoubin Bi, NCJ",
        "link": "https://arxiv.org/abs/2410.11419",
        "github_repo": null,
        "summary": "GS\u00b3 is a novel spatial and angular Gaussian-based representation with a triple splatting process for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit images.\n- Each spatial Gaussian uses a Lambertian plus a mixture of angular Gaussians as an effective reflectance function.\n- Self-shadowing is efficiently generated by splatting spatial Gaussians towards the light source and refining with an MLP.\n- An additional MLP compensates for global illumination effects.\n- Achieves a training time of 40-70 minutes and rendering speed of 90 fps on a single commodity GPU, outperforming state-of-the-art techniques in terms of quality/performance.",
        "classification": [
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://GSrelight.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-16"
    },
    {
        "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
        "authors": "Jia Zeng, Jisong Cai, Li Chen, Hongyang Li, qwbu",
        "link": "https://arxiv.org/abs/2410.08001",
        "github_repo": null,
        "summary": "-\"RoboDual\", a novel synergistic dual-system framework for robotic manipulation, combines a large-scale pre-trained generalist (OpenVLA) with an efficient, task-specific diffusion transformer-based specialist.\n- The specialist refines the generalist's high-level understanding and discretized actions through multimodal sensory inputs (RGB, depth, tactile) for precise real-time control.\n- RoboDual improves performance and efficiency over standalone generalist or specialist models, achieving a 12% gain on CALVIN and a 20% improvement in real-world multi-instruction tasks.\n- The system demonstrates strong generalization across varying positions, distractions, backgrounds, and objects with minimal training data (5% of demonstrations).\n- With a higher control frequency (15Hz), RoboDual enables more complex, dexterous control than is possible with current VLA models alone.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-16"
    },
    {
        "title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices",
        "authors": "Liangliang Zhao, Guoli Jia, Yuzhu Zhang, Zhiyuan Ma, iseesaw",
        "link": "https://arxiv.org/abs/2410.11795",
        "github_repo": null,
        "summary": "\u2022 This survey paper provides a comprehensive overview of efficient Diffusion Models (DMs), focusing on principles and practices related to architecture design, training, inference, and deployment.\n\u2022 The paper categorizes efficient DMs into six areas: principles, efficient architecture, training and fine-tuning, sampling and inference, deployment, and applications and discusses the tradeoffs between performance, efficiency, and computational costs. \n\u2022  It also emphasizes the potential of DMs to develop emergent capabilities similar to LLMs, highlighting the recent success of models like Sora in video generation. \n\u2022 The survey also discusses the limitations of current DMs, including computational complexity, limited generalization of fine-tuned models, and deployment challenges, and points out potential future research directions such as designing unified frameworks for multimodal control and integrating Mixture of Experts (MoE) designs. \n\u2022 It includes a GitHub repository compiling the surveyed papers with the same taxonomy for easy access and future updates.",
        "classification": [
            "Computer Vision",
            "Text-to-Image",
            "Image-to-Image",
            "Image-to-Video",
            "Text-to-Video",
            "Text-to-3D",
            "Image-to-3D"
        ],
        "github_urls": [
            "https://github.com/ponyzym/Efficient-DMs-Survey"
        ],
        "huggingface_urls": [],
        "date": "2024-10-16"
    },
    {
        "title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks",
        "authors": "Xiao Li, Guancheng Lin, Huiyu Bai, Linquan Wu, zfj1998",
        "link": "https://arxiv.org/abs/2410.12381",
        "github_repo": "https://github.com/HumanEval-V/HumanEval-V-Benchmark",
        "summary": "-\nHumanEval-V is introduced; a novel benchmark designed to evaluate the visual understanding and reasoning abilities of Large Multimodal Models (LMMs) through Python code generation tasks. \n- The benchmark comprises 108 coding tasks adapted from platforms like CodeForces and Stack Overflow, each requiring LMMs to integrate visual and textual information to generate functional code. \n- Evaluation results for 19 state-of-the-art LMMs reveal that even leading proprietary models struggle, with GPT-4o achieving 13% pass@1, highlighting limitations in visual reasoning and coding abilities. \n-  Ablation studies indicate current LMMs have limitations in vision reasoning and coding capabilities, showing significant performance improvement when image descriptions are provided.\n- Further analysis reveals that open-weight LMMs suffer deteriorated coding performance after vision-encoder integration, suggesting areas for future LMM research.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/HumanEval-V/HumanEval-V-Benchmark"
        ],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI",
        "authors": "Sicheng Zhou, Yangyang Yu, Kechen Fang, yetian, SijieCheng",
        "link": "https://arxiv.org/abs/2410.11623",
        "github_repo": null,
        "summary": " - VidEgoThink is introduced; a benchmark designed to assess egocentric video understanding capabilities for embodied AI, focusing on bridging the gap between Multimodal Large Language Models (MLLMs) and low-level control.\n- It incorporates four tasks: video question answering, hierarchical planning, visual grounding, and reward modeling.\n - Leverages GPT-4 to generate data automatically, which is filtered by human annotators.  This pipeline is based on the Ego4D dataset.\n- Experimental evaluation of various MLLMs, including GPT-4, open-source image and video-based models, reveals poor performance across all tasks, particularly in sequence and order understanding.\n - Findings indicate a need for significant advancements in foundational models for first-person Embodied AI applications.",
        "classification": [
            "Visual Question Answering",
            "Multimodal",
            "Video-Text-to-Text",
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio",
        "authors": "Hang Zhang, Yang Zhou, Yun Xing, Sicong Leng, ClownRat",
        "link": "https://arxiv.org/abs/2410.12787",
        "github_repo": null,
        "summary": "-\nThis paper investigates hallucinations in Large Multimodal Models (LMMs) across language, visual, and audio modalities.\n- Two key contributors to hallucinations are identified: overreliance on unimodal priors and spurious inter-modality correlations.\n- The Curse of Multi-Modalities (CMM) benchmark is introduced, which provides a detailed analysis of these underlying issues.\n- CMM converts hallucination evaluation into a binary classification task with object-level and event-level probing across 1200 samples with 2400 probing questions.\n- Experimental results reveal key vulnerabilities, including imbalances in modality integration and biases from training data.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "github.com/DAMO-NLP-SG/CMM"
        ],
        "huggingface_urls": [
            "cmm-damovl.site"
        ],
        "date": "2024-10-17"
    },
    {
        "title": "Revealing the Barriers of Language Agents in Planning",
        "authors": "Kai Zhang, Siyu Yuan, jiangjiechen, kexunz, hsaest",
        "link": "https://arxiv.org/abs/2410.12409",
        "github_repo": null,
        "summary": " - This paper investigates the limitations of current large language models (LLMs) in planning tasks using feature attribution analysis. \n- It identifies two key weaknesses: a limited understanding of constraints and the diminishing influence of questions as the planning horizon expands.\n- The study explores episodic and parametric memory updating strategies, finding that while they improve constraint and question utilization, they do not fully resolve the core issues.\n- The episodic memory updating reiterates constraints, making them easier for agents to recognize, but agents primarily understand it on a global level.\n-  Parametric memory updating enhances the impact of questions, yet agents still lose focus on them as the horizon increases; both strategies resemble shortcut learning and are insufficient for high-level planning.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception",
        "authors": "Conghui He, Bin Wang, Hengrui Kang, Zhiyuan Zhao",
        "link": "https://arxiv.org/abs/2410.12628",
        "github_repo": "https://github.com/opendatalab/DocLayout-YOLO",
        "summary": " - This paper introduces DocLayout-YOLO, a novel approach for Document Layout Analysis (DLA) that balances speed and accuracy. \n- DocLayout-YOLO employs document-specific optimizations in pre-training and model design, using the DocSynth-300K dataset generated by the Mesh-candidate BestFit algorithm. \n- The model incorporates a Global-to-Local Controllable Receptive Module (GL-CRM) to handle multi-scale variations in document elements. \n- DocLayout-YOLO achieves state-of-the-art performance on D\u2074LA (70.3% mAP), DocLayNet (79.7% mAP), and the newly introduced DocStructBench (78.8% mAP) datasets. \n - The method maintains an inference speed of 85.5 frames per second (FPS).",
        "classification": [
            "Object Detection",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/opendatalab/DocLayout-YOLO"
        ],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "Exploring Model Kinship for Merging Large Language Models",
        "authors": "Huajun Chen, Shumin Deng, Ningyu Zhang, Yunzhi Yao, Yedi Hu",
        "link": "https://arxiv.org/abs/2410.12613",
        "github_repo": "https://github.com/zjunlp/ModelKinship",
        "summary": "\n- This paper introduces \"model kinship\", a metric to assess the similarity between Large Language Models (LLMs), drawing an analogy to biological kinship, for enhanced model merging.\n- It is shown empirically that model kinship correlates with performance gains after merging, which helps guide the selection of candidate models for merging and escape local optima.\n- A novel merging strategy, \"Top-k Greedy Merging with Model Kinship\", is proposed, demonstrating improved performance on benchmark datasets by mitigating performance degradation and avoiding local optima during model evolution.\n- The analysis of model evolution through iterative merging reveals two distinct stages: a learning stage with rapid performance improvement and a saturation stage where improvements plateau, with the latter attributed to weight space convergence and high kinship values.\n- Model kinship is further suggested as a criterion for early stopping in the merging process, which improves efficiency without compromising performance gains.\n",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/zjunlp/ModelKinship"
        ],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
        "authors": "Yi Chang, Yahan Li, WhiteCatY, xiatingyu",
        "link": "https://arxiv.org/abs/2410.10672",
        "github_repo": "https://github.com/MLGroupJLU/MatrixNuclearNorm",
        "summary": "\u2022 This paper introduces Matrix Nuclear-Norm, a novel metric for evaluating the information compression and redundancy reduction capabilities of Large Language Models (LLMs).\n\u2022 The metric leverages the nuclear norm and its L1,2-norm approximation to quantify the data compression proficiency of LLMs.\n\u2022 Matrix Nuclear-Norm addresses the computational limitations of existing metrics like Matrix Entropy by reducing the time complexity from O(n\u00b3) to O(n\u00b2), eliminating the need for Singular Value Decomposition (SVD).\n\u2022 Experimental results on various LLMs, including Cerebras-GPT and Pythia, demonstrate that Matrix Nuclear-Norm effectively captures compression capabilities with values decreasing as model size increases.\n\u2022 Evaluations on benchmark datasets like AlpacaEval and Chatbot Arena confirm that the proposed metric reliably assesses and ranks model performance, achieving a balance between accuracy and computational efficiency.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/MLGroupJLU/MatrixNuclearNorm"
        ],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
        "authors": "Dahua Lin, Xinyu Fang, KennyUTC, zsytony, JingmingZ",
        "link": "https://arxiv.org/abs/2410.12405",
        "github_repo": "https://github.com/open-compass/ProSA",
        "summary": "\u2022 ProSA, a framework designed to evaluate and understand prompt sensitivity in LLMs, is introduced, incorporating a novel sensitivity metric, PromptSensiScore (PSS), and leveraging decoding confidence.\n\u2022 PSS quantifies the average discrepancy in LLM responses when given different semantic variants of the same instruction.\n\u2022 The study, spanning multiple tasks and models, reveals that prompt sensitivity varies across datasets and models, with larger models generally exhibiting better robustness, and few-shot examples, especially for larger models, mitigate sensitivity.\n\u2022 Subjective evaluations highlight increased sensitivity in complex reasoning tasks compared to straightforward ones, with higher model confidence correlating with increased prompt robustness.\n\u2022 Prompt sensitivity is linked to decoding confidence, where greater confidence corresponds to higher robustness against prompt variations.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/open-compass/ProSA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression",
        "authors": "Wenqi Shao, Jing Liu, Feng Chen, Yefei He, kpzhang996",
        "link": "https://arxiv.org/abs/2410.08584",
        "github_repo": null,
        "summary": "-\nZipVL is an efficient inference framework for Large Vision-Language Models (LVLMs) that addresses computational and memory bottlenecks through dynamic token sparsification and KV cache compression.\n-\nIt employs a layer-wise adaptive ratio assignment for important tokens based on attention score distribution, optimizing both prefill and decoding phases.\n-\nThe prefill phase is accelerated by performing attention only on important tokens, seamlessly integrating with existing attention implementations.\n-\nMixed-precision quantization is applied to the KV cache, using higher bit-width for important tokens and lower bit-width for others, reducing memory usage without significant performance loss.\n-\nExperiments show ZipVL accelerates prefill by 2.6x and reduces GPU memory by 50% with minimal accuracy reduction on Video-MME, outperforming fixed-ratio methods like FastV.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "Improving Long-Text Alignment for Text-to-Image Diffusion Models",
        "authors": "Chongxuan Li, Zehan Wang, Tianyu Pang, Chao Du, luping-liu",
        "link": "https://arxiv.org/abs/2410.11817",
        "github_repo": "https://github.com/luping-liu/LongAlign",
        "summary": "LongAlign is proposed to improve long-text alignment for text-to-image (T2I) generation using diffusion models.\n- The method introduces segment-level encoding, where long texts are divided into shorter segments and encoded individually before merging the results. \n- For preference optimization, decomposed CLIP-based preference models are used to fine-tune diffusion models, separating text-relevant alignment from other visual aspects. \n- A reweighting strategy is proposed to assign different weights to the text-relevant and text-irrelevant components, addressing overfitting. \n- Fine-tuning Stable Diffusion v1.5 with LongAlign for 20 hours on 6 A100 GPUs leads to improved alignment, outperforming models like PixArt-a and Kandinsky v2.2 on long text inputs.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/luping-liu/LongAlign"
        ],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models",
        "authors": "Yang Song, Cheng Lu",
        "link": "https://arxiv.org/abs/2410.11081",
        "github_repo": null,
        "summary": "\u2022 This paper introduces simplified continuous-time consistency models (sCMs), a new class of diffusion-based generative models trained for fast sampling.\n\u2022 sCMs address training instabilities in continuous-time models through improvements in diffusion process parameterization (TrigFlow), network architecture (time-conditioning and adaptive group normalization), and training objectives (adaptive weighting and progressive annealing).\n\u2022 These sCMs scale effectively, reaching 1.5 billion parameters on ImageNet 512x512 and achieving FID scores competitive with state-of-the-art diffusion models using only two sampling steps.\n\u2022 The paper demonstrates that continuous-time CMs achieve better sample quality by minimizing discretization errors compared to discrete-time variants.\n\u2022  sCMs produce more diverse samples and handle guidance better than variational score distillation (VSD), which can struggle with high guidance levels and produce overly smooth samples.",
        "classification": [
            "Unconditional Image Generation",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL",
        "authors": "Sonali Parbhoo, Arjun Jagota, Jared Joselowitz, skrishna",
        "link": "https://arxiv.org/abs/2410.12491",
        "github_repo": null,
        "summary": "\u2022 This paper introduces a novel approach to interpreting Large Language Models (LLMs) by applying Inverse Reinforcement Learning (IRL) to recover their implicit reward functions, focusing on toxicity-aligned LLMs.\n\u2022 Experiments conducted on toxicity-aligned LLMs of varying sizes extracted reward models that achieved up to 80.40% accuracy in predicting human preferences.\n\u2022 The analysis reveals insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the Reinforcement Learning from Human Feedback (RLHF) process.\n\u2022 The study demonstrates that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks.\n\u2022 The paper proposes that this work provides a new perspective for understanding and improving LLM alignment, with implications for responsible development.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "Neural Metamorphosis",
        "authors": "Xinchao Wang, Xingyi Yang",
        "link": "https://arxiv.org/abs/2410.11878",
        "github_repo": null,
        "summary": "-\nNeuMeta, a novel learning paradigm to construct self-morphable neural networks.\n- Instead of training separate models for different architectures or sizes, NeuMeta learns the continuous weight manifold of neural networks using neural implicit functions as hypernetworks. \n- These hypernetworks take coordinates within the model space as input and generate corresponding weight values on the manifold. \n- To enhance the smoothness of the manifold, NeuMeta employs weight matrix permutation and introduces noise during hypernetwork training.\n- Experimental results across image classification, semantic segmentation, and image generation tasks demonstrate that NeuMeta preserves full-sized model performance even at high compression rates, generalizes to unseen network configurations, and even outperforms individually trained models in some cases.",
        "classification": [
            "Image Classification",
            "Image Segmentation",
            "Unconditional Image Generation",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation",
        "authors": "Juan Carlos Climent Pardo, Yingya Li, Siena Placino, Jo\u00e3o Matos, shanchen",
        "link": "https://arxiv.org/abs/2410.12722",
        "github_repo": null,
        "summary": "\n- WorldMedQA-V is a new multilingual and multimodal dataset designed to evaluate the performance of multimodal language models (VLMs) on medical question answering tasks.\n- The dataset consists of 568 multiple-choice questions with images from real medical exams in Brazil, Israel, Japan, and Spain.\n- Evaluations of several popular open and closed-source VLMs reveal that GPT4o achieved the best performance, generally exceeding passing thresholds across countries and both local languages and English translations.\n- Including the associated image with the medical question generally improves the model performance, particularly for models with lower baseline accuracies.\n- The results also highlight persistent language disparities, where models showed relatively lower performance on Hebrew, potentially due to underrepresentation in pre-training datasets.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/WorldMedQA/V"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/WorldMedQA/V"
        ],
        "date": "2024-10-17"
    },
    {
        "title": "OMCAT: Omni Context Aware Transformer",
        "authors": "Andrew Tao, Rafael Valle, Matthieu Le, Karan Sapra, goarushi27",
        "link": "https://arxiv.org/abs/2410.12109",
        "github_repo": null,
        "summary": " - The paper introduces OMCAT (Omni Context Aware Transformer), a novel multimodal large language model designed for enhanced temporal understanding and cross-modal alignment in audio-visual contexts. \n- OMCAT leverages ROTE (Rotary Time Embeddings), a modification of RoPE (Rotary Position Embeddings), to encode absolute and relative temporal information, improving performance on time-anchored tasks.\n-  A new dataset, OCTAV (Omni Context and Temporal Audio Video), is also introduced, focusing on event transitions within videos and their correlation with audio cues, facilitating training for fine-grained temporal reasoning. \n-  OMCAT undergoes a three-stage training process: feature alignment, instruction tuning, and OCTAV-specific training, achieving state-of-the-art results on Audio-Visual Question Answering (AVQA) and temporal video grounding benchmarks, surpassing existing models on the OCTAV dataset by a significant margin. \n- The paper's contributions include a new model and dataset, demonstrating significant advancements in multimodal LLMs' capacity for fine-grained temporal and cross-modal understanding.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "Tracking Universal Features Through Fine-Tuning and Model Merging",
        "authors": "Desmond Elliott, nilq",
        "link": "https://arxiv.org/abs/2410.12391",
        "github_repo": null,
        "summary": "\n- This paper investigates the evolution of features in one-layer Transformer language models during fine-tuning and merging.\n- The study uses sparse autoencoders to extract and track features across models trained on different domains (English text, Python, Lua, TinyStories).\n- Findings reveal that few features persist across models, but those that do are often interpretable, relating to code-related elements like punctuation and formatting.\n- Case studies highlight a persistent variable assignment feature and a disappearing Python exception-handling feature.\n- The paper contributes to understanding feature dynamics in transfer learning scenarios.",
        "classification": [
            "Natural Language Processing",
            "Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities",
        "authors": "Jeff Dalton, Iain Mackie, Sean MacAvaney, Shubham Chatterjee, Thong Nguyen",
        "link": "https://arxiv.org/abs/2410.07722",
        "github_repo": "https://github.com/thongnt99/DyVo",
        "summary": "-\nDyVo, a novel dynamic vocabulary model, is introduced to enhance Learned Sparse Retrieval (LSR) by incorporating Wikipedia entities into the vocabulary.\n- The model utilizes a Dynamic Vocabulary (DyVo) head which leverages existing entity embeddings and an entity retrieval component to generate entity weights.\n- These weights are merged with word piece weights and used for efficient indexing and retrieval using an inverted index.\n- Experiments on three entity-rich document ranking datasets show DyVo consistently outperforms state-of-the-art baselines, demonstrating significant improvements over traditional LSR models by incorporating entities.\n- A few-shot generative entity retrieval approach using LLMs like Mixtral and GPT-4 is introduced, generating highly relevant entity candidates leading to superior performance compared to using linked entities or entities found by human annotators.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/thongnt99/DyVo"
        ],
        "huggingface_urls": [],
        "date": "2024-10-17"
    },
    {
        "title": "MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures",
        "authors": "kcz358, fuzhao, Junhao233, dghosal, jinjieni",
        "link": "https://arxiv.org/abs/2410.13754",
        "github_repo": null,
        "summary": "\n- MixEval-X is a benchmark for evaluating multimodal models across various input-output modalities, including image, video, audio, text, and actions.\n- The benchmark covers eight input-output modality combinations and uses a mixture of existing datasets and real-world web data to construct evaluations. \n- MixEval-X employs a novel multi-modal benchmark mixture and adaptation-rectification pipeline to optimize evaluation tasks by aligning them with real-world task distributions and mitigating biases. \n- Meta-evaluations demonstrate that MixEval-X effectively aligns benchmark samples with real-world distributions, with model rankings correlating strongly (up to 0.98) with crowd-sourced real-world evaluations. \n- The benchmark offers comprehensive leaderboards to rerank existing models and organizations across modalities.",
        "classification": [
            "Multimodal",
            "Any-to-Any",
            "Image-to-Text",
            "Video-Text-to-Text",
            "Text-to-Image",
            "Text-to-Video",
            "Text-to-Audio"
        ],
        "github_urls": [
            "https://mixeval-x.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Movie Gen: A Cast of Media Foundation Models",
        "authors": "Animesh Sinha, Andros Tjandra, Andrew Brown, Amit Zohar, Adam Polyak",
        "link": "https://arxiv.org/abs/2410.13720",
        "github_repo": null,
        "summary": " \n- Meta introduces MovieGen, a suite of foundation models for generating high-quality HD videos with synchronized audio, personalized characters, and video editing capabilities. \n- The core is a 30B parameter transformer (MovieGen Video) trained using a flow matching objective on a large-scale dataset of image-text and video-text pairs. It jointly models text-to-image and text-to-video generation with additional finetuning stages for personalization and editing. \n- MovieGen outperforms prior work, including commercial systems (Runway Gen-3, LumaLabs, OpenAI Sora) on overall video quality and demonstrates novel capabilities in personalization and precise video editing.  \n- A 13B parameter model (MovieGen Audio) generates high-quality sound effects and music synchronized with video using text or video input. \n- MovieGen is state-of-the-art on multiple media generation tasks, validated by comprehensive human and automated metric evaluations and benchmarks. ",
        "classification": [
            "Text-to-Video",
            "Text-to-Image",
            "Video-Text-to-Text",
            "Video-Text-to-Text",
            "Text-to-Audio",
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/MovieGenBench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
        "authors": "Yuxiao Qu, Yifan Song, yuexiang96, oottyy, jeepliu",
        "link": "https://arxiv.org/abs/2410.13824",
        "github_repo": null,
        "summary": "\n- This paper introduces MultiUI, a 7.3 million sample dataset synthesized from 1 million web page UIs using LLMs, for training multimodal models in text-rich visual understanding.\n- MultiUI covers nine diverse tasks across three categories (visual understanding and reasoning, text recognition, and grounding), enhancing model perception, comprehension, grounding, and reasoning capabilities. \n- Models trained on MultiUI demonstrate significant improvement, up to 48% on VisualWebBench and 19.1% on Mind2Web, outperforming larger models like LLaVA 1.6 34B and GPT-4V in GUI tasks. \n- MultiUI also generalizes well to non-web UI tasks like document understanding, OCR, and chart interpretation, showing strong cross-domain generalization.\n- This highlights the value of structured web UI data for advancing text-rich visual understanding in MLLMs.\n",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Document Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://neulab.github.io/MultiUI/"
        ],
        "date": "2024-10-18"
    },
    {
        "title": "MobA: A Two-Level Agent System for Efficient Mobile Task Automation",
        "authors": "Yixuan Jiang, Kunyao Lan, Yansi Li, Hao Tang, JamesZhutheThird",
        "link": "https://arxiv.org/abs/2410.13757",
        "github_repo": null,
        "summary": "-\nMobA, a novel two-level agent architecture designed to enhance the abilities of mobile phone assistants, using Multimodal Large Language Models (MLLMs).\n- \nComposed of a higher-level Global Agent for tasks such as command interpretation and task planning, and a lower-level Local Agent to select and execute actions based on current screen information and historical data.\n- \nA double reflection mechanism allowing the system to correct errors quickly and avoid sub-optimal operations, as well as an integrated memory module to track actions and optimize execution.\n- \nEvaluation performed on the Mobbench dataset containing 50 mobile tasks across 10 applications of varying difficulty, outperforming other mobile agents, achieving the highest milestone score of 66.2%.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
        "authors": "zdaxie, zizhpan, XCLiu, CNMaxwell, WuChengyue",
        "link": "https://arxiv.org/abs/2410.13848",
        "github_repo": null,
        "summary": "-\nJanus is an autoregressive multimodal model that decouples visual encoding pathways for understanding and generation tasks using a shared transformer architecture. \n- For understanding, it uses a SigLIP encoder for high-level semantic information, while for generation, it utilizes a VQ tokenizer focusing on fine-grained visual details. \n- This approach addresses the conflicting representational needs of the two tasks, enabling both strong performance and model flexibility. \n- Experimental results demonstrate that Janus outperforms other unified models of comparable size and matches or exceeds task-specific models on benchmarks like MMBench, SEED-Bench, POPE, MSCOCO, and GenEval. \n- The model's performance and flexibility make it a potential candidate for the next generation of unified multimodal models.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/deepseek-ai/Janus"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
        "authors": "Weijia Shi, Tianze Wang, Haoran Li, Kangyu Zhu, richardxp888",
        "link": "https://arxiv.org/abs/2410.13085",
        "github_repo": "https://github.com/richard-peng-xia/MMed-RAG",
        "summary": "MMed-RAG is a new multimodal retrieval-augmented generation (RAG) system designed to improve the factuality of Medical Large Vision-Language Models (Med-LVLMs).\n- It incorporates a domain-aware retrieval mechanism, adaptive context selection, and RAG-based preference fine-tuning to address misalignment issues and enhance alignment with ground truth.\n- The model achieves an average improvement of 43.8% in factual accuracy across five medical datasets and two tasks (medical VQA and report generation) compared to the original Med-LVLM. \n- It outperforms other decoding-based and RAG-based approaches on medical VQA and report generation tasks.\n- MMed-RAG demonstrates strong generalizability, achieving consistent improvements across various medical image modalities (radiology, ophthalmology, and pathology).\n- Through ablation studies, the contribution of each proposed component is validated, demonstrating its effectiveness in enhancing the factuality and performance of Med-LVLMs in different medical domains.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/richard-peng-xia/MMed-RAG"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models",
        "authors": "Keming Lu, Hongyu Lin, Bowen Yu, Le Yu, TangQiaoYu",
        "link": "https://arxiv.org/abs/2410.13841",
        "github_repo": null,
        "summary": "-\nThis paper introduces a unified perspective on delta parameter editing in post-trained large-scale models, formulating editing operations based on Riemann sum approximation of the loss difference.\n- This analysis categorizes existing methods into three performance classes: competitive (e.g., DARE, DELLA-Merging), decreased (e.g., BitDelta, Twin-Merging, TIES-Merging), and improved (e.g., EXPO), explaining their impact on model performance through the lens of Riemann sum approximation.\n- Extensive experiments on visual and language models (ViT, LLaMA 3, Qwen 2, Mistral) support the theoretical findings.\n- The paper further proposes extensions to existing techniques like DARE and BitDelta, generalizing their formats and improving applicability.\n- For example, introducing a factor *k* to DARE handles dropped parameters more effectively and expanding BitDelta to use multiple bits improves performance beyond the original post-trained model.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment",
        "authors": "Ke Xu, Jiaheng Liu, Shawn Wang, Zekun Moore Wang, kangz",
        "link": "https://arxiv.org/abs/2410.13785",
        "github_repo": null,
        "summary": "PopAlign is a framework for aligning large language models (LLMs) by diversifying contrasting patterns across prompt, model, and pipeline levels.\n- It integrates six distinct contrasting strategies: Prefix Contrast, Demon Contrast, Elicitive Contrast, NParam Contrast, Leaderboard Contrast, and Refine Contrast.\n- These strategies synthesize preference-contrastive data without requiring additional feedback labeling.\n- Experimental results demonstrate that PopAlign significantly outperforms existing methods on various alignment tasks and leaderboards.\n- Notably, PopAlign achieves higher scores than strong baselines trained on original labels, indicating its effectiveness in preference modeling and comprehensive alignment.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
        "authors": "Shuicheng Yan, Li Yuan, Bo Zhu, Chat-UniVi",
        "link": "https://arxiv.org/abs/2410.11842",
        "github_repo": "https://github.com/SkyworkAI/MoH",
        "summary": "-\nMixture-of-Head attention (MoH) is proposed, which integrates multi-head attention with a Mixture-of-Experts (MoE) mechanism by treating attention heads as experts.\n-\nMoH employs a router to select the top-k heads for each token, improving inference efficiency, and uses a weighted sum of outputs rather than standard summation, potentially enhancing performance.\n-\nShared heads in MoH retain constant activation, capturing general knowledge.\n-\nEvaluations on ViT, DiT, and LLMs show MoH outperforms multi-head attention using only 50%~90% of heads.\n-\nPre-trained models like LLaMA3-8B can be continue-tuned into MoH models, with MoH-LLaMA3-8B showing improved accuracy with fewer heads.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Unconditional Image Generation",
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SkyworkAI/MoH"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control",
        "authors": "Haonan Qiu, Xiang Wang, Hangjie Yuan, Shiwei Zhang, Yujie Wei",
        "link": "https://arxiv.org/abs/2410.13830",
        "github_repo": null,
        "summary": "-\nDreamVideo-2 is a zero-shot video customization framework that generates videos with specified subjects and motion trajectories guided by a single image and bounding box sequence, respectively, without test-time fine-tuning.\n-\nIt introduces reference attention, leveraging inherent model capabilities for subject learning, and a mask-guided motion module comprising a spatiotemporal encoder and ControlNet for precise motion control using box masks.\n-\nTo address motion control dominance over subject representation, it incorporates masked reference attention with blended latent mask modeling to prioritize subject identity at desired positions, along with a reweighted diffusion loss balancing subject learning and motion control by differentiating regional contributions within and outside bounding boxes.\n-\nDreamVideo-2 consistently outperforms current state-of-the-art methods in subject customization and motion control, as demonstrated on a newly curated, comprehensive single-subject video dataset with captions, masks, and bounding boxes.\n-\nThe dataset, code, and models will be made public.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "VidPanos: Generative Panoramic Videos from Casual Panning Videos",
        "authors": "Shiran Zada, Roni Paiss, Erika Lu, Jingwei Ma, fcole",
        "link": "https://arxiv.org/abs/2410.13832",
        "github_repo": null,
        "summary": "This paper introduces a novel method for generating panoramic videos from casually captured panning videos, effectively extending the field of view beyond the camera's limits. \n- The method addresses the challenge of limited spatio-temporal context windows in existing generative video models by employing a coarse-to-fine synthesis approach and spatial aggregation techniques.\n- It leverages pre-trained diffusion-based (Lumiere) and token-based (Phenaki) video generation models and adapts them to complete panoramic videos of arbitrary length and width, realistically filling in unseen regions.\n- The approach demonstrates success in generating coherent and plausible panoramas with dynamic elements like moving people and objects, even in challenging scenarios with camera motion.\n- Evaluation on synthetic and real-world panning videos shows qualitative and quantitative improvements over baseline methods, including linear interpolation, flow-based inpainting, and a recent video generation model (MAGVIT).",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Retrospective Learning from Interactions",
        "authors": "Anne Wu, Gloria Geng, Yiwei Chen, Mustafa Omer Gul, Zizhao Chen",
        "link": "https://arxiv.org/abs/2410.13852",
        "github_repo": null,
        "summary": " - This paper introduces RESPECT, a novel method for improving large language models (LLMs) through retrospective learning from implicit feedback signals in multi-turn interactions.\n- RESPECT leverages user responses such as rephrased requests, expressions of frustration, or task pivots as implicit feedback signals, eliminating the need for explicit annotations or feedback solicitation.\n - The method involves decoding feedback from past interactions by prompting the LLM to analyze interaction contexts and follow-up utterances.\n-  This decoded feedback is then used to re-train the LLM, resulting in continual improvement over multiple rounds of interaction and training.\n-  In a new multimodal interaction scenario called MULTIREF, where humans instruct an LLM to solve an abstract reasoning task, RESPECT demonstrates significant improvement, boosting task completion rate from 31% to 82% without external annotations.",
        "classification": [
            "Multimodal",
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://lil-lab.github.io/respect"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "FlatQuant: Flatness Matters for LLM Quantization",
        "authors": "Kang Zhao, Han Bao, Haoli Bai, Yuxuan Sun, lianlio",
        "link": "https://arxiv.org/abs/2410.09426",
        "github_repo": "https://github.com/ruikangliu/FlatQuant",
        "summary": " - FLATQUANT, a novel post-training quantization approach, enhances the flatness of Large Language Model (LLM) weights and activations through fast and learnable affine transformations, improving quantization accuracy and reducing error propagation.\n- FLATQUANT employs a lightweight, block-wise training strategy over calibration data and utilizes Kronecker decomposition for efficient affine transformations, minimizing memory and computational demands.\n- A single kernel fusing affine transformations and quantization reduces transformation overhead, resulting in inference speedups of up to 2.3x for prefill and 1.7x for decoding compared to the FP16 baseline. \n- FLATQUANT achieves state-of-the-art quantization results, including less than 1% accuracy drop for W4A4 quantization on LLaMA-3-70B, outperforming SpinQuant by 7.5%. \n- The method's effectiveness is shown on various LLMs (LLaMA-2/3, 7B to 70B parameters) across tasks like language modeling and question answering, demonstrating superior accuracy and inference latency compared to other state-of-the-art techniques.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/ruikangliu/FlatQuant"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "MedMobile: A mobile-sized language model with expert-level clinical capabilities",
        "authors": "Eric Karl Oermann, Daniel Alexander Alber, Anton Alaykin, Jaden Stryker, KrithikV",
        "link": "https://arxiv.org/abs/2410.09019",
        "github_repo": null,
        "summary": "-\nMedMobile, a fine-tuned 3.8B parameter phi-3-mini language model, demonstrates expert-level clinical reasoning capabilities, achieving a 75.7% accuracy on MedQA (USMLE), surpassing the passing score for physicians and outperforming previous state-of-the-art sub-5B parameter models by over 20%.\n- MedMobile leverages chain-of-thought prompting, ensemble methods, and supervised fine-tuning, with the latter contributing an 8.4% improvement in accuracy. \n- Unlike larger models, techniques such as k-shot prompting and retrieval-augmented generation did not enhance MedMobile's performance, possibly due to context window limitations, leaving potential avenues for future research. \n- This model holds promise for low-resource medical settings and democratizes access to advanced language models beyond large technology companies. \n- The model can be expanded to vision-language tasks by utilizing Phi-3-vision architecture.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/nyuolab/MedMobile"
        ],
        "huggingface_urls": [
            "https://huggingface.co/KrithikV/MedMobile"
        ],
        "date": "2024-10-18"
    },
    {
        "title": "Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation",
        "authors": "Jian Xue, Peidong Wang, Michael Levit, Mohammad Sadegh Rasooli, Sreyan Ghosh",
        "link": "https://arxiv.org/abs/2410.13198",
        "github_repo": null,
        "summary": " - This paper introduces DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach to improve the performance of generative error correction (GEC) models for automatic speech recognition (ASR) systems. \n- DARAG addresses limitations of traditional GEC models by augmenting training data with synthetic examples generated by prompting large language models (LLMs) and text-to-speech (TTS) models, simulating realistic ASR errors. \n- It also incorporates retrieval augmentation, extracting named entities from the training data and retrieving similar entities during correction to handle novel or unknown named entities more effectively.\n- Experimental results on various in-domain and out-of-domain settings show that DARAG consistently outperforms baseline methods, with relative word error rate (WER) improvements of 8%-30% in in-domain and 10%-33% in out-of-domain scenarios.\n-  DARAG improves named entity correction and shows the benefit of using synthetic data in low-resource domain adaptation setting as well.",
        "classification": [
            "Automatic Speech Recognition",
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning",
        "authors": "Chengwei Sun, Ran Ran, Yujia Wu, Jiwei Wei, Shiym",
        "link": "https://arxiv.org/abs/2410.13618",
        "github_repo": "https://github.com/SKDDJ/LoLDU",
        "summary": "\u2022 LoLDU is a novel Parameter-Efficient Fine-Tuning (PEFT) method that leverages Lower-Diag-Upper (LDU) decomposition to reduce the number of trainable parameters during fine-tuning.\n\u2022 LoLDU initializes low-rank matrices with orthogonal properties using LDU decomposition, focusing on optimizing a diagonal matrix for scaling transformations and dynamic adjustment of a scaling factor to align updates with the target matrix.\n\u2022 LoLDU achieves comparable performance to full fine-tuning and other PEFT methods while drastically reducing trainable parameters, sometimes down to 0.00025% of the original model.\n\u2022 Experimental results across various tasks, including instruction following, natural language understanding, image classification, and image generation, with models ranging from 86 million to 7 billion parameters (LLaMA2, RoBERTa, ViT, and Stable Diffusion) demonstrate LoLDU's effectiveness.\n\u2022 LoLDU excels in preserving pre-trained knowledge and enhancing generalization through the use of orthogonal lower and upper triangular matrices, outperforming LoRA on certain tasks while using significantly fewer parameters.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Text-to-Image",
            "Natural Language Processing",
            "Text Generation",
            "Text Classification"
        ],
        "github_urls": [
            "https://github.com/SKDDJ/LoLDU"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "BenTo: Benchmark Task Reduction with In-Context Transferability",
        "authors": "Lichao Sun, Ming Li, Hongyu Zhao, zhoutianyi",
        "link": "https://arxiv.org/abs/2410.13804",
        "github_repo": null,
        "summary": "BENTO: Benchmark Task Reduction with In-Context Transferability\n- This paper introduces a novel benchmark reduction method called BENTO (Benchmark Task Reduction) designed to reduce the evaluation cost of Large Language Models (LLMs). \n- BENTO leverages In-Context Transferability (ICT), a training-free approach to estimate the transferability between different tasks using in-context learning. \n- By analyzing the ICT matrix and applying spectral clustering, BENTO identifies representative tasks that capture the overall benchmark's essence. \n- The paper shows that BENTO can reduce the number of tasks in popular LLM benchmarks like MMLU and FLAN by up to 95% while maintaining evaluation accuracy within a 4% margin of the full benchmark. \n- This method is significantly more efficient than existing benchmark reduction techniques as it doesn't rely on computationally expensive fine-tuning or extensive training data.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/tianyi-lab/bento"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "AERO: Softmax-Only LLMs for Efficient Private Inference",
        "authors": "Brandon Reagen, Nandan Kumar Jha",
        "link": "https://arxiv.org/abs/2410.13060",
        "github_repo": null,
        "summary": "\n- AERO, a four-step architectural optimization framework, refines existing large language models (LLMs) for efficient private inference (PI) by removing nonlinearities and reducing FLOPs.\n- AERO systematically removes nonlinearities such as LayerNorm and GELU, proposes using ReLU in LayerNorm-free models, and designs a Softmax-only architecture tailored for PI.\n- A novel entropy regularization technique mitigates entropic overload, improving the performance of the Softmax-only model.\n- AERO achieves up to a 4.23x reduction in communication overhead and a 1.94x speedup in latency compared to the baseline.\n- Experiments were conducted on GPT-2 and Pythia-70M models, trained from scratch on CodeParrot and Languini datasets, demonstrating improvements across various context sizes and model depths.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
        "authors": "Fujun Luan, Sai Bi, Kai Zhang, Hao Tan, arthurhero",
        "link": "https://arxiv.org/abs/2410.12781",
        "github_repo": null,
        "summary": " \n- Long-LRM, a novel generalizable 3D Gaussian reconstruction model, processes lengthy image sequences for large-scale scene reconstruction.\n- The model architecture combines Mamba2 and transformer blocks for handling long input sequences, and incorporates token merging and Gaussian pruning for efficiency. \n- It reconstructs entire scenes from up to 32 images at 960x540 resolution in 1.3 seconds on a single A100 80G GPU, outperforming existing optimization-based methods in speed.\n- Evaluation on DL3DV-140 and Tanks and Temples demonstrates comparable or better novel view synthesis quality than optimization-based 3D Gaussian Splatting (3D GS), while being significantly faster. \n- The method offers the first feed-forward solution for wide-coverage scene-level Gaussian Splatting reconstruction, enabling large-scale scene reconstruction within seconds.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/arthurhero/projects/tree/main/llrm"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant",
        "authors": "Xiangyu Yue, Yu-Feng Li, Changsheng Li, Jiaming Han, Hoar012",
        "link": "https://arxiv.org/abs/2410.13360",
        "github_repo": "https://github.com/Hoar012/RAP-MLLM",
        "summary": "\n- This paper introduces Retrieval Augmented Personalization (RAP), a framework for personalizing Multimodal Large Language Models (MLLMs) by integrating user-specific visual concepts without requiring further training.\n- RAP employs a key-value database to store user-provided concept information (image, name, description), retrieves relevant information using a multimodal retriever based on user input (image and/or text), and feeds both the query and retrieved information to the MLLM for personalized response generation.\n- A dedicated dataset is created using a pipeline that leverages Gemini to automatically generate personalized captions, descriptions, and question-answer pairs associated with user-provided visual concepts. \n- Experimental results show that RAP-MLLMs, trained on this dataset using LLaVA and Phi-3V backbones, achieve superior performance in personalized image captioning and visual question answering compared to finetuning and other personalization methods, while also performing well on standard multimodal benchmarks like MMMU and InfoSeek.\n- RAP offers real-time concept editing and addition by updating the external database, providing flexibility and eliminating retraining needs, though performance depends on the robustness of the multimodal retriever.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/Hoar012/RAP-MLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization",
        "authors": "Shengpeng Ji, Ziang Zhang, Xize Cheng, Siqi Zheng, Ruiqi Li",
        "link": "https://arxiv.org/abs/2410.12957",
        "github_repo": null,
        "summary": "MuVi is a novel video-to-music generation framework that focuses on semantic alignment and rhythmic synchronization.\n- MuVi employs a non-autoregressive encoder-decoder architecture, using a pre-trained visual encoder and a flow-matching-based music generator.\nA visual adaptor connects the two modules and performs efficient compression of high-frame-rate visual features.\n- A contrastive music-visual pre-training scheme is introduced, utilizing negative samples from temporal shifts and random replacements to enhance rhythmic synchronization.\n- Experimental results demonstrate MuVi's superior performance over existing methods, achieving improvements in audio quality and temporal synchronization in generated music.",
        "classification": [
            "Text-to-Audio",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems",
        "authors": "Isack Lee, hbseong",
        "link": "https://arxiv.org/abs/2410.13334",
        "github_repo": null,
        "summary": " - This paper introduces PCJailbreak, a method to analyze how intentional biases in Large Language Models (LLMs), implemented for safety alignment, can be exploited to generate harmful content. \n- The method involves using LLM-generated keywords representing contrasting demographic groups in prompts containing harmful requests to assess the model's susceptibility to jailbreak attacks. \n- Experiments on various LLMs, including GPT models and open-source alternatives, revealed that intentional biases lead to significant differences in jailbreak success rates between marginalized and privileged groups. \n- The paper also proposes PCDefense, a mitigation strategy that uses prompts to adjust biases without the need for additional inference or models, unlike Guard Models. \n- The authors advocate for responsible development and deployment of LLMs, emphasizing careful consideration of safety measures to avoid unintended vulnerabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation",
        "authors": "Tim Oates, pdx97",
        "link": "https://arxiv.org/abs/2410.13293",
        "github_repo": null,
        "summary": " - This paper introduces SBI-RAG, a Schema-Based Instruction Retrieval-Augmented Generation framework, for enhancing math word problem solving using a Large Language Model (LLM).\n - SBI-RAG uses a schema classifier (trained on DistilBERT) to predict the problem's schema, which guides prompt creation for context retrieval using RAG and generates step-by-step solutions using Ollama Llama 3.1.\n - The authors evaluate SBI-RAG on GSM8K, comparing it with GPT-4 and GPT-3.5 Turbo, using a \"reasoning score\" to assess solution quality.\n - Results suggest SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially improving student learning.\n - The approach incorporates a schema classifier, structured prompt generation, schema-relevant RAG, and a new evaluation metric.",
        "classification": [
            "Question Answering",
            "Text2Text Generation",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "$\u03b3-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models",
        "authors": "Xiaoshuai Sun, Yiyi Zhou, Jiayi Ji, Gen Luo, YaxinLuo",
        "link": "https://arxiv.org/abs/2410.13859",
        "github_repo": null,
        "summary": "\n- This paper introduces $\\gamma$-MoD, a novel mixture-of-depth (MoD) adaptation strategy for enhancing the computational efficiency of existing Multimodal Large Language Models (MLLMs).\n- $\\gamma$-MoD employs a new metric called Rank of Attention Maps (ARank) to identify and replace redundant MLLM layers with MoD layers, dynamically allocating computational resources based on token relevance.\n- Two key designs, shared vision-language router and masked routing learning, are incorporated to maximize sparsity while preserving performance.\n- The shared router applies routing to the entire multimodal sequence for better optimization, and masked routing learning prevents critical tokens from being skipped during training.\n- Experiments on nine benchmarks show that $\\gamma$-MoD notably reduces training and inference time while maintaining competitive performance compared to existing dense and sparse MLLMs.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
        "authors": "Jun Zhu, Peize Sun, Hang Su, ChenDRAG",
        "link": "https://arxiv.org/abs/2410.09347",
        "github_repo": "https://github.com/thu-ml/CCA",
        "summary": "-\nThis paper introduces Condition Contrastive Alignment (CCA), a fine-tuning technique for autoregressive (AR) visual generation models to improve sample quality without relying on guided sampling methods like Classifier-Free Guidance (CFG).\n- CCA fine-tunes pre-trained models by contrasting positive and negative image-condition pairs, directly optimizing the model to achieve the desired target distribution, similar to alignment techniques used in language models.\n- Experimental results on LlamaGen and VAR models demonstrate significant improvement in guidance-free FID and IS scores after just one epoch of fine-tuning with CCA, achieving performance comparable to CFG while reducing sampling costs.\n- CCA offers a controllable trade-off between image diversity and fidelity similar to CFG by adjusting a training hyperparameter (\u03bb), further confirming their theoretical connection in targeting the same sampling distribution.\n- Combining CCA with CFG can lead to further performance gains, showcasing its potential as a complementary technique for enhancing visual generation.",
        "classification": [
            "Text-to-Image",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/thu-ml/CCA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Can MLLMs Understand the Deep Implication Behind Chinese Images?",
        "authors": "Xinrun Du, Yuelin Bai, Xi Feng, zhangysk, MING-ZCH",
        "link": "https://arxiv.org/abs/2410.13854",
        "github_repo": "https://github.com/MING_X/CII-Bench",
        "summary": " - This research introduces CII-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) ability to understand the implications behind Chinese images, including those deeply rooted in Chinese traditional culture.\n- CII-Bench includes 698 images across diverse domains and visual content types, paired with 800 multiple-choice questions to assess comprehension and reasoning abilities.\n- Experimental findings reveal a notable performance gap between MLLMs and humans, with models achieving a maximum accuracy of 64.4% compared to human accuracy averaging 78.2%.\n- A custom evaluation metric is designed using GPT-4 to better evaluate Chinese traditional painting comprehension, revealing model limitations in grasping complex cultural nuances.\n- Models benefit from image emotion hints in prompts, indicating ongoing struggles with emotional understanding crucial for accurate interpretation.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/MING_X/CII-Bench"
        ],
        "huggingface_urls": [
            "https://cii-bench.github.io/"
        ],
        "date": "2024-10-18"
    },
    {
        "title": "Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key",
        "authors": "Yunlin Mao, Jintao Huang, Daoze, wangxingjun778, Yingda",
        "link": "https://arxiv.org/abs/2410.10210",
        "github_repo": null,
        "summary": " - This research introduces a technique for enhancing the long-form output generation capabilities of Large Language Models (LLMs) through minimal tuning with high-quality data.\n- By curating a smaller, higher-quality dataset from the existing LongWriter-6k dataset, and combining it with a small fraction of alignment data, this method demonstrates comparable performance improvements to more compute-intensive training approaches.\n- Notably, the new dataset requires just 3.74% of the original training data, improving tuning efficiency by effectively addressing issues with data quality such as mismatched output lengths and missing instructions in the original data.\n- Evaluations based on length-following score (SL) and writing quality score (SQ) show improvements across various models, including the Qwen and GLM families.\n- This approach provides an efficient method for enhancing long-form output generation while preserving model coherence and alignment.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://www.modelscope.com/models/swift/MS-LongWriter-GLM4-9B-Chat",
            "https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2-7B-Instruct",
            "https://www.modelscope.com/models/swift/MS-LongWriter-Qwen2.5-7B-instruct",
            "https://www.modelscope.com/datasets/ZhipuAI/LongWriter-6k",
            "https://huggingface.co/datasets/THUDM/LongWriter-6k",
            "https://huggingface.co/THUDM/LongWriter-glm4-9b",
            "https://github.com/modelscope/evalscope/tree/main/evalscope/third_party/longbench_write",
            "https://www.modelscope.com/datasets/swift/longwriter-6k-filtered",
            "https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-Chinese",
            "https://huggingface.co/datasets/Magpie-Align/Magpie-Qwen2-Pro-200K-English",
            "https://huggingface.co/THUDM/glm-4-9b"
        ],
        "date": "2024-10-18"
    },
    {
        "title": "TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration",
        "authors": "Yali Wang, Yu Qiao, Kunchang Li, Shaobin Zhuang, markywg",
        "link": "https://arxiv.org/abs/2410.12183",
        "github_repo": null,
        "summary": "TransAgent is a novel framework that transfers knowledge from heterogeneous vision, language, and multimodal agents to enhance the generalization of Vision-Language (V-L) foundation models like CLIP.\n- It leverages 11 different pre-trained agents covering various tasks and modalities, including visual recognition, dense prediction, chatbot, text encoding, multimodal generation, and captioning.\n- The knowledge transfer is achieved through a unified distillation framework, where a Mixture-of-Agents (MoA) gating mechanism adaptively integrates knowledge from different agents.\n- TransAgent achieves state-of-the-art performance on 11 visual recognition datasets, outperforming CoOp by approximately 10% on average and 20% on EuroSAT under the same low-shot setting.\n- All pre-trained agent models can be unloaded after distillation, resulting in efficient deployment with no need for model ensembles in the inference phase.",
        "classification": [
            "Zero-Shot Image Classification",
            "Image Classification",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/markywg/transagent"
        ],
        "huggingface_urls": [],
        "date": "2024-10-18"
    },
    {
        "title": "Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation",
        "authors": "jihoonkim25, Gwanwoo, ktio, kimnamssya, hyungjoochae",
        "link": "https://arxiv.org/abs/2410.13232",
        "github_repo": null,
        "summary": "\u2022 This paper introduces World-Model-Augmented (WMA) web agents, which leverage world models to simulate the outcomes of actions for enhanced decision-making in web navigation.\n\u2022 WMA agents address the limitations of Large Language Models (LLMs) in long-horizon web navigation tasks by incorporating a world model that predicts the effects of actions, enabling the agent to foresee potential outcomes.\n\u2022 The authors propose a transition-focused observation abstraction method to overcome training challenges, where the world model is trained to generate natural language descriptions of state differences between time steps, rather than predicting the entire next observation.\n\u2022 The WMA agent employs a value function to estimate rewards for simulated next observations, guiding the policy model to select optimal actions.\n\u2022 Experimental results on WebArena and Mind2Web show that WMA agents improve policy selection, achieve state-of-the-art performance on Mind2Web, and demonstrate superior cost and time efficiency compared to tree-search-based agents.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/kyle8581/WMA-Agents"
        ],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models",
        "authors": "gychen, jzwangcuhk, BryanW, jiancheng, donghao-zhou",
        "link": "https://arxiv.org/abs/2410.13370",
        "github_repo": null,
        "summary": " - MagicTailor, a novel framework, is introduced to address Component-Controllable Personalization in Text-to-Image (T2I) diffusion models; the model allows modification of specific components of a personalized visual concept within generated images using additional visual references.\n -  It leverages two techniques: Dynamic Masked Degradation (DM-Deg), which perturbs unwanted visual semantics to mitigate semantic pollution, and Dual-Stream Balancing (DS-Bal) to balance the learning of visual semantics and address semantic imbalance.\n- In qualitative comparisons, MagicTailor demonstrates superior performance in generating text-aligned images with accurate concept and component integration compared to existing personalization methods like Textual Inversion, DreamBooth, Custom Diffusion, Break-A-Scene, and CLiC.\n- Quantitative evaluations show that MagicTailor achieves state-of-the-art results in both identity fidelity and text alignment using automatic metrics (CLIP-T, CLIP-I, DINO, and DreamSim) and user studies.\n- MagicTailor's versatility is highlighted through further applications, including decoupled concept and component generation and enhancing other generative tools like ControlNet and InstantMesh, by furnishing these tools with component control capabilities.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://correr-zhou.github.io/MagicTailor"
        ],
        "date": "2024-10-21"
    },
    {
        "title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models",
        "authors": "Yilin Guo, Yan Hu, wittenberg, amstrongzyf, TobyYang7",
        "link": "https://arxiv.org/abs/2410.14059",
        "github_repo": null,
        "summary": "-\nUCFE, a User-Centric Financial Expertise benchmark, is introduced to evaluate LLMs' ability to handle complex, real-world financial tasks using dynamic, task-specific interactions in a hybrid approach combining human and LLM evaluations.\n-\nBased on a user study with 804 participants, a dataset is created that incorporates various user intents and interactions across different user groups, serving as a foundation for benchmarking 12 LLMs using LLM-as-Judge methodology.\n-\nResults demonstrate a strong correlation (Pearson coefficient 0.78) between benchmark scores and human preferences, validating the UCFE dataset and evaluation method.\n-\nMid-sized LLMs (7B-14B parameters), fine-tuned on financial texts, achieve a balance between performance and resource efficiency.\n-\nThe user-centric design highlights the necessity of aligning AI systems with diverse user requirements in finance, setting the stage for enhanced, reliable AI-driven solutions.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/TobyYang7/UCFE-Benchmark"
        ],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples",
        "authors": "Daniel Jiang, Wenxuan Peng, Zhiqiu Lin, Nyandwi, BaiqiL",
        "link": "https://arxiv.org/abs/2410.14669",
        "github_repo": null,
        "summary": "\u2022 NaturalBench is a new benchmark designed for evaluating vision-language models (VLMs) on natural adversarial samples.\n\u2022 These are image-question pairs derived from real-world images and questions, which are easily answered by humans but pose a challenge for current VLMs.\n\u2022 The authors use a semi-automated approach to curate the benchmark, making use of CLIP and ChatGPT to source and filter questions from image caption datasets.\n\u2022 The benchmark comprises 10,000 human-verified question-answer samples, categorized by visual reasoning skill.\n\u2022 Evaluation results of 53 state-of-the-art VLMs demonstrate a significant performance gap compared to humans, suggesting the benchmark's efficacy in revealing areas for improvement.",
        "classification": [
            "Visual Question Answering",
            "Multimodal"
        ],
        "github_urls": [
            "https://linzhiqiu.github.io/papers/naturalbench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
        "authors": "Hayden Kwok-Hay So, Dayou Du, Shijie, CharyZeng, Retromonic",
        "link": "https://arxiv.org/abs/2410.13276",
        "github_repo": null,
        "summary": " - This paper introduces SeerAttention, a novel attention mechanism designed to improve the efficiency and scalability of Large Language Models (LLMs), especially those with long context windows, by learning intrinsic sparse attention rather than using predefined patterns.\n- SeerAttention augments conventional attention with a learnable gate, called Attention Gate (AttnGate), to dynamically select important blocks in an attention map and treat the rest as sparse.\n- It employs a customized FlashAttention kernel to extract the block-level ground truth of attention maps for efficient training of the gating network, minimizing overhead.\n- Evaluations show SeerAttention outperforms existing sparse attention methods in post-training and achieves near-lossless accuracy with high sparsity (up to 90%) during fine-tuning for long context extension using YaRN.\n-  With a block-sparse pattern, the attention kernel achieves up to a 5.67x speedup over the FlashAttention-2 dense baseline on a single A100 GPU.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/microsoft/SeerAttention"
        ],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts",
        "authors": "Yury Chekhovich, Anastasia Voznyuk, German Gritsai, andriygav",
        "link": "https://arxiv.org/abs/2410.14677",
        "github_repo": null,
        "summary": " - This paper presents a systematic review of datasets used in competitions and research papers dedicated to AI-generated content detection and proposes methods for evaluating the quality of such datasets.\n- The authors argue that the high performance of current detectors on benchmark datasets may be due to the poor quality of the evaluation datasets rather than the true effectiveness of the detectors.\n- The authors investigate different metrics, such as detecting low-quality generations with the use of metrics based on topological time series, detecting suspicious activation maps, and detecting sensibility to perturbations, such as text modification and sentence shuffling\n- The paper emphasizes the need for robust and qualitative methods to evaluate generated data to be secure against bias and low generalization ability of future models and provide a more comprehensive understanding of the dynamics between human and machine text.\n- The paper suggests that the use of high-quality generated data can be used for two purposes: enhancing the training of detection models and refining the training datasets themselves.",
        "classification": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion",
        "authors": "Shweta Bhardwaj, Yijun Liang, zhoutianyi",
        "link": "https://arxiv.org/abs/2410.13674",
        "github_repo": "http://github.com/tianyi-lab/DisCL",
        "summary": " - This paper introduces Diffusion Curriculum (DisCL), a novel paradigm for enhancing model performance with low-quality or scarce data by bridging the distribution gap between original and target data using synthetic data.\n- DisCL leverages image guidance in diffusion models to generate a spectrum of interpolated data, ranging from synthetic to real, offering diverse properties for curriculum learning. \n- The approach includes generating synthetic images with varying image guidance strengths and designing curricula to select data according to diversity and feature types for different training stages.\n- DisCL demonstrates significant and robust improvements in long-tail classification and learning from low-quality data, across various base model settings. \n - Experimental results on ImageNet-LT and iWildCam exhibit improvements, highlighting DisCL's effectiveness in challenging scenarios and its ability to improve hard data learning.",
        "classification": [
            "Image Classification",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/tianyi-lab/DisCL"
        ],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation",
        "authors": "Pengfei Hu, Pengcheng Xia, Chenyu Liu, Limin Lin, Hanbo-Cheng",
        "link": "https://arxiv.org/abs/2410.13726",
        "github_repo": "https://github.com/Hanbo-Cheng/DAWN-pytorch",
        "summary": "\n- DAWN (Dynamic frame Avatar With Non-autoregressive diffusion) is a novel framework for generating dynamic-length talking-head videos from a single portrait image and an audio clip in a non-autoregressive manner using a diffusion model.\n- It disentangles lip movements from head pose and blinks, using an Audio-to-Video Flow Diffusion Model (A2V-FDM) for lip-sync and a Pose and Blink generation Network (PBNet) for head and eye movements, improving temporal consistency and extrapolation capabilities.\n- A Two-stage Curriculum Learning (TCL) strategy enhances convergence and extrapolation by first training on short clips with fixed poses for lip motion, then on variable-length sequences with random poses for broader motion control.\n- DAWN achieves state-of-the-art performance on CREMA and HDTF datasets in terms of FID, FVD, lip-sync accuracy, identity preservation, and rhythmic head/blink movements.\n- Its non-autoregressive approach offers significantly faster generation speed compared to previous autoregressive and semi-autoregressive methods, while maintaining high visual quality and realism.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/Hanbo-Cheng/DAWN-pytorch"
        ],
        "huggingface_urls": [
            "https://hanbo-cheng.github.io/DAWN/"
        ],
        "date": "2024-10-21"
    },
    {
        "title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement",
        "authors": "Mengdi Wang, Huazheng Wang, Yue Wu, yokey, huiyuan23",
        "link": "https://arxiv.org/abs/2410.13828",
        "github_repo": null,
        "summary": "\n- This paper identifies a common pitfall in margin-based language model alignment methods used in Reinforcement Learning from Human Feedback (RLHF): the under-specification of ideal behavior on preferred and dispreferred responses. \n- This issue leads to two problems as the margin increases: an increase in unsafe responses, and a decrease in preferred, ideal responses. \n- The underlying cause is identified as the *gradient entanglement* effect, in which margin-based losses couple the preferred and dispreferred probabilities, thus often preventing ideal changes. \n- This effect is characterized by an inner product condition involving the gradients of preferred and dispreferred log-probabilities. \n- The theoretical analysis is empirically validated, and suggests potential mitigation through pairwise normalized gradient descent and sparsity regularized token masking.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/HumainLab/Understand_MarginPO"
        ],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "DPLM-2: A Multimodal Diffusion Protein Language Model",
        "authors": "Shujian Huang, Dongyu Xue, Fei Ye, Zaixiang Zheng, Xinyou Wang",
        "link": "https://arxiv.org/abs/2410.13782",
        "github_repo": null,
        "summary": "\u2022 DPLM-2 is a multimodal protein foundation model based on a discrete diffusion probabilistic framework that models both protein sequences and structures. \n\u2022 DPLM-2 employs a lookup-free quantizer (LFQ) to convert 3D coordinates to discrete tokens, facilitating structure learning within the language model. \n\u2022 It uses an efficient warm-up strategy, leveraging pre-trained sequence-based DPLM and evolutionary data to enhance structural modeling. \n\u2022 DPLM-2 demonstrates competitive performance in co-generation of structure and sequence, achieving high designability and outperforming ESM3-Open and Multiflow in structure-sequence compatibility. \n\u2022 DPLM-2 also shows strong results in various conditional generation tasks like folding, inverse folding, and motif scaffolding and structure-aware representations for predictive tasks.",
        "classification": [
            "Multimodal",
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media",
        "authors": "Mette Thun\u00f8, Rebecca M. M. Hicke, Ross Deans Kristensen-McLachlan, kardosdrur",
        "link": "https://arxiv.org/abs/2410.12791",
        "github_repo": null,
        "summary": "This paper introduces KeyNMF, a novel approach to topic modeling that leverages contextual embeddings and Non-negative Matrix Factorization (NMF).\n- KeyNMF extracts keywords from documents using contextual embeddings and then applies NMF to these embeddings to generate topics. \n- It is evaluated on Chinese news data and demonstrates competitive performance compared to other contextual topic models, especially in terms of external coherence.\n- KeyNMF is integrated with existing methods for analyzing information dynamics to study Chinese diaspora media's coverage of the 2024 European parliamentary elections. \n-  The pipeline identifies trends in novelty and resonance signals that correlate with key political events, demonstrating its effectiveness in capturing information dynamics.\n- The researchers find that KeyNMF enables nuanced analysis of information flow and agenda-setting within Chinese diaspora media during the election period.",
        "classification": [
            "Natural Language Processing",
            "Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ],
        "date": "2024-10-21"
    },
    {
        "title": "How Do Training Methods Influence the Utilization of Vision Models?",
        "authors": "Janis Keuper, Margret Keuper, Shashank Agnihotri, Paul Gavrikov",
        "link": "https://arxiv.org/abs/2410.14470",
        "github_repo": "https://github.com/paulgavrikov/layer_criticality",
        "summary": " - This research paper investigates how different training methods affect the utilization of layers in vision models, specifically focusing on ImageNet-1k classification using ResNet-50.\n-  The study's core finding reveals that training methods significantly influence layer criticality, indicating some layers are more crucial to the model's decisions than others.\n- Adversarial training increases criticality proportionally to the attack budget, while self-supervised learning emphasizes early layers and improved training recipes prioritize early operations.\n-  Contrary to previous findings, no single layer was consistently auxiliary across all training methods. \n- These findings were established by randomizing layer parameters and observing model performance to determine the criticality of each layer.",
        "classification": [
            "Computer Vision",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/paulgavrikov/layer_criticality"
        ],
        "huggingface_urls": [],
        "date": "2024-10-21"
    },
    {
        "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
        "authors": "Hongwei Liu, Maosong Cao, zsytony, KennyUTC, acylam",
        "link": "https://arxiv.org/abs/2410.16256",
        "github_repo": "https://github.com/open-compass/CompassJudger",
        "summary": "-\nCompassJudger-1 is introduced as the first open-source all-in-one judge LLM.\n- It supports unitary scoring, two-model comparisons, formatted evaluations, critique generation and diverse tasks.\n- A new benchmark called JudgerBench is created to evaluate judge models. It includes realistic human annotation from the LLM arena and GPT annotations on subjective benchmarks.\n- Training data for CompassJudger-1 includes several sources, like pair-wise data, critiques and reward data.\n- Several data filtering and sampling strategies are developed for training.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/open-compass/CompassJudger"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree",
        "authors": "lindahua, guoyww, yhcao, yuhangzang, Mar2Ding",
        "link": "https://arxiv.org/abs/2410.16268",
        "github_repo": "https://github.com/Mark12Ding/SAM2Long",
        "summary": "\n- SAM2Long is a training-free method for enhancing the Segment Anything Model 2 (SAM 2) for long video object segmentation. \n- The method introduces a constrained tree memory structure that maintains multiple segmentation pathways, reducing error accumulation and improving robustness against occlusions and object reappearances. \n- An object-aware memory bank selectively stores frames with clear object cues and modulates memory attention calculation based on occlusion scores to prioritize reliable information. \n- SAM2Long consistently outperforms SAM 2 across various model sizes and datasets, showing significant improvements, particularly in long-term and occlusion-heavy scenarios. \n- Experimental results on six video object segmentation benchmarks demonstrate the effectiveness of SAM2Long in handling complex videos and achieving enhanced performance without requiring additional training or parameters.",
        "classification": [
            "Video Classification",
            "Image Segmentation",
            "Object Detection"
        ],
        "github_urls": [
            "https://github.com/Mark12Ding/SAM2Long"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
        "authors": "hsli-cuhk, daijifeng, zengxingyu, gogoduan, LucasFang",
        "link": "https://arxiv.org/abs/2410.13861",
        "github_repo": "https://github.com/rongyaofang/PUMA",
        "summary": " \n- PUMA, a unified multimodal large language model (MLLM), is introduced, featuring multi-granular visual feature processing for diverse visual tasks. \n- The model uses a three-part architecture: a multi-granular image encoder (CLIP), a set of dedicated diffusion-based decoders, and an autoregressive MLLM. \n- PUMA is trained in two stages: Multimodal pretraining on large datasets (Laion-2B, Laion-Aesthetics, GRIT, The Pile, OCR-VQA-200K, LLaVAR) followed by task-specific instruction tuning. \n- The evaluation shows that PUMA excels in diverse text-to-image generation, image editing, conditional image generation, and understanding, outperforming existing unified MLLMs. \n- The multi-granular approach balances diversity and controllability by processing features at multiple levels from coarse-grained abstractions to fine-grained details.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text",
            "Image-to-Image",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/rongyaofang/PUMA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "Baichuan Alignment Technical Report",
        "authors": "dongguosheng, YijieZhou, TJU-Tianpengli, zilchshen, lin5547",
        "link": "https://arxiv.org/abs/2410.14940",
        "github_repo": null,
        "summary": "\u2022 This report introduces Baichuan Alignment, a comprehensive suite of techniques used to align the Baichuan series of large language models (LLMs), including optimization methods, data strategies, and evaluation processes.\n\u2022 Baichuan Alignment consists of three phases: Prompt Augmentation System (PAS) which transforms user queries into actionable instructions, Supervised Fine-Tuning (SFT) which trains LLMs for dialogue and complex tasks, and Preference Alignment which aligns LLMs with human preferences.\n\u2022 The alignment process employs several optimizations such as sample packing and multi-layer gradient checkpointing to increase training efficiency and model merging to improve performance across domains.\n\u2022 Evaluations of Qwen2-Nova-72B and Llama3-PBM-Nova-70B, instruct versions of open-source models optimized with Baichuan Alignment, show significant performance improvements across various benchmarks, outperforming official instruct versions and competing with leading LLMs.\n\u2022 Baichuan-Instruct, an internal model, demonstrates 17% to 28% user experience improvement in core capabilities, highlighting the effectiveness of the proposed alignment techniques.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B"
        ],
        "date": "2024-10-22"
    },
    {
        "title": "AutoTrain: No-code training for state-of-the-art models",
        "authors": "abhishek",
        "link": "https://arxiv.org/abs/2410.15735",
        "github_repo": "https://github.com/huggingface/autotrain-advanced",
        "summary": "AutoTrain (AutoTrain Advanced) is an open-source, no-code tool/library for training and fine-tuning machine learning models on a variety of tasks and modalities.\n- It supports various tasks, including large language model (LLM) fine-tuning, text classification/regression, token classification, sequence-to-sequence tasks, fine-tuning of sentence transformers, visual language model (VLM) fine-tuning, image classification/regression, and tabular data classification/regression.\n- AutoTrain simplifies the training process by providing a user-friendly interface and automating tasks such as dataset processing, hyperparameter tuning, and model validation.\n- It offers flexibility by supporting local and cloud-based training, multiple data formats (zip, CSV, JSONL), and various model architectures compatible with Hugging Face Transformers.\n- AutoTrain is designed for both novice and experienced users, enabling them to build and deploy high-performing models easily.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision",
            "Image Classification",
            "Object Detection",
            "Text Classification",
            "Token Classification",
            "Tabular Classification",
            "Tabular Regression",
            "Text2Text Generation",
            "Text Generation",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/huggingface/autotrain-advanced"
        ],
        "date": "2024-10-22"
    },
    {
        "title": "FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors",
        "authors": "Shih-Han Yen, Chang-Han Yeh, yulunliu, kkennethwu, chinyanglin",
        "link": "https://arxiv.org/abs/2410.16271",
        "github_repo": null,
        "summary": "FrugalNeRF is a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details.\n- It incorporates a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales, guiding training without relying on externally learned priors.\n- This method significantly reduces computational demands and accelerates training through self-adaptive mechanisms.\n- Experiments on LLFF, DTU, and RealEstate-10K show FrugalNeRF outperforms other few-shot NeRF methods in terms of quality and training time.\n- It achieves an optimal balance between rendering quality and training time without relying on complex scheduling for voxels.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://linjohnss.github.io/frugalnerf/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
        "authors": "Rui Min, Yantao Liu, juanli, Nuomei, TranSirius",
        "link": "https://arxiv.org/abs/2410.16184",
        "github_repo": "https://github.com/THU-KEG/RM-Bench",
        "summary": "This paper introduces RM-BENCH, a novel benchmark designed to evaluate reward models for language models.  RM-BENCH focuses on assessing reward models' sensitivity to subtle content differences and resistance to style biases, unlike existing benchmarks.  Extensive experiments show RM-BENCH strongly correlates with policy model performance, making it a reliable tool for selecting effective reward models. Results indicate that current state-of-the-art reward models perform poorly when faced with style bias, showcasing areas for improvement in future model development. The benchmark includes datasets across various domains, including chat, code, math, and safety, with style-controlled variations.",
        "classification": [
            "Natural Language Processing",
            "Text Classification",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/THU-KEG/RM-Bench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages",
        "authors": "Nyandwi, seungone, akariasai, yueqis, yuexiang96",
        "link": "https://arxiv.org/abs/2410.16153",
        "github_repo": null,
        "summary": "This paper introduces PANGEA, a fully open multilingual multimodal large language model (LLM) trained on a diverse 6M instruction dataset spanning 39 languages.  PANGEA significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts, showing comparable performance to state-of-the-art (SOTA) models in English while substantially exceeding them in multilingual scenarios.  The model's architecture is based on LLaVA-Next, using Qwen2-7B-Instruct as the language model backbone.  PANGEA, along with its associated data and code, is fully open-sourced to promote equitable and accessible access to robust multilingual MLLMs.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://neulab.github.io/Pangea/"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/cmarkea/table-vqa",
            "https://huggingface.co/datasets/deepvk/GQA-ru",
            "https://huggingface.co/datasets/cmarkea/doc-vqa",
            "https://huggingface.co/datasets/cmarkea/table-vqa",
            "https://huggingface.co/datasets/BUAADreamer/Chinese-LLaVA-Med-7B",
            "https://huggingface.co/datasets/LinkSoul-AI/Chinese-LLaVA",
            "https://huggingface.co/datasets/Toshi456/LLaVA-Japanese-Instruct",
            "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT"
        ],
        "date": "2024-10-22"
    },
    {
        "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception",
        "authors": "Zhiyuan Ji, jimi888, siminniu, MoCun, Robot2050",
        "link": "https://arxiv.org/abs/2410.12788",
        "github_repo": "https://github.com/IAAR-Shanghai/Meta-Chunking",
        "summary": " - This paper introduces Meta-Chunking, a novel text segmentation technique that leverages Large Language Models (LLMs) to divide documents into logically coherent chunks at a granularity between sentences and paragraphs.\n- Two strategies are proposed: Margin Sampling Chunking, which performs binary classification on consecutive sentences based on probability differences, and Perplexity Chunking, which analyzes perplexity distribution to identify chunk boundaries.\n- A dynamic merging strategy is also introduced to balance fine-grained and coarse-grained chunking, adjusting chunk sizes based on user-specified length requirements.\n- Experimental results across eleven datasets and four benchmarks demonstrate that Meta-Chunking significantly improves single-hop and multi-hop question answering performance in Retrieval-Augmented Generation (RAG) systems.\n- On the 2WikiMultihopQA dataset, for example, Meta-Chunking outperforms similarity chunking by 1.32 in F1 score while requiring only 45.8% of the processing time.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/IAAR-Shanghai/Meta-Chunking"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "Pre-training Distillation for Large Language Models: A Design Space Exploration",
        "authors": "Xin Lv, juanli, NeoZ123, bys0318, Wesleythu",
        "link": "https://arxiv.org/abs/2410.16215",
        "github_repo": null,
        "summary": "-\nThis paper explores pre-training distillation (PD), a method for transferring knowledge from a larger teacher LLM to a smaller student LLM during the pre-training phase.\n- The study investigates four key aspects of PD: logits processing, loss selection, scaling law (model and data size), and the use of offline vs. online logits.\n- A preliminary experiment using GLM-4-9B as the teacher and a 1.9B parameter student model shows a 1.6% average improvement across various datasets with PD.\n- Further experiments reveal that larger student LLMs benefit more from PD, while larger teacher LLMs don't always guarantee better results.\n- The best results are achieved by combining Kullback-Leibler divergence loss with language modeling loss using a Warmup-Stable-Decay scheduling strategy.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation",
        "authors": "Ping Wei, opotle, yegong, shuailu, EurekaWu123",
        "link": "https://arxiv.org/abs/2410.15748",
        "github_repo": null,
        "summary": " - Alchemy, a novel framework, synthesizes formal theorems through symbolic mutations to address the data scarcity challenge in Neural Theorem Proving (NTP).\n- For each candidate theorem, Alchemy identifies invocable theorems from Mathlib and performs mutations by replacing terms with equivalent forms or antecedents.\n- This method significantly increases the number of theorems in Mathlib from 110k to 6M.\n- Continual pretraining and supervised finetuning on this augmented dataset for Large Language Models leads to a 5% absolute performance improvement on the Leandojo benchmark and a 2.5% gain on the miniF2F benchmark.\n- The analysis of synthetic data composition and training paradigms offers valuable insights for developing strong theorem provers.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation",
        "authors": "Wei Ju, Xiao Luo, Shockzipper, XtremSup, luojunyu",
        "link": "https://arxiv.org/abs/2410.14745",
        "github_repo": null,
        "summary": "-\nSemiEvol is a semi-supervised fine-tuning framework designed to improve large language model (LLM) performance in scenarios with limited labeled data and abundant unlabeled data.\n-\nIt employs a bi-level knowledge propagation strategy, transferring knowledge from labeled data to unlabeled data through both model adaptation and context enhancement. \n- For unlabeled data utilization, it involves collaborative learning among multiple LLMs with diverse configurations and adaptive data selection.\n- Experimental results on various datasets, including MMLU, MMLU-Pro, ARC, FPB, USMLE, PubMedQA, and ConvFinQA, demonstrate significant performance improvements compared to SFT and self-evolution methods.\n- SemiEvol effectively utilizes both labeled and unlabeled data, enabling LLMs to adapt to specific scenarios more economically.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/luo-junyu/SemiEvol"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Solshine/reflection-llama-3.1-8B",
            "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B"
        ],
        "date": "2024-10-22"
    },
    {
        "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
        "authors": "GPaolo, albert9000, Xssama, ambroiseodt, abenechehab",
        "link": "https://arxiv.org/abs/2410.11711",
        "github_repo": "https://github.com/abenechehab/dicl",
        "summary": " - This paper introduces Disentangled In-Context Learning (DICL), a novel approach for applying in-context learning (ICL) with large language models (LLMs) to reinforcement learning (RL) environments with continuous state spaces.\n- DICL addresses the challenges of incorporating action information and handling state-action dimension interdependence by projecting the state-action vector into a latent space using Principal Component Analysis (PCA) where features are linearly uncorrelated, then applying ICL.\n- The paper demonstrates the effectiveness of DICL in two RL applications: model-based policy evaluation and data-augmented off-policy RL, showing improved sample efficiency in both cases.\n- A theoretical analysis provides a novel return bound for the policy evaluation algorithm resulting from multi-branch rollouts with the LLM-based dynamics model. \n- Additionally, the paper provides empirical evidence suggesting that LLMs offer well-calibrated uncertainty estimations, a desirable property for model-based RL algorithms.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/abenechehab/dicl"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement",
        "authors": "Yunshui Li, Gang Chen, Haozhe Zhao, Shuzheng Si, kaikai1",
        "link": "https://arxiv.org/abs/2410.15633",
        "github_repo": null,
        "summary": " - This paper introduces GATEAU, a novel framework for selecting influential samples to improve long-context alignment in large language models (LLMs).\n - GATEAU leverages Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM) to identify high-quality samples exhibiting strong long-range dependencies.\n - HMG compares perplexity scores from homologous models with different context windows to assess response generation difficulty, while CAM evaluates whether the model focuses on important input segments.\n - Experimental results show that LLMs trained on samples selected by GATEAU outperform those trained on the full dataset and various baselines across multiple benchmarks, including LongBench, LongBench-Chat, and MT-Bench.\n - Ablation studies demonstrate that both HMG and CAM contribute significantly to GATEAU's effectiveness.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Summarization",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy",
        "authors": "Travis Labrum, wangwilliamyang, xz97, Xianjun, billmianz",
        "link": "https://arxiv.org/abs/2410.13218",
        "github_repo": null,
        "summary": "This paper introduces CBT-BENCH, a new benchmark for evaluating large language models' (LLMs) ability to assist cognitive behavioral therapy (CBT).  CBT-BENCH includes three levels of tasks: basic CBT knowledge acquisition, cognitive model understanding, and therapeutic response generation.  The benchmark uses three new datasets (CBT-QA, CBT-CD, CBT-PC, CBT-FC and CBT-DP) to evaluate the LLMs across these tasks.  The results show that while LLMs perform well on knowledge-based tasks, they struggle with complex tasks that require deep understanding of patient cognitive structures and effective response generation.  This suggests the need for further research into how LLMs can be improved for real-world CBT applications.",
        "classification": [
            "Natural Language Processing",
            "Text Classification",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/mianzhang/CBT-Bench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
        "authors": "anoopk, prajdabre, dipsivenkatesh, safikhan, sumanthd",
        "link": "https://arxiv.org/abs/2410.13394",
        "github_repo": null,
        "summary": "This paper introduces CIA Suite, a framework for cross-lingual auto-evaluation of multilingual LLMs.  The framework includes a novel test set (RECON) with human annotations across six languages and a cross-lingual evaluator LLM (HERCULE). HERCULE leverages English reference answers to evaluate responses in other languages, addressing the scarcity of reference answers in low-resource scenarios. Experiments show HERCULE aligns more closely with human judgments than existing models, exhibiting effectiveness in zero-shot settings.  The CIA suite is publicly available to encourage further research.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text Classification",
            "Zero-Shot Classification"
        ],
        "github_urls": [
            "https://github.com/CIA"
        ],
        "huggingface_urls": [
            "huggingface.co/CIA-Suite"
        ],
        "date": "2024-10-22"
    },
    {
        "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
        "authors": "A K M Mahbubur Rahman, Md Fahim, amanchadha, tasnim, mubtasim",
        "link": "https://arxiv.org/abs/2410.15017",
        "github_repo": "https://github.com/mubtasimahasan/DM-Codec",
        "summary": "DM-Codec is a novel speech tokenizer that leverages a neural codec architecture with Residual Vector Quantization (RVQ) and incorporates two novel distillation approaches: LM-guided and combined LM and SM-guided distillation.\n- The LM-guided approach distills contextual representations from a Language Model (LM) and integrates them with acoustic representations, while the combined approach incorporates semantic representations from a Speech Model (SM) along with contextual and acoustic representations. \n- DM-Codec adopts a streamlined encoder-decoder framework enhanced by a multi-discriminator setup comprising Multi-Scale, Multi-Period, and Multi-Scale Short-Time Fourier Transform discriminators.\n- Experimental results on the LibriSpeech benchmark demonstrate that DM-Codec significantly outperforms state-of-the-art models, reducing Word Error Rate (WER) by up to 13.46%, Word Information Lost (WIL) by 9.82%, and improving speech quality and intelligibility. \n- The combined distillation approach results in a WER of 4.05 and a WIL of 6.61, surpassing existing speech tokenization models.",
        "classification": [
            "Audio",
            "Automatic Speech Recognition",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/mubtasimahasan/DM-Codec"
        ],
        "huggingface_urls": [],
        "date": "2024-10-22"
    },
    {
        "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
        "authors": "lindahua, jiaqiwang-rex, conghui, yhcao, yuhangzang",
        "link": "https://arxiv.org/abs/2410.17247",
        "github_repo": null,
        "summary": " - PyramidDrop is a novel visual redundancy reduction strategy for Large Vision-Language Models (LVLMs) designed to accelerate training and inference.\n- It partitions the LVLM into stages and progressively drops image tokens at each stage's end based on a lightweight similarity calculation with the instruction's last token. \n- This pyramid-like token reduction leverages the observation that token redundancy increases in deeper LVLM layers.\n- Experiments on LLaVA-NeXT-7B show 40% training time and 55% inference FLOPs reduction without performance loss on 15 vision-language tasks. \n- PyramidDrop also allows training with doubled resolution using only 70% of the original training time and serves as a plug-and-play inference acceleration strategy outperforming existing methods.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Document Question Answering"
        ],
        "github_urls": [
            "https://github.com/Cooperx521/PyramidDrop"
        ],
        "huggingface_urls": [],
        "date": "2024-10-23"
    },
    {
        "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
        "authors": "Jie-Ying Lee, Yi-Ruei Liu, Cheng-De Fan, yulunliu, stevenchang",
        "link": "https://arxiv.org/abs/2410.17249",
        "github_repo": null,
        "summary": "-\nSpectroMotion reconstructs dynamic specular scenes by combining 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields. \n- It introduces a residual correction technique for accurate surface normal computation during deformation, a deformable environment map adapting to time-varying lighting, and a coarse-to-fine training strategy. \n- The model outperforms existing methods in view synthesis of dynamic specular scenes. \n- It's the only 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes. \n- Evaluations on NeRF-DS and HyperNeRF datasets demonstrate superior performance in rendering complex, dynamic, and specular content.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-23"
    },
    {
        "title": "Aligning Large Language Models via Self-Steering Optimization",
        "authors": "Jingren, xphan, luyaojie, keminglu, sanmusunrise",
        "link": "https://arxiv.org/abs/2410.17131",
        "github_repo": null,
        "summary": " \n- This paper introduces Self-Steering Optimization (SSO), an algorithm designed for automated alignment of large language models (LLMs), eliminating the need for manual annotation. \n- SSO autonomously generates preference signals by prompting the policy model with contrastive principles and optimizing based on three objectives: steering the model towards chosen responses, maintaining on-policy behavior, and ensuring a consistent quality gap between responses.\n-  Experiments conducted on Qwen2 and Llama3.1 demonstrate SSO's ability to generate accurate and learnable signals, leading to significant performance improvements across various benchmarks without manual annotation or external models.\n- SSO enhanced the training of reward models using data generated during the alignment process, further highlighting its effectiveness.\n- This work contributes a scalable approach to preference optimization for more efficient and effective automated alignment.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/icip-cas/SSO"
        ],
        "huggingface_urls": [],
        "date": "2024-10-23"
    },
    {
        "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
        "authors": "Yuki Imajuku, gneubig, ku21fan, AtsuMiyai, shtapm",
        "link": "https://arxiv.org/abs/2410.17250",
        "github_repo": null,
        "summary": "JMMMU is a new large-scale Japanese benchmark dataset designed to evaluate Large Multimodal Models (LMMs) focusing on Japanese cultural understanding.\n- It comprises two subsets: a Culture-Agnostic (CA) subset, containing translations of culture-independent components from the MMMU benchmark, and a Culture-Specific (CS) subset with newly crafted questions related to Japanese culture.\n- The benchmark is significantly larger than existing culture-aware Japanese benchmarks, totaling 1,320 questions with 1,118 images across a diverse range of 28 subjects.\n- An evaluation of 15 open-source and 3 proprietary LMMs reveals up to 58.6% overall accuracy, indicating significant room for improvement in utilizing the Japanese context.\n- The results indicate that many LMMs perform worse on questions in Japanese compared to their English counterparts and highlight the importance of culture-specific evaluation.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/cyberagent/llava-calm2-siglip",
            "https://huggingface.co/datasets/SakanaAI/JA-Multi-Image-VQA",
            "https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500",
            "https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild"
        ],
        "date": "2024-10-23"
    },
    {
        "title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search",
        "authors": "dalistarh, ekurtic, SpiridonSunRotator, OliverSieberling",
        "link": "https://arxiv.org/abs/2410.14649",
        "github_repo": "https://github.com/IST-DASLab/EvoPress",
        "summary": "\u2022 EvoPress, a new evolutionary search approach for dynamic compression of large language models (LLMs), is introduced, offering provable convergence and low sample and iteration complexity.\n\u2022 EvoPress challenges the assumption of error monotonicity in LLM compression, demonstrating instances where lower per-layer error sums do not translate to better overall performance.\n\u2022 This method improves upon existing layer dropping, unstructured sparsity, and quantization techniques, setting new state-of-the-art results.\n\u2022 It achieves significant improvements, particularly at higher compression ratios, across various LLM families.\n\u2022 EvoPress converges efficiently, often within hours on a single GPU, even for large models, and a lightweight version is available for faster processing.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/IST-DASLab/EvoPress"
        ],
        "huggingface_urls": [],
        "date": "2024-10-23"
    },
    {
        "title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models",
        "authors": "Minlie Huang, Jie Zhou, Hao Zhou, fandong, t1101675",
        "link": "https://arxiv.org/abs/2410.17215",
        "github_repo": "https://github.com/thu-coai/MiniPLM",
        "summary": "\n- MINIPLM is a new Knowledge Distillation (KD) framework for pre-training Language Models (LMs) that refines the training data distribution using a teacher LM's knowledge.\n- It addresses the efficiency, flexibility, and effectiveness challenges of existing KD methods during pre-training through offline teacher inference, corpus-based operation, and a Difference Sampling technique that leverages the discrepancies between large and small LMs.\n- Experiments across various student LM sizes show that MINIPLM improves performance on 9 downstream tasks, language modeling capabilities, and reduces pre-training computation by 2.2 times compared to Vanilla KD which achieves similar performance but with more compute.\n- MINIPLM also supports KD across model families with different tokenizations, unlike existing online KD methods.\n- Further analysis suggests that MINIPLM improves pre-training data utilization, reducing the data demand by 2.4 times.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/thu-coai/MiniPLM"
        ],
        "huggingface_urls": [],
        "date": "2024-10-23"
    },
    {
        "title": "Mitigating Object Hallucination via Concentric Causal Attention",
        "authors": "Shijian Lu, Ivan Laptev, Yiheng Li, xing0047",
        "link": "https://arxiv.org/abs/2410.15926",
        "github_repo": "https://github.com/xing0047/cca-llava",
        "summary": "-\nThis paper introduces Concentric Causal Attention (CCA), a novel position alignment strategy for Large Vision-Language Models (LVLMs) designed to mitigate object hallucination, a phenomenon where LVLMs generate text responses misaligned with image content. \n- CCA addresses the limitations of Rotary Position Encoding (ROPE), commonly used in LVLMs, where long-term decay in attention can lead to hallucination. \n- The method reorganizes visual tokens in a concentric manner, reducing the relative distance between visual and instruction tokens and improving spatial locality. It also introduces a modified causal attention mask to support the 2-D structure of image data. \n- Experimental results on benchmarks like POPE, CHAIR, and MME demonstrate that CCA surpasses existing debiasing methods, improving accuracy and reducing hallucination. \n- CCA also enhances the overall perception capability of LVLMs in multiple-choice visual question answering tasks.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/xing0047/cca-llava.git"
        ],
        "huggingface_urls": [],
        "date": "2024-10-23"
    },
    {
        "title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes",
        "authors": "Thomas Hartvigsen, Jonathan Kropko, Zack Gottesman, Bryan R. Christ",
        "link": "https://arxiv.org/abs/2410.16930",
        "github_repo": null,
        "summary": "\n- MathNeuro is introduced; a method for isolating math-specific parameters in LLMs using forward passes, building upon existing work by calculating parameter importance with weights and activations, but with the key innovation of removing parameters also important for general language tasks measured on non-math datasets.\n- Pruning MathNeuro-identified parameters eliminates a LLM's math reasoning ability, while the impact on other tasks is similar to pruning random parameters.\n- Scaling up MathNeuro-identified parameters by a small constant (1.1 for smaller models and 1.01 for larger models) improves performance on GSM8K by 4-17% without affecting non-math performance.\n- MathNeuro remains effective with a single sample for parameter identification, demonstrating its data efficiency.\n- Math-specific parameters are distributed across the model's decoder blocks, suggesting math reasoning is not localized.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/bryanchrist/MathNeuro"
        ],
        "huggingface_urls": [],
        "date": "2024-10-23"
    },
    {
        "title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models",
        "authors": "conghui, KennyUTC, yhcao, yuhangzang, ziyuliu",
        "link": "https://arxiv.org/abs/2410.17637",
        "github_repo": "https://github.com/Liuziyu77/MIA-DPO",
        "summary": "**- MIA-DPO: a novel Multi-Image Augmented Direct Preference Optimization (DPO) framework, designed to enhance the multi-image understanding of Large Vision-Language Models (LVLMs).**\n**- MIA-DPO addresses the scarcity of diverse multi-image training data and high annotation costs by augmenting existing single-image data with noisy or unrelated images arranged in grid collages or pic-in-pic formats, reducing the need for manual annotation of multi-image data.**\n**- This framework leverages an attention-aware selection mechanism that filters out rejected responses by analyzing the attention value distribution across multiple images, allowing for automated, cost-effective, and scalable DPO data construction without relying on manual annotations or expensive APIs.**\n**- Experimental results demonstrate that MIA-DPO consistently outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on InternLM-XC2.5.**\n**- MIA-DPO maintains competitive performance on single-image tasks while boosting the performance on multi-image tasks, demonstrating its robustness across various architectures and its ability to handle both single and multiple images effectively.**",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/Liuziyu77/MIA-DPO"
        ],
        "huggingface_urls": [],
        "date": "2024-10-24"
    },
    {
        "title": "WorldSimBench: Towards Video Generation Models as World Simulators",
        "authors": "XihuiLiu, JeremyYin, LIJUNLI, Zhoues, CoachXP",
        "link": "https://arxiv.org/abs/2410.18072",
        "github_repo": null,
        "summary": "-\nWorldSimBench is introduced, a new dual evaluation framework for World Simulators, which are video generation models capable of producing actionable videos.\n- The framework consists of Explicit Perceptual Evaluation, assessing visual quality and alignment with human perception using a Human Preference Evaluator trained on a new HF-Embodied dataset with fine-grained human feedback.\n- It also includes Implicit Manipulative Evaluation, measuring the video-action consistency in embodied simulations by converting generated videos into control signals.\n- Tested across Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation scenarios, WorldSimBench reveals strengths and limitations of current World Simulators in generating physically consistent and actionable content.\n- The HF-Embodied Dataset contains 35,701 entries with multi-dimensional human feedback across the three scenarios, enabling both evaluation and broader applications for video generation models.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-24"
    },
    {
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "authors": "Jiacheng Ye, Yizhe Zhang, kiaia, shivamag99, Sansa",
        "link": "https://arxiv.org/abs/2410.17891",
        "github_repo": "https://github.com/HKUNLP/DiffuLLaMA",
        "summary": "\n- This paper introduces a novel approach to scaling Diffusion Language Models (DLMs) by adapting pre-trained autoregressive (AR) language models like GPT2 and LLaMA.\n- The adaptation method bridges the gap between AR and DLM objectives through attention mask annealing to remove causal masking bias and inheriting the shift operation from AR models.\n- The resulting models, DiffuGPT and DiffuLLaMA (up to 7B parameters), are trained on less than 200B tokens and evaluated on various benchmarks, demonstrating competitive performance with their AR counterparts and state-of-the-art results among existing DLMs.\n- DiffuLLaMA showcases promising in-context learning and infilling abilities.\n- The models and training code are released to facilitate further DLM research. \n",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/HKUNLP/DiffuLLaMA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-24"
    },
    {
        "title": "Lightweight Neural App Control",
        "authors": "Jianye Hao, ShaoKun-HW, Fahren24, gpap, semitable",
        "link": "https://arxiv.org/abs/2410.17883",
        "github_repo": null,
        "summary": " - This paper introduces Lightweight Multi-modal App Control (LiMAC), a novel mobile phone control architecture designed for efficient interaction and control across various Android apps.\n - LiMAC combines a small Action Transformer (AcT) with a fine-tuned vision-language model (VLM) to process textual goals and past mobile observations (screenshots, UI trees) and generate precise actions.\n -  AcT predicts action types (click, scroll, input text) and executes straightforward interactions, while the VLM handles complex text generation tasks (composing messages, search queries). \n - Experimental results on two mobile control datasets show LiMAC significantly outperforms fine-tuned open-source VLMs (Florence2, Qwen2-VL) and prompt engineering baselines using GPT-40, increasing overall action accuracy by up to 19% and 42% respectively. \n - LiMAC also executes tasks 30 times faster than GPT-40 methods, making it more suitable for real-time mobile applications.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-24"
    },
    {
        "title": "Scalable Ranked Preference Optimization for Text-to-Image Generation",
        "authors": "Sergey Tulyakov, Zeynep Akata, anilkagak2, hcoskun, shyamgopal",
        "link": "https://arxiv.org/abs/2410.18013",
        "github_repo": null,
        "summary": "\u2022 This research presents a method for cost-effectively fine-tuning Text-to-Image (T2I) models through a novel ranking-based Direct Preference Optimization (DPO) technique, called RankDPO, and a synthetic preference dataset (Syn-Pic), to improve the prompt following and image quality.\n\u2022 Syn-Pic, a synthetically labelled preference dataset, is constructed by generating images from several pre-trained T2I models using the same prompts as an existing human preference dataset (Pick-a-Picv2), then ranked using multiple pre-trained reward models to create rankings per prompt rather than pairwise comparisons.\n\u2022 RankDPO is a ranking-enhanced DPO objective that optimizes the many-way preference between generated images by leveraging the discounted cumulative gains (DCG) from ground truth scores to refine model preferences beyond pairwise preferences.\n\u2022 Experiments on the SDXL and SD3-Medium models demonstrate improved prompt following and visual quality using Syn-Pic and RankDPO, and the results further improve over other DPO variations and existing methods on DPG-Bench, T2I-Compbench, and GenEval.\n\u2022 In addition to outperforming existing methods, RankDPO only requires a fraction of the training time and images compared to other recent techniques like ELLA and reward fine-tuning methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-24"
    },
    {
        "title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
        "authors": "Yu Qiao, Liang Pan, Haozhe Xie, Lingdong Kong, Hengwei Bian",
        "link": "https://arxiv.org/abs/2410.18084",
        "github_repo": null,
        "summary": "DynamicCity is a novel 4D LiDAR generation framework based on Variational Autoencoder (VAE) and Diffusion Transformer (DiT) that generates large-scale, high-quality dynamic LiDAR scenes.\n- The VAE employs a novel Projection Module to compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which shows a 12.56 mIoU gain compared to naive averaging.\n- An Expansion & Squeeze Strategy is used to decode the HexPlane to 3D feature volumes in parallel. It leads to a 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction compared to the baseline method.\n- A padded rollout operation is proposed to make HexPlane feasible for DiT, which reorganizes all six feature planes of the HexPlane into a square 2D feature map.\n- DynamicCity outperforms other state-of-the-art methods on multiple metrics using the CarlaSC and Waymo datasets.",
        "classification": [
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-24"
    },
    {
        "title": "ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding",
        "authors": "Hermann Blum, Marc Pollefeys, Francis Engelmann, Silvan Weder, Guangda Ji",
        "link": "https://arxiv.org/abs/2410.13924",
        "github_repo": null,
        "summary": "-\nThe paper introduces ARKit LabelMaker, a large-scale, real-world 3D dataset with dense semantic annotations generated automatically using an enhanced LabelMaker pipeline.\n- It leverages ARKitScenes and complements it with automatically created dense annotations, addressing the lack of sufficient training data for 3D scene understanding.\n- The pipeline integrates Grounded-SAM and gravity alignment for improved quality and robustness, scaling to large datasets.\n- Experimental results on ScanNet, ScanNet200, and ScanNet++ benchmarks demonstrate that pre-training with ARKit LabelMaker significantly boosts the performance of MinkowskiNet and PointTransformerV3.\n- State-of-the-art performance is achieved on ScanNet and ScanNet200 datasets using ARKit LabelMaker pre-training.",
        "classification": [
            "Image Segmentation",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/cvg/LabelMaker/",
            "https://github.com/quantaji/labelmaker-mix3d",
            "https://github.com/quantaji/LabelMaker-Pointcept"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/labelmaker/arkit_labelmaker"
        ],
        "date": "2024-10-24"
    },
    {
        "title": "MedINST: Meta Dataset of Biomedical Instructions",
        "authors": "Zirui Song, Yu Yin, Zihan Zhang, Meng Fang, Wenhan Han",
        "link": "https://arxiv.org/abs/2410.13458",
        "github_repo": null,
        "summary": "\u2022 This paper introduces MEDINST, a large biomedical instruction meta-dataset comprising 133 tasks and over 7 million training examples spanning 12 distinct categories.\n\u2022 The authors curate MEDINST32, a benchmark derived from MEDINST consisting of 32 tasks with varying difficulty to evaluate large language models' (LLMs) generalization abilities in the biomedical domain. \n\u2022 Several LLMs are fine-tuned on MEDINST and show improved generalization performance across various biomedical tasks, as evaluated on MEDINST32. \n\u2022 The study finds that instruction fine-tuning is more effective than further pre-training on domain-specific data for adapting general LLMs to the biomedical domain.\n\u2022 Experimental results on MEDINST32 reveal that the models often struggle with generalization to new tasks when only fine-tuned on smaller datasets or in limited task formats, highlighting the value of large, comprehensive datasets.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Classification",
            "Token Classification",
            "Summarization",
            "Translation"
        ],
        "github_urls": [
            "https://github.com/aialt/MedINST"
        ],
        "date": "2024-10-24"
    },
    {
        "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
        "authors": "Drishti Sharma, Rishabh Maheshwary, Lester James V. Miranda, shayekh, srishti-hf1110",
        "link": "https://arxiv.org/abs/2410.15522",
        "github_repo": null,
        "summary": "\u2022 This paper introduces M-REWARDBENCH, a multilingual benchmark for evaluating reward models (RMs) across 23 languages and six tasks.\n\u2022 M-REWARDBENCH consists of 2.87k preference instances covering chat, safety, reasoning, and translation capabilities.\n\u2022 Evaluation results show a significant performance gap between English and non-English languages, with RMs exhibiting higher performance in English and variations across different languages.\n\u2022 The analysis indicates that translation quality positively impacts RM performance, with better translations leading to improved accuracy.\n\u2022 The authors also explore the sensitivity of different RM types to translation quality and analyze the performance variations across different language families and scripts.",
        "classification": [
            "Natural Language Processing",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/datasets/C4AI-Community/multilingual-reward-bench"
        ],
        "date": "2024-10-24"
    },
    {
        "title": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts",
        "authors": "Tianhua Li, Yuxuan Xie, kpzhang, wqshao126",
        "link": "https://arxiv.org/abs/2410.18071",
        "github_repo": null,
        "summary": "TP-Eval is a new evaluation framework for Multimodal Large Language Models (MLLMs) that addresses the issue of prompt sensitivity, where minor prompt variations can lead to significant performance fluctuations, resulting in underestimation or bias in evaluation.\n- It introduces a prompt customization method using an automatic prompt optimizer, tailored for MLLMs, to generate optimal prompts for each model, tapping their full potential.\n- This optimizer leverages a scorer, composed of the target MLLM and an answer analyzer, to iteratively refine prompts based on accuracy, semantic similarity to the original prompt, and introspection from incorrect responses.\n- Experiments on MMT-Bench and MMMU datasets demonstrate that TP-Eval effectively reduces underestimation and bias, revealing models' true capabilities and facilitating fairer comparisons.\n- TP-Eval also shows promising results in zero-shot settings using in-context learning, enabling prompt optimization even with limited data.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-24"
    },
    {
        "title": "Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss",
        "authors": "Kehan Li, Hang Zhang, LidongBing, Zhiqiang007, ClownRat",
        "link": "https://arxiv.org/abs/2410.17243",
        "github_repo": null,
        "summary": "-\nInf-CL, a novel tile-based contrastive loss implementation, is introduced to address the GPU memory limitations in scaling batch sizes for contrastive learning. \n- By partitioning the log-sum-exp (LSE) calculation into smaller tiles and iteratively accumulating the LSE term, Inf-CL avoids full materialization of the similarity matrix, significantly reducing memory overhead and enabling training with near-infinite batch sizes.\n- A multi-level tiling strategy further enhances practical efficiency by distributing computations across multiple GPUs with ring-based communication and within each GPU across CUDA cores with fused kernels.\n- Experimental results show that Inf-CL achieves unprecedented batch sizes (e.g., 12M for CLIP-ViT-L/14 on 32 A800 80GB GPUs) without sacrificing accuracy.\n- Compared to state-of-the-art memory-efficient solutions, Inf-CL demonstrates a two-order-of-magnitude reduction in memory while maintaining comparable speed.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [
            "https://github.com/DAMO-NLP-SG/Inf-CLIP"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "LOGO -- Long cOntext aliGnment via efficient preference Optimization",
        "authors": "Min Zhang, Qiaoming Zhu, Zechen Sun, douvleplus, ZetangForward",
        "link": "https://arxiv.org/abs/2410.18533",
        "github_repo": null,
        "summary": "This paper introduces LOGO (Long cOntext aliGnment via efficient preference Optimization), a novel training strategy to enhance the generation ability of Long-Context Models (LCMs) and address issues like hallucinations and instruction unfollowing.\n- LOGO employs reference-free preference optimization, guiding the model to distinguish between preferred and dis-preferred outputs, and a data construction pipeline leveraging open-source models.\n- It incorporates a position synthesis method, enabling training with a substantial 0.3B dataset on a single 8xA800 GPU within 16 hours.\n- Experimental results on LongBench show that LOGO significantly improves LCM performance, outperforming existing methods and approaching top closed-source models like GPT-4.\n- LOGO effectively scales context window size for short-context models and maintains performance on short-context tasks like MMLU, indicating its adaptability and minimal alignment tax.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/ZetangForward/LCM_Stack.git"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/namespace-Pt/long-llm-data"
        ],
        "date": "2024-10-25"
    },
    {
        "title": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch",
        "authors": "Qiaoming Zhu, Xiaobo Liang, douvleplus, XinyuShi, dyyyyyyyy",
        "link": "https://arxiv.org/abs/2410.18693",
        "github_repo": null,
        "summary": "-\nScaleQuest, a novel data synthesis method to generate large-scale question-answer pairs by leveraging \"small-sized\" open-source LLMs.\n-\nThe method uses a two-stage question-tuning process of Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to improve question quality.\n-\nA filtering process for language clarity, solvability, and appropriate difficulty is used along with reward-based filtering for high-quality responses.\n-\nExperiments on a dataset of 1 million math problem-solution pairs show improvements of 29.2% to 46.4% on MATH benchmark across mainstream open-source models, outperforming existing datasets and models like GPT-4-Turbo and Claude 3.5.\n-\nThe data synthesis method also proves to be cost-effective, with 10x reduced cost as compared to GPT-40.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/yyDing1/ScaleQuest"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Can Knowledge Editing Really Correct Hallucinations?",
        "authors": "kaishu666, apayani, XiongxiaoXu, canyuchen, BaixHuang",
        "link": "https://arxiv.org/abs/2410.16251",
        "github_repo": null,
        "summary": " - This paper introduces HalluEditBench, a benchmark designed to evaluate the effectiveness of knowledge editing methods in correcting hallucinations generated by Large Language Models (LLMs).\n- The benchmark includes a new dataset of over 6,000 verified hallucinations across 9 domains and 26 topics, collected from Llama2-7B, Llama3-8B, and Mistral-v0.3-7B.\n-  HalluEditBench assesses knowledge editing methods across five dimensions: Efficacy, Generalization, Portability, Locality, and Robustness, offering a more comprehensive evaluation compared to existing datasets.\n- The evaluation reveals that existing methods struggle with generalization, portability, and robustness, despite showing high performance on standard knowledge editing datasets. For instance, while FT-M and MEMIT achieve near-perfect scores on existing datasets, their efficacy in correcting real-world hallucinations is significantly lower.\n-  ICE and GRACE show superior performance in correcting hallucinations but have limitations in other aspects, particularly robustness, suggesting that the efficacy of current knowledge editing techniques is highly dependent on domains and LLMs and requires further research.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Framer: Interactive Frame Interpolation",
        "authors": "Wen Wang, BiaoGong, Azily, zkcys001, qiuyuu",
        "link": "https://arxiv.org/abs/2410.18978",
        "github_repo": null,
        "summary": "-\"Framer\" is an interactive frame interpolation framework that generates smooth transitions between two images, guided by user-defined point trajectories or an \"autopilot\" mode.\n- Framer employs a large-scale pre-trained image-to-video diffusion model, fine-tuned with both start and end frame conditioning for video interpolation and a control branch for customized point trajectory guidance.\n- The \"autopilot\" mode integrates a novel bi-directional point-tracking method to automatically estimate and refine keypoint trajectories across frames without manual input.\n- Experimental results show that Framer generates higher quality transitions and greater control in challenging scenarios, including image morphing, slow-motion video generation, time-lapse video creation, and cartoon interpolation compared to existing methods.\n- Framer enhances controllability and addresses the ambiguity in transitions using keypoint-based interactions that establish correspondences across frames.",
        "classification": [
            "Computer Vision",
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
        "authors": "flavoredquark, mohitbansal, davejacobs, NealWadhwa, yzli",
        "link": "https://arxiv.org/abs/2410.18975",
        "github_repo": null,
        "summary": "\n- UNBOUNDED, a generative infinite game transcending finite, hard-coded video game systems by integrating generative AI models.\n- It simulates character life in open-ended virtual worlds inspired by sandbox and digital pet games, incorporating unconstrained storytelling of tabletop RPGs.\n- It uses a specialized, distilled LLM for dynamic game mechanics, narrative, character interactions and IP-Adapter for consistent character visuals across environments.\n- Evaluations showed improvement in character simulation, instruction following, narrative coherence, and visual consistency compared to traditional related approaches, as well as real-time interactivity (refreshing every second).\n- It also features a novel regional image prompt adapter that allows consistent and flexible visual generation of character in various environments.",
        "classification": [
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs",
        "authors": "zifeishan, cnxup, zh2001, WooooDyy, hewei2001",
        "link": "https://arxiv.org/abs/2410.18798",
        "github_repo": "https://github.com/hewei2001/ReachQA",
        "summary": " - This paper introduces Code-as-Intermediary Translation (CIT), a method to improve visual reasoning in Multimodal Large Language Models (MLLMs) by using code to translate visual charts into text, which is then used by LLMs to generate and answer complex questions about the charts.\n- The authors construct REACHQA, a dataset with 3k reasoning-intensive charts and 20k question-answer pairs, using CIT and leveraging LLMs for data synthesis.\n- Experiments demonstrate that fine-tuning MLLMs on REACHQA enhances their performance on chart-related benchmarks, improving LLaVA-Next-Llama3-8B by over 30% on average and notably transferring abilities to general mathematical reasoning tasks like MathVista.\n- The study also suggests that expert rationales distilled from stronger LLMs significantly impact reasoning abilities, and the balance between recognition- and reasoning-oriented data influences model performance.\n- This work provides valuable insights into improving and evaluating multimodal reasoning in LLMs through the innovative use of code as an intermediary representation.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/hewei2001/ReachQA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances",
        "authors": "Adams Wai-Kin Kong, Zihan Zhou, Yuanzhi, devSulyvahn, LUSHILIN",
        "link": "https://arxiv.org/abs/2410.18775",
        "github_repo": "https://github.com/Shilin-LU/VINE",
        "summary": " - This paper introduces VINE, a novel invisible watermarking method robust against various image editing techniques, including image regeneration, global and local editing, and image-to-video generation.\n- VINE leverages a pre-trained diffusion model, SDXL-Turbo, as a generative prior for the watermark encoder, improving both image quality and robustness.\n- The method incorporates blurring distortions into the noise layers during training, enhancing robustness by exploiting the observation that image editing primarily affects high-frequency information.\n- Additionally, skip connections and zero-convolution layers are introduced to the VAE of SDXL-Turbo to maintain high fidelity between watermarked and original images.\n- Experimental results on W-Bench, a new comprehensive watermarking benchmark also introduced in this paper, demonstrate that VINE outperforms existing methods in terms of robustness and image quality under various image editing scenarios.",
        "classification": [
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/Shilin-LU/VINE"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Why Does the Effective Context Length of LLMs Fall Short?",
        "authors": "Shansan Gong, Lei Li, Ming Zhong, Jun Zhang, Chenxin An",
        "link": "https://arxiv.org/abs/2410.18745",
        "github_repo": null,
        "summary": "-\nThis paper introduces ShifTed Rotray position embeddING (STRING), a training-free method to improve the effective context length of Large Language Models (LLMs).\n- STRING addresses the issue of left-skewed position frequency distribution in LLMs by shifting well-trained position indices to overwrite less effective ones during inference.\n- This allows LLMs to better capture distant information within their existing training lengths, improving long-range dependency modeling.\n- Experimental results show STRING boosts the performance of LLMs like Llama 3.1 70B and Qwen-2 72B by a significant margin on benchmarks like RULER and InfiniteBench, achieving state-of-the-art results for open-source LLMs.\n- Notably, Llama 3.1 70B with STRING outperforms commercial models like GPT-4-128K and surpasses Claude 2 and Kimi-chat.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/HKUNLP/STRING"
        ],
        "date": "2024-10-25"
    },
    {
        "title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs",
        "authors": "Jujie He, Rui Yan, Jiacai Liu, zengliangcs, chrisliu298",
        "link": "https://arxiv.org/abs/2410.18451",
        "github_repo": null,
        "summary": "-\nThe paper introduces Skywork-Reward, a collection of data-centric methods for enhancing reward modeling in LLMs, along with a new dataset called Skywork-Reward, consisting of 80K curated preference pairs from public sources. \n- Skywork-Reward data collection focuses on important domains for RLHF optimization, such as math and code, using a smaller, higher-quality data composition compared to larger datasets like Preference 700K. \n- The paper details data selection and filtering strategies designed to prioritize pairs that effectively improve model performance, focusing on maximizing the margin between preferred and rejected responses during training. \n- This work also explores various loss functions and finds that the vanilla Bradley-Terry loss consistently outperforms other options. \n- As of October 2024, the resulting Skywork-Reward model series holds the top position on the RewardBench leaderboard, demonstrating the effectiveness of their approach.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/collections/Skywork/skywork-reward-model-66d7fbdebae0e60d00a6b60d",
            "https://huggingface.co/collections/Skywork/skywork-reward-data-collection-66d7fda6a5098dc77035336d"
        ],
        "date": "2024-10-25"
    },
    {
        "title": "MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms",
        "authors": "Lei Zhang, Shunlin Lu, Xuan Ju, Wenxun Dai, Ling-Hao Chen",
        "link": "https://arxiv.org/abs/2410.18977",
        "github_repo": null,
        "summary": "MotionCLR is an attention-based motion diffusion model for interactive human motion generation and editing.  The model architecture is U-Net-like and consists of CLR blocks containing convolutional, self-attention, cross-attention, and feed-forward network layers. MotionCLR supports training-free motion editing including motion (de-)emphasizing, in-place motion replacement, style transfer and example-based motion generation by manipulating self- and cross-attention activations. Experimental results on the HumanML3D dataset demonstrate comparable generation performance to state-of-the-art methods, along with improved explainability and editing control.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "authors": "Zeyu Li, Peijie Dong, Zhenheng Tang, Qi Li, Dominic789654",
        "link": "https://arxiv.org/abs/2410.18785",
        "github_repo": "https://github.com/lqinfdim/EditingEvaluation",
        "summary": " - This paper evaluates the impact of different model editing methods on the general abilities of Large Language Models (LLMs).\n- The study finds that existing editing methods lead to inevitable performance deterioration on general benchmarks, especially when the number of edits increases.\n- The research also reveals that instruction-tuned models are more robust to editing and that larger models are more resistant compared to smaller models.\n- Additionally, the study finds that editing can significantly weaken the safety of LLMs, even for safety-aligned models.\n- The results suggest that current editing methods are only suitable for small-scale knowledge updates, motivating further research on more practical and reliable editing methods.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/lqinfdim/EditingEvaluation"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning",
        "authors": "Han Hu, Yong Luo, Li Shen, Jianyuan Guo, Zhiwei840",
        "link": "https://arxiv.org/abs/2410.17779",
        "github_repo": "https://github.com/Hao840/ADEM-VL",
        "summary": "-\nADEM-VL is an efficient vision-language (VL) tuning framework based on pre-trained large language models (LLMs) that uses a parameter-free cross-attention mechanism for multimodal fusion.\n- This approach reduces trainable parameters and improves training and inference speed by embedding visual features into the language space and utilizing multiscale visual feature generation.\n- An adaptive fusion scheme dynamically discards less relevant visual information based on attention scores, allowing the model to concentrate on more pertinent visual features.\n- The model outperforms existing methods on ScienceQA by 0.77% with average accuracy and demonstrates comparable performance on image captioning tasks.\n- The framework suggests more efficient VL model development by utilizing intermediate-layer fusion.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/Hao840/ADEM-VL"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models",
        "authors": "Xiaofeng Shi, Hanyu Zhao, Chengwei Wu, Bo-Wen Zhang, ldwang",
        "link": "https://arxiv.org/abs/2410.18505",
        "github_repo": null,
        "summary": " \n- This paper introduces CCI3.0-HQ, a 500GB high-quality subset of the Chinese Corpora Internet 3.0 (CCI3.0) designed for pre-training large language models (LLMs).\n- The dataset was created using a novel two-stage hybrid filtering approach: 1. Fundamental Processing(safety filtering, text extraction, deduplication, and initial quality assessment) 2. High-Quality Processing (employs Qwen2-72B-Instruct to identify high-quality samples and train a smaller 0.5B classifier to filter the dataset).\n- A 0.5B parameter model trained from scratch on CCI3.0-HQ using 100B tokens achieved superior performance on 10 benchmarks compared to CCI3.0, SkyPile, and WanjuanV1 in zero-shot settings. \n- The introduced quality classifier (CCI3-HQ) also outperforms existing classifiers like FineWeb-edu, IndustryCorpus2, and ChineseWebText in terms of F1 score.\n- The dataset and the classifier are open-sourced to benefit the community in developing high-quality Chinese LLMs.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/FlagAI-Open/FlagAI/tree/master/examples/CCI3-HQ"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/BAAI/CCI3-HQ",
            "https://huggingface.co/datasets/BAAI/CCI3-Data",
            "https://huggingface.co/BAAI/CCI3-HQ-Classifier"
        ],
        "date": "2024-10-25"
    },
    {
        "title": "CAMEL-Bench: A Comprehensive Arabic LMM Benchmark",
        "authors": "Ines Riahi, Ali Alharthi, Omkar Thawakar, Sara Ghaboura, ahmedheakl",
        "link": "https://arxiv.org/abs/2410.18976",
        "github_repo": null,
        "summary": "CAMEL-Bench is a comprehensive Arabic LMM benchmark comprising eight diverse domains and 38 sub-domains, including multi-image understanding, complex visual perception, and video understanding.\n- It contains around 29,036 questions filtered from a larger pool of samples, and quality is manually verified by native speakers.\n- Evaluations of both closed-source, including GPT-4 series, and open-source LMMs were conducted.\n- GPT-4o achieved an overall score of 62%, revealing a need for improvement, especially among the best open-source models.\n- Closed-source models generally outperformed open-source models in most tests.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "WAFFLE: Multi-Modal Model for Automated Front-End Development",
        "authors": "Lin Tan, Shangshu Qian, jiang719, shanchao",
        "link": "https://arxiv.org/abs/2410.18362",
        "github_repo": null,
        "summary": "-\nWAFFLE is a new fine-tuning strategy for Multi-modal Large Language Models (MLLMs) designed to automate front-end development by generating HTML code from UI design images.\n- It incorporates a structure-aware attention mechanism, enabling MLLMs to better understand HTML structure and a contrastive fine-tuning approach to align the visual understanding of UI designs with the generated HTML code.\n- The evaluation on WebSight-Test shows improvements of up to +9.00 percentage points in HTML Match, +0.0982 in CW-SSIM, +32.99 in CLIP, and +27.12 percentage points in LLEM.\n- Similar improvements are observed on Design2Code, another benchmark, demonstrating WAFFLE's effectiveness in bridging the gap between visual UI designs and text-based HTML/CSS code.\n- WAFFLE, as a fine-tuning method, is model-agnostic and therefore applicable to any MLLMs.",
        "classification": [
            "Multimodal",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/lt-asset/Waffle"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Language Models are Symbolic Learners in Arithmetic",
        "authors": "Hanjie Chen, Ruidi Chang, Roy Xie, Zhiqi Li, Chunyuan Deng",
        "link": "https://arxiv.org/abs/2410.15580",
        "github_repo": null,
        "summary": "This paper investigates how Large Language Models (LLMs) learn arithmetic, specifically focusing on whether they leverage partial products during calculations and how they approach the task symbolically.\n- It finds that LLMs struggle to leverage partial products to solve multiplications and suggests that improvements in recognizing them arise from their symbol-learning process, not actual partial product calculation.\n- By decomposing arithmetic tasks into subgroups based on token-level analysis, the paper finds that LLMs treat a collection of different arithmetic operations similarly when subgroup complexity is fixed.\n- Through position-level accuracy analysis, it's observed that LLM learning follows a U-shaped curve, initially and finally performing well on easy patterns but struggling with harder patterns in between.\n- Overall, the study concludes that LLMs do not perform true calculations during arithmetic tasks. Rather, they act as symbolic learners by selecting subgroups based on complexity, which provides a novel framework for understanding these models' approach to arithmetic reasoning.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Stable Consistency Tuning: Understanding and Improving Consistency Models",
        "authors": "Hongsheng Li, Gsunshine, wangfuyun",
        "link": "https://arxiv.org/abs/2410.18958",
        "github_repo": "https://github.com/G-U-N/Stable-Consistency-Tuning",
        "summary": "-\nStable Consistency Tuning (SCT) is introduced, a novel method for training consistency models.\n-\nSCT builds upon Easy Consistency Tuning (ECT) and incorporates variance-reduced learning using the score identity to improve training stability and performance.\n-\nThe paper introduces a novel framework for understanding consistency models by modeling the denoising process of diffusion models as a Markov Decision Process (MDP) and framing consistency model training as value estimation through Temporal Difference (TD) Learning.\n-\nSCT achieves state-of-the-art results on ImageNet-64, achieving 1-step FID 2.42 and 2-step FID 1.55 for consistency models.\n-\nThe method extends ECT to multistep settings, allowing for deterministic multistep sampling, and explores classifier-free guidance in consistency models.",
        "classification": [
            "Unconditional Image Generation",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/G-U-N/Stable-Consistency-Tuning"
        ],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "Taipan: Efficient and Expressive State Space Language Models with Selective Attention",
        "authors": "Hanieh Deilamsalehy, Ruiyi Zhang, Thang M. Pham, Huy Huu Nguyen, chiennv",
        "link": "https://arxiv.org/abs/2410.18572",
        "github_repo": null,
        "summary": " - Taipan, a hybrid architecture for efficient long-context language modeling, combines the efficiency of Mamba-2 with Selective Attention Layers (SALs) to enhance long-range dependency handling.\n- SALs strategically select tokens requiring long-range interactions, refine their features, and augment them with attention, balancing efficiency and expressiveness.\n- Taipan scales to billions of parameters and demonstrates superior performance on various tasks, including zero-shot language modeling and memory-intensive tasks like in-context retrieval.\n- It achieves linear memory scaling, making it applicable for contexts up to 1 million tokens, and significantly outperforms Transformers and other SSM-based models on long sequences.\n- The ablation study emphasizes the importance of the attention budget and the absence of positional embeddings for efficient and enhanced generalization.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus"
        ],
        "date": "2024-10-25"
    },
    {
        "title": "Value Residual Learning For Alleviating Attention Concentration In Transformers",
        "authors": "Zhenzhong Lan, Zhiyun Jiang, Tianyi Wu, Zcchill",
        "link": "https://arxiv.org/abs/2410.17897",
        "github_repo": null,
        "summary": "\u2022 This paper introduces two novel Transformer variants: ResFormer and SVFormer.\n\u2022 ResFormer incorporates a residual connection from the first layer\u2019s value embeddings to subsequent layers\u2019 value embeddings to mitigate attention concentration, which is defined as a model\u2019s attention increasingly focuses on fewer tokens as the network depth increases.\n\u2022 SVFormer shares the first layer\u2019s value embeddings across all layers, reducing KV cache by approximately 50%.\n\u2022 Experimental results on a 20B SlimPajama dataset show ResFormer outperforms vanilla Transformer, DenseFormer, and NeuTRENO in training and downstream tasks.\n\u2022 SVFormer is shown to train faster than vanilla Transformer and perform better than GQA and CLA when sequence length is longer.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Zcchill/Value-Residual-Learning"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/cerebras/SlimPajama-627B"
        ],
        "date": "2024-10-25"
    },
    {
        "title": "Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits",
        "authors": "Roland Memisevic, Arash Behboodi, Hassan Dbouk, Ashish Khisti, mamaj92",
        "link": "https://arxiv.org/abs/2410.18234",
        "github_repo": null,
        "summary": "This paper introduces a canonical architecture for multi-draft speculative sampling, where multiple draft models independently generate proposal token sequences.\n- It demonstrates that the optimal draft selection scheme can be achieved through a two-step process: importance sampling to select an intermediate token and single-draft speculative sampling on the selected token.\n- For two identical draft models, an analytical expression for optimal acceptance probability is derived, along with a necessary and sufficient condition for achieving an acceptance probability of 1.\n- A new token selection scheme based on weighted importance sampling is proposed, along with heuristic approaches to reduce computational complexity.\n- Experimental results on OPT models across various tasks show consistent improvements in block efficiency and token rates compared to baseline methods, especially when using non-identical draft distributions.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-25"
    },
    {
        "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
        "authors": "Xiaojian Ma, Zhancun Mu, Zihao Wang, kevinLian, phython96",
        "link": "https://arxiv.org/abs/2410.17856",
        "github_repo": null,
        "summary": "ROCKET-1 is a novel, low-level policy that leverages visual-temporal context prompting, a communication protocol using object segmentation masks and interaction types from past and present observations to guide policy-environment interactions.\nROCKET-1 uses a causal transformer architecture that processes observations and object segmentations concatenated into a 4-channel image along with interaction types as conditions.\nExperiments in Minecraft demonstrate that agents using this approach achieve higher success rates on complex tasks, outperforming methods based on language, future image, or latent code prompting.\nA backward trajectory relabeling method efficiently generates segmentation annotations, enabling automatic dataset creation for training ROCKET-1.\nThe approach allows for spatial understanding in embodied decision-making, leading to agents accomplishing previously unattainable tasks like \u201cplace oak door on diamond block\u201d with a 91% success rate and others requiring long-horizon planning such as obtaining obsidian with a 70% success rate.",
        "classification": [
            "Reinforcement Learning",
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Continuous Speech Synthesis using per-token Latent Diffusion",
        "authors": "Hagai Aronowitz, Slava Shechtman, Arnon Turetzky, Avihu, NimrodShabtay1986",
        "link": "https://arxiv.org/abs/2410.16048",
        "github_repo": null,
        "summary": " - This paper introduces SALAD, a per-token latent diffusion model for zero-shot text-to-speech that operates on continuous representations, inspired by the per-token diffusion head for image generation.\n- It extends the image generation method to handle variable-length outputs, uses semantic tokens for context and stopping conditions, and doesn't require text-audio alignment.\n- Three SALAD variants are proposed: T2A (Text2Acoustic), S2A-AR (Semantic2Acoustic Autoregressive), and S2A-NAR (Semantic2Acoustic Non-Autoregressive), along with corresponding discrete baseline models for comparison.\n-  Evaluations on speech quality, intelligibility, and speaker similarity show that SALAD's T2A model achieves the highest intelligibility score.\n-  It also maintains speech quality and speaker similarity comparable to ground-truth audio based on subjective listening tests.",
        "classification": [
            "Text-to-Speech",
            "Audio"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
        "authors": "Jialing Zhang, Shuhao Gu, ZacLiu, bowen92, ldwang",
        "link": "https://arxiv.org/abs/2410.18558",
        "github_repo": null,
        "summary": "{- Introduced Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through quality filtering and deduplication. \n- Proposed a synthetic instruction generation method using open-source VLMs, detailed image annotations, and diverse question generation to improve data quality and scale. \n- Trained Aquila-VL-2B, a 2-billion parameter VLM based on the LLaVA-OneVision architecture, using Infinity-MM and synthetic data. \n- Aquila-VL-2B achieved state-of-the-art performance for models of similar scale on various visual benchmarks, including MMBench, MMStar, and MathVista. \n- Demonstrated that scaling instruction data and generating synthetic data can significantly improve the performance of open-source multimodal models.}",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main/scripts/train"
        ],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
        "authors": "Ping Zhang, Xiang Yue, Yuelin Bai, Ruoqi Liu",
        "link": "https://arxiv.org/abs/2410.19008",
        "github_repo": null,
        "summary": "-\nThis paper introduces PULSE, a new Multimodal Large Language Model (MLLM) tailored for electrocardiogram (ECG) image comprehension. \n-\nIt also presents ECGInstruct, a new instruction tuning dataset of over one million ECG image-text samples featuring realistic image synthesis and a diverse range of ECG-related tasks.  \n-\nA new evaluation benchmark, ECGBench, covering four key ECG image interpretation tasks across nine different datasets is also constructed. \n-\nPULSE achieves state-of-the-art results, significantly outperforming proprietary MLLMs such as GPT-40 by 15-30% accuracy on out-of-domain datasets.\n-\nAblation studies highlight the importance of diverse data sources and incorporating instruction tasks for ECG image comprehension.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Image-to-Text",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://aimedlab.github.io/PULSE/"
        ],
        "date": "2024-10-28"
    },
    {
        "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
        "authors": "Yu Qiao, Zhenyu Yang, Junhao Song, Chenyang Si, Zhengyao Lv",
        "link": "https://arxiv.org/abs/2410.19355",
        "github_repo": null,
        "summary": "\u2022 FasterCache is a training-free strategy designed to accelerate video diffusion model inference.\n\u2022 It uses a dynamic feature reuse strategy for attention modules, adjusting reused features across timesteps to balance detail and temporal consistency.\n\u2022 It introduces CFG-Cache, storing residuals between conditional and unconditional outputs to speed up classifier-free guidance.\n\u2022 Evaluation on models like Vchitect-2.0 shows significant speedups (e.g., 1.67x) while maintaining comparable video quality to the baseline.\n\u2022 It outperforms existing methods in both inference speed and video quality.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [
            "https://github.com/Vchitect/FasterCache"
        ],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
        "authors": "Ramaneswaran Selvakumar, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, S Sakshi",
        "link": "https://arxiv.org/abs/2410.19168",
        "github_repo": null,
        "summary": "-\nMMAU, a massive multi-task audio understanding and reasoning benchmark, is introduced to evaluate expert-level reasoning and knowledge retrieval abilities in Large Audio-Language Models (LALMs). \n- It consists of 10,000 expertly annotated audio-question-response pairs across speech, sound, and music domains, covering 27 distinct tasks, including 16 for reasoning and 11 for information extraction. \n- Evaluations of 18 open-source and proprietary LALMs reveal that even the best-performing model only achieves 53% accuracy on MMAU, with human performance at 82%, highlighting significant room for improvement.\n- Models performed best on sound-based tasks but struggled the most with music. Cascaded models employing audio captioning followed by an LLM achieved the best performance, suggesting the potential for independent advancements in audio perception and text-based reasoning.\n- A detailed error analysis highlights perceptual errors as the most common mistake, emphasizing the need for better audio processing capabilities in current models.",
        "classification": [
            "Audio",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Counting Ability of Large Language Models and Impact of Tokenization",
        "authors": "Chenyu You, Juntai Cao, Wyattz23",
        "link": "https://arxiv.org/abs/2410.19730",
        "github_repo": null,
        "summary": "This paper investigates the impact of tokenization on the counting abilities of Large Language Models (LLMs), demonstrating that tokenization choices significantly influence model performance on counting tasks.\n- The study adopts a model-agnostic approach, manipulating input string formats to control tokenization in both open and closed-source LLMs.\n- It is observed that byte-pair encoding (BPE), commonly used in LLMs, can severely degrade counting accuracy due to a mismatch between the unit being counted (letters) and the unit processed (tokens).\n- The research reveals that Chain-of-Thought (CoT) prompting significantly improves counting abilities by enabling iterative inductive reasoning in the text space, partially overcoming the inherent limitations of Transformer models in sequential computations.\n- Through extensive experiments, the study finds that clear item-separated tokenization, as opposed to letter-grouped tokenization, enhances counting accuracy. Furthermore, the experiments showed that lower-frequency characters are easier to count compared to higher-frequency ones.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning",
        "authors": "Yang Zhang, Tommi Jaakkola, code-terminator, yujianll",
        "link": "https://arxiv.org/abs/2410.19290",
        "github_repo": "https://github.com/UCSB-NLP-Chang/Prereq_tune.git",
        "summary": "-\nPREREQ-TUNE, a novel fine-tuning strategy designed to mitigate LLM hallucinations, is introduced.\n-\nPREREQ-TUNE incorporates a two-stage process: a prerequisite learning stage where a knowledge LoRA is trained to acquire necessary knowledge, followed by a supervised fine-tuning (SFT) stage where a skill LoRA focuses solely on learning task-specific skills.  The prerequisite learning stage enhances factuality by equipping the LLM with the required knowledge for subsequent fine-tuning, thereby reducing reliance on generating incorrect information.\n-\nThe method also utilizes fictitious synthetic data for multi-version training, further improving the grounding of LLM outputs to internal knowledge.  This decoupling of knowledge and skill learning allows for more robust factual generation and control.\n-\nExperiments on long-form generation (biography and medical QA) and short QA tasks demonstrate PREREQ-TUNE's superior performance compared to baselines, including those utilizing reinforcement learning and direct preference optimization.\n-\nAnalysis confirms the effectiveness of PREREQ-TUNE's disentanglement mechanism, even when trained solely on fictitious data, opening possibilities for new retrieval augmented generation (RAG) paradigms and knowledge-controlled text generation.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/UCSB-NLP-Chang/Prereq_tune.git"
        ],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
        "authors": "Valentina Pyatkin, Sachin Kumar, Yanai Elazar, Yizhong Wang, ljvmiranda921",
        "link": "https://arxiv.org/abs/2410.19133",
        "github_repo": "https://github.com/allenai/hybrid-preferences",
        "summary": " - This paper introduces a routing framework for preference learning that dynamically allocates instances to either human or LM annotators, creating a hybrid approach to data annotation.\n- The framework employs a performance prediction model (PPM) to estimate the performance of reward models trained on different mixes of human and LM annotations and uses this to strategically select an optimal combination.\n- Results on the MULTIPREF dataset and others show that the proposed hybrid preference approach significantly outperforms using either human or LM preferences exclusively, as well as random combinations, across several benchmarks.\n- Analysis of the framework highlights that instances with moderate semantic similarity, safety concerns, or intent complexity tend to benefit the most from human annotation.\n- The authors release the code, data, and annotation platform used to promote further research in efficient and effective preference data collection.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/allenai/hybrid-preferences"
        ],
        "huggingface_urls": [
            "https://hf.co/datasets/allenai/multipref"
        ],
        "date": "2024-10-28"
    },
    {
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
        "authors": "Sergey Levine, Kevin Frans, Qiyang Li, Max Wilcoxson",
        "link": "https://arxiv.org/abs/2410.18076",
        "github_repo": "https://github.com/rail-berkeley/supe",
        "summary": "\u2022 SUPE (Skills from Unlabeled Prior data for Exploration) leverages unlabeled prior trajectory data in two ways: offline for skill pretraining with a trajectory-segment VAE and online for training a high-level off-policy RL agent to compose these skills for efficient exploration.\n\u2022 SUPE uses an optimistic reward model to pseudo-label past trajectories, enabling their use as off-policy data for fast learning with limited online interactions.\n\u2022 The method outperforms prior approaches in a suite of long-horizon, sparse-reward tasks, including AntMaze, Kitchen, and Visual AntMaze, demonstrating faster learning and more efficient exploration by finding sparse reward signals more quickly.\n\u2022 Empirical evaluations show that leveraging unlabeled data during both offline and online phases is crucial for efficient exploration, with SUPE successfully solving tasks where other methods struggle.\n\u2022 Ablation studies confirm the benefits of both skill pretraining and online use of offline data, and demonstrate that SUPE is robust to data corruption scenarios like insufficient coverage and limited data.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/rail-berkeley/supe"
        ],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling",
        "authors": "Yunzhu Li, Kaifeng Zhang, MingtongZ",
        "link": "https://arxiv.org/abs/2410.18912",
        "github_repo": null,
        "summary": "-\nThis research introduces a new framework for learning object dynamics and generating 3D action-conditioned video predictions by combining dynamic 3D reconstruction with a graph-based neural dynamics model trained on multi-view RGB videos of robot-object interactions.\n- The approach utilizes 3D Gaussian Splatting (3DGS) to represent and track objects as particles, trains a Graph Neural Network (GNN) on these particles to model their dynamics under different robot actions, and employs an interpolation scheme to predict dense Gaussian motion for video prediction.\n- In comparison to other state-of-the-art methods, experimental results on various deformable objects, such as ropes, cloths, and toys, showed significant improvements in motion prediction accuracy and video prediction quality. \n- Additionally, the integration of this model within a Model Predictive Control framework showcased its efficacy in model-based planning for object manipulation tasks.\n- A key strength of this method is its ability to learn directly from real-world video data, potentially bridging the gap between simulation and real-world performance, despite the inherent limitations in dataset acquisition and handling complex real-world scenarios like significant occlusions or textureless objects.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Video Classification",
            "Image-to-Video",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Reflection-Bench: probing AI intelligence with reflection",
        "authors": "Yan Teng, Shuqi Kong, Haiquan Zhao, Yixu Wang, LingyuLi",
        "link": "https://arxiv.org/abs/2410.16270",
        "github_repo": "https://github.com/YabYum/ReflectionBench",
        "summary": "This paper introduces Reflection-Bench, a new benchmark designed to evaluate the reflection capabilities of Large Language Models (LLMs).\n- Reflection is defined as the ability of an intelligent system to adapt its beliefs or behaviors in response to unexpected outcomes, encompassing core cognitive functions such as perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection.\n- The benchmark comprises seven tasks adapted from cognitive science paradigms, including the oddball paradigm, n-back task, probabilistic reversal learning task, Wisconsin card sorting test, weather prediction task, double-choice Iowa gambling task, and meta-bandit task.\n- An evaluation of 13 prominent LLMs reveals that current models still fall short of human-level reflection abilities, particularly lacking meta-reflection capabilities. \n- The authors argue that reflection is a crucial aspect of intelligence and propose Reflection-Bench as a valuable tool for evaluating and furthering the development of more sophisticated AI systems.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/YabYum/ReflectionBench"
        ],
        "huggingface_urls": [],
        "date": "2024-10-28"
    },
    {
        "title": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation",
        "authors": "Remek, adgw, djstrong, lflis, chrisociepa",
        "link": "https://arxiv.org/abs/2410.18565",
        "github_repo": null,
        "summary": "- This paper introduces Bielik 7B v0.1, a 7-billion parameter generative text model based on the Mistral 7B v0.1 architecture and trained on a curated Polish corpora.\n- The model utilizes techniques such as Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, and incorporates architectural features like Sliding Window Attention and SwiGLU activation function for enhanced performance.\n- To evaluate the model, new benchmark frameworks, the Open PL LLM Leaderboard and Polish MT-Bench, were created for assessing NLP tasks and conversational abilities.\n- Bielik 7B v0.1 showed a significant improvement of 9 percentage points in the RAG Reader task compared to Mistral-7B-v0.1.\n- In subjective conversational evaluations, Bielik outperformed models with higher average scores on the Open PL LLM Leaderboard benchmarks.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/spaces/speakleash/mt-bench-pl",
            "https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard",
            "https://huggingface.co/datasets/teknium/OpenHermes-2.5",
            "https://huggingface.co/spaces/speakleash/open_pl_llm_leaderboard"
        ],
        "date": "2024-10-29"
    },
    {
        "title": "AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant",
        "authors": "Fangzhi Xu, Qiushi Sun, Zhuohang Dang, Minnan Luo, Chengyou Jia",
        "link": "https://arxiv.org/abs/2410.18603",
        "github_repo": null,
        "summary": "- AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents, has been introduced for automating diverse computer tasks.\n- It leverages a novel MetaAgent with an AgentToken strategy, enabling efficient management of diverse agents by representing each agent as a learnable token embedding and predicting the appropriate token(s) for task execution.\n- AgentStore allows for seamless third-party agent integration, enabling adaptability to evolving operating systems.\n- Evaluation on OSWorld and a mobile environment demonstrate its ability to improve performance in automating computer tasks, achieving a success rate of 23.85% on OSWorld\u2014more than double the previous best (11.21%).\n- AgentStore's ability to integrate agents and specialize them for specific tasks while maintaining general capabilities demonstrates significant improvement over single generalist or specialized agents in handling complex tasks within real-world OS environments.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "GPT-4o System Card",
        "authors": "Adam Perelman, Adam P. Goucher, Adam Lerer, Aaron Hurst, OpenAI",
        "link": "https://arxiv.org/abs/2410.21276",
        "github_repo": null,
        "summary": " - OpenAI's GPT-40 is an \"omni\" autoregressive model that accepts and generates combinations of text, audio, image, and video, trained end-to-end across these modalities.\n- GPT-40 matches GPT-4 Turbo's performance on English text and code, surpasses it in non-English languages, and demonstrates significant improvement on vision and audio understanding.\n- The model's training data includes publicly available data, code and math data, multimodal data (images, audio, and video), and proprietary data from partnerships, with a cutoff date of October 2023.\n- Prior to deployment, OpenAI performed risk assessments and mitigations with methods including safety classifiers, content filtering, and preference alignment to reduce harms such as information hazards, bias, and policy violations.\n- Deployment preparation encompassed a four-phased external red teaming process with over 100 participants to evaluate risks and test mitigations across multiple modalities and potential harms such as disallowed content and misinformation.",
        "classification": [
            "Multimodal",
            "Any-to-Any",
            "Audio",
            "Automatic Speech Recognition",
            "Text-to-Speech",
            "Text-to-Audio",
            "Computer Vision",
            "Image-to-Text",
            "Image Classification",
            "Object Detection",
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction",
        "authors": "Zhengren Wang, Junyuan Zhang, Bin Wang, Victor Shea-Jay Huang, Qintong Zhang",
        "link": "https://arxiv.org/abs/2410.21169",
        "github_repo": null,
        "summary": " - This survey paper provides a comprehensive overview of document parsing, consolidating recent advancements in modular pipeline systems and end-to-end models driven by large vision-language models (VLMs) and covering key methodologies, challenges, and future research directions.\n- The paper discusses core document parsing components, including layout detection, content extraction (text, tables, mathematical expressions), and multimodal data integration, examining algorithms for each stage.\n- It addresses the challenges faced by modular document parsing systems and VLMs in handling complex layouts, integrating modules, and recognizing high-density text.\n- The survey consolidates widely used datasets and evaluation metrics for document parsing tasks, providing valuable resources for researchers and practitioners.\n- Finally, the paper emphasizes the importance of developing larger, more diverse datasets and outlines future research directions in the field, such as handling complex layouts and improving OCR for densely packed text.",
        "classification": [
            "Natural Language Processing",
            "Document Question Answering",
            "Computer Vision",
            "Object Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "LongReward: Improving Long-context Large Language Models with AI Feedback",
        "authors": "Zhenyu Hou, Shulin Cao, Xin Lv, Zhongni Hou, Jiajie Zhang",
        "link": "https://arxiv.org/abs/2410.21252",
        "github_repo": null,
        "summary": "- LongReward, a novel method to improve long-context large language models (LLMs) using AI feedback, is introduced.\n- It uses an off-the-shelf LLM to assign rewards to model responses based on four dimensions: helpfulness, logicality, faithfulness, and completeness.\n- When combined with the reinforcement learning algorithm Direct Preference Optimization (DPO), LongReward significantly boosts the performance of long-context SFT models, outperforming baseline methods.\n- Experiments show improvements on long-context question answering and summarization and a positive impact on short instruction following.\n- LongReward enhances model capabilities by mitigating common issues like hallucinations and ineffective context utilization in long-context scenarios.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/THUDM/LongReward"
        ],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation",
        "authors": "Xiaotian Han, Huaibo Huang, Xiaoqiang Zhou, Yuang Ai, Ye27",
        "link": "https://arxiv.org/abs/2410.18666",
        "github_repo": "https://github.com/shallowdream204/DreamClear",
        "summary": "- This paper introduces DreamClear, a novel high-capacity image restoration model based on a Diffusion Transformer (DiT) architecture.\n- It employs a dual-branch framework with textual guidance from multimodal large language models (MLLMs) and a Mixture of Adaptive Modulator (MoAM) to handle diverse real-world degradations.\n- DreamClear leverages a new, privacy-safe, synthetic dataset of one million high-quality images generated using a novel dual-prompt learning data curation pipeline (GenIR). \n- The authors conducted extensive experiments on synthetic and real-world benchmarks demonstrating that DreamClear achieves state-of-the-art performance across a range of metrics, including perceptual metrics (LPIPS, DISTS, FID), no-reference metrics (NIQE, MANIQA, MUSIQ, CLIPIQA), and high-level vision tasks, and also achieves high scores in user preference studies.\n- The newly introduced GenIR dataset generation technique improves model generalizability and restoration performance when used for training.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/shallowdream204/DreamClear"
        ],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "A Survey of Small Language Models",
        "authors": "Samyadeep Basu, Yu Xia, Ryan Aponte, Xuan Shen, Chien Van Nguyen",
        "link": "https://arxiv.org/abs/2410.20011",
        "github_repo": null,
        "summary": "- This paper presents a comprehensive survey of Small Language Models (SLMs), focusing on architectures, training techniques, and model compression methods.\n- The authors introduce a novel taxonomy to categorize SLM optimization methods, considering techniques used in pre-processing, training, post-processing, and the constraints being optimized (e.g., inference compute, training time).\n- The survey covers lightweight architectures, efficient self-attention approximations, neural architecture search for model building, efficient pre-training and fine-tuning strategies, and model compression techniques like pruning, quantization, and knowledge distillation.\n- Additionally, it summarizes benchmark datasets and evaluation metrics commonly used for assessing SLM performance and lists various real-world applications enabled by SLMs, categorized by constraints like real-time interaction, content generation, edge inference, and privacy.\n- Lastly, the paper highlights important open challenges and future research directions for SLMs, such as hallucination, bias, inference-time energy efficiency, and data privacy.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "MarDini: Masked Autoregressive Diffusion for Video Generation at Scale",
        "authors": "Yanping Xie, Mengmeng Xu, Zijian Zhou, Shikun Liu, Haozhe Liu",
        "link": "https://arxiv.org/abs/2410.20280",
        "github_repo": null,
        "summary": "**- MarDini: An innovative family of video diffusion models that combines masked auto-regression (MAR) for temporal planning and a diffusion model (DM) for spatial generation within an efficient asymmetric framework.**  MAR operates on low-resolution inputs to create planning signals, while the lighter DM uses these signals alongside high-resolution inputs. \n**- This structure allows MarDini to perform various video generation tasks**: video interpolation, image-to-video generation, and video expansion by flexibly masking frames during training. \n**- The design prioritizes scalability by using a progressive training strategy and mask ratio tuning.**  MarDini is trained from scratch without reliance on image-based pre-training. \n**- Evaluated on VIDIM-Bench and VBench**: MarDini shows state-of-the-art performance on video interpolation while being computationally efficient, particularly in inference speed compared to other competitive video generation models. \n**- MarDini's efficiency is attributed to**: the asymmetric design allowing more compute resources for the MAR model at a lower resolution and the DM requiring fewer steps for convergence due to the informative planning signal.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Video Classification",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation",
        "authors": "Minhyuk Sung, Taehoon Yoon, Phillip Y. Lee",
        "link": "https://arxiv.org/abs/2410.20474",
        "github_repo": null,
        "summary": " - Introduces GrounDiT, a training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT).\n- Employs a novel noisy patch cultivation-transplantation mechanism, where patches corresponding to bounding boxes are denoised separately and then transplanted into the main image during generation.\n- Leverages the \"semantic sharing\" property of DiT, where jointly denoising a smaller patch alongside a generatable-size image results in the two becoming semantically similar.\n- Achieves state-of-the-art performance on HRS and DrawBench benchmarks, demonstrating superior spatial grounding compared to existing training-free methods, especially in complex scenarios with multiple or small bounding boxes.\n- Improves spatial accuracy on the HRS benchmark by +14.87% over the previous state-of-the-art method R&B and +7.88% over the base PixArt-\u03b1 model.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training",
        "authors": "Kurt Keutzer, Yao Lu, Ligeng Zhu, Han Cai, Haocheng Xi",
        "link": "https://arxiv.org/abs/2410.19313",
        "github_repo": "https://github.com/NVlabs/COAT",
        "summary": "- COAT is a novel FP8 training framework designed to reduce memory footprint and increase training speed for large models by compressing both optimizer states and activations.\n- It introduces Dynamic Range Expansion, aligning optimizer state distributions with FP8's range, thereby minimizing quantization error.\n- For activations, COAT proposes Mixed-Granularity Activation Quantization, using fine-grained quantization for non-linear layers and per-tensor quantization for linear layers.\n- COAT achieves nearly lossless performance while decreasing memory by 1.54x and increasing training speed by 1.43x on Llama 7B, 13B, and 30B models compared to BF16.\n- COAT facilitates training larger models on fewer GPUs by enabling full-parameter training of 7B models on a single GPU and supports doubling the micro-batch size for distributed training.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/NVlabs/COAT"
        ],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines",
        "authors": "Xiangyu Yue, Xiaohan Ding, Yiyuan Zhang, Zhixin Zhang",
        "link": "https://arxiv.org/abs/2410.21220",
        "github_repo": "https://github.com/cnzzx/VSA",
        "summary": "-\nVision Search Assistant, a novel framework to address the limitation of traditional methods in understanding unfamiliar visual content. \n-\nThe framework facilitates collaboration between VLMs and web agents, leveraging VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web.\n-\nBy integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. \n-\nIt involves Visual Content Formulation to represent visual content with correlated formulations, Web Knowledge Search with Chain of Search algorithm to obtain comprehensive web knowledge, and Collaborative Generation to generate the final answer. \n-\nExtensive experiments on open-set and closed-set QA benchmarks demonstrate that Vision Search Assistant significantly outperforms other models.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/cnzzx/VSA"
        ],
        "huggingface_urls": [
            "https://huggingface.co/spaces/opencompass/open_vlm_leaderboard",
            "https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b"
        ],
        "date": "2024-10-29"
    },
    {
        "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
        "authors": "Abhinav Shrivastava, Hao Chen, Yixuan Ren, Saksham Suri, Hanyu Wang",
        "link": "https://arxiv.org/abs/2410.21264",
        "github_repo": null,
        "summary": "-\nLARP is a novel video tokenizer designed for autoregressive (AR) generative models, utilizing a holistic tokenization scheme with learned queries to capture global and semantic video representations.\n-\nUnlike traditional patchwise tokenizers, LARP employs a set of learned queries to gather information from the visual content, enabling more holistic and semantic representations.\n-\nIt incorporates a lightweight AR transformer as a prior model during training, optimizing the latent space for AR generation and automatically determining an optimal token order.\n-\nLARP achieves state-of-the-art Frechet Video Distance (FVD) of 57 on the UCF101 class-conditional video generation benchmark, outperforming existing published and proprietary video generation models, demonstrating its efficacy for AR video generation tasks.\n-\nFurther scalability is shown by achieving better results when using larger AR generators with the LARP tokenizer and in improving generative representation efficiency when reducing tokens from 512 to 256.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "Fast Best-of-N Decoding via Speculative Rejection",
        "authors": "Jiahao Qiu, Huitao Yang, Ruiqi Zhang, Momin Haider, Hanshi Sun",
        "link": "https://arxiv.org/abs/2410.20290",
        "github_repo": null,
        "summary": "- This paper introduces Speculative Rejection, a novel inference-time alignment algorithm designed to improve the efficiency of Best-of-N decoding for large language models (LLMs).\n- The key idea is to dynamically reduce the batch size during generation by halting the generation of unpromising responses early, based on partial reward scores.\n- The algorithm starts with a large batch size, effectively simulating Best-of-N with large N and leverages a reward model to rank partial utterances and terminate low-scoring ones.\n- The results on the AlpacaFarm dataset demonstrate that Speculative Rejection can achieve higher rewards with similar latency while requiring significantly fewer GPUs (16-32 times less compute power) compared to standard Best-of-N. \n- The method is also shown to be effective in maximizing the probability of generated utterances.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Zanette-Labs/SpeculativeRejection"
        ],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "Neural Fields in Robotics: A Survey",
        "authors": "Abhinav Valada, Nick Heppert, Yen-Chen Lin, Mauro Comi, Muhammad Zubair Irshad",
        "link": "https://arxiv.org/abs/2410.20220",
        "github_repo": null,
        "summary": "\n- This survey paper offers a comprehensive overview of Neural Fields (NFs) and their transformative impact on robotics, encompassing various applications, strengths, and limitations.\n- It categorizes and reviews over 200 research papers, examining how NFs enhance perception, planning, and control in robotics.\n- Four key NF frameworks are presented: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting.\n- The paper explores applications of NFs in five major robotics domains: pose estimation, manipulation, navigation, physics simulations, and autonomous driving, providing insights into current progress and open challenges.\n- It concludes by outlining future research directions for NFs in robotics, proposing new avenues for development and application.\n",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Image-to-3D"
        ],
        "github_urls": [
            "robonerf.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "Language Models And A Second Opinion Use Case: The Pocket Professional",
        "authors": "David Noever",
        "link": "https://arxiv.org/abs/2410.20636",
        "github_repo": null,
        "summary": "\n- This research assesses Large Language Models (LLMs) as second opinion tools in complex medical and legal scenarios.\n- Evaluated LLM performance on 183 medical cases from Medscape and 21 Supreme Court legal cases, comparing responses with crowd-sourced physician opinions and documented legal votes respectively.\n- Found high accuracy in straightforward medical cases (>81%) but reduced performance (43%) in complex scenarios, suggesting LLMs may be valuable for generating differential diagnoses rather than as primary diagnostic tools.\n- Developed novel benchmarks for others to assess the reliability of responses by both LLMs and human practitioners, revealing high contestation among human experts.\n- Suggests that using LLMs as specialized agents for second opinions in medicine, especially in challenging cases, might be more appropriate than current approaches that emphasize automation of routine tasks.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/reveondivad/certify"
        ],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation",
        "authors": "Yang Gao, Jiacheng You, Yingdong Hu, Tong Zhang",
        "link": "https://arxiv.org/abs/2406.10615",
        "github_repo": null,
        "summary": "- This paper introduces SGRv2, a new visuomotor policy framework for robotic manipulation that improves sample efficiency by leveraging action locality, an inductive bias positing that robot actions are primarily influenced by the target object and local environment.\n- SGRv2 builds on the Semantic-Geometric Representation (SGR) framework but incorporates action locality throughout its design, using an encoder-decoder architecture for point-wise features, predicting relative target position, applying point-wise weights to highlight critical regions, and using dense supervision.\n- Extensive experiments on RLBench, ManiSkill2, and MimicGen benchmarks show that SGRv2 significantly outperforms existing methods, achieving state-of-the-art results with limited demonstrations, even as few as 5 for certain RLBench tasks.\n- In real-world experiments with a Franka Emika Panda robot, SGRv2 achieves considerably higher success rates compared to baselines.\n- Further evaluations highlight the model's emergent ability to focus on object affordances and generalize to unseen object colors.",
        "classification": [
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-29"
    },
    {
        "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
        "authors": "Denis Bobkov, Boris Mikheev, Alexey Zhavoronkin, Dmitrii Korzh, therem",
        "link": "https://arxiv.org/abs/2410.18057",
        "github_repo": null,
        "summary": "- Introduces CLEAR, a multimodal benchmark for evaluating machine unlearning (MU) in textual and visual modalities, focusing on removing information about specific individuals.\n- The benchmark includes a synthetic dataset of 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs, along with real-world face and visual question answering datasets for evaluating model retention.\n- Evaluates 10 existing MU methods adapted for multimodal unlearning, revealing that current state-of-the-art algorithms struggle in multimodal settings.\n- Demonstrates that simple L1 regularization on LoRA adapter weights during unlearning significantly mitigates catastrophic forgetting, improving the preservation of model performance on retained data.\n- Makes the dataset publicly available to encourage further research in the field.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Natural Language Processing",
            "Image-to-Text",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/therem/CLEAR"
        ],
        "date": "2024-10-30"
    },
    {
        "title": "SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization",
        "authors": "Chuang Gan, Donglai Wei, Jiawei Zhou, zmeng0116, EthanTaylor",
        "link": "https://arxiv.org/abs/2410.21411",
        "github_repo": "https://github.com/Mengzibin/SocialGPT",
        "summary": "- SocialGPT is a novel modular framework that leverages Vision Foundation Models (VFMs) and Large Language Models (LLMs) for social relation reasoning.\n- It employs VFMs to translate image content into a textual \"social story\" and utilizes LLMs for reasoning based on the generated story and provided bounding boxes.\n- This framework incorporates systematic design principles to enhance the collaboration between VFMs and LLMs, including comprehensive and domain-specific visual information extraction and a structured reasoning prompt named SocialPrompt.\n- SocialGPT achieves competitive zero-shot performance on PIPA and PISC datasets, outperforming previous state-of-the-art supervised methods on PIPA by 1.4%.\n- The framework also introduces Greedy Segment Prompt Optimization (GSPO) for automatic prompt tuning, resulting in significant performance improvements across various LLMs.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Zero-Shot Classification"
        ],
        "github_urls": [
            "https://github.com/Mengzibin/SocialGPT"
        ],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization",
        "authors": "Hongming Zhang, Wenhao Yu, Kaixin Ma, Wenlin Yao, Hongliang He",
        "link": "https://arxiv.org/abs/2410.19609",
        "github_repo": null,
        "summary": "- This paper introduces OpenWebVoyager, an open-source framework for building multimodal web agents that can explore real-world websites, receive feedback, and iteratively optimize their performance.\n- The agent architecture adapts the Idefics2-8b-instruct model, processing observations consisting of webpage screenshots and accessibility trees.\n-  OpenWebVoyager combines imitation learning from a GPT-40 powered web agent with an exploration-feedback-optimization cycle, where GPT-40 evaluates the agent's trajectory success.\n- Across multiple iterations on the WebVoyager and Mind2Web datasets, the agent shows improvement in task success rate, starting from 19.9% to 25.8% on the WebVoyager test set and 6.3% to 19.6% on the Mind2Web cross-task test set.\n- The results indicate that the iterative real-world exploration and optimization method is an effective way to improve the agent's real-world performance.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/MinorJerry/OpenWebVoyager"
        ],
        "date": "2024-10-30"
    },
    {
        "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning",
        "authors": "Paul Mineiro, ydeng9",
        "link": "https://arxiv.org/abs/2410.22304",
        "github_repo": null,
        "summary": "- This paper introduces Flow-DPO, a novel approach to improve Large Language Model (LLM) mathematical reasoning by generating high-quality reasoning traces through online multi-agent learning flows.\n- The method employs an incremental output production flow composed of multiple LLMs that iteratively communicate to construct solutions, similar to a multi-agent system.\n- The flow is trained using online Direct Preference Optimization (DPO) with rollouts, generating DPO pairs for each training example during answer chunk generation and updating the models in real-time.\n- Experimental results on MetaMath, GSM8K, and MATH datasets demonstrate that Flow-DPO generates higher-quality reasoning traces compared to direct model inference, leading to improved performance in mathematical reasoning tasks after supervised fine-tuning.\n- This improvement is particularly significant for the Llama-3-8B-instruct model, achieving a 20% improvement in validation accuracy on mathematical reasoning tasks during training.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
        "authors": "Ningxin Zheng, Size Zheng, Wenlei Bao, Li-Wen Chang, preminstrel",
        "link": "https://arxiv.org/abs/2410.21465",
        "github_repo": "https://github.com/bytedance/ShadowKV",
        "summary": "- SHADOWKV is a novel LLM inference system designed for enhanced throughput in long-context scenarios by storing a low-rank representation of the key cache on the GPU and offloading the value cache to the CPU.\n- It employs a precise KV selection strategy during decoding, utilizing landmarks and static outliers to minimize the sparse KV cache budget while maintaining accuracy.\n- Evaluations on benchmarks like RULER, LongBench, and Needle in a Haystack with various LLMs (Llama, GLM, Yi, Phi, Qwen) show that SHADOWKV can handle contexts up to 1M tokens.\n- It achieves up to a 6x increase in batch size and a 3.04x boost in throughput compared to full attention on an A100 GPU.\n- SHADOWKV's performance even surpasses the theoretical throughput of infinite batch size with full attention, assuming infinite GPU memory.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/bytedance/ShadowKV"
        ],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset",
        "authors": "Yongyuan Liang, Huanyu Li, Tao Huang, Yifei Sun, Guangqi Jiang",
        "link": "https://arxiv.org/abs/2410.22325",
        "github_repo": null,
        "summary": "- This paper introduces Manipulation Centric Representation (MCR), a framework for learning robotic visual representations that prioritize manipulation-relevant information, such as robot end-effectors and task-relevant objects.\n- MCR leverages large-scale robot datasets (e.g. DROID), and introduces a novel contrastive loss that aligns visual observations with robot proprioceptive state-action dynamics.\n- It uses two new training objectives: dynamics alignment loss and action prediction loss. A time contrastive loss is also used.\n- MCR outperforms previous state-of-the-art by 14.8% across four simulated robotic manipulation domains and also achieves a 76.9% improvement on three real-world robot tasks.\n- The paper also introduces \"manipulation centricity,\" a metric demonstrating a strong correlation with downstream policy performance in robotic manipulation tasks.",
        "classification": [
            "Robotics",
            "Image Feature Extraction",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning",
        "authors": "Sergey Levine, Jeffrey Wu, charlesxu0124, jianlanluo",
        "link": "https://arxiv.org/abs/2410.21845",
        "github_repo": null,
        "summary": "- This paper introduces a human-in-the-loop vision-based reinforcement learning (RL) system called HIL-SERL for dexterous robotic manipulation tasks.\n- HIL-SERL integrates human demonstrations and corrections, pretrained vision backbones, and a sample-efficient off-policy RL algorithm.\n- The system achieves near-perfect success rates and surpasses human cycle times on diverse manipulation tasks like dynamic object flipping, precise assembly, and dual-arm coordination, often within 1-2.5 hours of real-world training.\n- It demonstrates a significant performance improvement compared to imitation learning methods (e.g., 101% average success rate increase and 1.8x faster cycle time).\n- The approach trains policies that exhibit both reactive and predictive behaviors, adapting to the specific requirements of each task, from precise insertions to dynamic motions like Jenga whipping.",
        "classification": [
            "Reinforcement Learning",
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [
            "https://hil-serl.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation",
        "authors": "Hongjin Qian, Ziliang Zhao, Kelong Mao, dongguanting, ariya2357",
        "link": "https://arxiv.org/abs/2410.23090",
        "github_repo": null,
        "summary": "- This paper introduces CORAL, a large-scale benchmark designed to evaluate Retrieval-Augmented Generation (RAG) systems in multi-turn conversational settings.\n- CORAL is derived from Wikipedia, containing 8,000 information-seeking conversations covering various topics with citation labels.\n- It includes three tasks: passage retrieval, response generation, and citation labeling and proposes a framework to standardize different RAG methods.\n- Evaluations show that fine-tuned open-source LLMs outperform commercial closed-source LLMs in retrieval, and that input length reduction maintains response quality and improves citation accuracy.\n- The benchmark addresses challenges in multi-turn conversational RAG, such as redundant information and topic shifts, paving the way for evaluating and improving multi-turn conversational RAG systems.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Ariya12138/CORAL"
        ],
        "huggingface_urls": [],
        "date": "2024-10-31"
    },
    {
        "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks",
        "authors": "Korbinian P\u00f6ppel, Maximilian Beck, Vihang Patil, Thomas Adler, Thomas Schmied",
        "link": "https://arxiv.org/abs/2410.22391",
        "github_repo": null,
        "summary": "- This paper introduces the Large Recurrent Action Model (LRAM), a novel architecture for reinforcement learning employing modern recurrent neural networks, specifically xLSTM, for faster inference in robotics tasks.\n- LRAM utilizes an xLSTM core with linear time inference complexity and excels in sequence length extrapolation, making it suitable for real-time applications like robotics.\n- The model is trained on a large-scale multi-domain dataset of 894M transitions from 432 tasks across 6 domains.\n- Experiments on various model sizes demonstrate that xLSTM-based LRAMs achieve performance comparable to or exceeding Transformer-based models while offering significantly faster inference speeds.\n- Further analyses demonstrate that modern recurrent backbones are better suited for building LAMs than Transformers.",
        "classification": [
            "Reinforcement Learning",
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/ml-jku/LRAM"
        ],
        "huggingface_urls": [],
        "date": "2024-10-31"
    },
    {
        "title": "Stealing User Prompts from Mixture of Experts",
        "authors": "Nicholas Carlini, Jamie Hayes, Ilia Shumailov, Itay Yona",
        "link": "https://arxiv.org/abs/2410.22884",
        "github_repo": null,
        "summary": "- This paper introduces a novel attack, called MoE Tiebreak Leakage, which exploits a vulnerability in Mixture-of-Experts (MoE) models to extract user prompts.\n- The attack leverages the Expert-Choice-Routing (ECR) strategy, manipulating the order of inputs within a batch to cause predictable token dropping, thereby revealing information about a victim's prompt.\n- The authors successfully demonstrate the attack on a two-layer Mixtral model, extracting almost all secret messages (996/1000) across varying lengths (1-11 tokens), and achieving 99.9% success in recovering individual tokens (4833/4838).\n- The attack's complexity is O(VM\u00b2) for the number of queries to the target model and O(2DNVM\u00b2) for queries to a local model copy, where V is vocabulary size, M is prompt length, D is the number of layers, and N is the number of experts.\n- The paper discusses potential mitigations, including preserving in-batch data independence and introducing stochasticity into the model's routing or capacity parameters.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-31"
    },
    {
        "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
        "authors": "Xiao Zhou, Xiangxu Zhang, Lei Li, zl101",
        "link": "https://arxiv.org/abs/2410.20050",
        "github_repo": "https://github.com/CMIRB-benchmark/CMIRB",
        "summary": "- This paper introduces SL-HyDE (Self-Learning Hypothetical Document Embeddings), a novel approach for zero-shot medical information retrieval that eliminates the need for labeled data.\n- SL-HyDE leverages LLMs to generate hypothetical documents from user queries, and utilizes a retriever to find relevant documents based on these hypothetical documents.\n- It employs a self-learning mechanism to enhance both LLM document generation and retriever performance without relying on labeled medical data.\n- A new benchmark for Chinese Medical Information Retrieval (CMIRB) is introduced, consisting of five tasks and ten datasets derived from real-world scenarios.\n- Experimental results on CMIRB show SL-HyDE surpasses HYDE by 4.9% in NDCG@10 and demonstrates performance gains across various LLM and retriever combinations.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/CMIRB-benchmark/CMIRB"
        ],
        "huggingface_urls": [],
        "date": "2024-10-31"
    },
    {
        "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
        "authors": "Jan Eric Lenssen, Yongqin Xian, Muhammad Ferjad Naeem, Yue Fan, Haiyang Wang",
        "link": "https://arxiv.org/abs/2410.23168",
        "github_repo": "https://github.com/Haiyang-W/TokenFormer",
        "summary": "- TokenFormer, a novel fully attention-driven neural network architecture, is introduced, which treats model parameters as tokens, enhancing flexibility in token-parameter computations.\n- By utilizing a cross-attention mechanism between input tokens and learnable parameter tokens, TokenFormer allows for scaling model parameters without altering input or output dimensions, enabling progressive scaling by adding new key-value parameter pairs.\n- This approach facilitates efficient scaling by reusing pre-trained models, thereby significantly reducing training costs compared to training large transformer models from scratch.\n- Experimental results demonstrate that TokenFormer achieves comparable perplexity to Transformers trained from scratch on language modeling tasks while substantially reducing the training budget, and maintains similar performance in visual modeling and zero-shot classification tasks.\n- TokenFormer offers controllable computational costs for long-context modeling, preserves learned distributions during scaling, and shows potential for integration into Mixture-of-Experts frameworks and parameter-efficient tuning strategies.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision",
            "Image Classification",
            "Text Generation",
            "Zero-Shot Classification"
        ],
        "github_urls": [
            "https://github.com/Haiyang-W/TokenFormer"
        ],
        "huggingface_urls": [],
        "date": "2024-10-31"
    },
    {
        "title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
        "authors": "Robert West, Justin Deschenaux, Mikhail Terekhov, Chris Wendler, surokpro2",
        "link": "https://arxiv.org/abs/2410.22366",
        "github_repo": "https://github.com/surkovv/sdxl-unbox",
        "summary": "- This paper investigates the internals of SDXL Turbo, a few-step text-to-image diffusion model, using Sparse Autoencoders (SAEs).\n- SAEs are trained on the updates of transformer blocks within SDXL Turbo's denoising U-Net, learning interpretable features that causally influence image generation.\n- Analysis reveals specialized roles for different blocks: one for composition, another for local details, and one for color, illumination, and style.\n- Visualization techniques are developed to analyze feature interpretability and causal effects, supported by quantitative experiments.\n- The research contributes to understanding how text-to-image models function and provides tools for further mechanistic interpretability studies.",
        "classification": [
            "Text-to-Image",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/surkovv/sdxl-unbox"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective",
        "authors": "Tianyi Zhou, Yanhong Li, MingLiiii",
        "link": "https://arxiv.org/abs/2410.23743",
        "github_repo": "https://github.com/MingLiiii/Layer_Gradient",
        "summary": "- This research paper investigates the layer-wise gradient patterns in Large Language Models (LLMs) during instruction-tuning with different reasoning approaches (fast vs. slow thinking) and response types.\n- It uses spectral analysis, specifically Singular Value Decomposition (SVD) and nuclear norm, to characterize gradient behaviors across LLM layers for tasks involving math, commonsense reasoning, and knowledge learning.\n- Slow thinking, using detailed Chain-of-Thought (CoT), results in more stable and uniform gradient norms across layers compared to fast thinking, suggesting improved learning stability.\n- The gradients associated with slow thinking effectively differentiate correct from irrelevant responses in reasoning tasks, while in knowledge learning tasks, gradient norms are sensitive to knowledge popularity but not correctness. \n- The study also finds that instruction-tuned LLMs do not show significant advantages over pre-trained LLMs in identifying incorrect reasoning and have different gradient patterns for fast thinking responses, suggesting challenges in aligning with the instruction-tuning data.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/MingLiiii/Layer_Gradient"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents",
        "authors": "Pawan Goyal, Gajula Sai Chaitanya, Abhilash Nandy, Sombit Bose, Ankan Mullick",
        "link": "https://arxiv.org/abs/2410.22476",
        "github_repo": null,
        "summary": "- This paper introduces MLMCID, a pointer network-based architecture for joint extraction and detection of multi-label multi-class intents in task-oriented dialogue systems.\n- The MLMCID model uses an encoder-decoder framework with a pointer network and LSTM-based sequence generator to identify multiple intent spans within a sentence, along with their corresponding coarse- and fine-grained intent labels.\n- A new multilingual multi-label intent dataset (MLMCID-dataset) is also created from existing benchmark datasets. \n- The model outperforms baseline approaches, including large language models (LLMs) like Llama2 and GPT, on various MLMCID datasets in terms of accuracy and F1-score.\n-  The approach is also effective in few-shot settings and demonstrates the importance of multi-intent modeling for real-world conversational AI.",
        "classification": [
            "Natural Language Processing",
            "Text Classification",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/ankan2/multi-intent-pointer-network"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models",
        "authors": "Lei Hou, Bin Xu, Xiaozhi Wang, Hao Peng, Yunjia Qi",
        "link": "https://arxiv.org/abs/2410.24175",
        "github_repo": null,
        "summary": "- This paper introduces constraint back-translation, a novel data generation technique for improving complex instruction following in Large Language Models (LLMs).\n- The technique involves taking existing instruction-response pairs and using an LLM to generate constraints that are already implicitly satisfied by the response. \n- This method is used to create CRAB, a high-quality complex instruction-response dataset.\n- The method improves the performance of LLMs on complex instruction-following tasks, as measured by IFEval and FollowBench benchmarks.\n- It also serves as a useful auxiliary training objective during post-training by enhancing the model's understanding of complex constraints.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "Language Models can Self-Lengthen to Generate Long Texts",
        "authors": "Dayiheng Liu, An Yang, Bowen Yu, Tianyi Tang, Shanghaoran Quan",
        "link": "https://arxiv.org/abs/2410.23933",
        "github_repo": "https://github.com/QwenLM/Self-Lengthen",
        "summary": "- This paper introduces Self-Lengthen, an iterative training framework to improve the long text generation capabilities of Large Language Models (LLMs).\n- Self-Lengthen employs two roles: a Generator to produce initial responses and an Extender to lengthen these responses iteratively.\n- This method leverages the intrinsic knowledge of LLMs without needing additional data or proprietary models, addressing the training gap in current LLMs for long text generation.\n- Experimental results on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long text generation using open-source LLMs like Qwen2 and LLaMA3.\n- Notably, Self-Lengthen enhances the output length while preserving the quality, boosting output from 1,000 words to 8,000 in Qwen2.5.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/QwenLM/Self-Lengthen"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays",
        "authors": "Xinxing Xu, Sicong Leng, Yanyu Xu, Tan Li Hui Faith, youngzhou12",
        "link": "https://arxiv.org/abs/2410.21969",
        "github_repo": "https://github.com/yangzhou12/BenchX",
        "summary": "- BenchX, a unified benchmark framework, is proposed for evaluating Medical Vision-Language Pretraining (MedVLP) methods on chest X-rays, enabling head-to-head comparison and systematic analysis.\n- BenchX comprises three components: comprehensive datasets covering nine datasets and four medical tasks, benchmark suites to standardize data preprocessing and experimental setups, and unified fine-tuning protocols that accommodate heterogeneous MedVLP methods.\n- Baselines for nine state-of-the-art MedVLP methods are established using BenchX, revealing that some early methods can outperform recent ones with proper training strategies.\n- In particular, MGCA and MRM consistently demonstrate strong performance in most cases. \n- MedCLIP-ViT also delivers good performance on multi-label image classification tasks.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Image Segmentation",
            "Image-to-Text",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/yangzhou12/BenchX"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments",
        "authors": "Yunhua Zhou, Dong Zhang, Bo Wang, Pengyu Wang, Xinghao Wang",
        "link": "https://arxiv.org/abs/2410.23918",
        "github_repo": "https://github.com/xinghaow99/BitStack",
        "summary": "BitStack is a novel, training-free weight compression approach for large language models (LLMs) that enables megabyte-level trade-offs between memory usage and model performance.  It achieves this through iterative weight decomposition and considers parameter significance, resulting in approximately 1-bit per parameter residual blocks.  These blocks are sorted and stacked for dynamic loading based on memory availability.  Extensive experiments show BitStack matches or surpasses existing quantization baselines across various tasks, particularly at extreme compression ratios.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/xinghaow99/BitStack"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks",
        "authors": "Qingwei Lin, Jue Zhang, Zhiyang Zhang, Xiaoting Qin, Yingzhe Peng",
        "link": "https://arxiv.org/abs/2410.24032",
        "github_repo": null,
        "summary": "The paper introduces CARE, a collaborative chatbot system for personalized exploratory tasks that combines a multi-agent LLM framework with a structured UI.  CARE addresses limitations of existing LLM chatbots by extracting both explicit and implicit user needs through iterative query refinement and dynamic solution generation. A within-subject user study with 22 participants showed CARE was consistently preferred over a baseline LLM chatbot, reducing cognitive load and inspiring creativity.  CARE transforms LLM-based systems from passive information retrievers into proactive partners in personalized problem-solving and exploration. The study also revealed CARE's impact on facilitating better user experiences in complex tasks, by improving solution comprehensiveness and personalization.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "null"
        ],
        "huggingface_urls": [
            "null"
        ],
        "date": "2024-11-01"
    },
    {
        "title": "DELTA: Dense Efficient Long-range 3D Tracking for any video",
        "authors": "Sergey Tulyakov, Evangelos Kalogerakis, Chuang Gan, Peiye Zhuang, Tuan Duc Ngo",
        "link": "https://arxiv.org/abs/2410.24211",
        "github_repo": null,
        "summary": "- This paper introduces DELTA, a novel method for dense 3D motion tracking from monocular videos that achieves state-of-the-art accuracy while being significantly faster than previous approaches.\n- DELTA leverages a joint global-local attention mechanism for efficient tracking at reduced resolution, followed by a transformer-based upsampler for high-resolution predictions.\n- The model uses a coarse-to-fine strategy, beginning with coarse tracking through a spatio-temporal attention mechanism and followed by an attention-based upsampler for high-resolution predictions.\n- Extensive experiments demonstrate DELTA's superiority on multiple benchmarks, achieving new state-of-the-art results in both 2D and 3D dense tracking tasks.\n- The authors explore the impact of depth representation on tracking performance and identify log-depth as the optimal choice.",
        "classification": [
            "Computer Vision",
            "Video Classification",
            "Keypoint Detection",
            "Depth Estimation",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://snap-research.github.io/DELTA/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "Learning Video Representations without Natural Videos",
        "authors": "Yossi Gandelsman, Xinlei Chen, Xueyang Yu",
        "link": "https://arxiv.org/abs/2410.24213",
        "github_repo": null,
        "summary": "- This paper demonstrates that effective video representations can be learned from synthetic videos and natural images, without using any natural videos during training.\n- The authors propose a progression of synthetic video datasets with increasing complexity, modeling properties like motion, acceleration, and shape transformations.\n-  A VideoMAE model pre-trained on synthetic data closes 97.2% of the performance gap on UCF101 action classification compared to training from scratch and surpasses the performance of models pre-trained on UCF101 when natural image crops are included.\n- On UCF101-P, an out-of-distribution dataset, models trained on synthetic data outperform UCF101 pre-trained models on 11 of 14 corruption types.\n-  Analysis suggests correlations between frame diversity, similarity to natural data, and downstream performance, offering insights into creating more effective datasets for video representation learning.",
        "classification": [
            "Computer Vision",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/unicorn53547/video_syn_rep"
        ],
        "huggingface_urls": [],
        "date": "2024-11-01"
    },
    {
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "authors": "Fangzhi Xu, Zhenyu Wu, Zhiyong Wu, heroding77, QiushiSun",
        "link": "https://arxiv.org/abs/2410.23218",
        "github_repo": null,
        "summary": "- This paper introduces OS-Atlas, a large action model designed for generalist GUI agents, focusing on GUI grounding and out-of-distribution (OOD) generalization.\n- It leverages a novel multi-platform data synthesis toolkit, enabling the creation of a 13 million GUI element dataset spanning Windows, macOS, Linux, Android, and web interfaces.\n- OS-Atlas employs a two-stage training process: GUI grounding pre-training on the large dataset and action fine-tuning on existing agent datasets with a unified action space to mitigate conflicts.\n- Evaluations across six benchmarks and three platforms (mobile, desktop, web) show significant performance improvements over state-of-the-art models.\n- OS-Atlas-Base, the pre-trained model, serves as an open-source alternative to commercial VLMs for building GUI agents, achieving comparable performance in some settings.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "Constant Acceleration Flow",
        "authors": "Youngjoon Hong, Taehoon Lee, Sihyeon Kim, Sojin Lee, Dogyun Park",
        "link": "https://arxiv.org/abs/2411.00322",
        "github_repo": "https://github.com/mlvlab/CAF",
        "summary": "- Introduces Constant Acceleration Flow (CAF), a novel ODE-based generative model that integrates acceleration as a learnable variable for enhanced precision in image generation.\n- Proposes Initial Velocity Conditioning (IVC) and a reflow process for initial velocity to address the flow crossing problem, improving trajectory estimation.\n- Achieves state-of-the-art Fr\u00e9chet Inception Distance (FID) scores of 1.39 and 1.69 on CIFAR-10 and ImageNet 64x64, respectively, outperforming strong baselines in conditional image generation.\n- Demonstrates superior performance in one-step and few-step generation, coupling preservation, flow straightness, and inversion tasks.",
        "classification": [
            "Unconditional Image Generation",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/mlvlab/CAF"
        ],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "Randomized Autoregressive Visual Generation",
        "authors": "Liang-Chieh Chen, Xiaohui Shen, Xueqing Deng, turkeyju, yucornetto",
        "link": "https://arxiv.org/abs/2411.00776",
        "github_repo": "https://github.com/bytedance/1d-tokenizer",
        "summary": "- This paper introduces Randomized AutoRegressive modeling (RAR), a new training paradigm for autoregressive visual generation that enhances bidirectional context learning.\n- RAR employs a Vision Transformer architecture and operates by randomly permuting the input image token sequence during training with a probability *r* that linearly anneals from 1 to 0, encouraging bidirectional context learning while converging to a standard raster scan order by the end of training.\n-  This approach maximizes the expected likelihood over all factorization orders and thus improves the model's ability to capture bidirectional contexts without modifying the autoregressive framework, ensuring compatibility with language models.\n- On the ImageNet-256 benchmark, RAR achieves a state-of-the-art FID score of 1.48, outperforming not only prior autoregressive models but also leading diffusion-based and masked transformer-based methods.\n- RAR also shows strong scaling behavior and high sampling speed thanks to its compatibility with language model optimization techniques such as KV-caching.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [
            "https://github.com/bytedance/1d-tokenizer"
        ],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
        "authors": "Leon Bergen, Duncan Watson-Parris, Yadi Cao, yuqirose, Bohan22",
        "link": "https://arxiv.org/abs/2411.00412",
        "github_repo": null,
        "summary": "- This paper introduces a novel two-stage training method called Adapting While Learning (AWL) to enhance Large Language Models (LLMs) for solving scientific problems by incorporating tool usage and direct reasoning.\n- AWL consists of World Knowledge Distillation (WKD) which fine-tunes LLMs to internalize domain knowledge from solutions generated using tools and Tool Usage Adaptation (TUA) which trains LLMs to choose between direct answering and tool usage based on problem complexity.\n- Evaluation across six scientific benchmarks demonstrate average improvements of 28.18% in answer accuracy and 13.89% in tool usage precision compared to baselines and state-of-the-art models such as GPT-4 and Claude-3.5.\n- The model surpasses existing approaches on custom datasets that include complex and specialized scientific questions not commonly seen during pre-training.\n- It also showcases improved robustness in noisy data scenarios and adaptability to open-ended questions through integration with preference learning.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "Personalization of Large Language Models: A Survey",
        "authors": "Yijia Shao, Branislav Kveton, Ryan A. Rossi, Zhehao Zhang, Franck-Dernoncourt",
        "link": "https://arxiv.org/abs/2411.00027",
        "github_repo": null,
        "summary": "- This survey paper provides a comprehensive overview of personalized Large Language Models (LLMs), unifying research on personalized text generation and downstream task personalization.\n- It introduces a taxonomy for personalized LLM usage, formalizing foundations, and analyzing personalization granularity (user-level, persona-level, global preference).\n- The paper surveys techniques for personalization, including retrieval-augmented generation, prompting, representation learning, and reinforcement learning from human feedback (RLHF).\n- It also covers evaluation metrics and datasets for personalized LLMs, along with various applications like AI assistants, recommendation systems, and search engines.\n- Finally, it discusses open problems such as benchmarks, cold-start issues, bias, privacy, and multimodality in personalized LLMs.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models",
        "authors": "Sergio Martin, Clara P\u00e9rez-Molina, sascha-kirch, jolalde5",
        "link": "https://arxiv.org/abs/2411.00233",
        "github_repo": null,
        "summary": "- SambaMixer, a novel structured state space model (SSM) based on the MambaMixer architecture, is proposed for predicting the state of health (SOH) of Li-ion batteries. \n- It is designed to handle multivariate time signals and long-range temporal dependencies and leverages an anchor-based resampling method to ensure consistent sample lengths and augment data.\n- Evaluation on NASA's Li-ion battery discharge dataset demonstrates superior performance compared to state-of-the-art models on metrics such as MAE, RMSE, and MAPE for predicting SOH and AEOLE for EOL prediction.\n- A novel sample time-based positional encoding scheme is introduced to manage sample jitter, varying signal lengths, and recuperation effects in Li-ion batteries.\n- The model effectively predicts the dynamic behavior of SOH curves and accurately identifies EOL indicators.",
        "classification": [
            "Time Series Forecasting"
        ],
        "github_urls": [
            "https://github.com/sascha-kirch/samba-mixer"
        ],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "In-Context LoRA for Diffusion Transformers",
        "authors": "Huanzhang Dou, Yupeng Shi, Zhi-Fan Wu, Wei Wang, lhhuang",
        "link": "https://arxiv.org/abs/2410.23775",
        "github_repo": "https://github.com/ali-vilab/In-Context-LoRA",
        "summary": "- This paper introduces In-Context LoRA (IC-LORA), a novel approach for adapting text-to-image diffusion transformers (DiTs) to diverse generative tasks.\n- IC-LORA leverages the inherent in-context generation capabilities of pre-trained DiTs by concatenating images and prompts, followed by minimal LoRA tuning on a small dataset of 20-100 image sets.\n- This method simplifies the architecture compared to previous work like Group Diffusion Transformers (GDT) and achieves high-fidelity image set generation while preserving in-context abilities and model knowledge.\n- The proposed framework is task-agnostic in its architecture and pipeline, requiring only task-specific tuning data, enabling adaptation to a wide range of applications.\n- Image-conditional generation is also supported through SDEdit, allowing inpainting of masked images within the concatenated image, though maintaining consistency between input and generated images remains a challenge for future exploration.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/ali-vilab/In-Context-LoRA"
        ],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation",
        "authors": "Shukai Liu, Jian Yang, Congnan Liu, Ken Deng, Jiaheng Liu",
        "link": "https://arxiv.org/abs/2410.21157",
        "github_repo": null,
        "summary": "- This paper introduces M2RC-EVAL, a massively multilingual repository-level code completion benchmark encompassing 18 programming languages.\n- It offers two types of fine-grained annotations: bucket-level (based on abstract syntax tree depth) and semantic-level (categorizing completion scenarios).\n- The authors also present M2RC-INSTRUCT, a multilingual instruction dataset designed to improve repository-level code completion abilities in LLMs.\n- Experimental results show that incorporating cross-file context and fine-tuning on M2RC-INSTRUCT significantly enhances performance across various languages.\n- Code Llama with fine-tuning outperforms non-finetuned StarCoder after fine-tuning. ",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/M2RC-Eval-Team/M2RC-Eval"
        ],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models",
        "authors": "Chenhui Xue, Chaojie Yang, Tian Li, Nianhong Jiao, Shengkai Zhang",
        "link": "https://arxiv.org/abs/2410.22901",
        "github_repo": null,
        "summary": "- HelloMeme is a novel method for inserting adapters into text-to-image foundation models, enabling complex downstream tasks like meme video generation without sacrificing the base model's generalization ability.\n- The method focuses on optimizing the attention mechanism related to 2D feature maps, using Spatial Knitting Attentions (SK Attentions) to fuse high-level features (head pose, facial expressions) and fidelity-rich features from a reference image.\n-  It leverages a three-module architecture: HMReferenceNet extracts fidelity-rich features, HMControlNet extracts high-level features, and HMDenoisingNet combines these features to generate the output image or video frames.\n-  Evaluations on tasks like self-reenactment and cross-reenactment using metrics like FID, FVD, PSNR, SSIM, LPIPS, AED, and APD demonstrate that HelloMeme outperforms existing open-source state-of-the-art methods.\n- The method maintains compatibility with SD1.5 derivative models and offers value to the open-source community by releasing the associated code.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://songkey.github.io/hellomeme",
            "https://github.com/HelloVision/ExperimentsOnSKAttentions"
        ],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "WikiNER-fr-gold: A Gold-Standard NER Corpus",
        "authors": "Pierre-Fran\u00e7ois Marteau, Nicolas B\u00e9chet, Danrun Cao",
        "link": "https://arxiv.org/abs/2411.00030",
        "github_repo": null,
        "summary": "- This paper introduces WikiNER-fr-gold, a manually revised gold-standard version of the French portion of the WikiNER corpus, aimed at improving the quality of Named Entity Recognition (NER) resources.\n- The corpus consists of 20% of the original WikiNER-fr (26,818 sentences, ~700k tokens), randomly sampled and corrected for inconsistencies in annotation stemming from the semi-supervised nature of the original WikiNER.\n- The correction process focused on standardizing entity boundaries and categories, resolving ambiguous hyperlinks, and addressing inconsistencies in the application of annotation guidelines.\n- The authors analyzed the errors in the silver-standard WikiNER-fr, categorized them, and described the correction strategies employed.\n- The paper also discusses future work, including a broader assessment of entity categorization and potential automation of the correction process for the remaining WikiNER data.",
        "classification": [
            "Natural Language Processing",
            "Token Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "Survey of User Interface Design and Interaction Techniques in Generative AI Applications",
        "authors": "Reuben Luera, puneetm, zhangry868, subright, Franck-Dernoncourt",
        "link": "https://arxiv.org/abs/2410.22370",
        "github_repo": null,
        "summary": "- This survey paper presents a comprehensive overview of user interface (UI) design and interaction techniques in generative AI applications, focusing on user-guided interactions where users deliberately initiate actions.\n- It introduces four taxonomies: user-guided interaction techniques (prompting, selection, system manipulation, object manipulation), UI layouts (conversational, canvas, contextual, modular, simulated), human-AI engagement levels (passive, deterministic, assistive, collaborative), and generative AI application areas (content creation, data analysis, research, automation, assistance).\n- The goal is to provide a design compendium for generative AI designers and developers, lowering the entry barrier and offering a foundation for research in human-AI interaction.\n- The survey analyzes various generative AI systems and tools, categorizing their UI/UX patterns and interaction modalities, emphasizing how these choices enhance user experience and streamline generative processes.\n- The paper concludes by discussing open problems and challenges in generative AI, highlighting accessibility for users with disabilities and limited technical literacy, the future of UI design for evolving AI technologies, and ethical considerations regarding bias mitigation and misuse prevention.",
        "classification": [
            "Other"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset",
        "authors": "Jincen Shuai, Devasha Trivedi, Anish Pahilajani, Franck-Dernoncourt, namyongp",
        "link": "https://arxiv.org/abs/2411.00369",
        "github_repo": null,
        "summary": "- This paper introduces GRS-QA, a new multi-hop question answering dataset that includes explicit reasoning structures in the form of graphs for enhanced reasoning analysis of LLMs.\n- Unlike existing datasets that lack clear reasoning pathways, GRS-QA captures intricate structures by constructing reasoning graphs, where nodes denote textual contexts and edges signify logical flow.\n- GRS-QA offers benefits such as providing transparent reasoning steps for answer derivation, allowing fine-grained evaluation of LLM reasoning capabilities across diverse structures.\n- It also includes negative reasoning graphs, created by perturbing the structure of positive graphs, to isolate the impact of reasoning structures compared to content on question answering performance.\n- The authors benchmark state-of-the-art models on GRS-QA from retrieval, direct question answering, and retrieval-augmented generation perspectives, revealing that LLM performance degrades with increasing reasoning complexity and highlighting the importance of GRS-QA in pushing the limits of current QA models.",
        "classification": [
            "Question Answering",
            "Natural Language Processing",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-04"
    },
    {
        "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents",
        "authors": "Hao Yu, Siyi Cheng, Xueqiao Sun, Xiao Liu, Yifan Xu",
        "link": "https://arxiv.org/abs/2410.24024",
        "github_repo": "https://github.com/THUDM/Android-Lab",
        "summary": "- ANDROIDLAB, a systematic Android agent framework, is introduced for training and evaluating both open-source and closed-source large language models (LLMs) and large multimodal models (LMMs).\n- It offers an operation environment with different modalities, an action space, and a reproducible benchmark featuring 138 tasks across nine apps on Android virtual devices.\n- An Android Instruction dataset with 10.5k traces and 94.3k steps was created, facilitating fine-tuning of six open-source LLMs and LMMs. \n- Fine-tuning resulted in improved success rates, increasing from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. \n- Open-source models demonstrated significant performance gains after fine-tuning, approaching or even surpassing some closed-source models in certain aspects.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/THUDM/Android-Lab"
        ],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
        "authors": "Hanyu Lai, Iat Long Iong, Xiao Liu, Zehan Qi, tianjiezhang",
        "link": "https://arxiv.org/abs/2411.02337",
        "github_repo": null,
        "summary": "- WEBRL, a self-evolving online curriculum reinforcement learning framework, is introduced for training high-performance web agents using open LLMs.\n- WEBRL addresses challenges such as scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning by incorporating a self-evolving curriculum for task generation, a robust outcome-supervised reward model (ORM), and adaptive reinforcement learning strategies.\n- Applying WEBRL to open Llama-3.1 and GLM-4 models resulted in significant success rate improvements on WebArena-Lite, outperforming GPT-4-Turbo, GPT-40, and previous state-of-the-art open LLM-based web agents.\n- Llama-3.1-8B's success rate increased from 4.8% to 42.4%, while GLM-4-9B improved from 6.1% to 43%.\n- WEBRL effectively bridges the gap between open and proprietary LLM-based web agents by enabling more accessible and powerful autonomous web interaction systems.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/THUDM/WebRL"
        ],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "Training-free Regional Prompting for Diffusion Transformers",
        "authors": "Wenzhao Zheng, Jianjin Xu, wanghaofan, wangyida, antonio-c",
        "link": "https://arxiv.org/abs/2411.02395",
        "github_repo": "https://github.com/antonioo-c/Regional-Prompting-FLUX",
        "summary": "- This paper introduces a training-free regional prompting method for Diffusion Transformers (DiT), specifically FLUX.1, enhancing compositional text-to-image generation.\n- The method involves manipulating attention maps using regional prompt-mask pairs, enabling fine-grained control over object placement and attributes within generated images.\n- By modifying cross-attention and self-attention mechanisms within the DiT architecture, the approach ensures that each region attends to its corresponding textual description while maintaining overall image coherence and preventing unwanted interactions between unrelated regions.\n- Experiments demonstrate the method's ability to handle complex, multi-regional prompts with improved semantic alignment and precise regional differentiation, shown qualitatively with visual results and quantitatively with ablation studies, memory and runtime analysis.\n- This training-free nature eliminates the need for model retraining or additional data, offering efficiency and flexibility in image generation workflows.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/antonioo-c/Regional-Prompting-FLUX"
        ],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
        "authors": "Bin Hu, Junyu Zhang, Xingang Guo, Chengke Zou, Ray2333",
        "link": "https://arxiv.org/abs/2411.00836",
        "github_repo": null,
        "summary": "- DYNAMATH, a dynamic visual benchmark, is introduced to evaluate the robustness of Vision Language Models (VLMs) in mathematical reasoning.\n- The benchmark consists of 501 seed questions represented as Python programs, enabling automatic generation of diverse concrete questions with variations in visual and textual content.\n- An evaluation of 14 state-of-the-art VLMs on 5,010 generated questions revealed a significant gap between average-case and worst-case accuracy, indicating current VLMs' lack of robustness in handling question variations.\n- The analysis also found high repetition consistency in many models, suggesting that incorrect answers on certain variants are due to consistent errors rather than inherent randomness.\n- DYNAMATH provides insights to guide development of more robust VLMs and the paper suggests using adversarial training or reinforcement learning from human feedback with fine-grained process rewards as potential improvement strategies.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
        "authors": "Jiaqi Zhu, Xingwu Sun, Ruobing-Xie, Mimosa77, YanfengChen",
        "link": "https://arxiv.org/abs/2411.02265",
        "github_repo": "https://github.com/Tencent/Hunyuan-Large",
        "summary": "- Tencent introduces Hunyuan-Large, a 389 billion parameter (52 billion activated) open-source Mixture-of-Experts (MoE) model based on the Transformer architecture and capable of handling up to 256K tokens.\n- The model outperforms LLama3.1-70B on various benchmarks, including language understanding, generation, logical reasoning, mathematics, coding, and long-context tasks, and exhibits performance comparable to the much larger LLama3.1-405B model.\n- Key innovations include using large-scale synthetic data, a mixed expert routing strategy combining shared and specialized experts with recycle routing for discarded tokens, KV cache compression by grouped-query attention and cross-layer attention, and an expert-specific learning rate scaling strategy.\n- The model is pre-trained on 7 trillion tokens, including 1.5 trillion synthetic tokens, followed by post-training stages involving supervised fine-tuning and reinforcement learning from human feedback using direct preference optimization.\n- Both pre-trained and post-trained versions of Hunyuan-Large are released to the open-source community.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/Tencent/Tencent-Hunyuan-Large"
        ],
        "huggingface_urls": [
            "https://huggingface.co/tencent/Tencent-Hunyuan-Large"
        ],
        "date": "2024-11-05"
    },
    {
        "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "authors": "Yang Zhao, Zhijie Lin, Rui Lu, Bingyi Kang, Yang130",
        "link": "https://arxiv.org/abs/2411.02385",
        "github_repo": null,
        "summary": "- This paper investigates the ability of video generation models, specifically diffusion-based models like Variational Auto-Encoder (VAE) and Diffusion Transformer (DiT), to learn fundamental physical laws from visual data without human priors.\n- The study uses a 2D physics engine to generate synthetic datasets of videos governed by classical mechanics laws, enabling quantitative evaluation of model performance across in-distribution, out-of-distribution (OOD), and combinatorial generalization scenarios.\n- Scaling experiments show near-perfect in-distribution generalization but a failure to improve in OOD scenarios, suggesting scaling alone is insufficient for learning physical laws.\n- The analysis suggests the models generalize through case-based reasoning, prioritizing color > size > velocity > shape when matching to training data rather than abstracting underlying rules.\n-  The limitations of purely visual representations for complete physics modeling are also highlighted, with examples demonstrating how visual ambiguity leads to inaccurate predictions.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "Survey of Cultural Awareness in Language Models: Text and Beyond",
        "authors": "Junho Myung, Arnav Arora, Junyeong Park, jinjh0123, sidicity",
        "link": "https://arxiv.org/abs/2411.00860",
        "github_repo": null,
        "summary": "- This paper surveys efforts to incorporate cultural awareness into text-based and multimodal large language models (LLMs).\n- It defines cultural awareness in LLMs based on definitions from psychology and anthropology and examines methodologies for creating cross-cultural datasets and benchmarks, strategies for cultural inclusion in downstream tasks, and benchmarks for evaluating cultural awareness in LLMs.\n- The survey also discusses ethical implications of cultural alignment, the role of Human-Computer Interaction, and cultural alignment's role in social science research.\n- The paper identifies research gaps in current literature and provides suggestions for future research in areas such as cross-cultural LLMs and automatic context detection.\n- It organizes and compares efforts in incorporating culture into NLP and spans several modalities like image, video, audio and text.",
        "classification": [
            "Natural Language Processing",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
        "authors": "Quang Pham, Van Nguyen, Luong Tran, doantienthongbku, DavidNguyen",
        "link": "https://arxiv.org/abs/2411.00918",
        "github_repo": null,
        "summary": "- This paper introduces LibMoE, a comprehensive and modular framework designed to streamline the research, training, and evaluation of Mixture of Experts (MoE) algorithms in Large Language Models (LLMs).\n- LibMoE facilitates easier access to MoE research for a wider range of researchers by standardizing the training and evaluation process, and by reducing the computational cost via sparse upcycling from pre-trained LLMs.\n- The authors benchmark five state-of-the-art MoE algorithms with three model configurations across eleven datasets under a zero-shot setting.\n- Results show that all MoE algorithms achieve roughly similar performance when averaged across a variety of tasks.\n- Further analysis suggests the potential benefits of early stopping and the importance of balanced expert utilization in MoE models.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity",
        "authors": "Chaojun Xiao, Yingfa Chen, Chenyang Song, Yuqi Luo, SillyXu",
        "link": "https://arxiv.org/abs/2411.02335",
        "github_repo": null,
        "summary": "- This paper investigates activation sparsity in Large Language Models (LLMs), proposing a new metric called PPL-p% sparsity.\n- PPL-p% sparsity is performance-aware, versatile across activation functions, and precisely identifies weakly contributing neurons, improving upon existing metrics like CETT.\n- Through extensive experiments, the research reveals scaling laws relating activation sparsity to training data, activation function, width-depth ratio, and parameter scale.\n- ReLU activation is found to be superior to SiLU due to greater sparsity and comparable performance, with deeper models exhibiting higher sparsity below a certain bottleneck.\n- Notably, the limit of activation sparsity shows weak correlation with parameter scale, suggesting that activation patterns in LLMs are scale-insensitive.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/thunlp/SparsingLaw"
        ],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "GenXD: Generating Any 3D and 4D Scenes",
        "authors": "Linjie Li, Zhiwen Yan, Kevin Lin, Chung-Ching Lin, Yuyang Zhao",
        "link": "https://arxiv.org/abs/2411.02319",
        "github_repo": null,
        "summary": "- GenXD, a unified model, generates high-quality 3D and 4D scenes from any number of input images, leveraging multiview-temporal modules to disentangle camera and object motion and masked latent conditioning to support flexible conditioning views.\n- A novel data curation pipeline extracts camera poses and object motion strength from videos, leading to the creation of CamVid-30K, a large-scale 4D scene dataset.\n- GenXD integrates the latent diffusion model with multiview-temporal layers, including ResBlocks and transformers, to disentangle and fuse spatial and temporal information using an \u03b1-fusing strategy.\n- GenXD utilizes masked latent conditioning for image inputs, accommodating various input views without network modification and supporting both single and multi-view image/video generation within a single model.\n- Evaluations across real-world and synthetic datasets demonstrate GenXD's superior performance in single/multi-view 3D and 4D generation compared to existing methods, as evidenced by improved FID, FVD, PSNR, SSIM, and LPIPS scores.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/black-forest-labs/FLUX.1-schnell"
        ],
        "date": "2024-11-05"
    },
    {
        "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
        "authors": "Ryan A. Rossi, Seunghyun Yoon, Viet Dac Lai, Dang Nguyen, Franck-Dernoncourt",
        "link": "https://arxiv.org/abs/2411.01747",
        "github_repo": "https://github.com/adobe-research/dynasaur",
        "summary": "- DynaSaur is a novel LLM agent framework that dynamically creates and composes actions, represented as Python functions, enabling the agent to operate beyond a predefined action set.\n- At each step, the agent generates Python code to perform an action, accumulating these generated actions for reuse in future steps, enhancing flexibility and efficiency.\n- This framework outperforms existing methods on the GAIA benchmark, demonstrating its effectiveness in complex, long-horizon tasks.\n- Notably, DynaSaur allows the agent to recover from scenarios where the predefined action set is insufficient or existing actions fail due to unforeseen circumstances.\n- The dynamic action creation and accumulation capabilities enable the LLM agent to interact with various tools and systems, enhancing its ability to solve a diverse range of tasks.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/adobe-research/dynasaur"
        ],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
        "authors": "Menglin Jia, Ding Liu, Sen He, Haozhe Liu, kumarak",
        "link": "https://arxiv.org/abs/2411.02397",
        "github_repo": null,
        "summary": " - This paper introduces Adaptive Caching (AdaCache), a training-free method to accelerate video generation using Diffusion Transformers (DiTs).\n - AdaCache caches computations and dynamically allocates resources based on video content, which varies in complexity.\n - A Motion Regularization (MoReg) scheme further enhances AdaCache's efficiency by prioritizing compute allocation based on motion content.\n - Experiments demonstrate significant inference speedups (e.g., up to 4.7\u00d7 on Open-Sora) across multiple video DiT baselines without sacrificing generation quality.\n - AdaCache is a plug-and-play component, making it easily adaptable to existing DiTs.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [
            "https://adacache-dit.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models",
        "authors": "Virginia Smith, Mona Diab, Aashiq Muhamed",
        "link": "https://arxiv.org/abs/2411.00743",
        "github_repo": null,
        "summary": "- Introduces Specialized Sparse Autoencoders (SSAEs), which are designed to capture rare or infrequent features (tail concepts) within specific domains of foundation models.\n- Employs dense retrieval and TracIn reranking as effective methods for selecting training data relevant to the target domain, enabling targeted feature extraction without needing to scale to billions of features.\n- Utilizes Tilted Empirical Risk Minimization (TERM) as a training objective, demonstrating its effectiveness in enhancing tail concept representation in SSAEs compared to standard Empirical Risk Minimization (ERM).\n- Demonstrates through experiments on the Bias in Bios dataset that SSAEs improve interpretability by capturing rare features and significantly increase worst-group classification accuracy (12.5%) when used to remove spurious gender information.\n- Evaluation on downstream perplexity and Lo sparsity metrics shows SSAEs effectively capture domain-specific tail concepts, outperforming standard SAEs trained on general-purpose data.",
        "classification": [
            "Natural Language Processing",
            "Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks",
        "authors": "Muhammad Abdul-Mageed, Fakhraddin Alwajih, Abdellah El Mekki, El Moatez Billah Nagoudi, Gagan Bhatia",
        "link": "https://arxiv.org/abs/2411.01192",
        "github_repo": null,
        "summary": "- This paper introduces Swan, a family of dialect-aware, Arabic-centric, cross-lingual, and cross-cultural embedding models.\n- Swan includes Swan-Small, based on ARBERTv2, and Swan-Large, based on the pretrained Arabic large language model ArMistral.\n- A new comprehensive benchmark suite, ArabicMTEB, is proposed to evaluate the models, covering eight tasks and 94 datasets, including cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance.\n- Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks while maintaining monetary efficiency.\n- Swan-Small also shows strong performance, consistently surpassing Multilingual-E5-base on most Arabic tasks.",
        "classification": [
            "Natural Language Processing",
            "Sentence Similarity",
            "Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-05"
    },
    {
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
        "authors": "Weipeng Chen, Mang Wang, Wen Wang, Zhicheng Dou, Jiejun Tan",
        "link": "https://arxiv.org/abs/2411.02959",
        "github_repo": null,
        "summary": "- HtmlRAG is a novel Retrieval-Augmented Generation (RAG) system that utilizes HTML, instead of plain text, as the format for retrieved knowledge, aiming to preserve structural and semantic information often lost during HTML-to-text conversion.\n- HtmlRAG incorporates HTML cleaning, compression, and a two-step pruning process, involving an embedding model for coarse-grained pruning and a generative model for fine-grained pruning, to address challenges of long input sequences and noisy content.\n- This approach outperforms existing RAG systems, utilizing text-based and Markdown-based post-retrieval processes on six QA datasets, including ASQA, HotpotQA, NQ, TriviaQA, MuSiQue, and ELI5 datasets. \n- Specifically, it improves exact match by up to 4.5% on the NQ Dataset and 8.7% on the MuSiQue dataset when using Llama 3.1 70B instruct model.\n- The results demonstrate the effectiveness of utilizing HTML for knowledge modeling in RAG systems, particularly with powerful LLMs capable of handling complex HTML structures.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/plageon/HtmlRAG"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "LLaMo: Large Language Model-based Molecular Graph Assistant",
        "authors": "Hyunwoo J. Kim, Dohwan Ko, Minseong Bae, Jinyoung Park",
        "link": "https://arxiv.org/abs/2411.00871",
        "github_repo": "https://github.com/mlvlab/LLaMo",
        "summary": "- LLaMo, a Large Language Model-based Molecular graph assistant, integrates a molecular graph encoder and a large language model for instruction-following response generation in the molecular domain.\n- LLaMo uses a multi-level graph projector to abstract representations of each GNN layer and motif representations, bridging the graph encoder and language model.\n- Machine-generated molecular graph instruction data, created through a multi-turn conversation format from molecular descriptions and IUPAC names, are used for instruction tuning.\n- Experimental results demonstrate LLaMo's superior performance in molecular description generation, property prediction, and IUPAC name prediction, outperforming LLM-based models like GPT-4.\n- Ablation studies validate the contribution of the multi-level graph projector and the instruction tuning process.",
        "classification": [
            "Graph Machine Learning",
            "Multimodal",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/mlvlab/LLaMo"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
        "authors": "Shenzhi Wang, Yizeng Han, Bingyi Kang, Yulin Wang, Yang Yue",
        "link": "https://arxiv.org/abs/2411.02359",
        "github_repo": "https://github.com/yueyang130/DeeR-VLA",
        "summary": "- DeeR-VLA dynamically adjusts the size of activated Multimodal Large Language Models (MLLMs) based on situation complexity, improving computational efficiency for robotic tasks.\n- DeeR leverages a multi-exit MLLM architecture allowing early termination of processing once sufficient model capacity is reached for a given input, avoiding redundant computation.\n- The framework includes algorithms to set early-exit criteria based on predefined computational budgets (average/peak FLOPs, GPU memory), enabling adaptability to resource constraints.\n- A tailored training method integrates temporal information within the multi-exit architecture to ensure reasonable action predictions.\n- Evaluation on the CALVIN benchmark shows 5.2-6.5x reduction in LLM computational costs and 2-6x reduction in LLM GPU memory usage without compromising task performance.",
        "classification": [
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/yueyang130/DeeR-VLA"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "Sample-Efficient Alignment for LLMs",
        "authors": "Min Lin, Wee Sun Lee, Chao Du, Changyu Chen, Zichen Liu",
        "link": "https://arxiv.org/abs/2411.01493",
        "github_repo": null,
        "summary": "- This paper introduces SEA (Sample-Efficient Alignment), a Thompson sampling-based algorithm, for aligning Large Language Models (LLMs) with human preferences efficiently, addressing the bottleneck of extensive human feedback requirements in current alignment methods.\n- The approach frames LLM alignment as a contextual dueling bandit problem and emphasizes two key properties for sample efficiency: online interaction and active exploration.\n- SEA leverages an epistemic reward model (deep ensemble of reward models) for posterior sampling, policy-guided search for efficient response selection, and mixed preference learning (combining online user feedback and synthetic feedback from the reward model) to update the LLM policy online.\n- Experimental results across various model scales (1B, 2.8B, 6.9B parameters) and direct preference optimization methods (DPO, IPO, SLiC) show SEA achieves higher win rates against reference responses and significantly better sample efficiency compared to existing baselines, including passive online learning and other active exploration methods. \n- The authors release `oat`, an open-source, distributed learning system designed for online LLM alignment research, aiming to facilitate further studies and fair comparisons in the field.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/sail-sg/oat"
        ],
        "huggingface_urls": [
            "https://huggingface.co/docs/trl/main/en/online_dpo_trainer",
            "https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B"
        ],
        "date": "2024-11-06"
    },
    {
        "title": "DreamPolish: Domain Score Distillation With Progressive Geometry Generation",
        "authors": "Shiyu Huang, Wendi Zheng, Ming Ding, Yean Cheng, GhostCai",
        "link": "https://arxiv.org/abs/2411.01602",
        "github_repo": null,
        "summary": "- DreamPolish is a text-to-3D generation model that produces refined geometry and high-quality textures by leveraging multiple neural representations and a novel score distillation objective called Domain Score Distillation (DSD).\n- It uses a progressive geometry construction approach, starting with NeRF and transitioning to NeuS and DMTet for detailed surface information, and incorporates a surface polishing stage guided by a normal estimation prior.\n- DSD guides neural representations towards a domain in the latent space of pretrained text-to-image models that balances photorealism and stability during texture generation, addressing limitations of previous score distillation methods like SDS, VSD, and BSD.\n- Experimental results demonstrate that DreamPolish outperforms existing state-of-the-art methods in generating 3D objects with polished surfaces and photorealistic textures, as measured by PSNR, SSIM, LPIPS, and CLIP Score.\n- A user study further confirms the superior performance of DreamPolish, with users consistently preferring its generated 3D models over those produced by baseline methods.",
        "classification": [
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge",
        "authors": "Lashaw Salta, Chinmay Agrawal, Catalina Villouta, Andrew Langdon, ksoman",
        "link": "https://arxiv.org/abs/2411.02657",
        "github_repo": null,
        "summary": "- Zebra-Llama, a context-aware large language model specializing in Ehlers-Danlos Syndrome (EDS) information, was developed using a novel context-aware fine-tuning methodology.\n- The model leverages Retrieval-Augmented Generation (RAG) and is trained on a diverse dataset comprising medical literature, patient forums, and clinical resources, structured as question-context-answer triplets.\n- Evaluation on real-world questions from EDS patients and clinicians demonstrated Zebra-Llama's superior performance compared to the base Llama model across thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%), and citation reliability (70.6% vs. 52.3%).\n- A custom RAG API and Jupyter Notebook demo are also released.\n- The model and code are open-sourced to democratize expert-level knowledge in rare disease management.",
        "classification": [
            "Question Answering",
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/karthiksoman/zebra-llama"
        ],
        "huggingface_urls": [
            "https://huggingface.co/zebraLLAMA/zebra-Llama-v0.2"
        ],
        "date": "2024-11-06"
    },
    {
        "title": "Controlling Language and Diffusion Models by Transporting Activations",
        "authors": "Nicholas Apostoloff, Luca Zappella, Michal Klein, Arno Blaas, Pau Rodriguez",
        "link": "https://arxiv.org/abs/2410.23054",
        "github_repo": null,
        "summary": "- This paper introduces Activation Transport (ACT), a framework to steer activations in generative models (GMs) using optimal transport theory.\n- ACT generalizes existing activation steering methods by applying univariate maps to activations while preserving target distributions, improving controllability and robustness. \n- Linear-ACT, an inference-time intervention based on ACT, matches or outperforms other methods in toxicity mitigation, concept induction, and truthfulness in LLMs.\n- ACT effectively controls text-to-image diffusion models for fine-grained style control and concept negation. \n- The authors adapt ITI (Li et al., 2024) for text-to-image and find that ACT with a strength parameter of 1 consistently achieves strong conditioning across tasks and models.",
        "classification": [
            "Natural Language Processing",
            "Text-to-Image",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details",
        "authors": "Zirong Jin, Wanghao Du, Chenghong Li, Haolin Liu, Zhongjin Luo",
        "link": "https://arxiv.org/abs/2411.03047",
        "github_repo": null,
        "summary": "- This paper introduces GarVerseLOD, a new dataset and framework for high-fidelity 3D garment reconstruction from single in-the-wild images.\n- GarVerseLOD contains 6,000 high-quality, hand-crafted 3D garment models with detailed geometry, categorized into five common types, and structured with levels of detail (LOD) to improve learning and inference.\n- The method uses a hierarchical approach, starting with coarse shape estimation and progressively adding pose-induced global deformations and fine-scale local details.\n- A novel data labeling paradigm using conditional diffusion models generates realistic paired images for training.\n- Experimental results demonstrate GarVerseLOD's ability to reconstruct garments with various shapes and intricate deformations, outperforming existing methods in quality and robustness.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "Correlation of Object Detection Performance with Visual Saliency and Depth Estimation",
        "authors": "Dylan Seychell, mbar0075",
        "link": "https://arxiv.org/abs/2411.02844",
        "github_repo": "https://github.com/mbar0075/Object-Detection-Correlation-Saliency-vs-Depth",
        "summary": "- This paper investigates the correlation between object detection accuracy and two visual tasks: depth prediction and visual saliency prediction.\n- Experiments using models like DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model on COCO and Pascal VOC datasets reveal that visual saliency has a stronger correlation with object detection accuracy (mAp up to 0.459 on Pascal VOC) than depth prediction (mAp up to 0.283).\n- The correlation varies across object categories, with larger objects showing stronger correlations than smaller objects (up to three times higher).\n- The findings suggest that incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories.\n- The category-specific variations also offer insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.",
        "classification": [
            "Object Detection",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/mbar0075/Object-Detection-Correlation-Saliency-vs-Depth"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level",
        "authors": "Albert Thomas, Giuseppe Paolo, James Doran, Alexandre Maraval, Antoine Grosnit",
        "link": "https://arxiv.org/abs/2411.03562",
        "github_repo": null,
        "summary": "- Introduced Agent K v1.0, an end-to-end autonomous data science agent capable of automating the entire data science lifecycle, including task setup, solution generation, and submission to Kaggle competitions.\n- Employs a structured reasoning framework with a memory module for experience-based learning, enabling adaptation without retraining or backpropagation.\n- Achieved a 92.5% success rate in automating tasks across multiple modalities (tabular, computer vision, NLP, multimodal).\n- Ranked in the top 38% when compared against almost 6000 human competitors on Kaggle and reached a performance equivalent to Kaggle Grandmaster, winning 6 gold, 3 silver, and 7 bronze medals across diverse challenges.\n- Proposed a novel evaluation methodology and competitive benchmark using real-world Kaggle competitions to rigorously assess agent capabilities.",
        "classification": [
            "Natural Language Processing",
            "Tabular",
            "Computer Vision",
            "Multimodal",
            "Image Classification",
            "Tabular Classification",
            "Tabular Regression",
            "Time Series Forecasting"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-07"
    },
    {
        "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
        "authors": "Benyou Wang, Lichao Sun, Shunian Chen, Sicheng Lai, Dingjie Song",
        "link": "https://arxiv.org/abs/2411.03823",
        "github_repo": null,
        "summary": "- This paper introduces MM-Detect, a framework for detecting data contamination in Multimodal Large Language Models (MLLMs).\n- MM-Detect employs two methods: Option Order Sensitivity Test for multiple-choice questions and Slot Guessing for Perturbation Captions for caption-based questions. \n- Experiments on eleven MLLMs and five VQA datasets reveal varying degrees of contamination across models, impacting performance. \n- The study also finds that data leakage can originate from both the pre-training phase of the base LLMs and the multimodal fine-tuning phase. \n-  The results indicate that MM-Detect can identify contamination and demonstrate that training set leakage leads to performance inflation, creating unfair comparisons.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/MLLM-Data-Contamination/MM-Detect"
        ],
        "huggingface_urls": [],
        "date": "2024-11-07"
    },
    {
        "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
        "authors": "Jiaran Hao, Jason Klein Liu, Tianhao Cheng, Siming Huang, Zenithwang",
        "link": "https://arxiv.org/abs/2411.04905",
        "github_repo": null,
        "summary": "- OpenCoder, a top-tier code large language model (LLM) designed for code generation, reasoning, and agent systems, is introduced, boasting performance comparable to leading models while offering full transparency through the release of its training data, processing pipeline, and protocols.\n- OpenCoder's key ingredients for success include code-optimized heuristic rules for data cleaning and deduplication, incorporation of code-related text corpora, and utilization of high-quality synthetic data in annealing and fine-tuning.\n- The model architecture for OpenCoder is available in 1.5B and 8B parameter sizes, leveraging SwiGLU activation, RoPE positional embedding, and a vocabulary size of 96,640, with variations in layers, attention heads, and context window size between the two.\n- OpenCoder's training involves a two-stage instruction-tuning process: the first focuses on theoretical computer science question-answer pairs, while the second refines practical coding skills using high-quality code from GitHub.\n- Evaluation on benchmarks like HumanEval, MBPP, BigCodeBench, LiveCodeBench, and MultiPL-E reveals OpenCoder surpasses all previous fully open models and other open-access models at the 6B+ parameter scale.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/yuxiang630/hqcode"
        ],
        "date": "2024-11-08"
    },
    {
        "title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
        "authors": "David E. Jacobs, Nikhil Karnad, Shiran Zada, Roni Paiss, David Junhao Zhang",
        "link": "https://arxiv.org/abs/2411.05003",
        "github_repo": null,
        "summary": "- ReCapture, a novel method for generating videos with new camera trajectories from single user-provided videos, preserving original content and motion.\n- Employs a two-stage process: generating a noisy anchor video with the desired trajectory using either point cloud rendering or multiview diffusion, followed by refining it using masked video fine-tuning.\n- Masked video fine-tuning trains a temporal motion LoRA and context-aware spatial LoRA on known pixels from the anchor and source videos to enhance temporal consistency and complete missing information.\n- Post-processing with SDEdit further refines the output, reducing blur and artifacts.\n- Outperforms Generative Camera Dolly and other 4D reconstruction methods on Kubric and VBench datasets, demonstrating superior performance in generating high-quality videos with novel camera movements.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
        "authors": "Furu Wei, Shuming Ma, Hongyu Wang",
        "link": "https://arxiv.org/abs/2411.04965",
        "github_repo": null,
        "summary": "- BitNet a4.8 introduces 4-bit activations and a hybrid quantization and sparsification strategy for 1-bit Large Language Models (LLMs), aiming to reduce inference costs while maintaining performance comparable to the 1.58-bit BitNet b1.58 model.\n- The model employs 4-bit activations for inputs to attention and feed-forward network layers, and sparsifies intermediate states with 8-bit quantization to mitigate quantization errors caused by outlier channels. \n- It also incorporates a two-stage training approach (from 8-bit to 4-bit activations) for efficiency. \n- Experimental results show BitNet a4.8 achieves comparable performance to BitNet b1.58 with equivalent training costs but faster inference due to enabling INT4/FP4 kernels and supporting 3-bit KV cache.\n- Additionally, BitNet a4.8 activates only 55% of the parameters, further enhancing efficiency for large-scale LLM deployment and inference.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion",
        "authors": "Zilong Chen, Fangfu Liu, Shuo Chen, Wenqiang Sun, yikaiw",
        "link": "https://arxiv.org/abs/2411.04928",
        "github_repo": null,
        "summary": "- DimensionX is a novel framework that generates photorealistic 3D and 4D scenes from a single image using controllable video diffusion.\n- It introduces ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant datasets, enabling precise manipulation of spatial structure and temporal dynamics.\n- A trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation are introduced to bridge the gap between generated videos and real-world scenes. \n- Experimental results on various datasets demonstrate DimensionX's superior performance in controllable video generation, as well as 3D and 4D scene generation, compared to existing methods, as shown quantitatively in Table 1 and Table 2, as well as qualitatively in Figure 4 and Figure 5.\n- The project page can be found at https://chenshuo20.github.io/DimensionX/.",
        "classification": [
            "Text-to-3D",
            "Text-to-Video",
            "Image-to-3D",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
        "authors": "Ning Dong, Srinivasan Iyer, Liang Luo, Lili Yu, WxWx",
        "link": "https://arxiv.org/abs/2411.04996",
        "github_repo": null,
        "summary": "- This paper introduces Mixture-of-Transformers (MoT), a sparse multimodal transformer architecture designed to reduce the computational costs of pretraining large multimodal models.\n- MoT decouples the non-embedding parameters of the model by modality, including feed-forward networks, attention matrices, and layer normalization, enabling modality-specific processing with global self-attention over the full input sequence.\n- In the Chameleon 7B setting (autoregressive text and image generation), MoT matches the dense baseline performance using only 55.8% of the FLOPs.  With speech added, MoT reaches comparable speech performance using 37.2% of the FLOPs. \n- In the Transfusion setting, which uses multi-objective training with autoregressive text and diffusion-based image generation, a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics; a 7B MoT matches the image performance of the dense baseline with one-third of the FLOPs. \n- System profiling shows MoT achieves dense baseline image quality in 47.2% of the wall-clock time and text quality in 75.6% of the wall-clock time.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text",
            "Text Generation",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
        "authors": "Yi Yang, Wenhao Wang",
        "link": "https://arxiv.org/abs/2411.04709",
        "github_repo": null,
        "summary": "- This paper introduces TIP-I2V, a large-scale dataset of over 1.7 million text and image prompts specifically designed for image-to-video generation, accompanied by generated videos from five state-of-the-art models.\n- The dataset is sourced from Pika Discord channels and includes additional information like UUIDs, timestamps, embeddings, subjects, and NSFW scores.\n- Analysis reveals that TIP-I2V's prompts, which focus on animating existing image content, differ semantically from those in text-to-video and text-to-image datasets.\n- This dataset enables research into user preferences, improved model evaluation, misinformation detection, and source image tracing.\n- Initial benchmarks using TIP-I2V indicate that even early commercial image-to-video models can outperform open-source alternatives in key areas, emphasizing the importance of real-world user data.",
        "classification": [
            "Image-to-Video",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [
            "https://tip-i2v.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model",
        "authors": "Ho-Jin Choi, Kyeongjin Oh, Junyoung Youn, Dokyong Lee, Young-Jun Lee",
        "link": "https://arxiv.org/abs/2411.04496",
        "github_repo": "https://github.com/passing2961/Thanos",
        "summary": "- This paper introduces THANOS, a family of large language models (LLMs) designed to improve the quality of responses generated by conversational agents by infusing them with \"skill-of-mind.\"\n- Skill-of-mind is a process that involves considering social context, interpreting dialogue situations, planning an appropriate skill strategy, and selecting the most effective conversational skill for a given response. \n- The authors also created MULTIFACETED SKILL-OF-MIND, a dataset of 100k conversations annotated with explanations and conversational skills, to train THANOS. \n- Experimental results indicate that THANOS effectively predicts conversational skills and enhances response quality in various scenarios, promoting prosocial behavior in human evaluations. \n- This improvement is demonstrated by incorporating the generated skill-of-mind as input for LLM-based conversational agents, leading to better response quality.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/passing2961/Thanos"
        ],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation",
        "authors": "Chris Paxton, Soumith Chintala, Mohit Warke, Zhanqiu Guo, Peiqi Liu",
        "link": "https://arxiv.org/abs/2411.04999",
        "github_repo": null,
        "summary": "- DynaMem, a novel dynamic spatio-semantic memory architecture for open-world mobile manipulation, is introduced, enabling robots to adapt to changing environments.\n- DynaMem maintains and updates a voxelized pointcloud of the environment, incorporating object additions and removals, and supports object localization queries using both Vision-Language Model features and Multimodal Large Language Model question answering.\n- In real-world experiments on Stretch SE3 robots across various dynamic scenes, DynaMem achieves a 70% pick-and-drop success rate for non-stationary objects, more than double the performance of static systems.\n- A new dynamic benchmark, DynaBench, is introduced to evaluate dynamic spatio-semantic memory algorithms in 9 changing environments, and ablation studies demonstrate the effectiveness of key design choices.\n- DynaMem handles object permanence, going beyond existing systems that often return incorrect matches when the queried object is absent.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?",
        "authors": "Samuel Albanie, Kai Han, Jonathan Roberts",
        "link": "https://arxiv.org/abs/2411.05000",
        "github_repo": null,
        "summary": "- This paper introduces a new set of retrieval experiments to evaluate the long-context capabilities of Large Language Models (LLMs), called needle threading tasks.\n- These tasks involve following threads of linked information across different parts of the context and retrieving the final value, including single and multiple needle retrieval, conditional needle retrieval, threading and multi-threading, and branched threading variations.\n- The study evaluates 17 LLMs on these tasks using synthetically generated key-value pairs of UUIDs and finds that increased context length negatively impacts performance but concurrent threading is largely unaffected by concurrent queries.\n- It suggests the LLMs' \"effective\" context limit is shorter than stated due to performance degradation at longer context lengths.  \n- The paper introduces a task-specific and model-agnostic effective context limit metric and publicly releases the code and experimental data.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval",
        "authors": "Subhankar Maity, Aniket Deroy",
        "link": "https://arxiv.org/abs/2411.04752",
        "github_repo": null,
        "summary": "- This paper introduces RetrieveGPT, a novel approach for enhancing information retrieval from code-mixed conversations, particularly focusing on Roman transliterated Bengali mixed with English.\n- The approach uses GPT-3.5 Turbo with carefully designed prompts to evaluate document relevance to a given query, considering the sequential nature of conversations.\n- A mathematical model integrates GPT-3.5 Turbo's output, accounting for sequential dependencies among documents to determine relevance.\n- The model treats relevance detection as a problem of finding the optimal relevance chain across a sequence of documents.\n- Experiments on a Facebook dataset with Query Relevance files (QRels) demonstrate the effectiveness of the approach in extracting information from complex, code-mixed conversations.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation",
        "authors": "Igor Gilitschenski, Yash Kant, Ziyi Wu, Sherwin Bahmani, Koichi Namekata",
        "link": "https://arxiv.org/abs/2411.04989",
        "github_repo": null,
        "summary": "- SG-I2V, a self-guided, zero-shot framework for controllable image-to-video generation, is introduced, leveraging pre-trained video diffusion models without fine-tuning.\n- The method enables control over object motion and camera dynamics by aligning feature maps from self-attention layers and optimizing latent representations based on user-specified bounding box trajectories.\n- A high-frequency preservation post-processing step ensures quality by retaining original high-frequency noise.\n- Experimental results on the VIPSeg dataset demonstrate competitive motion fidelity and visual quality compared to supervised baselines and outperformance over zero-shot baselines.\n- The framework offers versatile control, animating diverse objects and camera motions while retaining object identity, as demonstrated in qualitative examples.",
        "classification": [
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/kmcodel/Projects/SG-I2V"
        ],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models",
        "authors": "Xiuyu Li, Tianle Cai, Zhekai Zhang, Yujun Lin, Muyang Li",
        "link": "https://arxiv.org/abs/2411.05007",
        "github_repo": "https://github.com/mit-han-lab/deepcompressor",
        "summary": "- SVDQuant, a novel post-training 4-bit quantization method for diffusion models, is introduced, which leverages a low-rank branch to absorb outliers in both weights and activations, thereby mitigating performance degradation from aggressive quantization.\n- A co-designed inference engine, Nunchaku, fuses the low-rank and 4-bit computation kernels to minimize overhead from additional memory access, enabling measured speedup even with extra branches and seamless integration with Low-Rank Adaptation (LoRA) without requiring re-quantization.\n- On a 12B parameter DiT model (FLUX.1), SVDQuant achieves 3.6x model size reduction and 3.0x speedup compared to a weight-only 4-bit quantized baseline on NVIDIA RTX 4090 GPUs. \n- It also demonstrates superior visual quality on various architectures like SDXL and PixArt-\u03a3, including both UNet and DiT backbones.\n- The method supports INT4 and FP4 data types and achieves a 10.1x speedup on a 16GB laptop RTX 4090 by eliminating the need for CPU offloading.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/mit-han-lab/deepcompressor",
            "https://github.com/mit-han-lab/nunchaku"
        ],
        "date": "2024-11-08"
    },
    {
        "title": "GazeGen: Gaze-Driven User Interaction for Visual Content Generation",
        "authors": "Kao-Den Chang, Wei-Te Mark Ting, Sai Qian Zhang, Ziyun Li, He-Yen Hsieh",
        "link": "https://arxiv.org/abs/2411.04335",
        "github_repo": null,
        "summary": "- GazeGen, a novel multimodal system, empowers users to generate and edit visual content (images and videos) by leveraging their gaze.\n- The core component is DFT Gaze, a lightweight gaze estimation model (281K parameters) derived from ConvNeXt V2-A through knowledge distillation and personalized with Adapters for real-time performance on edge devices.\n- GazeGen integrates DFT Gaze with object detection (using FastSAM and YOLOv9) and generative AI (Stable Diffusion) for gaze-driven addition, deletion, repositioning, material transfer of objects, and animation of images into videos.\n- Evaluation on AEA and OpenEDS2020 datasets showcases DFT Gaze's accuracy with low angular gaze error, outperforming existing real-time gaze estimation methods on edge devices.\n- The system simplifies complex visual editing tasks, enhancing user experience and accessibility in AR by allowing intuitive, hands-free content manipulation.",
        "classification": [
            "Computer Vision",
            "Object Detection",
            "Image-to-Video",
            "Text-to-Image",
            "Text-to-Video",
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-08"
    },
    {
        "title": "VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos",
        "authors": "Eric Xing, Jiale Cao, Wenqi Zhu, Hanan Gani, Shehan Munasinghe",
        "link": "https://arxiv.org/abs/2411.04923",
        "github_repo": null,
        "summary": "- VideoGLaMM, a large multimodal model designed for pixel-level visual grounding in videos, is introduced.\n- The model architecture comprises a Large Language Model (LLM), dual vision encoders (for spatial and temporal features), and a spatio-temporal decoder connected via tunable Vision-to-Language (V\u2192L) and Language-to-Vision (L\u2192V) adapters. \n- VideoGLaMM is trained on a new dataset of grounded conversation video question-answer triplets which include segmentation masks generated using a semi-automatic annotation pipeline. \n- The new dataset includes 38k video-QA triplets, 83k objects and 671k masks. \n- Experimental results demonstrate state-of-the-art performance on grounded conversation generation, visual grounding, and referring video segmentation tasks, outperforming existing approaches.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering",
            "Mask Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://mbzuai-oryx.github.io/VideoGLaMM"
        ],
        "date": "2024-11-08"
    },
    {
        "title": "Balancing Pipeline Parallelism with Vocabulary Parallelism",
        "authors": "Min Lin, Penghui Qi, Man Tsung Yeung, ufotalent",
        "link": "https://arxiv.org/abs/2411.05288",
        "github_repo": "https://github.com/sail-sg/VocabularyParallelism",
        "summary": "- This paper introduces Vocabulary Parallelism (VP), a novel technique designed to address computational and memory imbalances stemming from vocabulary layers in pipeline parallelism, a common strategy for training large language models.\n- VP partitions vocabulary layers across all pipeline devices and integrates the computation as passes within the pipeline schedule, similar to the handling of transformer layers.  The approach involves algorithms to reduce communication barriers within these vocabulary passes, thereby minimizing activation memory overhead.\n- When combined with memory-balanced pipeline schedules like V-Half, the proposed method achieves near-perfect balance in both memory and computation.\n- Experimental results show improvements in throughput ranging from 5% to 51% compared to existing methods, particularly under large vocabulary scenarios, with significant reductions in peak memory usage.\n- Notably, the benefits extend to various vocabulary and model sizes, demonstrating the robustness and generalizability of the approach.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/sail-sg/VocabularyParallelism"
        ],
        "huggingface_urls": [],
        "date": "2024-11-11"
    },
    {
        "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
        "authors": "Kaiwen Xiao, Zhongkai Wu, Wang Zhao, Yanning Zhou, Yuze He",
        "link": "https://arxiv.org/abs/2411.05738",
        "github_repo": null,
        "summary": "- StdGEN is a novel pipeline for generating semantically decomposed 3D characters from single images, utilizing a Semantic-aware Large Reconstruction Model (S-LRM).\n- S-LRM, a transformer-based model, jointly reconstructs geometry, color, and semantics from multi-view images, enabling efficient and effective decomposition.\n- StdGEN integrates a multi-view diffusion model for generating multi-view consistent RGB and normal maps, followed by S-LRM for reconstruction and a mesh refinement module for enhancing quality.\n- Experiments on Anime3D++ dataset demonstrate state-of-the-art performance in 3D anime character generation, surpassing existing baselines in geometry, texture, and decomposability.\n- The decomposable nature of the generated characters and the efficient pipeline unlock potential for various downstream applications.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-11"
    },
    {
        "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
        "authors": "Marina Danilevksy, Lucian Popa, Krishna Killamsetty, ishikaa",
        "link": "https://arxiv.org/abs/2411.04425",
        "github_repo": null,
        "summary": "- DELIFT (Data Efficient Language Model Instruction Fine-Tuning) is a novel algorithm that optimizes data selection across three key stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning.\n- It utilizes a pairwise utility metric to quantify the value of a data sample in improving the model's responses to other samples and leverages submodular functions for optimal subset selection.\n- DELIFT reduces fine-tuning data size by up to 70% without compromising performance, leading to significant computational savings. \n- It outperforms current data selection techniques by up to 26% across diverse tasks and model scales.\n- Experiments show minimal performance drops compared to using full datasets and even surpasses full dataset performance in niche tasks such as query rewriting.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://anonymous.4open.science/r/optimizing-data-selection-0CD0"
        ],
        "huggingface_urls": [],
        "date": "2024-11-11"
    },
    {
        "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
        "authors": "Jingyue Li, andstor",
        "link": "https://arxiv.org/abs/2411.02462",
        "github_repo": null,
        "summary": "- This paper conducts an empirical study on Parameter-Efficient Fine-Tuning (PEFT) methods for Large Language Models (LLMs) in generating unit tests for code.\n- It evaluates full fine-tuning, LoRA, (IA)\u00b3, and prompt tuning across various open-source LLMs (CodeGen, Code Llama, StarCoder) ranging from 350 million to 16 billion parameters and use well-established unit-test datasets for evaluation.\n- The findings indicate that LoRA generally performs best, matching or exceeding full fine-tuning's performance in several cases while using fewer parameters, whereas prompt tuning is the most resource-efficient but has more variable performance.\n- They evaluate using codebleu to evaluate the similarity between the generated tests and the reference tests.\n-  The results also show that both full fine-tuning and PEFT methods are mostly resistant to catastrophic forgetting, sometimes even improving the code generation capabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/andstor/methods2test_small",
            "https://huggingface.co/andstor/peft-unit-test-generation-experiments"
        ],
        "date": "2024-11-11"
    },
    {
        "title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation",
        "authors": "Yuqing Yang, Xufang Luo, Aoqi Wu, Weiquan Huang, Yif29",
        "link": "https://arxiv.org/abs/2411.04997",
        "github_repo": null,
        "summary": "- LLM2CLIP is a novel approach that integrates Large Language Models (LLMs) into Contrastive Language-Image Pre-training (CLIP) to enhance visual representation learning.\n- It addresses the limitations of LLMs' output features by applying a caption contrastive fine-tuning strategy, which increases their discriminability and enables them to act as a more powerful teacher for CLIP's visual encoder.\n- This approach improves CLIP's ability to handle longer and more complex captions and incorporate richer knowledge from LLMs, without significant computational overhead.\n- LLM2CLIP demonstrates significant improvements across various cross-modal tasks, exceeding previous state-of-the-art models like EVA02 by a substantial margin on text and image retrieval benchmarks, and even enabling cross-lingual capabilities for models trained solely on English data.\n- It also shows promising results in multimodal training with models like LLaVA 1.5, consistently outperforming the original CLIP across several benchmarks.",
        "classification": [
            "Multimodal",
            "Image Feature Extraction",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://aka.ms/llm2clip"
        ],
        "date": "2024-11-11"
    },
    {
        "title": "Improving the detection of technical debt in Java source code with an enriched dataset",
        "authors": "Rick Kazman, Davide Di Ruscio, Phuong T. Nguyen, Anh M. T. Bui, Nam Le Hai",
        "link": "https://arxiv.org/abs/2411.05457",
        "github_repo": null,
        "summary": "- This paper introduces TESORO, a new dataset for detecting technical debt (TD) in Java source code, which includes both comments and the corresponding source code.\n- A pipeline is proposed for enriching technical debt data by extracting Self-Admitted Technical Debts (SATD) comments and their associated source code units.\n- The study demonstrates that incorporating source code context enhances the performance of state-of-the-art SATD detection models, and an ensemble approach combining predictions from different code context lengths yields even better results.\n- The paper investigates the accuracy of different pre-trained language models (PLMs) in detecting TD solely from source code, revealing the superior performance of CodeBERT and its variant GraphCodeBERT, and highlighting the potential of LLMs in this task.\n- The curated TESORO dataset is expected to catalyze future research in the domain of TD detection and facilitate the identification of other software artifacts such as code smells.",
        "classification": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [
            "https://github.com/NamCyan/tesoro"
        ],
        "huggingface_urls": [],
        "date": "2024-11-11"
    },
    {
        "title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models",
        "authors": "Gal Chechik, Lior Wolf, Dvir Samuel Yuval Atzmon, Rinon Gal, Yoad Tewel",
        "link": "https://arxiv.org/abs/2411.07232",
        "github_repo": null,
        "summary": "- Add-it is a training-free method for adding objects into images using a simple text prompt and a pretrained diffusion model like Stable Diffusion.\n- It extends the multi-modal attention mechanism by incorporating information from a source image, and dynamically weights the attention from the source image, target image, and text prompt.\n- The method uses structure transfer from the source image by noising the source and denoising it with the target, and applies subject-guided latent blending using SAM-2 to maintain existing structures from the source image.\n- Add-it outperforms existing methods on image insertion benchmarks for both real and generated images, showing a significant increase in object placement plausibility.\n- Human evaluation further shows Add-it is preferred by users against other methods, indicating more realistic and natural-looking results. ",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision",
        "authors": "Xinrun Du, Weiming Ren, Zheyang Xiong, Cong Wei, wenhu",
        "link": "https://arxiv.org/abs/2411.07199",
        "github_repo": null,
        "summary": "- OMNI-EDIT is a novel instruction-based image editing model trained using a specialist-to-generalist framework with supervision from seven specialized editing models covering tasks like object manipulation, style transfer, and background changes.\n- It leverages a diffusion-transformer architecture called EditNet to facilitate interaction between the control branch (editing instructions) and the original branch (image content), enabling more effective and nuanced edits.\n- Importance sampling based on scores from large multimodal models like GPT-40, distilled to InternVL2, is used to enhance the quality of the training data by filtering out noisy or low-quality generated samples.\n- OMNI-EDIT handles images of any aspect ratio and high resolution and outperforms existing models on a curated benchmark called OMNI-EDIT-BENCH, demonstrating superior performance in perceptual quality, semantic consistency, and adherence to editing instructions.\n- Both automatic and human evaluations show significant improvement, with a 20% overall improvement observed over the best baseline editing model (CosXL-Edit) in human evaluation.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models",
        "authors": "Hui Huang, Yingshui Tan, Jiaheng Liu, Shilong Li, Yancheng He",
        "link": "https://arxiv.org/abs/2411.07140",
        "github_repo": null,
        "summary": "- This paper introduces Chinese SimpleQA, a benchmark designed to evaluate the factuality of Large Language Models (LLMs) in answering short questions in Chinese.\n- The dataset consists of 3000 high-quality question-answer pairs across six diverse topics, with a focus on static answers that do not change over time.\n- The questions and answers in Chinese SimpleQA are short, simplifying evaluation with existing LLMs, and the grading process employs the OpenAI API.\n- Initial findings indicate that Chinese SimpleQA is challenging for existing LLMs, with only a few achieving passing scores.\n- The research demonstrates the importance of model size, calibration, and the effectiveness of Retrieval-Augmented Generation (RAG) strategies in improving LLM performance in Chinese factuality.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models",
        "authors": "Tiffany Cai, Yogesh Balaji, Maciej Bala, Yuval Atzmon, NVIDIA",
        "link": "https://arxiv.org/abs/2411.07126",
        "github_repo": null,
        "summary": "- Edify Image is a family of cascaded pixel-space diffusion models that generate photorealistic high-resolution images with enhanced controllability, using a novel multi-scale Laplacian diffusion process.\n- The Laplacian Diffusion Model simulates resolution-varying diffusion by attenuating different image frequency bands at varying rates, enabling precise detail capture and refinement across multiple scales.\n- Edify Image supports various applications, including text-to-image synthesis with long prompts and camera controls, 4K upsampling with a ControlNet-based approach, ControlNets for various modalities, 360\u00b0 HDR panorama generation via sequential inpainting, and finetuning for image customization.\n- The models achieve high fidelity and diversity in image generation, demonstrated by results across various aspect ratios, camera controls, long prompts, and different subject categories.\n- Finetuning experiments show the model's adaptability to personalization and stylization tasks, while maintaining compatibility with pre-trained ControlNets.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization",
        "authors": "Yongbin Li, Fei Huang, Cheng Fu, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2411.06208",
        "github_repo": null,
        "summary": "- This paper introduces IOPO (Input-Output Preference Optimization), a new alignment method for Large Language Models (LLMs) that aims to improve complex instruction following.\n- Unlike existing methods like DPO that focus on output preference, IOPO considers both input and output preferences, enabling LLMs to better understand fine-grained constraints within complex instructions.\n- A new benchmark called TRACE, comprising 120K training and 1K evaluation instances with varying constraint complexities, is also introduced to facilitate training and evaluation of complex instruction following abilities.\n- Experiments on TRACE, IFEval, and CFBench demonstrate IOPO's effectiveness, showing improvements of 2.18% and 3.13% over DPO on in-domain and out-of-domain datasets, respectively.\n- Ablation studies confirm that both input and output preference modeling are crucial for instruction following, especially in complex scenarios.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
        "authors": "Maojia Song, Chaoqun Liu, Hou Pong Chan, Liying Cheng, Yew Ken Chia",
        "link": "https://arxiv.org/abs/2411.06176",
        "github_repo": null,
        "summary": "- Introduces M-LongDoc, a benchmark dataset with 851 samples for evaluating multimodal long document understanding.\n- Proposes a retrieval-aware tuning approach to improve model performance in document question answering.\n- Presents an automated evaluation framework leveraging multiple judge models to assess the correctness of open-ended solutions.\n- Demonstrates that the retrieval-aware tuning approach achieves a 4.6% relative improvement in answer correctness compared to baseline open-source models.\n- Finds that existing models struggle with figure and table-based questions, highlighting a multimodal bias.",
        "classification": [
            "Multimodal",
            "Document Question Answering"
        ],
        "github_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "Watermark Anything with Localized Messages",
        "authors": "Matthijs Douze, Teddy Furon, Alain Durmus, Pierre Fernandez, Tom Sander",
        "link": "https://arxiv.org/abs/2411.07231",
        "github_repo": "https://github.com/facebookresearch/watermark-anything",
        "summary": "- This paper introduces the Watermark Anything Model (WAM), a novel deep learning model for localized image watermarking.\n- WAM employs an encoder-decoder architecture for embedding and a ViT encoder with a pixel decoder for extracting watermarks, enabling the segmentation of watermarked regions and the recovery of multiple distinct messages from localized areas within an image. \n- The model is trained in two stages: the first stage focuses on robustness at low resolution, while the second refines for imperceptibility and multiple watermark handling using JND modulation and multi-mask training. \n- WAM demonstrates competitive performance against existing methods in terms of imperceptibility and robustness, specifically against inpainting and splicing attacks.\n- Notably, WAM showcases new capabilities in localizing watermarked areas in spliced images and extracting multiple 32-bit messages with high accuracy from small regions, exceeding the performance of traditional global watermarking techniques.",
        "classification": [
            "Computer Vision",
            "Image Segmentation"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/watermark-anything"
        ],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "Counterfactual Generation from Language Models",
        "authors": "Ryan Cotterell, Anej Svete, vesteinn, Shauli",
        "link": "https://arxiv.org/abs/2411.07180",
        "github_repo": null,
        "summary": "- This paper proposes a framework for generating counterfactual text from language models (LMs) by treating them as Generalized Structural Equation Models (GSEMs) and using the Gumbel-max trick.\n- This approach allows for modeling the joint distribution of original and counterfactual strings, enabling investigation of cause-and-effect relationships within LMs.\n- An algorithm based on hindsight Gumbel sampling infers the noise variables that produced an observed string, facilitating the generation of its counterfactual.\n- Experiments on GPT2-XL and LLaMA3-8b, with interventions like knowledge editing and linear steering, reveal unintended side effects, showing that these techniques are not as surgical as intended.\n- This work highlights the need for more precise intervention methods in LMs to minimize unintended changes in generated text.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/shauli-ravfogel/lm-counterfactuals"
        ],
        "huggingface_urls": [
            "https://huggingface.co/intfloat/e5-base-v2",
            "https://huggingface.co/jujipotle/honest_llama3_8B_instruct",
            "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
        ],
        "date": "2024-11-12"
    },
    {
        "title": "Game-theoretic LLM: Agent Workflow for Negotiation Games",
        "authors": "Julie Chen, Alfonso Amayuelas, Lingyao Li, Ollie Liu, Wenyue Hua",
        "link": "https://arxiv.org/abs/2411.05990",
        "github_repo": "https://github.com/Wenyueh/game_theory",
        "summary": "- This paper introduces game-theory-inspired workflows to enhance the rationality of Large Language Models (LLMs) in strategic decision-making, particularly within negotiation games.\n- The authors evaluate several state-of-the-art LLMs across various complete and incomplete information games and find that LLMs often deviate from rational strategies, especially in complex games.\n- They design distinct workflows incorporating principles like dominant strategy search, backward induction, and Bayesian belief updating to guide LLMs' reasoning and improve decision-making.\n- Experimental results demonstrate that these workflows significantly improve LLM performance in identifying optimal strategies, achieving near-optimal allocations, and reducing susceptibility to exploitation during negotiations.\n-  The paper further explores the meta-strategic considerations of workflow adoption, highlighting a novel research direction in analyzing the rationality of using such workflows in different scenarios.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/Wenyueh/game_theory"
        ],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models",
        "authors": "Yiyan Qi, Zhouchi Lin, Huanyi Su, Junxi Liu, Xiaojun Wu",
        "link": "https://arxiv.org/abs/2411.06272",
        "github_repo": "https://github.com/IDEA-FinAI/Golden-Touchstone",
        "summary": "- This paper introduces Golden Touchstone, a bilingual (English and Chinese) benchmark for evaluating Financial Large Language Models (FinLLMs).\n- The benchmark covers eight core financial NLP tasks, including sentiment analysis, question answering, and summarization, with 22 datasets in total. \n- The authors also release Touchstone-GPT, a FinLLM trained using continuous pre-training and financial instruction tuning on a 100B token dataset and 300k instruction-response pairs.\n- Evaluation results on Golden Touchstone show that while existing LLMs and FinLLMs perform reasonably well on some tasks like sentiment analysis, they struggle with more complex tasks like relation extraction and stock prediction.\n- Touchstone-GPT shows competitive performance compared to the other models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Classification",
            "Summarization"
        ],
        "github_urls": [
            "https://github.com/IDEA-FinAI/Golden-Touchstone"
        ],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction",
        "authors": "Adam Mahdi, Harry Mayne, Filip Sondej, Yushi Yang",
        "link": "https://arxiv.org/abs/2411.06424",
        "github_repo": null,
        "summary": "- This paper investigates the internal mechanisms of Direct Preference Optimization (DPO) for toxicity reduction in language models, challenging the existing explanation that it primarily works by dampening the most toxic neurons.\n- By ablating toxic neurons and applying activation patching, the study finds that only 31.8% of toxicity reduction stems from dampened toxic neurons.\n- Instead, DPO reduces toxicity through a more complex process involving multiple neuron groups, accumulating effects by both reducing writing in the toxic direction and actively promoting anti-toxicity in the residual stream.\n- DPO's adjustments to neuron activations are noisy, with many neurons actually increasing toxicity, suggesting a balancing process between opposing neuron effects to achieve overall toxicity reduction.\n- The research provides a more nuanced understanding of DPO's mechanism, suggesting potential for targeted interventions to replicate its effects and improve safety in language models.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Yushi-Y/dpo-toxic-neurons"
        ],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "KMM: Key Frame Mask Mamba for Extended Motion Generation",
        "authors": "Feng Chen, Qi Chen, Akide Liu, Zeyu Zhang, Ha0Tang",
        "link": "https://arxiv.org/abs/2411.06481",
        "github_repo": null,
        "summary": "- This paper introduces Key Frame Mask Mamba (KMM), a novel architecture for extended motion generation that addresses the challenges of memory decay and poor text-motion alignment in previous methods by strategically masking key frames based on local density and minimum distances between high density motion embeddings within the latent space.\n- Using a customized contrastive learning strategy to learn dynamic text embeddings, the approach improves motion-text alignment by dynamically learning text encodings and enabling the generation of more accurate and aligned motion sequences.\n- On the BABEL dataset, KMM achieves state-of-the-art performance with a reduction of more than 57% in FID and 70% in parameters compared to existing methods.\n- A newly introduced benchmark, BABEL-D, focuses on evaluating text-motion alignment for directional instructions, and demonstrates KMM\u2019s superior performance.\n- User studies further confirmed the efficacy of the proposed KMM model through qualitative and quantitative analysis of the generated motions across a diversity of prompts.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-12"
    },
    {
        "title": "SAMPart3D: Segment Any Part in 3D Objects",
        "authors": "Xiaoyang Wu, Liangjun Lu, Yuan-Chen Guo, Yukun Huang, Yunhan Yang",
        "link": "https://arxiv.org/abs/2411.07184",
        "github_repo": "https://github.com/yhyang-myron/SAMPart3D-website",
        "summary": "- SAMPart3D, a novel zero-shot 3D part segmentation framework, segments any 3D object into semantic parts at multiple granularities without predefined part labels or text prompts.\n- It leverages a two-stage distillation process: first, pre-training a 3D feature extraction backbone (PTv3-object, adapted from Point Transformer V3) on Objaverse dataset by distilling visual features from DINOv2; second, fine-tuning scale-conditioned MLPs to distill 2D masks from SAM for multi-granularity segmentation.\n- After segmentation, it employs Multimodal Large Language Models (MLLMs) to assign semantic labels based on multi-view renderings of the segmented parts.\n- A new benchmark, PartObjaverse-Tiny, with detailed semantic and instance part annotations for 200 complex 3D objects is introduced, addressing limitations of existing datasets.\n- Experimental results show that SAMPart3D outperforms existing methods in zero-shot 3D part segmentation and facilitates applications like part-level editing and interactive segmentation.",
        "classification": [
            "Image Segmentation",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/yhyang-myron/SAMPart3D-website"
        ],
        "huggingface_urls": [],
        "date": "2024-11-13"
    },
    {
        "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
        "authors": "Chengyue Wu, Wen Liu, Xiaokang Chen, Xingchao Liu, Yiyang Ma",
        "link": "https://arxiv.org/abs/2411.07975",
        "github_repo": null,
        "summary": "- JanusFlow is a unified multimodal model that integrates autoregressive language models with rectified flow for both image understanding and generation tasks.\n- The model uses a minimalist architecture with a lightweight encoder and decoder to adapt the LLM for rectified flow operations, and employs two key strategies: decoupling understanding and generation encoders and aligning their representations during training.\n- On text-to-image generation, JanusFlow surpasses existing models on benchmarks like MJHQ FID-30k, GenEval, and DPG-Bench.\n- It also achieves state-of-the-art performance in multimodal comprehension tasks on benchmarks like MMBench, SeedBench, and GQA, exceeding specialized models.\n- These results are achieved with a compact 1.3B parameter LLM, showing the potential for efficient and versatile vision-language models.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/DeepFloyd/IF-I-XL-v1.0"
        ],
        "date": "2024-11-13"
    },
    {
        "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
        "authors": "Radha Poovendran, Luyao Niu, Fengqing Jiang, Zhangchen Xu, yuchenlin",
        "link": "https://arxiv.org/abs/2411.07133",
        "github_repo": null,
        "summary": "- This paper challenges the assumption that larger language models (LLMs) are always better teachers for instruction tuning of smaller LLMs, a phenomenon termed the \"Larger Models' Paradox.\"\n- Through experiments across five base models and twenty response generators, they demonstrate that larger models within a model family don't always lead to better instruction-following performance in smaller models after fine-tuning.\n- Existing metrics for data selection, such as quality, difficulty, and response length, fail to predict response generator effectiveness because they don't account for teacher-student model compatibility.\n- They propose a new metric, Compatibility-Adjusted Reward (CAR), that considers both the reward of generated responses and their compatibility with the base model (measured by loss on the base model), which outperforms baseline metrics in predicting response generator effectiveness.\n- Open-source models like Gemma-2-9b-it and Qwen2.5-72B-Instruct were found to be highly effective as response generators, outperforming even closed-source models like GPT-4 in some cases.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Magpie-Align/Magpie-100K-Generator-Zoo"
        ],
        "date": "2024-11-13"
    },
    {
        "title": "Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings",
        "authors": "Derek Cheung, Arianna Rampini, Pradyumna Reddy, Aliasghar Khani, adityasanghi",
        "link": "https://arxiv.org/abs/2411.08017",
        "github_repo": null,
        "summary": "- Introduces WaLa, a novel billion-parameter 3D generative model employing wavelet-based compact latent encodings, addressing computational and dimensionality challenges in 3D generation.\n- WaLa compresses a 256\u00b3 signed distance field into a 12\u00b3 x 4 latent grid achieving a 2,427x compression ratio, enabling efficient training of large networks without impacting inference time.\n- Demonstrates state-of-the-art performance across multiple datasets, improving generation quality, diversity, and computational efficiency, generating high-quality 3D shapes within 2-4 seconds.\n- Supports diverse input modalities, including text, sketches, single/multi-view images, voxels, and point clouds, enhancing framework versatility.\n- Open-sources the largest pre-trained 3D generative models across different modalities to promote reproducibility.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/AutodeskAILab/WaLa"
        ],
        "huggingface_urls": [],
        "date": "2024-11-13"
    },
    {
        "title": "Large Language Models Can Self-Improve in Long-context Reasoning",
        "authors": "Mo Yu, Lemao Liu, Zesen Cheng, Cheng Yang, Siheng99",
        "link": "https://arxiv.org/abs/2411.08147",
        "github_repo": "https://github.com/SihengLi99/SEALONG",
        "summary": "- This paper introduces SEALONG, a self-improving method for Large Language Models (LLMs) to enhance long-context reasoning.\n- SEALONG samples multiple reasoning trajectories from the LLM, scores them using Minimum Bayes Risk (MBR), and fine-tunes using either supervised learning or preference optimization.\n- Experiments show improvement on LLMs like Llama-3.1-8B-Instruct by 4.2 points.\n- SEALONG outperforms existing methods reliant on human annotations or expert models by leveraging the LLM's own generated outputs.\n- The results demonstrate substantial potential for LLMs to self-improve in long-context reasoning and suggests promise for further research.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/SihengLi99/SEALONG"
        ],
        "huggingface_urls": [],
        "date": "2024-11-14"
    },
    {
        "title": "EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation",
        "authors": "Guosheng Zhao, Jiayu Wang, Feng Liu, Kang Zhao, Xiaofeng Wang",
        "link": "https://arxiv.org/abs/2411.08380",
        "github_repo": null,
        "summary": "- This paper introduces EgoVid-5M, a large-scale, high-quality dataset specifically designed for egocentric video generation, comprising 5 million 1080p video clips with detailed action annotations, including fine-grained kinematic control and high-level textual descriptions.\n- The dataset undergoes a rigorous cleaning process, ensuring frame consistency, action coherence, and motion smoothness, addressing the challenges posed by dynamic viewpoints and diverse actions in egocentric videos.\n- Various video generation baselines trained on EgoVid-5M show significant improvement in egocentric video generation quality.\n- The authors also introduce EgoDreamer, a novel model that leverages both action descriptions and kinematic control to drive egocentric video generation, demonstrating its capability to generate realistic and diverse egocentric videos.\n- Experimental results demonstrate that EgoVid-5M and EgoDreamer enhance the performance of several video generation models across metrics like CD-FVD, semantic and action consistency, clarity, and motion smoothness and strength.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-14"
    },
    {
        "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
        "authors": "Hanqi Yan, Minjun Zhu, Hongbo Zhang, Chak Tou Leong, Qingyu Yin",
        "link": "https://arxiv.org/abs/2411.07618",
        "github_repo": null,
        "summary": "- This paper introduces Feature-level constrained Preference Optimization (FPO), a novel method for aligning Large Language Models (LLMs) with human preferences.\n- FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment, simplifying the alignment process while ensuring stability.\n- FPO achieves an above 5% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines on benchmark datasets such as AlpacaEval-2 and Arena-Hard.\n- By constraining the shifts of sparse features during training, FPO achieves results that meet or exceed the effectiveness of sequential KL divergence with lower computational cost (17.6% reduction compared to TDPO2).\n- FPO combines the efficiency of SimPO with the constraint quality of sequential KL, making it a promising solution for efficient and controllable LLM alignment.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-14"
    },
    {
        "title": "CamemBERT 2.0: A Smarter French Language Model Aged to Perfection",
        "authors": "Beno\u00eet Sagot, \u00c9ric de la Clergerie, Rian Touchent, Francis Kulumba, Wissam Antoun",
        "link": "https://arxiv.org/abs/2411.08868",
        "github_repo": null,
        "summary": "- This paper introduces two new versions of the French language model CamemBERT: CamemBERTav2 and CamemBERTv2.\n- CamemBERTav2 is based on the DeBERTaV3 architecture and uses a Replaced Token Detection (RTD) training objective, while CamemBERTv2 uses the RoBERTa architecture with a Masked Language Modeling (MLM) objective.\n- Both models are trained on a significantly larger and more up-to-date dataset than their predecessors, with an updated tokenizer to better handle the nuances of modern French text.\n- Evaluations on various NLP tasks, including question answering, named entity recognition, and text classification, show that both models significantly outperform previous versions, particularly CamemBERTav2 which yielded the best performance in most cases.\n- All models and checkpoints are publicly available on Hugging Face.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Classification",
            "Token Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/almanach?search_models=camembert+v2"
        ],
        "date": "2024-11-14"
    },
    {
        "title": "Can sparse autoencoders be used to decompose and interpret steering vectors?",
        "authors": "Adam Mahdi, Yushi Yang, Harry Mayne",
        "link": "https://arxiv.org/abs/2411.08790",
        "github_repo": null,
        "summary": "- This paper investigates why applying sparse autoencoders (SAEs) directly to steering vectors results in inaccurate decompositions, hindering interpretability.\n- Two primary reasons are identified: steering vectors fall outside the training distribution of SAEs, and SAEs cannot accommodate meaningful negative projections present in steering vectors.\n- The study uses the corrigibility steering vector as a case study, demonstrating that the SAE decomposition is largely driven by the encoder bias, overshadowing the steering vector's contribution.\n- Scaling the steering vector's L2-norm does not resolve the out-of-distribution issue, as default components present in model activations are absent in steering vectors due to the subtraction process involved in their creation.\n-  The restriction of SAEs to non-negative reconstruction coefficients leads to misinterpretations, as it overlooks negative projections in feature directions, and these negative projections can also cause spurious positive activations in other features due to negative cosine similarity between features.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/HarryMayne/SV_interpretability"
        ],
        "huggingface_urls": [
            "https://huggingface.co/google/gemma-scope-2b-pt-res/tree/main/layer_14/width_16k/average_10_173"
        ],
        "date": "2024-11-14"
    },
    {
        "title": "MagicQuill: An Intelligent Interactive Image Editing System",
        "authors": "Qiuyu Wang, Hao Ouyang, wwen1997, bruceyyu, LiuZichen",
        "link": "https://arxiv.org/abs/2411.09703",
        "github_repo": null,
        "summary": "- MagicQuill is an intelligent interactive image editing system built upon diffusion models that uses brushstrokes (add, subtract, and color) to edit images.\n- A Multimodal Large Language Model (MLLM) predicts user intentions from brushstrokes and image context, suggesting contextually relevant text prompts in real-time, a process referred to as Draw&Guess.\n- An Editing Processor with a two-branch architecture (inpainting and control) enables precise and controllable edits according to user brushstrokes and prompts.\n- Quantitative results on a custom dataset, comparing against methods like SmartEdit, SketchEdit and BrushNet, demonstrate superior edge alignment, color fidelity and controllable image generation in MagicQuill.\n- User studies showcase the system's effectiveness in accurately predicting user intent and significantly improving editing efficiency compared to other MLLMs such as LLaVA-1.5, LLaVA-Next, and GPT-4.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-15"
    },
    {
        "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models",
        "authors": "Jun Zhu, Hang Su, Yikai Wang, Jonathan Lorraine, Zhengyi Wang",
        "link": "https://arxiv.org/abs/2411.09595",
        "github_repo": null,
        "summary": "- LLaMA-Mesh is a novel approach that unifies 3D mesh generation with Large Language Models (LLMs) by representing meshes as plain text, allowing seamless integration without tokenizer or vocabulary modifications.\n- The model leverages the OBJ file format, treating vertex coordinates and face definitions as text sequences, enabling LLMs to process 3D data directly.\n- A supervised fine-tuning (SFT) dataset with text-3D pairs and interleaved dialogues is used to train a pretrained LLaMA model, enabling it to generate 3D meshes from text prompts, understand 3D meshes, and maintain conversational abilities.\n- LLaMA-Mesh achieves mesh generation quality comparable to specialized 3D generation models while preserving the LLMs' language capabilities, as demonstrated by qualitative and quantitative results.\n- The work represents a significant step towards integrating multi-modal content generation within a unified language model.",
        "classification": [
            "Text-to-3D",
            "Multimodal",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-15"
    },
    {
        "title": "Cut Your Losses in Large-Vocabulary Language Models",
        "authors": "Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun, Alexander Hertzberg, Brody Huval, erikwijmans",
        "link": "https://arxiv.org/abs/2411.09009",
        "github_repo": "https://github.com/apple/ml-cross-entropy",
        "summary": "- This paper introduces Cut Cross-Entropy (CCE), a novel method to compute cross-entropy loss and its gradients without materializing the full logits matrix in memory.\n- CCE performs matrix multiplications and log-sum-exp operations within flash memory, reducing the memory footprint for loss calculation from gigabytes to megabytes.\n- It leverages softmax sparsity to further improve throughput by skipping negligible gradient computations.\n- Experiments on large language models like Gemma 2 and Llama 3 demonstrate significant memory reduction and increased batch size without impacting training speed or convergence.\n- The proposed method facilitates training larger language models with extended vocabulary sizes under memory constraints.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/apple/ml-cross-entropy"
        ],
        "huggingface_urls": [],
        "date": "2024-11-15"
    },
    {
        "title": "ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?",
        "authors": "Zhongwei Wan, Che Liu, Shan Chen, Jian Yu, canyuchen",
        "link": "https://arxiv.org/abs/2411.06469",
        "github_repo": null,
        "summary": "- ClinicalBench, a new benchmark, was introduced to evaluate the performance of LLMs and traditional machine learning models on clinical prediction tasks.\n- The benchmark includes three common clinical prediction tasks (Length-of-Stay, Mortality, and Readmission), two real-world clinical databases (MIMIC-III and MIMIC-IV), 22 LLMs with varying sizes, and 11 traditional ML models.\n- Through empirical studies, traditional machine learning models outperformed both general-purpose and medical LLMs across all tasks and datasets, even with varying model sizes, prompting, or fine-tuning strategies.\n- This suggests a potential deficiency in the clinical reasoning and decision-making capabilities of current LLMs.\n- The benchmark and code are publicly available to facilitate further research in this domain.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
            "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
            "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3",
            "https://huggingface.co/google/gemma-2-9b-it",
            "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct",
            "https://huggingface.co/Qwen/Qwen2-1.5B-Instruct",
            "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
            "https://huggingface.co/01-ai/Yi-1.5-6B-Chat",
            "https://huggingface.co/01-ai/Yi-1.5-9B-Chat",
            "https://huggingface.co/01-ai/Yi-1.5-34B-Chat",
            "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
            "https://huggingface.co/internlm/internlm2_5-7b-chat",
            "https://huggingface.co/openbmb/MiniCPM3-4B",
            "https://huggingface.co/epfl-llm/meditron-7b",
            "https://huggingface.co/epfl-llm/meditron-70b",
            "https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20",
            "https://huggingface.co/BioMistral/BioMistral-7B",
            "https://huggingface.co/m42-health/Llama3-Med42-8B",
            "https://huggingface.co/m42-health/Llama3-Med42-70B",
            "https://huggingface.co/PharMolix/BioMedGPT-LM-7B",
            "https://huggingface.co/internistai/base-7b-v0.2",
            "https://huggingface.co/docs/transformers/en/index"
        ],
        "date": "2024-11-15"
    },
    {
        "title": "Hermes: A Large Language Model Framework on the Journey to Autonomous Networks",
        "authors": "Merouane Debbah, Antonio De Domenico, Ali Maatouk, Fadhel Ayed, nicopi",
        "link": "https://arxiv.org/abs/2411.06490",
        "github_repo": null,
        "summary": "- Hermes is a novel chain-of-agent LLM framework designed to automate the creation of Network Digital Twins (NDTs) using blueprints, enhancing the path towards autonomous network management.\n- Unlike existing NDT approaches that require distinct architectures for each use case, Hermes uses LLMs to generate step-by-step logical blocks (blueprints) for NDT construction, improving flexibility and scalability.\n- The framework consists of a Designer agent for creating and refining the blueprint based on network data and policies, and a Coder agent for translating the blueprint into executable Python code.\n- Experimental results across four autonomous network tasks demonstrated that Hermes with GPT-40 as LLM consistently outperforms chain-of-thought prompting and direct code generation, achieving success rates up to 80%.\n- While open-source LLMs show limited performance independently, integrating them with a library of expert-designed models significantly improves their effectiveness in NDT blueprint design, highlighting the potential for broader application.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-15"
    },
    {
        "title": "Sharingan: Extract User Action Sequence from Desktop Recordings",
        "authors": "Kehong Yuan, Jue Zhang, Xiaoting Qin, Yi Ren, Yanting Chen",
        "link": "https://arxiv.org/abs/2411.08768",
        "github_repo": null,
        "summary": "- This paper introduces Sharingan, a method for extracting user action sequences from desktop recordings using Vision-Language Models (VLMs).\n- Two approaches are proposed: Direct Frame-Based (DF) which directly inputs video frames to VLMs, and Differential Frame-Based (DiffF) which incorporates explicit frame differences.\n- Evaluation on two datasets (a basic self-curated dataset and an advanced benchmark adapted from GUI-World) shows DF achieves 70-80% accuracy, with extracted sequences replayable through Robotic Process Automation (RPA).\n- While VLMs show potential, explicitly incorporating UI changes can degrade performance, favoring the DF approach.\n- This work represents the first application of VLMs for this task, contributing new methods, benchmarks, and insights for future research in desktop automation and user behavior understanding.",
        "classification": [
            "Video-Text-to-Text",
            "Robotics"
        ],
        "github_urls": [],
        "date": "2024-11-15"
    },
    {
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "authors": "LiYuan, sunlichao137, Yibing, Pengjin, Xkev",
        "link": "https://arxiv.org/abs/2411.10440",
        "github_repo": null,
        "summary": "- This paper introduces LLaVA-01, a Vision Language Model (VLM) designed for improved multi-stage reasoning in visual question answering.\n- LLaVA-01 uses a structured approach with four stages: summarization, visual interpretation, logical reasoning, and conclusion generation, unlike traditional chain-of-thought prompting.\n- A new dataset, LLaVA-01-100k, was created with structured reasoning annotations from various visual question answering sources, used to fine-tune the Llama-3.2-11B-Vision-Instruct model.\n- A novel stage-level beam search method enables efficient inference-time scaling.\n- LLaVA-01 surpasses larger and closed-source models like Gemini-1.5-pro and GPT-40-mini on multiple multimodal reasoning benchmarks.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-18"
    },
    {
        "title": "GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation",
        "authors": "doubling, hongfz16, ZhaoyangLyu, sczhou, yslan",
        "link": "https://arxiv.org/abs/2411.08033",
        "github_repo": null,
        "summary": "- GAUSSIANANYTHING, a novel 3D generation framework, introduces an interactive Point-Cloud structured Latent space for scalable, high-quality 3D generation and editing.\n- The framework utilizes a 3D VAE that takes multi-view posed RGB-D-N renderings as input, encoding them into a point cloud-structured latent space, which is then decoded into surfel Gaussians.\n- Cascaded latent diffusion modeling is performed on this latent space, enabling shape-texture disentanglement and supporting multi-modal conditional 3D generation from point clouds, captions, and single/multi-view images.\n- Experimental results on multiple datasets show GAUSSIANANYTHING outperforms existing methods in both text- and image-conditioned 3D generation, demonstrating its effectiveness in generating and editing high-quality, editable 3D content.\n- The proposed Point Cloud-structured Latent space enables interactive 3D editing by directly manipulating the generated point cloud, similar to manipulations in 2D models, and facilitates geometry-texture disentanglement.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-18"
    },
    {
        "title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use",
        "authors": "Mingyu Ouyang, AnalMom, QuStar, SiyuanH",
        "link": "https://arxiv.org/abs/2411.10323",
        "github_repo": "https://github.com/showlab/computer_use_ootb",
        "summary": "- This paper presents a preliminary case study on Claude 3.5 Computer Use, a new large language model (LLM) designed for GUI automation.\n- The study evaluates the model's abilities in planning, action execution, and critic feedback across various desktop environments, including web search, office productivity software, workflow applications, and video games.\n- The researchers propose a novel framework, Computer Use Out-of-the-Box (OOTB), which offers a cross-platform solution for easy implementation and benchmarking of GUI automation models.\n- The study identifies some limitations in Claude 3.5's handling of tasks, including scrolling-based navigation and text selection precision, as well as occasional inaccuracies in final outcome evaluations.\n- The findings suggest the need for future research to focus on improved selection capabilities, more accurate validation feedback, and enhanced adaptability to real-world application complexities.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/showlab/computer_use_ootb"
        ],
        "huggingface_urls": [],
        "date": "2024-11-18"
    },
    {
        "title": "Number it: Temporal Grounding Videos like Flipping Manga",
        "authors": "Vito328, zhouzhouyi, tms28k, kaleidudu, Liang0223",
        "link": "https://arxiv.org/abs/2411.10332",
        "github_repo": "https://github.com/yongliang-wu/NumPro",
        "summary": "- This paper introduces Number-Prompt (NumPro), a novel method to enhance Video Temporal Grounding (VTG) in Video Large Language Models (Vid-LLMs) by adding numerical identifiers to video frames, mimicking numbered manga panels.\n- NumPro transforms VTG into an intuitive visual sequence alignment task, enabling Vid-LLMs to directly link visual content with temporal information.\n- Experiments show that NumPro significantly improves VTG performance across various Vid-LLMs, both in training-free and fine-tuned settings (NumPro-FT).\n- NumPro-FT achieves state-of-the-art results on Charades-STA and ActivityNet, surpassing previous top methods by up to +11.8% and +9.9% in mIoU for moment retrieval, respectively, and +8.5% in mAP for highlight detection on QVHighlights.\n- Furthermore, NumPro minimally impacts general video-QA performance, allowing Vid-LLMs to maintain robust general video understanding while excelling at temporally precise grounding tasks.",
        "classification": [
            "Video-Text-to-Text",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/yongliang-wu/NumPro"
        ],
        "huggingface_urls": [],
        "date": "2024-11-18"
    },
    {
        "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
        "authors": "wolf1110, AJZhou, liuyangbian, yina0, lucky-lance",
        "link": "https://arxiv.org/abs/2411.10640",
        "github_repo": null,
        "summary": "- BlueLM-V-3B is a 3 billion parameter multimodal large language model designed for mobile devices, featuring a 2.7B parameter language model and a 400M parameter vision encoder (SigLIP).\n- It introduces a relaxed aspect ratio matching method for dynamic image resolution, reducing the number of image tokens processed by the vision encoder without sacrificing model performance.\n- System optimizations include batched image encoding, pipeline parallelism for image processing, token downsampling, chunked computing of input tokens, and mixed-precision quantization.\n- BlueLM-V-3B achieves state-of-the-art performance on the OpenCompass benchmark, with an average score of 66.1, surpassing larger models like MiniCPM-V-2.6 and InternVL2-8B.\n- On a MediaTek Dimensity 9300 processor, it uses 2.2GB of memory, encodes 768x1536 images in 2.1 seconds, and generates text at 24.4 tokens/second.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "Generative World Explorer",
        "authors": "Daniel Khashabi, Alan Yuille, Tianmin Shu, jienengchen, TaiMingLu",
        "link": "https://arxiv.org/abs/2411.11844",
        "github_repo": null,
        "summary": "- The paper introduces Generative World Explorer (Genex), a novel egocentric video generation model that allows embodied agents to \"mentally\" explore 3D environments (e.g., urban scenes) by generating imagined future observations, and then use these observations to update the agent's beliefs about the world, which enable the agent to make more informed decisions.\n- The architecture is based on a video diffusion model that takes an initial egocentric view (represented as a panoramic image), the intended movement direction as action input, and then streams a video of future egocentric observations. \n- A spherical-consistent learning objective is introduced to ensure that generated pixels are continuous in the spherical space and improve the consistency and coherence of generated videos during long imaginative exploration.\n- The experimental evaluation, with a new synthetic urban scene dataset (Genex-DB) and new Embodied QA benchmark (Genex-EQA), shows that Genex generates high-quality and consistent observations during imaginative exploration and improves an existing LLM agent\u2019s decision-making process.\n- The authors extend Genex to multi-agent scenarios, where an agent infers the perspectives of other agents to make decisions based on a more complete understanding of the situation.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/Beckschen/genex"
        ],
        "huggingface_urls": [
            "https://generative-world-explorer.github.io/"
        ],
        "date": "2024-11-19"
    },
    {
        "title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering",
        "authors": "Ben He, Boxi Cao, Xinyu Lu, Yanjiang Liu, Xinyan Guan",
        "link": "https://arxiv.org/abs/2411.11504",
        "github_repo": null,
        "summary": "This paper introduces verifier engineering, a novel post-training paradigm for foundation models.  It leverages automated verifiers to perform verification tasks and deliver feedback to the models, enhancing their capabilities.  The framework systematically categorizes this process into three stages: search, verify, and feedback.  The authors provide a comprehensive overview of state-of-the-art research in each stage. This approach is presented as a fundamental pathway toward achieving Artificial General Intelligence.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/icip-cas/Verifier-Engineering"
        ],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "AnimateAnything: Consistent and Controllable Animation for Video Generation",
        "authors": "Rong Zhang, Hong Li, Chi Wang, Guojun Lei, yikaiw",
        "link": "https://arxiv.org/abs/2411.10836",
        "github_repo": null,
        "summary": "- AnimateAnything is a novel approach for controllable video generation that unifies various control signals (text, camera trajectory, motion annotations) into a common optical flow representation.\n- The model architecture consists of two stages: a Unified Flow Generation stage that converts control signals into optical flow using a multi-scale control feature fusion network and a Video Generation stage that uses the optical flow to guide video generation using a diffusion model.\n- A frequency-based stabilization module is incorporated to improve the temporal coherence of generated videos and reduce flickering caused by large-scale motion.\n- The approach outperforms state-of-the-art methods in terms of video quality, consistency, and controllability, as demonstrated by quantitative and qualitative evaluations.\n- Extensive experiments on various datasets show AnimateAnything's robustness and superior performance in handling various control signals and generating high-quality videos.",
        "classification": [
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
        "authors": "Michael Carbin, Matei Zaharia, Erik Lindgren, Mathew Jacob, mrdrozdov",
        "link": "https://arxiv.org/abs/2411.11767",
        "github_repo": null,
        "summary": "\n- This paper investigates the performance of rerankers, which are models that rescore documents retrieved by an initial IR system.\n- The authors find that existing rerankers exhibit diminishing returns when scoring increasingly more documents, often degrading in quality beyond a certain threshold.\n- This contrasts with the common assumption that rerankers consistently improve retrieval quality as the number of scored documents increases.\n- Qualitative analyses reveal that the rerankers frequently assign high scores to documents that have minimal semantic overlap with the query.\n- The researchers suggest that listwise reranking using large language models may offer a solution to improve reranker robustness.",
        "classification": [
            "Document Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "SlimLM: An Efficient Small Language Model for On-Device Document Assistance",
        "authors": "Viet Dac Lai, Seunghyun Yoon, Phat T. Nguyen, Thang M. Pham, Franck-Dernoncourt",
        "link": "https://arxiv.org/abs/2411.09944",
        "github_repo": null,
        "summary": "- SlimLM, a series of small language models (SLMs) optimized for document assistance tasks on mobile devices like smartphones, is introduced.\n- The models range from 125M to 1B parameters and are pre-trained on SlimPajama-627B and fine-tuned on DocAssist, a new dataset constructed for summarization, question answering, and question suggestion.\n- SlimLM models demonstrate comparable or superior performance to existing SLMs of similar sizes on a Samsung Galaxy S24, efficiently handling contexts up to 800 tokens.\n- An Android application demonstrates SlimLM's real-world applicability.\n- The models offer a balance between performance, efficiency, and privacy for on-device document processing, potentially reducing reliance on server-based APIs.",
        "classification": [
            "Natural Language Processing",
            "Document Question Answering",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers",
        "authors": "Haomiao Jiang, Joshua Geddes, mnandwana, helloterran, josephliu-roblox",
        "link": "https://arxiv.org/abs/2411.10510",
        "github_repo": "https://github.com/Roblox/SmoothCache",
        "summary": "- SmoothCache is a model-agnostic inference acceleration technique for Diffusion Transformers (DiTs) that leverages the high similarity between layer outputs across adjacent diffusion timesteps.\n- By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference.\n- This method achieves 8% to 71% speedup while maintaining or even improving generation quality across diverse modalities (image, video, and audio).\n- It has been shown to be effective on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio.\n- SmoothCache is compatible with various common solvers in diffusion transformers and requires only a single hyperparameter.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Text-to-Audio",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/Roblox/SmoothCache"
        ],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "Top-$n\u03c3$: Not All Logits Are You Need",
        "authors": "Liusheng Huang, Hongli Xu, Jianchun Liu, tomorrowdawn",
        "link": "https://arxiv.org/abs/2411.07641",
        "github_repo": null,
        "summary": "- This paper introduces top-\n\u03c3, a novel sampling method for large language models (LLMs) that operates directly on pre-softmax logits by leveraging a statistical threshold.\n- The method distinguishes between a Gaussian-distributed noisy region and a distinct informative region in the logits, enabling efficient token filtering without complex probability manipulations.\n- Unlike existing methods, top-\n\u03c3 maintains a stable sampling space regardless of temperature scaling, making it suitable for test-time scaling techniques.\n- Experimental results across four reasoning-focused datasets demonstrate that top-\n\u03c3 outperforms existing sampling approaches and greedy decoding, while maintaining consistent performance at high temperatures.\n- The theoretical analysis of top-no provides further insights into its behavior and temperature invariance property.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "StableV2V: Stablizing Shape Consistency in Video-to-Video Editing",
        "authors": "Dong Liu, Yunwei Lan, Kaidong Zhang, Rui Li, Chang Liu",
        "link": "https://arxiv.org/abs/2411.11045",
        "github_repo": "https://github.com/AlonzoLeeeooo/StableV2V",
        "summary": "- StableV2V is a novel video editing method that prioritizes shape consistency between the edited video and user prompts, addressing limitations of existing methods that often produce inconsistent results.\n- It consists of three components: a Prompted First-Frame Editor (PFE) to edit the initial frame based on the prompt, an Iterative Shape Aligner (ISA) to ensure shape consistency across frames by simulating and refining depth maps, and a Conditional Image-to-Video Generator (CIG) to produce the final edited video.\n- Experimental results on the DAVIS-Edit benchmark demonstrate StableV2V's superior performance compared to state-of-the-art methods in various metrics, including visual quality, temporal consistency, and alignment with user prompts.\n- StableV2V is also shown to be more efficient than most competing methods, while achieving significant improvements in challenging scenarios involving large shape changes.\n- StableV2V supports a wide range of editing applications, including text-based editing, image-based editing, video style transfer, and video inpainting.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/AlonzoLeeeooo/StableV2V"
        ],
        "huggingface_urls": [
            "https://huggingface.co/AlonzoLeeeooo/StableV2V",
            "https://huggingface.co/datasets/AlonzoLeeeooo/DAVIS-Edit"
        ],
        "date": "2024-11-19"
    },
    {
        "title": "Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts",
        "authors": "Nanyi Fei, Hongpeng Lin, Guoxing Yang, Yanqi Dai, Jinqiang Long",
        "link": "https://arxiv.org/abs/2411.10669",
        "github_repo": "https://github.com/MetabrainAGI/Awaker",
        "summary": "- This paper introduces Awaker2.5-VL, a Mixture of Experts (MoE) architecture designed for Multimodal Large Language Models (MLLMs) to address the \"multi-task conflict\" issue, where mixing data from various tasks leads to performance degradation.\n- Awaker2.5-VL utilizes multiple sparsely activated expert models, each specializing in a specific task, along with a global expert for general capabilities, and a gating network to control expert activation.\n- The model employs Low-Rank Adaptation (LoRA) for each expert to reduce training and inference costs and incorporates a simplified routing strategy for enhanced training stability.\n- The paper uses Qwen2-VL-7B-Instruct as its base model and evaluates performance on benchmarks such as MME-Realworld, MME-Realworld-CN, and MMBench.\n- Awaker2.5-VL achieves state-of-the-art results on the MME-Realworld-CN benchmark and shows significant improvements over the base model on other benchmarks, including a 5-point improvement in overall score on MME-Realworld-CN.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering",
            "Object Detection"
        ],
        "github_urls": [
            "https://github.com/MetabrainAGI/Awaker"
        ],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on",
        "authors": "Chengming Xu, Qingdong He, Donghao Luo, Xiaobin Hu, Boyuan Jiang",
        "link": "https://arxiv.org/abs/2411.10499",
        "github_repo": null,
        "summary": "- FitDiT, a novel Diffusion Transformer (DiT) model designed for virtual try-on, focuses on enhancing high-resolution features related to garment patterns. \n- FitDiT introduces a two-stage training strategy, first using garment priors evolution to fine-tune garment feature extraction and then using a customized DiT block for virtual try-on. \n- A frequency-spectra distance loss aids in preserving intricate garment details in pixel space while a dilated-relaxed mask augmentation strategy improves size-aware fitting. \n-  Quantitative results on VITON-HD, DressCode, and a self-collected high-resolution dataset (CVDD) demonstrate FitDiT's significant performance improvements over existing try-on models. \n- FitDiT shows superiority by reducing FID error by 71.6% over existing models, demonstrating remarkable texture preservation and improved garment fitting, which makes it suitable for real-world try-on applications.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "LL\u00e4Mmlein: Compact and Competitive German-Only Language Models from Scratch",
        "authors": "Andreas Hotho, Julia Wunderle, Jan Pfister",
        "link": "https://arxiv.org/abs/2411.11171",
        "github_repo": null,
        "summary": "- This paper introduces LL\u00e4Mmlein, two new German-only decoder-only LLMs (120M and 1B parameters), trained from scratch on a filtered and preprocessed version of the RedPajama dataset.\n- A new German tokenizer with a 32,000 token vocabulary was created and models were evaluated on the SuperGLEBer and lm-evaluation-harness-de benchmarks, showing competitive performance against similarly sized models and even some larger models.\n- The 1B model matches the performance of much larger models, like the German finetuned Llama 8B on the SuperGLEBer benchmark and its instruction-tuned version outperforms all other 1B models on TruthfulQA by at least 6%.\n- Intermediate checkpoints were analyzed to track the learning dynamics, revealing varying rates of improvement across different tasks.\n- All artifacts, including the models, code, and data, will be open-sourced to promote transparency and future research within the German NLP community.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering",
            "Token Classification",
            "Sentence Similarity"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/cis-lmu/bavarian_to_english",
            "https://huggingface.co/LSX-UniWue/Guanako",
            "https://huggingface.co/FreedomIntelligence/alpaca-gpt4-deutsch",
            "https://huggingface.co/FreedomIntelligence/evol-instruct-deutsch",
            "https://huggingface.co/FreedomIntelligence/sharegpt-deutsch",
            "https://huggingface.co/DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1"
        ],
        "date": "2024-11-19"
    },
    {
        "title": "Adaptive Decoding via Latent Preference Optimization",
        "authors": "Jason Weston, Asli Celikyilmaz, Ping Yu, Ilia Kulikov, Shehzaad Dhuliawala",
        "link": "https://arxiv.org/abs/2411.09661",
        "github_repo": null,
        "summary": "- This paper introduces Adaptive Decoding, a method for dynamically adjusting the sampling temperature during language model generation, leading to improved performance on tasks requiring varying levels of creativity and factual accuracy.\n- The core component is the Adaptive Decoder module, a small neural network added to the LLM's architecture, predicting the optimal temperature at either the token or sequence level.\n- Latent Preference Optimization (LPO), a novel training approach based on Direct Preference Optimization, trains the Adaptive Decoder by sampling multiple responses with varying temperatures, scoring them with a reward model, and learning to prefer temperatures associated with higher-ranked responses.\n- Experiments on a combined dataset (UltraMathStories) of math, creative writing, and general instructions show that Adaptive Decoding outperforms fixed temperature baselines.\n- Additionally, the method demonstrates success in constrained creative writing, where it learns to use low temperatures for constrained parts and higher temperatures for creative parts of the text, outperforming greedy and high-temperature baselines.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-19"
    },
    {
        "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
        "authors": "Fei Li, Qi Yang, Kun Ding, Robert Zhang, MarkWang",
        "link": "https://arxiv.org/abs/2411.11925",
        "github_repo": "https://github.com/MarkXCloud/CSpD",
        "summary": "- This paper introduces Continuous Speculative Decoding (CSpD), a novel method for accelerating autoregressive image generation models that use continuous tokens, such as those based on diffusion processes.\n- CSpD generalizes the speculative decoding algorithm from discrete tokens to continuous space by establishing an acceptance criterion based on the probability density functions of draft and target models' output distributions. \n- It addresses inconsistencies between draft and target outputs through denoising trajectory alignment and token pre-filling, and uses acceptance-rejection sampling to efficiently resample tokens from a modified distribution.\n- Experimental results on ImageNet 256x256 generation with MAR models demonstrate a speedup of up to 2.33x while maintaining generation quality, as measured by FID and IS. \n- The authors suggest that more substantial performance gains are expected with larger models and broader scale disparities.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [
            "https://github.com/MarkXCloud/CSpD"
        ],
        "huggingface_urls": [],
        "date": "2024-11-20"
    },
    {
        "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
        "authors": "Jeffrey Ichnowski, Christopher G. Atkeson, Jean Oh, Uksang Yoo, Yunchao Yao",
        "link": "https://arxiv.org/abs/2411.12734",
        "github_repo": null,
        "summary": "- SWIFT, a novel system for dynamic in-hand manipulation with soft robots, is introduced, enabling a soft robotic hand to learn and execute the complex task of pen spinning.\n- Unlike prior work relying on simulations or object models, SWIFT employs a trial-and-error learning approach based solely on real-world data, using a self-supervised process and a parameterized action space.\n- Using a soft, multi-finger gripper attached to a robotic arm, the system learns to grasp, spin, and catch a pen by optimizing the parameters of grasping location, servo targets for spinning, and delay time for catching, achieving a 100% success rate after 130 trials across three pens with different weights and weight distributions.\n- The system's robustness and generalizability are demonstrated by its successful application to other objects with diverse shapes and weights, such as a brush (10/10 success rate) and a screwdriver (5/10 success rate), after minimal additional training.\n- This work highlights the potential of soft robotics in performing complex dynamic in-hand manipulation tasks, bridging the gap between the capabilities of human hands and current limitations of soft robotic dexterity.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://soft-spin.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-11-20"
    },
    {
        "title": "RedPajama: an Open Dataset for Training Large Language Models",
        "authors": "Shane Adams, Yonatan Oren, Quentin Anthony, Daniel Fu, Maurice Weber",
        "link": "https://arxiv.org/abs/2411.12372",
        "github_repo": null,
        "summary": "- The paper releases RedPajama-V1, an open reproduction of the LLaMA training dataset, and RedPajama-V2, a new web-based dataset comprising over 100 trillion tokens with quality signals for filtering.\n- RedPajama-V2 dataset emphasizes transparency by documenting its creation process, offering data at scale, and includes artifacts and quality signals for filtering and creating higher quality datasets.\n- This dataset has been instrumental in training production-ready large language models, such as Snowflake Arctic, Salesforce's XGen, and AI2's OLMo.\n- Ablation studies are conducted using decoder-only transformer models with up to 1.6B parameters, demonstrating how quality signals enhance dataset curation.\n- The study emphasizes the potential of RedPajama in building more transparent, high-performing large language models.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "github.com/togethercomputer/RedPajama-Data"
        ],
        "huggingface_urls": [
            "huggingface.co/datasets/togethercomputer/RedPajama-Data-1T",
            "huggingface.co/datasets/togethercomputer/RedPajama-Data-V2"
        ],
        "date": "2024-11-20"
    },
    {
        "title": "Building Trust: Foundations of Security, Safety and Transparency in AI",
        "authors": "Huamin Chen, Mark Bestavros, Emily Fox, Garth Mollett, huzaifas-sidhpurwala",
        "link": "https://arxiv.org/abs/2411.12275",
        "github_repo": null,
        "summary": "- This paper explores the security and safety implications of publicly available AI models, particularly large language models (LLMs).\n- It reviews current security and safety scenarios, highlighting challenges like issue tracking, remediation, and the lack of established lifecycle and ownership processes for AI models.\n- The paper proposes comprehensive strategies to enhance security and safety for both model developers and end-users.\n- It discusses the distinction between AI security (protecting systems from threats) and AI safety (preventing harm from the system's operation), emphasizing the need for a holistic approach to AI risk management.\n- The paper also suggests adapting existing vulnerability disclosure processes for AI security flaws and proposes the establishment of a central body for tracking safety hazards.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-20"
    },
    {
        "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
        "authors": "D. J. Bora, tamang0000",
        "link": "https://arxiv.org/abs/2411.12240",
        "github_repo": null,
        "summary": "- This research paper evaluates the performance of tokenizers used by 12 Large Language Models (LLMs) across all 22 official languages of India.\n- The study uses Normalized Sequence Length (NSL) as the key metric for evaluation and finds that the SUTRA tokenizer outperforms other models, including Indic-specific models, in 14 out of 22 languages.\n- Notable findings include SUTRA's superior performance with Indic languages, GPT-40's improvement over GPT-4 in processing Indian languages, and the comparatively limited performance of Project Indus.\n- This suggests that Project Indus' better performance on some Indian language is tied to the training on common scripts (Devanagari) between the languages and not to the linguistic understanding.\n- The study highlights the importance of developing targeted tokenization strategies for multilingual and Indic-centric LLMs.",
        "classification": [
            "Natural Language Processing",
            "Token Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-20"
    },
    {
        "title": "SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration",
        "authors": "Jun Zhu, Jia Wei, Pengle Zhang, Haofeng Huang, jt-zhang",
        "link": "https://arxiv.org/abs/2411.10958",
        "github_repo": "https://github.com/thu-ml/SageAttention",
        "summary": "- SageAttention2 is a new attention mechanism that uses 4-bit matrix multiplication and additional precision-enhancing techniques to accelerate attention computation while maintaining precision.\n- It quantizes matrices Q and K to INT4 in warp-level granularity, and matrices P and V to FP8, along with smoothing techniques for Q and V to enhance accuracy.\n- An adaptive mixed-precision method employs 8-bit attention for problematic layers/timesteps and 4-bit attention for others, further improving accuracy.\n- On an RTX 4090, SageAttention2 achieves a peak performance of 485 TOPS, surpassing FlashAttention2 and xformers by approximately 3.1x and 5.4x, respectively.\n- Comprehensive experiments across diverse models, including large language, image generation, and video generation models, demonstrate negligible end-to-end metric loss with SageAttention2.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Image-to-Video",
            "Text-to-Video",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/thu-ml/SageAttention"
        ],
        "huggingface_urls": [],
        "date": "2024-11-21"
    },
    {
        "title": "VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models",
        "authors": "Jiashuo Yu, Yinan He, Xiaojie Xu, Fan Zhang, Ziqi Huang",
        "link": "https://arxiv.org/abs/2411.13503",
        "github_repo": "https://github.com/Vchitect/VBench",
        "summary": "- VBench++ is a comprehensive benchmark suite designed for evaluating video generative models, encompassing both text-to-video (T2V) and image-to-video (I2V) generation.\n- It introduces a hierarchical Evaluation Dimension Suite with 16 disentangled dimensions covering aspects like video quality, condition consistency, and trustworthiness.\n- It offers specialized prompt suites and an adaptive Image Suite to facilitate fair and comprehensive evaluation across different models and content types.\n- Human preference annotations are collected for each dimension, demonstrating strong alignment between VBench++ evaluations and human perception.\n- VBench++ provides valuable insights into model strengths and weaknesses, guiding future development in video generation, and is fully open-sourced.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/Vchitect/VBench"
        ],
        "huggingface_urls": [
            "https://huggingface.co/spaces/Vchitect/VBench_Leaderboard"
        ],
        "date": "2024-11-21"
    },
    {
        "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
        "authors": "Mohan Kankanhalli, Jing Ma, Dongxu Li, teowu, Ziyang",
        "link": "https://arxiv.org/abs/2411.13281",
        "github_repo": null,
        "summary": "- Introduces VideoAutoArena, an automated arena-style benchmark for evaluating Large Multimodal Models (LMMs) in video analysis using simulated user interactions and open-ended questions.\n- Employs user simulation with role-playing by LMM agents to generate diverse, realistic questions based on video content and user personas with varying degrees of relevance to the video.\n- Implements a fault-driven evolution strategy to progressively increase question complexity, challenging models to address increasingly difficult video analysis scenarios.\n- Leverages an automated judging system based on the LMM-as-a-Judge paradigm, comparing responses and ranking models using a modified ELO rating system, showing strong alignment with human judgment (87.29%).\n- Introduces an auxiliary benchmark, VideoAutoBench, streamlining LMM evaluation by comparing model responses against human-selected winners from a subset of VideoAutoArena battles, demonstrating consistent ranking results between the arena and the bench.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://videoautoarena.github.io/"
        ],
        "date": "2024-11-21"
    },
    {
        "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents",
        "authors": "Cheng Chang, Kai Zhang, Boyu Gou, Boyuan Zheng, Yu Gu",
        "link": "https://arxiv.org/abs/2411.06559",
        "github_repo": null,
        "summary": "- WEB-DREAMER, a novel model-based planning paradigm for language agents in web environments, is introduced, leveraging LLMs as world models.\n- The approach involves simulating potential outcomes for each candidate action using natural language descriptions generated by LLMs, and then evaluating these imagined outcomes to determine the optimal action at each step.\n- This method addresses the safety risks and practical constraints of directly applying tree search on live websites, particularly for irreversible actions.\n- Empirical results on VisualWebArena and Mind2Web-live demonstrate substantial improvements over reactive baselines.\n- The findings highlight the potential of LLMs as world models and open new avenues for research in optimizing LLMs for world modeling and developing advanced model-based planning algorithms.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/OSU-NLP-Group/WebDreamer"
        ],
        "huggingface_urls": [],
        "date": "2024-11-21"
    },
    {
        "title": "SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory",
        "authors": "Jenq-Neng Hwang, Hsiang-Wei Huang, Cheng-Yen Yang, Nitre, wchai",
        "link": "https://arxiv.org/abs/2411.11922",
        "github_repo": "https://github.com/yangchris11/samurai",
        "summary": "- SAMURAI, an enhanced adaptation of SAM 2 for visual object tracking, is introduced, incorporating temporal motion cues and a motion-aware memory selection mechanism.\n- It addresses SAM 2's limitations in handling crowded scenes and occlusions by predicting object motion and refining mask selection, enabling robust and accurate tracking without retraining or fine-tuning.\n- SAMURAI achieves state-of-the-art zero-shot performance on LaSOT, LaSOText, GOT-10k, and other VOT benchmarks, showing a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k.\n- It operates in real-time and demonstrates strong generalization across diverse datasets, achieving competitive results compared to fully supervised methods on LaSOT.\n- SAMURAI's motion modeling and memory selection modules enhance visual tracking accuracy without additional computational overhead, offering a reliable real-time solution for online VOT.",
        "classification": [
            "Computer Vision",
            "Object Detection"
        ],
        "github_urls": [
            "https://github.com/yangchris11/samurai"
        ],
        "huggingface_urls": [],
        "date": "2024-11-21"
    },
    {
        "title": "Stylecodes: Encoding Stylistic Information For Image Generation",
        "authors": "CiaraRowles",
        "link": "https://arxiv.org/abs/2411.12811",
        "github_repo": "https://github.com/CiaraStrawberry/stylecodes",
        "summary": "- This research introduces StyleCodes, an open-source architecture with training methods for style-conditioned image generation.\n- Stylecodes encodes image styles into short, shareable base64 codes, which offer greater control and collaboration compared to previous image-based conditioning or prompt engineering techniques.\n- The model consists of an attention-based autoencoder and a ControlNet-like UNet decoder, enabling the transfer and control of style from a given image to generated outputs by utilizing residual connections into a frozen pretrained model. \n- Trained using a combination of datasets like MidJourney, CommonCanvas, and JourneyDB, the encoder compresses image styles into 20-digit base64 codes, and initial results indicate effective style transfer with minimal quality loss compared to other methods, along with flexibility in prompt usage and compatibility with existing fine-tuned models. \n- Further investigation is needed regarding computational costs for larger models and biases introduced by dataset limitations, but the work presents a promising approach to more social and collaborative image generation.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/CiaraStrawberry/stylecodes"
        ],
        "huggingface_urls": [],
        "date": "2024-11-21"
    },
    {
        "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training",
        "authors": "Cunxiao Du, Tongyao Zhu, Chao Du, Qian Liu, haonan3",
        "link": "https://arxiv.org/abs/2411.13476",
        "github_repo": "https://github.com/haonan3/AnchorContext",
        "summary": "- This paper introduces AnchorAttention, a novel attention mechanism designed to improve long-context training of Large Language Models (LLMs) by addressing the numerical instability of Rotary Position Embedding (RoPE) when using BFloat16 precision.\n- AnchorAttention mitigates the issue by treating the first token in the input sequence as a shared anchor with a consistent position ID, visible to all documents within the context window, but masking out attention across different documents.\n- This reduces attention computations and the accumulation of numerical errors while ensuring the model learns a full spectrum of rotational angles in RoPE. \n- Experiments on the RULER benchmark and real-world long-context datasets like LongBench show that AnchorAttention outperforms standard full attention and intra-document attention, boosting performance and reducing training time by over 50%.\n- AnchorAttention maintains performance on medium and short context benchmarks such as MMLU and HellaSwag while excelling in long context settings.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/haonan3/AnchorContext"
        ],
        "huggingface_urls": [],
        "date": "2024-11-21"
    },
    {
        "title": "ORID: Organ-Regional Information Driven Framework for Radiology Report Generation",
        "authors": "Dongnan Liu, Ziyong Feng, Xiang An, Tiancheng Gu, Kaichengalex",
        "link": "https://arxiv.org/abs/2411.13025",
        "github_repo": null,
        "summary": "- This paper introduces ORID, an Organ-Regional Information Driven framework, for generating radiology reports from medical images. \n- ORID leverages a fine-tuned multi-modal large language model (LLaVA-Med-RRG) to generate organ-specific diagnostic descriptions.\n- It employs an organ-based cross-modal fusion module and an organ importance coefficient analysis module to integrate image and text features effectively and to reduce the influence of unrelated organ regions, respectively. \n- The framework is evaluated on IU-Xray and MIMIC-CXR datasets, achieving state-of-the-art performance in NLG metrics on both datasets. \n- ORID also demonstrates superior clinical efficacy on MIMIC-CXR compared to other models.",
        "classification": [
            "Image-to-Text",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-21"
    },
    {
        "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
        "authors": "Yangzhou Liu, Yue Cao, Wenhai Wang, Zhe Chen, Weiyun Wang",
        "link": "https://arxiv.org/abs/2411.10442",
        "github_repo": null,
        "summary": "- This paper introduces Mixed Preference Optimization (MPO), a method for enhancing the multimodal reasoning capabilities of Large Language Models (LLMs), by combining supervised fine-tuning loss with preference and quality losses.\n- A new dataset, MMPR, a large-scale multimodal reasoning preference dataset is created using an automated preference data construction pipeline.\n- The InternVL2-8B-MPO model, trained using MPO, achieves state-of-the-art performance on MathVision (25.7% accuracy) among open-source models and a score of 67.0% on MathVista, outperforming the baseline InternVL2-8B by 8.7 points.\n- MPO also leads to improved performance on hallucination benchmarks like POPE and CRPE, and complex VQA benchmarks like MM-Vet and LLaVA-bench, comparable to the much larger InternVL2-76B model.\n- The paper includes ablation studies demonstrating the impact of different optimization algorithms, data scale, and hyperparameters on performance.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Question Answering",
            "Text Generation",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions",
        "authors": "Tianqi Shi, Hao Wang, Bo Zeng, Huifeng Yin, Yu Zhao",
        "link": "https://arxiv.org/abs/2411.14405",
        "github_repo": null,
        "summary": "- Marco-01 is a large language model fine-tuned for enhanced reasoning abilities, focusing on both disciplines with standard answers (math, physics, coding) and open-ended problem-solving.\n- The model leverages Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies.\n- Marco-01 achieved accuracy improvements of +6.17% on MGSM (English) and +5.60% on MGSM (Chinese) datasets, demonstrating enhanced reasoning capabilities. \n- In translation tasks, Marco-01 excels at understanding colloquial nuances, translating slang expressions accurately, and surpassing tools like Google Translate.\n- It employs different granularities for MCTS actions ('steps' and 'mini-steps') to navigate diverse problem complexities and incorporates reflection mechanisms for self-correction and error detection in its reasoning process.",
        "classification": [
            "Question Answering",
            "Natural Language Processing",
            "Translation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs",
        "authors": "Amanpreet Singh, Weijia Shi, Rulin Shao, jacquelinehe, akariasai",
        "link": "https://arxiv.org/abs/2411.14199",
        "github_repo": null,
        "summary": "- Introduced OpenScholar, a retrieval-augmented language model (LM) designed to synthesize information from scientific literature.\n- OpenScholar leverages a specialized datastore of 45 million open-access papers, retrievers trained on scientific text, and an iterative self-feedback generation process.\n- Developed ScholarQABench, a multi-domain benchmark with nearly 3,000 expert-written queries and answers across computer science, physics, neuroscience, and biomedicine, to evaluate OpenScholar and other LMs.\n- Demonstrated that OpenScholar outperforms existing systems, including GPT-4, on ScholarQABench, particularly in citation accuracy, with human evaluation showing preference for OpenScholar responses over expert-written ones in over half of cases.\n- Open-sourced the code, models, datastore, data, and a public demo for OpenScholar and ScholarQABench.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/AkariAsai/OpenScholar",
            "https://github.com/AkariAsai/ScholarBench"
        ],
        "huggingface_urls": [
            "https://huggingface.co/OpenScholar/openscholar-v1"
        ],
        "date": "2024-11-22"
    },
    {
        "title": "Multimodal Autoregressive Pre-training of Large Vision Encoders",
        "authors": "Michal Klein, Philipp Dufter, Xiujun Li, Mustafa Shukor, efini",
        "link": "https://arxiv.org/abs/2411.14402",
        "github_repo": "https://github.com/apple/ml-aim",
        "summary": "- AIMv2, a family of generalist vision encoders, is introduced, utilizing a novel multimodal autoregressive pre-training method with image and text inputs.\n- The model architecture consists of a vision transformer encoder with prefix attention, followed by a causal multimodal decoder that autoregressively generates image patches and text tokens.\n- AIMv2 demonstrates strong performance across various vision and multimodal tasks, including image recognition, object detection, grounding, and multimodal understanding benchmarks.\n- Notably, AIMv2 outperforms state-of-the-art contrastive models like CLIP and DINOv2 in several tasks, exhibiting strong scaling properties with increasing data and parameters.\n- The model achieves 89.5% accuracy on ImageNet-1k with a frozen trunk after high-resolution finetuning.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Image Classification",
            "Object Detection",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/apple/ml-aim"
        ],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "Ultra-Sparse Memory Network",
        "authors": "Defa Zhu, Qiyang Min, Taoer, xyzed, FetchFortune",
        "link": "https://arxiv.org/abs/2411.12364",
        "github_repo": null,
        "summary": "- This paper introduces UltraMem, a novel architecture incorporating large-scale, ultra-sparse memory layers to enhance the performance of transformer models.\n- UltraMem builds upon the Product Key Memory (PKM) concept and introduces several improvements including Tucker Decomposed Query-Key Retrieval, Implicit Value Expansion, and Multi-Core Scoring.\n- The paper claims that UltraMem reduces inference latency while maintaining model performance.\n- Experiments show UltraMem achieves up to 6x speedup compared to Mixture of Experts (MoE) models at the same scale and with a given computation budget.\n- It also exhibits favorable scaling properties, outperforming traditional and MoE models on various benchmarks including MMLU, TriviaQA, and BBH.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
        "authors": "Zijia Chen, Wonmin Byeon, Shizhe Diao, Yonggan Fu, Xin Dong",
        "link": "https://arxiv.org/abs/2411.13676",
        "github_repo": null,
        "summary": "- Hymba, a family of small language models (LLMs), introduces a hybrid-head parallel architecture integrating transformer attention and state space models (SSMs) for improved efficiency and performance.\n- This architecture allows parallel processing, leveraging attention heads for high-resolution recall and SSM heads for efficient context summarization within the same layer.\n- Hymba incorporates learnable meta tokens, prepended to prompts to store critical information and optimize attention allocation, along with cross-layer key-value sharing and partial sliding window attention for a compact cache.\n- Hymba-1.5B-Base surpasses all sub-2B public models and even outperforms Llama-3.2-3B by 1.32% in average accuracy on commonsense reasoning tasks, with an 11.67\u00d7 cache size reduction and 3.49\u00d7 throughput improvement.\n- The instruction-tuned model, Hymba-1.5B-Instruct, achieves state-of-the-art results on various downstream tasks, including GSM8K, GPQA, and the Berkeley function-calling leaderboard.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/NVIDIA/Hymba-1.5B-Base",
            "https://huggingface.co/NVIDIA/Hymba-1.5B-Instruct"
        ],
        "date": "2024-11-22"
    },
    {
        "title": "Natural Language Reinforcement Learning",
        "authors": "Mengyue Yang, Haotian Fu, Ziyu Wan, Xidong Feng, Benjamin-eecs",
        "link": "https://arxiv.org/abs/2411.14251",
        "github_repo": "https://github.com/waterhorse1/Natural-language-RL",
        "summary": "- This paper introduces Natural Language Reinforcement Learning (NLRL), a novel paradigm that reinterprets core RL components (objectives, policies, value functions, Bellman equation) as language-based constructs, leveraging LLMs for improved efficiency, stability, and interpretability.\n- NLRL agents are implemented using LLMs as language policies, value function approximators, Monte Carlo/TD operators, and policy improvement operators. \n- The framework's effectiveness is demonstrated in Maze, Breakthrough, and Tic-Tac-Toe, showcasing its capacity for pure prompting enhancements and gradient-based training with textual feedback. \n- NLRL excels in leveraging textual feedback for decision-making, offering a more stable learning process compared to traditional scalar reward signals. \n- In Tic-Tac-Toe, NLRL demonstrates enhanced performance compared to a PPO baseline and LLM prompting approaches, highlighting the advantages of language-based learning and reasoning in RL.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/waterhorse1/Natural-language-RL"
        ],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
        "authors": "Winston Hu, Jingkang Yang, Hai-Long Sun, Zuyan, THUdyh",
        "link": "https://arxiv.org/abs/2411.14432",
        "github_repo": null,
        "summary": "- Insight-V is a novel system designed to enhance the visual reasoning capabilities of Multimodal Large Language Models (MLLMs) by generating structured reasoning data and employing a multi-agent training approach.\n- The system utilizes a two-step data generation pipeline with a progressive strategy to create diverse reasoning paths and a multi-granularity assessment method to ensure data quality, followed by training a multi-agent MLLM system with a reasoning agent and a summary agent to decompose the problem-solving process.\n- The reasoning agent produces detailed reasoning steps, while the summary agent assesses and selectively utilizes the reasoning to answer the question, with iterative DPO used to refine reasoning quality.\n- Evaluation on seven visual reasoning benchmarks demonstrates significant performance gains, with an average improvement of 7.0% for LLaVA-NeXT and 2.9% for a stronger base MLLM, showcasing the effectiveness and generalizability of Insight-V.\n- Insight-V\u2019s data generation pipeline and multi-agent system enables improvements to MLLM reasoning capabilities without needing expensive human labor.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/dongyh20/Insight-V"
        ],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "Stable Flow: Vital Layers for Training-Free Image Editing",
        "authors": "Kfir Aberman, Egor Nemchinov, Ohad Fried, Or Patashnik, omriav",
        "link": "https://arxiv.org/abs/2411.14430",
        "github_repo": null,
        "summary": "- Stable Flow, a training-free image editing method, leverages the limited diversity of flow-based diffusion models, specifically FLUX and SD3, for consistent edits.\n- The method identifies \"vital layers\" within the Diffusion Transformer (DiT) architecture crucial for image formation through an automatic process that measures content deviation upon layer bypassing.\n- By selectively injecting attention features from a source image into the vital layers during the generation of an edited image, Stable Flow performs various image manipulations, including non-rigid editing, object addition, removal, and global scene editing.\n- An improved image inversion technique called \"latent nudging\" addresses the reconstruction limitations of FLUX for real image editing by perturbing the clean latent before inversion.\n- Quantitative and qualitative comparisons, along with user studies, show Stable Flow's effectiveness across various tasks while maintaining fidelity to the source image content and adherence to textual editing prompts.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages",
        "authors": "Tae-Sun Chung, Akhil Kedia, Bethel Melesse Tessema",
        "link": "https://arxiv.org/abs/2411.14343",
        "github_repo": "https://github.com/bethelmelesse/unifiedcrawl",
        "summary": "- UnifiedCrawl, a method to create large monolingual datasets for low-resource languages by efficiently filtering and extracting data from the Common Crawl corpus using minimal compute resources.\n- Demonstrates that fine-tuning multilingual LLMs with this data using efficient adapter methods like QLoRA significantly improves performance on low-resource languages while minimizing VRAM usage.\n- Shows large improvements in language modeling perplexity and few-shot prompting scores on downstream question-answering tasks.\n- Provides an affordable approach to improve LLMs for low-resource languages using consumer hardware.\n- The extracted Amharic dataset, UnifiedCrawl-Amharic, is significantly larger than existing Amharic datasets and leads to a 24% improvement in F1 score and 77% improvement in EM score on the Amharic question answering dataset, AmQA, when fine-tuning a 4.5B parameter XGLM model using QLoRA.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/bethelmelesse/unifiedcrawl"
        ],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control",
        "authors": "Zhenguo Li, Lanqing Hong, Bo Xiao, Kai Chen, Ruiyuan Gao",
        "link": "https://arxiv.org/abs/2411.13807",
        "github_repo": null,
        "summary": "- MagicDriveDiT, a novel DiT-based diffusion model, generates high-resolution, long street view videos with precise control over object positions, road semantics, and camera trajectories.\n- The model utilizes flow matching for enhanced scalability and incorporates spatial-temporal conditional encoding for precise control over spatial-temporal latents from CogVAE.\n- A progressive bootstrapping training strategy transitions from short, low-resolution videos to long, high-resolution videos, improving convergence and generalization.\n- MagicDriveDiT outperforms previous state-of-the-art methods in generating realistic street scene videos with higher resolution and longer durations, demonstrated by lower FVD scores and improved mAP and mIoU.\n- The model extrapolates to generate videos longer than those seen during training and supports diverse control signals, expanding its potential applications in autonomous driving simulations.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
        "authors": "Neel Nanda, Senthooran Rajamanoharan, Oscar Obeso, Javier Ferrando",
        "link": "https://arxiv.org/abs/2411.14257",
        "github_repo": null,
        "summary": "- This paper investigates the mechanisms behind hallucinations in large language models (LLMs), focusing on entity recognition as a key factor.\n- Using sparse autoencoders (SAEs) as an interpretability tool, the researchers discovered directions in the representation space that detect whether a model recognizes an entity, indicating a form of self-knowledge about its capabilities.\n- These directions are causally relevant, capable of steering the model to refuse answering questions about known entities or hallucinate about unknown ones. \n- The study demonstrates that chat fine-tuning repurposes this existing mechanism from the base model, and that unknown entity recognition directions disrupt the factual recall mechanism by suppressing attention of attribute extraction heads.\n- Additionally, the researchers identify SAE latents seemingly representing uncertainty, which are predictive of incorrect answers.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/jbloom/Gemma-2b-IT-Residual-Stream-SAEs",
            "https://huggingface.co/datasets/HuggingFaceFW/fineweb"
        ],
        "date": "2024-11-22"
    },
    {
        "title": "Patience Is The Key to Large Language Model Reasoning",
        "authors": "Yijiong Yu",
        "link": "https://arxiv.org/abs/2411.13082",
        "github_repo": null,
        "summary": "- This paper proposes a method for improving large language model (LLM) reasoning ability by encouraging a more patient reasoning style, without requiring new knowledge or skills.\n- The method involves training LLMs to favor thorough reasoning processes by using preference optimization, where detailed reasoning is treated as positive examples and simple answers as negative examples.\n- The model is fine-tuned using a lightweight dataset of mathematical problems and their solutions, which are further refined into more detailed steps.\n- Experimental results demonstrate a performance increase of up to 6.7% on the GSM8k benchmark, surpassing several previous works despite using less data and a simpler method.\n- Although the method increases inference time, the trade-off is considered worthwhile for enhanced accuracy in complex problem-solving.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-22"
    },
    {
        "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
        "authors": "Sungroh Yoon, Heeseung Kim, Yeongtak, chaehun, jychoi",
        "link": "https://arxiv.org/abs/2411.14793",
        "github_repo": null,
        "summary": "- This paper introduces a novel Style-friendly SNR sampler to enhance personalized artistic style learning in large-scale text-to-image diffusion models by fine-tuning.\n- Motivated by the observation that styles emerge at higher noise levels, this sampler modifies the noise level sampling during fine-tuning, biasing it toward higher noise levels (lower log-SNR values) where stylistic features emerge.\n- Experiments fine-tune FLUX-dev and Stable Diffusion 3.5 using LoRA on style references from StyleDrop, demonstrating improved style capture over baselines using metrics like DINO and CLIP similarity, along with human evaluation of style and text alignment.\n- It achieves higher DINO and CLIP-I scores and outperforms even increasing the LoRA rank or model capacity.\n- The method enables new applications like creating coherent multi-panel comics and stylized typography from single reference images.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training",
        "authors": "Hamish Ivison, Shengyi Huang, Valentina Pyatkin, Jacob Morrison, Nathan Lambert",
        "link": "https://arxiv.org/abs/2411.15124",
        "github_repo": null,
        "summary": "- T\u00dcLU 3 is a family of open-source, state-of-the-art language models fine-tuned from Llama 3.1 using a novel four-stage post-training recipe.\n- The recipe includes supervised fine-tuning, preference tuning using Direct Preference Optimization, and a novel Reinforcement Learning with Verifiable Rewards (RLVR) stage.\n- RLVR trains models against ground truth rewards for specific skills with verifiable answers (e.g., math, precise instruction following). \n- T\u00dcLU 3 models outperform existing open-weight models and some closed models, like GPT 3.5 Turbo and GPT 40 mini, in terms of average performance across benchmarks.\n- The release includes model weights, training code, evaluation suite, and datasets related to various core skills.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Question Answering",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/allenai/open-instruct",
            "https://github.com/allenai/olmes"
        ],
        "huggingface_urls": [
            "https://hf.co/allenai/Llama-3.1-Tulu-3-70B",
            "https://hf.co/allenai/Llama-3.1-Tulu-3-8B",
            "https://hf.co/collections/allenai/tulu-3-datasets-673b8df14442393f7213f372"
        ],
        "date": "2024-11-25"
    },
    {
        "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
        "authors": "Shaun Khoo, shingurding, gabrielchua",
        "link": "https://arxiv.org/abs/2411.12946",
        "github_repo": null,
        "summary": "- This paper introduces a flexible, data-free methodology for developing guardrails for Large Language Models (LLMs) to prevent off-topic misuse.\n- The methodology involves using an LLM to generate a synthetic dataset of on-topic and off-topic prompts based on a qualitative problem analysis, then fine-tuning embedding (jina-embeddings-v2-small-en) or cross-encoder (stsb-roberta-base) models on this data to create off-topic classifiers.\n- The resulting guardrails outperform baseline methods, including cosine similarity, KNN, and zero-shot classification using LLMs, demonstrating higher precision in identifying off-topic prompts.\n- By framing the detection task as assessing relevance between system and user prompts, the guardrail effectively generalizes to other misuse categories such as jailbreak and harmful prompts.\n- The synthetic dataset and trained models are open-sourced to facilitate research and development in LLM safety.",
        "classification": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/gabrielchua/off-topic",
            "https://huggingface.co/collections/govtech/off-topic-guardrail-673838a62e4c661f248e81a4"
        ],
        "date": "2024-11-25"
    },
    {
        "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
        "authors": "Xinchao Wang, Qiaochu Xue, Xingyi Yang, Songhua Liu, Zhenxiong Tan",
        "link": "https://arxiv.org/abs/2411.15098",
        "github_repo": null,
        "summary": "- OminiControl is a parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models for image generation by leveraging a parameter reuse mechanism, which enables the DiT to use itself as an encoder.\n- It uses the existing VAE encoder to process conditioning images, which are integrated with learnable position embeddings alongside latent noise in the denoising network. \n- This design facilitates direct multi-modal attention interactions between condition and generation tokens, improving information exchange and control signal propagation. \n- Experimental results demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation tasks. \n- A new training dataset, Subjects200K, containing over 200,000 identity-consistent image pairs was created to advance research in subject-consistent generation.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/Yuanshi9815/OminiControl",
            "https://github.com/Yuanshi9815/Subjects200K"
        ],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal Models",
        "authors": "Ziwei Liu, Bo Li, Yifei Shen, Kaichen Zhang",
        "link": "https://arxiv.org/abs/2411.14982",
        "github_repo": null,
        "summary": "- This paper introduces a framework to interpret the internal representations of Large Multimodal Models (LMMs), focusing on understanding their open-semantic features.\n- It employs Sparse Autoencoders (SAE) to disentangle complex representations into simpler, human-understandable features.\n- An automatic interpretation pipeline leverages larger LMMs to provide zero-shot explanations of these features, offering insights into model behavior.\n- The framework demonstrates the ability to steer LMM behavior by manipulating specific features, enabling targeted interventions and analysis of model decisions.\n- The authors analyze LLaVA-NeXT-8B using LLaVA-OV-72B and demonstrate effective feature interpretation and behavior steering, contributing to a deeper understanding of LMMs' strengths and weaknesses.",
        "classification": [
            "Multimodal",
            "Image Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
        "authors": "Xiu Su, Le Zhuo, Hairong Shi, Wei Huang, Songhao Han",
        "link": "https://arxiv.org/abs/2411.14794",
        "github_repo": "https://github.com/hshjerry/VideoEspresso",
        "summary": "- VideoEspresso, a large-scale video question-answering (VideoQA) dataset, is introduced, focusing on fine-grained video reasoning and featuring VideoQA pairs that retain spatial and temporal context.\n- An automatic pipeline is used to construct the dataset, employing semantic key-frame extraction and GPT-40 for generating QA pairs and enriching them with Chain-of-Thought annotations.\n- A Hybrid LVLMs Collaboration framework is proposed, incorporating a Frame Selector and a two-stage reasoning LVLM to address cost-effectiveness and accuracy in video reasoning.\n- Evaluation on a 14-task benchmark against popular LVLMs shows superior performance, highlighting improved video reasoning capabilities compared to baseline methods.\n- Both objective and subjective evaluations, incorporating metrics such as logic, factuality, accuracy, and conciseness, showcase the dataset and model efficacy.",
        "classification": [
            "Visual Question Answering",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/hshjerry/VideoEspresso"
        ],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction",
        "authors": "Pieter Abbeel, Jinwoo Shin, Sihyun Yu, Huiwon Jang, younggyoseo",
        "link": "https://arxiv.org/abs/2411.14762",
        "github_repo": null,
        "summary": "- This paper introduces CoordTok, a novel video tokenizer designed for efficient encoding of long videos.\n- CoordTok leverages a coordinate-based patch reconstruction approach, encoding videos into factorized triplane representations and reconstructing patches corresponding to randomly sampled (x, y, t) coordinates, enabling efficient training on long videos.\n- Experiments demonstrate that CoordTok significantly reduces the number of tokens required to encode long videos compared to existing methods, achieving comparable reconstruction quality with substantially fewer tokens (e.g., encoding a 128-frame video into 1280 tokens compared to 6144 or 8192 tokens for baselines).\n- CoordTok's efficiency enables memory-efficient training of a diffusion transformer for long video generation, capable of generating 128 frames at once, leading to improved generation quality.\n- Further analysis reveals that CoordTok effectively captures temporal coherence in videos, leading to more efficient tokenization.",
        "classification": [
            "Computer Vision",
            "Text-to-Video",
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "Novel View Extrapolation with Video Diffusion Priors",
        "authors": "Shijian Lu, Ling Shao, KunhaoLiu",
        "link": "https://arxiv.org/abs/2411.14208",
        "github_repo": null,
        "summary": "- ViewExtrapolator, a novel view synthesis approach, leverages Stable Video Diffusion (SVD) priors to extrapolate novel views far beyond training view ranges.\n- It refines artifact-prone renderings from radiance fields or point clouds by guiding SVD's denoising process to preserve scene content using guidance and resampling annealing.\n- This training-free method enhances clarity and realism without fine-tuning SVD, making it efficient.\n- ViewExtrapolator generalizes to various 3D rendering methods, showing superior performance in novel view extrapolation compared to existing techniques.\n- Quantitative and qualitative results demonstrate its effectiveness across different scenes and rendering approaches.",
        "classification": [
            "Computer Vision",
            "Image-to-Image",
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "MyTimeMachine: Personalized Facial Age Transformation",
        "authors": "David W. Jacobs, Annie N. Wang, Bang Gong, Jiaye Wu, Luchao Qi",
        "link": "https://arxiv.org/abs/2411.14521",
        "github_repo": null,
        "summary": "- MyTimeMachine (MyTM) is a personalized facial age transformation network that leverages a personal photo collection (as few as 50 images) and a global aging prior (SAM) to generate realistic age-transformed faces.\n- MyTM introduces a novel Adapter Network, which combines personalized aging features learned from the individual's photo collection with global aging features from SAM, generating a re-aged image using StyleGAN2.\n- The training process utilizes three loss functions: personalized aging loss, extrapolation regularization, and adaptive w-norm regularization, which ensure identity preservation, accurate aging within and beyond the training data's age range, and distinct age-related facial changes.\n- MyTM outperforms existing global age transformation methods and naive personalization techniques in both age regression and age progression tasks, achieving higher identity preservation while maintaining age accuracy.\n- The approach extends to video re-aging, producing temporally consistent results by applying face-swapping techniques to a re-aged keyframe generated by MyTM.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
        "authors": "Maciej Wolczyk, Ulyana Piterbarg, Samuel Coward, Bart\u0142omiej Cupia\u0142, pagli98",
        "link": "https://arxiv.org/abs/2411.13543",
        "github_repo": null,
        "summary": "- BALROG, a new benchmark designed to assess the agentic capabilities of LLMs and VLMs in complex, dynamic game environments, is introduced.\n- The benchmark incorporates six diverse reinforcement learning environments: BabyAI, Crafter, TextWorld, Baba Is AI, MiniHack, and NetHack, each testing various agentic skills like long-term planning and spatial reasoning.\n- Evaluations of several state-of-the-art LLMs and VLMs reveal that while they perform reasonably well on simpler tasks, they struggle significantly with more challenging environments like NetHack.\n- A notable observation is the poor performance of VLMs when presented with visual input, indicating a deficiency in vision-based decision-making.\n- The BALROG benchmark and framework, with its fine-grained metrics and support for various prompting strategies, are open-sourced to facilitate research in the agentic community.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "One to rule them all: natural language to bind communication, perception and action",
        "authors": "Giuseppe Boccignone, Dimitri Ognibene, colo286",
        "link": "https://arxiv.org/abs/2411.15033",
        "github_repo": null,
        "summary": "- This research presents a novel architecture for robot action planning which integrates Large Language Models (LLMs), perception, and action planning using a modified ReAct framework.\n- The core component, the Planner Module, translates natural language commands into executable actions and uses LLMs in a modified ReAct framework to dynamically adjust plans based on real-time feedback and environmental perception. \n- The system leverages scene graphs for semantic mapping and context understanding while utilizing an execution control mechanism and a failure management system to enable robust error handling. \n- Preliminary testing on RoBee, a humanoid robot, in a simulated kitchen and bedroom environment demonstrates the efficacy of the architecture.\n- It is able to handle simple, and moderately complex requests with good success rates, with future work directed to improve performance on complex, and open-ended requests and refining real-world integration.",
        "classification": [
            "Robotics",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
        "authors": "Ge Yang, Sai Aneesh Suryadevara, Xuanbin Peng, Yuchen Song, Ri-Zhao Qiu",
        "link": "https://arxiv.org/abs/2411.15131",
        "github_repo": null,
        "summary": "- WildLMa is a framework for in-the-wild mobile manipulation with quadruped robots, addressing the challenges of generalizability, long-horizon task execution, and complex manipulation beyond pick-and-place.\n- It combines a whole-body controller adapted for VR teleoperation, a skill library (WildLMa-Skill) learned through imitation learning and heuristics, and a language interface (WildLMa-Planner) for LLM-based skill coordination.\n- WildLMa-Skill enhances generalizability through language-conditioned imitation learning with CLIP and improves success rate in grasping tasks over existing RL baselines and a zero-shot method using only tens of demonstrations.\n- WildLMa-Planner utilizes a hierarchical approach for coordinating skills for long-horizon tasks, successfully completing tasks like cleaning up trash and rearranging items.\n- The framework is demonstrated through quantitative evaluations (showing improved success rates) and qualitative results on real-world robot deployments in diverse environments.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://wildlma.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-11-25"
    },
    {
        "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
        "authors": "Qing Wang, Ziwei Liu, Tengfei Wang, xanderhuang",
        "link": "https://arxiv.org/abs/2411.15138",
        "github_repo": null,
        "summary": "- Material Anything is a novel diffusion-based framework for generating physically based rendering (PBR) materials for any 3D object, addressing the limitations of existing methods in handling diverse lighting and texture conditions.\n- It employs a two-stage pipeline: an image-space material diffusion model with a triple-head architecture and rendering loss, followed by a UV-space material refinement model to ensure consistency.\n- A confidence mask guides the model to leverage lighting cues or semantic information based on input quality, enabling it to handle various scenarios like texture-less, albedo-only, and generated objects.\n- A progressive material generation strategy maintains consistency across multiple views.\n- Experimental results on Material3D, a new dataset of 80K objects, demonstrate Material Anything's state-of-the-art performance, exceeding existing texture generation and optimization-based methods in quality and efficiency.",
        "classification": [
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
        "authors": "Sungroh Yoon, Heeseung Kim, Jooyoung Choi, Chaehun Shin",
        "link": "https://arxiv.org/abs/2411.15466",
        "github_repo": null,
        "summary": "- This paper introduces Diptych Prompting, a novel zero-shot, subject-driven text-to-image generation framework based on diptych inpainting using large-scale text-to-image models (specifically, FLUX).\n- The method reinterprets subject-driven generation as an inpainting task where a reference image of the subject is placed in the left panel of a diptych, and the right panel is generated through text-conditioned inpainting based on a user prompt describing the desired context.\n- To improve subject fidelity and prevent content leakage, the framework incorporates background removal from the reference image and enhances attention weights between the diptych panels during inpainting.\n- Experimental results on DreamBench show that Diptych Prompting outperforms existing encoder-based image prompting methods in both qualitative comparisons and a human preference study, effectively capturing both subject details and text prompts.\n- Furthermore, the method's versatility is demonstrated by extending it to stylized image generation and subject-driven image editing.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/black-forest-labs/FLUX.1-dev",
            "https://huggingface.co/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta"
        ],
        "date": "2024-11-26"
    },
    {
        "title": "Knowledge Transfer Across Modalities with Natural Language Supervision",
        "authors": "Marco Grangetto, Emanuele Aiello, luca-molinaro, carloalbertobarbano",
        "link": "https://arxiv.org/abs/2411.15611",
        "github_repo": null,
        "summary": "- This paper introduces Knowledge Transfer, a method for teaching pre-trained visual models new concepts using only textual descriptions.\n- The method leverages cross-modal interactions, hypothesizing that pre-trained visual encoders possess sufficient low-level features (shape, appearance, color) to represent unknown high-level concepts when provided with a textual description.\n- Explicit Knowledge Transfer involves synthesizing images via model inversion based on the textual description and then fine-tuning the visual encoder using an image-text matching loss.\n- Experiments demonstrate successful introduction of novel concepts into CLIP and ViLT models, with improved zero-shot performance on various tasks, including classification, segmentation, and image-text retrieval.\n- The method also shows potential for out-of-domain generalization, such as introducing medical concepts into models trained on natural images.",
        "classification": [
            "Multimodal",
            "Zero-Shot Classification",
            "Zero-Shot Image Classification",
            "Image Classification",
            "Image Segmentation",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
        "authors": "Chengshuai Zhao, Alimohammad Beigi, Liangjie Huang, Bohan Jiang, Dawei Li",
        "link": "https://arxiv.org/abs/2411.16594",
        "github_repo": "https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge",
        "summary": "- This paper surveys the emerging field of \"LLM-as-a-judge,\" where Large Language Models (LLMs) are used for scoring, ranking, or selecting content in various AI tasks.\n- The paper introduces a taxonomy categorizing LLM-as-a-judge approaches based on what to judge (e.g., helpfulness, harmlessness), how to judge (tuning, prompting), and where to judge (evaluation, alignment, retrieval, reasoning).\n- The study compiles existing benchmarks for LLM-as-a-judge evaluation, covering aspects like general performance, bias detection, and domain-specific tasks.\n- The authors highlight challenges such as bias, dynamic judgment, and the need for self-judging capabilities.\n- Finally, they suggest promising future directions like incorporating Retrieval Augmented Generation (RAG), creating bias-aware datasets, and developing human-LLM co-judgement systems.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "MH-MoE:Multi-Head Mixture-of-Experts",
        "authors": "Furu Wei, Shuming Ma, Xun Wu, Shaohan Huang",
        "link": "https://arxiv.org/abs/2411.16205",
        "github_repo": null,
        "summary": "- This paper introduces a novel implementation of Multi-Head Mixture-of-Experts (MH-MoE), enhancing the multi-head mechanism by enabling it to attend to information from various representation spaces within different experts.\n- The key modifications include adding a \"heads\" dimension to the token dimension and incorporating two linear projection layers at the beginning and end of the MoE layer.\n- The new implementation maintains FLOPs and parameter parity with sparse Mixture of Experts models, which improves efficiency.\n- Experimental results on language models show that this new implementation leads to quality improvements over both vanilla MoE and fine-grained MoE models.\n- Additionally, the paper demonstrates MH-MoE's compatibility with 1-bit Large Language Models (LLMs), like BitNet.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
        "authors": "Mohit Bansal, Jaehong Yoon, Han Lin, Jialu Li, Zun Wang",
        "link": "https://arxiv.org/abs/2411.16657",
        "github_repo": null,
        "summary": "- DREAMRUNNER is a novel story-to-video generation model that uses a hierarchical system with a large language model (LLM) to create detailed scripts and plans for multi-scene, character-driven videos.\n- It employs retrieval-augmented test-time adaptation with motion-oriented videos and reference images to learn motion and subject priors, enabling fine-grained video generation with enhanced motion quality and seamless transitions.\n- DREAMRUNNER introduces SR3AI, a spatial-temporal region-based 3D attention and prior injection module, to bind objects and motions precisely while controlling frame-level semantics.\n- Evaluated on a new dataset, DreamStorySet, and T2V-CompBench, DREAMRUNNER outperforms existing methods in character consistency, text alignment, and smooth transitions, demonstrating strong performance in generating multi-character interactions and compositional video generation.\n- Notably, DREAMRUNNER shows a 13.1% relative improvement in character consistency, an 8.56% relative gain in text-following ability, and a 27.2% relative improvement in transition smoothness compared to previous state-of-the-art methods.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [
            "https://dreamrunner-story2video.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "Factorized Visual Tokenization and Generation",
        "authors": "Zheng Zhang, Pichao Wang, Ziteng Gao, Jianxiong Gao, Zechen Bai",
        "link": "https://arxiv.org/abs/2411.16681",
        "github_repo": null,
        "summary": "- This paper introduces Factorized Quantization (FQ), a novel approach to improve visual tokenization for image generation by decomposing a large codebook into multiple smaller, independent sub-codebooks.\n- FQ enhances training stability and efficiency by reducing lookup complexity.\n- A disentanglement regularization mechanism and representation learning with pre-trained vision models like CLIP and DINOv2 encourage diversity and semantic richness in learned features, leading to more expressive representations.\n- Experiments show FQGAN substantially improves image reconstruction quality, achieving state-of-the-art performance, and can be effectively adapted for auto-regressive image generation.\n- The proposed FQGAN model outperforms existing VQ-based and LFQ-based models in image reconstruction quality, as measured by reconstruction FID.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://showlab.github.io/FQGAN"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?",
        "authors": "Yuxiang Zheng, Yixiu Liu, Xuefeng Li, Haoyang Zou, Zhen Huang",
        "link": "https://arxiv.org/abs/2411.16489",
        "github_repo": null,
        "summary": "- This paper investigates replicating OpenAI's O1 model capabilities, particularly its long-thought chain reasoning abilities using knowledge distillation from O1's API and supervised fine-tuning.\n- The authors demonstrate that a 72B parameter base model, fine-tuned on a distilled dataset of tens of thousands of samples, outperforms the O1-preview model on the American Invitational Mathematics Examination (AIME).\n-  The study also explores the generalization capabilities of the distilled model on diverse tasks including hallucination, safety, and open-domain QA, demonstrating strong generalization and reduced susceptibility to sycophancy despite training solely on mathematical problem-solving data.\n- The authors emphasize the potential risks of over-reliance on distillation techniques, highlighting concerns about transparency, innovation stagnation, and the cultivation of first-principles thinking in AI research. \n- A Technical Transparency Index (TTI) framework is proposed for evaluating and comparing O1 replication efforts based on transparency and reproducibility, advocating for a shift in research culture toward more transparent and innovative approaches.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/GAIR-NLP/01-Journey"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI",
        "authors": "Zhe Chen, Bin Fu, Wei Li, Yanzhou Su, foreverbeliever",
        "link": "https://arxiv.org/abs/2411.14522",
        "github_repo": "https://github.com/uni-medical/GMAI-VL",
        "summary": "- This paper introduces GMAI-VL, a large vision-language model for general medical AI, and GMAI-VL-5.5M, a comprehensive multimodal medical dataset.\n- GMAI-VL-5.5M is constructed from 219 specialized medical datasets, covering 13 medical imaging modalities and 18 specialties, totaling 5.5M image-text pairs.\n- GMAI-VL employs a three-stage training strategy: shallow alignment (projector training), deep alignment (vision encoder and projector training), and instruction tuning.\n- Experimental results indicate that GMAI-VL achieves state-of-the-art performance across various multimodal medical tasks, including visual question answering and medical image diagnosis, outperforming existing models on benchmarks such as VQA-RAD, OmniMedVQA, and Health & Medicine track of MMMU.\n- The authors claim that the comprehensive task coverage, diverse modalities, and high-quality of GMAI-VL-5.5M contribute to GMAI-VL's superior performance.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/uni-medical/GMAI-VL"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "VisualLens: Personalization through Visual History",
        "authors": "Zhaojiang Lin, Yi Lu, Kai Sun, Deqing Fu, Wang Bill Zhu",
        "link": "https://arxiv.org/abs/2411.16034",
        "github_repo": null,
        "summary": "- VisualLens is a novel approach for personalized recommendations that leverages a user's task-agnostic visual history, such as personal photos, instead of relying on traditional task-specific interaction logs or textual data.\n- The method addresses challenges like diversity and noise in visual histories by retrieving relevant images, extracting visual embeddings, captions, and aspect words, and using an iterative refinement process to improve aspect extraction.\n- VisualLens employs a grid-based approach to process multiple images simultaneously, reducing computational overhead, and uses a joint training strategy to enhance aspect word generation and candidate prediction.\n- Experimental results on two newly created benchmarks, Google Review-V and Yelp-V, demonstrate significant improvements over baselines and state-of-the-art methods, including a 1.6-4.6% improvement on Hit@3 over GPT-40.\n- The model uses an 8x8 grid for images along with their related captions and aspect words, which are combined with a d=2048 vector input to make predictions.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "One Diffusion to Generate Them All",
        "authors": "Aniruddha Kembhavi, Christopher Clark, Sangho Lee, Tuan Pham, Duong H. Le",
        "link": "https://arxiv.org/abs/2411.16318",
        "github_repo": "https://github.com/lehduong/OneDiffusion",
        "summary": "- OneDiffusion, a unified, large-scale diffusion model, supports bidirectional image synthesis and understanding across diverse tasks, including text-to-image generation, image editing using various modalities (depth, pose, layout, semantic maps), image restoration (deblurring, upscaling), and multi-view generation.\n- The model employs a straightforward approach, treating all tasks as frame sequences with varying noise scales during training, eliminating the need for specialized architectures.\n- Using a novel \"One-Gen\" dataset, integrating diverse sources and synthetic data, enables scalable joint training across various tasks and resolutions.\n- Experimental results demonstrate OneDiffusion achieves competitive or state-of-the-art performance on text-to-image generation, multi-view generation, depth estimation, and ID customization, showcasing its strong generalization capabilities.\n- OneDiffusion supports novel conditioning setups, including text-to-multi-view and image-to-multi-view generation.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal",
            "Image-to-3D",
            "Computer Vision",
            "Depth Estimation",
            "Keypoint Detection"
        ],
        "github_urls": [
            "https://github.com/lehduong/OneDiffusion"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "Cautious Optimizers: Improving Training with One Line of Code",
        "authors": "Qiang Liu, Bo Liu, Lizhang Chen, Kaizhao Liang",
        "link": "https://arxiv.org/abs/2411.16085",
        "github_repo": "https://github.com/kyleliang919/C-Optim",
        "summary": "- This paper introduces Cautious Optimizer, a one-line code modification for momentum-based optimizers like AdamW and Lion.\n- The modification involves masking updates where the proposed update direction and current gradient are misaligned, leading to faster and more stable training.\n- Theoretical analysis shows that this change preserves convergence guarantees and speeds up loss reduction without breaking the Hamiltonian function or Lyapunov analysis.\n- Empirical results demonstrate significant speed-ups of up to 1.47x on tasks such as LLaMA and MAE pretraining.\n- The cautious variants achieve improved sample efficiency with virtually no computational overhead, outperforming standard optimizers across different model sizes and datasets.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/kyleliang919/C-Optim"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
        "authors": "Forrest McKee, David Noever",
        "link": "https://arxiv.org/abs/2411.14486",
        "github_repo": null,
        "summary": "- This research paper introduces a novel evaluation framework, the \"impossible test,\" using a dataset of 675 fundamentally unsolvable problems to assess large language models' (LLMs) ability to acknowledge uncertainty.\n- Twelve state-of-the-art LLMs were evaluated on their propensity to admit \"I don't know\" rather than generate incorrect responses, with the best models achieving 62-68% accuracy in acknowledging uncertainty.\n- An inverse relationship was observed between problem difficulty and model accuracy, with GPT-4 demonstrating better uncertainty recognition on harder problems (35.8%) compared to simpler ones (20%).\n- The study reveals that models may be more prone to speculate when problems seem solvable and highlights variations in uncertainty acknowledgment across problem categories.\n- The impossible test contributes to Artificial General Intelligence (AGI) assessment by emphasizing uncertainty recognition as crucial for evaluating machine intelligence and prompting new directions for model training.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/reveondivad/certify"
        ],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
        "authors": "Soonwoo Kwon, Jin-Young Kim, Jiho Jang, Byeongjun Park, Hyojun Go",
        "link": "https://arxiv.org/abs/2411.16443",
        "github_repo": null,
        "summary": "- SplatFlow is a novel framework for text-to-3D Gaussian Splatting (3DGS) generation and editing, consisting of a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder).\n- The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses conditioned on text prompts, effectively handling diverse scene scales and complex camera trajectories.\n- The GSDecoder translates these latent representations into pixel-aligned 3DGS using an efficient feed-forward approach.\n- SplatFlow utilizes training-free inversion and inpainting for 3DGS editing and supports various 3D tasks like object editing, novel view synthesis, and camera pose estimation, outperforming existing methods on MVImgNet and DL3DV-7k datasets in FID and CLIP Score.\n- Leveraging Stable Diffusion 3\u2019s encoder enhances generation quality and enables flexible integration with other generative models.",
        "classification": [
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "Predicting Emergent Capabilities by Finetuning",
        "authors": "Sergey Levine, Dan Klein, Eric Wallace, sea-snell",
        "link": "https://arxiv.org/abs/2411.16035",
        "github_repo": null,
        "summary": "- This paper proposes a novel method for predicting the emergence of capabilities in large language models (LLMs) by leveraging task-specific finetuning.\n- The key insight is that finetuning shifts the point of emergence to smaller models, and the amount of data controls the magnitude of this shift. \n- They introduce an \"emergence law\", a parametric function fitted to finetuning data to model this shift, extrapolating it to predict few-shot emergence.\n-  Evaluating on standard NLP benchmarks (MMLU, GSM8K, CommonsenseQA, and CoLA), they demonstrate accurate emergence prediction up to 4x the FLOPs in advance using only small pre-emergence models.\n- Case studies on data quality assessment (OpenLLaMA v1 vs. v2) and complex capability prediction (LLaMA 2 on APPS coding) showcase potential real-world applications.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation",
        "authors": "Zhongying Deng, Haoyu Wang, Yanjun Li, Ying Chen, Jin Ye",
        "link": "https://arxiv.org/abs/2411.14525",
        "github_repo": null,
        "summary": "- This paper introduces SegBook, a large-scale benchmark for evaluating transfer learning in volumetric medical image segmentation.\n- It employs STU-Net, a scalable model pre-trained on a large full-body CT dataset (TotalSegmentator), to evaluate transfer learning across 87 public datasets with diverse modalities, targets, and sizes.\n- The results reveal a bottleneck effect where fine-tuning benefits diminish beyond a certain dataset size, with greater improvement on small and large datasets than medium-sized ones.\n- The pre-trained models show effective transfer to other modalities like MRI, irrespective of target presence during pre-training.\n- Moreover, pre-training on CT structural data shows promising results in both structure and lesion detection, highlighting adaptability across tasks.",
        "classification": [
            "Image Segmentation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "From CISC to RISC: language-model guided assembly transpilation",
        "authors": "Abdulrahman Mahmoud, Rania Hossam, Chaimaa Abi, Ahmed Heakl",
        "link": "https://arxiv.org/abs/2411.16341",
        "github_repo": null,
        "summary": "- This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM and RISC-V assembly.\n- The approach uses a fine-tuned DeepSeekCoder-1.3B model with an extended tokenizer to handle assembly code specifics and a longer context window to capture complex semantic relationships.\n- Evaluated on real-world applications and compared against Apple's Rosetta 2, CRT achieves 79.25% translation accuracy from x86 to ARMv5 and 88.68% from x86 to RISC-V, along with 1.73x speedup, 1.47x better energy efficiency, and 2.41x better memory efficiency compared to Rosetta 2.\n- Analysis of the transpiled code reveals that the model maintains functional correctness despite syntactic variations, demonstrating the feasibility of LLM-based binary translation.\n- Further investigation revealed specific error patterns related to register allocation, memory addressing, and numerical token handling, indicating potential areas for future improvements.",
        "classification": [
            "Natural Language Processing",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/blog/lorinma/yi-coder"
        ],
        "date": "2024-11-26"
    },
    {
        "title": "Best of Both Worlds: Advantages of Hybrid Graph Sequence Models",
        "authors": "Bryan Perozzi, Clayton Sanford, Mahdi Karami, Ali Parviz, Ali Behrouz",
        "link": "https://arxiv.org/abs/2411.15671",
        "github_repo": null,
        "summary": "- This paper introduces Graph Sequence Model (GSM), a unifying framework for adapting sequence models to graph-structured data, encompassing tokenization, local encoding, and global encoding steps.\n- Theoretical analysis reveals that recurrent models excel in counting tasks while Transformers struggle due to permutation equivariance; however, both suffer from representational collapse with increasing depth.\n- The paper also finds transformers are more effective than recurrent models for connectivity tasks. \n- This paper proposes GSM++, a hybrid model using hierarchical tokenization based on the Hierarchical Affinity Clustering (HAC) algorithm and a hybrid Transformer-Recurrent architecture.\n- Experimental results demonstrate GSM++'s superior performance on various benchmark graph tasks compared to existing methods.",
        "classification": [
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-26"
    },
    {
        "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
        "authors": "Shiwei Wu, Zhengyuan Yang, Difei Gao, Linjie Li, Kevin Qinghong Lin",
        "link": "https://arxiv.org/abs/2411.17465",
        "github_repo": "https://github.com/showlab/ShowUI",
        "summary": "- ShowUI, a novel 2B parameter vision-language-action (VLA) model, is designed to enhance Graphical User Interface (GUI) automation by processing visual screenshots, textual instructions, and generating appropriate actions.\n- The model employs UI-Guided Visual Token Selection, a technique that leverages connected components within UI screenshots to represent redundant visual areas, thus reducing computational overhead by 33% and improving training speed by 1.4x.\n- ShowUI implements Interleaved Vision-Language-Action Streaming to effectively manage interactions across different modalities and handle navigation history within multi-step GUI tasks.\n- Trained on a small, high-quality GUI instruction-following dataset consisting of only 256K samples, ShowUI achieves 75.1% accuracy in zero-shot screenshot grounding, outperforming larger models trained on larger datasets.\n- ShowUI also demonstrates strong navigation capabilities across diverse environments, including web, mobile, and online platforms.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/showlab/ShowUI"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "Star Attention: Efficient LLM Inference over Long Sequences",
        "authors": "Boris Ginsburg, Fei Jia, Shantanu Acharya",
        "link": "https://arxiv.org/abs/2411.17116",
        "github_repo": "https://github.com/NVIDIA/Star-Attention",
        "summary": "- Star Attention, a two-phase block-sparse approximation method, is introduced to improve the efficiency of Large Language Model (LLM) inference over long sequences.\n- The method involves an initial context encoding phase with blockwise local attention, followed by a query encoding and token generation phase using global attention via a distributed softmax algorithm.\n- This approach allows context length to scale linearly with the number of hosts, reducing memory requirements and inference time.\n- Evaluation on Llama3.1-8B and Llama3.1-70B across long-context benchmarks shows up to 11x faster inference while maintaining 95-100% accuracy compared to baseline methods.\n- Star Attention is compatible with existing LLM optimization techniques like Flash Attention and KV cache compression for potential further speed improvements.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/NVIDIA/Star-Attention"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration",
        "authors": "Honggang Chen, Donglin Wang, Pengxiang Ding, Xuyang Liu, Yuhang Han",
        "link": "https://arxiv.org/abs/2411.17686",
        "github_repo": null,
        "summary": "- This paper introduces a novel \"filter-correlate-compress\" paradigm for training-free token reduction in Multimodal Large Language Models (MLLMs), decomposing the process into three distinct stages for improved interpretability and flexibility.\n- A suite of methods called FiCoCo is proposed, implementing this paradigm with variants for different MLLM inference phases.\n- FiCoCo leverages intermediate computation products to minimize FLOPs and achieves up to 82.4% FLOPs reduction with minimal performance impact.\n- Experimental results on 10 multimodal benchmarks show FiCoCo outperforms state-of-the-art training-free token reduction methods.\n- Notably, FiCoCo achieves comparable performance to LLaVA-1.5-7B with just 17.6% of the computational cost and 67.6% of GPU memory in practical applications.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://ficoco-accelerate.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
        "authors": "Xinyu Fang, Bo Li, Shukang Yin, Chaoyou Fu, yifanzhang114",
        "link": "https://arxiv.org/abs/2411.15296",
        "github_repo": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks",
        "summary": "- This paper presents a comprehensive survey of evaluation methods for Multimodal Large Language Models (MLLMs), categorizing existing benchmarks based on the capabilities they assess (foundational, behavioral, and application-oriented).\n- The survey discusses the benchmark construction process, including data collection, annotation, and potential challenges like data contamination and benchmark diversity.\n- It also covers evaluation methods (human, LLM/MLLM, and script-based) and introduces common metrics and available toolkits for streamlined evaluation.\n- The paper highlights current limitations of MLLMs, such as struggling with fine-grained perception, complex chart understanding, and long-context reasoning.\n- Finally, it proposes future directions for benchmark development, including creating well-defined capability taxonomies, focusing on real-world applications, and evaluating more diverse modalities beyond vision and language.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Document Question Answering",
            "Video-Text-to-Text",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
        "authors": "Ying-Tian Liu, Yuan-Chen Guo, Xin Yu, Lp256, yuanze1024",
        "link": "https://arxiv.org/abs/2411.14740",
        "github_repo": null,
        "summary": "- TEXGen is a novel generative diffusion model designed for mesh texturing, using a UV texture map representation for scalability and high-resolution detail preservation.\n- It employs a hybrid 2D-3D network architecture that combines 2D convolutions in UV space with sparse convolutions and attention layers in 3D space to learn both local details and global 3D dependencies.\n- Trained on a large dataset derived from Objaverse, TEXGen generates high-resolution textures guided by text prompts and single-view images, operating in a feed-forward manner without test-time optimization.\n- It outperforms existing generalizable texture generation methods in terms of quality and speed, achieving significantly lower FID and KID scores while being considerably faster.\n- The model supports various applications like text-guided texture synthesis, inpainting, and texture completion from sparse views.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/CVMI-Lab/TEXGen"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "Pathways on the Image Manifold: Image Editing via Video Generation",
        "authors": "David Bensa\u00efd, Roy Velich, Daniel Silver, Gal Yona, Noam Rotstein",
        "link": "https://arxiv.org/abs/2411.16819",
        "github_repo": null,
        "summary": "- This research introduces Frame2Frame (F2F), a novel image editing framework that leverages video generation for enhanced image manipulation.\n- F2F reformulates image editing as a temporal process, utilizing image-to-video models to generate smooth transitions from the original image to the desired edit, guided by temporally descriptive captions.\n- This approach traverses the image manifold continuously, ensuring consistent edits while preserving key elements of the original image and achieving improved edit accuracy.\n-  Evaluations on TEdBench and a new dataset, PosEdit (focused on human pose transformations), demonstrate state-of-the-art results, outperforming existing image-to-image methods in both edit accuracy and image preservation.\n- Further, a human evaluation confirms F2F's superior performance, showcasing its effectiveness in producing edits that align better with user intent while maintaining high visual quality.",
        "classification": [
            "Image-to-Image",
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
        "authors": "Judith E Fan, Alex Zhao, Kristine Zheng, Tamar Rott Shaham, Yael Vinker",
        "link": "https://arxiv.org/abs/2411.17673",
        "github_repo": null,
        "summary": "- SketchAgent is a novel language-driven sequential sketch generation method that leverages pre-trained multimodal Large Language Models (LLMs) and requires no training or fine-tuning.\n- The core innovation lies in its intuitive sketching language, introduced to the model via in-context examples, enabling the LLM to \"draw\" using string-based actions processed into vector graphics and then rendered onto a pixel canvas.\n- By drawing stroke-by-stroke, coupled with an LLM's sequential nature and rich prior knowledge, SketchAgent captures the dynamic and evolving process of human sketching.\n- Evaluations demonstrate SketchAgent's ability to generate diverse sketches, engage in dialogue-driven drawing, collaborate with humans in real-time, and edit sketches via chat, effectively capturing the sequential and dynamic aspects of human sketching.\n- This approach marks a departure from optimization-based methods, which lack temporal structure, and paves the way for intuitive, interactive artificial sketching systems.",
        "classification": [
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://sketch-agent.csail.mit.edu/"
        ],
        "date": "2024-11-27"
    },
    {
        "title": "Learning 3D Representations from Procedural 3D Programs",
        "authors": "Zezhou Cheng, Xuweiyi Chen",
        "link": "https://arxiv.org/abs/2411.17467",
        "github_repo": null,
        "summary": "- This paper proposes Point-MAE-Zero, a self-supervised learning framework for 3D point cloud representations using procedurally generated 3D shapes.\n- Point-MAE-Zero leverages Point-MAE, a masked autoencoding framework and trains it on synthetic data generated using procedural programs. \n- It achieves comparable performance to Point-MAE-SN (trained on ShapeNet) on ModelNet40 and outperforms Point-MAE-SN on ScanObjectNN and part segmentation, demonstrating the efficacy of learning from procedurally generated data.\n- The study reveals that current self-supervised learning methods prioritize geometric structures over semantic content, evident from the comparable performance despite the synthetic dataset lacking explicit semantic annotations.\n- An analysis shows improved performance in Point-MAE-Zero with increased geometric diversity and dataset size of the procedurally generated shapes, indicating the importance of diverse and larger scale datasets in self-supervised 3D representation learning.",
        "classification": [
            "Computer Vision",
            "Image Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://point-mae-zero.cs.virginia.edu/"
        ],
        "date": "2024-11-27"
    },
    {
        "title": "SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE",
        "authors": "XIngang Pan, Tengfei Wang, Shangchen Zhou, Yushi Lan, Yongwei Chen",
        "link": "https://arxiv.org/abs/2411.16856",
        "github_repo": null,
        "summary": "- SAR3D is a novel framework for fast 3D object generation and comprehensive understanding leveraging a multi-scale 3D vector-quantized variational autoencoder (VQVAE) and autoregressive modeling.\n- The multi-scale 3D VQVAE tokenizes 3D objects into hierarchical levels of tokens, enabling next-scale prediction training, which significantly reduces generation time, achieving speeds as fast as 0.82 seconds on an A6000 GPU.\n- For 3D generation, SAR3D employs an autoregressive model that predicts the next scale of the latent triplane representation, conditioned on single image or text prompts.\n- SAR3D-LLM, an extension of SAR3D for understanding, aligns the latent space of the 3D VQVAE with a large language model, enabling detailed 3D captioning and simultaneous generation and captioning.\n- Experimental results demonstrate that SAR3D surpasses existing 3D generation methods in both speed and quality, and enables detailed captioning of generated and encoded 3D objects.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [
            "https://cyw-3d.github.io/projects/SAR3D/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting",
        "authors": "Ping Hu, Liqian Ma, Lu Zhang, Pengxiang Li, Yicheng Yang",
        "link": "https://arxiv.org/abs/2411.17223",
        "github_repo": "https://github.com/mycfhs/DreamMix",
        "summary": "- DreamMix is a novel diffusion-based generative model for subject-driven image inpainting, enabling customized insertion of objects into images with text-driven attribute editing.\n- It utilizes a disentangled inpainting framework with Local Content Generation (LCG) and Global Context Harmonization (GCH) stages to ensure precise object placement and harmonious blending with the background.\n- An Attribute Decoupling Mechanism (ADM) diversifies training data to mitigate overfitting on provided subject images, enhancing the model's ability to generalize to new attributes. \n- A Textual Attribute Substitution (TAS) module refines text embeddings during inference, further improving the accuracy of attribute editing.\n- Experimental results demonstrate DreamMix's superior performance in identity preservation and attribute editing compared to existing methods, as evidenced by quantitative metrics and user studies.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/mycfhs/DreamMix"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models",
        "authors": "Yifan Song, Xuqing Yang, Zhihui Xie, Yuancheng Wei, Lei Li",
        "link": "https://arxiv.org/abs/2411.17451",
        "github_repo": null,
        "summary": "- Introduces VL-RewardBench, a benchmark designed to evaluate Vision-Language Generative Reward Models (VL-GenRMs) and address limitations of existing evaluation methods that rely on biased AI-generated preferences or simplistic queries.\n- VL-RewardBench consists of 1,250 examples across three domains: general multimodal queries, visual hallucination detection, and complex reasoning tasks, curated through AI-assisted annotation with human verification.\n- Evaluation of 16 leading VL-GenRMs reveals that even top models like GPT-40 achieve only moderate accuracy on VL-RewardBench while open-source models struggle, highlighting the benchmark's challenging nature.\n- Analysis shows VL-GenRMs struggle more with basic visual perception than complex reasoning and that test-time scaling benefits vary by model capacity.\n- Demonstrates that critic training of VL-GenRMs for judgment substantially improves their evaluation abilities and that benchmark performance strongly correlates with downstream Best-of-N sampling effectiveness in MMMU-Pro.",
        "classification": [
            "Multimodal",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis",
        "authors": "Yong Man Ro, Hosu Lee, Hyunjun Kim, Junho Kim",
        "link": "https://arxiv.org/abs/2411.16173",
        "github_repo": null,
        "summary": "- SALOVA, a novel video-LLM framework, enhances long-form video comprehension through a targeted retrieval process using a new dataset and architectural design.\n- The SceneWalk dataset, with 87.8K densely captioned long videos, enables capturing scene continuity and rich descriptive context for improved long-form video understanding.\n- SALOVA integrates a dynamic routing mechanism and spatio-temporal projector to retrieve relevant video segments efficiently, addressing the limitations of current video-LMMs in context length and memory overhead.\n- A FocusFast approach analyzes selected segments in detail while maintaining overall context awareness for enhanced video interpretation.\n- Experiments demonstrate SALOVA's superior performance in processing complex long videos, reducing information loss, and maintaining contextual integrity, outperforming existing video-LMMs on benchmarks like Video-MME and LongVideoBench, especially for medium to long videos.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens",
        "authors": "Haitao Mi, Zhisong Zhang, Thomas Hartvigsen, Tao Ge, Xu Ouyang",
        "link": "https://arxiv.org/abs/2411.17691",
        "github_repo": null,
        "summary": "- This paper reveals that low-bit quantization tends to favor undertrained large language models (LLMs).\n- The authors study over 1500 quantized LLM checkpoints and derive scaling laws for quantization-induced degradation (QiD) concerning training tokens, model size, and bit width.\n- They propose using QiD to measure LLM training levels and predict quantization performance for models trained with 100 trillion tokens, finding that low-bit quantization might not be desirable for such extensively trained models.\n- Based on the scaling laws derived, the authors predict the number of training tokens needed to reach different training levels based on the magnitude of QiD for different LLM sizes.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/Xu-C"
        ],
        "date": "2024-11-27"
    },
    {
        "title": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts",
        "authors": "Jingdi Le, Wei Liu, Yunqing Liu, Jiatong Li, qq8933",
        "link": "https://arxiv.org/abs/2411.14721",
        "github_repo": null,
        "summary": "- MolReFlect, a teacher-student framework, is introduced to perform fine-grained molecule-caption alignments in the molecule-caption translation task.\n- A larger teacher LLM extracts important phrases from molecule SMILES or captions, aligning them with corresponding characteristics or sub-structures in a zero-shot manner. \n- In-Context Selective Reflection refines these alignments using similar examples and perplexity-based selection by a smaller student LLM.\n- Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT) enhances the student LLM's learning by reformatting context examples into a thought chain, incorporating fine-grained alignments and reasoning. \n- MolReFlect achieves state-of-the-art performance on the ChEBI-20 dataset, outperforming baselines like ICMA and BioT5 in both Mol2Cap and Cap2Mol tasks without extra modalities or complex structures.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)",
        "authors": "Abhilekh Borah, Sainath Reddy Sankepally, Subhankar Ghosh, Shashwat Bajpai, Nasrin Imanpour",
        "link": "https://arxiv.org/abs/2411.16754",
        "github_repo": null,
        "summary": "- This paper introduces the Visual Counter Turing Test (VCT^2), a benchmark dataset with ~130K images generated by state-of-the-art text-to-image models like Stable Diffusion, DALL-E 3, and Midjourney, along with corresponding real images and captions from MS COCO and Twitter.\n- It also proposes the Visual AI Index (VAI), a new metric for evaluating the visual quality of AI-generated images based on seven criteria, including texture complexity, color distribution, and object coherence.\n- The authors evaluate 15 existing AI-generated image detection methods on VCT^2 and find that they struggle to effectively detect images from newer models, especially proprietary ones like Midjourney.\n- They demonstrate a correlation between higher VAI scores and increased difficulty in detection, suggesting that improvements in image generation quality pose new challenges for detection techniques.\n- The paper makes the VCT^2 dataset and VAI code publicly available to foster research in AI-generated image detection and quality assessment.",
        "classification": [
            "Computer Vision",
            "Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation",
        "authors": "Xiaodong Cun, Yong Zhang, Juan Cao, Ziyao Huang, Ziyi Xu",
        "link": "https://arxiv.org/abs/2411.17383",
        "github_repo": null,
        "summary": "- AnchorCrafter is a novel diffusion-based video generation framework designed to create realistic anchor-style product promotion videos by animating reference human images with specific products and motion controls.\n- It incorporates human-object interactions (HOI) through two key components: HOI-appearance perception, which enhances object appearance recognition and disentangles object and human appearances, and HOI-motion injection, which enables precise control of object trajectories and handles inter-occlusions.\n- Additionally, an HOI-region reweighting loss enhances the learning of object details.\n- Experimental results demonstrate superior performance in video quality, object perception, and hand generation, surpassing existing methods in object appearance preservation and shape awareness.\n-  AnchorCrafter effectively preserves object appearance and shape and generates high-quality videos with consistent human appearance and motion.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://cangcz.github.io/Anchor-Crafter/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-27"
    },
    {
        "title": "ROICtrl: Boosting Instance Control for Visual Generation",
        "authors": "KevinQHLin, pcma, ynie, 365sleep, guyuchao",
        "link": "https://arxiv.org/abs/2411.17949",
        "github_repo": null,
        "summary": "- The paper introduces ROICtrl, a novel adapter for pretrained diffusion models that enhances instance control in visual generation by incorporating regional instance control, where each instance is governed by a bounding box and a free-form caption.\n- ROICtrl utilizes ROI-Align and a new operation called ROI-Unpool to enable accurate and efficient ROI manipulation on high-resolution feature maps.\n- Unlike previous methods that rely on implicit position encoding or explicit attention masks, ROICtrl achieves superior performance in regional instance control with significantly reduced computational costs.\n- The proposed ROICtrl-Bench benchmark provides a comprehensive evaluation of instance control capabilities, covering both template-based and free-form instance captions.\n- Experiments demonstrate that ROICtrl outperforms existing methods on various benchmarks, achieving state-of-the-art performance and improved efficiency.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://roictrl.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment",
        "authors": "ranjaykrishna, Tim666, lzy8465, Dipsy0830, shuaishuaicdp",
        "link": "https://arxiv.org/abs/2411.17188",
        "github_repo": null,
        "summary": "- This paper introduces ISG (Interleaved Scene Graph), a new evaluation framework for assessing the quality of interleaved text and image generation.\n- ISG uses a scene graph structure to capture relationships between text and image blocks, enabling multi-level evaluation (holistic, structural, block-level, and image-specific).\n- Alongside ISG, a new benchmark dataset ISG-BENCH containing 1,150 samples across 8 categories and 21 subcategories is introduced to assess model performance on complex language-vision dependencies.\n- Experimental results reveal that current unified vision-language models perform sub-optimally at generating interleaved content; compositional models that split language and image generation perform better but also have room to improve.\n- A new compositional baseline agent ISG-AGENT, based on a \u201cplan-execute-refine\u201d pipeline achieves performance improvement compared to other evaluated methods.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://interleave-eval.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
        "authors": "Ruiqi Gao, holynski, atrevithick, doinkda, rundi",
        "link": "https://arxiv.org/abs/2411.18613",
        "github_repo": null,
        "summary": "- CAT4D is a novel method for generating dynamic 3D scenes from monocular video using a multi-view video diffusion model.\n- The model is trained on a diverse combination of datasets and uses a novel sampling approach to transform a single video into a multi-view video, enabling robust 4D reconstruction.\n- CAT4D demonstrates competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, outperforming existing methods.\n- The model's creative capabilities are highlighted through its ability to generate 4D scenes from both real and generated videos.\n- The method is applicable to various tasks, including novel view synthesis, dynamic scene reconstruction, and sparse view reconstruction.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Video Classification",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/google-research/cat4d"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Large Language Model-Brained GUI Agents: A Survey",
        "authors": "Gezelligheid520, liqul, bowenli, shilhe, vyokky",
        "link": "https://arxiv.org/abs/2411.18279",
        "github_repo": null,
        "summary": "- This paper presents a comprehensive survey of Large Language Model (LLM)-brained Graphical User Interface (GUI) agents, exploring their evolution, components, techniques, and applications.\n- LLM-brained GUI agents represent a new frontier in human-computer interaction, allowing users to interact with and control software applications using natural language.\n- The survey covers key aspects such as agent frameworks, data collection strategies, model optimization methods, evaluation metrics, and real-world use cases.\n- The paper also identifies key research gaps and challenges in the field, including privacy concerns, latency limitations, and the need for improved human-agent interaction.\n- It proposes future directions for research and development, focusing on enhancing agent capabilities, ensuring safety and reliability, and addressing ethical considerations.",
        "classification": [
            "Natural Language Processing",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
        "authors": "Sankalp Sinha, mzafzal, saali14, alootikki, SadilKhan",
        "link": "https://arxiv.org/abs/2411.17945",
        "github_repo": null,
        "summary": "- This paper introduces MARVEL-40M+, a large-scale dataset with over 40 million text annotations for 8.9 million 3D assets, aimed at improving text-to-3D generation.\n- MARVEL, a multi-stage annotation pipeline, leverages open-source pretrained multi-view VLMs and LLMs to produce hierarchical descriptions ranging from detailed to concise tags.\n-  A two-stage text-to-3D framework, MARVEL-FX3D, is presented, which fine-tunes Stable Diffusion with MARVEL-40M+ annotations and utilizes a pretrained image-to-3D network.\n- Evaluations indicate that MARVEL-40M+ outperforms existing datasets in annotation quality and diversity, and MARVEL-FX3D achieves state-of-the-art results in text-to-3D generation.\n- Experimental results show that MARVEL-FX3D generates textured 3D meshes from text within 15 seconds, achieving superior prompt fidelity and overall preference compared to existing methods.",
        "classification": [
            "Text-to-3D",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient",
        "authors": "Xinchao Wang, Gongfan Fang, horseee, Zigeng",
        "link": "https://arxiv.org/abs/2411.17787",
        "github_repo": "https://github.com/czg1225/CoDe",
        "summary": "- This paper introduces Collaborative Decoding (CoDe), a novel decoding strategy for Visual Auto-Regressive (VAR) models that significantly improves efficiency without compromising image quality.\n- CoDe partitions the multi-scale inference process into a collaboration between a large and a small model, where the large model generates low-frequency content and the small model refines high-frequency details.\n- Experimental results show that CoDe achieves a 1.7x speedup and reduces memory usage by around 50% compared to the original VAR model, with only a negligible increase in FID score.\n- Further improvements can be achieved by reducing drafting steps, leading to an impressive 2.9x speedup and reaching 41 images/s at 256x256 resolution on a single NVIDIA 4090 GPU.\n- Specialized fine-tuning is proposed to further optimize each model's performance, leading to a notable performance boost with limited additional cost.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/czg1225/CoDe"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "authors": "Haoran Yin, xinggangw, bojiang-bentoml, csy71, LegendBC",
        "link": "https://arxiv.org/abs/2411.15139",
        "github_repo": "https://github.com/hustvl/DiffusionDrive",
        "summary": "- This paper introduces DiffusionDrive, a novel truncated diffusion model for end-to-end autonomous driving that incorporates prior multi-mode anchors and truncates the diffusion schedule, resulting in a 10x reduction in denoising steps compared to vanilla diffusion models.\n- The model architecture consists of an efficient cascade diffusion decoder that enhances interaction with conditional scene context, achieving superior diversity and quality in just 2 steps.\n- On the NAVSIM dataset, DiffusionDrive achieves 88.1 PDMS without bells and whistles, outperforming previous state-of-the-art methods and running at a real-time speed of 45 FPS on an NVIDIA 4090.\n- Qualitative results on challenging scenarios demonstrate that DiffusionDrive robustly generates diverse and plausible driving actions.\n- The truncated diffusion policy addresses the limitations of vanilla diffusion models in real-time autonomous driving by efficiently capturing the multi-mode nature of driving behaviors.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/hustvl/DiffusionDrive"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
        "authors": "Diego Valsesia, emagli, mosams, u-michieli, Ema97x",
        "link": "https://arxiv.org/abs/2411.17786",
        "github_repo": null,
        "summary": "- DreamCache is a novel finetuning-free approach to personalized image generation that utilizes feature caching to overcome limitations of existing methods.\n- It caches a small number of reference image features from a subset of layers and a single timestep of a pretrained diffusion model, enabling dynamic modulation of generated image features.\n- The model employs lightweight, trained conditioning adapters to achieve state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters.\n- DreamCache is both computationally more efficient and versatile than existing models and is shown to outperform current state-of-the-art baselines in quantitative results.\n- The approach is shown to be effective even with a reduced number of reference images and works with different backbones.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
        "authors": "Yunyuan Ge, LiuhanChen, hexianyi, Jinfa, BestWishYsh",
        "link": "https://arxiv.org/abs/2411.17440",
        "github_repo": null,
        "summary": "- This paper introduces ConsisID, a novel tuning-free identity-preserving text-to-video generation model based on the Diffusion Transformer (DiT) architecture.\n- ConsisID addresses the challenges of existing methods by employing a frequency-aware heuristic control scheme that injects identity control signals into the frequency domain.\n- The model decouples identity features into high- and low-frequency components, injecting them into specific locations within the DiT to enhance both global and fine-grained feature preservation.\n- Experimental results demonstrate that ConsisID outperforms state-of-the-art methods in identity preservation, visual quality, text relevance, and motion amplitude across multiple metrics.\n- The authors also conduct a user study, which further confirms the superiority of ConsisID compared to existing methods.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [
            "https://pku-yuangroup.github.io/ConsisID"
        ],
        "huggingface_urls": [
            "null"
        ],
        "date": "2024-11-28"
    },
    {
        "title": "Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis",
        "authors": "Xiaoming Li, cavanloy, OAOA, itsmag11",
        "link": "https://arxiv.org/abs/2411.17769",
        "github_repo": "https://github.com/itsmag11/Omegance",
        "summary": "- This paper introduces Omegance, a novel single-parameter technique for controlling the granularity of diffusion-based image and video synthesis.\n- Omegance dynamically scales the predicted noise during each denoising step without requiring model retraining, architectural modifications, or additional computational overhead.\n- It allows for flexible granularity control, including global, spatial (using omega masks), and temporal (using omega schedules) adjustments.\n- The method demonstrates high performance on various image and video synthesis tasks, including text-to-image, image-to-image, and text-to-video generation.\n- The proposed technique adapts to different diffusion models and schedulers, offering a versatile and user-friendly solution for nuanced image and video generation.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Image-to-Video"
        ],
        "github_urls": [
            "https://github.com/itsmag11/Omegance"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing",
        "authors": "Shiguang Shan, Hong Chang, Heylon, flow2023, LiyiGang",
        "link": "https://arxiv.org/abs/2411.16781",
        "github_repo": null,
        "summary": "- UniPose is a novel multimodal framework that unifies human pose comprehension, generation, and editing tasks.  It utilizes a pose tokenizer to convert 3D poses into discrete tokens for seamless integration with LLMs.\n- The model architecture incorporates a mixture of visual encoders (CLIP and a pose-specific visual encoder) to enhance fine-grained pose perception.\n- UniPose demonstrates superior performance across various pose-relevant tasks compared to existing methods, including pose comprehension, generation, and editing, as shown by experimental results in tables 2, 3, 4, and 5.\n- The model exhibits zero-shot generalization capabilities, enabling it to adapt to unseen tasks and enhance pose estimation, as shown in Figure 5.\n- UniPose addresses the limitations of existing methods by providing a unified multimodal framework that seamlessly handles multiple modalities, enabling finer-grained pose perception and more complex pose editing.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding",
        "authors": "Xingyu Chen, Tian Liang, Jiahao Xu, Ziyin Zhang, zptu",
        "link": "https://arxiv.org/abs/2411.18462",
        "github_repo": "https://github.com/Geralt-Targaryen/SVIP",
        "summary": "- This paper introduces SVIP, a self-verification length policy for speculative decoding systems that dynamically determines the length of draft sequences based on the entropy of each draft token distribution.\n- SVIP achieves up to 20% walltime speedup on SpecBench and 60% speedup on MT-Bench for long-form generation.\n- The proposed method is training-free and compatible with any existing speculative decoding methods that generate draft tokens autoregressively.\n- Experimental results demonstrate consistent wall-time improvements on GliDe & CaPE and EAGLE-2.\n- The core idea is to adaptively adjust the draft length based on token difficulty, unlike traditional methods using a fixed draft length.",
        "classification": [
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Geralt-Targaryen/SVIP"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format",
        "authors": "Jiansheng Wei, Jianxin Liang, Xiaojun Meng, Yueqian Wang, ColorfulAI",
        "link": "https://arxiv.org/abs/2411.17991",
        "github_repo": "https://github.com/yellow-binary-tree/MMDuet",
        "summary": "- This paper introduces a novel video-text duet interaction format for VideoLLMs, addressing limitations of existing whole-video interaction methods.\n- The proposed format allows continuous video playback with interspersed text messages from both user and model, enabling real-time responses and improved time-sensitive task performance.\n- A new dataset, MMDuetIT, is created to train VideoLLMs on this new format, including dense video captioning, multi-answer grounded video QA, and temporal video grounding tasks.\n- Experiments demonstrate that using the video-text duet interaction format significantly improves VideoLLM performance on time-sensitive tasks compared to baselines.\n- The paper introduces a new model, MMDuet, which achieves state-of-the-art performance on various time-sensitive video tasks, with minimal training effort.",
        "classification": [
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/yellow-binary-tree/MMDuet"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Adaptive Blind All-in-One Image Restoration",
        "authors": "Javier Vazquez-Corral, Shaolin Su, Luis Herranz, davidserra9",
        "link": "https://arxiv.org/abs/2411.18412",
        "github_repo": null,
        "summary": "- This paper introduces an Adaptive Blind All-in-One Image Restoration (ABAIR) model that addresses multiple and composite degradations in images with a flexible structure that easily incorporates new degradations.\n- The ABAIR model comprises three phases: pre-training with synthetic degradations, single-task adaptation using low-rank adapters, and multi-task integration via a lightweight degradation estimator.\n- The model demonstrates significant improvements over existing state-of-the-art all-in-one image restoration methods on five- and three-task setups, generalizing well to unseen degradations and composite distortions.\n- The adaptive combination of adapters enhances the flexibility of the model, making it easily adaptable to new degradations without needing to retrain the whole model.\n- This method effectively handles various distortions such as rain, haze, noise, blur, and low-light conditions, and also generalizes well to composite distortions and unseen degradation types.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://aba-ir.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters",
        "authors": "Houqiang Li, Wengang Zhou, Kai Ma, Jinxu Xiang, jasongzy",
        "link": "https://arxiv.org/abs/2411.18197",
        "github_repo": null,
        "summary": "- This paper introduces Make-It-Animatable, a novel data-driven framework for efficiently generating animation-ready 3D character models from various input representations (meshes and 3D Gaussian splats) in under one second.\n- The framework employs a particle-based shape autoencoder for a coarse-to-fine representation of the 3D character, followed by a structure-aware transformer to model the bone structures and generate high-quality blend weights, bone positions, and pose transformations.\n- Experiments show significant improvements in both quality and speed compared to existing automatic rigging methods, such as Meshy and Tripo, surpassing them in terms of animation quality and the ability to handle diverse shapes and poses.\n- The method supports various 3D representations, arbitrary poses, and generates accurate and robust results even for characters with non-standard skeleton structures or additional accessories.\n- The authors demonstrate the effectiveness and efficiency of their approach through extensive experiments, showing its applicability to a wide range of 3D characters and animation styles.",
        "classification": [
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics",
        "authors": "Mohammed Elseiagy, Dinesh Saggurthi, Juan Lugo, pythn, Sarim-Hash",
        "link": "https://arxiv.org/abs/2411.15872",
        "github_repo": "https://github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics",
        "summary": "- This paper introduces a novel approach to brain tumor segmentation using MedNeXt, an architecture combining the strengths of transformers and convolutional networks.\n- The proposed method employs a comprehensive model ensembling technique and thorough post-processing steps for improved accuracy.\n- The model achieved state-of-the-art results on the BraTS 2024 SSA dataset, with an average Dice Similarity Coefficient (DSC) of 0.896 and an average Hausdorff Distance (HD95) of 14.682.\n-  On the BraTS 2024 Pediatric Tumor dataset, the model achieved an average DSC of 0.830 and an average HD95 of 37.508, demonstrating robustness across different populations.\n- The study highlights the importance of addressing data distribution shifts in training data through comprehensive model ensembling and post-processing for reliable tumor segmentation across diverse populations.",
        "classification": [
            "Image Segmentation"
        ],
        "github_urls": [
            "https://github.com/MIC-DKFZ/MedNeXt"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
        "authors": "Yihao Chen, Yuda Xiong, Yuqin Yang, Gen luo, Qing Jiang",
        "link": "https://arxiv.org/abs/2411.18363",
        "github_repo": "https://github.com/IDEA-Research/ChatRex",
        "summary": "- ChatRex, a novel multimodal large language model (MLLM), is introduced, featuring a decoupled architecture to address the conflict between perception and understanding tasks, commonly observed in existing MLLMs.\n- The model utilizes a Universal Proposal Network (UPN), a DETR-based model trained with granularity-based prompt learning, to provide robust object proposals for the LLM's retrieval-based detection process, eliminating coordinate prediction issues.\n- A new dataset, Rexverse-2M, containing two million image-region-text annotation triplets with varying granularities, was created with a fully automated data engine to train the model on joint perception and understanding tasks.\n- ChatRex demonstrates strong performance on object detection benchmarks like COCO (48.5 mAP) and LVIS (43.1 mAP), comparable to dedicated object detectors, and outperforms other MLLMs, particularly in multi-object scenes.\n- The model maintains competitive results on general multimodal benchmarks, showcasing robust understanding and dialogue capabilities, enhanced by the integration of perception abilities.",
        "classification": [
            "Multimodal",
            "Object Detection",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/IDEA-Research/ChatRex"
        ],
        "huggingface_urls": [],
        "date": "2024-11-28"
    },
    {
        "title": "Training and Evaluating Language Models with Template-based Data Generation",
        "authors": "yifAI",
        "link": "https://arxiv.org/abs/2411.18104",
        "github_repo": "https://github.com/iiis-ai/TemplateMath",
        "summary": "- This paper introduces Template-based Data Generation (TDG), a novel method for generating large-scale, high-quality mathematical datasets using GPT-4 to automatically generate meta-templates.\n- The TDG method leverages parameterized templates and a reject-sampling-based verification process to ensure data quality and scalability.\n- A dataset called TemplateGSM, consisting of over 7 million synthetically generated grade school math problems with verified solutions, is created using TDG.\n- Experiments show that TemplateGSM significantly improves the performance of LLMs in mathematical reasoning tasks.\n- The authors release both the TemplateGSM dataset and the TDG code to facilitate further research and development.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/iiis-ai/TemplateMath"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/math-ai/TemplateGSM"
        ],
        "date": "2024-11-28"
    },
    {
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "authors": "Jingdi Lei, jwu323, ZonglinY, Duke-de-Artois, qq8933",
        "link": "https://arxiv.org/abs/2411.18203",
        "github_repo": null,
        "summary": "- This paper introduces Critic-V, a novel framework designed to enhance the reasoning capabilities of Vision-Language Models (VLMs) by incorporating a critic model that provides feedback during the reasoning process.\n- Critic-V features a Reasoner-Critic architecture where the Reasoner generates reasoning paths, and the Critic offers natural language critiques for refinement, inspired by the Actor-Critic paradigm and leveraging in-context reinforcement learning.\n- The Critic model is trained using Direct Preference Optimization (DPO) on a new dataset with critiques ranked by a Rule-based Reward (RBR) function to enhance its critic capabilities. \n- Evaluation results show that Critic-V significantly improves the performance of existing VLMs, like Qwen2-VL-7B and DeepSeek-VL-7B, on 5 out of 8 benchmarks, surpassing even GPT-4V on several datasets and particularly improving mathematical reasoning tasks.\n- Critic-V integrates a dynamic text-based policy for the Reasoner and uses constructive feedback from the preference-optimized Critic to create a reliable and context-sensitive multimodal reasoning process.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-29"
    },
    {
        "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
        "authors": "Hangwei Qian, Weijia Wu, Zhuohang Dang, Changliang Xia, ChengyouJia",
        "link": "https://arxiv.org/abs/2411.17176",
        "github_repo": null,
        "summary": "- This paper introduces ChatGen, a novel framework for automating the text-to-image generation process from freestyle chat inputs.\n- It proposes a multi-stage evolution strategy (ChatGen-Evo) that equips language models with essential skills for prompt crafting, model selection, and argument configuration.\n- This approach outperforms baseline methods on a new benchmark dataset, ChatGenBench, which features diverse freestyle chat inputs paired with desired image outputs and generation parameters.\n- ChatGen-Evo with 2B parameters achieves comparable performance to a larger 8B parameter supervised baseline model.\n- The multi-stage evolution strategy results in high-quality images aligning with the diverse requirements from freestyle chat instructions.",
        "classification": [
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "https://chengyou-jia.github.io/ChatGen-Home"
        ],
        "huggingface_urls": [],
        "date": "2024-11-29"
    },
    {
        "title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models",
        "authors": "Barbara Hammer, Robin Chan, Petra Bevandic, rizavelioglu",
        "link": "https://arxiv.org/abs/2411.18350",
        "github_repo": null,
        "summary": "- Introduces Virtual Try-Off (VTOFF), a new task for generating standardized garment images from real-world photos of clothed individuals.\n- Presents TryOffDiff, a model based on Stable Diffusion with SigLIP visual conditioning.\n- Demonstrates superior performance over baseline methods on a modified VITON-HD dataset, achieving higher fidelity in garment reconstruction.\n- Shows that traditional image generation metrics are insufficient for evaluating VTOFF and proposes DISTS as a more suitable metric.\n- Highlights the potential of VTOFF for enhancing product imagery, improving generative model evaluation, and future research in high-fidelity image reconstruction.",
        "classification": [
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://rizavelioglu.github.io/tryoffdiff/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-29"
    },
    {
        "title": "Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models",
        "authors": "Jong Chul Ye, Bryan S Kim, kjm981995",
        "link": "https://arxiv.org/abs/2411.17041",
        "github_repo": null,
        "summary": "- Free$^2$Guide is a novel gradient-free framework designed to enhance text-video alignment in diffusion-based generative models by leveraging Large Vision-Language Models (LVLMs).\n- The framework uses path integral control to approximate guidance during video generation, eliminating the need for gradients from reward functions and accommodating non-differentiable reward models like LVLMs.\n- Free$^2$Guide improves text alignment by processing multiple video frames and incorporates temporal understanding into the reward mechanism, allowing the use of powerful black-box vision-language model APIs.\n- It allows the ensembling of multiple reward models like LVLMs and image-based models to synergistically guide video generation, enhancing both text alignment and overall video quality.\n- Experiments demonstrate that using LVLMs in Free$^2$Guide significantly improved text alignment in video generation when compared to baseline models in various metrics, and exhibits improved general video quality across a range of attributes.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [
            "https://kjm981995.github.io/free2guide/"
        ],
        "huggingface_urls": [],
        "date": "2024-11-29"
    },
    {
        "title": "Morph: A Motion-free Physics Optimization Framework for Human Motion Generation",
        "authors": "Hao Liu, Xin Zhao, Ruibing Hou, Mingshuang Luo, Zhuo Li",
        "link": "https://arxiv.org/abs/2411.14951",
        "github_repo": null,
        "summary": "- Morph is a novel motion-free physics optimization framework for generating realistic human motion from text or music, comprising a Motion Generator (MG) and a Motion Physics Refinement (MPR) module.\n- The MG synthesizes motion data, while the MPR, trained on this synthetic data, uses a motion imitator within a physics simulator to refine the generated motion, enforcing physical constraints and aligning its distribution with a discriminator using reinforcement learning.\n- This physics-refined data then fine-tunes the MG to enhance its realism.\n- Experiments on HumanML3D and AIST++ datasets show Morph improves physical plausibility metrics (e.g. Penetration, floating) drastically, while achieving competitive generation quality (FID, R-Precision) compared to state-of-the-art methods, across different generator architectures (diffusion, autoregressive, masked modeling).\n- This framework addresses the challenge of physically implausible artifacts in generated motion by leveraging synthetic data, making it a cost-effective and versatile solution.",
        "classification": [
            "Text-to-Video",
            "Text-to-3D",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-29"
    },
    {
        "title": "LongKey: Keyphrase Extraction for Long Documents",
        "authors": "Jean Paul Barddal, Cinthia Obladen de Almendra Freitas, Jeovane Honorio Alves, RaduState",
        "link": "https://arxiv.org/abs/2411.17863",
        "github_repo": "https://github.com/jeohalves/longkey",
        "summary": "- LongKey, a novel framework for keyphrase extraction from lengthy documents, leverages an encoder-based language model, specifically Longformer, to capture extended text intricacies, supporting up to 96K tokens.\n- It employs a max-pooling embedder to consolidate context across the document, refining keyphrase candidate representation.\n- Validated on LDKP datasets and six diverse unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based methods, achieving an F1@5 of 39.55% on LDKP3K and 41.81% on LDKP10K.\n- A component analysis confirmed the significant contribution of the keyphrase embedding pooler in enhancing performance.\n- While robust on long documents, LongKey's performance on short-context datasets suggests further development for broader applicability.",
        "classification": [
            "Natural Language Processing",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/jeohalves/longkey"
        ],
        "huggingface_urls": [],
        "date": "2024-11-29"
    },
    {
        "title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
        "authors": "Xintong Zhang, doubling, edward2021, buaahsh, daixuancheng",
        "link": "https://arxiv.org/abs/2411.19930",
        "github_repo": null,
        "summary": "- This paper introduces a novel approach for domain-specific post-training of Multimodal Large Language Models (MLLMs), focusing on data synthesis, training pipelines, and task evaluation.\n- A visual instruction synthesizer is developed using open-source models to extract diverse visual instruction tasks from domain-specific image-caption pairs, outperforming manual rules, GPT-4, and GPT-4V in enhancing MLLM performance on specialized domains such as biomedicine and food.\n- A single-stage training pipeline, combining synthetic tasks and image-caption pairs, is proposed to improve task diversity and mitigate catastrophic forgetting compared to traditional two-stage training.\n- Experiments on various MLLMs (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B) across biomedicine and food domains show consistent improvement on diverse domain-specific tasks using the resulting AdaMLLM (Adapted Multimodal Large Language Model).\n- All implementations will be open-sourced to support further research.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/AdaptLLM"
        ],
        "date": "2024-12-02"
    },
    {
        "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS",
        "authors": "Zengqi Wen, Feihu Che, Shuai Zhang, fmk345, Jinyang23",
        "link": "https://arxiv.org/abs/2411.18478",
        "github_repo": null,
        "summary": "- This paper introduces HiAR-ICL, a novel automated reasoning paradigm that enhances in-context learning (ICL) by shifting the focus from specific examples to abstract thinking patterns, termed \"thought cards.\"\n- HiAR-ICL uses Monte Carlo Tree Search (MCTS) to construct these thought cards from a small seed dataset and employs a cognitive complexity framework to dynamically match problems with appropriate thought cards during inference.\n- Five atomic reasoning actions, including System Analysis, One-Step Thought, Chain-of-Thought, Divide and Conquer, and Self-Reflection and Refinement, are defined as fundamental building blocks for these patterns.\n- The approach outperforms state-of-the-art methods on the MATH benchmark with Qwen2.5-7B-Instruct (79.6% accuracy), surpassing GPT-40 (76.6%) and Claude 3.5 (71.1%).\n- HiAR-ICL reduces time complexity compared to existing tree search methods by leveraging pre-computed reasoning patterns.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm",
            "huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data"
        ],
        "date": "2024-12-02"
    },
    {
        "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
        "authors": "MoonQiu, weilllllls, Jeff-Wang, StevenZhang, LiewFeng",
        "link": "https://arxiv.org/abs/2411.19108",
        "github_repo": null,
        "summary": "- TeaCache, a training-free caching mechanism for Diffusion Transformer (DiT) models in video generation, leverages timestep embedding modulated noisy input to estimate output differences and selectively cache intermediate outputs, improving inference speed.\n- Unlike uniform caching strategies, TeaCache dynamically adapts to the varying differences between model outputs across timesteps by using a rescaling strategy based on polynomial fitting to refine the estimation of model output similarity for optimal timestep selection.\n- Experiments show TeaCache achieves substantial acceleration over existing training-free methods (up to 4.41x over Open-Sora-Plan) while maintaining or even improving visual quality (negligible -0.07% Vbench score degradation) across different models, resolutions, and video lengths.\n- It effectively balances efficiency and visual quality compared to methods like PAB, achieving higher speedups with competitive or superior image quality metrics (VBench, LPIPS, SSIM, PSNR).\n- The approach is compatible with various SOTA DiT-based generation models (Open-Sora, Open-Sora-Plan, Latte) and scales well with Dynamic Sequence Parallelism (DSP) to multiple GPUs, further boosting performance for high-resolution, long video generation.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
        "authors": "Mingu Kang, Minseo Kim, Jisoo Kim, junwann, whwjdqls99",
        "link": "https://arxiv.org/abs/2411.19527",
        "github_repo": null,
        "summary": "- DisCoRD, a novel method for human motion generation, decodes discrete motion tokens into continuous motion using rectified flow, combining the naturalness of continuous representations with the faithfulness of discrete methods.\n- It employs an iterative refinement process in continuous space, capturing fine-grained dynamics and ensuring smoother motion, and uses discrete tokens as conditions in raw motion space to reduce noise.\n- A new metric, symmetric Jerk Percentage Error (sJPE), is introduced to evaluate both under-reconstruction and frame-wise noise in motion.\n- Extensive evaluations across text-to-motion, co-speech gesture, and music-to-dance generation demonstrate state-of-the-art performance, achieving an FID of 0.032 on HumanML3D and 0.169 on KIT-ML.\n- DisCoRD is adaptable to any discrete-based motion generation framework and improves naturalness without sacrificing faithfulness.",
        "classification": [
            "Text-to-Video",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
        "authors": "nav4, nailon-nvidia, talor-abr, tomer-nv, abercovich",
        "link": "https://arxiv.org/abs/2411.19146",
        "github_repo": null,
        "summary": "- Puzzle, a novel framework, leverages decomposed Neural Architecture Search (NAS) and Blockwise Local Distillation (BLD) with Mixed-Integer Programming to create inference-optimized Large Language Models (LLMs) tailored to specific hardware.\n- This framework optimizes models by creating heterogeneous architectures with varying block configurations, reducing redundant computations while preserving performance.\n- The resulting model, Nemotron-51B derived from Llama-3.1-70B-Instruct, achieves up to 2.17x inference speedup on a single NVIDIA H100 GPU while retaining 98.4% of the parent model's accuracy.\n- Demonstrating unprecedented efficiency, Nemotron-51B's training required only 45B tokens compared to over 15T for its parent, setting a new benchmark for throughput and memory efficiency.\n- This work also introduces a derivative of Llama-3.1-8B-Instruct further demonstrating Puzzle's capacity to create highly efficient models across various hardware and parameter scales.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "Video Depth without Video Models",
        "authors": "toshas, PeterTor, peterjohnson, dnarnhofer, Bingxin",
        "link": "https://arxiv.org/abs/2411.19189",
        "github_repo": null,
        "summary": "- RollingDepth, a novel image-based video depth estimation model, leverages a multi-frame latent diffusion model (LDM) adapted from a single-image LDM.\n- The model processes short video snippets with varying dilation rates, capturing temporal context through a modified cross-frame self-attention mechanism.\n- A robust, optimization-based global co-alignment algorithm assembles the depth snippets into a consistent video.\n- An optional refinement step using the LDM with decreasing snippet dilation rate further enhances details in the depth video.\n- RollingDepth achieves state-of-the-art performance on zero-shot benchmarks, outperforming both dedicated video depth estimators and high-performing single-frame models, particularly in challenging scenes with varying depth ranges.",
        "classification": [
            "Depth Estimation",
            "Computer Vision",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos",
        "authors": "bys0318, AlbertHuyb, lshmouse, thuzhaowang, hyz317",
        "link": "https://arxiv.org/abs/2411.19950",
        "github_repo": null,
        "summary": "- AlphaTablets, a novel 3D plane representation using rectangles with alpha channels, is introduced for accurate and flexible 3D planar surface modeling.\n- Differentiable rasterization is formulated on top of AlphaTablets, allowing efficient rendering of 3D planes into images and enabling gradient-based optimization.\n- A novel bottom-up 3D planar reconstruction pipeline from monocular videos is proposed; it initializes AlphaTablets using superpixels and geometric cues and then refines them via differentiable rendering.\n- A merging scheme facilitates the growth and refinement of AlphaTablets into complete planar structures.\n- Experiments on ScanNet demonstrate state-of-the-art performance in 3D planar reconstruction.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://hyzcluster.github.io/alphatablets"
        ],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing",
        "authors": "Hyunjun Kim, dwightro, arkimjh, lakelee",
        "link": "https://arxiv.org/abs/2411.19460",
        "github_repo": null,
        "summary": "- Introduces Video-Ma$^2$mba, a novel model for long-form video understanding that replaces the attention mechanism in Large Multimodal Models (LMMs) with State Space Models (SSMs), achieving linear scaling in time and memory.\n- Employs Multi-Axis Gradient Checkpointing (MA-GC) to enhance memory efficiency by retaining only essential activations across multiple computational axes.\n- Processes long video sequences, equivalent to over two hours of continuous video at 1 FPS, on a single GPU by handling the full sequence without frame sampling.\n- Improves accuracy and relevance of responses in long video understanding tasks by capturing detailed temporal dynamics.\n- Demonstrates substantial advantages over existing frameworks on benchmarks like Video-MME and LongVideoBench, showcasing its efficiency in handling lengthy video content and responding effectively to complex queries.",
        "classification": [
            "Video-Text-to-Text",
            "Visual Question Answering",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "Trajectory Attention for Fine-grained Video Motion Control",
        "authors": "Xingang-Pan, Jianlou, PKUWilliamYang, Vicky0522, zeqixiao",
        "link": "https://arxiv.org/abs/2411.19324",
        "github_repo": null,
        "summary": "- This paper introduces trajectory attention, a novel approach for fine-grained camera motion control in video generation.\n- It works by performing attention along available pixel trajectories, providing a stronger inductive bias for precise motion control and content consistency.\n- The approach models trajectory attention as an auxiliary branch alongside traditional temporal attention, enabling synergy between motion control and new content generation.\n- Experiments show significant improvements in precision and long-range consistency for camera motion control on both images and videos while maintaining high generation quality.\n- The method's efficiency and extensibility to other video motion control tasks highlight its broad applicability.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers",
        "authors": "willi-menapace, aliaksandr-siarohin, guochengqian, universome, sherwinbahmani",
        "link": "https://arxiv.org/abs/2411.18673",
        "github_repo": null,
        "summary": "- This paper introduces AC3D, a new state-of-the-art model for generative video modeling with precise 3D camera control built upon a pre-trained 11.5B parameter Video Latent Diffusion Transformer (VDiT).\n- The authors analyze camera motion and find it is low-frequency, leading to adjusted training schedules and improved quality.\n- They also discover VDiTs implicitly perform camera pose estimation, enabling targeted conditioning and reduced training parameters.\n- A curated dataset of dynamic videos with static cameras helps the model differentiate between camera and scene motion, improving generated video dynamics.\n- AC3D surpasses previous models by 18% in video fidelity and 25% in camera steering accuracy, demonstrated on RE10K and MSR-VTT.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://snap-research.github.io/ac3d"
        ],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion",
        "authors": "Xiatian Zhu, Hai X. Pham, Isma Hadji, Adrian Bulat, Haosen Yang",
        "link": "https://arxiv.org/abs/2411.18552",
        "github_repo": null,
        "summary": "- FAM diffusion, a novel training-free method, enhances high-resolution image generation using pre-trained latent diffusion models by introducing Frequency Modulation (FM) and Attention Modulation (AM) modules.\n- The FM module leverages the Fourier domain, conditioning low-frequency components during high-resolution denoising for global structure consistency, while the AM module refines local texture patterns by leveraging attention maps from native resolution denoising.\n- This one-pass approach seamlessly integrates with any latent diffusion model without architectural changes or finetuning, offering efficient high-resolution image generation.\n- Extensive qualitative and quantitative results on the Laion-5B dataset demonstrate FAM diffusion\u2019s superior performance in terms of FIDC, KIDC, and CLIP Score across various scaling factors, surpassing existing methods like DemoFusion, AccDiffusion, FouriScale, and HiDiffusion.\n- Notably, FAM diffusion achieves this quality improvement with negligible latency overheads compared to direct inference at target resolutions.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-02"
    },
    {
        "title": "LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification",
        "authors": "nljubesi, TajaKuzman",
        "link": "https://arxiv.org/abs/2411.19638",
        "github_repo": null,
        "summary": "- This paper introduces a novel teacher-student framework utilizing Large Language Models (LLMs) for multilingual news topic classification without manual annotation.\n- A GPT model serves as the teacher, automatically annotating news articles in Slovenian, Croatian, Greek, and Catalan with IPTC Media Topic labels to create a training dataset.\n- Smaller BERT-like student models, specifically XLM-ROBERTa, are then fine-tuned on this dataset, achieving comparable performance to the teacher model while being more computationally efficient. \n- The study demonstrates that student models achieve high performance with limited training data and exhibit strong zero-shot cross-lingual capabilities. \n-  The best performing model, a multilingual IPTC news topic classifier, is publicly released.",
        "classification": [
            "Natural Language Processing",
            "Text Classification",
            "Zero-Shot Classification"
        ],
        "github_urls": [
            "https://github.com/TajaKuzman/IPTC-Media-Topic-Classification"
        ],
        "huggingface_urls": [
            "https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier"
        ],
        "date": "2024-12-02"
    },
    {
        "title": "X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models",
        "authors": "lindahua, TheYJ, yuhangzang, tongwu2020, Zery",
        "link": "https://arxiv.org/abs/2412.01824",
        "github_repo": "https://github.com/SunzeY/X-Prompt",
        "summary": "- X-Prompt, an auto-regressive large vision-language model, is introduced for in-context image generation across various tasks.\n- It uses a novel design to compress features from in-context examples, enabling longer context sequences and better generalization to unseen tasks.\n- A unified training objective for text and image prediction allows the model to leverage task awareness from context.\n- Experimental results show competitive performance on several image generation tasks, including text-to-image generation, and improved in-context learning capabilities on novel tasks compared to baselines like OmniGen.\n- The method also exhibits strong capabilities in the image editing task by using Retrieval-Augmented Image Editing (RAIE), where a relevant image example is retrieved as in-context information to enhance editing performance.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Image Segmentation",
            "Depth Estimation",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/SunzeY/X-Prompt"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
        "authors": "LiruiZhao, yefly, xuzhaopan, xiaopengpeng, lyuukuu",
        "link": "https://arxiv.org/abs/2411.18499",
        "github_repo": null,
        "summary": "- Introduces GATE OpenING (OpenING), a benchmark for evaluating open-ended interleaved image-text generation, comprised of 5,400 human-annotated instances across 56 real-world tasks and 23 meta-topics.\n- Presents IntJudge, a novel judging model trained with a Reference-Augmented Generation (RAG) approach and an Interleaved Arena for data annotation, achieving 82.42% agreement with human judgments, outperforming GPT-4 by 11.34%.\n- Demonstrates through experiments on OpenING that integrated pipelines for interleaved generation outperform end-to-end models, highlighting the potential of two-stage generators with unified architectures.\n- Reveals that generating high-quality, coherent interleaved content remains challenging for existing models, while GPT-generated text often surpasses human quality, and human-annotated images are preferred over generated ones.\n- Provides a comprehensive leaderboard and analysis of various interleaved generation methods, offering insights for future model development and benchmark design.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Text-to-Image"
        ],
        "github_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis",
        "authors": "Dmitry Baranchuk, Valentin Khrulkov, Mikhail Khoroshikh, Anton Voronov, SpiridonSunRotator",
        "link": "https://arxiv.org/abs/2412.01819",
        "github_repo": null,
        "summary": "- This paper introduces SWITTI, a scale-wise transformer model for text-to-image synthesis.\n- SWITTI uses a non-autoregressive approach, predicting higher resolution versions of the image progressively while attending to the current scale, unlike traditional autoregressive models which attend to all previous scales. \n- The model incorporates architectural changes to enhance training stability, including using FP32 for the model head, \"sandwich\" normalizations, and SwiGLU activation. \n- It also disables classifier-free guidance at the final scales, leading to a 20% speed increase and improved fine-grained detail generation. \n- Human preference studies and automated evaluations show that SWITTI outperforms existing T2I autoregressive models and is competitive with state-of-the-art diffusion models while being up to 7x faster.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/black-forest-labs/FLUX.1-dev"
        ],
        "date": "2024-12-03"
    },
    {
        "title": "Open-Sora Plan: Open-Source Large Video Generation Model",
        "authors": "Xinhua Cheng, Yunyang Ge, Lin-Chen, BestWishYsh, LanguageBind",
        "link": "https://arxiv.org/abs/2412.00131",
        "github_repo": "https://github.com/PKU-YuanGroup/Open-Sora-Plan",
        "summary": "- Open-Sora Plan is an open-source project focused on generating high-resolution, long-duration videos from various user inputs, including text prompts, images, and structure control signals. \n- The model architecture comprises three key components: a Wavelet-Flow Variational Autoencoder (WF-VAE) for efficient feature extraction in the frequency domain, a Joint Image-Video Skiparse Denoiser (a modified 3D full attention structure) optimized for understanding spatiotemporal dynamics, and various condition controllers for incorporating user inputs. \n- The project also introduces several assistant strategies like Min-Max Token Strategy, Adaptive Gradient Clipping, and Prompt Refinement to enhance training and inference efficiency. \n- Evaluations show Open-Sora Plan achieves state-of-the-art results in aesthetic quality and motion smoothness, especially demonstrating proficiency in generating complex scenes. \n- Further improvements are planned, focusing on model scaling, data enhancement, and novel algorithmic implementations.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/PKU-YuanGroup/Open-Sora-Plan"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video",
        "authors": "Zhaoyang Zeng, Tianhe Ren, Shilong Liu, Hongyang Li, Jinyuan Qu",
        "link": "https://arxiv.org/abs/2411.18671",
        "github_repo": null,
        "summary": "- TAPTRv3 is a DETR-like framework for robustly tracking any point in long videos, improving upon TAPTRv2 by addressing its limitations in feature querying.\n- It leverages spatial context through Context-aware Cross-Attention (CCA) for enhanced attention scores and eliminates distractions in feature comparisons.\n- For temporal context, it introduces Visibility-aware Long-Temporal Attention (VLTA) to consider past frames' visibility and address feature drifting.\n- A global matching module reinitializes point queries upon scene cut detection for tracking re-establishment.\n- TAPTRv3 achieves state-of-the-art performance on challenging datasets, outperforming TAPTRv2 and remaining competitive even against models trained with larger datasets.",
        "classification": [
            "Computer Vision",
            "Keypoint Detection"
        ],
        "github_urls": [
            "taptr.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "o1-Coder: an o1 Replication for Coding",
        "authors": "Jinlin Xiao, Jiangming Shu, Yuqi Yang, Shangxi Wu, Yuxiang Zhang",
        "link": "https://arxiv.org/abs/2412.00154",
        "github_repo": "https://github.com/ADaM-BJTU/O1-CODER",
        "summary": "- This paper introduces O1-CODER, a framework attempting to replicate OpenAI's O1 model, focusing on coding tasks and enhancing System-2 thinking through Reinforcement Learning (RL) and Monte Carlo Tree Search (MCTS).\n- The framework incorporates a Test Case Generator (TCG) for automated code evaluation, MCTS for generating reasoning data, and iterative fine-tuning of a policy model, initially producing pseudocode and subsequently full code.\n- O1-CODER uses pseudocode-based prompting and behavioral actions, addressing the challenges of self-play RL in code generation, including evaluation and process reward design.\n- While initial experiments on the MBPP benchmark show a slight decrease in overall pass rate with pseudocode, a significant improvement in the Average Sampling Pass Rate suggests enhanced reasoning capabilities when the generated pseudocode is correct.\n- The paper further discusses the broader implications of moving beyond human-recorded data, the potential of self-play+RL in complex problem solving, and the challenges in applying O1-like models to real-world applications requiring dynamic environment interaction.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/ADaM-BJTU/O1-CODER"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "TinyFusion: Diffusion Transformers Learned Shallow",
        "authors": "Xinchao Wang, Xinyin Ma, Kunjun Li, Gongfan Fang",
        "link": "https://arxiv.org/abs/2412.01199",
        "github_repo": "https://github.com/VainF/TinyFusion",
        "summary": "- TinyFusion, a novel depth pruning method, compresses diffusion transformers by removing redundant layers through end-to-end learning, focusing on optimizing the recoverability of the pruned model for better post-fine-tuning performance.\n- Introduces a differentiable sampling technique for making pruning decisions learnable and utilizes a co-optimized parameter, often implemented with LoRA, to simulate future fine-tuning, effectively integrating pruning and fine-tuning processes.\n- Demonstrates superior performance in compressing various transformer-based diffusion models like DiTs, MARs, and SiTs, achieving a 2x speedup with DiT-XL at less than 7% of the original training cost and a FID score of 2.86.\n- Employs MaskedKD, a variant of knowledge distillation, to mitigate the impact of massive activations in hidden states, further enhancing the recoverability and performance of pruned models.\n- Shows that minimizing immediate calibration loss after pruning might not be ideal for diffusion transformers, as models with higher initial losses pruned by TinyFusion can achieve significantly better FID scores after fine-tuning compared to methods focusing solely on minimizing calibration loss.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [
            "https://github.com/VainF/TinyFusion"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
        "authors": "Yueh-Hua Wu, Yong Man Ro, Yu-Chiang Frank Wang, Ryo Hachiuma, BK-Lee",
        "link": "https://arxiv.org/abs/2412.01822",
        "github_repo": null,
        "summary": "- Introduces VLSI, a new Vision Language Model (VLM) family (2B and 7B parameter sizes) that uses a novel natural language-based distillation process called \"Verbalized Layers-to-Interactions\" to transfer knowledge from large to small VLMs.\n- Employs layer-wise distillation with intermediate \"verbalizers\" to project features into natural language, which allows smaller VLMs to better align with the reasoning processes of larger VLMs, unlike traditional methods that focus solely on final-layer imitation.\n- Achieves notable performance improvements over GPT-4V (11.0% for 2B and 17.4% for 7B model sizes) on various vision-language benchmarks without increasing model size, module merging, or architectural modifications.\n- Validated across ten diverse benchmarks, demonstrating state-of-the-art performance and improved efficiency, especially for deployment on resource-constrained devices.\n- Offers an easily implementable and adaptable approach across different model architectures, showing significant gains with both Qwen2-VL and LLaVA-OV backbones.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
        "authors": "Liuhan Chen, Yang Ye, Zongjian Li, BestWishYsh, LanguageBind",
        "link": "https://arxiv.org/abs/2411.17459",
        "github_repo": "https://github.com/PKU-YuanGroup/WF-VAE",
        "summary": "- WF-VAE, a novel autoencoder designed for latent video diffusion models, leverages multi-level wavelet transforms to create a main energy flow pathway that prioritizes low-frequency video information in the latent representation.\n- This architecture simplifies the backbone design, reducing computational costs by bypassing low-frequency information through the backbone.\n- A Causal Cache mechanism maintains latent space integrity during block-wise inference, eliminating flickering artifacts often seen in reconstructed videos.\n- Experimental results show WF-VAE achieves state-of-the-art performance in reconstruction quality and computational efficiency, outperforming existing video VAEs.\n- It achieves 2x higher throughput and 4x lower memory consumption while maintaining highly competitive reconstruction quality and improving video generation metrics (FVD and IS) when paired with diffusion models.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/PKU-YuanGroup/WF-VAE"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
        "authors": "Huaizhong Zhang, Zhengyu Lin, Weiye Xiao, Jianping Jiang, caizhongang",
        "link": "https://arxiv.org/abs/2412.00174",
        "github_repo": null,
        "summary": "- SOLAMI is a novel end-to-end social Vision-Language-Action (VLA) model for generating multimodal responses (speech and motion) in interactions with 3D autonomous characters.\n- The model uses separate tokenizers for speech and motion, converting them into discrete tokens that are fed into a decoder-only LLM backbone (AnyGPT-base, based on LLaMA2-7B).\n- The model is trained in three stages: tokenizer training, multi-task pre-training for modality alignment (motion-text and speech-text), and instruction tuning on a synthetic multimodal social interaction dataset called SynMSI.\n- Quantitative results on SynMSI show that SOLAMI outperforms baseline methods (LLM+Speech, AnyGPT fine-tuned, and DLP) in terms of motion quality and inference latency, generating more natural and coherent responses.\n- A user study conducted with a VR interface further validates SOLAMI's superior performance, demonstrating enhanced user experience across metrics such as motion coherence, motion interaction, speech consistency, and overall experience.",
        "classification": [
            "Multimodal",
            "Text-to-Speech",
            "Text-to-Video",
            "Text-to-3D",
            "Robotics"
        ],
        "github_urls": [
            "https://solami-ai.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
        "authors": "Yuan Zhou, Qiuyue Wang, Yuxuan Cai, hyang0511, Cakeyan",
        "link": "https://arxiv.org/abs/2412.01316",
        "github_repo": null,
        "summary": "- Presto, a novel video diffusion model, generates 15-second videos with long-range coherence and rich content using a Segmented Cross-Attention (SCA) strategy.\n- SCA divides hidden states into temporal segments, allowing each to cross-attend to a corresponding sub-caption, enhancing coherence without additional parameters.\n- The LongTake-HD dataset, comprising 261k content-rich videos with progressive sub-captions, facilitates high-quality long video generation.\n- Presto achieves 78.5% on VBench Semantic Score and 100% on Dynamic Degree, outperforming state-of-the-art methods in content richness and coherence.\n- A user study confirms Presto's superiority in scenario diversity, coherence, and text-video alignment compared to open-source and commercial alternatives.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input",
        "authors": "Alessandro Farinelli, Alberto Castellini, Gianni Franchi, e-zorzi, ftaioli",
        "link": "https://arxiv.org/abs/2412.01250",
        "github_repo": null,
        "summary": "- Introduces Collaborative Instance Navigation (CoIN), a new task for embodied agents involving interactive dialogue with humans to locate target objects in unknown environments.\n- Presents AIUTA, a training-free method leveraging Vision-Language Models (VLMs) and Large Language Models (LLMs) to facilitate agent self-dialogue, reducing reliance on full initial descriptions.\n- Includes a novel Normalized-Entropy based technique to estimate and mitigate VLM uncertainty during object description generation.\n- Introduces CoIN-Bench, a new benchmark with real and simulated human evaluations for CoIN, and demonstrates AIUTA\u2019s state-of-the-art performance on zero-shot instance navigation, handling user input flexibility.\n- Proposes IDKVQA, a dedicated dataset for evaluating VLM uncertainty estimation and shows the superiority of their approach.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://intelligolabs.github.io/COIN"
        ],
        "date": "2024-12-03"
    },
    {
        "title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety",
        "authors": "Jing Shao, Xuanjing Huang, LLLeo612, Max9803, Foreshhh",
        "link": "https://arxiv.org/abs/2411.19939",
        "github_repo": null,
        "summary": "- This paper introduces VLSBench, a new multimodal visual leakless safety benchmark with 2.4k image-text pairs designed to address the Visual Safety Information Leakage (VSIL) problem in existing multimodal safety benchmarks.\n- VSIL occurs when sensitive image content is revealed in the text query, allowing models to bypass visual processing and make safety decisions based on text alone. \n- VLSBench pairs images with neutral text queries, forcing models to rely on visual understanding for safety assessments. \n- Experimental results show that VLSBench is challenging for both open-source and closed-source Multimodal Large Language Models (MLLMs), with even the best performing model only achieving a 49.78% safety rate. \n- The study also found that multimodal alignment methods outperform textual alignment on VLSBench, highlighting the importance of multimodal reasoning for visual safety in the absence of VSIL.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
        "authors": "atcbosselut, jjzha, jebish7, shayekh, angelika",
        "link": "https://arxiv.org/abs/2411.19799",
        "github_repo": null,
        "summary": "- This paper introduces INCLUDE, a multilingual benchmark dataset designed to evaluate the regional knowledge understanding of large language models (LLMs).\n- INCLUDE consists of 197,243 multiple-choice questions across 44 languages and 15 scripts, collected from various sources, including academic exams, professional certifications, and regional licenses.\n- The benchmark is designed to address the lack of high-quality evaluation resources in languages other than English and to capture cultural nuances associated with each language.\n- Experimental results demonstrate that current LLMs achieve high variance in performance between different languages and often struggle with questions requiring regional knowledge.\n- Analysis suggests that performance limitations stem from model's grasp of specialized regional knowledge for different languages.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/CohereForAI/include-base-44"
        ],
        "date": "2024-12-03"
    },
    {
        "title": "Efficient Track Anything",
        "authors": "Chenchen Zhu, Lemeng Wu, Xiaoyu Xiang, Chong Zhou, yunyangx",
        "link": "https://arxiv.org/abs/2411.18933",
        "github_repo": null,
        "summary": "- The paper introduces EfficientTAM, a lightweight model for video object segmentation and \"track anything\" tasks that prioritizes efficiency for real-world applications, especially on mobile devices.\n- EfficientTAM utilizes a plain, non-hierarchical Vision Transformer (ViT) as the image encoder and employs an efficient memory module with a novel cross-attention mechanism to reduce computational complexity.\n- The model is trained on SA-1B and SA-V datasets and achieves comparable performance to SAM 2 with a 2x speedup on A100 GPUs and a 2.4x parameter reduction.\n- On mobile devices like the iPhone 15 Pro Max, EfficientTAM runs at ~10 FPS with reasonable quality.\n- EfficientTAM also shows promising results on image segmentation benchmarks, outperforming the original SAM and achieving a 20x speedup on A100 GPUs with a 20x parameter reduction.",
        "classification": [
            "Image Segmentation",
            "Video Classification",
            "Object Detection"
        ],
        "github_urls": [
            "https://yformer.github.io/efficient-track-anything/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information",
        "authors": "Rui Zhang, Ranran Haoran Zhang, Sarkar Snigdha Sarathi Das, Yusen Zhang, ryokamoi",
        "link": "https://arxiv.org/abs/2412.00947",
        "github_repo": "https://github.com/psunlpgroup/VisOnlyQA",
        "summary": "- This paper introduces VisOnlyQA, a new dataset designed to evaluate the visual perception capabilities of Large Vision Language Models (LVLMs) on questions related to geometric and numerical information in scientific figures.\n- VisOnlyQA includes 1,200 multiple-choice questions across 12 tasks and four categories of figures, along with 70k synthetic training instances.\n- Experiments with 20 LVLMs, including GPT-40 and Gemini 1.5 Pro, reveal poor performance on VisOnlyQA compared to near-perfect human performance.\n- Fine-tuning on synthetic data shows potential but limited improvement, suggesting both training data and model architecture need improvement.\n- The authors observed that stronger language models enhanced the visual perception of LVLMs, even though the dataset focuses solely on visual perception.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/psunlpgroup/VisOnlyQA"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation",
        "authors": "Wenhu Chen, Cong Wei, Jie Min, hyang0511, wren93",
        "link": "https://arxiv.org/abs/2412.00927",
        "github_repo": null,
        "summary": "- VISTA, a novel Video SpatioTemporal Augmentation framework, synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets by spatially and temporally combining videos.\n- It leverages a large language model, Gemini 1.5-Pro, to generate question-answer pairs related to the newly synthesized videos.\n- VISTA-400K, a 400,000-sample video instruction-following dataset based on this approach, improves the performance of various video Large Multimodal Models (LMMs) by an average of 3.3% across four long-video understanding benchmarks.\n- Introduction of HRVideoBench, the first high-resolution video understanding benchmark, on which VISTA-finetuned models show a 6.5% performance gain.\n- Ablation studies demonstrate that disabling proposed augmentations reduces model performance, highlighting the quality and importance of the generated data.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision",
            "Video Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://tiger-ai-lab.github.io/VISTA/"
        ],
        "date": "2024-12-03"
    },
    {
        "title": "Steering Rectified Flow Models in the Vector Field for Controlled Image Generation",
        "authors": "Yezhou Yang, Dimitris N. Metaxas, Song Wen, mpatel57",
        "link": "https://arxiv.org/abs/2412.00100",
        "github_repo": null,
        "summary": "- FlowChef, a novel method for controlled image generation using rectified flow models (RFMs), is introduced, offering a unified framework for tasks such as linear inverse problems, image editing, and classifier guidance.\n- FlowChef steers the denoising trajectory within the vector field by gradient skipping without requiring backpropagation through ODE solvers or inversion, leading to improved computational efficiency.\n- The method addresses the limitations of current diffusion-based and flow-based methods, achieving state-of-the-art results with reduced computational costs.\n- Evaluations demonstrate superior performance across linear inverse problems, image editing on PIE benchmark, and classifier-guided style transfer, outperforming baselines on metrics like PSNR, SSIM, and LPIPS while using less memory and time.\n- FlowChef\u2019s adaptability is highlighted through extensions to multi-object image editing and 3D multiview synthesis using large-scale models like Flux.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos",
        "authors": "Hangyu Guo, Haoze Zhao, Haoran Tang, Meng Cao, zhangysk",
        "link": "https://arxiv.org/abs/2412.01800",
        "github_repo": null,
        "summary": "- PhysGame, a benchmark designed to evaluate the ability of Video Large Language Models (Video LLMs) to identify and interpret physical commonsense violations in gameplay videos.\n- Constructed using 880 gameplay videos annotated with multiple-choice questions focused on uncovering glitches that defy physical commonsense understanding, categorized across four primary physical domains: mechanics, kinematics, optics, and material properties, subdivided into twelve distinct categories.\n-  Analysis shows open-source Video LLMs underperforming compared to proprietary counterparts, leading to the creation of PhysInstruct and PhysDPO, two datasets containing over 174k training examples in total. \n- The proposed physical knowledge-enhanced Video LLM, PhysVLM, trained on the introduced datasets, reaches state-of-the-art performance on PhysGame, outperforming existing open-source and commercial models. \n- PhysVLM demonstrates strong generalizability, achieving high scores on standard video understanding benchmarks such as Video-MME and VCG.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/PhysGame/PhysGame"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait",
        "authors": "Gyoungsu Chae, Dongchan Min, Taekyung Ki",
        "link": "https://arxiv.org/abs/2412.01064",
        "github_repo": null,
        "summary": "- FLOAT is a novel audio-driven talking portrait video generation model based on flow matching in a learned motion latent space, enabling efficient design of temporally consistent motion.\n- It introduces a transformer-based vector field predictor with frame-wise conditioning, and supports emotion enhancement driven by speech-driven emotion labels.\n- FLOAT outperforms existing state-of-the-art audio-driven talking portrait methods on HDTF and RAVDESS datasets in terms of visual quality, motion fidelity, and efficiency, achieving FID scores of 21.10 and 31.68, respectively.\n- The use of flow matching allows for faster and higher quality sampling compared to diffusion-based methods, while the motion latent space ensures temporal consistency and expressiveness. \n- Ablation studies confirm the benefits of the proposed FMT architecture and the use of speech-driven emotional labels",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Audio"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models",
        "authors": "Jingren Zhou, Bolin Ding, Yaliang Li, Xuchen Pan, yanxi-chen",
        "link": "https://arxiv.org/abs/2411.19477",
        "github_repo": null,
        "summary": "- This paper proposes a two-stage algorithm for enhancing the test-time compute of Large Language Models (LLMs), aiming to boost their success probability on challenging tasks.\n- The algorithm first generates multiple candidate solutions and then selects the best one through a knockout tournament, where pairs of solutions are compared multiple times.\n- Theoretical analysis proves that the failure probability of this algorithm decreases exponentially with increased compute, given the assumptions that the LLM has a non-zero probability of generating a correct solution and can distinguish between correct and incorrect solutions better than random chance.\n- Empirical results on the MMLU-Pro benchmark validate these assumptions and demonstrate performance improvement with increased test-time compute, especially for reasoning-focused questions.\n- The paper discusses limitations and future research directions, including potential for handling complex tasks via decomposition and exploring more efficient algorithms with provable scaling laws.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning",
        "authors": "Noel Crespi, Reza Farahbaksh, callmesan",
        "link": "https://arxiv.org/abs/2412.01408",
        "github_repo": null,
        "summary": "- This paper proposes a few-shot cross-lingual audio abuse detection method using Model-Agnostic Meta-Learning (MAML) with pre-trained audio representations in low-resource settings.\n- The model leverages Whisper and Wav2Vec and evaluates two feature normalization strategies: Temporal Mean and L2 normalization.\n- Experiments are conducted on the ADIMA dataset, comprising abusive audio clips in 10 Indian languages.\n- The best-performing model is Whisper with L2-Norm normalization, achieving accuracy scores ranging from 78.98% to 85.22% in the 100-shot setting.\n- A feature visualization study shows that language similarity can enhance cross-lingual abuse detection, especially in low-resource settings.",
        "classification": [
            "Audio",
            "Audio Classification",
            "Natural Language Processing",
            "Zero-Shot Classification"
        ],
        "github_urls": [
            "https://github.com/callmesanfornow/fsl-audio-abuse.git"
        ],
        "huggingface_urls": [],
        "date": "2024-12-03"
    },
    {
        "title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
        "authors": "cqf, tfl01, AI4VR, Jethro37, Cheliosoops",
        "link": "https://arxiv.org/abs/2412.02259",
        "github_repo": null,
        "summary": "- This paper introduces VideoGen-of-Thought (VGoT), a novel collaborative and training-free framework for generating multi-shot videos from a single user-provided sentence.\n- VGoT addresses the challenges of maintaining logical narrative and visual consistency by incorporating four modules: script generation, keyframe generation, shot-level video generation, and cross-shot smoothing.\n- The framework begins by elaborating a concise user prompt into detailed shot descriptions covering character, background, relations, camera pose, and HDR lighting.\n- Keyframes consistent with the character portrayal are generated using identity-preserving embeddings. Finally, individual video shots are synthesized and then seamlessly connected through a cross-shot smoothing mechanism. \n- Experimental results indicate that VGoT surpasses existing text-to-video generation methods in generating coherent multi-shot videos while exhibiting higher scores on visual quality metrics such as PSNR and IS, particularly excelling in cross-shot consistency as validated through user studies.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-04"
    },
    {
        "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability",
        "authors": "zptu, Thu-redrobot, SihengLi, Chufan, Jiahao004",
        "link": "https://arxiv.org/abs/2411.19943",
        "github_repo": null,
        "summary": "- This paper introduces cDPO, a novel token-level contrastive estimation and preference optimization framework designed to enhance the reasoning capabilities of Large Language Models (LLMs).\n- cDPO identifies \"critical tokens\" within incorrect reasoning trajectories by comparing the generation likelihood from positive and negative models fine-tuned on correct and incorrect reasoning trajectories, respectively.\n- It then leverages these contrastive likelihoods as token-level rewards during preference optimization, thereby guiding the model to avoid generating critical tokens that lead to erroneous outcomes.\n- Experimental results on GSM8K and MATH500 benchmarks demonstrate that cDPO significantly outperforms existing example-level and step-level baseline strategies (p < 0.005) across various LLMs, including Llama-3 (8B and 70B) and DeepSeek-math (7B), achieving average accuracies of 77.2% and 33.4% respectively.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-04"
    },
    {
        "title": "Free Process Rewards without Process Labels",
        "authors": "iseesaw, stingning, ganqu, wendili, lievan",
        "link": "https://arxiv.org/abs/2412.01981",
        "github_repo": "https://github.com/lifan-yuan/ImplicitPRM",
        "summary": "- This paper introduces implicit Process Reward Models (PRMs), which can be derived from Outcome Reward Models (ORMs) trained on response-level labels without needing expensive step-level annotations.\n- By parameterizing the outcome reward as the log-likelihood ratio of policy and reference language models, a PRM can be automatically learned during ORM training, significantly reducing the data collection and training costs.\n- Experiments on MATH demonstrate that the implicit PRM outperforms a strong MCTS-based baseline, Math-Shepherd, with less than 1/38 of the training data and achieves state-of-the-art performance compared to open-source reward models.\n- Further analysis shows scaling data and using majority voting improves performance, but incorporating step labels during training provides no gains.\n- The paper suggests that the reference model can even be omitted for models pre-trained with preference learning without harming the performance, increasing the inference efficiency.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/lifan-yuan/ImplicitPRM"
        ],
        "date": "2024-12-04"
    },
    {
        "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
        "authors": "Sunxy111, Xiaomabufei, senfu, PeihaoChen, Hoyard",
        "link": "https://arxiv.org/abs/2412.01292",
        "github_repo": null,
        "summary": "- LSceneLLM, an adaptive framework for enhancing large 3D scene understanding, is introduced, which addresses the challenges of accurately locating task-relevant visual information within high-density point clouds.\n- It employs a scene magnifier module with a dense token selector and an adaptive self-attention mechanism to dynamically identify task-relevant areas, guided by the LLM's visual preferences, and extract and fuse detailed information from these regions.\n- A new cross-room understanding benchmark, XR-Scene, featuring XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption tasks, is also presented for comprehensive evaluation of large 3D scene understanding.\n- Experimental results demonstrate LSceneLLM's state-of-the-art performance on various 3D tasks and benchmarks, including both indoor and outdoor large-scene understanding, as well as existing single-room scene understanding benchmarks.\n- Integrating the scene magnifier module with existing 3D-VLMs leads to significant performance improvements.",
        "classification": [
            "Computer Vision",
            "Visual Question Answering",
            "Multimodal",
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-04"
    },
    {
        "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
        "authors": "zichenwen, ouyanglinke, binwang, qintong21, Carkham",
        "link": "https://arxiv.org/abs/2412.02592",
        "github_repo": "https://github.com/opendatalab/OHR-Bench",
        "summary": "- This paper introduces OHRBench, a new benchmark designed to evaluate the cascading impact of Optical Character Recognition (OCR) on Retrieval-Augmented Generation (RAG) systems.\n- OHRBench includes a diverse dataset of PDF documents from six real-world applications, along with questions based on multimodal elements, as well as a set of perturbed structured data designed to explore the fine-grained effect of two identified primary types of OCR noise on RAG systems: Semantic Noise and Formatting Noise.\n- Through comprehensive evaluation with existing OCR solutions, results demonstrate that none of the extracted structured data from these solutions are competent for constructing high-quality knowledge bases for RAG, as they all suffer performance losses of at least 7.5%.\n- Further analysis reveals that all retrievers and large language models are significantly affected by Semantic Noise, especially in tables and formulas, while Formatting Noise affects specific retrievers and large language models differently.\n- Finally, through experiments with Vision-Language Models in the generation stage, results show that combining image and OCR text as input can improve performance significantly, demonstrating potential for further research on integrating VLMs in RAG.",
        "classification": [
            "Document Question Answering",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/opendatalab/OHR-Bench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-04"
    },
    {
        "title": "MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation",
        "authors": "Dongyoon Han, Song Park, Seungho Lee, Minhyun Lee, bhheo",
        "link": "https://arxiv.org/abs/2411.19067",
        "github_repo": "https://github.com/naver-ai/maskris",
        "summary": "- This paper introduces MaskRIS, a novel training framework for Referring Image Segmentation (RIS) that leverages image and text masking alongside Distortion-aware Contextual Learning (DCL).\n- MaskRIS addresses the limitations of conventional data augmentation techniques in RIS by mitigating semantic conflicts and enhancing data diversity.\n- This framework consists of a primary path that processes original inputs for training stability, and a secondary path that processes masked inputs to enhance model robustness.\n- MaskRIS achieves state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg, demonstrating significant improvements in oIoU scores compared to existing methods.\n- For example, MaskRIS improves oIoU by 1.82%, 1.33%, and 2.25% on RefCOCO validation, testA, and testB, respectively, over CARIS.",
        "classification": [
            "Multimodal",
            "Image Segmentation",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/naver-ai/maskris"
        ],
        "huggingface_urls": [],
        "date": "2024-12-04"
    },
    {
        "title": "A dynamic parallel method for performance optimization on hybrid CPUs",
        "authors": "Liu Yucheng, Luo Yu, Haihao",
        "link": "https://arxiv.org/abs/2411.19542",
        "github_repo": null,
        "summary": "- This paper introduces a dynamic parallel method for optimizing Large Language Model (LLM) inference performance on hybrid CPUs, which addresses the issue of imbalanced hardware capabilities among different cores.\n- The method dynamically balances the workload for each core before parallel processing begins, leading to significant performance improvements.\n- It integrates this new parallel method into Neural Speed, an optimized x86 assembly code framework based on llama.cpp.\n- The results demonstrate over 90% average memory bandwidth utilization on two hybrid Intel CPUs during 4-bit LLM inference, a 20%-30% improvement over the original OpenMP method in Neural Speed, and up to a 3.7x speedup compared to llama.cpp.\n- This dynamic approach adapts to varying system conditions and maximizes CPU performance by dynamically adjusting kernel workload distribution based on real-time performance ratios.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-04"
    },
    {
        "title": "VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval",
        "authors": "Nabeel Mohammed, Md Rizwan Parvez, shafin5, dpaul06",
        "link": "https://arxiv.org/abs/2412.01558",
        "github_repo": "https://github.com/dpaul06/VideoLights",
        "summary": "- VideoLights is a novel framework for joint Video Highlight Detection (HD) and Moment Retrieval (MR) using a Bi-Directional Cross-Modal Fusion (Bi-CMF) Network within a transformer architecture.\n- The model incorporates a Feature Refinement and Alignment (FRA) Module to refine visual features and align them with textual features at local and global levels, and a Unidirectional Joint-Task Feedback Mechanism (Uni-JFM) to enhance task correlation.\n- VideoLights leverages features from Large Vision-Language Models (LVLMs) like BLIP-2, CLIP, and SlowFast, and employs intelligent model pre-training with synthetic data generated by LVLMs. \n- Adaptive hard positive/negative loss functions are utilized for adaptive error penalization and improved learning.\n- The model achieves state-of-the-art performance on QVHighlights, TVSum, and Charades-STA benchmarks, outperforming existing methods by significant margins and improving MR metrics such as R@0.5 by up to 6.81% and HD metrics such as mAP by up to 6.9%.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/dpaul06/VideoLights"
        ],
        "huggingface_urls": [],
        "date": "2024-12-04"
    },
    {
        "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
        "authors": "Khoi Nguyen, anhttran1111, termanteus, aengusng, viettmab",
        "link": "https://arxiv.org/abs/2412.02687",
        "github_repo": null,
        "summary": "- SNOOPI, a novel framework, enhances one-step text-to-image diffusion models through Proper Guidance-SwiftBrush (PG-SB) and Negative-Away Steer Attention (NASA).\n- PG-SB improves training stability by dynamically adjusting teacher model guidance scales during Variational Score Distillation (VSD), addressing common instabilities across diverse model architectures.\n- NASA introduces negative prompting to one-step diffusion models, a previously unsupported feature, by manipulating cross-attention layers, refining image generation by suppressing undesired features.\n- Evaluation on benchmarks like HPSv2 shows SNOOPI outperforming existing one-step diffusion models, achieving a state-of-the-art score of 31.08.\n-  The enhanced control and stability of SNOOPI make rapid, high-quality text-to-image generation more practical.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
        "authors": "liuziwei7, guoyww, mimihe, tongwu2020, jingtan",
        "link": "https://arxiv.org/abs/2412.03552",
        "github_repo": null,
        "summary": "- Imagine360 is a novel framework that generates immersive 360\u00b0 videos from standard perspective videos, using a dual-branch denoising structure with panorama and perspective branches.\n- It incorporates cross-domain spherical attention with antipodal masking to capture long-range motion dependencies and ensure plausible spherical patterns.\n- Elevation-aware designs handle diverse video inputs with varying elevation angles, and a resource-friendly fine-tuning strategy optimizes performance on limited data.\n- Experimental results demonstrate superior graphics quality and motion coherence compared to existing 360\u00b0 video generation methods.\n- Imagine360 also exhibits a bonus advantage in achieving superior results for panorama image outpainting.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
        "authors": "An Zhao, slysun, haoranxu, mengcy, SYZhang0805",
        "link": "https://arxiv.org/abs/2412.03515",
        "github_repo": "https://github.com/happyw1nd/ScoreLiDAR",
        "summary": "- This paper introduces ScoreLiDAR, a novel distillation method for 3D LiDAR scene completion diffusion models, enabling efficient and high-quality scene completion.\n- ScoreLiDAR adapts Variational Score Distillation (VSD) and incorporates a Structural Loss, including scene-wise and point-wise terms, to capture geometric structure information and enhance detail retention.\n- ScoreLiDAR achieves a >5x speedup, completing scenes in ~5 seconds compared to ~30 seconds for the state-of-the-art LiDiff model.\n- Evaluation on SemanticKITTI and KITTI-360 datasets demonstrates superior performance with lower Chamfer Distance (CD) and Jensen-Shannon Divergence (JSD) compared to existing methods.\n- Ablation studies confirm the effectiveness of the structural loss in improving completion quality.",
        "classification": [
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/happywlnd/ScoreLiDAR"
        ],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
        "authors": "sweetrabor, gaozong, xuwang, liqingzju, leo1117",
        "link": "https://arxiv.org/abs/2412.03069",
        "github_repo": null,
        "summary": "- Introduces TokenFlow, a novel unified image tokenizer with a dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining alignment via shared index mapping, bridging the gap between multimodal understanding and generation.\n- Demonstrates state-of-the-art autoregressive image generation with a GenEval score of 0.55 at 256x256 resolution and strong reconstruction performance (FID 0.63 at 384x384), surpassing methods like EMU3 and LlamaGen with fewer sampling steps. \n- Achieves a new state-of-the-art in multimodal understanding, surpassing LLaVA-1.5 13B by 7.2% on average by leveraging the Qwen-2.5-14B language model. \n- Shows that discrete visual input can outperform continuous visual baselines for the first time on understanding tasks. \n- Maintains high codebook utilization (95%+) even with large codebooks (over 130K), exceeding prior approaches in capacity and efficiency.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding",
        "authors": "asdfg80, slvjul, zd11024",
        "link": "https://arxiv.org/abs/2412.00493",
        "github_repo": null,
        "summary": "- This paper introduces Video-3D LLM, a novel generalist model for 3D scene understanding.\n- The model leverages a Video LLM framework, processing video frames augmented with corresponding 3D spatial coordinates obtained from depth images.\n- It enhances 3D scene understanding by creating position-aware video representations through the integration of 3D position encodings derived from spatial coordinates.\n- A maximum coverage sampling technique optimizes the balance between computational cost and performance.\n- The model achieves state-of-the-art performance on benchmarks like ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D, outperforming LLaVA-3D while using only 26% of its 3D data.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/LaVi-Lab/Video-3D-LLM"
        ],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
        "authors": "Chengwh, bluestyle97, Yw22, ZyZcuhk, l-li",
        "link": "https://arxiv.org/abs/2412.03517",
        "github_repo": null,
        "summary": "- NVComposer is a novel view synthesis (NVS) model that generates novel views from multiple sparse and unposed images without requiring external alignment processes like pose estimation or pre-reconstruction.\n- It uses an image-pose dual-stream diffusion model to generate novel views and implicitly predict camera poses for input images, and a geometry-aware feature alignment module distills 3D knowledge from a pre-trained dense stereo model.\n- NVComposer achieves state-of-the-art performance on generative multi-view NVS tasks, outperforming existing methods on RealEstate10K and DL3DV datasets.\n- The model's performance improves as the number of unposed input views increases, demonstrating its ability to leverage additional information.\n- Unlike other multi-view methods that suffer performance degradation with more input views, NVComposer utilizes the extra information effectively, enhancing its utility for flexible and accessible generative NVS systems.",
        "classification": [
            "Image-to-Image",
            "Computer Vision",
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models",
        "authors": "SunYoung Park, Daeyoung Kim, kimyoungjune, hojunssss",
        "link": "https://arxiv.org/abs/2411.19103",
        "github_repo": null,
        "summary": "- This paper introduces VARCO-VISION-14B, a bilingual (Korean-English) vision-language model based on Qwen-2.5-14B-Instruct as its language model and SigLIP as its vision encoder, trained using a four-stage process involving feature alignment, supervised fine-tuning, and preference optimization.\n- The model outperforms similarly sized open-source models on Korean multimodal benchmarks and achieves comparable performance to larger proprietary models, demonstrating strong bilingual capabilities.\n- Five Korean evaluation datasets are released alongside the model, including four closed-set (K-MMBench, K-SEED, K-MMStar, K-DTCBench) and one open-set (K-LLaVA-W) benchmarks, translated and validated from established English benchmarks to assess bilingual proficiency and document, table and chart understanding.\n- VARCO-VISION exhibits proficient grounding, referring, and OCR capabilities in both languages.\n- The authors aim to promote open research in Korean VLMs with this release and encourage further development of bilingual multimodal models.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering",
            "Document Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/NCSOFT/VARCO-VISION-14B",
            "https://huggingface.co/datasets/NCSOFT/K-MMBench",
            "https://huggingface.co/datasets/NCSOFT/K-SEED",
            "https://huggingface.co/datasets/NCSOFT/K-MMStar",
            "https://huggingface.co/datasets/NCSOFT/K-DTCBench",
            "https://huggingface.co/datasets/NCSOFT/K-LLAVA-W"
        ],
        "date": "2024-12-05"
    },
    {
        "title": "CleanDIFT: Diffusion Features without Noise",
        "authors": "Bj\u00f6rn Ommer, FrankFundel, kolja-b, stefan-baumann, kliyer",
        "link": "https://arxiv.org/abs/2412.03439",
        "github_repo": null,
        "summary": "- CleanDIFT, a novel feature extraction method for diffusion models, produces noise-free, timestep-independent, general-purpose features.\n- CleanDIFT fine-tunes a trainable copy of a pre-trained diffusion model on clean images, aligning its features with the timestep-dependent internal representations of the original model using projection heads.\n- This approach eliminates the need to add noise to input images during feature extraction and avoids task-specific timestep tuning.\n- Experiments demonstrate significant performance improvements over existing diffusion feature methods across tasks like semantic correspondence, depth estimation, semantic segmentation, and classification, and sets a new state-of-the-art in zero-shot unsupervised semantic correspondence.\n- CleanDIFT also offers a 50x speedup for supervised semantic correspondence compared to methods requiring DDIM inversion.",
        "classification": [
            "Image Feature Extraction",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation",
        "authors": "zouzx, yhyang-myron, XingqiaoAn, bennyguo, huanngzh",
        "link": "https://arxiv.org/abs/2412.03558",
        "github_repo": null,
        "summary": "- MIDI is a novel paradigm for compositional 3D scene generation from a single image using multi-instance diffusion models, extending pre-trained image-to-3D object generation models.\n- It incorporates a multi-instance attention mechanism to capture inter-object interactions and spatial coherence directly within the generation process, eliminating the need for multi-step procedures.\n- MIDI is trained on scene-level data to supervise interactions between 3D instances and incorporates single-object data for regularization to maintain generalization ability.\n- Experimental results on synthetic, real-world, and stylized datasets demonstrate that MIDI achieves state-of-the-art performance in image-to-scene generation by accurately modeling inter-object relationships and producing better alignment with input images.\n- MIDI successfully generates coherent and accurate 3D scenes from various image inputs, highlighting its potential and generalization capabilities.",
        "classification": [
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "One Shot, One Talk: Whole-body Talking Avatar from a Single Image",
        "authors": "Boyang Guo, Leipeng Hu, JuyongZhang, YudongGuo, xiangjun-xj",
        "link": "https://arxiv.org/abs/2412.01106",
        "github_repo": null,
        "summary": "- This paper introduces a novel method for creating animatable, expressive, whole-body talking avatars from a single image, addressing the challenge of dynamic modeling and generalization to novel movements.\n- It proposes a pipeline that leverages pose-guided image-to-video diffusion models to generate pseudo-labels, followed by training a hybrid 3DGS-mesh avatar representation constrained by regularizations to handle inconsistencies in these labels.\n- The approach combines the single input image with imperfect pseudo video frames generated from diverse motion sequences and head animation techniques.\n- A perceptual-based loss is employed to improve appearance modeling and several regularization terms are introduced to stabilize the avatar reconstruction process and guide the Gaussian deformation based on mesh information.\n- Experimental results demonstrate superior performance compared to existing methods, even outperforming some techniques that use video input, on cross-identity motion reenactment tasks.",
        "classification": [
            "Image-to-Video",
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
        "authors": "Dandan Zheng, Kecheng Zheng, Yutong Feng, Shuai Tan, BiaoGong",
        "link": "https://arxiv.org/abs/2412.03085",
        "github_repo": null,
        "summary": "- Mimir is a novel text-to-video generation framework that integrates large language models (LLMs) within a diffusion model for enhanced text comprehension.\n- It employs a \"token fuser\" to combine features from both text encoders (like T5) and decoder-only LLMs (like Phi-3.5), addressing the distribution gap between these models.\n- This design allows Mimir to leverage existing video priors in diffusion models while capitalizing on the enhanced reasoning and precise understanding of LLMs.\n- Quantitative and qualitative evaluations on VBench demonstrate Mimir's superior performance, particularly in handling multiple objects, spatial relationships, and short, descriptive prompts.\n- A user study further confirms Mimir's improved capabilities in instruction following, physics simulation, and overall visual quality compared to existing state-of-the-art models.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://lucaria-academy.github.io/Mimir/"
        ],
        "date": "2024-12-05"
    },
    {
        "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
        "authors": "Xiaojun Quan, Tianyuan Shi, Longguang Zhong, Fanqi Wan, Ziyi Yang",
        "link": "https://arxiv.org/abs/2412.03187",
        "github_repo": "https://github.com/SLIT-AI/WRPO",
        "summary": "- This paper introduces Weighted-Reward Preference Optimization (WRPO), a novel implicit model fusion method for enhancing the capabilities of a Large Language Model (LLM) by leveraging preference optimization between source LLMs and a target LLM.\n- WRPO eliminates the need for vocabulary alignment and matrix fusion, enabling efficient scaling to accommodate diverse LLMs and mitigating distributional deviations through a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs.\n- Experiments conducted on MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrated that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines.\n- Using LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a 46.2% win rate against GPT-4-0314 on Arena-Hard, showcasing significant performance improvements.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SLIT-AI/WRPO"
        ],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training",
        "authors": "Yi-Zhe Song, Kai Zou, Hmrishav Bandyopadhyay, ChenDY",
        "link": "https://arxiv.org/abs/2412.02030",
        "github_repo": null,
        "summary": "- NitroFusion is a novel single-step diffusion model for high-fidelity image generation that employs a dynamic adversarial training framework.\n- It utilizes a large, dynamic pool of specialized discriminator heads focusing on different image quality aspects and noise levels, providing diverse feedback to guide the generation process and prevent overfitting by refreshing 1% of heads each iteration. \n- NitroFusion also introduces a multi-scale strategy with dual training objectives for balancing image coherence and prompt alignment, using global heads to assess overall image structure and local heads to examine fine-grained details. \n- Unlike traditional step-reduction approaches, NitroFusion offers a flexible deployment through bottom-up refinement, enabling a trade-off between speed and quality by dynamically choosing between 1 to 4 denoising steps with the same model weights. \n- Experimental results demonstrate that NitroFusion outperforms existing single-step methods across multiple evaluation metrics, exhibiting superior quality on advanced metrics like Aesthetic Score and ImageReward, and often surpasses the performance of recent fast diffusion models while maintaining the speed advantages of single-step generation.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-05"
    },
    {
        "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
        "authors": "Zhongyuan Wang, Zhizheng Zhang, Qi Su, chengchi, Zhoues",
        "link": "https://arxiv.org/abs/2412.04455",
        "github_repo": null,
        "summary": "This paper introduces Code-as-Monitor (CaM), a novel paradigm that leverages vision-language models (VLMs) for both reactive and proactive robotic failure detection. CaM formulates both reactive and proactive failure detection as spatio-temporal constraint satisfaction problems.  Experimental results across three simulators and a real-world setting demonstrate that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% compared to baselines under severe disturbances. CaM is also shown to be integrated with open-loop control policies to form closed-loop systems enabling long-horizon tasks in cluttered scenes with dynamic environments.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://zhoues.github.io/Code-as-Monitor/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
        "authors": "tianbaoxiexxx, ludunjie, ZeonLap, kugwzk, ranpox",
        "link": "https://arxiv.org/abs/2412.04454",
        "github_repo": null,
        "summary": "- AGUVIS, a unified pure vision-based framework, is introduced for building generalizable GUI agents that operate with vision-based observations and a plugin-enabled action system, enhancing cross-platform adaptability.\n- A two-stage training process is employed: first for GUI grounding, followed by planning and reasoning.\n- The model leverages vision-based grounding to improve generalization and reduce inference costs while employing a standardized action space with a plugin system to facilitate consistent learning.\n- Through experiments, AGUVIS surpasses previous state-of-the-art methods on benchmarks like ScreenSpot, Multimodal-Mind2Web, and AndroidControl, achieving the first fully autonomous pure vision GUI agent capable of performing tasks independently.\n- This model demonstrates its efficiency by considerably reducing USD costs and input tokens compared to GPT-40 on Mind2Web-Live.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "A Noise is Worth Diffusion Guidance",
        "authors": "Minjae Kim, Sanghyun Lee, Jiwon Kang, Donghoon Ahn, Min-Jaewon",
        "link": "https://arxiv.org/abs/2412.03895",
        "github_repo": null,
        "summary": " - The paper introduces NoiseRefine, a novel method that enhances the quality of images generated by diffusion models without using guidance techniques like classifier-free guidance (CFG).\n - NoiseRefine refines the initial noise input to the diffusion model by learning a mapping from standard Gaussian noise to a 'guidance-free noise space'.\n - The model uses a multistep score distillation technique to train efficiently and avoids the high computational cost of backpropagation through multiple steps.\n - Experiments show that NoiseRefine achieves comparable image quality to CFG while being significantly faster.\n - The paper analyzes how refined noise contributes to generating high-quality images by investigating the role of different frequency components and demonstrating the effectiveness of low-frequency components in forming layouts.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://cvlab-kaist.github.io/NoiseRefine/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Evaluating Language Models as Synthetic Data Generators",
        "authors": "Seongyun Lee, Vijay Viswanathan, Xiang Yue, Juyoung Suk, seungone",
        "link": "https://arxiv.org/abs/2412.03679",
        "github_repo": null,
        "summary": "- This paper introduces AGORABENCH, a benchmark for evaluating the effectiveness of large language models (LLMs) as synthetic data generators for training other LMs.\n- The benchmark uses standardized settings and a new metric, Performance Gap Recovered (PGR), to compare the quality of synthetic data generated by different LLMs across various tasks and data generation methods.\n- The study finds that an LLM's ability to generate high-quality training data does not necessarily correlate with its problem-solving abilities, but rather with intrinsic properties of the data such as response quality, perplexity, and instruction difficulty.\n- Strategic choices like output format and cost-conscious model selection can significantly impact the effectiveness of data generation, with generating larger datasets from cheaper models sometimes outperforming smaller datasets from more expensive models.\n- The code and data for AGORABENCH are publicly available.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/neulab/data-agora"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "MV-Adapter: Multi-view Consistent Image Generation Made Easy",
        "authors": "Ran Yi, Haoran Wang, pookiefoof, bennyguo, huanngzh",
        "link": "https://arxiv.org/abs/2412.03632",
        "github_repo": null,
        "summary": "- This research presents MV-Adapter, a novel plug-and-play adapter designed to enhance text-to-image (T2I) diffusion models for generating multi-view consistent images. \n- MV-Adapter duplicates self-attention layers to decouple multi-view learning from original model training, preserving prior knowledge and using a parallel attention architecture for integrating multi-view attention and image cross-attention. \n- It incorporates a unified condition guider that encodes camera and geometry information, enabling versatile applications such as text/image-guided 3D generation and texturing. \n- Experimental results demonstrate MV-Adapter's high efficiency, adaptability, and versatility in generating 768-resolution multi-view images on Stable Diffusion XL (SDXL). \n- It achieves a new quality standard, outperforming existing methods in text-to-multiview and image-to-multiview generation tasks in terms of visual fidelity and consistency with conditions.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/black-forest-labs/flux"
        ],
        "huggingface_urls": [
            "https://huggingface.co/xinsir/controlnet-openpose-sdxl-1.0",
            "https://huggingface.co/xinsir/controlnet-scribble-sdxl-1.0",
            "https://huggingface.co/xinsir/controlnet-tile-sdxl-1.0",
            "https://huggingface.co/TencentARC/t2i-adapter-sketch-sdxl-1.0",
            "https://huggingface.co/h94/IP-Adapter",
            "https://huggingface.co/cagliostrolab/animagine-xl-3.1",
            "https://huggingface.co/goofyai/3d_render_style_xl",
            "https://huggingface.co/JerryOrbachJr/Chalk-Sketch-SDXL",
            "https://huggingface.co/ming-yang/sdxl_chinese_ink_lora",
            "https://huggingface.co/TheLastBen/Papercut_SDXL",
            "https://huggingface.co/ByteDance/SDXL-Lightning",
            "https://huggingface.co/latent-consistency/lcm-sdxl"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "Negative Token Merging: Image-based Adversarial Feature Guidance",
        "authors": "Yejin Choi, Ranjay Krishna, Weijia Shi, Lindsey Li, Jaskirat Singh",
        "link": "https://arxiv.org/abs/2412.01339",
        "github_repo": null,
        "summary": "- NegToMe introduces a training-free method for adversarial guidance in diffusion models using reference images instead of text prompts, enabling finer control over visual concepts.\n- NegToMe enhances output diversity (e.g., race, gender) by minimizing feature similarity between generated images and improves visual dissimilarity to copyrighted material by 34.57% when guided with copyrighted references.\n- The method involves merging tokens in transformer blocks during reverse diffusion, pushing generated image features away from matched features in the reference image.\n- It shows improved diversity while maintaining image quality and prompt alignment across various classifier-free guidance scales and diffusion models, including those lacking native negative prompt support like Flux.\n- It incurs a marginal increase (<4%) in inference time.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://negtome.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Densing Law of LLMs",
        "authors": "Xu Han, Guoyang Zeng, Weilin Zhao, Jie Cai, xcjthu",
        "link": "https://arxiv.org/abs/2412.04315",
        "github_repo": null,
        "summary": "- This paper introduces \"capacity density\" to evaluate the training quality of Large Language Models (LLMs) across different scales, considering both effectiveness and efficiency.\n- Capacity density is calculated as the ratio of a model's effective parameter size (the size a reference model would need to achieve equivalent performance) to its actual parameter size.\n- The paper proposes a two-step process to predict downstream task performance: 1) estimate the relationship between parameter size and language modeling loss and 2) estimate the relationship between loss and downstream task performance using a sigmoid function.\n- An empirical law, the \"Densing Law,\" is revealed, showing that the maximum capacity density of open-source base LLMs exhibits exponential growth, doubling approximately every three months.\n- This trend suggests that future LLM development should prioritize improving capacity density rather than solely increasing parameter size, enabling optimal performance with minimal computational overhead.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
        "authors": "Dianqi Li, Haiping Wu, Jianwei Yang, Jiuhai Chen, zhoutianyi",
        "link": "https://arxiv.org/abs/2412.04424",
        "github_repo": "https://github.com/JiuhaiChen/Florence-VL",
        "summary": "- Florence-VL, a new family of Multimodal Large Language Models (MLLMs), leverages the generative vision foundation model Florence-2 as its visual encoder, enabling it to capture diverse visual features at different levels of detail and under various prompts.\n- A novel Depth-Breadth Fusion (DBFusion) mechanism concatenates visual features from different layers (depth) and under multiple prompts (breadth), providing a rich visual representation to the language model.\n- This model is trained with a two-stage process: end-to-end pretraining on a large image captioning dataset followed by fine-tuning on a diverse set of instruction-tuning datasets.\n- Quantitative analysis and visualization demonstrate improved vision-language alignment compared to models using CLIP or SigLIP encoders. \n- Florence-VL achieves state-of-the-art results across 25 multimodal and vision-centric benchmarks, including VQA, OCR, Chart understanding, and knowledge-based reasoning tasks, outperforming other advanced MLLMs like Cambrian.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/JiuhaiChen/Florence-VL"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
        "authors": "Yuqi Zhang, Bin Yan, Yi Jiang, Jinlai Liu, Jian Han",
        "link": "https://arxiv.org/abs/2412.04431",
        "github_repo": null,
        "summary": "- Infinity is a bitwise visual autoregressive model for high-resolution image synthesis.\n- It uses a bitwise token prediction framework with an infinite-vocabulary tokenizer and classifier and bitwise self-correction mechanism.\n- It outperforms top-tier diffusion models like SD3-Medium and SDXL on benchmarks like GenEval, ImageReward, and HPSv2.1, achieving a win rate of 66%.\n- Infinity generates a 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium.\n- The model exhibits strong scaling capabilities by increasing the image tokenizer vocabulary size and the corresponding transformer size.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/FoundationVision/Infinity"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Towards Universal Soccer Video Understanding",
        "authors": "Yanfeng Wang, Ya Zhang, Hao Jiang, haoningwu, Homie0609",
        "link": "https://arxiv.org/abs/2412.01820",
        "github_repo": null,
        "summary": "- This paper introduces MatchVision, a novel visual-language foundation model designed for comprehensive soccer video understanding.\n- The model leverages a spatiotemporal attention mechanism inspired by TimeSformer, which allows it to effectively capture dynamic information within soccer videos.\n- MatchVision is trained on SoccerReplay-1988, a new dataset containing 1988 full soccer matches with rich annotations, alongside existing datasets like SoccerNet, that is significantly larger and more diverse.\n- The paper benchmarks MatchVision on various downstream tasks such as event classification, commentary generation, and foul recognition.\n- Experimental results demonstrate state-of-the-art performance across multiple benchmarks, highlighting the model's effectiveness and the value of the new dataset.",
        "classification": [
            "Computer Vision",
            "Video Classification",
            "Text Generation",
            "Multimodal"
        ],
        "github_urls": [
            "https://jyrao.github.io/UniSoccer/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing",
        "authors": "Juncheng Li, Xiangtai Li, Ling Yang, WeiChow, BryanW",
        "link": "https://arxiv.org/abs/2412.04280",
        "github_repo": "https://github.com/viiika/HumanEdit",
        "summary": "- This paper introduces HumanEdit, a new high-quality dataset for instruction-guided image editing created with extensive human input to improve alignment with user preferences, addressing a gap in current datasets that rely heavily on automatic generation.\n- HumanEdit consists of 5,751 high-resolution image pairs, each with an editing instruction, description, and mask, covering six instruction types (Action, Add, Counting, Relation, Remove, and Replace).\n- A four-stage annotation pipeline involving tutorials, image selection, instruction/edit generation, and administrator review with human feedback ensures data quality.\n- HumanEdit supports both masked and mask-free editing and has superior diversity and image resolution compared to existing datasets.\n- Benchmark results on HumanEdit across various baselines reveal that while methods generally perform well on Add and Remove tasks, improvements are needed for more complex instructions and consistency in aligning with human preferences.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/viiika/HumanEdit"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/BryanW/HumanEdit"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "Personalized Multimodal Large Language Models: A Survey",
        "authors": "Zhehao Zhang, Yu Xia, Hanjia Lyu, Junda Wu, Franck-Dernoncourt",
        "link": "https://arxiv.org/abs/2412.02142",
        "github_repo": null,
        "summary": "- This paper presents a comprehensive survey of personalized multimodal large language models (MLLMs), examining their architectures, training methods, and applications.\n- The authors propose a taxonomy for categorizing techniques used to personalize MLLMs, focusing on instruction, alignment, generation, and fine-tuning methods.\n- The survey also summarizes applications of personalized MLLMs, including text generation, image generation, recommendation, and retrieval tasks.\n- They also present an overview of commonly used datasets and evaluation metrics for personalized MLLMs.\n- Finally, the authors highlight key open challenges in the field, including benchmarking, evaluation metrics, diverse modalities, modality fusion, and theoretical foundations.",
        "classification": [
            "Multimodal",
            "Text Generation",
            "Image-to-Text",
            "Image-to-Image",
            "Text2Text Generation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality",
        "authors": "Hong Zhou, Shaoxuan He, Yuanyu He, Feng Chen, Yefei He",
        "link": "https://arxiv.org/abs/2412.04062",
        "github_repo": null,
        "summary": "- ZipAR is a training-free, plug-and-play parallel decoding framework designed to accelerate auto-regressive visual generation by exploiting the spatial locality in images and decoding spatially adjacent tokens in parallel.\n- Unlike traditional next-token prediction, which processes tokens sequentially, ZipAR introduces \"next-set prediction\" to decode tokens from different rows concurrently based on a local window size that defines the spatial adjacency.\n- This reduces the number of forward passes required, significantly improving generation efficiency without needing additional training or model modifications.\n- Experiments demonstrate up to a 91% reduction in forward passes on Emu3-Gen while maintaining image quality.\n- ZipAR is compatible with various auto-regressive visual generation models, including LlamaGen and Lumina-mGPT, showing consistent improvements across different models and tasks.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities",
        "authors": "Yanfeng Wang, Weidi Xie, Ya Zhang, Ziheng Zhao, haoningwu",
        "link": "https://arxiv.org/abs/2412.04106",
        "github_repo": null,
        "summary": "- MRGen, a diffusion-based data engine, synthesizes medical images for unannotated modalities, enabling training of segmentation models on these modalities. \n- MRGen leverages MedGen-1M, a large-scale curated dataset containing CT and MRI images with modality labels, attributes, regions, organ information, and a subset of masks. \n- MRGen is trained in two stages: text-guided pretraining on MedGen-1M and mask-conditioned fine-tuning on a subset with masks.\n- MRGen takes text prompts and organ masks to generate synthetic medical images for segmentation training.\n- Experimental results demonstrate MRGen effectively generates high-quality MR images and boosts segmentation performance on unannotated modalities, outperforming existing image generation techniques like CycleGAN and DualNorm.",
        "classification": [
            "Image Segmentation",
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
        "authors": "Jian Gang Ngui, David I. Adelani, Cl\u00e9mentine Fourrier, Angelika Romanou, Shivalika Singh",
        "link": "https://arxiv.org/abs/2412.03304",
        "github_repo": null,
        "summary": "- This paper introduces Global-MMLU, a 42-language multilingual multi-domain question-answering dataset designed to address cultural and linguistic biases in MMLU.\n- It includes culturally sensitive (CS) and culturally agnostic (CA) subsets, allowing for more nuanced evaluations.\n- The creation involved professional and community annotators for translation and post-editing, expanding language coverage and improving translation quality.\n- The paper also quantifies the impact of cultural biases, with analysis revealing that 28% of MMLU questions require culturally specific knowledge and a disproportionate focus on Western cultures.\n- State-of-the-art model evaluations on Global-MMLU highlight the impact of these biases on performance rankings, advocating for reporting performance on CS and CA subsets separately to provide a more comprehensive understanding of model capabilities across cultures.",
        "classification": [
            "Question Answering",
            "Translation",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/datasets/CohereForAI/Global-MMLU"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "Monet: Mixture of Monosemantic Experts for Transformers",
        "authors": "Jaewoo Kang, Kee-Eung Kim, Young Jin Ahn, affjljoo3581",
        "link": "https://arxiv.org/abs/2412.04139",
        "github_repo": "https://github.com/dmis-lab/Monet",
        "summary": " - This paper introduces MONET, a novel Mixture-of-Experts (MoE) architecture designed to enhance the mechanistic interpretability of large language models (LLMs) by addressing the issue of polysemanticity.\n- MONET incorporates sparse dictionary learning directly into end-to-end MoE pretraining, enabling the scaling of the expert count to 262,144 per layer while maintaining parameter efficiency.\n- The model's performance is evaluated across various benchmarks, showing competitive results with dense LLMs while offering superior knowledge manipulation capabilities.\n- Through qualitative and quantitative analyses, MONET demonstrates mutual exclusivity of knowledge across experts, enabling robust knowledge manipulation without performance degradation.\n- The code and pretrained checkpoints for MONET are publicly available.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/dmis-lab/Monet"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
        "authors": "Yusuke Kato, Zichun Liao, Akash Gokul, Konstantinos Kallidromitis, Shufan Li",
        "link": "https://arxiv.org/abs/2412.01169",
        "github_repo": "https://github.com/jacklishufan/OmniFlows",
        "summary": "- OmniFlow is a novel generative model designed for any-to-any generation tasks, using a modular architecture inspired by Stable Diffusion 3's MMDiT.\n- It extends the rectified flow (RF) framework to handle multiple modalities (text, image, audio) jointly and introduces a novel guidance mechanism for flexible control over modality interaction in generated outputs.\n- OmniFlow outperforms previous any-to-any models on various tasks, achieving competitive performance with state-of-the-art specialist models in text-to-image and text-to-audio generation.\n- Its modular design allows individual component pretraining and merging with pretrained single-task models, reducing training resource requirements compared to training from scratch.\n- Evaluations show significant improvements over existing any-to-any models in image quality, text-image alignment, and CLIP scores, particularly on the GenEval benchmark.",
        "classification": [
            "Any-to-Any",
            "Multimodal",
            "Text-to-Image",
            "Text-to-Audio"
        ],
        "github_urls": [
            "https://github.com/jacklishufan/OmniFlows"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Discriminative Fine-tuning of LVLMs",
        "authors": "Ioannis Maniadis Metaxas, Anestis Zaganidis, Alexandros Xenos, Adrian Bulat, Yassine Ouali",
        "link": "https://arxiv.org/abs/2412.04378",
        "github_repo": null,
        "summary": "- This paper introduces VladVA, a novel training approach for discriminative fine-tuning of Large Vision-Language Models (LVLMs).\n- VladVA converts a generative LVLM into a discriminative one by employing both contrastive and next-token prediction losses on image-text pairs with varying lengths and granularities.\n- The approach uses a parameter-efficient adaptation method that involves soft prompting and LoRA, thereby allowing for effective training on smaller datasets with limited compute.\n- On standard image-text retrieval benchmarks, VladVA shows significant improvement, achieving gains from +4.7% to +7.0% in absolute terms over similarly sized state-of-the-art CLIP-like models.\n- Additionally, the model demonstrates notable gains in compositionality tasks, showing improved language understanding over the standard two-tower image-text models.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Text-to-Image",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "KV Shifting Attention Enhances Language Modeling",
        "authors": "Weipeng Chen, Bingning Wang, Wei Cheng, xumingyu16",
        "link": "https://arxiv.org/abs/2411.19574",
        "github_repo": null,
        "summary": "- This paper introduces KV shifting attention, a novel attention mechanism designed to improve the efficiency of induction heads in large language models (LLMs).\n- KV shifting attention decouples keys and values in the attention mechanism, reducing the depth and width requirements for induction heads, enabling single-layer transformers to perform induction tasks effectively.\n- Theoretical analysis and empirical validation demonstrate that KV Shifting attention achieves comparable or superior performance to conventional multi-layer transformers in language modeling tasks.\n- The authors apply KV shifting attention to large language pre-training models with up to 19B parameters and show improved performance and faster convergence compared to baseline models using standard attention mechanisms.\n- KV shifting attention introduces a bias towards learning induction, which is beneficial for language modeling across diverse model scales.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/xumingyu16/Baseline_2.9B",
            "https://huggingface.co/xumingyu16/KV_shifting_2.9B"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models",
        "authors": "Zhichao Liao, Fulong Ye, Pengze Zhang, Qichao Sun, Crayon-Shinchan",
        "link": "https://arxiv.org/abs/2412.04146",
        "github_repo": null,
        "summary": "- AnyDressing is a novel multi-garment virtual dressing method that generates customized character images based on any combination of garments and personalized text prompts.\n- The model architecture consists of two primary networks: GarmentsNet, which extracts detailed clothing features using a Garment-Specific Feature Extractor, and DressingNet, which integrates these features for virtual dressing using a Dressing-Attention module and an Instance-Level Garment Localization Learning strategy.\n- A Garment-Enhanced Texture Learning strategy is also incorporated to improve fine-grained texture details of garments in synthesized images.\n- AnyDressing outperforms existing methods by demonstrating superior consistency in clothing style and texture, better text fidelity, and effective handling of background contamination and garment confusion in multi-garment dressing scenarios.\n- The model can also be used as a plugin, compatible with other extensions such as ControlNet, IP-Adapter, and LoRA, enhancing its versatility in diverse applications.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/crayon-shinchan/AnyDressing"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "authors": "Yangzhou Liu, Yue Cao, Zhe Chen, qishisuren, Weiyun1025",
        "link": "https://arxiv.org/abs/2412.05271",
        "github_repo": null,
        "summary": "- This paper introduces InternVL 2.5, a series of advanced multimodal large language models (MLLMs) built upon InternVL 2.0, enhancing training and testing strategies and data quality.\n- InternVL 2.5 maintains the core \"ViT-MLP-LLM\" architecture, integrating an incrementally pre-trained InternViT-6B or InternViT-300M vision encoder with various large language models (LLMs) like InternLM 2.5 and Qwen 2.5.\n- The models demonstrate competitive performance, rivaling leading commercial models like GPT-40 and Claude-3.5-Sonnet, and achieving state-of-the-art results on benchmarks like MMMU and MathVista.\n- Key findings include the reduced dependency on training data with larger vision encoders, the impact of improved data quality, and the benefits of test-time scaling, especially with Chain-of-Thought (CoT) reasoning.\n- InternVL 2.5 is released open-source, aiming to push the boundaries of open-source multimodal models and facilitate further research in multimodal AI.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text",
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/OpenGVLab/InternVL"
        ],
        "huggingface_urls": [
            "//huggingface.co/OpenGVLab/InternVL2_5-78B",
            "https://huggingface.co/spaces/OpenGVLab/InternVL"
        ],
        "date": "2024-12-09"
    },
    {
        "title": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment",
        "authors": "Cheng Jin, Xiaomeng Yang, Junyan Wang, Zhiyu Tan, Yibin Wang",
        "link": "https://arxiv.org/abs/2412.04814",
        "github_repo": null,
        "summary": "- This paper introduces LiFT, a novel fine-tuning method that leverages human feedback to better align text-to-video (T2V) models with human preferences.\n- The approach involves collecting human feedback on generated videos across three dimensions: semantic consistency, motion smoothness, and video fidelity.  A reward model (LIFT-CRITIC), based on a large multimodal model, is trained on this data to learn a reward function that predicts human preferences.\n- The T2V model is then fine-tuned using reward-weighted likelihood maximization to align its output with human expectations.\n- In experiments, the fine-tuned CogVideoX-2B model outperformed the larger CogVideoX-5B model across all 16 metrics of the VBench benchmark.\n- The results demonstrate the potential of using human feedback and reward learning to significantly improve the quality and alignment of T2V generation.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-09"
    },
    {
        "title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale",
        "authors": "Yuelin Bai, Tuney Zheng, Jarvis Guo, yuexiang96, luodian",
        "link": "https://arxiv.org/abs/2412.05237",
        "github_repo": null,
        "summary": "The paper introduces MAmmoTH-VL, a multimodal large language model (MLLM) trained on a newly created dataset containing 12 million instruction-response pairs.  The dataset was constructed using a cost-effective method employing open-source models.  MAmmoTH-VL-8B, based on the LLaVA-OneVision architecture, outperforms existing open-source models on various benchmarks, particularly those involving intricate reasoning. The model shows state-of-the-art performance on MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Ablation studies reveal the effectiveness of key components such as rewriting and self-filtering.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://mammoth-vl.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-09"
    },
    {
        "title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
        "authors": "Kyunghoon Bae, Soyoung An, LG AI Research, lhg912, Sunkyoung",
        "link": "https://arxiv.org/abs/2412.04862",
        "github_repo": null,
        "summary": "The paper introduces EXAONE 3.5, a series of instruction-tuned language models available in three sizes (32B, 7.8B, and 2.4B).  The models demonstrate strong instruction-following capabilities and achieve high performance in various benchmarks, particularly in real-world scenarios and long-context understanding.  EXAONE 3.5 models outperform many similar-sized models on benchmark datasets. The models are open for research purposes and can be downloaded from HuggingFace.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/LGAI-EXAONE"
        ],
        "date": "2024-12-09"
    },
    {
        "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
        "authors": "Mingyu Ding, Yixiao Ge, Yizhuo Li, Yuying Ge, Yi Chen",
        "link": "https://arxiv.org/abs/2412.04445",
        "github_repo": null,
        "summary": "- Moto is a new robotics model that leverages latent motion tokens as a bridging language for autoregressive pre-training on video data and robot manipulation.\n- Moto-GPT is pre-trained through next motion token prediction, learning motion-related prior knowledge from videos and transferring this knowledge to downstream tasks via fine-tuning.\n- Moto-GPT is a transformer-based architecture (similar to GPT) with a motion tokenizer that encodes motion between frames and action query tokens for robot control prediction during fine-tuning.\n- Experimental results demonstrate that Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks compared to various baseline models.\n- Moto-GPT performs particularly well in low-resource scenarios, highlighting its effective transfer of learned knowledge from video data to downstream tasks.",
        "classification": [
            "Robotics",
            "Multimodal",
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://chenyi99.github.io/moto/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-09"
    },
    {
        "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
        "authors": "Sem Park, Xi Liu, Wenyan Cong, Hanqing Zhu, Kyriection",
        "link": "https://arxiv.org/abs/2412.05270",
        "github_repo": null,
        "summary": "- APOLLO, a new memory-efficient optimizer for Large Language Models (LLMs), is proposed, offering SGD-level memory cost while maintaining or exceeding AdamW's performance.\n- It leverages structured learning rate updates (channel-wise or tensor-wise) and approximates them in a low-rank auxiliary space using random projections, eliminating the need for costly SVD operations.\n- APOLLO-Mini, an extremely memory-efficient variant, utilizes tensor-wise scaling with a rank-1 auxiliary space, achieving similar performance to AdamW with drastically reduced memory usage.\n- Experimental results on various LLaMA model sizes (60M to 7B) show APOLLO consistently outperforms AdamW and other memory-efficient methods in pre-training, even achieving a 2.8 reduction in validation perplexity with significantly lower memory overhead.\n- APOLLO also offers practical system-level advantages including enhanced throughput (3x on LLaMA 7B compared to AdamW) and improved model scalability, enabling LLaMA-13B pre-training with naive DDP on a single A100-80GB GPU and LLaMA-7B training on a single GPU with less than 12GB memory when combined with quantization.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-09"
    },
    {
        "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
        "authors": "Cuong Pham, Anh Tran, Khoi Nguyen, Quang Nguyen, Tung11",
        "link": "https://arxiv.org/abs/2412.04301",
        "github_repo": null,
        "summary": "- SwiftEdit is a novel, real-time text-guided image editing tool that leverages a one-step diffusion model and a one-step inversion framework, achieving edits in 0.23 seconds on an A100 GPU.\n- It introduces a two-stage training strategy for the inversion network: the first stage uses synthetic data generated by a one-step text-to-image model (SwiftBrushv2), while the second stage utilizes real images with perceptual and regularization losses to bridge the domain gap and ensure editability.\n- The method employs an attention rescaling technique with a self-guided editing mask, allowing flexible control over edit strength and precise localization.\n- Extensive quantitative and qualitative evaluations on the PieBench benchmark show that SwiftEdit significantly outperforms existing multi-step methods in terms of speed (at least 50x faster) while maintaining competitive editing quality and outperforms few-step editing approaches in terms of edit quality while maintaining a speed advantage (at least 5x faster).\n- A user study further demonstrates SwiftEdit's preference over other methods due to its balance of background preservation and accurate semantic edits.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-09"
    },
    {
        "title": "GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration",
        "authors": "Yu Wang, Xuefei Ning, Yukun Huang, fjxmlzn, NinaKarine",
        "link": "https://arxiv.org/abs/2412.04440",
        "github_repo": null,
        "summary": "- GENMAC, an iterative multi-agent framework, is proposed for compositional text-to-video generation.\n- It uses a three-stage collaborative workflow: DESIGN, GENERATION, and REDESIGN, with an iterative loop between GENERATION and REDESIGN for refinement.\n- The REDESIGN stage uses four sequentially executed MLLM-based agents: verification, suggestion, correction, and output structuring.\n- A self-routing mechanism adaptively selects the appropriate correction agent from a collection of specialized agents.\n- Experiments on T2V-CompBench show state-of-the-art performance across seven compositional aspects, significantly outperforming 17 existing methods, with notable improvement in generative numeracy.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://karine-h.github.io/GenMAC/"
        ],
        "date": "2024-12-09"
    },
    {
        "title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
        "authors": "Xiansong Lai, Haodong Xiang, Crayon-Shinchan, ChaosLiao, Valentina-Zhang",
        "link": "https://arxiv.org/abs/2412.03428",
        "github_repo": null,
        "summary": "- This paper introduces 2DGS-Room, a novel method for high-fidelity indoor scene reconstruction using seed-guided 2D Gaussian splatting.\n- The model incorporates a seed-guided mechanism to control the distribution and density of 2D Gaussians, improving geometric accuracy and efficiency.\n- It utilizes monocular depth and normal priors to provide geometric constraints for details and textureless regions, respectively.\n- Multi-view consistency constraints are employed to further enhance reconstruction quality and mitigate artifacts.\n- Extensive experiments demonstrate that 2DGS-Room achieves state-of-the-art performance in indoor scene reconstruction on ScanNet and ScanNet++ datasets.",
        "classification": [
            "Image-to-3D"
        ],
        "github_urls": [
            "https://valentina-zhang.github.io/2DGS-Room/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-09"
    },
    {
        "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling",
        "authors": "Haiyang Yu, Nan Xu, Kun Chen, Xinghua Zhang, iiiiwis",
        "link": "https://arxiv.org/abs/2412.04905",
        "github_repo": "https://github.com/MozerWang/DEMO",
        "summary": "- This paper introduces Dialogue Element Modeling (DEMO), a new research task focusing on two core competencies: Element Awareness and Dialogue Agent Interaction.\n- Element Awareness involves reverse-engineering dialogue elements like goal, persona, and scene, while Dialogue Agent Interaction focuses on goal-directed multi-turn dialogue modeling.\n- A novel benchmark, DEMO, is proposed to facilitate comprehensive dialogue modeling and assessment in both English and Chinese, covering various dialogue elements and tasks.\n- A DEMO agent, trained using imitation learning and expert experience, demonstrates superior performance in both in-domain and out-of-domain tasks, exceeding several existing LLMs.\n- Experimental results show that current LLMs have room for improvement in dialogue element modeling, especially in feature perception tasks.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/MozerWang/DEMO"
        ],
        "huggingface_urls": [],
        "date": "2024-12-09"
    },
    {
        "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
        "authors": "Keming Lu, Beichen Zhang, Zhenru Zhang, RunjiLin, chujiezheng",
        "link": "https://arxiv.org/abs/2412.06559",
        "github_repo": null,
        "summary": "- This paper introduces ProcessBench, a new benchmark for evaluating the ability of language models to identify erroneous steps in mathematical reasoning.\n- ProcessBench consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems, with each test case containing a step-by-step solution annotated with error locations by human experts.\n- Through extensive evaluation on ProcessBench, the authors found that existing Process Reward Models (PRMs) typically fail to generalize to more challenging math problems and underperform compared to critic models (prompted general language models).\n- The best open-source model, QwQ-32B-Preview, demonstrates competitive critique capability with the proprietary model GPT-40, but still lags behind the reasoning-specialized o1-mini.\n- This work aims to foster future research in reasoning process assessment and pave the way toward scalable oversight of language models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/QwenLM/ProcessBench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-10"
    },
    {
        "title": "Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models",
        "authors": "Wanxiang Che, Libo Qin, Yuxi Xie, Tianhao Niu, LooperXX",
        "link": "https://arxiv.org/abs/2412.05939",
        "github_repo": "https://github.com/LooperXX/MMGiC",
        "summary": "- This paper introduces MMGIC, a multimodal dataset featuring multi-grained concept annotations for Multimodal Large Language Models (MLLMs), including coarse-grained image captions, fine-grained object labels and regions, and label descriptions.\n- The authors propose a general MLLM framework and structured template to integrate these multi-grained annotations, facilitating vision-language alignment across different granularities.\n- Experiments demonstrate that MMGIC enhances MLLM performance in comprehension and generation tasks compared to training solely on image captions.\n- MMGIC and image-caption data complement each other; a curriculum learning strategy of pre-training on image captions and then MMGIC yields the best results, with gains of 3.95% and 2.34% on POPE and SEED-Bench, respectively. \n- The study explores various data recipes for multi-grained annotations and their impact on MLLM performance, demonstrating their complementary nature.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/LooperXX/MMGiC"
        ],
        "huggingface_urls": [],
        "date": "2024-12-10"
    },
    {
        "title": "Training Large Language Models to Reason in a Continuous Latent Space",
        "authors": "Zhiting Hu, Xian Li, DiJia Su, Sainbayar Sukhbaatar, Shibo Hao",
        "link": "https://arxiv.org/abs/2412.06769",
        "github_repo": null,
        "summary": "- This paper introduces COCONUT (Chain of Continuous Thought), a novel paradigm for training large language models (LLMs) to reason in a continuous latent space, rather than the traditional language space used in chain-of-thought (CoT) prompting.\n- COCONUT utilizes the last hidden state of the LLM as a continuous representation of the reasoning state (\"continuous thought\") and feeds it directly back to the LLM as the next input embedding.\n- This latent reasoning approach allows the model to encode multiple potential next reasoning steps, enabling a breadth-first search (BFS) behavior and improved performance on logical reasoning tasks requiring backtracking compared to traditional CoT.\n- Experimental results on GSM8k, ProntoQA, and a newly proposed ProsQA dataset demonstrate the effectiveness of COCONUT, particularly in scenarios involving substantial planning and search.\n- The findings suggest that latent reasoning can be more efficient and adaptable for complex reasoning, offering insights into future research on LLM reasoning and problem-solving.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-10"
    },
    {
        "title": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation",
        "authors": "Ying Shan, Yixiao Ge, Yizhuo Li, Yuying Ge",
        "link": "https://arxiv.org/abs/2412.04432",
        "github_repo": "https://github.com/TencentARC/Divot",
        "summary": "- This paper introduces Divot, a Diffusion-Powered Video Tokenizer, which uses a diffusion process for self-supervised video representation learning.\n- Divot is composed of a pre-trained Vision Transformer (ViT) encoder, a Spatial-Temporal transformer, and a Perceiver Resampler to get video representations.\n- It leverages a video diffusion model to predict the noise added to the VAE latents of video frames, conditioned on Divot\u2019s features.\n- The authors also introduce Divot-LLM, integrating Divot with a pre-trained LLM, which achieves competitive performance in video comprehension and zero-shot video generation benchmarks.\n- Divot-LLM also excels in video storytelling, generating interleaved narratives and corresponding videos.",
        "classification": [
            "Text-to-Video",
            "Video-Text-to-Text",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/TencentARC/Divot"
        ],
        "huggingface_urls": [],
        "date": "2024-12-10"
    },
    {
        "title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale",
        "authors": "Tiejun Huang, Zhengxiong Luo, Haoge Deng, Infinite888, bruiiii",
        "link": "https://arxiv.org/abs/2412.06699",
        "github_repo": null,
        "summary": "- See3D, a visual-conditional multi-view diffusion model, is introduced for open-world 3D creation trained on a large-scale dataset of internet videos called WebVi3D containing 320 million frames from 16 million video clips.\n- This model uses a novel visual-condition, a 2D-inductive signal from time-dependent noise added to masked videos, eliminating the need for pose annotations, thereby enabling large-scale training.\n- A warping-based pipeline is used in conjunction with See3D to achieve high-fidelity 3D generation by iteratively refining the geometry of novel views.\n- See3D achieves state-of-the-art zero-shot and open-world single and sparse view reconstruction, outperforming models trained on constrained 3D datasets.\n- The model also supports image-conditioned 3D creation tasks such as 3D editing without requiring fine-tuning.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-10"
    },
    {
        "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers",
        "authors": "Hang Li, Yang Liu, Yuanshun Yao, Jinghan Jia, xiaojunxu",
        "link": "https://arxiv.org/abs/2412.03123",
        "github_repo": "https://github.com/xiaojunxu/multi-bit-text-watermark",
        "summary": "- This paper introduces a novel method for embedding imperceptible multi-bit watermarks into text using LLM-based paraphrasers.\n- The method uses a pair of fine-tuned LLMs, one as the encoder to embed the watermark and another as the decoder to extract it. The encoder injects the watermark by alternatively paraphrasing sentences based on a binary code, while the decoder classifies each sentence to extract the embedded bits.\n- The method achieves high detection accuracy (over 99.99% AUC) with small LLMs (1.1B parameters) while maintaining the semantic similarity between the original and watermarked texts.\n- It also demonstrates robustness against word substitutions and sentence paraphrasing perturbations and generalizes well to out-of-distribution data.\n- The watermark's stealthiness is evaluated through both human and LLM-based analysis, showing it is difficult to distinguish between watermarked and non-watermarked text.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/xiaojunxu/multi-bit-text-watermark"
        ],
        "huggingface_urls": [],
        "date": "2024-12-10"
    },
    {
        "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
        "authors": "Mingyang Sun, Siteng Huang, Shangke Lyu, Pengxiang Ding, Zhefei Gong",
        "link": "https://arxiv.org/abs/2412.06782",
        "github_repo": null,
        "summary": "- CARP (Coarse-to-Fine AutoRegressive Policy) is introduced for visuomotor policy learning; it combines autoregressive model efficiency with diffusion model performance. \n- CARP is a transformer-based model that uses multi-scale action tokenization; a coarse-to-fine autoregressive prediction refines actions, and it is trained using cross-entropy loss with relaxed markovian assumptions.\n- In simulated robotics benchmarks with single and multi-task settings, CARP matches or exceeds diffusion model performance with 10x faster inference.\n- On real-world robotic arm manipulation tasks, CARP demonstrates a 10% improvement in success rates over baseline with faster inference.\n- The flexibility of the GPT-style architecture allows CARP to easily extend to multi-task settings by just adding a task embedding.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-10"
    },
    {
        "title": "Evaluating and Aligning CodeLLMs on Human Preference",
        "authors": "JustinLin610, huybery, misakamage, instro, jx-yang",
        "link": "https://arxiv.org/abs/2412.05210",
        "github_repo": null,
        "summary": "- This paper introduces CodeArena, a new human-curated benchmark for evaluating code large language models (LLMs) based on human preference, addressing the gap between code correctness and user satisfaction.\n- CodeArena contains 397 diverse samples across 40 coding categories and 44 programming languages, focusing on real-world coding scenarios.\n- In addition, the authors create SynCode-Instruct, a large-scale (20B token) synthetic instruction dataset generated by scaling instructions from web sources and generating corresponding code snippets with unit tests where applicable. Fine-tuning Qwen2.5-Coder using SynCode-Instruct leads to state-of-the-art performance for open-source code LLMs. \n- Evaluations of 40+ LLMs on CodeArena show significant differences compared to execution-based benchmarks and a large performance gap between open-source and closed-source LLMs.\n- This work also shows that fine-tuning with larger synthetic instruction data sets improves code generation and benchmark performance in code LLMs.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://codearenaeval.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation",
        "authors": "Chao Tang, LXT, zengyh1900, JingboWang, jianzongwu",
        "link": "https://arxiv.org/abs/2412.07589",
        "github_repo": null,
        "summary": "- DiffSensei, a novel framework for generating customized manga panels, bridges Multi-Modal Large Language Models (MLLMs) with diffusion-based image generators, enabling dynamic multi-character control and narrative consistency.\n- The framework uses an MLLM as a text-compatible identity adapter, allowing character features to adjust based on panel captions and ensuring layout control through masked cross-attention injection and dialog embedding.\n- A new large-scale dataset, MangaZero, containing 43,264 manga pages and 427,147 annotated panels, supports the training and evaluation of models on customized manga generation.\n- Experimental results show DiffSensei outperforms existing story visualization models in character consistency, layout controllability, and text adherence, marking a significant advancement in manga generation by enabling text-adaptable character customization.\n- A human preference study further validates DiffSensei\u2019s superior performance, highlighting its capability to create coherent and expressive manga panels.",
        "classification": [
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "STIV: Scalable Text and Image Conditioned Video Generation",
        "authors": "jefflai, JesseAllardice, tsujuifu, wenzehu, Jiasenlu",
        "link": "https://arxiv.org/abs/2412.07730",
        "github_repo": null,
        "summary": "- STIV (Scalable Text and Image Conditioned Video Generation) is a new text- and image-conditioned video generation model based on a Diffusion Transformer (DiT) architecture, integrating image conditions via frame replacement and text conditions through a joint image-text conditional classifier-free guidance.\n- This design enables STIV to handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks concurrently, and it's adaptable to various applications like video prediction and frame interpolation.\n- The 8.7B parameter STIV model achieves state-of-the-art performance on VBench benchmarks, scoring 83.1 on T2V and 90.1 on TI2V at 512x512 resolution, outperforming existing models like CogVideoX-5B, Pika, Kling, and Gen-3.\n- A progressive training approach involving text-to-image and text-to-video stages, alongside techniques like QK-norm, sandwich-norm, and MaskDiT, enhances stability and efficiency during model scaling.\n- The model utilizes flow matching as a training objective and incorporates rotary positional embeddings, micro-conditions, and a novel joint image-text classifier-free guidance strategy to improve performance and address motion-related issues.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Hidden in the Noise: Two-Stage Robust Watermarking for Images",
        "authors": "Niv Cohen, chegde, rtealwitter, penfever, kasraarabi",
        "link": "https://arxiv.org/abs/2412.04653",
        "github_repo": null,
        "summary": "- This paper introduces WIND, a two-stage robust watermarking method for images generated by diffusion models.\n- WIND leverages the initial noise of diffusion models as a distortion-free watermark and augments it with generated Fourier patterns to embed group identifiers, enabling efficient detection.\n- The two-stage framework reduces the search space during detection, increasing efficiency while maintaining robustness.\n- Experimental results demonstrate that WIND achieves state-of-the-art robustness to forgery and removal attacks compared to existing methods like Tree-Ring and RingID.\n- Additionally, the paper outlines a strategy to apply the watermarking method to non-synthetic images using diffusion inpainting.",
        "classification": [
            "Computer Vision",
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics",
        "authors": "Yuqian Zhou, He Zhang, Zhifei Zhang, jimmie33, xichenhku",
        "link": "https://arxiv.org/abs/2412.07774",
        "github_repo": null,
        "summary": "- UniReal is a unified framework based on a diffusion transformer model, designed for various image generation and editing tasks using a \"discontinuous\" frame generation approach, effectively treating diverse input and output images as pseudo video frames.\n- It leverages a hierarchical prompting scheme with context and image prompts to guide image generation and editing under a shared text prompt, enabling better coordination between tasks and reducing ambiguity from mixed data sources.\n- The model utilizes universal supervision from large-scale video data by treating pairs of discontinuous video frames with captions as instructive editing data, and automatically constructing data for other image tasks from video datasets. \n- UniReal surpasses state-of-the-art methods in various tasks such as instructive image editing and customized image generation, demonstrating its effectiveness in following instructions and maintaining image fidelity.\n- Evaluations on DreamBench, EMU Edit, and MagicBrush demonstrate UniReal\u2019s superior capabilities in detail preservation, alignment with user intent, and overall image quality across a wide range of image generation and editing tasks.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Image Segmentation",
            "Image-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
        "authors": "conghui, friskit, Liam-Liu, wanderkid, ouyanglinke",
        "link": "https://arxiv.org/abs/2412.07626",
        "github_repo": "https://github.com/opendatalab/OmniDocBench",
        "summary": "- OmniDocBench, a new multi-source benchmark for automated document content extraction, addresses limitations in existing methods regarding document diversity and comprehensive evaluation.\n- The benchmark includes a high-quality evaluation dataset with nine diverse document types and a flexible framework with 19 layout and 14 attribute labels for multi-level assessments.\n- An exhaustive comparison of modular pipelines and end-to-end methods highlights their limitations in handling document diversity.\n- Evaluations demonstrate that pipeline tools outperform general VLMs on common documents, while VLMs show better generalization on specialized data and robustness on pages with challenging attributes.\n- OmniDocBench sets a robust and fair standard, enabling fairer comparisons and guiding future development in document parsing.",
        "classification": [
            "Computer Vision",
            "Document Question Answering"
        ],
        "github_urls": [
            "https://github.com/opendatalab/OmniDocBench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models",
        "authors": "myownskyW7, guandao, Dubhe-zmc, justimyhxu, tongwu2020",
        "link": "https://arxiv.org/abs/2412.07674",
        "github_repo": null,
        "summary": "- This research introduces FiVA (Fine-grained Visual Attribute Dataset), a novel dataset designed to improve controllable image generation with text-to-image diffusion models by focusing on fine-grained visual attributes.\n- The FiVA dataset features a well-organized taxonomy of visual attributes and contains around 1 million generated images with corresponding attribute annotations.\n- FiVA-Adapter, a fine-grained visual attribute adaptation framework is introduced, which enables transferring visual attributes from multiple source images into a generated target image.\n- FiVA-Adapter includes a multimodal encoder (Q-former) to handle attribute tag instructions within the diffusion process, allowing for more precise control and customization of visual elements.\n- Experimental results demonstrate the superior performance of this method in extracting specific visual attributes, high textual alignment, and the capability of combining different attributes, as compared to baseline methods.",
        "classification": [
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/FiVA/FiVA"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
        "authors": "Dongping Chen, Ethan Shen, Cheng-Yu Hsieh, Zelun Luo, Mahtab Bigverdi",
        "link": "https://arxiv.org/abs/2412.03548",
        "github_repo": null,
        "summary": "- This research introduces Perception Tokens, intrinsic image representations designed to improve visual reasoning abilities in Multimodal Language Models (MLMs).\n- Perception tokens, such as depth maps and bounding boxes, are incorporated as intermediate reasoning steps in the MLM's chain-of-thought process using a novel training method called AURORA.\n- AURORA leverages a VQVAE to transform visual representations into tokens and trains the MLM in a multi-task framework with a curriculum learning strategy.\n- The proposed LLaVA-AURORA model demonstrates significant improvement over fine-tuning baselines on benchmarks including BLINK, CVBench, and SEED-Bench for depth estimation and object counting tasks.\n- Experimental results exhibit the effectiveness of Perception Tokens, resulting in a 6.4% boost for relative depth estimation on BLINK and up to 11.3% improvement on object counting across datasets.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Depth Estimation",
            "Object Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Video Motion Transfer with Diffusion Transformers",
        "authors": "Sergey Tulyakov, fabvio, philiptorr, aliaksandr-siarohin, alexpondaven",
        "link": "https://arxiv.org/abs/2412.07776",
        "github_repo": null,
        "summary": "- DiTFlow is a novel method for transferring motion from a reference video to a newly synthesized one specifically designed for Diffusion Transformers (DiTs).\n- It leverages the global attention mechanism of DiTs to extract motion patterns across video frames, which are represented as Attention Motion Flows (AMFs).\n- DiTFlow guides the latent denoising process of the DiT model by optimizing the latent representation of the video such that its AMF matches the AMF of the reference video. \n- The method also enables zero-shot motion transfer by optimizing positional embeddings within the DiT.\n- Experimental results on the DAVIS dataset using CogVideoX demonstrate that DiTFlow outperforms existing motion transfer methods across multiple metrics, including motion fidelity and image quality, and also in human evaluation.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/snap-research/DiTFlow"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Pondaven/DiTFlow"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "EMOv2: Pushing 5M Vision Model Frontier",
        "authors": "Zhucun Xue, Teng Hu, Jiangning Zhang, LXT, hhy724",
        "link": "https://arxiv.org/abs/2412.06674",
        "github_repo": "https://github.com/zhangzjn/EMOv2",
        "summary": "- EMOv2, a family of lightweight vision models, introduces an Improved Inverted Residual Mobile Block (i2RMB) and a parameter-sharing spanning attention mechanism for enhanced efficiency and performance in dense prediction tasks.\n- The i2RMB builds upon the Meta Mobile Block (MMBlock), a unified abstraction of Inverted Residual Blocks (IRBs), Multi-Head Self-Attention (MHSA), and Feed-Forward Networks (FFNs), simplifying architecture design.\n- EMOv2 achieves state-of-the-art accuracy on various tasks, including image recognition, dense prediction, and image generation, outperforming models with similar parameter counts.\n- EMOv2-5M achieves 79.4 Top-1 accuracy on ImageNet-1K, exceeding comparable CNN- and attention-based models, and a 41.5 mAP on object detection surpassing previous EMO-5M by +2.6. \n- With a robust training strategy, EMOv2-5M attains 82.9 Top-1 accuracy, establishing a new benchmark for 5M magnitude models.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Object Detection",
            "Image Segmentation",
            "Video Classification",
            "Unconditional Image Generation"
        ],
        "github_urls": [
            "https://github.com/zhangzjn/EMOv2"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Granite Guardian",
        "authors": "Tejaswini Pedapati, Subhajit Chaudhury, Manish Nagireddy, Inkit Padhi, Giandomenico",
        "link": "https://arxiv.org/abs/2412.07724",
        "github_repo": "https://github.com/ibm-granite/granite-guardian",
        "summary": "- This paper introduces Granite Guardian, a suite of safeguard models (2B and 8B parameter sizes) designed for comprehensive risk detection in Large Language Models, addressing prompt and response risks related to social biases, security (jailbreaking), and RAG-specific issues like context relevance, groundedness, and answer relevance.\n- The models are trained on a combined dataset of human-annotated data from diverse sources and synthetic data generated to specifically cover adversarial attacks and RAG hallucinations. \n- Evaluation on standard benchmarks like the OpenAI Moderation Evaluation Dataset, HarmBench, and ToxicChat show that Granite Guardian outperforms existing open-source models on key metrics like AUC, F1, and recall.\n- On RAG hallucination benchmarks (groundedness), Granite Guardian achieves an average AUC of 0.854 on the TRUE dataset and performs competitively with dedicated models explicitly trained for groundedness detection.\n- The models are released open-source to encourage community adoption and promote responsible development of safer LLM applications.",
        "classification": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [
            "https://github.com/ibm-granite/granite-guardian"
        ],
        "huggingface_urls": [
            "https://huggingface.co/ibm-granite/granite-guardian-hap-38m"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
        "authors": "Jianhua Han, Runhui Huang, Junwei Yang, Guansong Lu, Chunwei Wang",
        "link": "https://arxiv.org/abs/2412.06673",
        "github_repo": null,
        "summary": "- ILLUME, a unified multimodal large language model (MLLM), seamlessly integrates understanding and generation capabilities using a next-token prediction approach.\n- It incorporates a semantic vision tokenizer and a progressive multi-stage training to enhance data efficiency, requiring only 15M image-text pairs for pretraining.\n- ILLUME introduces a self-enhancing multimodal alignment scheme where the model assesses consistency between generated images and text descriptions for synergistic improvement.\n- Experiments show ILLUME competing with state-of-the-art unified and specialized MLLMs across visual understanding, generation, and editing.\n- It outperforms previous best models on several benchmarks by significant margins, like a 25% improvement on MMMU and 14% on SEED.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "ObjCtrl-2.5D: Training-free Object Control with Camera Poses",
        "authors": "Chen Change Loy, Shangchen Zhou, Yushi Lan, Zhouxia Wang",
        "link": "https://arxiv.org/abs/2412.07721",
        "github_repo": null,
        "summary": "- ObjCtrl-2.5D is a training-free object motion control method for text-to-video generation that leverages 3D trajectories derived from 2D trajectories and depth maps, converting them into camera poses to guide object motion.\n- The approach models object movement as camera motion to leverage existing Camera Motion Control T2V (CMC-T2V) models without requiring additional training. \n- A Layer Control Module isolates object and background layers to apply distinct camera poses, enabling object-specific motion control.\n- Shared Warping Latent enhances control precision by sharing low-frequency latent information across frames within the warped object regions.\n- ObjCtrl-2.5D outperforms existing training-free methods in control accuracy and supports complex object motion like rotation, showing potential for advanced control in 3D video generation, but its performance is still behind training-based methods.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation",
        "authors": "Menghan Xia, Sida Peng, Xintao Wang, Xian Liu, lemonaddie",
        "link": "https://arxiv.org/abs/2412.07759",
        "github_repo": null,
        "summary": "- 3DTrajMaster is a novel approach for manipulating multi-entity 3D motions in video generation using entity-specific 6DoF pose sequences as input, leveraging a plug-and-play 3D-motion grounded object injector.\n- The object injector fuses entity descriptions and trajectories into latent embeddings, which are then combined and fed into a gated self-attention layer for motion fusion, preserving the video diffusion prior and generalizing to diverse entities and trajectories.\n- A new 360\u00b0-Motion Dataset is constructed using UE rendering, correlating 3D human and animal assets with GPT-generated trajectories and capturing motion with 12 cameras, addressing the lack of suitable training data.\n- A domain adaptor and annealed sampling strategy mitigate video quality degradation during training and inference, respectively.\n- Experimental results demonstrate state-of-the-art accuracy and generalization for controlling multi-entity 3D motions, outperforming existing methods in pose accuracy and handling complex scenarios like 3D occlusions.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation",
        "authors": "Kazuhiro Fukui, Erica K. Shimomoto, Lincon S. Souza, Pedro H. V. Valois",
        "link": "https://arxiv.org/abs/2412.07334",
        "github_repo": "https://github.com/phvv-me/frame-representation-hypothesis.git",
        "summary": "- This paper introduces the Frame Representation Hypothesis (FRH), a new framework for interpreting and controlling Large Language Models (LLMs) by modeling multi-token words as frames, which are ordered sequences of independent vectors.\n- FRH extends the Linear Representation Hypothesis (LRH) which was limited to single-token words, and makes it applicable to multi-token words and thus any textual data.\n- The paper proposes Concept Frames, which are centroids of a set of word frames that share a common concept, and shows that over 99% of words composed of several tokens are composed of linearly independent token vectors.\n- A Top-k Concept-Guided Decoding method is introduced for steering text generation using chosen concepts, which helps to expose and potentially remediate biases present in LLMs.\n- Experiments on Llama 3.1, Gemma 2, and Phi 3 demonstrate FRH's ability to expose biases and steer text generation, showing its applicability to enhance the transparency and controllability of LLMs.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/phvv-me/frame-representation-hypothesis.git"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation",
        "authors": "Umberto Michieli, Pietro Zanuttigh, Mete Ozay, obohdal, donaldssh",
        "link": "https://arxiv.org/abs/2412.05148",
        "github_repo": null,
        "summary": "- LoRA.rar introduces a novel method for merging subject and style LoRAs for personalized image generation using a small (0.5M parameter) hypernetwork.\n- The hypernetwork is trained on a dataset of content and style LoRA pairs and predicts merging coefficients in real-time for new, unseen pairs, eliminating the need for computationally expensive optimization used by other methods like ZipLoRA.\n- A new evaluation metric, MARS2, based on Multimodal Large Language Models (MLLMs) is introduced to address the limitations of existing metrics for evaluating content-style fidelity.\n- LoRA.rar outperforms existing merging strategies and ZipLoRA in subject-style personalization as measured by MARS2 and human evaluations.\n- The method achieves a significant speedup of over 4000x in the merging process compared to optimization-based methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Fully Open Source Moxin-7B Technical Report",
        "authors": "Sung-En Chang, Yixin Shen, Zhenglun Kong, Xuan Shen, Pu Zhao",
        "link": "https://arxiv.org/abs/2412.06845",
        "github_repo": null,
        "summary": "- Moxin-7B is a fully open-source large language model (LLM) based on the Mistral architecture, extended to 36 blocks from 32, and trained on over 2 trillion tokens.\n- It incorporates grouped-query attention (GQA), sliding window attention (SWA), and a rolling buffer cache for efficient long-context handling (up to 32K tokens).\n- The model was trained in three phases: initial pre-training with 2k and 4k context lengths and a final capability enhancement phase using curated data from Hugging Face and evaluation benchmarks.\n- Evaluation on standard benchmarks like ARC, HellaSwag, MMLU, Winogrande, and PIQA demonstrates superior zero-shot performance against other 7B models and competitive results in few-shot settings.\n- The chat model variant, Moxin-7B-chat, outperforms baselines on MTBench, showcasing strong alignment capabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/moxin-org/Moxin-LLM"
        ],
        "huggingface_urls": [
            "https://huggingface.co/moxin-org/moxin-llm-7b",
            "https://huggingface.co/moxin-org/moxin-chat-7b"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation",
        "authors": "Felice Dell'Orletta, Marco Avvenuti, Amaury Trujillo, Alessio Miaschi, Lorenzo Cima",
        "link": "https://arxiv.org/abs/2412.07338",
        "github_repo": null,
        "summary": "- This paper proposes and evaluates strategies for generating contextualized and personalized counterspeech using a LLaMA2-13B model.\n- The model is instructed to generate counterspeech by leveraging contextual information such as community, conversation details, and user history.\n- The study finds that contextualized counterspeech significantly outperforms generic counterspeech in adequacy and persuasiveness.\n-  A mixed-design crowdsourcing experiment revealed a poor correlation between quantitative evaluation metrics and human evaluations. \n- This highlights the importance of human evaluation in assessing nuanced aspects of generated content like artificiality.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/dfurman/Llama-2-13B-Instruct-v0.2"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment",
        "authors": "Jitendra Malik, Masayoshi Tomizuka, Chenfeng Xu, Yilin Wu, Ran Tian",
        "link": "https://arxiv.org/abs/2412.04835",
        "github_repo": null,
        "summary": "- This paper introduces Representation-Aligned Preference-based Learning (RAPL), a method for efficiently aligning visuomotor robot policies with end-user preferences using minimal human feedback.\n- RAPL focuses on aligning the robot's visual representation with the human's, rather than relying on costly reinforcement learning from human feedback.\n- In simulation, RAPL achieves comparable performance to ground truth and outperforms baselines on both standard and cluttered manipulation tasks. \n- Hardware experiments show RAPL aligns a diffusion-based policy with 5x less real human preference data than traditional RLHF. \n- RAPL also demonstrates strong zero-shot generalization across different robot embodiments.",
        "classification": [
            "Robotics",
            "Reinforcement Learning",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Chimera: Improving Generalist Model with Domain-Specific Experts",
        "authors": "Renrui Zhang, Renqiu Xia, Hongbin Zhou, Mingsheng Li, Tianshuo Peng",
        "link": "https://arxiv.org/abs/2412.05983",
        "github_repo": null,
        "summary": "- Chimera, a new multimodal pipeline, enhances Large Multi-modal Models (LMMs) with domain-specific experts to improve performance on specialized tasks like multimodal reasoning and visual content extraction.\n- The model integrates multiple expert encoders into a single LMM using a progressive training strategy and a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism to address optimization imbalances.\n- Chimera achieves state-of-the-art performance on MathVista and MathVerse, outperforming comparable-scale LMMs and specialized expert models.\n- The model also excels in visual structural extraction tasks for charts, tables, and documents, achieving near-specialist-level results on benchmarks like ChartQA-SE, Table-SE, and Doc-SE.\n- The authors plan to open-source Chimera and the training datasets to facilitate future research on LMMs.",
        "classification": [
            "Multimodal",
            "Question Answering",
            "Table Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "A New Federated Learning Framework Against Gradient Inversion Attacks",
        "authors": "Weihong Ren, Xiaodan Zhang, Wenhao Chen, Shuang Zeng, gpx333",
        "link": "https://arxiv.org/abs/2412.07187",
        "github_repo": "https://github.com/Pengxin-Guo/HyperFL",
        "summary": "- This paper introduces HyperFL, a novel federated learning framework designed to protect against gradient inversion attacks (GIA) without relying on existing defense mechanisms like SMC, HE, or DP.\n- HyperFL employs a dual-pronged approach: network decomposition (splitting the model into a shared feature extractor and a private classifier) and hypernetwork sharing (using a hypernetwork to generate the feature extractor's parameters, with only the hypernetwork's parameters being shared).\n- Two configurations of HyperFL are presented: the main configuration for simpler tasks and HyperFL-LPM, which utilizes pre-trained models as fixed feature extractors and generates trainable adapter parameters via a hypernetwork for complex tasks.\n- Theoretical analysis demonstrates the convergence of HyperFL, and experimental results showcase its privacy-preserving capability and competitive performance compared to FedAvg.\n- HyperFL outperforms DP-based methods in terms of accuracy while providing comparable privacy protection, indicating a more favorable privacy-utility trade-off.",
        "classification": [
            "Computer Vision",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/Pengxin-Guo/HyperFL"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints",
        "authors": "lemonaddie, ziyangy, Xintao, menghanxia, jianhongbai",
        "link": "https://arxiv.org/abs/2412.07760",
        "github_repo": null,
        "summary": "- SynCamMaster is a novel text-to-video model that generates synchronized multi-view videos of dynamic, open-domain scenes from user-specified viewpoints.\n- The model architecture involves a pre-trained text-to-video diffusion model enhanced with a multi-view synchronization module and a camera encoder.\n- The multi-view synchronization module uses cross-view self-attention within the diffusion transformer blocks to ensure inter-view feature consistency.\n- A hybrid training dataset comprised of multi-view images, monocular videos, and Unreal Engine rendered videos is used to overcome data scarcity.\n- Quantitative and qualitative comparisons demonstrate SynCamMaster's superior performance over baseline methods in generating coherent, synchronized multi-view videos.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/KwaiVGI/SynCamMaster"
        ],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations",
        "authors": "MAJIARUI, SYZhang0805, yeezlee, mengcy, hyllbd",
        "link": "https://arxiv.org/abs/2412.08580",
        "github_repo": null,
        "summary": "- This paper introduces LAION-SG, a large-scale dataset with scene graph annotations for training complex image-text models.\n- LAION-SG is an enhancement of LAION-Aesthetics V2 (6.5+) with high-quality scene graph annotations by GPT-4, featuring multiple objects, detailed attributes, and relationships.\n- A new foundation model, SDXL-SG, based on Stable Diffusion XL, incorporates scene graph information through a graph neural network to improve complex scene generation.\n- Both quantitative and qualitative results show that models trained on LAION-SG significantly outperform those trained on existing datasets like COCO-Stuff and Visual Genome.\n- A new benchmark, CompSG-Bench, has been established to evaluate models on complex image generation, setting a new standard.",
        "classification": [
            "Text-to-Image",
            "Graph Machine Learning",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/mengcye/LAION-SG"
        ],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "POINTS1.5: Building a Vision-Language Model towards Real World Applications",
        "authors": "Xiao Zhou, Le Tian, yangyu1, kavio, YuanLiuuuuuu",
        "link": "https://arxiv.org/abs/2412.08443",
        "github_repo": null,
        "summary": "- POINTS1.5 is a new vision-language model based on the LLaVA architecture, which uses a pre-trained vision encoder, a randomly initialized projector, and a pre-trained large language model.\n- It incorporates a NaViT-style vision encoder that supports dynamic high resolution, eliminating the need to split images into tiles and improving performance on text-intensive tasks.\n- The model adds bilingual support (Chinese and English) and uses a refined chat template for pre-training, improving performance over its predecessor, POINTS1.0.\n- A rigorous filtering method is applied to visual instruction tuning datasets to remove samples with grammatical errors and questions answerable without images, further improving the quality of the training data.\n- POINTS1.5-7B achieves top ranking on the OpenCompass leaderboard among models under 10B parameters, outperforming models several times larger.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/WePOINTS/WePOINTS"
        ],
        "huggingface_urls": [
            "https://huggingface.co/WePOINTS/POINTS-1-5-Qwen-2-5-7B-Chat"
        ],
        "date": "2024-12-12"
    },
    {
        "title": "Learning Flow Fields in Attention for Controllable Person Image Generation",
        "authors": "AdityaPatel, Wall-dandelion, Yuren, shikunl, franciszzj",
        "link": "https://arxiv.org/abs/2412.08486",
        "github_repo": "https://github.com/franciszzj/Leffa",
        "summary": "- Leffa, a novel regularization loss for controllable person image generation, guides attention by learning flow fields, thereby reducing fine-grained detail distortion without increasing model parameters or inference costs.\n- Leffa transforms the attention map between target query and reference key into a flow field, warping the reference image to better align with the target, encouraging accurate attention to the correct reference key regions during training.\n- In virtual try-on and pose transfer experiments, Leffa achieves state-of-the-art performance across VITON-HD, DressCode, and DeepFashion datasets, surpassing existing methods in FID and KID metrics and preserving finer details, demonstrating its effectiveness in detail preservation.\n- Leffa's model-agnostic nature is validated through seamless integration with existing diffusion models (IDM-VTON and CatVTON), significantly boosting performance without added parameters, establishing it as a versatile solution for diverse diffusion-based models.\n- Human studies further confirm that Leffa generates significantly better quality images when comparing to the prior arts on tasks including virtual try-on and pose transfer.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/franciszzj/Leffa"
        ],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "StyleMaster: Stylize Your Video with Artistic Generation and Translation",
        "authors": "Huijuan Huang, whluo, qq8933, Xintao, zixuan-ye",
        "link": "https://arxiv.org/abs/2412.07744",
        "github_repo": null,
        "summary": "- StyleMaster is a novel video generation and translation model that leverages both global and local style representations, along with a motion adapter and a gray tile ControlNet, to achieve high-quality stylized video generation and accurate video style transfer.\n- The model architecture includes a contrastive learning strategy for training a global style extractor, local patch selection to capture texture details, a dual cross-attention mechanism for style injection, a motion adapter to enhance temporal and style quality, and a gray tile ControlNet for content guidance.\n- StyleMaster outperforms existing state-of-the-art methods, like VideoComposer and StyleCrafter, on several stylization tasks, including text-to-video generation and video/image style transfer, as demonstrated by both qualitative and quantitative results.\n- The quantitative results show significant improvements in text alignment, style similarity, visual and dynamic quality, and motion smoothness compared to competitors.\n- The model demonstrates enhanced robustness and ability to accurately capture and transfer diverse styles, while maintaining reasonable content preservation for effective style transfer.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Video Classification",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
        "authors": "JustinOh, LeeYG, lelady, xysun, stnamjef",
        "link": "https://arxiv.org/abs/2412.06234",
        "github_repo": null,
        "summary": "- This paper introduces Generative Densification (GD), a novel method to improve the quality of 3D reconstructions from feed-forward Gaussian models by selectively densifying Gaussian primitives.\n- GD up-samples feature representations and generates corresponding fine Gaussians in a single forward pass, leveraging embedded prior knowledge for enhanced generalization, in contrast to iterative splitting methods used in per-scene optimization.\n- The method utilizes a point-level transformer for efficient processing and incorporates a confidence mask to filter Gaussians that do not require further densification.\n- Integrating GD with LaRa and MVSplat models achieves state-of-the-art performance on large-scale datasets like Gobjaverse and RE10K, demonstrating significant improvements in reconstructing intricate details.\n- Qualitative analysis shows enhanced capture of thin structures and fine details, and cross-dataset evaluation confirms robust generalizability across different datasets.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://stnamjef.github.io/GenerativeDensification/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "StreamChat: Chatting with Streaming Video",
        "authors": "Shiyi Lan, hsli-cuhk, LucasFang, Zhiding, jjjjh",
        "link": "https://arxiv.org/abs/2412.08646",
        "github_repo": null,
        "summary": "- This paper introduces StreamChat, a novel approach for enhancing Large Multimodal Models (LMMs) to interact with streaming video content by dynamically updating the visual context at each decoding step using a cross-attention based architecture and visual feedforward network (V-FFN).\n- A parallel 3D-ROPE mechanism is used to better encode temporal information, and a dense instruction-tuning dataset based on existing dense caption datasets is created to train the model.\n- StreamChat outperforms state-of-the-art video LMMs in streaming interaction scenarios, demonstrating its superior ability to handle dynamic video content, even outperforming LLaVA-Video-72B with a smaller 7B model.\n- StreamChat also achieves competitive performance on established image and video benchmarks.\n-  The model effectively captures video dynamics and adjusts responses accordingly by incorporating the latest video information at each decoding step for more temporally aligned responses.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation",
        "authors": "Frag1le",
        "link": "https://arxiv.org/abs/2412.07797",
        "github_repo": null,
        "summary": "- Mogo is a novel GPT-type text-to-motion generation model composed of a hierarchical Residual Vector Quantized Variational Autoencoder (RVQ-VAE) and a Hierarchical Causal Transformer.\n- The RVQ-VAE discretizes motion sequences, while the Hierarchical Causal Transformer generates base motion sequences and infers residuals across layers, enabling the model to generate high-quality and diverse human motions from text descriptions.\n- Mogo outperforms existing GPT-type models on HumanML3D and KIT-ML datasets, achieving state-of-the-art FID scores. \n- It also demonstrates superior zero-shot performance on the CMP dataset, indicating its strong generalization capabilities and adaptability to unseen motion types. \n- Additionally, Mogo can generate longer motion sequences (up to 13 seconds) compared to existing datasets and integrates prompt engineering to enhance generation quality in zero/few-shot scenarios.",
        "classification": [
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models",
        "authors": "Tomer Michaeli, Inbar Huberman-Spiegelglas, Matan Kleiner, Vladimir Kulikov",
        "link": "https://arxiv.org/abs/2412.08629",
        "github_repo": null,
        "summary": "- FlowEdit is a novel, inversion-free, optimization-free, and model-agnostic method for text-based image editing using pre-trained text-to-image (T2I) flow models.\n- Unlike traditional editing-by-inversion methods, FlowEdit constructs an ordinary differential equation (ODE) that directly maps the source image distribution to the target distribution, leading to better structure preservation and higher-quality edits.\n- The method achieves state-of-the-art results on complex editing tasks, as demonstrated with Stable Diffusion 3 and FLUX.\n- FlowEdit\u2019s direct path between distributions results in lower transport costs compared to inversion-based methods, leading to better fidelity to the source image.\n- The method's model-agnostic nature facilitates seamless transferability between different model architectures and sampling schemes.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements",
        "authors": "Chi Zhang, Hao Wang, Beier Zhu, Xue Song, Mingkun Lei",
        "link": "https://arxiv.org/abs/2412.08503",
        "github_repo": null,
        "summary": "- StyleStudio, a novel text-driven style transfer model, addresses challenges like overfitting to reference styles, limited stylistic control, and misalignment with text content by employing three strategies.\n- It introduces cross-modal Adaptive Instance Normalization (AdaIN) to better integrate style and text features, Style-based Classifier-Free Guidance (SCFG) for selective style control, and a teacher model during early generation to stabilize layouts and reduce artifacts. \n- Evaluations show significant improvements in style transfer quality, alignment with text prompts, and layout stability across different styles. \n- The model achieves higher text alignment scores compared to existing methods and performs favorably in user studies assessing text alignment and style similarity.\n- The approach is versatile and can be integrated into various style transfer frameworks without requiring fine-tuning.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-12"
    },
    {
        "title": "MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation",
        "authors": "Lijie Wen, Shaolin Zhu, liboaccn",
        "link": "https://arxiv.org/abs/2412.07147",
        "github_repo": null,
        "summary": "- Introduced MIT-10M, a large-scale parallel corpus for multilingual image translation, containing over 10 million image-text pairs derived from real-world data and spanning 14 languages, 28 categories, and three difficulty levels.\n- The dataset underwent extensive cleaning and multilingual translation validation, including OCR annotation, NSFW detection, and sensitive content filtering.\n- Experiments demonstrated MIT-10M's superior performance in evaluating models on challenging real-world image translation tasks, particularly multi-line text in complex images. \n- Fine-tuning Qwen2-VL with MIT-10M resulted in significant improvements, tripling performance compared to the baseline.\n- The dataset promotes the development of more robust and adaptable multilingual image translation models.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/liboaccn/MIT-10M"
        ],
        "date": "2024-12-12"
    },
    {
        "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
        "authors": "Rui Qian, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Pan Zhang",
        "link": "https://arxiv.org/abs/2412.09596",
        "github_repo": "https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive",
        "summary": "- InternLM-XComposer2.5-OmniLive (IXC2.5-OL) is a multimodal system designed for real-time interaction with streaming video and audio inputs, addressing the limitations of current Multimodal Large Language Models (MLLMs) in continuous perception, memory, and reasoning.\n- IXC2.5-OL consists of three modules: a Streaming Perception Module processing multimodal information, a Multi-modal Long Memory Module integrating and retrieving short-term and long-term memories, and a Reasoning Module handling queries.\n- The system simulates human-like cognition by disentangling streaming perception, reasoning, and memory mechanisms, allowing simultaneous processing of information.\n- Evaluation across audio and video benchmarks demonstrates IXC2.5-OL's superior performance, achieving state-of-the-art results on StreamingBench for real-time video interactions and competitive results on other benchmarks like MLVU and Video-MME.\n- IXC2.5-OL excels in real-time video interactions while demonstrating competitive results among open-source models on several audio and video understanding benchmarks.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Audio",
            "Automatic Speech Recognition"
        ],
        "github_urls": [
            "https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Phi-4 Technical Report",
        "authors": "Ronen Eldan, S\u00e9bastien Bubeck, Harkirat Behl, Jyoti Aneja, Marah Abdin",
        "link": "https://arxiv.org/abs/2412.08905",
        "github_repo": null,
        "summary": "- Phi-4 is a 14-billion parameter language model trained with an emphasis on data quality, incorporating synthetic data generated by diverse techniques like multi-agent prompting and instruction reversal.\n- Phi-4 surpasses its teacher model (GPT-4) on STEM-focused QA, demonstrating that the data generation and post-training techniques provide capabilities beyond distillation.\n- The training recipe focuses on three pillars: synthetic data generation, curation of high-quality organic data, and innovative post-training techniques like rejection sampling and a new approach to Direct Preference Optimization (DPO).\n- Phi-4's performance on reasoning tasks is comparable to or surpasses larger models, exceeding Llama-3.1 on benchmarks like GPQA and MATH, and scoring high on the November 2024 AMC math competitions, indicating robust reasoning abilities and lack of overfitting.\n- Post-training includes supervised fine-tuning (SFT), pivotal token search-based DPO, and judge-guided DPO, refining the model's alignment with human preferences, improving reasoning, safety, robustness, and mitigating hallucinations.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
        "authors": "Willie Neiswanger, Jinyi Hu, Tianyu Yu, Ollie Liu, jrzhang",
        "link": "https://arxiv.org/abs/2412.08737",
        "github_repo": null,
        "summary": "- This paper introduces Euclid, a family of Multimodal Large Language Models (MLLMs) specifically optimized for enhanced low-level visual perception (LLVP) in geometric tasks.\n- Euclid employs a curriculum learning strategy with synthetically generated high-fidelity visual descriptions of geometric shapes and their relationships, addressing the limitations of existing MLLMs in accurately perceiving detailed geometric information. \n-  A new benchmark dataset called *Geoperception*, derived from the Geometry-3K corpus, is introduced to evaluate the model\u2019s ability to precisely transcribe 2D geometric information from images.\n-  Euclid outperforms leading open-source and closed-source models on the Geoperception benchmark, demonstrating strong generalization capabilities to novel geometry shapes. \n- For instance, Euclid surpasses the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain tasks and 10.65% on average.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision"
        ],
        "github_urls": [
            "github.com/euclid-multimodal/Euclid"
        ],
        "huggingface_urls": [
            "huggingface.co/euclid-multimodal"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "Multimodal Latent Language Modeling with Next-Token Diffusion",
        "authors": "Li Dong, Zhiliang Peng, Wenhui Wang, Hangbo Bao, Yutao Sun",
        "link": "https://arxiv.org/abs/2412.08635",
        "github_repo": null,
        "summary": "- LatentLM, a novel multimodal generative model, is introduced, which seamlessly integrates continuous and discrete data using causal transformers and next-token diffusion.\n- The model employs a variational autoencoder (VAE) to represent continuous data as latent vectors and next-token diffusion for autoregressive generation of these vectors, overcoming variance collapse issues with the use of \u03c3-VAE.\n- LatentLM shows superior performance in image generation, surpassing Diffusion Transformers in both performance and scalability, as evidenced by its favorable FID and IS scores on ImageNet.\n- When integrated into multimodal large language models, LatentLM provides a unified interface, outperforming existing methods in language modeling, text-to-image generation, and vision-language understanding tasks, as demonstrated by its better perplexity scores and FID scores on MS-COCO.\n- In text-to-speech synthesis, LatentLM achieves state-of-the-art results, outperforming VALL-E 2 in speaker similarity and robustness while requiring 10x fewer decoding steps.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text",
            "Text-to-Speech"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/DiT"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
        "authors": "Zhennan Shen, Dunjie Lu, Yiheng Xu, cxiong, ZeonLap",
        "link": "https://arxiv.org/abs/2412.09605",
        "github_repo": null,
        "summary": "- AgentTrek is a novel data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials.\n- It automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment.\n- A VLM-based evaluator ensures the correctness of the generated trajectories, resulting in a dataset with 10,398 trajectories.\n- Experiments demonstrate that training GUI agents with the synthesized trajectories significantly improves their grounding and planning performance compared to current models.\n- The proposed method is more cost-efficient than traditional human annotation methods, paving the way for large-scale GUI agent training.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://agenttrek.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM",
        "authors": "Hao Shao, Guanglu Song, Bingqi Ma, Dongzhi Jiang, Zhuofan Zong",
        "link": "https://arxiv.org/abs/2412.09618",
        "github_repo": null,
        "summary": "- EasyRef is a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and text prompts.\n- It leverages the multi-image comprehension and instruction-following capabilities of a multimodal large language model (MLLM) to capture consistent visual elements within multiple images.\n- EasyRef introduces an efficient reference aggregation strategy and a progressive training scheme to mitigate computational costs and enhance fine-grained detail preservation.\n- Experimental results demonstrate that EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA in terms of aesthetic quality and zero-shot generalization.\n- A new multi-reference image generation benchmark, MRBench, is introduced to facilitate the evaluation of multi-reference image generation methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://easyref-gen.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion",
        "authors": "Ziwei Liu, Xingang Pan, Xin Huang, Tengfei Wang, Zexin He",
        "link": "https://arxiv.org/abs/2412.09593",
        "github_repo": null,
        "summary": "- Neural LightRig is a novel framework that enhances object normal and PBR material estimation from single images by leveraging multi-light conditions generated via a diffusion prior.\n- It utilizes a two-stage process involving a multi-light diffusion model, built upon a stable diffusion model, which synthesizes images of the input object under varying directional light sources.\n- A large G-buffer model, with a U-Net architecture, subsequently processes these multi-light images to produce high-resolution surface normals and PBR material maps.\n- Neural LightRig is trained on a new synthetic relighting dataset named LightProp and employs data augmentation strategies to bridge the domain gap between synthetic and generated multi-light images.\n- Extensive evaluations demonstrate that Neural LightRig outperforms previous state-of-the-art methods in surface normal estimation, PBR material estimation, and single-image relighting.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://projects.zxhezexin.com/neural-lightrig"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations",
        "authors": "Eunbyung Park, Youngjoon Hong, Jaemin Oh, kangnamgyu27",
        "link": "https://arxiv.org/abs/2412.05994",
        "github_repo": null,
        "summary": "- This paper proposes Physics-Informed Gaussians (PIGs), a novel method for approximating solutions to Partial Differential Equations (PDEs) that combines learnable Gaussian feature embeddings with a lightweight neural network.\n- PIG dynamically adjusts the positions and shapes of Gaussians during training, enabling it to effectively capture high-frequency components and complex solution behaviors, addressing the limitations of traditional Physics-Informed Neural Networks (PINNs) that rely on Multi-Layer Perceptrons (MLPs) which suffer from spectral bias. \n- PIG maintains the same optimization framework as PINNs, using numerous collocation points to compute PDE residuals and gradient-based optimization, but requires fewer parameters and demonstrates faster convergence.\n- Experimental results on various PDEs, including Allen-Cahn, Helmholtz, Klein-Gordon, Flow Mixing, and Nonlinear Diffusion equations, show that PIG achieves competitive accuracy compared to existing methods, often outperforming methods using large MLPs or high-resolution parametric grids.\n- The adaptability of Gaussians allows PIG to efficiently approximate solutions by concentrating computational resources in regions with high residual losses or singularities, offering advantages over fixed grid structures.",
        "classification": [
            "Other"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training",
        "authors": "Arpit Sahni, Huseyin Coskun, Xijie Huang, Jierun Chen, Dongting Hu",
        "link": "https://arxiv.org/abs/2412.09619",
        "github_repo": null,
        "summary": "- This paper introduces SnapGen, a novel text-to-image model designed for mobile devices that generates high-resolution images.\n- SnapGen employs efficient network architectures, including a thinner and shorter UNet, and improved training techniques such as flow matching and multi-level knowledge distillation.\n- The model achieves competitive performance on ImageNet-1K and T2I benchmarks, surpassing large-scale models while using significantly fewer parameters (e.g., 7x smaller than SDXL).\n- SnapGen demonstrates high-resolution image generation (1024x1024) on mobile devices within approximately 1.4 seconds, showcasing its efficiency and speed.\n- The authors conduct extensive experiments to validate the model's performance, including quantitative benchmarks and a user study comparing its image quality with existing state-of-the-art methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Learned Compression for Compressed Learning",
        "authors": "Neeraja J. Yadwadkar, Dan Jacobellis",
        "link": "https://arxiv.org/abs/2412.09405",
        "github_repo": null,
        "summary": "- This paper introduces WaLLoC (Wavelet Learned Lossy Compression), a novel neural codec architecture for compressed-domain learning.\n- WaLLoC combines linear transform coding with nonlinear dimensionality-reducing autoencoders, placing a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform.\n- This design enables WaLLoC to achieve computationally efficient encoding, high compression ratios, and uniform dimensionality reduction\u2014key requirements for effective compressed learning.\n- Experiments demonstrate WaLLoC's superior performance compared to existing autoencoders used in state-of-the-art latent diffusion models across several metrics, including image classification, colorization, document understanding, and music source separation.\n- WaLLoC achieves up to 20x dimensionality reduction, making it an effective drop-in replacement for resolution reduction in accelerating downstream models without sacrificing accuracy.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Image-to-Image",
            "Audio-to-Audio",
            "Audio Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://ut-sysml.org/walloc/"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition",
        "authors": "Longxiang Tang, Senqiao Yang, Yuqi Liu, Chengyao Wang, Zhisheng Zhong",
        "link": "https://arxiv.org/abs/2412.09501",
        "github_repo": null,
        "summary": "- Lyra, an efficient Multimodal Large Language Model (MLLM), enhances multimodal abilities with a focus on speech, including long speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction.\n- Lyra leverages existing open-source large models and a multi-modality LoRA to reduce training costs, uses a latent multi-modality regularizer and extractor to strengthen relationships between modalities (especially speech), and introduces a high-quality dataset with 1.5M multimodal samples and 12K long speech samples.\n- Compared to other omni-models, Lyra achieves state-of-the-art performance on vision-language, vision-speech, and speech-language benchmarks.\n- Lyra demonstrates superior efficiency with fewer computational resources, less training data, and faster training and inference speed across speech, image, and video tasks.\n- The model supports sound and speech understanding and generation, handles complex long speech inputs, and exhibits enhanced omni-comprehension capabilities.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Text-to-Speech",
            "Text-to-Audio",
            "Automatic Speech Recognition"
        ],
        "github_urls": [
            "https://github.com/dvlab-research/Lyra"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios",
        "authors": "Xiaobao Wu, Sitao Cheng, Liangming Pan, Wenyue Hua, Ruiwen Zhou",
        "link": "https://arxiv.org/abs/2412.08972",
        "github_repo": "https://github.com/skyriver-2000/RuleArena",
        "summary": "- RULEARENA, a benchmark designed to evaluate LLMs' ability to follow complex real-world rules in reasoning across three domains: airline baggage fees, NBA transactions, and tax regulations.\n- It assesses proficiency in handling intricate instructions requiring long-context understanding, logical reasoning, and accurate mathematical computation.\n- Two key distinctions from existing benchmarks: extends beyond first-order logic and grounds tasks in authentic scenarios.\n- Findings reveal LLM limitations: difficulty applying rules, inaccurate math, and overall poor performance.\n- Highlights challenges in rule-guided reasoning for LLMs in real-world applications.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/skyriver-2000/RuleArena"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
        "authors": "Judy Hoffman, Daniel Bolya, Sangmin Lee, Ajay Bati, Fiona Ryan",
        "link": "https://arxiv.org/abs/2412.09586",
        "github_repo": "http://github.com/fkryan/gazelle",
        "summary": "- Gaze-LLE, a novel transformer-based model, streamlines gaze target estimation by leveraging features from a frozen DINOv2 encoder, eliminating the need for complex, handcrafted multi-branch architectures.\n- The model utilizes a single feature representation for the scene and incorporates a person-specific positional prompt to decode gaze with a lightweight module, significantly reducing the number of learnable parameters compared to prior works.\n- Gaze-LLE achieves state-of-the-art performance across multiple gaze estimation benchmarks, including GazeFollow, VideoAttentionTarget, and ChildPlay, demonstrating the effectiveness of leveraging pretrained visual feature representations from foundational models.\n- The model exhibits strong cross-dataset performance without fine-tuning, highlighting its generalizability to diverse domains.\n-  Gaze-LLE is computationally efficient, achieving state-of-the-art results in under 1.5 GPU hours.",
        "classification": [
            "Computer Vision",
            "Keypoint Detection"
        ],
        "github_urls": [
            "http://github.com/fkryan/gazelle"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
        "authors": "Lilach Eden, Roy Bar-Haim, Yotam Perlitz, Odellia Boni, Ariel Gera",
        "link": "https://arxiv.org/abs/2412.09569",
        "github_repo": null,
        "summary": "- This paper introduces JuStRank, a novel benchmark designed to assess the efficacy of Large Language Models (LLMs) in performing system-level ranking of other LLMs.\n- The benchmark employs a dataset of responses generated by various LLMs to a set of instructions and evaluates the judges' ability to rank the systems based on the quality of their responses, using correlation with a human-generated ranking as the primary metric.\n-  The study evaluates 48 state-of-the-art judges, encompassing both general-purpose LLMs and specialized reward models, and reveals that several smaller reward models can perform comparably to much larger LLMs in this ranking task.\n- The authors also investigate the influence of various judge realizations (e.g., absolute numeric scores, Likert-scale ratings, comparative judgments) on ranking accuracy and observe a significant impact, with absolute scores generally outperforming comparative judgments.\n-  The analysis identifies two key emergent properties of system-level judges: decisiveness, characterized by the tendency to amplify differences between strong and weak systems, and system-specific bias.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation",
        "authors": "Jianwei Yang, Jianfeng Gao, Humphrey Shi, Zhengyuan Yang, Jitesh Jain",
        "link": "https://arxiv.org/abs/2412.09585",
        "github_repo": "https://github.com/SHI-Labs/OLA-VLM",
        "summary": "- OLA-VLM introduces a novel approach to enhance visual representations within Multimodal Large Language Models (MLLMs) by distilling knowledge from specialized teacher vision encoders into the LLM's intermediate layers during pretraining.\n- This method involves a coupled optimization of predictive visual embedding and next text-token prediction, incorporating embedding losses at specific LLM layers to align with target visual features from encoders trained on tasks like segmentation, depth estimation, and image generation.\n- The approach also utilizes specialized tokens enriched with target-specific information, creating an implicit visual chain of thought within the LLM's input sequence.\n- OLA-VLM demonstrates improved performance over single and multi-encoder baselines on various benchmarks, including up to an 8.7% boost on the Depth task in CV-Bench, showcasing its superior visual understanding capabilities.\n- The embedding optimization strategy is hypothesized to yield better projector initialization for the instruction fine-tuning stage, leading to efficient and accurate visual processing within the MLLM while using only a single vision encoder during inference.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision",
            "Depth Estimation",
            "Image Segmentation"
        ],
        "github_urls": [
            "https://github.com/SHI-Labs/OLA-VLM"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective",
        "authors": "David Samuel, Freddy Wetjen, Lemei Zhang, Vladislav Mikhailov, Javier de la Rosa",
        "link": "https://arxiv.org/abs/2412.09460",
        "github_repo": null,
        "summary": "- This research investigates the impact of copyrighted material on Norwegian Large Language Models (LLMs) by training models using various datasets, including copyrighted and non-copyrighted materials.\n- The study uses Mistral 7B v0.1 architecture as base and creates different versions by pre-training from scratch and also warm-starting and fine-tuning with copyrighted materials and instructions.\n- Evaluations are conducted using a new benchmarking suite with 28 NLP tasks designed for Norwegian, demonstrating that copyrighted material leads to performance improvements across diverse tasks, particularly specialized ones.\n- The paper finds that warm-starting with other languages reduces the impact of adding Norwegian copyrighted data.\n- The findings highlight ethical and legal considerations regarding the use of copyrighted materials in LLM development and provide empirical evidence for policy discussions on author compensation and copyright in the digital age.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/mistralai/Mistral-7B-v0",
            "https://huggingface.co/datasets/mimir-project/mimir-bias",
            "https://huggingface.co/datasets/ltg/nortruthfulqa_mc",
            "https://huggingface.co/datasets/ltg/nortruthfulqa_gen",
            "https://huggingface.co/datasets/ltg/noropenbookqa",
            "https://huggingface.co/datasets/ltg/nrk",
            "https://huggingface.co/datasets/ltg/norcommonsensega",
            "https://huggingface.co/datasets/mimir-project/noridiom",
            "https://huggingface.co/datasets/SamiaT/NorSumm"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "Word Sense Linking: Disambiguating Outside the Sandbox",
        "authors": "Roberto Navigli, Alberte Fern\u00e1ndez-Castro, Luigi Procopio, Edoardo Barba, Andrei Stefan Bejgu",
        "link": "https://arxiv.org/abs/2412.09370",
        "github_repo": null,
        "summary": "- This paper introduces Word Sense Linking (WSL), a new task that aims to bridge the gap between Word Sense Disambiguation (WSD) research and its practical application in downstream tasks.\n- WSL requires systems to identify and disambiguate all spans in a given text using only a reference sense inventory, without relying on pre-identified spans or candidate senses.\n- A novel retriever-reader architecture is proposed for WSL, which inverts the traditional concept detection and candidate generation steps of WSD to overcome limitations in handling unannotated spans.\n- The model outperforms state-of-the-art WSD systems adapted for the WSL setting by a significant margin, demonstrating its robustness and efficiency.\n- A new WSL benchmark dataset, built by expanding the existing WSD evaluation datasets, facilitates comprehensive evaluation of both precision and recall.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Babelscape/WSL"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction",
        "authors": "Ying Shan, Shenghua Gao, Jiale Xu",
        "link": "https://arxiv.org/abs/2412.09573",
        "github_repo": null,
        "summary": "- FreeSplatter is a feed-forward framework that generates 3D Gaussians and estimates camera parameters from uncalibrated sparse-view images.\n- It uses a transformer architecture with self-attention blocks to process multi-view image tokens and decode them into pixel-wise 3D Gaussian primitives.\n- FreeSplatter outperforms state-of-the-art baselines on object-centric and scene-level datasets in reconstruction quality and pose estimation accuracy, even surpassing pose-dependent models on object-centric tasks.\n- Two variants, FreeSplatter-O and FreeSplatter-S, are trained for object and scene reconstruction, showcasing its adaptability.\n- It enhances downstream applications like text/image-to-3D, simplifying the process by eliminating the need for explicit camera pose alignment.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "DisPose: Disentangling Pose Guidance for Controllable Human Image Animation",
        "authors": "Zhihong Zhu, Junjie Cao, Yuhang Yang, Yaowei Li, Hongxiang Li",
        "link": "https://arxiv.org/abs/2412.09349",
        "github_repo": "https://github.com/lihxxx/DisPose",
        "summary": "- DisPose, a plug-and-play module, enhances controllable human image animation by disentangling pose guidance into motion field guidance and keypoint correspondence, thus improving quality and consistency without additional dense inputs.\n- It generates a dense motion field from a sparse motion field derived from skeleton poses and the reference image, providing region-level guidance while maintaining generalization.\n- DisPose extracts diffusion features from reference image keypoints and transfers them to the target pose based on motion trajectories for enhanced appearance consistency.\n- Implemented as a hybrid ControlNet, it integrates seamlessly with existing models, improving quality without modifying their parameters.\n-  Quantitative and qualitative evaluations demonstrate DisPose's superior performance compared to current methods, including improvements in metrics like FID-FVD and VBench scores on a TikTok dataset and an unseen dataset.",
        "classification": [
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/lihxxx/DisPose"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "LoRACLR: Contrastive Adaptation for Customization of Diffusion Models",
        "authors": "Pinar Yanardag, Federico Tombari, Thomas Hofmann, enisimsar",
        "link": "https://arxiv.org/abs/2412.09622",
        "github_repo": null,
        "summary": "- LoRACLR is a novel approach for multi-concept image generation that merges multiple pre-trained LoRA models into a single unified model using a contrastive objective, eliminating the need for individual fine-tuning or access to original training data.\n- This method adapts pre-existing LoRA models to function cohesively by aligning their weight spaces, ensuring each concept retains high fidelity while remaining compatible in joint compositions.\n- The contrastive objective enforces distinct boundaries between concepts, preventing feature interference and preserving individual identities.\n- LORACLR demonstrates superior performance compared to state-of-the-art methods in both qualitative and quantitative evaluations, exhibiting enhanced image quality, compositional coherence, and identity preservation, particularly in complex scenarios with multiple concepts.\n- This approach provides a practical and scalable solution for personalized image synthesis, enabling efficient and high-quality generation of complex scenes without retraining or relying on heavy computational overhead.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/windwhinny/chilloutmix"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts",
        "authors": "Mohit Bansal, Chongyang Zhao, Zun Wang, Yicong Hong, Gengze Zhou",
        "link": "https://arxiv.org/abs/2412.05552",
        "github_repo": null,
        "summary": "- This paper proposes State-Adaptive Mixture of Experts (SAME), a novel model for versatile language-guided visual navigation that can interpret and execute instructions with varying levels of granularity.\n- SAME utilizes a mixture of expert networks specialized in different navigation skills, such as exploration and instruction-following, routed based on the agent's current state (attended language and visual observation).\n- The model effectively addresses the challenge of conflicting learning objectives in multi-task training by selectively activating experts based on the input state, promoting shared knowledge learning while maintaining task-specific capabilities.\n- Experimental results on seven navigation tasks including R2R, RxR-EN, REVERIE, OBJECTNAV, CVDN, SOON, and R2R-CE, demonstrate that SAME outperforms or achieves comparable performance to task-specific agents, showing the efficacy of the state-adaptive expert routing mechanism.\n- Further analysis reveals that applying MoE on visual queries in the cross-attention layer of the navigation policy yields superior results compared to applying it on feed-forward networks, highlighting the importance of cross-modal attention in action selection.",
        "classification": [
            "Computer Vision",
            "Reinforcement Learning",
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/GengzeZhou/SAME"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Arbitrary-steps Image Super-resolution via Diffusion Inversion",
        "authors": "Chen Change Loy, Kang Liao, Zongsheng Yue",
        "link": "https://arxiv.org/abs/2412.09013",
        "github_repo": "https://github.com/zsyOAOA/InvSR",
        "summary": "- This paper introduces InvSR, a novel image super-resolution (SR) technique based on diffusion inversion, which leverages a pre-trained diffusion model (SD-Turbo) without modifying its architecture.\n- InvSR employs a Partial Noise Prediction (PnP) strategy to determine an optimal noise map for a given low-resolution (LR) image, enabling efficient initialization of the reverse sampling process.\n-  A deep noise predictor network is trained to estimate this noise map, enabling flexible sampling with arbitrary steps ranging from one to five.\n- Experimental results on ImageNet-Test and real-world datasets demonstrate that InvSR with a single sampling step achieves comparable or superior results to state-of-the-art one-step SR methods and competitive results compared to multi-step methods.\n- InvSR provides a flexible and efficient sampling mechanism adaptable to various degradation types, making it practical for real-world applications.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/zsyOAOA/InvSR"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages",
        "authors": "Srinivasan Umesh, rumourscape",
        "link": "https://arxiv.org/abs/2412.09025",
        "github_repo": null,
        "summary": "- This paper introduces Shiksha, a dataset and model for translating technical content into Indian languages.\n- The dataset comprises 2.8 million high-quality, parallel translation pairs across 8 Indian languages, extracted from NPTEL lecture transcriptions.\n- A 3.3B parameter NLLB model is fine-tuned using LoRA on this dataset, demonstrating improvements in both in-domain and out-of-domain translation tasks.\n- Evaluation on a held-out test set and the Flores+ benchmark shows improved performance over baseline NLLB and comparable results to IndicTrans2.\n- The models are integrated into a tool called Translingua, which aids human annotators in translating NPTEL lectures.",
        "classification": [
            "Translation",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/SPRINGLab"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "GenEx: Generating an Explorable World",
        "authors": "danyaljj, jiahaoplus, lambertxiao, tshu, TaiMingLu",
        "link": "https://arxiv.org/abs/2412.09624",
        "github_repo": null,
        "summary": "- GenEx introduces a new framework for generating explorable 3D worlds from a single image, allowing embodied AI agents to navigate and interact.\n- The system generates 360\u00b0 panoramic video streams, offering a continuous and immersive environment grounded in physical principles through the use of physics engines for data curation.\n- GenEx leverages GPT-assisted agents for both goal-agnostic exploration and goal-driven navigation, enabling them to refine beliefs, simulate outcomes, and make informed decisions based on imagined observations.\n- Evaluated using metrics such as FVD, SSIM, LPIPS, and PSNR, GenEx demonstrates high-quality world generation and robust loop consistency over long trajectories.\n- The framework also supports multi-agent scenarios and demonstrates potential applications in embodied decision-making, showcasing the transformative capabilities of generative AI for world exploration and planning.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Text-to-3D",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
        "authors": "minione, lichengyu, YannDubs, nicholswang, orrzohar",
        "link": "https://arxiv.org/abs/2412.10360",
        "github_repo": null,
        "summary": "- The paper introduces Apollo, a family of state-of-the-art Large Multimodal Models (LMMs) designed for enhanced video understanding, capable of processing hour-long videos efficiently.\n- Apollo utilizes a unified architecture employing a combination of InternVideo2 and SigLIP-SO400M encoders, with features concatenated and resampled using a Perceiver Resampler before being fed to a large language model (LLM).\n- The authors claim Apollo-3B outperforms most existing 7B models, achieving a score of 58.4 on Video-MME (without subtitles), 68.7 on MLVU, and 62.7 on their proposed benchmark, ApolloBench. \n- Apollo-7B achieves state-of-the-art performance amongst 7B LMMs with scores of 61.2 on Video-MME, 70.9 on MLVU, and 66.3 on ApolloBench, demonstrating competitiveness with some 30B models.\n- The study also explores various design choices, such as video sampling strategies, encoder combinations, and data composition, introducing the concept of \"Scaling Consistency,\" where design decisions from smaller models effectively transfer to larger models.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
        "authors": "Saeed Yahya Alseiari, Mohammed Irfan Kurpath, hishamcholakkal, HuggingSara, sahalshajim",
        "link": "https://arxiv.org/abs/2412.07769",
        "github_repo": "https://github.com/mbzuai-oryx/BiMediX2",
        "summary": "- BiMediX2 is a bilingual (Arabic-English) Large Multimodal Model (LMM) with a unified architecture integrating text and visual modalities for advanced medical image understanding and applications.\n- It leverages the Llama 3.1 architecture with integrated text and visual capabilities, supporting text and multi-turn conversations involving medical images and trained on a 1.6M sample bilingual healthcare dataset (BiMed-V).\n- BiMediX2 outperforms state-of-the-art models in medical LLM and VLM evaluation benchmarks, exceeding GPT-4 by 9% in UPHILL factual accuracy and showing over 9% improvement in English and 20% in Arabic on multimodal medical evaluations.\n- A new bilingual GPT4-based medical LLM benchmark called BiMed-MBench was introduced.\n- The model excels in medical Visual Question Answering, Report Generation, and Report Summarization tasks across diverse imaging modalities.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-Text-to-Text",
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [
            "https://github.com/mbzuai-oryx/BiMedix2"
        ],
        "date": "2024-12-16"
    },
    {
        "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption",
        "authors": "BradyFU, zhenheny, SherryX, nankepan, AnonMegumi",
        "link": "https://arxiv.org/abs/2412.09283",
        "github_repo": null,
        "summary": "- InstanceCap, a novel instance-aware structured caption framework, is proposed to enhance the fidelity and detail of text-to-video (T2V) generation by creating more accurate video captions.\n- It uses an auxiliary models cluster (AMC) to extract instance-level information, including class, appearance, actions, motion, and positions, which helps reduce hallucinations and improves caption accuracy and consistency.\n- An improved Chain-of-Thought (CoT) process with Multimodal Large Language Models (MLLMs) refines prompts into structured phrases.\n- A new 22k InstanceVid dataset with instance-aware structured captions is introduced for training T2V models and a prompt enhancement pipeline, InstanceEnhancer, is designed for inference to further improve caption generation.\n- Experimental results on video reconstruction and T2V generation using the fine-tuned Open-Sora model with InstanceVid demonstrate that InstanceCap enhances detail fidelity and action accuracy compared to existing captioning methods.",
        "classification": [
            "Text-to-Video",
            "Video-Text-to-Text",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/NJU-PCALab/InstanceCap"
        ],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "Large Action Models: From Inception to Implementation",
        "authors": "Eliblo1969, substill, shilhe, Lujunting, vyokky",
        "link": "https://arxiv.org/abs/2412.10047",
        "github_repo": "https://github.com/microsoft/UFO/tree/main/dataflow",
        "summary": "- This paper introduces Large Action Models (LAMs), a new type of AI model designed to perform actions in both physical and digital environments, extending the capabilities of Large Language Models (LLMs).\n- LAMs are trained using a four-phase approach: task-plan pretraining, learning from experts, self-boosting exploration, and learning from a reward model.\n- The authors demonstrate the effectiveness of LAMs by integrating them into a Windows OS-based agent, showing superior performance in task completion compared to LLMs like GPT-40, particularly in scenarios requiring precise interaction and manipulation within specific environments.\n- The LAM achieved an 81.2% Task Success Rate (TSR), surpassing GPT-40's 67.2% and GPT-40 Mini's 62.3% in a Word application environment, demonstrating the effectiveness of LAMs over traditional LLMs in action-oriented tasks.\n- The paper concludes by discussing the current limitations of LAMs and identifying key areas for future research.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/microsoft/UFO/tree/main/dataflow"
        ],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion",
        "authors": "JacobYuan, Ruihang, weilllllls, StevenZhang, MoonQiu",
        "link": "https://arxiv.org/abs/2412.09626",
        "github_repo": null,
        "summary": "- FreeScale, a tuning-free inference paradigm, enhances the resolution of pre-trained diffusion models for image and video generation by fusing information from different receptive scales.\n- The method employs tailored self-cascade upscaling and restrained dilated convolution to maintain visual structure and utilizes scale fusion within self-attention layers to combine global and local details, mitigating repetitive patterns.\n- FreeScale successfully generates images up to 8k resolution and produces higher-fidelity videos without fine-tuning.\n- Quantitative results on LAION-5B and WebVid-10M datasets demonstrate FreeScale's superior performance compared to existing methods in terms of FID, KID, and FVD metrics, often achieving best or second-best scores.\n- Qualitative comparisons and a user study further validate the enhanced quality and coherence of the generated visual content.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation",
        "authors": "Dana Berman, Matan Cohen, Asaf Shul, yedid, danielwinter",
        "link": "https://arxiv.org/abs/2412.08645",
        "github_repo": null,
        "summary": "- ObjectMate introduces a tuning-free method for object insertion and subject-driven generation, utilizing a novel \"object recurrence prior.\"\n- This prior leverages the recurrence of everyday objects across large, unlabeled image datasets to create a massive, supervised training dataset with diverse poses, lighting, and scenes.\n- The model architecture is based on a straightforward text-to-image diffusion model trained on this dataset, taking object views and scene descriptions as input.\n- ObjectMate achieves state-of-the-art results on object insertion and subject-driven generation tasks, outperforming existing methods in identity preservation and photorealistic composition.\n- The paper also introduces a new object insertion evaluation dataset with ground truth data and proposes a new metric for identity preservation that aligns better with human perception, validated through a user study.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing",
        "authors": "Fan Tang, Changwang Mei, duke1852022, MagicBag, yingying87",
        "link": "https://arxiv.org/abs/2412.07517",
        "github_repo": "https://github.com/HolmesShuan/FireFlow",
        "summary": "- FireFlow is a zero-shot image editing approach leveraging Rectified Flows (ReFlows), specifically enhancing inversion and editing capabilities while maintaining the generation strengths of models like FLUX.\n- A novel second-order numerical solver is introduced for ReFlow inversion, achieving higher accuracy and faster runtime (3x speedup) compared to existing techniques, by reusing intermediate velocity estimations.\n- Demonstrates effective 8-step semantic image editing and stylization guided by prompts, preserving original content integrity.\n- Evaluation shows FireFlow's superior performance in image reconstruction with lower errors and faster convergence.\n- Outperforms or competes with other editing approaches in preservation and CLIP similarity on the PIE-Bench dataset using only 8 steps.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/HolmesShuan/FireFlow"
        ],
        "date": "2024-12-16"
    },
    {
        "title": "Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation",
        "authors": "morninghaze, baochenxi, wzk1015, JackyZhuo, wbs2788",
        "link": "https://arxiv.org/abs/2412.09428",
        "github_repo": "https://github.com/wbs2788/VMB",
        "summary": "- This paper introduces Visuals Music Bridge (VMB), a novel multimodal music generation framework that uses text and music as explicit bridges for enhanced cross-modal alignment.\n- VMB consists of three core components: a Multimodal Music Description Model (MMDM) to convert visual input into text descriptions; a Dual-track Music Retrieval module to retrieve relevant music pieces; and an Explicitly Conditioned Music Generation framework to synthesize music.\n- The Explicitly Conditioned Music Generation module consists of a latent diffusion transformer (DiT) and employs Music ControlFormer and Stylization Module to enable high-quality generation.\n- The proposed method addresses challenges like data scarcity, weak cross-modal alignment, and limited controllability in existing multimodal music generation methods.\n- Experimental results on video-to-music, text-to-music, image-to-music, and controllable music generation tasks demonstrate that VMB significantly improves music quality, modality, and customization alignment compared to previous methods.",
        "classification": [
            "Multimodal",
            "Text-to-Audio",
            "Video-Text-to-Text",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/wbs2788/VMB"
        ],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding",
        "authors": "wzk1015, Einsiedler, hehesang, Changyao, cpsxhao",
        "link": "https://arxiv.org/abs/2412.09604",
        "github_repo": null,
        "summary": "- SynerGen-VL is a unified Multimodal Large Language Model (MLLM) designed for synergistic image understanding and generation using a single architecture and training process with a next-token prediction paradigm.\n- It introduces a token folding mechanism with a hierarchical architecture to compress input image token sequences, enabling efficient handling of high-resolution images and a decoder that reconstructs the image during generation.\n- Vision-expert-based progressive alignment pretraining integrates visual capabilities into the pretrained LLM, minimizing disruption to existing knowledge by using image-specific Feed-Forward Networks (FFNs) and aligning visual representations with the LLM's representation space.\n- Trained on large-scale mixed image-text data, SynerGen-VL achieves competitive performance compared to existing encoder-free unified MLLMs with comparable or smaller parameter sizes and narrows the gap with task-specific state-of-the-art models.\n- With 2.4B activated parameters, SynerGen-VL matches the performance of Emu3, which has 8B parameters, demonstrating its strong potential as a next-generation unified MLLM.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
        "authors": "Chengruidong, luoxufang, qianhuiwu, iofu728, liyucheng",
        "link": "https://arxiv.org/abs/2412.10319",
        "github_repo": null,
        "summary": "- Introduces SCBench, a benchmark designed to evaluate efficient long-context methods, particularly for shared context and multi-round interactions where KV Cache is reused.\n- Assesses four key long-context abilities: String Retrieval, Semantic Retrieval, Global Information processing, and Multi-tasking across 12 tasks with two shared context modes (multi-turn and multi-request).\n- Evaluates 13 long-context methods across four stages (generation, compression, retrieval, and loading) and eight categories on six open-source long-context LLMs.\n- Finds that sub-O(n) memory methods struggle in multi-turn scenarios, sparse encoding with O(n) memory performs robustly, and dynamic sparsity is more expressive for KV caches than static patterns.\n- Identifies attention distribution shift issues in long-generation scenarios, impacting performance even for O(n) memory methods.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs",
        "authors": "SultanR",
        "link": "https://arxiv.org/abs/2412.08347",
        "github_repo": null,
        "summary": "- This paper introduces SmolTulu-1.7b-Instruct, an instruction-tuned language model based on Huggingface's SmolLM2-1.7B and adapted from AllenAI's Tulu 3 training pipeline.\n- The research focuses on the impact of learning rate to batch size ratios on model performance across different tasks, finding that higher ratios benefit reasoning tasks while lower ratios are optimal for pattern recognition tasks.\n- SmolTulu achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval (\u039411%) and 51.6% on GSM8K (13.4%) for mathematical reasoning.\n- The model also achieved 57.1% on ARC (15.4%) with an alternate version. \n-  Training recipes and ablation studies are released to promote further research in efficient model alignment and optimization for small language models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct"
        ],
        "date": "2024-12-16"
    },
    {
        "title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers",
        "authors": "Pinar Yanardag, Kavana Venkatesh, ydalva",
        "link": "https://arxiv.org/abs/2412.09611",
        "github_repo": null,
        "summary": "- Introduces FluxSpace, a novel framework for disentangled image editing using rectified flow transformers, such as Flux.\n- Leverages the representational power of joint transformer blocks within Flux to enable fine-grained and coarse-level semantic edits during inference time without requiring additional training.\n- Employs a linear editing scheme within the FluxSpace representation enabling disentangled control over various image attributes and overall style through manipulation of attention outputs.\n- Demonstrates through qualitative and quantitative experiments, supported by a user study, the effectiveness of FluxSpace in achieving semantic edits while preserving image content and subject identity, outperforming state-of-the-art methods in image editing.\n- Successfully performs image edits in both real and generated images from various domains ranging from portraits to complex scenes, as well as stylized image modifications.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/black-forest-labs/FLUX.1-dev"
        ],
        "date": "2024-12-16"
    },
    {
        "title": "Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images",
        "authors": "Ilker Hacihaliloglu, Leonid Sigal, Clayton Allard, moein99, yasimed",
        "link": "https://arxiv.org/abs/2412.09910",
        "github_repo": "https://github.com/yasamin-med/P2P",
        "summary": "- This paper introduces Prompt2Perturb (P2P), a novel language-guided adversarial attack method that generates perturbations in medical images using text instructions.\n- The approach leverages learnable prompts within the text encoder of a diffusion model, like Stable Diffusion, to create subtle yet effective perturbations that remain imperceptible while guiding the model towards targeted misclassifications.\n- Unlike existing diffusion-based attacks, P2P directly updates text embeddings instead of retraining diffusion models or adding noise to the latent space, thus suitable for data-scarce medical image domains.\n- P2P optimizes only the early reverse diffusion steps, boosting efficiency while ensuring that the generated adversarial examples incorporate subtle noise, therefore maintaining ultrasound image quality.\n- Experimental results on three breast ultrasound datasets demonstrate that P2P outperforms state-of-the-art attack techniques in FID and LPIPS, yielding both more natural and more effective adversarial examples compared to existing methods.",
        "classification": [
            "Computer Vision",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/yasamin-med/P2P"
        ],
        "huggingface_urls": [],
        "date": "2024-12-16"
    },
    {
        "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
        "authors": "douzc, Benen2024, wuyongkang, jinjiajie, lixiaoxi45",
        "link": "https://arxiv.org/abs/2412.11919",
        "github_repo": "https://github.com/sunnynexus/RetroLLM",
        "summary": "- RetroLLM is a novel framework that integrates retrieval and generation within a unified auto-regressive decoding process in LLMs, allowing direct generation of fine-grained evidence from a corpus using constrained decoding.\n- It employs hierarchical FM-Index constraints, generating corpus-constrained clues to identify relevant documents before evidence generation to mitigate false pruning.\n- It introduces forward-looking constrained decoding, utilizing document FM-Index to identify future windows and a relevance model to score these windows for improved evidence accuracy.\n- Experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance in in-domain and out-of-domain tasks, outperforming traditional RAG and more complex RAG strategies.\n- RetroLLM also significantly reduces token consumption compared to existing RAG methods due to more precise retrieval granularity.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/sunnynexus/RetroLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models",
        "authors": "Yu Qiao, liuziwei7, Ziqi, shulin16, Fan-s",
        "link": "https://arxiv.org/abs/2412.09645",
        "github_repo": "https://github.com/Vchitect/Evaluation-Agent",
        "summary": "- This paper introduces Evaluation Agent, a new framework for evaluating visual generative models (both image and video) that mimics human evaluation strategies.\n- It employs Large Language Model (LLM)-powered agents to dynamically adjust evaluation pathways, generate tailored prompts based on user needs, and provide detailed explanations of results.\n- Evaluation Agent significantly reduces evaluation time compared to traditional methods while maintaining comparable result quality, achieving a 90% reduction in evaluation time and also supports open-ended queries and model comparisons.\n- Experiments were conducted on various open-source models and benchmarks, showcasing the efficiency and versatility of the agent.\n- Evaluation Agent is fully open-sourced and can be scaled across various visual generative models and tools.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/Vchitect/Evaluation-Agent"
        ],
        "huggingface_urls": [
            "https://vchitect.github.io/Evaluation-Agent-project/"
        ],
        "date": "2024-12-17"
    },
    {
        "title": "ColorFlow: Retrieval-Augmented Image Sequence Colorization",
        "authors": "Yong Liu, yshan2u, ZyZcuhk, juxuan27, JunhaoZhuang",
        "link": "https://arxiv.org/abs/2412.11815",
        "github_repo": null,
        "summary": "- This paper introduces ColorFlow, a novel three-stage framework for reference-based image sequence colorization, designed to maintain consistent colors for characters and objects across frames.\n- The model uses a Retrieval-Augmented Pipeline (RAP) to extract relevant color patches from a reference image pool, an In-context Colorization Pipeline (ICP) with a dual-branch design for color identity extraction and colorization leveraging diffusion models and self-attention, and a Guided Super-Resolution Pipeline (GSRP) for upsampling and detail enhancement.\n-  A new benchmark dataset, ColorFlow-Bench, consisting of 30 manga chapters with reference images, is also introduced for evaluation.\n-  Experimental results on ColorFlow-Bench demonstrate state-of-the-art performance, achieving a 37% reduction in FID score compared to existing colorization models and ranking first in user studies for aesthetic quality, similarity to reference, and sequential consistency.\n-  The method excels in fine-grained color identity preservation and image quality improvement, making it potentially beneficial for industrial applications like manga and cartoon colorization.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "BrushEdit: All-In-One Image Inpainting and Editing",
        "authors": "yshan2u, ZyZcuhk, juxuan27, BianYx, Yw22",
        "link": "https://arxiv.org/abs/2412.10316",
        "github_repo": null,
        "summary": "- BrushEdit is an interactive image editing framework that combines language models and a dual-branch inpainting technique for seamless edits such as adding/removing objects and making structural changes with free-form masks.\n- It leverages pre-trained multimodal large language models (MLLMs) to interpret user instructions, identify editing types and target objects, and generate textual descriptions of the edited image.\n- The Editing Conductor, built on BrushNet, uses a mixed fine-tuning strategy with random and segmentation masks, allowing it to handle diverse mask-based inpainting tasks.\n- Experimental results on PIE-Bench, BrushBench, and EditBench demonstrate BrushEdit\u2019s superior performance in preserving unedited regions, ensuring accurate text-alignment, and outperforming existing methods in image editing and inpainting tasks.\n- BrushEdit offers flexible control over base diffusion model selection and scale adjustment, enhancing its practical value for diverse user needs.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Byte Latent Transformer: Patches Scale Better Than Tokens",
        "authors": "spermwhale, Chunting, marg33, benjamin-mlr, artidoro",
        "link": "https://arxiv.org/abs/2412.09871",
        "github_repo": null,
        "summary": "- BLT (Byte Latent Transformer) is a new byte-level LLM architecture that dynamically groups bytes into patches based on next-byte entropy, allocating more compute to complex segments.\n- It uses a local encoder and decoder for byte-patch transformations and a global latent transformer for patch processing, matching token-based models at scale while improving inference efficiency and robustness.\n- BLT achieves parity with Llama 3 in training FLOP-controlled performance while using up to 50% fewer FLOPS at inference, and shows better scaling trends with simultaneous increases in model and patch size.\n- It demonstrates qualitative improvements on reasoning, long-tail generalization, noisy input robustness, and sub-word aspect awareness, surpassing token-based models in these areas.\n- BLT's code is released for both training and inference.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/blt"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Causal Diffusion Transformers for Generative Modeling",
        "authors": "Haoqi Fan, Shi Guan, Deyao Zh, Chaorui Deng, Andy1621",
        "link": "https://arxiv.org/abs/2412.12095",
        "github_repo": null,
        "summary": "- CausalFusion, a decoder-only transformer model, dual-factorizes data across sequential tokens and diffusion noise levels, effectively unifying autoregressive (AR) and diffusion models for generative tasks.\n- This architecture allows flexible interpolation between AR and diffusion generation modes during training and inference, supporting arbitrary numbers of tokens, sequence order, and inference compute levels.\n- CausalFusion achieves state-of-the-art performance on ImageNet class-conditional generation, outperforming DiT and other baselines while using fewer parameters.\n- It also demonstrates strong capabilities in multimodal generation, including joint image-captioning and text-to-image generation, and exhibits zero-shot image manipulation abilities thanks to its AR nature.\n- Additionally, CausalFusion excels in visual representation learning tasks when fine-tuned for image classification and captioning, surpassing DiT in both domains.",
        "classification": [
            "Text-to-Image",
            "Image-to-Text",
            "Image-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Smaller Language Models Are Better Instruction Evolvers",
        "authors": "Hua Zhou, Yaqi Zhang, Lulu Zhao, dongguanting, Chaox72",
        "link": "https://arxiv.org/abs/2412.11231",
        "github_repo": "https://github.com/HypherX/Evolution-Analysis",
        "summary": "- This paper investigates the effectiveness of smaller language models (SLMs) compared to larger language models (LLMs) in evolving more complex and diverse instructions for instruction tuning.\n- Through experiments across three instruction evolution scenarios (Evol-Instruct, AutoIF, and Auto Evol-Instruct), the study demonstrates that SLMs outperform LLMs in evolving instructions, leading to better performance in downstream tasks including instruction following, mathematical reasoning, and code generation.\n- The authors hypothesize that SLMs' broader output space during instruction generation, due to their relatively weaker instruction-following capabilities compared to LLMs, results in more complex and diverse instructions.\n- They propose a new metric called Instruction Complex-Aware IFD (IC-IFD), incorporating instruction complexity into the original IFD score for a more accurate evaluation of instruction data effectiveness without requiring instruction tuning.\n- Experimental results demonstrate that SLMs generate more complex and diverse instructions than LLMs leading to improved performance in downstream tasks.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/HypherX/Evolution-Analysis"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations",
        "authors": "Jiaqiwang, Dubhe-zmc, jingtan, tongwu2020, lizb6626",
        "link": "https://arxiv.org/abs/2412.12083",
        "github_repo": null,
        "summary": "- IDArb, a diffusion-based model, performs intrinsic decomposition from an arbitrary number of images with varying illumination, generating albedo, normal, metallic, and roughness maps.\n- It uses a cross-view, cross-component attention mechanism within a UNet architecture, adapted from Stable Diffusion, to fuse information across views and intrinsic components, promoting consistency.\n- A novel illumination-augmented, view-adaptive training strategy, using a new dataset called ARB-Objaverse, enhances robustness under different lighting and view conditions.\n- Evaluation on synthetic and real-world data shows IDArb significantly outperforms existing methods quantitatively and qualitatively, demonstrating state-of-the-art intrinsic decomposition.\n- This facilitates downstream tasks like relighting, material editing, photometric stereo, and 3D reconstruction, also improving optimization-based inverse rendering.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models",
        "authors": "howang, yuxiaod, lrxl, wangcunxiang, CCCCCC",
        "link": "https://arxiv.org/abs/2412.11605",
        "github_repo": "https://github.com/thu-coai/SPaR",
        "summary": "- This paper introduces SPaR, a self-play framework that uses tree-search refinement to enhance the instruction-following capabilities of Large Language Models (LLMs).\n- SPaR involves an actor LLM generating responses and a refiner LLM critiquing and refining them through a tree-search process to create preference pairs for training.\n- This method aims to highlight key differences for instruction following by minimizing extraneous variations often present in independently sampled responses used by other preference learning methods.\n- Experiments demonstrate that a LLaMA-8B model trained with SPaR surpasses GPT-4-Turbo on the IFEval benchmark and shows promising scalability with larger models like LLaMA3-70B.\n- The study also finds that scaling inference in tree search improves performance, and the refiner's abilities can exceed the initially distilled LLM, suggesting potential for continuous self-improvement.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/thu-coai/SPaR"
        ],
        "date": "2024-12-17"
    },
    {
        "title": "Wonderland: Navigating 3D Scenes from a Single Image",
        "authors": "Hanwen Liang, ZanyRumata, guochengqian, vidit98, jlcao2",
        "link": "https://arxiv.org/abs/2412.12091",
        "github_repo": null,
        "summary": "- Wonderland introduces a novel pipeline for generating high-quality 3D scenes from a single image in a feed-forward manner, overcoming limitations of existing methods like multi-view data requirements and per-scene optimization.\n- It leverages a camera-guided video diffusion model with dual-branch conditioning for generating 3D-aware video latents, capturing multi-view information while ensuring 3D consistency.\n- A novel latent-based large reconstruction model (LaLRM) then efficiently decodes these video latents into 3D Gaussian Splattings (3DGS), achieving significant compression and facilitating wide-scope scene representation.\n- Extensive evaluations demonstrate that Wonderland significantly outperforms state-of-the-art methods on benchmark datasets for single-view 3D scene generation, especially in out-of-domain images, demonstrating superior visual quality, wider scope, and efficiency.\n- The model effectively leverages video diffusion model latents for 3D reconstruction, enabling high-fidelity 3D scene generation from single images.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://snap-research.github.io/wonderland/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs",
        "authors": "junweiliang, StarYDY, zhifeichen097, spongy, Xxlbigbrother",
        "link": "https://arxiv.org/abs/2412.11258",
        "github_repo": null,
        "summary": "- GaussianProperty is a training-free framework that predicts physical properties of materials for 3D Gaussians using Segment Anything (SAM) and GPT-4V(ision).\n- It employs a global-local reasoning module for 2D images by leveraging SAM's segmentation capability and GPT-4V's recognition capability to estimate physical properties.\n- These properties are then projected from multi-view 2D images to 3D Gaussians using a voting strategy.\n-  The framework enables applications in physics-based dynamic simulation by leveraging Material Point Method (MPM) and robot grasping by developing a grasping force prediction strategy based on the estimated properties.\n- Experiments on material segmentation, dynamic simulation, and real-world robotic grasping demonstrate the effectiveness of GaussianProperty in enhancing downstream tasks.",
        "classification": [
            "Computer Vision",
            "Multimodal",
            "Image-to-3D",
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator",
        "authors": "Xiaozhe Ren, Yihang Gao, Jiawei Li, Guoxuan Chen, shihan96",
        "link": "https://arxiv.org/abs/2412.12094",
        "github_repo": null,
        "summary": "- SepLLM is a plug-and-play framework that accelerates LLM inference by compressing segments of text into separator tokens and eliminating redundant tokens. \n- It leverages a data-dependent sparse attention mechanism, retaining only initial, neighboring, and separator tokens and implementing efficient kernels for training acceleration.\n- Experimental results show that using the Llama-3-8B backbone, SepLLM can reduce KV cache by over 50% while maintaining comparable performance on GSM8K-CoT. \n- In streaming settings, SepLLM can effectively process sequences of up to 4 million tokens or more. \n- SepLLM addresses the limitations of other methods by maintaining consistent performance between training and inference and by achieving substantial reductions in computational costs and training time.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "sepllm.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture",
        "authors": "wubingheng, JingzeShi",
        "link": "https://arxiv.org/abs/2412.11834",
        "github_repo": "https://github.com/LoserCheems/Doge",
        "summary": "- This paper introduces Wonderful Matrices, a novel foundation model architecture combining sequence and state transformations for enhanced efficiency and effectiveness in language modeling.\n- The architecture integrates Rotary Position Embedding (ROPE) for unified positional encoding in hybrid algorithms, Dynamic Mask Attention (DMAttn) for selective filtering of past states, and Cross Domain Mixture of Experts (CDMOE) for reduced parameter redundancy and efficient expert retrieval.\n- The paper demonstrates the effectiveness of each individual module (ROPE, DMAttn, CDMOE) through empirical validation, showing improvements in perplexity and multi-query associative recall.\n- Experimental results on language modeling tasks demonstrate that Wonderful Matrices outperforms other architectures like LlaMa3, Mamba2, and Jamba across various evaluation metrics, especially with increasing parameter scale.\n- The architecture uses a combination of State Space Duality (SSD) and DMAttn modules for sequence transformation and CDMOE modules for state transformation, achieving a balance between efficiency and performance.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/LoserCheems/Doge"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors",
        "authors": "Jian Yang, Zeyu Cai, yingtai, JesseZhang, XiaokunSun",
        "link": "https://arxiv.org/abs/2412.11586",
        "github_repo": null,
        "summary": "- StrandHead is a novel text-to-3D head avatar generation framework that creates disentangled 3D hair with strand-level representations using geometric priors, eliminating the need for 3D hair training data by leveraging pre-trained 2D diffusion models.\n- A differentiable prismatization algorithm converts hair strands into watertight prismatic meshes, facilitating strand-level modeling, and uses mesh-based renderers, and physics-based simulations.\n- The framework incorporates orientation consistency and curvature regularization losses to maintain realistic hair strand distributions and overall hairstyle shapes.\n- StrandHead achieves state-of-the-art results in generating realistic and diverse 3D heads and hair, outperforming existing methods in visual quality and text alignment.\n- It supports flexible hairstyle transfer, editing, and physics-based rendering and simulation, broadening its applications in various fields, including gaming and virtual reality.",
        "classification": [
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes",
        "authors": "YuLiu, BuzzBeater, JunfengNi, YixinChen, JasonAplp",
        "link": "https://arxiv.org/abs/2412.11457",
        "github_repo": null,
        "summary": "- MOVIS, a novel view synthesis model, enhances structural awareness in diffusion models for multi-object scenes by incorporating depth and object masks as input features and predicting novel view object masks as an auxiliary task.\n- A structure-guided timestep sampling scheduler balances the learning of global object placement and fine-grained detail recovery during training.\n- MOVIS outperforms baseline models on various synthetic and real-world datasets, including C3DFS, Objaverse, Room-Texture, 3D-FRONT, and SUNRGB-D, demonstrating superior performance in novel view synthesis and cross-view consistency.\n- The model's ability to generate plausible novel views with consistent object placement, geometry, and appearance, as evidenced by qualitative and quantitative results (PSNR, SSIM, LPIPS, IoU, Hit Rate, matching distance), highlights its potential for 3D-aware multi-object tasks.\n- MOVIS exhibits strong generalization capability by effectively synthesizing novel views on unseen datasets, showcasing its robustness and potential for broader application.",
        "classification": [
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
        "authors": "prateekv",
        "link": "https://arxiv.org/abs/2412.11449",
        "github_repo": null,
        "summary": "- WHISPER-GPT, a novel hybrid large language model (LLM) for speech and music generation, leverages continuous audio representations (mel-spectrograms) alongside discrete acoustic tokens within a single Transformer decoder-only architecture.\n- This hybrid approach addresses context length limitations encountered in purely discrete token-based LLMs by incorporating continuous audio information while retaining the advantages of discrete tokens for sampling and generation.\n- Experimental results on LibriSpeech TTS and a music dataset demonstrate that WHISPER-GPT with 4M parameters achieves comparable performance to a 40M parameter purely token-based LLM, showcasing the efficiency of the hybrid representation.\n- The model predicts the next token given the past acoustic tokens and mel-spectrogram slices, improving the next token prediction metrics like negative log-likelihood and perplexity.\n- Future work involves using this hybrid LLM to fine-tune other audio tasks such as generating multi-scale acoustic tokens and generate high-fidelity audio samples conditioned on them.",
        "classification": [
            "Audio",
            "Text-to-Audio",
            "Text-to-Speech"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning",
        "authors": "Yihuai Gao, Aaditya Prasad, Robert Holmberg, William Chong, jimmyyhwu",
        "link": "https://arxiv.org/abs/2412.10447",
        "github_repo": null,
        "summary": "- This research paper introduces TidyBot++, an open-source, inexpensive, robust, and flexible holonomic mobile manipulator designed for robot learning research, particularly for real-world household tasks.\n- The key feature of TidyBot++ is its holonomic base, enabled by powered casters, which allows independent and simultaneous control of all planar degrees of freedom, enhancing maneuverability and simplifying mobile manipulation tasks compared to nonholonomic bases.\n- The robot is equipped with a user-friendly mobile phone teleoperation interface using WebXR, enabling easy data collection for imitation learning, and experiments demonstrated successful training of policies for various household tasks.\n- A head-to-head comparison with a differential drive base showed the advantages of holonomic drive in terms of efficiency and policy learning performance, especially for tasks requiring lateral movement.\n- The design prioritizes research flexibility, using easily modifiable frames and readily available components from the FIRST Robotics Competition ecosystem, making it highly customizable and easy to assemble and repair.",
        "classification": [
            "Robotics",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "http://tidybot2.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning",
        "authors": "Aleksandr Beznosikov, Philip Zmushko, pichuginad, Andron00e",
        "link": "https://arxiv.org/abs/2412.11689",
        "github_repo": null,
        "summary": "- This paper explores the vulnerability of Vertical Federated Learning (VFL), particularly Split Learning (SL), to feature reconstruction attacks, focusing on Model Inversion (MI) and Feature-space Hijacking (FSHA).\n- It theoretically and experimentally demonstrates that these attacks are ineffective against Multilayer Perceptron (MLP)-based client-side models due to the inability to exploit prior knowledge of data distribution in the absence of dense layers before the cut layer.\n- The paper suggests that the success of existing attacks is largely attributed to the convolutional nature of client models used in image-based VFL, and argues that MLP-based models offer inherent data protection without requiring additional defense mechanisms.\n- Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets using both CNN and MLP-based client models validate the hypothesis, demonstrating that MLP models prevent feature reconstruction while maintaining comparable accuracy.\n- Furthermore, the use of Fr\u00e9chet Inception Distance (FID) is proposed as a more suitable metric for evaluating the effectiveness of defenses against these attacks compared to Mean Squared Error (MSE), especially for complex images.",
        "classification": [
            "Computer Vision",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/Andron00e/JAST"
        ],
        "huggingface_urls": [],
        "date": "2024-12-17"
    },
    {
        "title": "Are Your LLMs Capable of Stable Reasoning?",
        "authors": "Linchen Xiao, Hongwei Liu, Junnan Liu, zsytony, Harold-lkk",
        "link": "https://arxiv.org/abs/2412.13147",
        "github_repo": "https://github.com/open-compass/GPassK",
        "summary": "- This paper introduces G-Pass@k, a novel evaluation metric designed to assess both the potential and stability of Large Language Models (LLMs) in complex reasoning tasks, particularly mathematical problem-solving.\n- G-Pass@k quantifies an LLM's consistency in generating correct solutions across multiple generations by considering varying thresholds of correctness, thereby capturing limitations in traditional metrics like Greedy Accuracy and Pass@k, which often overlook output stability.\n- A new dynamic benchmark called LiveMathBench is introduced, comprising challenging mathematical problems from various competitions to minimize data leakage and ensure relevance to the latest advancements in LLM capabilities. \n- Through extensive experiments on LiveMathBench and other datasets, the paper reveals that current LLMs, including specialized and chain-of-thought enhanced models, exhibit significant instability in their reasoning abilities, with performance drops of up to 90% in challenging scenarios. \n- The findings underscore the inadequacy of conventional evaluation methods and highlight the need for stability-aware metrics like G-Pass@k for a more realistic assessment of LLM capabilities in complex reasoning tasks.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/open-compass/GPassk",
            "https://github.com/open-compass/GPassK"
        ],
        "huggingface_urls": [],
        "date": "2024-12-18"
    },
    {
        "title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models",
        "authors": "Xiaoshuai Song, Zhuoma GongQue, Runqi Qiao, Shanglin Lei, YiFan Zhang",
        "link": "https://arxiv.org/abs/2412.12606",
        "github_repo": null,
        "summary": "- This paper introduces the Multi-Dimensional Insights (MDI) benchmark, a new benchmark for evaluating large multimodal models (LMMs) on real-world personalization tasks.\n- The MDI benchmark consists of over 500 images and 1.2k human-posed questions across six common real-world scenarios, focusing on two key dimensions: question complexity and age demographics.\n- Questions are categorized into simple and complex levels to assess basic understanding and reasoning abilities, respectively, while also being stratified across young, middle-aged, and older age groups to evaluate personalized responses.\n- Initial evaluations using the MDI benchmark reveal that while strong models like GPT-4 achieve a 79% accuracy on age-related tasks, there remains significant room for improvement in addressing the diverse needs and preferences of different age groups in real-world scenarios.\n- The MDI benchmark aims to foster development towards reliable, personalized human assistants by offering a comprehensive evaluation framework covering a broad spectrum of real-world personalized needs.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-18"
    },
    {
        "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain",
        "authors": "Ji-Rong Wen, Zhicheng Dou, Jiejun Tan, ShootingWong",
        "link": "https://arxiv.org/abs/2412.13018",
        "github_repo": "https://github.com/RUC-NLPIR/OmniEval",
        "summary": "- This paper introduces OmniEval, an automatic and omnidirectional benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in the financial domain.\n- The benchmark employs a matrix-based evaluation system categorizing queries into five tasks and 16 financial topics for a comprehensive assessment of diverse query scenarios.\n- It uses a multi-dimensional data generation approach combining GPT-4-based automatic generation and human annotation, achieving an 87.47% acceptance ratio in human evaluations.\n- A multi-stage evaluation system assesses both retrieval and generation performance, and robust evaluation metrics from rule-based (MAP, Rouge) and LLM-based methods ensure reliable assessment.\n- Experiments on various retrievers and LLMs demonstrate OmniEval's comprehensiveness and highlight performance variations across topics and tasks, showing improvement opportunities for RAG systems in the financial domain.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/RUC-NLPIR/OmniEval"
        ],
        "huggingface_urls": [],
        "date": "2024-12-18"
    },
    {
        "title": "Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers",
        "authors": "Pulkit Agrawal, Jeff Gore, Jinyeop Song, Seungwook Han",
        "link": "https://arxiv.org/abs/2412.12276",
        "github_repo": null,
        "summary": "- This paper proposes a \"concept encoding-decoding\" mechanism to explain how transformers perform in-context learning (ICL).\n- The core idea is that transformers learn to encode different latent concepts (e.g., grammatical rules or arithmetic operations) into distinct, separable representations, and simultaneously develop concept-specific decoding algorithms.\n- Through experiments on synthetic and natural ICL tasks (part-of-speech tagging and bitwise arithmetic), the authors show that this mechanism emerges during training and exists in pretrained language models of varying scales (Gemma-2 and Llama).\n- They introduce a metric called \"Concept Decodability\" (CD) to quantify the separability of latent concepts in representations and demonstrate that CD is predictive of ICL performance.\n- Causal interventions and finetuning experiments further validate that concept encoding is causally related to and predictive of ICL performance.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-18"
    },
    {
        "title": "MIVE: New Design and Benchmark for Multi-Instance Video Editing",
        "authors": "Munchurl Kim, Jihyong Oh, Soo Ye Kim, Agus Gunawan, Samuel Teodoro",
        "link": "https://arxiv.org/abs/2412.12877",
        "github_repo": null,
        "summary": "- MIVE, a novel mask-based zero-shot multi-instance video editing framework, enables users to specify localized edits to multiple objects within a video using individual instance captions and masks.\n- The framework incorporates two key modules: Disentangled Multi-Instance Sampling (DMS) to minimize attention leakage between edited instances, and Instance-centric Probability Redistribution (IPR) to improve the accuracy and faithfulness of edits.\n- A new benchmark dataset, MIVE Dataset, featuring 200 diverse videos with instance-level captions and masks is introduced, alongside a novel Cross-Instance Accuracy (CIA) score to evaluate attention leakage.\n- Extensive quantitative and qualitative results on MIVE Dataset and a user study demonstrate that MIVE outperforms state-of-the-art methods in terms of editing faithfulness, accuracy, and leakage prevention.\n- MIVE sets a new benchmark for multi-instance video editing, showcasing its ability to disentangle multiple edits and generate faithful localized modifications in videos.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://kaist-viclab.github.io/mive-site/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-18"
    },
    {
        "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks",
        "authors": "Kritanjali Jain, Yuxuan Tang, Boxuan Li, Yufan Song, Frank F. Xu",
        "link": "https://arxiv.org/abs/2412.14161",
        "github_repo": null,
        "summary": "- This paper introduces TheAgentCompany, a benchmark for evaluating AI agents on real-world tasks simulating a software company environment.\n- The benchmark includes 175 tasks across various job functions like software engineering, project management, and finance, requiring agents to interact with web interfaces, code, and simulated colleagues via chat and email.\n- The evaluation includes both autonomous completion and partial credit based on checkpoint achievements, assessing the agents' ability to manage complex workflows.\n- Experiments with different LLMs (Claude, Gemini, GPT-40, Llama, Qwen) reveal that even the best model (Claude 3.5 Sonnet) achieves only 24% full and 34.4% partial completion, showing limitations in tasks demanding social interactions and handling complex interfaces.\n- Despite the leading LLM's strong performance, the high cost per task ($6.34) highlights the need for further research and optimization of cost-effectiveness in real-world deployments.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/TheAgentCompany/TheAgentCompany",
            "https://github.com/TheAgentCompany/experiments"
        ],
        "date": "2024-12-19"
    },
    {
        "title": "AniDoc: Animation Creation Made Easier",
        "authors": "Wen Wang, Qiuyu Wang, Hanlin Wang, Hao Ouyang, Yihao Meng",
        "link": "https://arxiv.org/abs/2412.14173",
        "github_repo": null,
        "summary": "- AniDoc is a novel video line art colorization tool based on a video diffusion model that automatically transforms sketch sequences into colored animations guided by a reference character design.\n- It incorporates a correspondence-matching mechanism to address misalignment between the character design and sketches, enhancing color accuracy and consistency.\n- The model uses binarized sketches and background augmentation during training to reflect real-world scenarios and improve robustness.\n- A two-stage training strategy allows for sparse sketch input, enabling automated interpolation and colorization.\n- AniDoc demonstrates superior performance in qualitative and quantitative comparisons with existing approaches, showcasing high fidelity to the reference character design and temporal coherence.",
        "classification": [
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "FashionComposer: Compositional Fashion Image Generation",
        "authors": "Hao Luo, Xiaogang Xu, Xi Chen, Yiyang Wang, Sihui Ji",
        "link": "https://arxiv.org/abs/2412.14168",
        "github_repo": null,
        "summary": "- FashionComposer is a novel diffusion-based model for compositional fashion image generation that takes multi-modal inputs such as text descriptions, parametric human models, garment images, and face images.\n- It employs a universal framework with a reference UNet and subject-binding attention to handle diverse input modalities and compose multiple visual assets in one pass, supporting applications like virtual try-on and album generation.\n- The model is trained on a scaled dataset constructed using existing datasets augmented with masked garments and generated captions, showing superior performance in multi-reference customization compared to existing methods like Emu2 and Collage Diffusion.\n- For consistent human image generation in albums, FashionComposer introduces correspondence-aware attention and latent code alignment to maintain both consistency and fidelity.\n- In virtual try-on tasks, FashionComposer achieves state-of-the-art results, outperforming other methods on standard benchmarks like VITON-HD for single garment and showing promising results on DressCode for multi-garment and outfit try-on scenarios.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning",
        "authors": "Rudolf Lioutikov, Pulkit Agrawal, Jyothish Pari, Moritz Reuss",
        "link": "https://arxiv.org/abs/2412.12953",
        "github_repo": null,
        "summary": "- This paper introduces Mixture-of-Denoising Experts (MoDE), a novel Diffusion Policy architecture that leverages a Mixture of Experts (MoE) Transformer with noise-conditioned routing for efficient and scalable multitask imitation learning.\n- MoDE incorporates noise-conditioned self-attention and expert caching, reducing active parameters by 40% and inference costs by 90% compared to dense Transformer baselines.\n- It achieves state-of-the-art performance on 134 tasks across four imitation learning benchmarks (CALVIN and LIBERO), surpassing both CNN and Transformer-based Diffusion Policies.\n- Pretraining MoDE on diverse robotics data leads to substantial performance gains, reaching scores of 4.01 on CALVIN ABC and 0.95 on LIBERO-90.\n- Ablation studies highlight the importance of noise-conditioned routing and specialized experts for efficient and robust action generation in diffusion policies.",
        "classification": [
            "Robotics",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://mbreuss.github.io/MoDE_Diffusion_Policy/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation",
        "authors": "Jiaming Sun, Songyou Peng, Jingxiao Chen, Sida Peng, Haotong Lin",
        "link": "https://arxiv.org/abs/2412.14015",
        "github_repo": null,
        "summary": "- This paper introduces Prompt Depth Anything, a new paradigm for metric depth estimation by prompting a depth foundation model with low-cost LiDAR depth as the metric prompt.\n- It proposes a concise prompt fusion architecture that integrates LiDAR depth at multiple scales within a DPT decoder, along with a scalable data pipeline that includes synthetic LiDAR simulation and pseudo ground truth generation from 3D reconstruction using real data.\n- An edge-aware depth loss is also introduced to address the limitations of pseudo ground truth.\n- The approach achieves state-of-the-art results on standard benchmarks like ARKitScenes and ScanNet++ across different metrics.\n- It shows practical benefits for downstream applications including 3D reconstruction and generalized robotic object grasping.",
        "classification": [
            "Depth Estimation",
            "Computer Vision"
        ],
        "github_urls": [
            "https://PromptDA.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities",
        "authors": "Loic Landrieu, Clement Mallet, Nicolas Gonthier, Guillaume Astruc",
        "link": "https://arxiv.org/abs/2412.14123",
        "github_repo": "https://github.com/gastruc/AnySat",
        "summary": "- AnySat, a novel multimodal and multiresolution model for Earth Observation, is introduced, leveraging a Joint Embedding Predictive Architecture (JEPA) and scale-adaptive spatial encoders.\n- Trained on GeoPlex, a diverse dataset comprising various modalities, resolutions, and scales, the model demonstrates state-of-the-art performance across several downstream tasks, including land cover mapping, tree species identification, and flood segmentation.\n- AnySat's versatility allows it to seamlessly handle diverse EO datasets with varying properties and modalities, eliminating the need for dataset-specific retraining.\n- Evaluation on GeoPlex and external datasets showcases performance improvements, particularly in classification tasks and smaller datasets, due to the enhanced representation learning from diverse data sources.\n- AnySat's efficiency allows for linear probing for semantic segmentation with competitive results, reducing training costs significantly and opening possibilities for wider application in environmental monitoring.",
        "classification": [
            "Image Segmentation",
            "Image Classification",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/gastruc/AnySat"
        ],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "GUI Agents: A Survey",
        "authors": "Namyong Park, Gang Wu, Yu Wang, Jian Chen, dangmn",
        "link": "https://arxiv.org/abs/2412.13501",
        "github_repo": null,
        "summary": "- This survey paper provides a comprehensive overview of Graphical User Interface (GUI) agents, which leverage Large Foundation Models (LFMs) to automate human-computer interaction.\n- It categorizes GUI agents based on benchmarks, evaluation metrics, architectures (perception, reasoning, planning, and acting), and training methods, proposing a unified framework for understanding their capabilities.\n- The paper discusses various datasets and interactive environments used for evaluating GUI agents, distinguishing between closed-world and open-world settings, and static and dynamic environments.\n- It also covers different architectural designs for perception (accessibility-based, HTML/DOM-based, screen-visual-based, and hybrid), reasoning, planning (with internal and external knowledge), and acting modules.\n- Finally, the survey summarizes training methods, including prompt-based and training-based approaches (pre-training, fine-tuning, and reinforcement learning), and identifies open challenges and future research directions in GUI agent research, such as intent understanding, security, and latency.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment",
        "authors": "Yubo Chen, Pengfei Cao, Tianyi Men, Hongbang Yuan, Zhuoran Jin",
        "link": "https://arxiv.org/abs/2412.13746",
        "github_repo": null,
        "summary": "- This paper introduces RAG-RewardBench, the first benchmark designed for evaluating reward models (RMs) within Retrieval Augmented Generation (RAG) settings, aiming to improve preference alignment between RAG models and human preferences.\n- The benchmark includes 1,485 preference pairs across four RAG-specific scenarios: multi-hop reasoning, fine-grained citation, appropriate abstaining, and conflict robustness, sourced from 18 datasets using six retrievers and 24 RALMs.\n- An LLM-as-a-judge approach is employed to enhance preference annotation efficiency and achieve a strong correlation (0.84 Pearson correlation) with human annotations.\n- Evaluation results on 45 existing RMs show the top-ranked model reaches only 78.3% accuracy, highlighting the benchmark's challenging nature and the need for RMs specifically tailored for RAG.\n- The paper finds that existing trained RALMs demonstrate minimal improvement (0.6%) in preference alignment over base LLMs based on their performance on RAG-RewardBench, suggesting a need to shift training towards preference-aligned approaches.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/jinzhuoran/RAG-RewardBench"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/"
        ],
        "date": "2024-12-19"
    },
    {
        "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN",
        "authors": "Shiwei Liu, Lu Yin, Pengxiang Li",
        "link": "https://arxiv.org/abs/2412.13795",
        "github_repo": "https://github.com/pixeli99/MixLN",
        "summary": "- Mix-LN, a novel normalization technique for Large Language Models (LLMs), combines Pre-LN and Post-LN to address the inefficiency of deeper layers often observed in LLMs trained with Pre-LN.\n- Mix-LN applies Post-LN to early layers and Pre-LN to deeper layers, promoting more uniform gradients and enabling effective contribution from all layers during training.\n- Experiments across various model sizes (70M to 7B parameters) show Mix-LN consistently outperforms Pre-LN, Post-LN, and their variants, improving pre-training perplexity and demonstrating better performance in supervised fine-tuning and reinforcement learning from human feedback.\n- The improved performance is attributed to Mix-LN's ability to promote healthier gradient norms and representation diversity across all layers, leading to more effective learning and generalization.\n- The study highlights the importance of optimizing normalization techniques in LLMs to fully leverage the potential of deep layers and improve overall model capacity and efficiency.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/pixeli99/MixLN"
        ],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "Learning from Massive Human Videos for Universal Humanoid Pose Control",
        "authors": "Junjie Ye, Tianheng Shi, Siqi Song, Siheng Zhao, Jiageng Mao",
        "link": "https://arxiv.org/abs/2412.14172",
        "github_repo": null,
        "summary": "- This paper introduces Humanoid-X, a large-scale dataset with over 20 million humanoid robot poses and corresponding text descriptions, designed for universal humanoid pose control.\n- A new large humanoid model, UH-1, is proposed. UH-1 uses a Transformer architecture to translate text instructions into corresponding actions for controlling humanoid robots. It supports both text-to-keypoint and text-to-action control modes.\n- UH-1 is trained on Humanoid-X and shows strong generalization in text-based humanoid control, outperforming existing two-stage methods on the HumanoidML3D benchmark by over 23% in FID score.\n- Extensive simulated and real-world experiments demonstrate that UH-1 can reliably translate textual commands into diverse and accurate humanoid actions, achieving nearly 100% success rate in real-world deployment.\n- The scalability of Humanoid-X is demonstrated to improve model performance by training UH-1 on various dataset sizes.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers",
        "authors": "Yupeng Shi, Zhi-Fan Wu, Wei Wang, Lianghua Huang, bibona",
        "link": "https://arxiv.org/abs/2412.12571",
        "github_repo": "https://github.com/ali-vilab/ChatDiT",
        "summary": "- ChatDiT is a novel zero-shot, general-purpose, interactive visual generation framework built upon pre-trained diffusion transformers (DiTs) without requiring fine-tuning or architectural modifications.\n- It leverages the inherent in-context generation capabilities of DiTs, allowing users to create complex multi-image outputs, edit images, generate illustrated articles, and design character settings through free-form natural language interaction.\n- This is achieved using a multi-agent system composed of an Instruction-Parsing Agent, a Strategy-Planning Agent, and an Execution Agent, which collaboratively interpret instructions, formulate generation plans, and execute actions using an in-context toolkit of DiTs.\n- Evaluation on IDEA-Bench shows that ChatDiT outperforms existing methods, including specialized multi-task frameworks and rephrasing-based models, achieving a top score of 23.19 out of 100.\n- Despite its strong performance, certain limitations exist, such as difficulty in preserving fine details and identity, especially when handling long contexts with multiple subjects or elements, highlighting areas for future research.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/ali-vilab/ChatDiT"
        ],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "VidTok: A Versatile and Open-Source Video Tokenizer",
        "authors": "Li Song, Xinle Cheng, Junliang Guo, Tianyu He, Anni Tang",
        "link": "https://arxiv.org/abs/2412.13061",
        "github_repo": "https://github.com/microsoft/VidTok",
        "summary": "- VidTok is a versatile and open-source video tokenizer designed for both continuous and discrete tokenization.\n- It uses a novel architecture that handles spatial and temporal sampling separately for efficiency, using 2D convolutions and an AlphaBlender operator.\n- For discrete tokenization, VidTok leverages Finite Scalar Quantization (FSQ) to improve training stability and codebook utilization.\n- A two-stage training strategy, involving pre-training on low-resolution videos and fine-tuning the decoder on high-resolution videos, further enhances performance.\n- Experimental results demonstrate state-of-the-art performance across various metrics, including PSNR, SSIM, LPIPS, and FVD, outperforming existing models on standard benchmarks and a web video dataset.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/microsoft/VidTok"
        ],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
        "authors": "Anis Kacem, Kseniya Cherenkova, Dimitrios Mallis, Elona Dupont, Danila Rukhovich",
        "link": "https://arxiv.org/abs/2412.14042",
        "github_repo": null,
        "summary": "- CAD-Recode is a novel Large Language Model (LLM)-based method for reconstructing Computer-Aided Design (CAD) models from 3D point clouds.\n- The architecture comprises a point cloud projector that converts a point cloud into input tokens for the LLM and an LLM fine-tuned to decode these tokens into Python code using the CadQuery library.\n- CAD-Recode is trained on a new synthetic dataset of one million CAD sketch-extrude sequences represented as Python code.\n- On DeepCAD and Fusion360 datasets, CAD-Recode achieves 10x lower mean Chamfer Distance than existing methods, demonstrating significantly improved geometric fidelity and setting a new state-of-the-art.\n- Additionally, the generated Python code is interpretable by off-the-shelf LLMs, allowing for CAD editing and CAD-specific question answering directly from the point cloud.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge",
        "authors": "Shuai Zhao, Ruiwen Zhou, Yuxi Xie, Liangming Pan, Xiaobao Wu",
        "link": "https://arxiv.org/abs/2412.13670",
        "github_repo": null,
        "summary": "- This paper introduces AntiLeak-Bench, an automated anti-leakage benchmarking framework for Large Language Models (LLMs).\n- It addresses data contamination issues in LLM evaluation by constructing test samples with updated real-world knowledge, ensuring the knowledge is absent from LLMs' training sets.\n- A fully automated workflow is designed to build and update the benchmark, eliminating the need for human labor and reducing maintenance costs.\n- Experiments with various LLMs demonstrate a performance drop after the cutoff time, highlighting data contamination issues in LLM evaluations.\n- Results manifest the effectiveness of AntiLeak-Bench for contamination-free evaluation.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/bobxwu/AntiLeak-Bench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-19"
    },
    {
        "title": "Qwen2.5 Technical Report",
        "authors": "Losin94, bowenYu, bzheng, huybery, Baosong",
        "link": "https://arxiv.org/abs/2412.15115",
        "github_repo": null,
        "summary": "- Qwen2.5 is a series of large language models (LLMs) trained on 18 trillion tokens of data, improving upon its predecessor Qwen2 through enhanced pre-training and post-training techniques.\n- The models range from 0.5B to 72B parameters in open-weight offerings and include Mixture-of-Experts (MoE) models, Qwen2.5-Turbo and Qwen2.5-Plus, for hosted solutions.\n- Qwen2.5-72B-Instruct demonstrates competitive performance against Llama-3-405B-Instruct, a model five times its size.\n- Qwen2.5-Turbo and Qwen2.5-Plus exhibit superior cost-effectiveness while competing with GPT40-mini and GPT40 respectively.\n- Qwen2.5 also serves as a foundation for specialized models like Qwen2.5-Math and Qwen2.5-Coder, broadening its applicability to specific domains.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/QwenLM/Qwen2.5"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Qwen"
        ],
        "date": "2024-12-20"
    },
    {
        "title": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval",
        "authors": "BoZhaoHuggingFace, yzwang, Shitao, zl101, JUNJIE99",
        "link": "https://arxiv.org/abs/2412.14475",
        "github_repo": null,
        "summary": "- MegaPairs, a novel data synthesis method that uses vision-language models (VLMs) and open-domain images with a massive synthetic dataset for universal multimodal retrieval is proposed.\n- This method constructs heterogeneous KNN triplets using three similarity models (CLIP vision encoder, DINO vision encoder, and CLIP text encoder) to sample correlated image pairs.\n- It then utilizes MLLM and LLM annotators for relationship description and pseudo retrieval instruction generation resulting in triplets (Image query, Text instruction, Image target). \n- MMRet models trained on MegaPairs demonstrate SOTA zero-shot results on 4 Composed Image Retrieval benchmarks and MMEB's 36 datasets, outperforming baselines by 8.1% on CIRCO using 70x less data. \n- Further downstream fine-tuning shows that the model maintains leading performance on the benchmarks mentioned above.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Progressive Multimodal Reasoning via Active Retrieval",
        "authors": "douzc, yutaozhu94, dengmengjie, Snow-Nation, dongguanting",
        "link": "https://arxiv.org/abs/2412.14835",
        "github_repo": null,
        "summary": "- This paper introduces AR-MCTS, a framework designed to improve multi-step multimodal reasoning in Multimodal Large Language Models (MLLMs) by combining Active Retrieval (AR) and Monte Carlo Tree Search (MCTS).\n- AR-MCTS employs a unified retrieval module to gather key insights from a hybrid-modal corpus, aiding in problem-solving.\n- It utilizes MCTS with active retrieval to automatically generate step-wise annotations, enhancing the diversity and reliability of the reasoning process.\n- A process reward model (PRM) is progressively aligned through step-wise Direct Preference Optimization (DPO) and Supervised Fine-tuning (SFT) for automated verification.\n- Experimental results across various MLLMs and benchmarks show AR-MCTS's effectiveness in boosting performance, optimizing sampling diversity and accuracy, and demonstrating improvement in complex reasoning scenarios, particularly on WE-MATH's S3 metrics and general reasoning tasks like GAOKAO-MM.",
        "classification": [
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
        "authors": "wangxz098, haopeng01, NeoZ123, tsq2000, bys0318",
        "link": "https://arxiv.org/abs/2412.15204",
        "github_repo": null,
        "summary": "- Introduces LongBench v2, a challenging benchmark designed to evaluate the deep understanding and reasoning capabilities of Large Language Models (LLMs) in long-context scenarios across diverse real-world tasks.\n- The benchmark consists of 503 multiple-choice questions spanning six major task categories, with contexts ranging from 8k to 2M words, focusing on complex reasoning rather than simple information retrieval.\n- Data collection involves nearly 100 highly educated individuals and employs rigorous automated and manual review processes, resulting in a high-quality dataset where even human experts achieve only 53.7% accuracy under time constraints.\n- Evaluation shows that the best-performing model achieves 57.7% accuracy, surpassing the human baseline by 4% when leveraging chain-of-thought prompting during inference.\n- The results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle long-context challenges and call for further exploration in this direction.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/THUDM/LongBench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "How to Synthesize Text Data without Model Collapse?",
        "authors": "XingtaiHF, iseesaw, Hengli, daixuancheng, xuekai",
        "link": "https://arxiv.org/abs/2412.14689",
        "github_repo": null,
        "summary": "- This paper proposes ToEdit (Token Editing), a novel technique for synthesizing text data that mitigates model collapse, a degenerative process where language models overfit to synthetic data distributions.\n- ToEdit employs token-level editing on human-produced data, guided by a pre-trained language model's probability distribution, to create semi-synthetic data.\n- This method theoretically constrains the test error within a fixed upper bound, preventing the error accumulation observed in iterative training on synthetic data.\n- Experimental results across pre-training, continual pre-training, and supervised fine-tuning demonstrate that ToEdit enhances model performance compared to using purely synthetic or mixed synthetic and human-produced data.\n- Statistical analyses reveal that synthetic data suffers from coverage collapse and over-concentration of n-gram features, issues addressed by ToEdit's approach.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Flowing from Words to Pixels: A Framework for Cross-Modality Evolution",
        "authors": "Andrew Brown, Alan Yuille, Xi Yin, mannatsingh, QHL067",
        "link": "https://arxiv.org/abs/2412.15213",
        "github_repo": null,
        "summary": "- CrossFlow, a novel framework for cross-modal flow matching, leverages variational encoders and a novel classifier-free guidance technique to directly map one modality's distribution to another's.\n- For text-to-image generation, CrossFlow uses a vanilla transformer without cross-attention, unlike existing methods that rely on complex architectures and conditioning mechanisms.\n- Demonstrating improved scaling, CrossFlow slightly outperforms standard flow matching baselines in zero-shot FID-30K and achieves comparable CLIP scores, given the same data, model size, and training budget.\n- CrossFlow exhibits semantic latent space arithmetic, enabling meaningful output edits through latent manipulation.\n- Its generalizability is showcased by comparable or superior performance in image captioning, depth estimation, and image super-resolution compared to state-of-the-art techniques.",
        "classification": [
            "Text-to-Image",
            "Image-to-Text",
            "Depth Estimation",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "https://cross-flow.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis",
        "authors": "lmwang, cqf, felixcheng97, qiuyuu, hlwang06",
        "link": "https://arxiv.org/abs/2412.15214",
        "github_repo": null,
        "summary": "- LeviTor is a novel method for 3D trajectory control in image-to-video synthesis that combines depth information with K-means clustered points of object masks without explicit 3D trajectory tracking.\n- It leverages a high-quality Video Object Segmentation (VOS) dataset (SA-V) for training and a user-friendly inference pipeline that simplifies 3D trajectory input.\n- LeviTor achieves state-of-the-art performance on standard video generation metrics (FID, FVD) and motion control metrics (ObjMC) compared to existing approaches like DragNUWA and DragAnything.\n- The model effectively manages the proximity changes of objects and produces video results with complex motions (like orbiting) and object occlusions that aren't possible with 2D trajectory controls.\n- Ablation studies validate the importance of the combined depth and instance information, along with the number of control points for effective 3D motion representation.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion",
        "authors": "Ye Liu, hpfister, dwei, EthanTaylor, Kakituken",
        "link": "https://arxiv.org/abs/2412.14462",
        "github_repo": "https://github.com/KaKituken/affordance-aware-any",
        "summary": "- This paper introduces Mask-Aware Dual Diffusion (MADD), a novel dual-stream diffusion model for affordance-aware object insertion into images. \n- MADD utilizes a DINOv2 encoder for foreground guidance and a frozen VAE encoder for background encoding, combined with a unified position prompt encoder.\n- It simultaneously denoises both the RGB image and a mask of the inserted object, facilitating better object placement consistent with real-world affordances.\n- The authors claim state-of-the-art performance on a new dataset, SAM-FB, derived from SA-1B and consisting of over 3 million image-object pairs across more than 3,000 categories, achieving a FID score of 13.53 and CLIP score of 0.8727 using mask prompts.\n-  MADD also shows strong generalization ability on in-the-wild images, adjusting the inserted object's position, size, and view for realistic compositions, even with ambiguous or null position prompts.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/KaKituken/affordance-aware-any"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation",
        "authors": "Yuejiang Dong, yshan2u, bluestyle97, pookiefoof, thuzhaowang",
        "link": "https://arxiv.org/abs/2412.15200",
        "github_repo": null,
        "summary": "- DI-PCG introduces a novel, efficient approach to Inverse Procedural Content Generation (I-PCG) using a diffusion transformer model.\n- This lightweight model (7.6M parameters) treats PCG parameters as the denoising target and uses observed images as conditions to control 3D asset generation.\n- DI-PCG efficiently recovers accurate parameters within seconds, generalizing well to diverse, real-world images without needing external datasets, thanks to features from a pre-trained visual foundation model.\n- Experiments show superior performance on I-PCG and image-to-3D tasks, validated qualitatively and quantitatively against state-of-the-art methods including Shap-E, Michelangelo, and InstantMesh.\n- The method offers a promising step toward a 3D generation pipeline focused on learning construction parameters, rather than modeling the 3D object directly.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling",
        "authors": "wping, ctnzr, shoeybi, ychenNLP, zihanliu",
        "link": "https://arxiv.org/abs/2412.15084",
        "github_repo": null,
        "summary": "- AceMath, a suite of large language models (LLMs) designed for complex math problem-solving and featuring specialized reward models for solution evaluation, is introduced.\n- The instruction-tuned math models are developed through a two-stage supervised fine-tuning (SFT) process, starting with general domain SFT and followed by targeted math domain fine-tuning using curated prompts and synthetically generated responses.\n- AceMath-72B-Instruct outperforms existing open-weight and proprietary LLMs, including Qwen2.5-Math-72B-Instruct, GPT-40, and Claude-3.5 Sonnet, on a variety of math reasoning benchmarks.\n- A new comprehensive benchmark, AceMath-RewardBench, is introduced for evaluating math reward models; the associated AceMath-72B-RM reward model achieves state-of-the-art performance.\n- Combining AceMath-72B-Instruct with AceMath-72B-RM yields the highest average rm@8 score across math reasoning benchmarks.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency",
        "authors": "Federico Tombari, Yongqin Xian, thofmann, Alessiot, enisimsar",
        "link": "https://arxiv.org/abs/2412.15216",
        "github_repo": null,
        "summary": "- UIP2P, an unsupervised model for instruction-based image editing, eliminates the need for ground-truth edited images during training by introducing Cycle Edit Consistency (CEC).\n- CEC enforces consistency by applying forward and backward edits in one training step, leveraging alignment between text and images in the CLIP embedding space and ensuring coherence in image and attention spaces.\n- This approach allows training on datasets with real image-caption pairs or image-caption-edit triplets, outperforming supervised methods across a broader range of edits.\n- Empirically, UIP2P shows better performance in qualitative comparisons and user studies across various datasets for diverse tasks like color modification, object removal, and structural changes.\n- Ablation studies demonstrate the importance of loss functions and the efficiency of UIP2P requiring fewer steps than other models like InstructPix2Pix for high-quality edits.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception",
        "authors": "Ke Zhu, Jing Hao, FuNz, cloud913, syp115",
        "link": "https://arxiv.org/abs/2412.14233",
        "github_repo": "https://github.com/syp2ysy/DCE",
        "summary": "- This paper introduces DCE (Descriptive Caption Enhancement), a novel image captioning engine that leverages off-the-shelf visual specialist models to extract detailed object attributes and relationships from images.\n- These attributes, combined with LLM-generated region captions and relational information, produce richer and more comprehensive descriptions than existing methods relying solely on LLMs or human annotation.\n- Experimental results demonstrate that DCE-generated captions significantly improve the performance of Large Multimodal Models (LMMs) across 14 visual question answering and multimodal benchmarks, exceeding human and other LLM-generated captions.\n- DCE utilizes open-source models for caption generation, reducing the costs and improving the efficiency compared to methods using expensive models like GPT-4V.\n- The authors plan to release the DCE source code and pipeline to promote further research and enable easy integration of other visual specialists into multimodal models.",
        "classification": [
            "Multimodal",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/syp2ysy/DCE"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
        "authors": "Qing Li, Yunqing Liu, Jiatong Li, schrodingers-tiger, Duke-de-Artois",
        "link": "https://arxiv.org/abs/2412.14642",
        "github_repo": "https://github.com/phenixace/TOMG-Bench",
        "summary": "- This paper introduces TOMG-Bench, the first benchmark designed to evaluate the ability of Large Language Models (LLMs) to perform open-domain text-based molecule generation.\n- TOMG-Bench comprises three core tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom), each with three subtasks containing 5,000 samples.\n- It includes an automated evaluation system assessing generated molecules' quality and accuracy, alongside an instruction-tuning dataset OpenMolIns, extracted from PubChem, to enhance LLM performance.\n- The benchmarking of 25 LLMs showcases the current limitations and potential in this field; with OpenMolIns, Llama-3.1-8B outperforms open-source general LLMs, even surpassing GPT-3.5-turbo on the TOMG-Bench by 46.5%.\n- The researchers identified the challenge of open molecule generation for existing LLMs and constructed the corresponding benchmark and the instructional dataset.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/phenixace/TOMG-Bench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Move-in-2D: 2D-Conditioned Human Motion Generation",
        "authors": "Feng Liu, Difan Liu, Jui-Hsien Wang, Yang Zhou, hsinh",
        "link": "https://arxiv.org/abs/2412.13185",
        "github_repo": null,
        "summary": "- Move-in-2D introduces a novel approach to generate human motion sequences conditioned on a 2D scene image and a text prompt, using a diffusion model with a transformer architecture.\n- The model architecture consists of CLIP and DINO encoders for text and image inputs, respectively, which are incorporated into the model via in-context conditioning and AdaLN.\n- A new large-scale dataset, HiC-Motion, is collected from open-domain internet videos and annotated with 3D human motion, text prompts, and scene images to train and evaluate the model.\n- Experimental results demonstrate that Move-in-2D generates human motion that aligns with the scene image and text prompt and improves the quality of human motion in video synthesis tasks, outperforming existing methods on several metrics, including FID, accuracy, diversity, and multimodality.\n- The generated motion is shown to be compatible with existing video generation frameworks and significantly enhance the generation of scene-consistent human actions and dynamics, overcoming limitations of methods that depend on existing motion sequences.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Parallelized Autoregressive Visual Generation",
        "authors": "jshfeng, zhenheny, Ikuinen, ShuhuaiRen, Epiphqny",
        "link": "https://arxiv.org/abs/2412.15119",
        "github_repo": null,
        "summary": "- This paper introduces Parallelized Autoregressive Visual Generation (PAR), a method for accelerating autoregressive image and video generation models without modifying model architecture or tokenizers.\n- PAR leverages the insight that distant visual tokens often have weak dependencies and can be generated in parallel, while local tokens with strong dependencies require sequential generation.\n- The method involves generating initial tokens for each region sequentially to establish global structure, followed by parallel generation of tokens at corresponding positions across different regions.\n- Experiments on ImageNet and UCF-101 demonstrate a 3.6x speedup with comparable quality and up to a 9.5x speedup with minimal quality degradation across both image and video generation tasks.\n- This parallel generation strategy is seamlessly integrated into standard autoregressive transformers using a reordering mechanism and learnable token embeddings to manage transitions between sequential and parallel modes.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Unconditional Image Generation",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/epiphqny/PAR-project"
        ],
        "huggingface_urls": [],
        "date": "2024-12-23"
    },
    {
        "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
        "authors": "Yilong Lai, Zhenglin Wang, zhoudeyu, lzhang472, callanwu",
        "link": "https://arxiv.org/abs/2412.13649",
        "github_repo": null,
        "summary": "- SCOPE is a novel framework designed to optimize Key-Value (KV) cache compression for long-context generation in Large Language Models (LLMs), addressing the often-overlooked decoding phase.\n- It decouples KV cache optimization for prefill and decoding phases, preserving essential information from the prefill while dynamically allocating heavy hitters during decoding using a sliding window approach.\n- Further memory optimization is achieved through adaptive and discontinuous strategies, reducing memory usage and transfer overhead.\n- Experimental results on LONGGENBENCH show that SCOPE maintains comparable performance to full KV cache with a 35% overall compression rate, outperforming existing unified compression methods.\n- SCOPE also demonstrates its generalizability and effectiveness as a plug-in to other prefill-only KV cache compression methods.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Linking-ai/SCOPE"
        ],
        "huggingface_urls": [],
        "date": "2024-12-23"
    },
    {
        "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning",
        "authors": "yiwu, ZhangShenao, hendrydong, Shibo-UCSD, jwhj",
        "link": "https://arxiv.org/abs/2412.16145",
        "github_repo": null,
        "summary": "- This paper introduces OREO (Offline REasoning Optimization), an offline reinforcement learning method designed to improve the multi-step reasoning abilities of Large Language Models (LLMs).\n- OREO jointly learns a policy model and value function by optimizing the soft Bellman Equation, enabling it to leverage unpaired data with sparse rewards and perform better credit assignment compared to methods like Direct Preference Optimization (DPO).\n- The approach is evaluated on mathematical reasoning (GSM8K, MATH) and embodied agent control (ALFWorld) tasks, demonstrating consistent improvements over baseline methods including rejection sampling, DPO, and KTO across different model sizes.\n- Notably, a 1.5B model achieves 52.5% accuracy on MATH using only the original training set, and iterative OREO shows continued improvement with additional training rounds.\n- The learned value function can also guide tree search during inference, leading to further performance gains (up to 17.9% relative improvement over greedy decoding on MATH).",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/jwhj/OREO"
        ],
        "huggingface_urls": [],
        "date": "2024-12-23"
    },
    {
        "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
        "authors": "wxcTest, ZhenxiongTang, flyingman",
        "link": "https://arxiv.org/abs/2412.16112",
        "github_repo": "https://github.com/Huage001/CLEAR",
        "summary": "- CLEAR, a convolution-like local attention mechanism, linearizes pre-trained Diffusion Transformers (DiTs) for efficient high-resolution image generation.\n- By restricting feature interactions within a local window and employing knowledge distillation, CLEAR achieves comparable image quality to original DiTs while significantly reducing computational complexity.\n- Experiments show a 99.5% reduction in attention computations and 6.3x speedup for 8K image generation, with preserved zero-shot generalization across models and plugins, and improved support for multi-GPU parallel inference.\n- CLEAR also exhibits cross-resolution generalizability, enabling high-resolution image synthesis with limited fine-tuning, unlike methods that rely on coarse-to-fine upscaling.\n- Fine-tuning CLEAR on self-generated images proves more effective than on real-world datasets, potentially due to distribution mismatch and inherent training difficulties associated with using pre-trained models.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/Huage001/CLEAR"
        ],
        "huggingface_urls": [],
        "date": "2024-12-23"
    },
    {
        "title": "Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
        "authors": "Akio Hayakawa, mittu1204, TakashiShibuyaSony, mi141, hkchengrex",
        "link": "https://arxiv.org/abs/2412.15322",
        "github_repo": null,
        "summary": "- MMAudio, a novel multimodal joint training framework for synthesizing high-quality, synchronized audio from video and optional text conditions, is introduced. \n- This model uses a transformer-based architecture with visual, text, and audio branches, jointly trained on audio-visual and text-audio data, and incorporates a conditional synchronization module for precise temporal alignment. \n- MMAudio achieves state-of-the-art performance in video-to-audio generation on public benchmarks, outperforming existing methods in audio quality, semantic alignment, and audio-visual synchronization, while maintaining a low inference time. \n- Notably, it also demonstrates competitive performance in text-to-audio generation without fine-tuning. \n- The joint training strategy enables accessible data scaling and cross-modal understanding, which are key to the model's success.",
        "classification": [
            "Multimodal",
            "Text-to-Audio",
            "Video-Text-to-Text",
            "Audio"
        ],
        "github_urls": [
            "hkchengrex.github.io/MMAudio"
        ],
        "huggingface_urls": [],
        "date": "2024-12-23"
    },
    {
        "title": "MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design",
        "authors": "chuanjieliu, xiaonans, JamesTheZ",
        "link": "https://arxiv.org/abs/2412.14590",
        "github_repo": null,
        "summary": "- MixLLM is a novel large language model (LLM) quantization technique that employs a global mixed-precision approach between output features, assigning higher bit-widths to features with greater impact on model accuracy, resulting in reduced memory consumption without compromising performance.\n- It identifies high-salience output channels by estimating their contribution to the final loss globally across all model layers, unlike previous methods that focus on per-layer salience.\n- MixLLM uses 8-bit symmetric activation quantization and 4-bit asymmetric weight quantization in a group-wise manner to maintain accuracy and employs a two-step dequantization process leveraging int8 Tensor Cores and fast integer-to-float conversion for optimized system efficiency.\n- Evaluation on popular LLMs like Llama 3.1 and Qwen2.5 across various benchmarks, including perplexity and downstream tasks like MMLU-Pro and BBH, demonstrates that MixLLM with only 4.4 bits for weights outperforms existing 4-bit methods and achieves results comparable to 5-bit quantization, while also exceeding the system performance of float16 and state-of-the-art 4-bit solutions.\n- Additionally, MixLLM with 8-bit weight quantization shows negligible accuracy loss compared to the float16 baseline, underscoring the efficacy of its group-wise activation quantization and system design.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-23"
    },
    {
        "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
        "authors": "navigli, mbrack, PSaiml, sted97, felfri",
        "link": "https://arxiv.org/abs/2412.15035",
        "github_repo": null,
        "summary": "- This paper introduces M-ALERT, a multilingual benchmark for evaluating the safety of Large Language Models (LLMs) across five languages (English, French, German, Italian, and Spanish).\n- M-ALERT comprises 75k prompts (15k per language), translated and adapted from the ALERT benchmark, covering a wide range of safety categories.\n- Experiments on 10 state-of-the-art LLMs reveal inconsistencies in safety performance across languages and categories, with some models exhibiting language-specific vulnerabilities while others show consistently unsafe behavior in certain high-risk categories.\n- The study finds a less pronounced correlation between model safety and size compared to the impact of instruction tuning.\n- M-ALERT also facilitates category and policy-specific evaluations, highlighting its practical use for policy compliance assessment in LLMs.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/felfri/M-ALERT"
        ],
        "date": "2024-12-23"
    },
    {
        "title": "Sequence Matters: Harnessing Video Models in 3D Super-Resolution",
        "authors": "juxhee, blee, yi0109-park, HEOK, lanikoisgod",
        "link": "https://arxiv.org/abs/2412.11525",
        "github_repo": null,
        "summary": "- This paper proposes a novel method for 3D super-resolution that leverages pre-trained Video Super-Resolution (VSR) models.\n- The method addresses the limitations of existing approaches that rely on Single Image Super-Resolution (SISR) and often produce results lacking 3D consistency.\n- The approach introduces efficient algorithms for ordering multi-view LR images into sequences suitable for VSR models, and these algorithms consider camera pose and visual feature similarity for optimal ordering.\n- It utilizes a multi-threshold approach for subsequence generation in which a stricter threshold is initially used in denser image regions, ensuring smoother trajectories. The threshold is then gradually loosened for sparser areas.\n- The proposed method achieves state-of-the-art performance on benchmark datasets, including NeRF-Synthetic and Mip-NeRF 360, outperforming baselines like NeRF-SR and DiSR-NeRF quantitatively and qualitatively.",
        "classification": [
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-23"
    },
    {
        "title": "Fietje: An open, efficient LLM for Dutch",
        "authors": "BramVanroy",
        "link": "https://arxiv.org/abs/2412.15450",
        "github_repo": null,
        "summary": "- This paper introduces Fietje, a family of 2.7B parameter decoder-only transformer language models for Dutch based on Phi-2.\n- Fietje was trained on 28B Dutch tokens from Wikipedia and the CulturaX dataset and comes in three versions: base, instruct, and chat.\n- At the time of its release, Fietje achieved competitive results with larger language models, sometimes even outperforming 7B models on ARC and MMLU benchmarks.\n- Evaluations on various Dutch NLP benchmarks demonstrated its efficacy compared to similar-sized models and established it as a significant step toward accessible language technology for Dutch.\n- The benchmark results also highlight the rapid advancement of the field and show that smaller multilingual models that were released after Fietje generally perform better.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Translation"
        ],
        "github_urls": [
            "https://github.com/BramVanroy/fietje-2",
            "https://github.com/BramVanroy/clin34-benchmarks"
        ],
        "huggingface_urls": [
            "https://huggingface.co/collections/BramVanroy/fietje-2-662cb803ed5cc4f617404146"
        ],
        "date": "2024-12-23"
    },
    {
        "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners",
        "authors": "Zifei Shan, Yijun Wang, Lulu Zhao, Yuzhen Huang, Weihao Zeng",
        "link": "https://arxiv.org/abs/2412.17256",
        "github_repo": null,
        "summary": "- B-STAR, a novel self-improving framework, balances exploration and exploitation by dynamically adjusting hyperparameters like temperature and reward thresholds to optimize a proposed balance score metric throughout training iterations.\n- This approach enhances the model's ability to generate both diverse and high-quality responses, addressing the limitations of current self-improving methods that often stagnate after a few iterations.\n- The effectiveness of B-STAR is validated across mathematical problem-solving (GSM8K and MATH datasets), coding challenges (APPS dataset), and commonsense reasoning (ARC-Challenge dataset).\n- Experimental results show significant performance improvement over various self-improving baselines (STaR/ReST-EM, Iterative RFT, Online RFT).\n- For example, B-STAR shows improved Pass@1 accuracy and sustained improvement over multiple training iterations without degradation, unlike other methods, demonstrating effective management of the exploration-exploitation trade-off.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/hkust-nlp/B-STaR"
        ],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response",
        "authors": "Zhiping Xiao, Jingyang Yuan, Xiao Luo, Junyu Luo, kaize0409",
        "link": "https://arxiv.org/abs/2412.14922",
        "github_repo": null,
        "summary": "- ROBUSTFT is a robust supervised fine-tuning framework designed to enhance the performance of Large Language Models (LLMs) in the presence of noisy response data, which is a common issue in real-world applications.\n- It employs a two-stage process: noise detection and denoising.  Noise detection leverages a multi-expert system with reasoning-enhanced LLMs and a consistency checker, while denoising uses context-enhanced relabeling with a review agent and entropy-based data selection.\n- The method was evaluated on three LLMs (Gemma2-9B, Llama3.1-8B, Llama3.2-3B) and five datasets (MMLU, ARC, PubMedQA, Drop, FPB) under varying noise levels (30%, 50%, and 70%).\n- Experimental results show consistent performance improvements across various noise conditions and datasets compared to vanilla LLMs and baseline denoising methods.\n- The framework also proves particularly beneficial for smaller models and maintains stable performance even with rephrased instructions, validating its robustness and generalizability.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/luo-junyu/RobustFT"
        ],
        "huggingface_urls": [
            "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B",
            "https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT"
        ],
        "date": "2024-12-24"
    },
    {
        "title": "Diving into Self-Evolving Training for Multimodal Reasoning",
        "authors": "Yu Cheng, Fan Zhou, Xiwen Zhang, Junlong Li, Wei Liu",
        "link": "https://arxiv.org/abs/2412.17451",
        "github_repo": null,
        "summary": "- This paper introduces M-STAR, a novel self-evolving training framework for enhancing multimodal reasoning abilities of Large Multimodal Models (LMMs) without relying on human-annotated chain-of-thought data.\n- M-STAR systematically analyzes and optimizes three key components of self-evolving training: training methods, reward models, and prompt variations.\n- It presents a continuous self-evolving training scheme, trains the first process-based reward model for multimodal reasoning, and demonstrates that adding unlabeled data is only effective with reliable reward signals.\n- Additionally, M-STAR incorporates dynamic temperature adjustment to balance exploration and exploitation during training to counter exploration loss.\n- Experiments on five multimodal reasoning benchmarks show that M-STAR significantly improves the performance of models with varying sizes, such as MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B), and InternVL2 (2B), consistently surpassing pre-trained models across various subtasks.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/hkust-nlp/mstar"
        ],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
        "authors": "Yu Wang, Xuefei Ning, Enshu Liu, fjxmlzn",
        "link": "https://arxiv.org/abs/2412.17153",
        "github_repo": null,
        "summary": "- Distilled Decoding (DD) is proposed, a novel method that allows pre-trained autoregressive (AR) models to generate images in few steps (e.g., one or two) by distilling a deterministic mapping from Gaussian noise to the output distribution of the AR model through flow matching.\n- DD is evaluated on ImageNet 256x256 and LAION-COCO datasets, showing significant speed improvements while achieving acceptable FID scores.\n- For image generation, DD reduces sampling steps from 10 to 1 for VAR and 256 to 1 for LlamaGen, leading to 6.3x and 217.8x speedups, respectively.\n- For text-to-image generation with LlamaGen, DD can reduce steps from 256 to 2 with a 92.9x speed up while minimally impacting FID scores.\n- The training process doesn't need original training data of the AR model.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Unconditional Image Generation",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/imagination-research/distilled-decoding"
        ],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
        "authors": "Jiaxin Xie, Jingye Chen, Yingqing He, Yang Fei, Yazhou Xing",
        "link": "https://arxiv.org/abs/2412.17805",
        "github_repo": null,
        "summary": "- This paper introduces a novel cross-modal Video Variational Autoencoder (VAE) designed for high-fidelity video encoding, especially for videos containing large motion.\n- The proposed model uses a two-stage spatiotemporal architecture, combining the strengths of simultaneous and sequential spatial-temporal compression to enhance detail and motion recovery.\n- By incorporating text guidance from text-video paired datasets, the model improves reconstruction quality, particularly in detail preservation and temporal stability.\n- The model is trained jointly on both image and video datasets, enabling it to perform both image and video autoencoding, enhancing reconstruction quality and versatility.\n- Experimental results on benchmark datasets demonstrate superior performance compared to state-of-the-art methods, showing significant improvements in metrics like PSNR, SSIM, and LPIPS, especially in large-motion scenarios.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
        "authors": "Arthur Szlam, Jun Xie, Jiaxing Wu, Jonas Pfeiffer, Luyang Liu",
        "link": "https://arxiv.org/abs/2412.17747",
        "github_repo": null,
        "summary": "- This paper introduces a novel method called \"differentiable cache augmentation\" to enhance frozen decoder-only Large Language Models (LLMs) by adding a coprocessor that operates on the model's key-value cache.\n- The coprocessor, trained using the language modeling loss, augments the cache with latent embeddings, improving the fidelity of subsequent decoding without modifying the original LLM architecture.\n- Experimental results show that this approach consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks, such as GSM8K and MMLU, even in zero/few-shot settings.\n- The approach allows asynchronous and offline coprocessor operation, opening possibilities for more deliberate and computationally intensive reasoning processes in future research.\n- The method outperforms the baseline model and a related method called \"Pause Token\" on tasks like GSM8K, showcasing the effectiveness of the learned context-dependent dynamic embeddings.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "Revisiting In-Context Learning with Long Context Language Models",
        "authors": "Oh, Geunseob, Prakhar Gupta, Sun Jae Lee, Jinheon Baek",
        "link": "https://arxiv.org/abs/2412.16926",
        "github_repo": null,
        "summary": "- This paper revisits In-Context Learning (ICL) with Long Context Language Models (LCLMs) and challenges the prevailing assumption that sophisticated example selection strategies are crucial for optimal performance.\n- Through experiments on 18 datasets across 4 tasks, the study finds that simple random sampling is as effective as more complex selection methods in many-shot ICL scenarios.\n- The paper identifies a new challenge with LCLMs: underutilization of expanded context capacity, especially in low-resource tasks. \n- It proposes a data augmentation technique to address this which involves generating synthetic examples and filtering low-quality ones, leading to performance improvements of up to 5%.\n- The study also finds that while LCLMs benefit from larger contexts, performance plateaus and may decline when the context becomes extremely long, especially with noisy examples present, suggesting future research directions for improving robustness in LCLMs.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Translation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "Outcome-Refining Process Supervision for Code Generation",
        "authors": "Jindong Wang, Zhengran Zeng, Yidong Wang, Weizheng Gu, Zhuohao Yu",
        "link": "https://arxiv.org/abs/2412.15118",
        "github_repo": "https://github.com/zhuohaoyu/ORPS",
        "summary": "- This paper introduces Outcome-Refining Process Supervision (ORPS), a novel paradigm for enhancing code generation by treating outcome refinement as the process to be supervised.\n- It leverages a tree-structured exploration space, enabling models to maintain multiple reasoning trajectories, guided by execution feedback as objective anchors for evaluation. \n- The framework combines beam search with a self-critique mechanism, where the model analyzes reasoning chains and execution outcomes before generating rewards, eliminating the need for trained Process Reward Models (PRMs).\n- ORPS demonstrates significant improvements across various benchmarks, achieving a 26.9% average increase in Pass@1 and a 42.2% reduction in execution time compared to existing methods. \n- These results highlight the effectiveness of coupling structured reasoning space with concrete feedback signals for solving complex coding tasks, offering a scalable and efficient solution.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/zhuohaoyu/ORPS"
        ],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
        "authors": "Jie Zhou, Yunlong Liang, Fandong Meng, Jiaan Wang",
        "link": "https://arxiv.org/abs/2412.17498",
        "github_repo": "https://github.com/krystalan/DRT-o1",
        "summary": "- This paper introduces DRT-01, a new model that integrates long chain-of-thought (CoT) into neural machine translation (MT), specifically targeting sentences with similes or metaphors from literature books. \n- A multi-agent framework with a translator, advisor, and evaluator is used to iteratively refine translations, generating long-thought MT data. GPT-40 is then employed to enhance the readability of the generated data.\n- DRT-01 is trained on this data using Qwen2.5-7B/14B as backbones. \n- Experimental results show improvements of 7.33~8.26 BLEU and 1.66~3.36 CometScore over the baselines.\n- Notably, DRT-01-7B surpasses QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showcasing its effectiveness.",
        "classification": [
            "Translation",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/krystalan/DRT-o1"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Unbabel/wmt22-cometkiwi-da",
            "https://huggingface.co/Unbabel/wmt22-comet-da",
            "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
            "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct"
        ],
        "date": "2024-12-24"
    },
    {
        "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents",
        "authors": "Junxiao Yang, Jingzhuo Zhou, Yida Lu, Shiyao Cui, Zhexin Zhang",
        "link": "https://arxiv.org/abs/2412.14470",
        "github_repo": "https://github.com/thu-coai/Agent-SafetyBench",
        "summary": "- This paper introduces AGENT-SAFETYBENCH, a comprehensive benchmark designed to evaluate the safety of LLM agents.\n- The benchmark encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes.\n- An evaluation of 16 popular LLM agents reveals that none achieve a safety score above 60%.\n- Analysis identifies two key safety defects: a lack of robustness and a lack of risk awareness.\n- The study finds that relying solely on defense prompts is insufficient for addressing these safety issues, suggesting the need for more advanced strategies.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/thu-coai/Agent-SafetyBench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "NILE: Internal Consistency Alignment in Large Language Models",
        "authors": "Hongru Wang, Bowei He, Yufei Wang, Qiyuan Zhang, Minda Hu",
        "link": "https://arxiv.org/abs/2412.16686",
        "github_repo": null,
        "summary": "- The paper introduces NILE (iNternal consIstency aLignmEnt), a framework designed to improve the quality of Instruction Fine-Tuning (IFT) datasets for Large Language Models (LLMs) by aligning the datasets with the LLMs' internal knowledge.\n- NILE works by eliciting the internal knowledge of a pre-trained LLM, revising existing dataset answers using this internal knowledge, and filtering out inconsistent samples using a novel Internal Consistency Filtering (ICF) method.\n- Experiments show that NILE-aligned IFT datasets boost LLM performance across multiple benchmarks, including up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2.\n- Ablation studies validate each component of the framework\u2014Internal Knowledge Extraction, Knowledge-Aware Sample Revision, and Internal Consistency Filtering\u2014confirming their contribution to the improved performance.\n- The results demonstrate the significance of dataset consistency with pre-trained internal knowledge for maximizing LLM potential.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "LearnLM: Improving Gemini for Learning",
        "authors": "Andrea Huber, Aliya Rysbek, Aditya Srikanth Veerubhotla, Abhinit Modi, LearnLM Team",
        "link": "https://arxiv.org/abs/2412.16429",
        "github_repo": null,
        "summary": "- This paper introduces LearnLM, a new large language model (LLM) based on Gemini 1.5 Pro and fine-tuned specifically for educational applications.\n- LearnLM is trained using a method called pedagogical instruction following, which uses system-level instructions to guide desired pedagogical behaviours, rather than defining specific pedagogical behaviours. \n- It incorporates Reinforcement Learning from Human Feedback (RLHF) for enhanced adherence to nuanced instructions and user preferences.  \n- Human evaluation results show a significant preference for LearnLM over GPT-40, Claude 3.5, and Gemini 1.5 Pro across various learning scenarios. \n- LearnLM is available as an experimental model on Google AI Studio.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "OpenAI o1 System Card",
        "authors": "Adam Richardson, Adam Lerer, Adam Kalai, Aaron Jaech, OpenAI",
        "link": "https://arxiv.org/abs/2412.16720",
        "github_repo": null,
        "summary": "- OpenAI introduces the \"o1\" large language model family, trained with reinforcement learning for complex reasoning using chain-of-thought, enhancing safety and robustness.\n- The models demonstrate state-of-the-art performance in benchmarks related to generating illicit advice, stereotyped responses, and known jailbreaks due to deliberative alignment.\n- Trained on diverse public, proprietary, and custom datasets, o1 shows enhanced performance in jailbreak evaluations and adherence to safety guidelines compared to GPT-40.\n- The o1 models also demonstrate significant improvements in mitigating hallucinations, especially in factual question answering, and improved performance in tasks assessing demographic fairness.\n- Despite advancements, potential safety risks stemming from increased intelligence are acknowledged, highlighting the need for continuous improvement in alignment and safety methods and extensive stress-testing.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning",
        "authors": "Jinlin Xiao, Yuhang Wang, Jiangming Shu, Yuqi Yang, Yuxiang Zhang",
        "link": "https://arxiv.org/abs/2412.16849",
        "github_repo": "https://github.com/ADaM-BJTU/OpenRFT",
        "summary": "- OpenRFT adapts a generalist reasoning model for domain-specific tasks using reinforcement fine-tuning (RFT), addressing challenges like limited training data and lack of reasoning step data.\n- It leverages domain-specific samples through question augmentation, synthesizing reasoning process data using a teacher model, and few-shot in-context learning (ICL) to enhance RL exploration.\n- Evaluated on SciKnowEval, OpenRFT demonstrates significant performance improvements with limited samples, averaging 11% accuracy increase compared to a baseline model.\n- The study highlights that data augmentation, stronger reasoning foundation models, and aligned action space contribute to better RFT performance.\n-  OpenRFT's effectiveness depends on the availability of high-quality generalist reasoning models and corresponding Process Reward Models (PRMs).",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/ADaM-BJTU/OpenRFT"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Skywork"
        ],
        "date": "2024-12-24"
    },
    {
        "title": "Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding",
        "authors": "Qun Liu, Jianxin Liang, Xiaojun Meng, Yueqian Wang, ColorfulAI",
        "link": "https://arxiv.org/abs/2412.17295",
        "github_repo": "https://github.com/yellow-binary-tree/Friends-MMC",
        "summary": "- This paper introduces Friends-MMC, a multimodal multi-party conversation (MMC) dataset derived from the TV series *Friends*. \n- The dataset includes over 24,000 utterances paired with video contexts, speaker annotations, and bounding boxes of faces, facilitating research on character-centered understanding in conversations.\n- The authors propose a baseline method for conversation speaker identification leveraging visual and textual models, combined with a quadratic binary optimization solver, demonstrating its effectiveness compared to existing pre-trained models.\n- For conversation response prediction, they fine-tune generative dialogue models on Friends-MMC and show that incorporating speaker information improves performance. \n-  They argue for increased attention to modeling speaker information in conversational understanding research.",
        "classification": [
            "Multimodal",
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/yellow-binary-tree/Friends-MMC"
        ],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World",
        "authors": "Runze Fan, Jiadi Su, Shijie Xia, Jiahe Jin, Yanheng He",
        "link": "https://arxiv.org/abs/2412.17589",
        "github_repo": null,
        "summary": "- PC Agent, an AI system designed to perform complex digital work tasks, is introduced, transferring human cognition to AI agents. \n- The system consists of three key components: PC Tracker, a lightweight infrastructure for collecting human-computer interaction data; a cognition completion pipeline for enriching interaction data with semantics and thought processes; and a multi-agent architecture comprising a planning agent trained on cognitive trajectories and a robust visual grounding agent with self-validation. \n- PC Agent is trained and evaluated in the context of creating PowerPoint presentations, a task requiring complex multi-software coordination and higher-level cognitive functions. \n- Preliminary results demonstrate PC Agent\u2019s capacity to generate complex, multi-page presentations with as few as 133 training examples, showcasing substantial data efficiency in learning from human cognitive processes. \n- The multi-agent design combines specialized visual grounding capabilities with cognitive planning, enabling completion of complex, multi-step tasks involving up to 50 actions.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-24"
    },
    {
        "title": "DepthLab: From Partial to Complete",
        "authors": "Hao Ouyang, Shuzhe Wang, Qiuyu Wang, Ka Leong Cheng, Zhiheng Liu",
        "link": "https://arxiv.org/abs/2412.18153",
        "github_repo": null,
        "summary": "- DepthLab is a novel dual-branch diffusion-based model for depth inpainting conditioned on RGB images and partial depth information.\n- The model architecture features a Reference U-Net for RGB feature extraction and an Estimation U-Net for depth completion, with layer-by-layer feature fusion for enhanced visual guidance.\n- DepthLab excels in preserving scale consistency with known depth regions and demonstrates strong generalization capabilities across diverse scenarios, outperforming existing methods on NYUv2, KITTI, ETH3D, ScanNet, and DIODE datasets for depth inpainting.\n- The model's effectiveness extends to various downstream tasks, including 3D scene inpainting, text-to-3D scene generation, sparse-view reconstruction with DUST3R, and LiDAR depth completion.\n- Random scale normalization during training and a diverse masking strategy contribute to the robustness and versatility of DepthLab.",
        "classification": [
            "Depth Estimation",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://johanan528.github.io/depthlab_web/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding",
        "authors": "Dmitry Yudin, wingrune",
        "link": "https://arxiv.org/abs/2412.18450",
        "github_repo": "https://github.com/CognitiveAISystems/3DGraphLLM",
        "summary": "- 3DGraphLLM introduces a novel approach for 3D scene understanding by combining semantic graphs and large language models (LLMs).\n- The model architecture includes pre-trained encoders for 3D point clouds and semantic relationships, and a pre-trained LLM, trained via a two-stage approach using ground truth and predicted instance segmentation of point clouds.\n- 3DGraphLLM represents a 3D scene as a flattened sequence of learnable embeddings of object subgraphs, including object identifiers and relationships with k-nearest neighbors, which is then fed to an LLM.\n- Experimental results on ScanRefer, Multi3DRefer, and Scan2Cap datasets demonstrate state-of-the-art performance for the proposed method, improving F1@0.5 scores by +5.8% and +4.4% and CIDEr@0.5 by +5.8% respectively on the mentioned datasets.\n- Ablation studies confirm the benefit of incorporating semantic graph representation for several 3D vision-language tasks.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision",
            "Object Detection",
            "Image-to-Text",
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [
            "https://github.com/CognitiveAISystems/3DGraphLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization",
        "authors": "Ning Ding, Kaiyan Zhang, Xingtai Lv, Che Jiang, Ermo Hua",
        "link": "https://arxiv.org/abs/2412.17739",
        "github_repo": null,
        "summary": "- This paper proposes Fourier Position Embedding (FoPE) to improve the length generalization of Language Models (LMs). \n- FoPE enhances the periodic extension of attention by addressing the spectral damage caused by linear layers, activation functions, and under-trained frequency components. \n- Unlike Rotary Position Embedding (ROPE), which treats each dimension as a single-frequency function, FoPE models each dimension as a Fourier series of different frequency components.\n- FoPE also clips inadequately trained frequency components, replacing them with zero-frequency components to preserve long-wavelength information. \n- Experiments across various model scales demonstrate FoPE\u2019s superior length generalization compared to ROPE and ALiBi, maintaining stable perplexity and consistent accuracy in a needle-in-haystack task within varying context windows.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/TsinghuaC3I/Fourier-Position-Embedding"
        ],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
        "authors": "Zhaoyang Zhang, Wenze Liu, Xiaoyu Li, Xiaodong Cun, Minghong Cai",
        "link": "https://arxiv.org/abs/2412.18597",
        "github_repo": "https://github.com/TencentARC/DiTCtrl",
        "summary": "- DiTCtrl is a novel, tuning-free method for multi-prompt video generation using the Multi-Modal Diffusion Transformer (MM-DiT) architecture.\n- DiTCtrl analyzes MM-DiT's attention mechanism, finding similarities with UNet-like diffusion models, and introduces a mask-guided KV-sharing strategy and latent blending for smooth transitions between prompts.\n- It enables the generation of longer videos with consistent object motion and semantic transitions without additional training.\n- DiTCtrl achieves state-of-the-art performance on MPVBench, a new benchmark designed for multi-prompt video generation.\n- The authors also present MPVBench, a new benchmark specifically designed for multi-prompt video generation to evaluate the performance.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [
            "https://github.com/TencentARC/DiTCtrl"
        ],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
        "authors": "Borchmann",
        "link": "https://arxiv.org/abs/2412.17758",
        "github_repo": null,
        "summary": "- This paper challenges the perceived difficulty of the ARC Challenge dataset, arguing that it appears harder for Large Language Models (LLMs) primarily due to an evaluation setup that hinders direct comparison of answer choices rather than inherent complexity.\n- The authors highlight a shift in evaluation practices where some researchers have adopted a fairer comparison scheme, allowing models to see all answer options simultaneously, which dramatically improves performance.\n- This fairer approach reduces performance gaps in other benchmarks, such as SocialIQa (SIQA), and even leads to superhuman results in OpenBookQA, suggesting that evaluation methods significantly shape perceived difficulty.\n- They demonstrate that switching from evaluating answers in isolation to evaluating them alongside other options leads to substantial performance gains, up to 35% improvement on ARC Challenge for some LLMs.\n- The paper proposes guidelines to ensure multiple-choice evaluations accurately reflect model capabilities by recommending the use of an evaluation setup where LLMs can compare answer options directly.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models",
        "authors": "Jianyuan Wang, Tom Monnier, Iro Laina, Roman Shapovalov, Minghao Chen",
        "link": "https://arxiv.org/abs/2412.18608",
        "github_repo": null,
        "summary": "- PartGen is a novel approach that generates compositional 3D objects with meaningful parts from text, images, or unstructured 3D objects using multi-view diffusion models.\n- The method addresses the ambiguity in part segmentation and completion by using a two-stage generative approach that first extracts plausible part segmentations and then completes and reconstructs these parts in 3D.\n- This completion process accounts for the context of the entire object ensuring that parts integrate cohesively and can even hallucinate entirely invisible parts.\n- Evaluation on generated and real 3D assets demonstrates that PartGen significantly outperforms existing segmentation and part-extraction baselines.\n- PartGen has downstream applications like 3D part editing based on text instructions.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing",
        "authors": "Jun Zhu, Jianfei Chen, Ziteng Wang",
        "link": "https://arxiv.org/abs/2412.14711",
        "github_repo": "https://github.com/thu-ml/ReMoE",
        "summary": "- ReMoE, a fully differentiable Mixture-of-Experts (MoE) architecture based on ReLU routing, is proposed as a drop-in replacement for standard TopK routing, offering continuous training and dynamic expert allocation.\n- ReLU routing manages experts' on/off states independently and is combined with an adaptive load balancing L1 regularization to control sparsity.\n- ReMoE outperforms traditional TopK MoE and other routing methods across various model sizes, expert counts, and granularity levels on the LLaMA architecture trained on The Pile dataset.\n- ReMoE demonstrates improved scalability and performance gains compared to TopK MoE with increasing expert counts, suggesting its effectiveness with larger expert pools.\n- ReMoE exhibits dynamic expert allocation based on token frequency and stronger domain specialization, leading to efficient resource utilization and improved model expressivity.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/thu-ml/ReMoE"
        ],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval",
        "authors": "Divya Chaudhary, Vinija Jain, Aman Chadha, Vinesh Kumar Gande, Aakash Mahalingam",
        "link": "https://arxiv.org/abs/2412.15443",
        "github_repo": null,
        "summary": "- This paper introduces SKETCH, a novel methodology that enhances the Retrieval Augmented Generation (RAG) retrieval process by integrating semantic text retrieval with knowledge graphs.\n- SKETCH merges structured and unstructured data for a more holistic comprehension, aiming to improve retrieval performance and maintain context integrity.\n- Evaluated across four diverse datasets (QUALITY, QASPER, NarrativeQA, and Italian Cuisine), SKETCH consistently outperformed baseline approaches on key RAGAS metrics, including answer_relevancy, faithfulness, context_precision, and context_recall.\n- Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics.\n- These results highlight SKETCH's capability to deliver more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems by addressing challenges in handling large-scale discourse structures and complex queries across various domains.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-25"
    },
    {
        "title": "Token-Budget-Aware LLM Reasoning",
        "authors": "Zhenyu Chen, Shiqing Ma, Shiyu Zhao, Chunrong Fang, Tingxu Han",
        "link": "https://arxiv.org/abs/2412.18547",
        "github_repo": "https://github.com/GeniusHTX/TALE",
        "summary": "- This paper introduces TALE (Token-Budget-Aware LLM rEasoning), a framework designed to optimize the reasoning process in Large Language Models (LLMs) by dynamically managing token budgets.\n- TALE estimates a token budget for each problem based on its complexity, then incorporates this budget into the prompt to guide the LLM's reasoning.\n- This method addresses the issue of token redundancy in current LLMs, which often produce unnecessarily lengthy reasoning processes, leading to increased costs and computational overhead.\n- Experimental results demonstrate that TALE significantly reduces token usage by an average of 68.64% while maintaining competitive accuracy (less than 5% decrease).\n- This suggests that TALE offers a practical approach to balancing efficiency and accuracy in LLM reasoning.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/GeniusHTX/TALE"
        ],
        "huggingface_urls": [],
        "date": "2024-12-26"
    },
    {
        "title": "YuLan-Mini: An Open Data-efficient Language Model",
        "authors": "Jie Chen, Jiapeng Wang, Jia Deng, Huatong Song, Yiwen Hu",
        "link": "https://arxiv.org/abs/2412.17743",
        "github_repo": "https://github.com/RUC-GSAI/YuLan-Mini",
        "summary": "- This paper introduces YuLan-Mini, a 2.42B parameter open-source language model trained on 1.08T tokens.\n- The model uses a decoder-only transformer architecture with grouped-query attention, SwiGLU activation, rotary positional embeddings, and embedding tying.\n- Key innovations for pre-training include a robust optimization method to improve training stability, a focused data pipeline with data cleaning and scheduling, and an annealing approach for data selection and long context training.\n- The model is evaluated on benchmarks for math, code generation, reasoning, and language understanding, showing comparable performance to larger industry models trained on significantly more data.\n- YuLan-Mini achieves state-of-the-art results for similar-sized models and demonstrates significant improvements in training efficiency.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/RUC-GSAI/YuLan-Mini"
        ],
        "huggingface_urls": [],
        "date": "2024-12-27"
    },
    {
        "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
        "authors": "Xinting Huang, Shuaiyi Li, Kelong Mao, Zhisong Zhang, ChenlongDeng",
        "link": "https://arxiv.org/abs/2412.17483",
        "github_repo": null,
        "summary": "- This paper investigates gist token-based context compression methods for improving long-context processing in large language models (LLMs).\n- The study evaluates different gist-based model architectures categorized by memory location (recurrent or key-value cache) and gist granularity (coarse or fine-grained).\n- Experiments across language modeling, reasoning, and long-context tasks show that fine-grained key-value cache models achieve near-lossless performance on some tasks, while struggling with others like reranking.\n- Three failure patterns are identified: \"lost by the boundary,\" where generation quality degrades near segment boundaries; \"lost if surprise,\" where unexpected details are lost; and \"lost along the way,\" where models struggle to recover exact rehearsals. \n- Two mitigation strategies, fine-grained autoencoding and segment-wise token importance estimation, show improvements, especially in addressing boundary effects and precise recall.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-27"
    },
    {
        "title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs",
        "authors": "Wanlong Liu, Xidong Wang, Ke Ji, Zhenyang Cai, Junying Chen",
        "link": "https://arxiv.org/abs/2412.18925",
        "github_repo": null,
        "summary": "- This paper introduces HuatuoGPT-01, a medical Large Language Model (LLM) designed for complex reasoning.\n- The model is trained in two stages: firstly, it learns complex reasoning by searching for correct reasoning trajectories guided by a medical verifier, and secondly, it refines this skill with reinforcement learning using verifier-based rewards.\n- It is trained on 40k verifiable medical problems converted from closed-set exam questions, supplemented by a general domain dataset for enhanced generalization.\n- HuatuoGPT-01 significantly outperforms existing general and medical-specific LLMs on multiple medical benchmarks, including MedQA, MedMCQA, and PubMedQA, as well as more challenging datasets MMLU-Pro and GPQA.\n- Ablation studies and other analysis demonstrate that the method of complex reasoning and reinforcement learning boosts performance compared to non-CoT or simple CoT approaches.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/FreedomIntelligence/HuatuoGPT-01"
        ],
        "huggingface_urls": [],
        "date": "2024-12-30"
    },
    {
        "title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment",
        "authors": "Kunchang Li, Chenting Wang, Yinan He, Zhilin Li, Ziang Yan",
        "link": "https://arxiv.org/abs/2412.19326",
        "github_repo": "https://github.com/OpenGVLab/TPO",
        "summary": "- This paper introduces Task Preference Optimization (TPO), a novel method to enhance Multimodal Large Language Models (MLLMs) with fine-grained visual task alignment.\n- TPO utilizes learnable task tokens and corresponding task heads for region, temporal, and mask-related visual tasks and incorporates visual task data during training via a local-to-global training process to improve both multimodal dialogue and task-specific performance.\n- Experiments on VideoChat and LLaVA demonstrate a 14.6% average improvement in multimodal performance on benchmarks like MVBench, VideoMME, NExT-GQA, MLVU, and SEED-Bench2.\n- The model performs competitively with state-of-the-art models on tasks like spatial grounding, moment retrieval, highlight detection, and tracking.\n- Ablation studies validate the efficacy of different TPO components, co-training benefits, and task data scaling.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/OpenGVLab/TPO"
        ],
        "huggingface_urls": [],
        "date": "2024-12-30"
    },
    {
        "title": "Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models",
        "authors": "Hengshuang Zhao, Chao Du, Tianyu Pang, Ziang Zhang, Zehan Wang",
        "link": "https://arxiv.org/abs/2412.18605",
        "github_repo": null,
        "summary": "- Introduces Orient Anything, a novel model designed for estimating 3D object orientation in single- and free-view images using a simple visual encoder and multiple prediction heads.\n- Collects a 2M image dataset with precise orientation annotations by rendering 3D models from random views after automatically annotating the front face of the 3D objects using a VLM and geometric analysis.\n- Proposes a robust training objective that models 3D orientation as probability distributions of three angles (polar, azimuth, and rotation) and predicts object orientation by fitting these distributions.\n- Employs DINOv2 initialization and data augmentation (random cropping during training and object masking during inference) to enhance synthetic-to-real transfer.\n- Achieves state-of-the-art orientation estimation accuracy in rendered and real images, outperforming existing expert models and large vision-language models, and exhibits impressive zero-shot generalization across diverse real-world scenarios.",
        "classification": [
            "Computer Vision",
            "Object Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-30"
    },
    {
        "title": "The Superposition of Diffusion Models Using the It\u00f4 Density Estimator",
        "authors": "Kirill Neklyudov, Alexander Tong, Avishek Joey Bose, Lazar Atanackovic, Marta Skreta",
        "link": "https://arxiv.org/abs/2412.17762",
        "github_repo": "https://github.com/necludov/super-diffusion",
        "summary": "- This paper introduces SUPERDIFF, a novel method for combining multiple pre-trained diffusion models at the inference stage without retraining.\n- SUPERDIFF leverages a new scalable It\u00f4 density estimator for the log-likelihood of the diffusion SDE, which incurs no additional overhead.\n- The method is theoretically grounded in the principle of superposition from physics and offers two algorithms: one for sampling from a mixture of densities (logical OR) and another for sampling from equal density regions (logical AND).\n- Empirically, SUPERDIFF demonstrates improved performance on image generation tasks, such as generating diverse images on CIFAR-10 and more faithful prompt-conditioned image editing using Stable Diffusion.\n- It also shows promise in protein generation, achieving better designability and novelty compared to individual models or simple averaging.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/necludov/super-diffusion"
        ],
        "huggingface_urls": [],
        "date": "2024-12-30"
    },
    {
        "title": "From Elements to Design: A Layered Approach for Automatic Graphic Design Composition",
        "authors": "Ji Li, Ting Liu, Danqing Huang, Shizhao Sun, Jiawei Lin",
        "link": "https://arxiv.org/abs/2412.19712",
        "github_repo": null,
        "summary": "- This research presents LaDeCo, a novel layered approach for automatic graphic design composition using large multimodal models (LMMs).\n- LaDeCo incorporates a layer planning module, using GPT-4 to categorize input elements into semantic layers (background, underlay, logo/image, text, and embellishment).\n- The model then generates design compositions layer by layer, rendering previous layers as images and feeding them back to the LMM for contextual information.\n- Experimental results on the Crello dataset demonstrate that LaDeCo outperforms baseline models in design composition and related subtasks.\n- LaDeCo also offers flexibility for specific design tasks like content-aware layout and typography generation without task-specific training and enables resolution adjustment, design variation, and element addition.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-30"
    },
    {
        "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
        "authors": "Shang-Tse Chen, Saurav Sahay, Shachi H Kumar, Hsuan Su, farnhua",
        "link": "https://arxiv.org/abs/2412.19512",
        "github_repo": null,
        "summary": "- This paper introduces a method to mitigate safety degradation in fine-tuned Large Language Models (LLMs) without requiring additional safety data.\n- The method involves merging the weights of a pre-trained safety-aligned base model and its fine-tuned version after training on a downstream task.\n- Experimental results across various models (Llama-3, Gemma-2B-it), downstream tasks (reasoning, medical assistance, code generation, tool usage), and merging methods (linear merging, model stock, SLERP, DARE) show improvement in safety and downstream task performance.\n- The proposed approach reduces the Attack Success Rate (ASR) by up to 30% while enhancing downstream task performance, offering a simple yet robust solution.\n- Linear merging is highlighted as a practical method due to its efficacy and computational efficiency",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/allenai/wildguard"
        ],
        "date": "2024-12-30"
    },
    {
        "title": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images",
        "authors": "Yoshitaka Ushiku, Tosho Hirasawa, Shohei Tanaka, Kuniaki Saito, Risa Shinoda",
        "link": "https://arxiv.org/abs/2412.17606",
        "github_repo": "https://github.com/omron-sinicx/SBSFigures",
        "summary": "- This paper introduces SBS Figures (Stage-by-Stage Synthetic Figures), a new dataset for pre-training figure question answering (QA) models.\n- The dataset is generated through a novel three-stage pipeline involving visualization target data generation, figure rendering via Python code, and QA pair generation, leveraging LLMs at each stage and producing 1 million synthetic chart figures with associated data and QA pairs.\n- Models pre-trained on SBS Figures demonstrate a strong performance boost on real-world chart QA datasets like ChartQA, outperforming models trained from scratch or other synthetic datasets.\n- This method allows efficient training of QA models even with a limited amount of real-world data by first pre-training on the large-scale SBS Figures dataset.\n- An ablation study investigating various factors in the pipeline reveals the importance of diverse figure appearance, high-quality LLM-generated QA pairs, and the scale of the dataset for optimal pre-training effectiveness.",
        "classification": [
            "Visual Question Answering",
            "Document Question Answering",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/omron-sinicx/SBSFigures"
        ],
        "huggingface_urls": [],
        "date": "2024-12-30"
    },
    {
        "title": "VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models",
        "authors": "Junfu Pu, Zhongang Qi, Xiaodong Cun, Yong Zhang, Tao Wu",
        "link": "https://arxiv.org/abs/2412.19645",
        "github_repo": null,
        "summary": "- VideoMaker, a novel framework leverages the inherent capabilities of Video Diffusion Models (VDMs) for zero-shot customized video generation, eliminating the need for additional training or external alignment models.\n- It utilizes the VDM itself for fine-grained feature extraction from reference images, treating a noise-free image input as a special case of the diffusion process at timestep 0.\n-  For feature injection, VideoMaker employs the VDM's spatial self-attention mechanism to directly interact subject features with the generated content within each frame, thereby improving subject fidelity and maintaining video diversity.\n-  A Guidance Information Recognition Loss aids the model in distinguishing between reference information and generated content during training, further enhancing performance.\n- Experiments on customized human and object video generation datasets demonstrate VideoMaker's superior performance compared to existing methods by achieving high subject fidelity while preserving text alignment.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-30"
    },
    {
        "title": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization",
        "authors": "Tao Yuan, Yuxin Song, Yifan Sun, Xiu-Shen Wei, axxkaya",
        "link": "https://arxiv.org/abs/2412.18525",
        "github_repo": null,
        "summary": "- This paper introduces \"Explanatory Instructions,\" a novel approach to define computer vision task objectives using detailed linguistic descriptions of transformations between images, moving beyond terminological classifications (e.g., \"segmentation\").\n- A large-scale dataset, DECVT, comprising 12 million image-instruction-output triplets, is created, enabling training of an autoregressive vision-language model (AR-based VLM) on various vision tasks and their linguistic objectives.\n- The AR-based VLM, trained on DECVT, demonstrates instruction-level zero-shot capabilities on seen tasks and promising task-level zero-shot generalization on unseen vision tasks like HED-to-Image, Canny-to-Image, and Depth-to-Image.\n- Quantitative evaluations on image editing and generation benchmarks indicate that the proposed model achieves improvement over some baselines, demonstrating its potential as a vision generalist, despite requiring further improvement.\n- The paper highlights the limitations in the model's zero-shot capabilities for tasks like Image-to-Canny or Image-to-Depth and discusses potential reasons, including the image tokenizer's alignment with text during pretraining.",
        "classification": [
            "Computer Vision",
            "Image-to-Image",
            "Zero-Shot Object Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "On the Compositional Generalization of Multimodal LLMs for Medical Imaging",
        "authors": "Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai",
        "link": "https://arxiv.org/abs/2412.20070",
        "github_repo": "https://github.com/FreedomIntelligence/Med-MAT",
        "summary": "- This paper introduces Med-MAT, a Visual Question Answering (VQA) dataset designed to investigate compositional generalization (CG) in Multimodal Large Language Models (MLLMs) applied to medical imaging.\n- Med-MAT comprises 106 medical datasets spanning 11 modalities, 14 anatomical areas, and 13 medical tasks, categorized by MAT-Triplets (Modality, Anatomical area, Task) to facilitate CG analysis.\n- Experiments using LLaVA and other MLLMs on Med-MAT confirm that these models can leverage CG to understand unseen medical image combinations, demonstrating improved performance in classification tasks compared to single-task or unrelated multi-task training.\n- The paper shows that increasing the volume of CG combinations improves model understanding, and that CG assists data-efficient learning even with limited target data.\n- The study also demonstrates the existence of CG across different MLLM backbones (LLaVA, Qwen2-VL, Llama 3.2) and its applicability in detection tasks, highlighting its broad applicability and robustness in medical image analysis.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/FreedomIntelligence/Med-MAT"
        ],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
        "authors": "Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu",
        "link": "https://arxiv.org/abs/2412.20993",
        "github_repo": null,
        "summary": "- Dynasor optimizes inference-time compute for LLM reasoning queries by tracking and scheduling requests within reasoning queries and using certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically.\n- Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost.\n- Dynasor reduces compute by up to 50% in batch processing and sustains 3.3x higher query rates or 4.7x tighter latency SLOs in online serving on diverse datasets and algorithms.\n- Certaindex quantifies how certain the LLM is in approaching its final answer during reasoning, which correlates with computational resources required for correct solutions.\n- Dynasor outperforms other systems by reducing token usage by 9-52% without sacrificing accuracy on offline workloads and achieving higher sustainable request rates and tighter SLO scales.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
        "authors": "Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung",
        "link": "https://arxiv.org/abs/2412.21037",
        "github_repo": null,
        "summary": "- TANGOFLUX, a novel text-to-audio (TTA) model based on a hybrid architecture of Multimodal Diffusion Transformer (MMDIT) and Diffusion Transformer (DiT) blocks, is introduced. \n- The model leverages rectified flows for audio generation, enabling faster inference speeds and utilizes CLAP-Ranked Preference Optimization (CRPO) for alignment. \n- CRPO iteratively generates synthetic audio preference data by ranking model outputs based on CLAP similarity scores and then optimizes the model using a novel loss function that prevents performance degradation. \n- Experimental results show that TANGOFLUX achieves state-of-the-art performance on objective metrics such as Frechet Distance and CLAP score while significantly reducing inference time compared to existing models. \n- Furthermore, human evaluations confirm that TANGOFLUX generates audio of higher quality and greater relevance to the input text compared to other leading TTA models.",
        "classification": [
            "Text-to-Audio",
            "Audio"
        ],
        "github_urls": [
            "https://github.com/declare-lab/TangoFlux"
        ],
        "huggingface_urls": [
            "https://huggingface.co/declare-lab/TangoFlux",
            "https://huggingface.co/datasets/declare-lab/CRPO",
            "https://huggingface.co/spaces/declare-lab/TangoFlux"
        ],
        "date": "2024-12-31"
    },
    {
        "title": "Edicho: Consistent Image Editing in the Wild",
        "authors": "Ceyuan Yang, Qiuyu Wang, Yinghao Xu, Hao Ouyang, Qingyan Bai",
        "link": "https://arxiv.org/abs/2412.21079",
        "github_repo": "https://github.com/EzioBy/edicho",
        "summary": "- Edicho is a training-free, plug-and-play method for consistent image editing across multiple images in the wild, leveraging explicit correspondence.\n- It enhances the self-attention mechanism and classifier-free guidance within diffusion models by incorporating pre-computed correspondence, ensuring coherent edits across images.\n- Fusing features from unconditional embeddings, inspired by null-text inversion, further improves consistency without compromising image quality. \n- Experimental results show Edicho's superior performance in quantitative metrics and qualitative assessments for both local and global editing tasks.\n- The method is robust to variations in lighting, backgrounds, perspectives, and occlusions common in real-world images.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/EzioBy/edicho"
        ],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "Bringing Objects to Life: 4D generation from 3D objects",
        "authors": "Gal Chechik, Dvir Samuel, Ori Malca, Ohad Rahamim",
        "link": "https://arxiv.org/abs/2412.20422",
        "github_repo": null,
        "summary": "- This paper introduces 3to4D, a novel method for generating 4D (dynamic 3D) content from a given 3D object and a text prompt describing the desired motion.\n- The method first creates a static 4D Neural Radiance Field (NeRF) from the input 3D mesh, preserving its appearance and structure. Then, it uses an image-to-video diffusion model, guided by the text prompt, to animate the NeRF, adding motion while maintaining the object's identity.\n- To improve motion realism, the paper proposes two enhancements: an incremental viewpoint selection protocol that samples camera perspectives around the object during optimization and an attention-masked Score Distillation Sampling (SDS) loss that focuses learning on object-relevant regions.\n- Experimental results show that 3to4D outperforms baseline methods, achieving up to three times better identity preservation measured by LPIPS, and demonstrates a better balance between visual quality and dynamic content.\n- The method effectively animates user-provided 3D objects according to text prompts, which enables users to create custom 4D content for various applications such as virtual worlds, media, and gaming.",
        "classification": [
            "Text-to-3D",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation",
        "authors": "Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu",
        "link": "https://arxiv.org/abs/2412.21199",
        "github_repo": null,
        "summary": "- This paper introduces \"self-invoking code generation,\" a new task designed to evaluate LLMs' progressive reasoning and problem-solving abilities where models must use the solution from a base problem to address a more complex, related problem.\n- Three new benchmarks are created: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, which are derived from existing benchmarks using a proposed method.\n- Experimental results on 20 LLMs reveal a significant performance gap between traditional code generation and self-invoking code generation, where models often struggle to utilize their own generated code effectively.\n- It's shown that instruction-tuned models offer limited improvement over base models in self-invoking tasks, indicating a need for more advanced training methods for this type of problem.\n- Analysis of failure modes reveals challenges LLMs face with self-invocation, emphasizing areas for future improvement in code generation and reasoning capabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Question Answering",
            "Feature Extraction"
        ],
        "github_urls": [
            "github.com/CodeEval-Pro/CodeEval-Pro"
        ],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "Facilitating large language model Russian adaptation with Learned Embedding Propagation",
        "authors": "Daniil Chernyshev, RefalMachine",
        "link": "https://arxiv.org/abs/2412.21140",
        "github_repo": null,
        "summary": "- This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models (LLMs) to new languages requiring less training data and minimal disruption of existing LLM knowledge.\n- LEP employs an embedding propagation technique, bypassing instruction-tuning and directly integrating new language knowledge. \n- A new benchmark, Darumeru, is introduced to evaluate text generation robustness during training for Russian adaptation. \n- Applying LEP to adapt LLaMa-3-8B and Mistral-7B for Russian, across four vocabulary adaptation scenarios, demonstrates competitive performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct. \n- Further improvements are observed using self-calibration and additional instruction-tuning, exceeding existing models' performance.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Translation"
        ],
        "github_urls": [
            "https://github.com/RefalMachine/ruadapt",
            "https://github.com/RefalMachine/llmtf_open"
        ],
        "huggingface_urls": [
            "https://huggingface.co/RefalMachine"
        ],
        "date": "2024-12-31"
    },
    {
        "title": "Training Software Engineering Agents and Verifiers with SWE-Gym",
        "authors": "Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan",
        "link": "https://arxiv.org/abs/2412.21139",
        "github_repo": "https://github.com/SWE-Gym/SWE-Gym",
        "summary": "- This paper introduces SWE-Gym, a new training environment for real-world software engineering (SWE) agents.\n- SWE-Gym consists of 2,438 Python tasks from GitHub, each with an executable runtime environment, unit tests, and a natural language task description.\n- Using SWE-Gym to train language model-based SWE agents led to up to a 19% improvement in resolve rate on SWE-Bench Verified and Lite datasets. \n- By training verifiers on agent trajectories from SWE-Gym and combining them with fine-tuned SWE agents, the resolve rate further increased to 32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, setting a new state-of-the-art for open-weight agents.\n- The paper analyzes the scaling behavior of both the training process and the inference-time scaling using verifiers, revealing continuous improvements with increased compute budget.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/SWE-Gym/SWE-Gym"
        ],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
        "authors": "Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo",
        "link": "https://arxiv.org/abs/2412.20005",
        "github_repo": "https://github.com/zjunlp/OneKE",
        "summary": "- OneKE is a dockerized schema-guided knowledge extraction system based on LLMs and a multi-agent design.\n- The system uses three agents: a Schema Agent, an Extraction Agent, and a Reflection Agent to handle various extraction scenarios.\n- It supports extraction from various data sources like web pages and PDF documents without fine-tuning and incorporates a configurable knowledge base to improve performance and allow error debugging.\n- Experimental results on CrossNER and NYT-11-HRL datasets demonstrate the efficacy of OneKE, with the Case Retrieval component of the Extraction Agent showing significant improvement.\n- Case studies on web news and book knowledge extraction further illustrate the practical applicability of OneKE.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/zjunlp/OneKE"
        ],
        "huggingface_urls": [],
        "date": "2024-12-31"
    },
    {
        "title": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization",
        "authors": "Tao Yuan, Yuxin Song, Yifan Sun, Xiu-Shen Wei, axxkaya",
        "link": "https://arxiv.org/abs/2412.18525",
        "github_repo": null,
        "summary": "- This paper introduces \"Explanatory Instructions,\" a method for achieving zero-shot generalization in computer vision tasks by providing linguistic descriptions of transformations between images rather than just task labels.\n- The authors create a 12-million-triplet dataset of image-instruction-output pairs spanning diverse vision tasks and train an autoregressive vision-language model (AR-based VLM).\n- This AR-based VLM takes both images and explanatory instructions as input and demonstrates instruction-level zero-shot capabilities for seen tasks and strong zero-shot generalization for unseen tasks such as image editing, inpainting, and outpainting.\n- While demonstrating improvement over previous models in zero-shot generalization, the proposed model still lags behind specialized or vision generalist models on standard benchmarks, particularly in quantitative metrics for controllable image generation tasks like Canny-to-Image and HED-to-Image.\n- Qualitative results showcase its capacity for tasks like low-light enhancement, deraining, desnowing, and more by following previously unseen instructions, showing promise for flexible, unified vision task understanding.",
        "classification": [
            "Computer Vision",
            "Image-to-Image",
            "Text-to-Image",
            "Zero-Shot Object Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "On the Compositional Generalization of Multimodal LLMs for Medical Imaging",
        "authors": "Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai",
        "link": "https://arxiv.org/abs/2412.20070",
        "github_repo": "https://github.com/FreedomIntelligence/Med-MAT",
        "summary": "- This paper introduces Med-MAT, a visual question answering (VQA) dataset designed to investigate compositional generalization (CG) in multimodal large language models (MLLMs) applied to medical imaging.\n- Med-MAT comprises 106 medical datasets categorized by modality, anatomical area, and task (MAT-Triplet), forming 53 subsets with 11 modalities, 14 anatomical areas, and 13 medical tasks.\n- Experiments demonstrate that MLLMs can leverage CG to understand unseen medical images, which is a key driver of the generalization observed in multi-task training.\n- Further analysis shows that CG supports data-efficient training with limited data and demonstrates consistent performance across different MLLM backbones.\n- The authors also explore CG between detection and classification tasks, finding that MLLMs can combine knowledge from both to improve classification accuracy.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/FreedomIntelligence/Med-MAT"
        ],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "Bringing Objects to Life: 4D generation from 3D objects",
        "authors": "Gal Chechik, Dvir Samuel, Ori Malca, Ohad Rahamim",
        "link": "https://arxiv.org/abs/2412.20422",
        "github_repo": null,
        "summary": "- This paper introduces 3to4D, a novel method for generating 4D (dynamic 3D) content from a given 3D object and a text prompt describing the desired motion.\n- The method first creates a static 4D Neural Radiance Field (NeRF) representation of the 3D object and then animates it using an image-to-video diffusion model, conditioned on the text prompt and rendered views of the 3D object.\n- It incorporates two enhancements: an incremental viewpoint selection protocol for promoting realistic movement and a masked Score Distillation Sampling (SDS) loss to focus optimization on the object.\n- Experimental results on the Google Scanned Objects (GSO) dataset show that 3to4D outperforms baseline methods adapted for this task, achieving up to three times better identity preservation (measured by LPIPS) while maintaining good visual quality and motion fidelity.\n- The proposed method effectively balances preserving the original object's identity with generating dynamic content according to the text prompt.",
        "classification": [
            "Text-to-3D",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
        "authors": "Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu",
        "link": "https://arxiv.org/abs/2412.20993",
        "github_repo": null,
        "summary": "- Dynasor optimizes inference-time compute for LLM reasoning queries by tracking and scheduling requests within reasoning queries and using certaindex, a proxy that measures statistical reasoning progress.\n- Certaindex guides compute allocation dynamically, allocating more compute to hard queries, reducing compute for simpler ones, and terminating unpromising queries early.\n- Evaluated on diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustains 3.3\u00d7 higher query rates or 4.7\u00d7 tighter latency SLOs in online serving.\n- Dynasor is implemented as a scheduling layer compatible with existing serving engines.\n- The system utilizes certaindex as a narrow interface between itself and the applications to support various current and future reasoning algorithms.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
        "authors": "Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung",
        "link": "https://arxiv.org/abs/2412.21037",
        "github_repo": null,
        "summary": "- TANGOFLUX, a 515M parameter text-to-audio (TTA) model based on a hybrid multimodal and diffusion transformer architecture, generates up to 30 seconds of 44.1kHz audio in 3.7 seconds on a single A40 GPU using rectified flows.\n- The model uses CLAP-Ranked Preference Optimization (CRPO), which iteratively generates synthetic audio preference data and employs CLAP as a proxy reward model to improve alignment with textual descriptions.\n- Evaluation on AudioCaps and a challenging out-of-distribution dataset reveals that TANGOFLUX achieves state-of-the-art performance across various objective metrics, including Frechet Distance, KL divergence, CLAP score, and Inception Score, outperforming models like Tango 2, AudioLDM 2, and Stable Audio Open.\n- Human evaluation further confirms TANGOFLUX's superior audio quality and strong alignment with textual prompts.\n- Ablation studies demonstrate the effectiveness of CRPO's online data generation, the use of CLAP as a reward model, and the improvement of LCRPO over LDPO-FM loss for preference optimization.",
        "classification": [
            "Text-to-Audio",
            "Audio"
        ],
        "github_urls": [
            "https://github.com/declare-lab/TangoFlux"
        ],
        "huggingface_urls": [
            "https://huggingface.co/declare-lab/TangoFlux",
            "https://huggingface.co/spaces/declare-lab/TangoFlux",
            "https://huggingface.co/datasets/declare-lab/CRPO"
        ],
        "date": "2025-01-01"
    },
    {
        "title": "Edicho: Consistent Image Editing in the Wild",
        "authors": "Ceyuan Yang, Qiuyu Wang, Yinghao Xu, Hao Ouyang, Qingyan Bai",
        "link": "https://arxiv.org/abs/2412.21079",
        "github_repo": "https://github.com/EzioBy/edicho",
        "summary": "- Edicho is a training-free, plug-and-play method for consistent image editing in the wild, using explicit image correspondence to guide the editing process.\n- It leverages a correspondence-guided denoising process with attention manipulation and a modified classifier-free guidance (CFG) computation incorporating correspondence, along with feature fusion from unconditional embeddings for improved consistency and quality.\n- Unlike previous training-free methods relying on implicit correspondence from attention weights, Edicho utilizes pre-computed explicit correspondence for more robust and accurate feature transfer, effectively handling variations in lighting, background, perspective, and occlusions.\n- Experiments demonstrate superior performance in both qualitative and quantitative metrics for local and global editing tasks compared to existing state-of-the-art methods.\n- The approach is compatible with most diffusion-based editing methods such as ControlNet and BrushNet.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/EzioBy/edicho"
        ],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
        "authors": "Jianhui Pang, Zhiwei He, Tian Liang, Jiahao Xu, Xingyu Chen",
        "link": "https://arxiv.org/abs/2412.21187",
        "github_repo": null,
        "summary": "- This paper investigates the \"overthinking\" issue in large language models (LLMs), particularly those like OpenAI's o1, where excessive computational resources are used for simple problems.\n- The authors introduce novel efficiency metrics from both outcome (accuracy improvement relative to token usage) and process (diversity of reasoning strategies) perspectives to evaluate the rational use of computational resources.\n- They propose a self-training paradigm using the PRM12K dataset and strategies to mitigate overthinking by streamlining reasoning processes while preserving accuracy.\n- Experimental results on various mathematical reasoning datasets, including GSM8K, MATH500, GPQA, and AIME, demonstrate that the proposed approach reduces computational overhead without compromising model performance.\n-  For example, the approach reduces token output by 48.6% while maintaining accuracy on the MATH500 dataset when applied to QwQ-32B-Preview.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "Facilitating large language model Russian adaptation with Learned Embedding Propagation",
        "authors": "Daniil Chernyshev, RefalMachine",
        "link": "https://arxiv.org/abs/2412.21140",
        "github_repo": null,
        "summary": "- This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models (LLMs) to new languages without requiring extensive instruction-tuning data.\n- LEP works by propagating learned embeddings from a pre-trained, instruction-tuned LLM in one language to a new LLM initialized with a vocabulary tailored for the target language.\n- The method was evaluated on Russian adaptation of Mistral-7B and LLaMa-3-8B using a new benchmark called Darumeru, specifically designed for evaluating text generation robustness in Russian.\n- Results show that LEP achieves competitive performance compared to existing models like OpenChat 3.5 and LLaMa-3-8B-Instruct, demonstrating its effectiveness in language adaptation while reducing costs associated with traditional instruction-tuning.\n- Further improvements were observed by calibrating the adapted models through self-instruct tuning and additional instruction-tuning steps, highlighting the potential of LEP for enhancing LLM performance beyond existing benchmarks.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Translation"
        ],
        "github_urls": [
            "https://github.com/RefalMachine/ruadapt",
            "https://github.com/RefalMachine/llmtf_open"
        ],
        "huggingface_urls": [
            "https://huggingface.co/RefalMachine"
        ],
        "date": "2025-01-01"
    },
    {
        "title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
        "authors": "Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo",
        "link": "https://arxiv.org/abs/2412.20005",
        "github_repo": "https://github.com/zjunlp/OneKE",
        "summary": "- OneKE is a dockerized schema-guided knowledge extraction system based on LLMs and a multi-agent design.\n- It supports various data formats (web, PDF) and domains (science, news) through different agents.\n- The system includes a configurable knowledge base for schema configuration and error debugging.\n- Evaluation on benchmark datasets and case studies demonstrate OneKE's efficacy and adaptability.\n- The system is open-source and supports different LLMs without fine-tuning.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/zjunlp/OneKE"
        ],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "Slow Perception: Let's Perceive Geometric Figures Step-by-step",
        "authors": "Liang Zhao, Jia Wang, Yumeng Li, Youyang Yin, Haoran Wei",
        "link": "https://arxiv.org/abs/2412.20631",
        "github_repo": null,
        "summary": "- This paper introduces \"slow perception\" (SP), a novel two-stage approach for improving geometric figure parsing in computer vision.\n- SP first decomposes complex figures into basic point-line units and then uses a \"perceptual ruler\" to trace each line segment stroke-by-stroke, mimicking human perception.\n- This method addresses the limitations of current Large Vision Language Models (LVLMs) which struggle to accurately copy and understand complex geometric figures. \n- Experiments on a synthetic dataset (SP-1) and a real-world dataset of middle school exam figures show that SP improves F1-score by 6% compared to baseline methods.\n- The paper also finds an inference time scaling law where slower, more deliberate tracing leads to better performance.",
        "classification": [
            "Computer Vision",
            "Object Detection"
        ],
        "github_urls": [
            "https://github.com/Ucas-HaoranWei/Slow-Perception"
        ],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "PERSE: Personalized 3D Generative Avatars from A Single Portrait",
        "authors": "Hanbyul Joo, Inhee Lee, Hyunsoo Cha",
        "link": "https://arxiv.org/abs/2412.21206",
        "github_repo": null,
        "summary": "- PERSE generates personalized, animatable 3D avatars from a single portrait image, allowing continuous and disentangled editing of facial attributes.\n- It leverages a novel pipeline to create a synthetic 2D video dataset with diverse attribute variations, which is used to train a 3D Gaussian Splatting-based avatar model.\n- To ensure smooth attribute interpolation, a latent space regularization technique is introduced, using interpolated 2D faces as supervision.\n- PERSE outperforms existing methods in reconstruction quality and identity preservation, as demonstrated by quantitative metrics and user studies on interpolated samples.\n-  Additionally, it offers efficient fine-tuning using Low-Rank Adaptation (LoRA) to integrate new facial attributes from real images.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hyunsoocha.github.io/perse/"
        ],
        "date": "2025-01-01"
    },
    {
        "title": "Training Software Engineering Agents and Verifiers with SWE-Gym",
        "authors": "Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan",
        "link": "https://arxiv.org/abs/2412.21139",
        "github_repo": "https://github.com/SWE-Gym/SWE-Gym",
        "summary": "- This paper introduces SWE-Gym, a new training environment for software engineering (SWE) agents designed to address the limitations of current resources, which often lack executable environments and reward signals.\n- SWE-Gym contains 2,438 real-world Python tasks from GitHub issues, each with a codebase, an executable runtime environment, unit tests, and a natural language task description.\n- The authors demonstrate that fine-tuning a 32B Qwen-2.5 language model with SWE-Gym can achieve state-of-the-art resolve rates of 32.0% and 26.0% on the SWE-Bench Verified and Lite test sets, respectively.\n- This involves an improvement of 19% compared to existing methods on these benchmarks.\n- This is further enhanced by training a verifier on agent trajectories, enabling inference-time scaling through candidate solution selection.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/SWE-Gym/SWE-Gym"
        ],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation",
        "authors": "Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu",
        "link": "https://arxiv.org/abs/2412.21199",
        "github_repo": null,
        "summary": "- This paper introduces \"self-invoking code generation,\" a new task to evaluate LLMs' progressive reasoning and problem-solving skills by requiring them to solve a base problem and then use its solution to address a more complex related problem.\n- Three new benchmarks, HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, are created by extending existing datasets using a proposed general recipe and involve Deepseek-V2.5 for problem generation and human expert review.\n- Experiments on over 20 LLMs reveal that while models excel in traditional code generation, their performance declines significantly on self-invoking tasks, with even top models like o1-mini showing a substantial drop.\n- Instruction-tuned models exhibit only marginal improvements in self-invoking tasks compared to base models.\n- Analysis of failure modes highlights LLMs' struggle with generating code that can effectively self-invoke and suggests limitations in instruction-based fine-tuning for such complex tasks.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation",
            "Question Answering",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/CodeEval-Pro/CodeEval-Pro"
        ],
        "huggingface_urls": [],
        "date": "2025-01-01"
    },
    {
        "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis",
        "authors": "Yian Wang, Chuanyang Jin, Kanzhi Cheng, heroding77, QiushiSun",
        "link": "https://arxiv.org/abs/2412.19723",
        "github_repo": null,
        "summary": "- OS-Genesis is a novel GUI data synthesis pipeline that automates the construction of high-quality and diverse agent trajectories without human supervision or predefined tasks.\n- Instead of relying on predefined tasks, OS-Genesis allows agents to explore environments freely and interact step-wise, retroactively deriving tasks from interactions. This interaction-driven approach reverses the conventional trajectory collection process and enables more effective exploration.\n- The pipeline incorporates a trajectory reward model (TRM) to filter and prioritize the synthesized trajectories for more effective utilization.\n- Evaluations on challenging online mobile benchmarks like AndroidWorld show that OS-Genesis doubles the performance of existing task-driven methods and outperforms other data synthesis approaches on unseen, out-of-distribution apps.\n-  Analysis suggests that OS-Genesis significantly increases the diversity of both generated instructions and trajectories, more closely mirroring human-like interactions with digital environments than existing synthetic data generation methods.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-02"
    },
    {
        "title": "Xmodel-2 Technical Report",
        "authors": "Jiang Ling, Qu Zhijiu, Lin Qingquan, Liu Yang, valeriaWong",
        "link": "https://arxiv.org/abs/2412.19638",
        "github_repo": "https://github.com/XiaoduoAILab/Xmodel-2",
        "summary": "- Xmodel-2 is a 1.2 billion parameter large language model specialized for reasoning tasks, using a unified set of hyperparameters across different model scales for efficient experimentation and configuration transfer.\n- Trained on 1.5 trillion tokens, Xmodel-2 employs the Warmup-Stable-Decay (WSD) learning rate scheduler from MiniCPM for enhanced training efficiency and stability.\n- It achieves state-of-the-art performance on complex reasoning and agent-based tasks compared to other models in the 1-2 billion parameter range, demonstrating the effectiveness of its design and training approach.\n- The model architecture is similar to Llama 2 and incorporates techniques like embedding sharing, deep-and-thin structure and grouped-query attention for optimized training and inference.\n- The model exhibits good calibration properties with predicted confidence closely aligned with actual correctness probabilities.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation",
            "Text2Text Generation",
            "Fill-Mask",
            "Sentence Similarity",
            "Feature Extraction",
            "Summarization",
            "Translation"
        ],
        "github_urls": [
            "https://github.com/XiaoduoAILab/Xmodel-2"
        ],
        "huggingface_urls": [],
        "date": "2025-01-02"
    },
    {
        "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
        "authors": "Yongliang Shen, Jiashuo Sun, Xin Li, Hang Zhang, Wenqi Zhang",
        "link": "https://arxiv.org/abs/2501.00958",
        "github_repo": "https://github.com/DAMO-NLP-SG/multimodal_textbook",
        "summary": "- This paper introduces \"Textbook\", a large-scale, high-quality multimodal dataset designed for vision-language pretraining.\n- The dataset is constructed from 2.5 years of online instructional videos, totaling 22,000 class hours and covering subjects like mathematics, physics, and chemistry. \n- The videos are processed into an interleaved format of images and text, focusing on coherent context and knowledge density.\n- Experimental results demonstrate that models pretrained on \"Textbook\" achieve significant improvements on knowledge- and reasoning-intensive tasks, such as ScienceQA and MathVista.\n- The dataset improves few-shot performance, attributed to its video-centric interleaved design that enhances in-context learning capabilities.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/DAMO-NLP-SG/multimodal_textbook"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control",
        "authors": "Xiang Bai, Sihui Ji, Xi Chen, Hao Luo, Yuanpeng Tu",
        "link": "https://arxiv.org/abs/2501.01427",
        "github_repo": null,
        "summary": "- VideoAnydoor is a novel, end-to-end framework for high-fidelity video object insertion with precise motion control.\n- It leverages a text-to-video diffusion model combined with an ID extractor for preserving object identity and a pixel warper for fine-grained motion control and detail preservation. \n- A reweighted reconstruction loss and an image-video mixed training strategy are employed to further enhance insertion quality.\n- VideoAnydoor surpasses existing methods in qualitative and quantitative comparisons on various video editing tasks, including virtual try-on and face swapping.\n- This method enables zero-shot customization of video content without requiring task-specific fine-tuning.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Image-to-Video"
        ],
        "github_urls": [
            "https://videoanydoor.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings",
        "authors": "Dayiheng Liu, Bo Zheng, Bowen Yu, Jiaxi Yang, Shanghaoran Quan",
        "link": "https://arxiv.org/abs/2501.01257",
        "github_repo": null,
        "summary": "- This paper introduces CODEELO, a new benchmark for evaluating competition-level code generation capabilities of Large Language Models (LLMs).\n- CODEELO uses problems from the CodeForces platform, along with contest divisions, problem difficulty ratings, and problem algorithm tags. \n- The benchmark employs a unique evaluation method where solutions are submitted directly to the CodeForces platform, ensuring accurate judgments with special judge support and a consistent execution environment. \n- An Elo rating system is implemented to provide human-comparable ratings for LLMs based on their performance. \n- Initial evaluation results demonstrate that OpenAI's o1-mini and QwQ-32B-Preview models achieve high Elo ratings, outperforming most open-source models.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/Qwen/CodeElo"
        ],
        "date": "2025-01-03"
    },
    {
        "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
        "authors": "Boqiang Zhang, Zesen Cheng, Wentong Li, Hang Zhang, Yuqian Yuan",
        "link": "https://arxiv.org/abs/2501.00599",
        "github_repo": null,
        "summary": "- The VideoRefer Suite is introduced, which consists of a new dataset (VideoRefer-700K), a model (VideoRefer), and a benchmark (VideoRefer-Bench) designed to improve the spatial-temporal object understanding capabilities of Video Large Language Models (LLMs).\n- VideoRefer, built upon VideoLLaMA2, incorporates a novel spatial-temporal object encoder that extracts pixel-level regional features within single frames and aggregates temporal information across multiple frames using a Temporal Token Merge Module.\n- The VideoRefer-700K dataset was created using a multi-agent data engine, incorporating diverse object-level instructions including descriptions, short captions, and multi-round question-answer pairs.\n- Experimental results demonstrate that VideoRefer outperforms existing generalist and specialist models on VideoRefer-Bench and a traditional video referring benchmark, HC-STVG, showing improved performance in metrics like subject correspondence, appearance and temporal description, and hallucination detection.\n- Moreover, VideoRefer also shows performance improvements on general video understanding benchmarks like Perception-Test, MVBench, and VideoMME, indicating its broader applicability.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
        "authors": "Xinggang Wang, Jingfeng Yao",
        "link": "https://arxiv.org/abs/2501.01423",
        "github_repo": "https://github.com/hustvl/LightningDiT",
        "summary": "- This paper introduces VA-VAE, a novel Vision Foundation model Aligned Variational AutoEncoder, and LightningDiT, an enhanced Diffusion Transformer framework, to address the optimization dilemma in latent diffusion models, where increasing the feature dimension of visual tokens improves reconstruction but hinders generation quality.\n- VA-VAE aligns the latent space with pre-trained vision foundation models during visual tokenizer training using Vision Foundation model alignment Loss (VF Loss), thereby improving the feature distribution of high-dimensional latent spaces.\n- LightningDiT incorporates improved training strategies and architectural designs based on DiT to fully exploit the potential of VA-VAE.\n- This integrated system achieves state-of-the-art performance (FID score of 1.35) on ImageNet 256x256 generation, showing a 21x convergence speedup (FID of 2.11 in 64 epochs) compared to the original DiT.\n- The proposed approach allows for faster convergence of Diffusion Transformers in high-dimensional latent spaces, enabling improved generation performance.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [
            "https://github.com/hustvl/LightningDiT"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
        "authors": "Wenbo Su, Jiaheng Liu, Weixun Wang, Yanan Wu, Xiaoshuai Song",
        "link": "https://arxiv.org/abs/2501.01264",
        "github_repo": null,
        "summary": "- ProgCo, a novel program-driven self-correction method for Large Language Models (LLMs), is introduced, comprising program-driven verification (ProgVe) and refinement (ProgRe).\n- ProgVe employs self-generated, self-executing pseudo-programs for enhanced verification logic and validation, while ProgRe uses dual reflection and refinement on both responses and verification programs to mitigate misleading feedback in complex reasoning.\n- Experiments across instruction-following and mathematical reasoning tasks demonstrates ProgCo's efficacy in achieving effective self-correction.\n- Combining ProgCo with real program tools further enhances performance.\n- ProgCo shows greater improvement in mathematical reasoning tasks compared to existing baselines, highlighting its effectiveness in complex reasoning scenarios.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "A3: Android Agent Arena for Mobile GUI Agents",
        "authors": "Guozhi Wang, Liang Liu, Jiayu Zhang, Hanhao Li, Yuxiang Chai",
        "link": "https://arxiv.org/abs/2501.01149",
        "github_repo": null,
        "summary": "- This paper introduces Android Agent Arena (A3), a new comprehensive evaluation platform for mobile GUI agents, designed to address the limitations of static frame evaluations in prior work.\n- A3 features 201 tasks across 21 widely used third-party apps, categorized into operation, single-frame query, and multi-frame query tasks, along with a larger action space for broader agent compatibility and real-time online evaluations.\n- The platform offers two evaluation methods: task-specific functions and an LLM-based evaluation system using GPT-40 and Gemini 1.5 Pro, which enables semi-autonomous and fully autonomous task evaluation.\n- Experimental results demonstrate that fine-tuned agents struggle in dynamic environments due to a lack of action history and self-correction abilities.\n- Business-level LLMs show potential for information query tasks but face challenges in coordinate-based actions, while specialized agents like AppAgent achieve higher success rates due to advanced reasoning and planning capabilities.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://yuxiangchai.github.io/Android-Agent-Arena/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
        "authors": "Md Hasebul Hasan, Md Tanvir Parvez, Md Tanvir Hassan, Mahir Labib Dihan, eunus",
        "link": "https://arxiv.org/abs/2501.00316",
        "github_repo": null,
        "summary": "- This paper introduces MAPEVAL, a benchmark designed to evaluate the geo-spatial reasoning capabilities of foundation models.\n- MAPEVAL features three task types: textual, API-based, and visual, requiring models to process diverse geo-spatial contexts, perform compositional reasoning, and interact with map tools.\n- An evaluation of 28 prominent foundation models using MAPEVAL revealed that while models like Claude-3.5-Sonnet, GPT-40, and Gemini-1.5-Pro performed competitively, substantial performance gaps exist, especially in the API-based tasks, and all models fall short of human performance.\n- The benchmark includes 700 unique multiple-choice questions spanning locations across 180 cities and 54 countries.\n- Further analyses suggest that integrating external tools, like calculators, can enhance performance in specific sub-tasks, such as calculating straight-line distances and cardinal directions.",
        "classification": [
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/MapEval"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling",
        "authors": "Sijia Luo, Jifan Yu, Jing Zhang, Xiaokang Zhang, KAKA22",
        "link": "https://arxiv.org/abs/2501.01054",
        "github_repo": null,
        "summary": "- This paper introduces CodeRM-8B, a lightweight unit test generator designed for efficient and high-quality unit test scaling in code reward modeling.\n- The authors demonstrate that scaling the number of unit tests improves the quality of the reward signal, particularly for more complex problems, leading to better identification of correct code solutions.\n- CodeRM-8B is trained using a novel data synthesis pipeline that produces high-quality unit tests from existing code instruction-tuning datasets. It leverages supervised fine-tuning (SFT) on Llama3.1-8B and implements a dynamic scaling mechanism adapting to problem difficulty for improved efficiency.\n- Experimental results on HumanEval Plus, MBPP Plus, and LiveCodeBench show significant performance improvements across various models, with CodeRM-8B achieving gains of 18.43% for Llama3-8B and 3.42% for GPT-40-mini on HumanEval Plus, comparable to the much larger Llama3.1-70B model.\n- Dynamic unit test scaling further boosts performance by allocating more computation resources to harder problems, leading to an additional 0.5% performance gain on MBPP Plus under fixed computational cost.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://code-reward-model.github.io"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction",
            "https://huggingface.co/datasets/BAAI/TACO"
        ],
        "date": "2025-01-03"
    },
    {
        "title": "MLLM-as-a-Judge for Image Safety without Human Labeling",
        "authors": "Felix Juefei-Xu, Xiaowen Lin, Shiyu Zhao, Shuming Hu, Zhenting Wang",
        "link": "https://arxiv.org/abs/2501.00192",
        "github_repo": null,
        "summary": "- This paper introduces CLUE (Constitutional MLLM JUdgE), a novel framework for zero-shot image safety judgment using Multimodal Large Language Models (MLLMs) and a predefined safety constitution (a set of safety rules).\n- CLUE addresses challenges like subjective rules, complex constitutions, and model biases by objectifying rules, assessing rule-image relevance using contrastive models, and employing debiased token probabilities with logical precondition chains for judgments.\n- The method includes a deeper reasoning mechanism with cascaded chain-of-thought processes, if necessary, offering high confidence and explanations.\n- The experiments demonstrate CLUE's significant improvement over zero-shot and fine-tuning baselines for image safety classification using objective rules.\n- For example, CLUE with InternVL2-76B achieves 95.9% recall, 94.8% accuracy, and an F1-score of 0.949 on a newly created benchmark.",
        "classification": [
            "Multimodal",
            "Zero-Shot Classification",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing",
        "authors": "Jiajun Zhu, Yuehao Wang, Ruisi Cai, Peihao Wang, pragsri8",
        "link": "https://arxiv.org/abs/2501.00658",
        "github_repo": "https://github.com/VITA-Group/SSM-Bottleneck",
        "summary": "- This paper identifies two key limitations of State Space Models (SSMs): strong recency bias, hindering recall of distant information and robustness, and over-smoothing in deeper architectures, making token representations indistinguishable.\n- The authors theoretically demonstrate the recency bias of SSMs, showing that influential scores between tokens decay exponentially with distance, and empirically validate this through a long-context retrieval task, where SSMs struggle compared to Transformers.\n- Over-smoothing is revealed through scaling experiments and theoretical analysis, demonstrating that SSM layers diminish pairwise differences between memory states, causing performance to plateau and decline with increasing depth.\n- A novel \"polarization\" technique is proposed, reserving dedicated channels in state transition matrices for values of zero and one, simultaneously addressing both recency and over-smoothing.\n- Experiments on associative recall demonstrate that polarization enhances long-range recall accuracy and enables SSMs to benefit from deeper architectures.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/VITA-Group/SSM-Bottleneck"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
        "authors": "Md Rizwan Parvez, Mohammed Eunus Ali, mahirlabibdihan",
        "link": "https://arxiv.org/abs/2412.21015",
        "github_repo": null,
        "summary": "- MAPQATOR is a web application designed to streamline the creation of map-based question answering (QA) datasets by integrating with any map API in a plug-and-play manner, reducing manual effort.\n- It offers visualization tools and caches API responses for consistent ground truth and data reliability, enabling complex geospatial reasoning evaluation and improvement.\n- Evaluation shows MAPQATOR speeds up annotation by at least 30 times compared to manual methods.\n- The system addresses the gap in efficiently annotating language-map reasoning tasks, aiding in developing reliable LLM training datasets.\n- MAPQATOR facilitates better geospatial understanding by centralizing data retrieval, annotation, and visualization, benefiting tasks like complex map reasoning.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/mapqator/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration",
        "authors": "Ceyuan Yang, Yang Zhao, Meng Wei, Zhijie Lin, Jianyi Wang",
        "link": "https://arxiv.org/abs/2501.01320",
        "github_repo": null,
        "summary": "- SeedVR, a novel diffusion transformer model designed for generic video restoration, tackles resolution constraints and long video lengths effectively using shifted window attention and variable-sized windows near boundaries.\n- A causal video variational autoencoder (CVVAE) compresses the input video, significantly reducing computational costs while maintaining high reconstruction quality.\n- Trained on a large-scale dataset of images and videos with native and varying resolutions using a multi-stage progressive training strategy, the model is robust to diverse real-world video restoration tasks. \n- SeedVR surpasses existing methods in speed and performance on various benchmarks, including synthetic, real-world, and AI-generated videos.\n- Notably, it's at least twice as fast as other diffusion-based video restoration methods while having significantly more parameters (2.48B), and achieves state-of-the-art results across diverse benchmarks.",
        "classification": [
            "Text-to-Video",
            "Video Classification",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler"
        ],
        "date": "2025-01-03"
    },
    {
        "title": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization",
        "authors": "Haozhou Sun, Zihan Jia, Zhenbang Xu, Haodong Chen, Yongle Huang",
        "link": "https://arxiv.org/abs/2501.01245",
        "github_repo": "https://github.com/KyleHuang9/SeFAR",
        "summary": "- SeFAR, a semi-supervised fine-grained action recognition framework, is introduced, utilizing dual-level temporal elements modeling and moderate temporal perturbation for enhanced performance.\n- The model captures both fine-grained temporal details and broader temporal context through its dual-level modeling.\n- A novel strong augmentation strategy incorporating moderate temporal perturbation within fine-grained elements is proposed, while preserving temporal order in context elements for consistency regularization.\n- An adaptive regulation mechanism stabilizes training by dynamically adjusting loss weights based on the distribution of teacher model predictions.\n- SeFAR achieves state-of-the-art results on FineGym, FineDiving, UCF101, and HMDB51 datasets and demonstrates improved fine-grained understanding abilities for Multimodal Large Language Models (MLLMs).",
        "classification": [
            "Computer Vision",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/KyleHuang9/SeFAR"
        ],
        "huggingface_urls": [],
        "date": "2025-01-03"
    },
    {
        "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
        "authors": "jzzzzk, Shengcong, lyuukuu, pathcn, SiyuanH",
        "link": "https://arxiv.org/abs/2501.01895",
        "github_repo": null,
        "summary": "- ENERVERSE is a novel framework for generating embodied future spaces for robotic manipulation tasks, integrating chunk-wise autoregressive video diffusion with a sparse memory context for long-range sequence generation and a Free Anchor View (FAV) space for flexible perspectives and enhanced 3D understanding.\n- It uses convolutional and bidirectional attention for local chunk modeling and a chunkwise unidirectional generative paradigm with sparse memory for long sequences, theoretically enabling infinite sequence generation.\n- A data engine pipeline integrates a generative model and 4D Gaussian Splatting (4DGS) to address data scarcity and the sim-to-real gap by iteratively enhancing data quality and diversity.\n- Evaluations on RT-1 dataset for video generation quality and LIBERO benchmark for policy performance demonstrates state-of-the-art results, particularly in long-range manipulation tasks, outperforming methods like DynamicCrafter and OpenVLA.\n- The model's effectiveness is further validated by real-world experiments with AgiBot robots in industrial scenarios requiring precise manipulation.",
        "classification": [
            "Robotics",
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-06"
    },
    {
        "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
        "authors": "hertin, shenyunhang, yifanzhang114, xiongwang, linhaojia13",
        "link": "https://arxiv.org/abs/2501.01957",
        "github_repo": "https://github.com/VITA-MLLM/VITA",
        "summary": "- VITA-1.5 is a multimodal large language model (MLLM) that integrates vision, language, and speech modalities using a three-stage training approach, enabling real-time vision and speech interaction.\n- The model architecture consists of vision and audio encoders with adapters connected to an LLM, and an end-to-end speech generation module, eliminating the need for external ASR and TTS systems.\n- The training process involves vision-language training, followed by audio input tuning and audio output tuning stages which aims at minimizing the training conflicts between the multiple modalities.\n- VITA-1.5 achieves comparable performance to state-of-the-art models on image and video understanding benchmarks and exhibits significant improvements in speech capabilities.\n- Evaluation on ASR benchmarks shows that VITA-1.5 outperforms specialized speech models in both Mandarin and English tasks, highlighting the models ability to integrate effective real-time vision and audio-speech interaction.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Video-Text-to-Text",
            "Automatic Speech Recognition",
            "Text-to-Speech"
        ],
        "github_urls": [
            "https://github.com/VITA-MLLM/VITA"
        ],
        "huggingface_urls": [
            "https://huggingface.co/OpenGVLab/InternViT-300M-448px"
        ],
        "date": "2025-01-06"
    },
    {
        "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
        "authors": "jrwen, whenfra, yifanli, JohnCage, Richard1999",
        "link": "https://arxiv.org/abs/2501.01904",
        "github_repo": "https://github.com/RUCAIBox/Virgo",
        "summary": "- This paper introduces Virgo, a multimodal slow-thinking system designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) for complex visual tasks.\n- The core approach involves fine-tuning a capable MLLM (Qwen2-VL-72B-Instruct) with a small amount of textual long-form thought data, hypothesizing that slow-thinking capacity is primarily associated with the language component and can transfer across modalities.\n- Experimental results on MathVerse, MathVision, OlympiadBench, and MMMU benchmarks demonstrate that Virgo achieves competitive performance compared to commercial reasoning systems, sometimes even surpassing them.\n- It was found that textual reasoning data is generally more effective than visual reasoning data for improving the reasoning ability of the MLLMs.\n- Further analysis suggests that harder tasks benefit more from long thought reasoning, but excessively long reasoning processes may lead to performance degradation; moreover, current visual instruction generation methods do not show significant advantages over textual ones.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/RUCAIBox/Virgo"
        ],
        "huggingface_urls": [],
        "date": "2025-01-06"
    },
    {
        "title": "Graph Generative Pre-trained Transformer",
        "authors": "XiaolinXu, y6q9, RArchered, Spony, xchen16",
        "link": "https://arxiv.org/abs/2501.01073",
        "github_repo": null,
        "summary": "- This paper introduces the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures by predicting the next token in a sequence representing nodes and edges.\n- The model uses a novel sequence-based representation to efficiently encode graph data and employs a transformer decoder architecture for sequence learning.\n- G2PT is shown to achieve superior performance in graph generation tasks compared to existing methods, especially in molecule generation.\n- For downstream tasks like goal-oriented generation and graph property prediction, fine-tuning strategies are proposed, further demonstrating the model's versatility.\n- Extensive experiments on molecule and graph datasets show G2PT's strong performance and generalizability across diverse downstream tasks.",
        "classification": [
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-06"
    },
    {
        "title": "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation",
        "authors": "Jiajun Xu, Yuanming Yang, Jiale Cheng, Yu Huang, xujz0703",
        "link": "https://arxiv.org/abs/2412.21059",
        "github_repo": "https://github.com/THUDM/VisionReward",
        "summary": "- This paper introduces VisionReward, a fine-grained, multi-dimensional reward model for aligning text-to-image and text-to-video generation models with human preferences.\n- VisionReward decomposes human preferences into multiple dimensions, each represented by a series of judgment questions that are linearly weighted and summed to produce an interpretable score. \n-  It surpasses existing methods in preference prediction, particularly in video assessment, outperforming VideoScore by 17.2%.\n- A multi-objective preference learning algorithm (MPO) is also introduced to address the issue of confounding factors within preference data and optimize visual generation models.\n- The authors' approach surpasses existing image and video scoring methods based on both machine metrics and human evaluation.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/THUDM/VisionReward"
        ],
        "huggingface_urls": [],
        "date": "2025-01-06"
    },
    {
        "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
        "authors": "Louise Li, Lyle Goodyear, ngoodman, michaelyli, obiwan96",
        "link": "https://arxiv.org/abs/2501.01540",
        "github_repo": null,
        "summary": "- BoxingGym, a new benchmark, has been introduced to evaluate the performance of autonomous agents in experimental design and model discovery within a scientific context.\n- The benchmark uses 10 simulated environments based on real-world scientific models, allowing agents to actively experiment and revise theories based on data.\n- Evaluation metrics include Expected Information Gain for experiment design and a communication-based approach where an agent's explanation enables a novice agent to make predictions for model discovery.\n- Initial experiments show that current LLMs, augmented or not with statistical modeling capabilities, struggle with both experimental design and model discovery.\n- The benchmark aims to promote research on agents capable of iterative model discovery through active experimentation and communication.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/kanishkg/boxing-gym/tree/v0.1.0-beta"
        ],
        "huggingface_urls": [],
        "date": "2025-01-06"
    },
    {
        "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models",
        "authors": "anoperson, Franck-Dernoncourt, ryanrossi, ntnghia1811, Hieuman",
        "link": "https://arxiv.org/abs/2501.00874",
        "github_repo": null,
        "summary": "- LUSIFER is a novel zero-shot approach that adapts English LLM-based embedding models for multilingual tasks without requiring explicit multilingual supervision.\n- It leverages XLM-R's multilingual representations and a learnable connector to transfer language understanding to English-optimized LLM embedding models.\n- Experimental results on 123 datasets across 14 languages show a significant performance increase, averaging 3.19 points across all tasks, with substantial gains for medium and low-resource languages.\n- In cross-lingual scenarios involving over 100 languages, LUSIFER surpasses existing English-centric models by 5.75 points on average.\n- This approach enhances multilingual representation capabilities without the need for explicit multilingual supervision.",
        "classification": [
            "Natural Language Processing",
            "Sentence Similarity",
            "Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/hieum98/lusifer"
        ],
        "huggingface_urls": [],
        "date": "2025-01-06"
    },
    {
        "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
        "authors": "yingtai, zhenheny, chenzhao, yinhongliu, SherryX",
        "link": "https://arxiv.org/abs/2501.02976",
        "github_repo": null,
        "summary": "- STAR, a novel Spatial-Temporal Augmentation approach, leverages text-to-video (T2V) diffusion priors for real-world video super-resolution, enhancing spatial details and temporal consistency.\n- It introduces a Local Information Enhancement Module (LIEM) before global self-attention in the T2V model to mitigate artifacts caused by complex real-world degradations.\n- A Dynamic Frequency (DF) Loss guides the model to prioritize different frequency information during diffusion steps, improving fidelity.\n- Experiments demonstrate STAR outperforms state-of-the-art methods on both synthetic and real-world video super-resolution benchmarks in clarity (DOVER scores) while maintaining robust temporal consistency.\n- Ablation studies show the effectiveness of proposed LIEM, DF Loss and demonstrate improvement when scaling to larger T2V models like CogVideoX.",
        "classification": [
            "Computer Vision",
            "Text-to-Video",
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/THUDM/CogVideoX-5b"
        ],
        "date": "2025-01-07"
    },
    {
        "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
        "authors": "lindahua, yhcao, KennyUTC, yuhangzang, BeichenZhang",
        "link": "https://arxiv.org/abs/2501.03226",
        "github_repo": "https://github.com/beichenzbc/BoostStep",
        "summary": "- BoostStep, a novel method, enhances the mathematical reasoning capabilities of Large Language Models (LLMs) by improving single-step reasoning through refined in-context learning.\n- It addresses the granularity mismatch and negative-effect noise within traditional in-context learning examples by providing step-level guidance with a \"first-try\" strategy.\n- This strategy retrieves highly related in-context examples based on the model's initial reasoning attempt for each step, improving reasoning quality.\n- BoostStep improves the performance of GPT-40 and Qwen2.5-Math-72B by 3.6% and 2.0% respectively on various mathematical benchmarks, and by 7.5% when combined with Monte Carlo Tree Search (MCTS).\n- It seamlessly integrates with MCTS, improving both candidate generation and decision-making processes.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/beichenzbc/BoostStep"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
        "authors": "myownskyW7, lindahua, yhcao, yuhangzang, Mar2Ding",
        "link": "https://arxiv.org/abs/2501.03218",
        "github_repo": "https://github.com/Mark12Ding/Dispider",
        "summary": "- Dispider is a novel system designed for active, real-time interaction with streaming videos, disentangling perception, decision, and reaction into asynchronous modules.\n- It features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction using scene-based features and historical interactions.\n- An asynchronous interaction module provides detailed responses without interrupting video processing, ensuring real-time performance and multi-step reasoning capabilities.\n- Experiments on StreamingBench and a subset of ETBench show Dispider significantly outperforms existing online video LLM models in temporal grounding and proactive response generation.\n- It also maintains strong performance on conventional video QA tasks across long-video benchmarks such as EgoSchema, VideoMME, and MLVU, demonstrating effectiveness in handling long video lengths and complex interactions.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Mark12Ding/Dispider"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Personalized Graph-Based Retrieval for Large Language Models",
        "authors": "Franck-Dernoncourt, namyongp, Ojasmitha17, Tobilee, StevenAu",
        "link": "https://arxiv.org/abs/2501.02157",
        "github_repo": null,
        "summary": "- This paper introduces PGraphRAG, a framework leveraging user-centric knowledge graphs for personalized text generation with LLMs.\n- PGraphRAG enhances personalization by augmenting prompts with user-relevant context retrieved from the knowledge graph, improving contextual understanding and output quality.\n- A new benchmark, the Personalized Graph-based Benchmark for Text Generation, is presented to evaluate the effectiveness of PGraphRAG.\n- Experimental results demonstrate that PGraphRAG significantly outperforms state-of-the-art personalization methods, especially in cold-start scenarios with limited user history.\n- The integration of structured user knowledge graphs through PGraphRAG allows for richer, contextually appropriate personalized responses.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/PGraphRAG-benchmark/PGR-LLM"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Test-time Computing: from System-1 Thinking to System-2 Thinking",
        "authors": "Jia Xu, Kaixin Wu, Hai Ye, douvleplus, Yisam",
        "link": "https://arxiv.org/abs/2501.02497",
        "github_repo": null,
        "summary": "- This paper surveys test-time computing methods, categorizing them into System-1 (for perceptual tasks) and System-2 (for cognitive tasks) models.\n- For System-1, test-time adaptation methods like parameter updates, input modification, representation editing, and output calibration are discussed, focusing on enhancing robustness and generalization.\n- For System-2, techniques such as repeated sampling, self-correction, and tree search are explored, aiming to improve reasoning and planning abilities.\n- The paper traces the evolution from System-1 to System-2 thinking, emphasizing test-time computing's role in this transition, and suggests potential future directions.\n- The o1 model is used as an example of the test-time computing scaling effect where increased computational effort at inference leads to improved performance in complex reasoning.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Dereck0602/Awesome_Test_Time_LLMs"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
        "authors": "willieneis, oliu-io, upup-ashton-wang, Johannes, oliu-io",
        "link": "https://arxiv.org/abs/2501.02045",
        "github_repo": null,
        "summary": "- METAGENE-1 is a 7-billion parameter autoregressive transformer model, pre-trained on a novel corpus of 1.5 trillion base pairs of metagenomic DNA and RNA sequences from wastewater.\n- The model utilizes byte-pair encoding (BPE) tokenization tailored for metagenomic sequences and a decoder-only architecture.\n- In benchmarks, METAGENE-1 achieves state-of-the-art results on genomic tasks like pathogen detection and sequence embedding, outperforming models trained on curated species genomes.\n- It demonstrates potential for pandemic monitoring and early detection of emerging health threats through wastewater analysis.\n- The model's open-source release aims to accelerate research in genomic anomaly detection while acknowledging safety considerations for future model development.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "github.com/metagene-ai"
        ],
        "huggingface_urls": [
            "huggingface.co/metagene-ai"
        ],
        "date": "2025-01-07"
    },
    {
        "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
        "authors": "Yijun Li, yingcongchen, HeZhang, zhifeichen097, wileewang",
        "link": "https://arxiv.org/abs/2501.03006",
        "github_repo": null,
        "summary": "- TransPixar introduces a novel approach for generating RGBA videos from text, extending pre-trained Diffusion Transformer (DiT) based text-to-video models.\n- The method incorporates alpha-specific tokens and LoRA-based fine-tuning for joint RGB and alpha channel generation.\n- By optimizing the attention mechanism, specifically the RGB-attend-to-Alpha component, TransPixar achieves strong alignment between RGB and alpha channels.\n- This approach effectively addresses the limitations of existing methods, which often struggle with transparency and complex visual effects due to limited RGBA training data.\n- Experimental results demonstrate TransPixar's ability to generate diverse and consistent RGBA videos, as showcased in qualitative comparisons and user studies that indicate superior performance compared to alternative methods.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://wileewang.github.io/TransPixar/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Ingredients: Blending Custom Photos with Video Diffusion Transformers",
        "authors": "Di Qiu, MichaelFan, Changqian, Debang, onion",
        "link": "https://arxiv.org/abs/2501.01790",
        "github_repo": "https://github.com/feizc/Ingredients",
        "summary": "- Ingredients, a training-free framework, is introduced for customizing video generation with video diffusion transformers by incorporating multiple user-provided ID photos.\n- The framework includes a facial extractor, a multi-scale projector, and an ID router to handle ID features, embed them into the video diffusion transformer context, and allocate the ID embeddings to corresponding regions, respectively.\n- Ingredients supports multi-ID customization without prompt constraints, offering flexibility and precision in video synthesis.\n- Evaluations show Ingredients' superior performance in generating high-quality, editable videos with consistent multi-human customization, exceeding baseline methods quantitatively and qualitatively.\n- The data, code, and model weights are publicly available for research.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/feizc/Ingredients"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct",
            "https://huggingface.co/openai/clip-vit-base-patch32"
        ],
        "date": "2025-01-07"
    },
    {
        "title": "DepthMaster: Taming Diffusion Models for Monocular Depth Estimation",
        "authors": "Ruijie Zhu, Hao Zhang, Bo Li, Zerong Wang, Ziyang Song",
        "link": "https://arxiv.org/abs/2501.02576",
        "github_repo": null,
        "summary": "- DepthMaster, a novel single-step diffusion model, is proposed for monocular depth estimation, enhancing generalization and detail preservation by adapting generative features for the discriminative task.\n- A Feature Alignment module integrates semantic information by aligning feature distributions of the diffusion model with a high-quality external encoder, mitigating overfitting to texture details.\n- A Fourier Enhancement module refines high-frequency details by operating in the frequency domain, balancing low-frequency structure and high-frequency details to improve visual quality, simulating multi-step refinement in a single pass.\n- A two-stage training strategy first focuses on global scene structure using the Feature Alignment module and then refines fine-grained details using the Fourier Enhancement module.\n- DepthMaster achieves state-of-the-art zero-shot performance and superior detail preservation, outperforming other diffusion-based methods on various datasets, demonstrating a 17.2% improvement over Marigold on KITTI in terms of absolute relative error.",
        "classification": [
            "Depth Estimation",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
        "authors": "Yaniv Taigman, Shelly Sheynin, Amit Zohar, Yuval Kirstain, GuyYariv",
        "link": "https://arxiv.org/abs/2501.03059",
        "github_repo": null,
        "summary": "- This paper introduces THROUGH-THE-MASK, a two-stage image-to-video generation model that leverages mask-based motion trajectories. \n- The first stage generates mask-based motion trajectories conditioned on the input image, initial segmentation mask, and a motion-specific text prompt. \n- The second stage generates the video, conditioned on the reference image, generated mask-based motion trajectories, a global text prompt, and object-specific prompts using masked cross-attention and masked self-attention mechanisms. \n- The model achieves state-of-the-art performance on the Image-Animation-Bench and a newly introduced SA-V-128 benchmark, outperforming existing methods in various metrics, including temporal coherence and text faithfulness. \n- Ablation studies demonstrate the effectiveness of the masked attention mechanism and the advantage of using mask-based trajectories over optical flow.",
        "classification": [
            "Image-to-Video",
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
        "authors": "Yijin Li, Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, wangfuyun",
        "link": "https://arxiv.org/abs/2501.02690",
        "github_repo": null,
        "summary": "- GS-DiT, a novel video generation framework, introduces pseudo 4D Gaussian fields constructed via efficient dense 3D point tracking to enable 4D control in videos, including multi-camera shooting and dolly zoom effects, without requiring multi-view training data.\n- A novel efficient Dense 3D Point Tracking (D3D-PT) method is proposed, outperforming SpatialTracker in accuracy and accelerating inference speed significantly.\n- The framework constructs pseudo 4D Gaussian fields from 3D point trajectories, rendering novel view videos to guide a fine-tuned pre-trained Diffusion Transformer (DiT) model, dubbed GS-DiT.\n- GS-DiT generates videos conditioned on rendered videos from the pseudo 4D Gaussian field and effectively generalizes to real-world videos due to its training strategy that leverages monocular videos.\n- The model enables advanced cinematic effects by manipulating Gaussian field and camera intrinsics, showcasing 4D control capabilities beyond camera poses, as demonstrated by its superior performance in multi-camera shooting compared to GCD and MonST3R.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models",
        "authors": "Weiqiang Wang, Huijia Zhu, Yaojie Lu, Shuhen Zhou, Yanjiang Liu",
        "link": "https://arxiv.org/abs/2501.01830",
        "github_repo": null,
        "summary": "- AUTO-RT, a reinforcement learning framework, automatically explores and optimizes complex attack strategies to uncover security vulnerabilities in Large Language Models (LLMs) through malicious queries.\n- It introduces Early-terminated Exploration to accelerate exploration by focusing on high-potential attack strategies and a Progressive Reward Tracking algorithm to dynamically refine the search trajectory towards successful vulnerability exploitation.\n- It operates in a black-box setting, requiring only access to a model's textual outputs, making it adaptable to diverse LLMs.\n- Experiments across various LLMs demonstrate that AUTO-RT detects a broader range of vulnerabilities with a faster detection speed and 16.63% higher success rate compared to existing methods.\n- AUTO-RT improves exploration efficiency and automatically optimizes attack strategies.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/icip-cas/Auto-RT"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Samba-asr state-of-the-art speech recognition leveraging structured state-space models",
        "authors": "Kartik-angadi, kruthika, SyedAbdul",
        "link": "https://arxiv.org/abs/2501.02832",
        "github_repo": null,
        "summary": "- Samba-ASR is a novel Automatic Speech Recognition (ASR) model employing the Mamba architecture for both encoding and decoding, built upon state-space models (SSMs).\n- Unlike transformer-based ASR models, Samba-ASR utilizes efficient state-space dynamics to model dependencies, resulting in significant performance gains and linear scaling with input length.\n- Across benchmarks such as Gigaspeech, LibriSpeech, and SPGISpeech, Samba-ASR demonstrates state-of-the-art performance, surpassing existing open-source transformer-based ASR models.\n- Notably, Samba-ASR achieves a Word Error Rate (WER) of 1.17% on LibriSpeech Clean, 2.48% on LibriSpeech Other, 9.12% on Gigaspeech, and 1.84% on SPGISpeech.\n- The inherent efficiency of the Mamba architecture translates to reduced training time and inference latency, making Samba-ASR a scalable and robust solution for diverse ASR tasks.",
        "classification": [
            "Automatic Speech Recognition"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use",
        "authors": "Yufei Xu, Xuesong Yao, Zhengyin Du, Junjie Ye, maverick1994",
        "link": "https://arxiv.org/abs/2501.02506",
        "github_repo": null,
        "summary": "- ToolHop, a new dataset with 995 multi-hop queries and 3,912 associated tools, is introduced to evaluate large language models (LLMs) in multi-hop tool use scenarios.\n- ToolHop employs a query-driven data construction approach involving tool creation, document refinement, and code generation.\n- An evaluation of 14 LLMs across five model families (LLaMA, Qwen, Gemini, Claude, and GPT) reveals that even the top-performing model (GPT-4) only achieves 49.04% accuracy.\n- Analysis suggests that providing LLMs with tools significantly improves their performance, but there are still significant challenges.\n- Different LLM families exhibit distinct tool-use patterns, with Qwen tending towards parallel calls that result in hallucinations, while GPT leverages tool feedback effectively to improve tool usage.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/bytedance-research/ToolHop"
        ],
        "date": "2025-01-07"
    },
    {
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models",
        "authors": "chuyi777",
        "link": "https://arxiv.org/abs/2501.03262",
        "github_repo": "https://github.com/OpenRLHF/OpenRLHF",
        "summary": "- REINFORCE++ is an enhanced version of the REINFORCE algorithm designed for aligning large language models (LLMs) with human preferences by incorporating key optimizations from Proximal Policy Optimization (PPO) while eliminating the need for a critic network.\n- The approach focuses on simplicity, training stability, and efficiency.\n- Empirical evaluations demonstrate that REINFORCE++ exhibits superior stability compared to Group Relative Policy Optimization (GRPO) and achieves higher computational efficiency than PPO while maintaining comparable performance.\n- REINFORCE++ integrates token-level KL penalties and a PPO-clip loss to enhance training stability, and it uses mini-batch updates with reward normalization, clipping, and scaling for efficiency.\n- Experiments were conducted on general and mathematical domains, showing that REINFORCE++ effectively addresses reward and output length hacking issues while improving reward increase per unit KL divergence.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/OpenRLHF/OpenRLHF"
        ],
        "huggingface_urls": [
            "https://huggingface.co/OpenRLHF/Llama-3-8b-sft-mixture",
            "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
            "https://huggingface.co/datasets/OpenRLHF/prompt-collection-v0.1",
            "https://huggingface.co/datasets/OpenRLHF/preference_700K",
            "https://huggingface.co/datasets/meta-math/MetaMathQA"
        ],
        "date": "2025-01-08"
    },
    {
        "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models",
        "authors": "Lefan Wang, Weihan Wang, Zhuoyi Yang, LiquidAmmonia, wenyi",
        "link": "https://arxiv.org/abs/2501.02955",
        "github_repo": null,
        "summary": "- This paper introduces MotionBench, a new benchmark designed to evaluate the fine-grained motion comprehension capabilities of video understanding models.\n- MotionBench consists of 8,052 questions across six motion-related categories, sourced from diverse web videos, public datasets, and synthetic videos, addressing a gap in existing benchmarks.\n- Experimental results reveal that current state-of-the-art Vision Language Models (VLMs) perform poorly on MotionBench, struggling to understand subtle motions, often achieving accuracy below 60%.\n- To address this, the authors propose a novel Through-Encoder (TE) Fusion method, enhancing video feature representation by deeply integrating fusion within the visual encoder.\n- TE Fusion achieves state-of-the-art performance on MotionBench, demonstrating particular advantages in high compression ratio scenarios, outperforming existing feature compression methods on other video understanding benchmarks.",
        "classification": [
            "Visual Question Answering",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://motion-bench.github.io"
        ],
        "date": "2025-01-08"
    },
    {
        "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
        "authors": "Yang Feng, Zhe Yang, Qingkai Fang, Shaolei Zhang",
        "link": "https://arxiv.org/abs/2501.03895",
        "github_repo": "https://github.com/ictnlp/LLaVA-Mini;",
        "summary": "- LLaVA-Mini is an efficient large multimodal model (LMM) that minimizes the number of vision tokens while maintaining performance comparable to larger models like LLaVA.\n- It introduces a modality pre-fusion module to combine visual and textual information before compression, enabling the model to use only one vision token per image or video frame.\n- LLaVA-Mini incorporates a query-based compression module to reduce the number of vision tokens, leading to significant improvements in computational efficiency (77% FLOPs reduction), inference speed (2.92x faster), and memory usage.\n- Experiments across 11 image and 7 video benchmarks demonstrate LLaVA-Mini's ability to achieve comparable performance to LLaVA with just 1 vision token instead of 576, even on high-resolution images and long videos.\n- It also allows for processing long videos exceeding 10,000 frames on a 24GB GPU, a significant advancement in efficient real-time multimodal interaction.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/ictnlp/LLAVA-Mini"
        ],
        "huggingface_urls": [
            "https://huggingface.co/ICTNLP/1lava-mini-llama-3.1-8b"
        ],
        "date": "2025-01-08"
    },
    {
        "title": "Cosmos World Foundation Model Platform for Physical AI",
        "authors": "Yogesh Balaji, Maciej Bala, Arslan Ali, Niket Agarwal, NVIDIA",
        "link": "https://arxiv.org/abs/2501.03575",
        "github_repo": "https://github.com/NVIDIA/Cosmos",
        "summary": "- NVIDIA introduces the Cosmos World Foundation Model (WFM) Platform, a comprehensive suite of tools for building customizable world models designed for Physical AI applications.\n- The platform offers pre-trained WFMs based on transformer architectures for both diffusion and autoregressive approaches, trained on a massive dataset of 100M video clips and 20M hours of video.\n- Cosmos WFM introduces innovations like a video data curation pipeline, causal video tokenizers for efficient representation, and techniques for scaling model training with joint image and video training on a 10,000-GPU cluster.\n- The platform also includes a diffusion decoder to enhance autoregressive model generation, a prompt upsampler, and a robust guardrail system for safety.\n- Evaluation demonstrates state-of-the-art performance in various downstream tasks including camera control for 3D world generation, robotic manipulation through instruction following and action conditioning, and multi-view generation for autonomous driving scenarios, outperforming existing methods like VideoLDM and CamCo.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/NVlabs/ShotBench",
            "https://github.com/NVIDIA/Cosmos-Tokenizer",
            "https://github.com/NVIDIA/Cosmos",
            "https://github.com/pytorch-labs/gpt-fast"
        ],
        "huggingface_urls": [],
        "date": "2025-01-08"
    },
    {
        "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos",
        "authors": "Shilin Xu, Zilong Huang, Tao Zhang, Xiangtai Li, HarborYuan",
        "link": "https://arxiv.org/abs/2501.04001",
        "github_repo": null,
        "summary": "- Sa2VA is a novel unified model for dense grounded understanding of images and videos, integrating the Segment Anything Model 2 (SAM-2) with Large Language and Vision Assistant (LLaVA)-like Multimodal Large Language Models (MLLMs).\n- This architecture unifies text, image, and video data within a shared token space, enabling instruction-guided mask generation by SAM-2, which facilitates grounded multimodal understanding.\n- Sa2VA supports various tasks such as image and video conversations, referring image/video segmentation, and grounded caption generation through a single-shot instruction-tuning process. \n- The model achieves state-of-the-art performance across multiple tasks, including referring video object segmentation, outperforming previous methods by a significant margin on the Ref-SAV dataset (over 15% under zero-shot settings).\n-  A key contribution is the introduction of Ref-SAV, a new large-scale dataset for referring video segmentation, which consists of more than 72,000 video object expressions. ",
        "classification": [
            "Multimodal",
            "Image Segmentation",
            "Visual Question Answering",
            "Video Classification",
            "Mask Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/ByteDance/Sa2VA-4B"
        ],
        "date": "2025-01-08"
    },
    {
        "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control",
        "authors": "Zhiyang Dou, Jiahao Lu, Rui Yan, Zekai Gu, pengHTYX",
        "link": "https://arxiv.org/abs/2501.03847",
        "github_repo": "https://github.com/IGL-HKUST/DiffusionAsShader",
        "summary": "- DaS, a 3D-aware video diffusion model, introduces versatile video control by leveraging 3D tracking videos as control signals.\n- DaS uses a transformer-based latent diffusion model with a variational autoencoder (VAE), where the 3D tracking video is processed by a trainable copy of the denoising DiT.\n- The 3D tracking videos, generated from colored dynamic 3D points, enhance temporal consistency and enable precise control over various tasks such as mesh-to-video generation, camera control, motion transfer, and object manipulation.\n- Experiments demonstrate DaS's superior performance in camera control and motion transfer compared to existing methods.\n- DaS exhibits strong control capabilities across diverse tasks after being fine-tuned on less than 10k videos using 8 H800 GPUs for only 3 days.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/IGL-HKUST/DiffusionAsShader"
        ],
        "huggingface_urls": [],
        "date": "2025-01-08"
    },
    {
        "title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting",
        "authors": "Jihyong Oh, Won-Sik Cheong, Jun Young Jeong, Joonsoo Kim, Sangwoon Kwak",
        "link": "https://arxiv.org/abs/2501.03714",
        "github_repo": null,
        "summary": "- MoDec-GS, a memory-efficient 3D Gaussian Splatting framework, reconstructs novel views from videos containing complex motion using Global-to-Local Motion Decomposition (GLMD).\n- GLMD employs Global Anchor Deformation (GAD) for global motion, deforming the position and attributes of anchors within a Global Canonical Scaffold-GS, and Local Gaussian Deformation (LGD) refines local motion by explicitly deforming reconstructed 3D Gaussians using a shared local hexplane.\n- Temporal Interval Adjustment (TIA) automatically controls the temporal intervals of each Local Canonical Scaffold-GS during training without pre-computed motion data, optimizing representation based on scene motion complexity.\n- MoDec-GS significantly reduces model size while maintaining or improving rendering quality compared to existing methods based on experiments of three monocular video datasets.\n- Evaluation on the iPhone, HyperNeRF, and Nvidia datasets demonstrates an average 70% reduction in model size and improved PSNR and SSIM while retaining comparable LPIPS scores.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-08"
    },
    {
        "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
        "authors": "Hongyu Lin, Jia Zheng, Hao Kong, Xinyan Guan, Forceless",
        "link": "https://arxiv.org/abs/2501.03936",
        "github_repo": "https://github.com/icip-cas/PPTAgent",
        "summary": "- PPTAgent, a novel framework, redefines automatic presentation generation as an interactive, edit-based workflow using both a document and reference presentation as inputs.\n- The two-stage framework first analyzes reference presentations for structural patterns and content through slide clustering and schema extraction, and then generates new slides via code actions, ensuring consistency and alignment through feedback mechanisms.\n- PPTAgent introduces PPT Eval, a comprehensive evaluation framework for presentation quality across three dimensions: Content, Design, and Coherence, using a multi-dimensional LLM-as-a-judge approach.\n- Experimental results demonstrate that PPTAgent outperforms current end-to-end text-generation methods, achieving a 97.8% success rate and a 3.67 average PPT Eval score across three dimensions: Content, Design, and Coherence, indicating high-quality presentation generation.\n- A new presentation dataset Zenodo10K consisting of 10,448 presentations with diverse domains is collected from Zenodo.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/icip-cas/PPTAgent"
        ],
        "huggingface_urls": [],
        "date": "2025-01-08"
    },
    {
        "title": "MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control",
        "authors": "Guoying Zhao, Huai-Qian Khor, Xingxun Jiang, Tuomas Varanka, Mengting Wei",
        "link": "https://arxiv.org/abs/2501.02260",
        "github_repo": "https://github.com/weimengting/MagicFace",
        "summary": "- MagicFace is a novel diffusion-based model for high-fidelity facial expression editing that leverages Action Units (AUs) for precise and localized control.\n- The model architecture comprises a denoising UNet conditioned on AU variations, an ID encoder to preserve identity details through self-attention, and an Attribute Controller to maintain background and pose consistency.\n- MagicFace outperforms existing facial expression editing methods in terms of AU accuracy, identity preservation, and image similarity.\n- The model exhibits strong generalization to out-of-domain images and characters with unseen AUs and visual styles, allowing for flexible and extreme expression editing.\n- MagicFace utilizes AU dropout during training with classifier-free guidance, providing control over expression intensity",
        "classification": [
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/weimengting/MagicFace"
        ],
        "huggingface_urls": [
            "https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"
        ],
        "date": "2025-01-08"
    },
    {
        "title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers",
        "authors": "Zexin Yan, Bohao Peng, Bin Xia, Yaoyang Liu, julianjuaner",
        "link": "https://arxiv.org/abs/2501.03931",
        "github_repo": "https://github.com/dvlab-research/MagicMirror/",
        "summary": "- Magic Mirror is a novel framework for generating identity-preserved videos using a dual-branch facial feature extractor, a lightweight cross-modal adapter, and a two-stage training strategy.\n- The dual-branch extractor captures both identity and structural facial features, which are then integrated with text embeddings through a cross-modal adapter with Conditioned Adaptive Normalization (CAN).\n- The two-stage training strategy involves pre-training on image data for robust identity representation and fine-tuning on video data for temporal consistency.\n- This method leverages Video Diffusion Transformers and outperforms existing methods in generating high-quality videos with dynamic motion while maintaining strong identity consistency.\n- Experimental results demonstrate superior performance across multiple metrics, including facial similarity, motion dynamics, and text alignment, without requiring per-identity fine-tuning.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/dvlab-research/MagicMirror/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-08"
    },
    {
        "title": "Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback",
        "authors": "Tao Chen, Botian Shi, Xiangchao Yan, Jiakang Yuan, BoZhang",
        "link": "https://arxiv.org/abs/2501.03916",
        "github_repo": null,
        "summary": "- DOLPHIN is a closed-loop, open-ended automatic research framework designed to automate the scientific research process, encompassing idea generation, experimental verification, and feedback.\n- It retrieves and ranks relevant papers based on topic and task attributes, generates novel research ideas guided by these papers, filters them for novelty and independence, and formulates experimental plans using LLMs.\n- DOLPHIN automatically generates and debugs code using an exception-traceback-guided process and analyzes experimental results to provide feedback for subsequent idea generation rounds.\n- Experimental results on benchmarks like CIFAR-100, ModelNet40, and SST-2 show that DOLPHIN generates ideas that improve performance over baselines and, in some cases, achieves state-of-the-art results, demonstrating its potential for automated research.\n-  DOLPHIN automatically generated methods based on PointNet that showed comparable performance to human-designed state-of-the-art 3D classification methods on ModelNet40.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Zero-Shot Object Detection",
            "Text-to-3D",
            "Image-to-3D",
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-08"
    },
    {
        "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
        "authors": "Youran Sun, Yifei Liu, Xinyu Guan, J-shang, lynazhang",
        "link": "https://arxiv.org/abs/2501.04519",
        "github_repo": "https://github.com/microsoft/rStar",
        "summary": "- rStar-Math is a novel framework that allows smaller language models to achieve state-of-the-art mathematical reasoning capabilities comparable to OpenAI's models.\n- It employs Monte Carlo Tree Search (MCTS) with a math policy SLM and a process reward model (PRM), and introduces innovations in data synthesis, reward modeling, and self-evolution.\n- A code-augmented chain-of-thought data synthesis method generates verifiable reasoning steps, and a process preference model (PPM) is trained using a pairwise ranking loss, eliminating the need for precise step-level reward annotation.\n- The system iteratively evolves the policy SLM and PPM to improve reasoning capabilities without relying on distillation from larger models.\n- Evaluations on various benchmarks show significant performance boosts, rivaling or exceeding OpenAI ol on competition-level problems, even with smaller model sizes.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/microsoft/rStar"
        ],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics",
        "authors": "Xinzhe Ni, Yiyao Yu, Yifan Wang, fun6668, AntimageTHU",
        "link": "https://arxiv.org/abs/2501.04686",
        "github_repo": null,
        "summary": "- This paper introduces URSA-7B, a multimodal large language model (MLLM) designed for enhanced mathematical reasoning, using a three-module synthesis strategy integrating chain-of-thought (CoT) distillation, trajectory format rewriting, and format unification for training data creation.\n- The model architecture comprises a hybrid vision encoder (SAM-B and SigLIP-L) combined with Qwen2.5-Math-7B-Instruct and trained with an MLP projector aligner between the vision and language models.\n- URSA-7B achieves state-of-the-art performance on several multimodal mathematical reasoning benchmarks, including MathVista, MathVerse, and DYNAMATH, outperforming other open-source models and some closed-source models.\n- For test-time scaling, a dual-view process supervision data synthesis strategy is proposed, generating the DualMath-1.1M dataset and URSA-RM-7B, a verifier model that improves URSA-7B's reasoning path selection and accuracy.\n- URSA-RM-7B shows strong out-of-distribution (OOD) verification capabilities, particularly on the Multimath-7B CoT solutions, indicating improved robustness and generalisation in multimodal mathematical reasoning",
        "classification": [
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though",
        "authors": "Kanishk Gandhi, Charlie Snell, Violet Xiang, nlile, Asap7772",
        "link": "https://arxiv.org/abs/2501.04682",
        "github_repo": null,
        "summary": "- This paper proposes Meta Chain-of-Thought (Meta-CoT), a framework extending traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning process.\n- Meta-CoT models the latent \"thinking\" process involved in complex reasoning, addressing the limitations of traditional CoT in capturing non-linear, iterative, and latent exploration and verification.\n- Empirical evidence from state-of-the-art models like OpenAI's \"o1\" and DeepSeek-R1 shows behaviors consistent with internalized search, supporting the Meta-CoT hypothesis.\n- The authors outline a training pipeline for Meta-CoT, incorporating instruction tuning with linearized search traces and reinforcement learning.\n-  A \"Big MATH\" project is introduced, aiming to create a dataset of over 1,000,000 verifiable math problems to facilitate research in this area.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
        "authors": "Jialian Wu, Ximeng Sun, Ze Wang, Yusheng Su, Samuel Schmidgall",
        "link": "https://arxiv.org/abs/2501.04227",
        "github_repo": null,
        "summary": "- This paper introduces Agent Laboratory, an LLM-based autonomous framework designed to accelerate the research process in machine learning by completing literature reviews, conducting experiments, and writing research reports.\n- The framework uses a pipeline of specialized LLM agents and accepts a human-provided research idea as input, producing a code repository and a research report as output, and allows for human feedback and guidance at each stage.\n- Agent Laboratory was evaluated with different LLMs, and it was found that OpenAI's o1-preview model generated the best research outcomes, and that the generated machine learning code is able to achieve state-of-the-art performance compared to existing methods.\n- Furthermore, human feedback was found to greatly improve the research quality, and Agent Laboratory decreased research costs by 84% compared to other autonomous research methods.\n- It was found that Agent Laboratory is effective at creating viable ML experiments and reducing the required effort for researchers.",
        "classification": [
            "Other"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
        "authors": "Xinya Du, Wei Yang, Ziming Luo, Ason-jay, ZonglinY",
        "link": "https://arxiv.org/abs/2501.04306",
        "github_repo": "https://github.com/du-nlp-lab/LLM4SR",
        "summary": "- This survey paper explores the transformative role of Large Language Models (LLMs) in the scientific research process, covering hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing.\n- The paper analyzes how LLMs contribute to each stage, summarizing methodologies, benchmarks, and evaluation methods, and identifying current challenges and future research directions.\n- It provides a comprehensive overview of LLM applications across the entire scientific workflow, unlike previous surveys that focused on specific LLM capabilities or individual research stages.\n- The survey identifies key components and trends in each application area, such as feedback modules in hypothesis discovery, agent-based automation in experiment implementation, and multi-model architectures in peer review generation.\n- The authors conclude that while LLMs face limitations in areas like planning, prompt robustness, and domain-specific expertise, their ongoing development holds immense potential to revolutionize scientific research by enhancing productivity, fostering innovation, and promoting collaboration.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/du-nlp-lab/LLM4SR"
        ],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "GeAR: Generation Augmented Retrieval",
        "authors": "Hao Sun, Yuefeng Zhan, Jianfeng Liu, Shaohan Huang, noobimp",
        "link": "https://arxiv.org/abs/2501.02772",
        "github_repo": null,
        "summary": "- This paper introduces Generation Augmented Retrieval (GeAR), a novel retrieval method that incorporates fusion and decoding modules to generate relevant text from documents based on the fused representation of the query and the document, enhancing fine-grained information retrieval.\n- GeAR consists of a bi-encoder for initial encoding of queries and documents, a fusion encoder utilizing cross-attention to combine query and document embeddings, and a text decoder to generate relevant information from the fused representation.\n- The model is trained using contrastive learning loss for retrieval and language modeling loss for generation, enabling joint optimization of retrieval and fine-grained understanding.\n- Experimental results demonstrate competitive performance in document retrieval and units localization tasks across various datasets, showing improvements over traditional retrieval methods, especially in capturing fine-grained semantic relationships.\n- GeAR also exhibits promising information generation capabilities and offers insights into the interpretation of retrieval results through visualization of information localization and cross-attention weights.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
        "authors": "Xueyu Hu, Congkai Xie, Zishu Wei, Yuhang Liu, pengxiang",
        "link": "https://arxiv.org/abs/2501.04575",
        "github_repo": "https://github.com/Reallm-Labs/InfiGUIAgent",
        "summary": "- InfiGUIAgent, a Multimodal Large Language Model (MLLM)-based Graphical User Interface (GUI) agent, is introduced for enhanced task automation on computing devices.\n- The agent employs a two-stage supervised fine-tuning approach where the first stage focuses on fundamental GUI understanding, grounding, and visual-language comprehension, while the second stage integrates advanced reasoning skills, including hierarchical and expectation-reflection reasoning, using synthesized trajectory data.\n- InfiGUIAgent leverages a modular action space design enabling flexible action combinations and utilizes reference-augmented annotation for precise spatial referencing in GUI interactions.\n- Experimental results on ScreenSpot and AndroidWorld benchmarks demonstrate InfiGUIAgent's superior performance compared to several open-source baselines.\n- The model achieves a 76.3% accuracy on ScreenSpot, surpassing models like ShowUI and UGround-7B, and a 0.09 overall success rate on AndroidWorld, outperforming similar-sized models and some with larger parameter sizes, showcasing its effective GUI task automation capabilities without relying on additional GUI metadata.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/Reallm-Labs/InfiGUIAgent"
        ],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation",
        "authors": "Chee Seng Chan, Jiankang Deng, Jia Wei Sii, Jing Yang, Kam Woh Ng",
        "link": "https://arxiv.org/abs/2501.04144",
        "github_repo": "https://github.com/kamwoh/chirpy3d",
        "summary": "- Chirpy3D is a novel framework for fine-grained 3D generation that lifts 2D understanding to 3D using multi-view diffusion and models part latents as continuous distributions.\n- It introduces a continuous part-aware latent space enabling interpolation and sampling of new parts and a self-supervised feature consistency loss ensuring stable generation of unseen parts. \n- Chirpy3D empowers creation of novel 3D objects by interpolating and combining unseen part compositions. \n- It uses a pre-trained MVDream model fine-tuned with 2D images and additional objectives for regularization and part disentanglement. \n- Experimental results show that Chirpy3D outperforms existing methods in generating creative 3D objects with unprecedented fine-grained details, evidenced by superior FIDCLIP scores and visual fidelity.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/kamwoh/chirpy3d"
        ],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images",
        "authors": "Varun Jampani, James M. Rehg, Aaryaman Vasishta, Zixuan Huang, mboss",
        "link": "https://arxiv.org/abs/2501.04689",
        "github_repo": null,
        "summary": "- SPAR3D is a novel two-stage 3D reconstruction model from single images leveraging a point diffusion model and a meshing transformer.\n- Stage one generates a sparse colored point cloud conditioned on the input image using a point diffusion model similar to Point-E but enhanced with classifier-free guidance and albedo prediction.\n- Stage two transforms the point cloud into a detailed mesh using a triplane transformer conditioned on both the point cloud and image features, jointly estimating geometry, texture, and illumination using a differentiable renderer with a Disney PBR shader.\n- SPAR3D achieves state-of-the-art performance on GSO and OmniObject3D datasets, outperforming existing regression and generative models while maintaining high efficiency (0.7s inference). \n- Using point clouds as an intermediate representation also facilitates interactive user edits by modifying the point cloud and quickly regenerating the mesh.",
        "classification": [
            "Image-to-3D"
        ],
        "github_urls": [
            "https://spar3d.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-09"
    },
    {
        "title": "DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization",
        "authors": "Rajarshi Roy, Danush Khanna, Suranjana Trivedy, Amitava Das, amanchadha",
        "link": "https://arxiv.org/abs/2501.03271",
        "github_repo": null,
        "summary": "- This paper introduces DPO-Kernels, a framework enhancing Direct Preference Optimization (DPO) for aligning large language models (LLMs) with human preferences by integrating kernel methods and diverse divergence measures.\n- DPO-Kernels incorporates kernelized representations, divergence alternatives (Jensen-Shannon, Hellinger, R\u00e9nyi, Bhattacharyya, Wasserstein, and f-divergences), and data-driven selection of optimal kernel-divergence pairs.\n- It introduces a Hierarchical Mixture of Kernels (HMK) to combine local and global kernels for precise and large-scale semantic modeling, automatically selecting the optimal kernel mixture during training.\n- Evaluations on 12 datasets demonstrate state-of-the-art generalization across various alignment tasks, including factuality, safety, reasoning, and instruction following.\n- Despite increased computational cost (3-4x higher than standard DPO), the improvements in alignment and generalization justify the overhead, with future work aiming to address scalability challenges.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Anthropic/hh-rlhf",
            "https://huggingface.co/datasets/nvidia/HelpSteer",
            "https://huggingface.co/datasets/lmsys/chatbot_arena_conversations",
            "https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k",
            "https://huggingface.co/datasets/tatsu-lab/alpaca_farm/viewer/alpaca_human_preference",
            "https://huggingface.co/datasets/stanfordnlp/SHP-2",
            "https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized",
            "https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs"
        ],
        "date": "2025-01-09"
    },
    {
        "title": "An Empirical Study of Autoregressive Pre-training from Videos",
        "authors": "Ilija Radosavovic, jitendra1995, yossig, rravishankar, brjathu",
        "link": "https://arxiv.org/abs/2501.05453",
        "github_repo": null,
        "summary": "- This paper introduces Toto, a family of autoregressive video models trained on next-token prediction, treating videos as sequences of visual tokens using a causal transformer model with a LLaMa architecture and dVAE tokenization.\n- Toto models are pre-trained on a massive dataset of over one trillion visual tokens from diverse video and image sources, enabling joint training.\n- Evaluation on various downstream tasks, including image recognition (ImageNet), video classification (Kinetics-400), action anticipation (Ego4D), video tracking, object permanence, and robotic manipulation, demonstrated competitive performance compared to existing methods.\n- The paper explored design choices like tokenizers, probing methods, model architectures, and training resolution. Attention pooling was found to improve representation quality in decoder-only models.\n- Scaling studies revealed that Toto exhibits power-law scaling behavior with compute, similar to large language models, albeit at a slower rate.",
        "classification": [
            "Video Classification",
            "Image Classification",
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-10"
    },
    {
        "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
        "authors": "jamestompkin, kuleshov, Skylion007, Eva1209",
        "link": "https://arxiv.org/abs/2501.05441",
        "github_repo": "https://github.com/brownvc/R3GAN/",
        "summary": "- This paper introduces R3GAN (\"Re-GAN\"), a new baseline Generative Adversarial Network (GAN) architecture focusing on training stability and modernization.\n- R3GAN utilizes a regularized relativistic GAN loss (RpGAN + R1 + R2) which mathematically guarantees local convergence, addressing mode collapse and instability issues.\n- By discarding ad-hoc tricks and incorporating modern architectures like ResNet and grouped convolutions, R3GAN simplifies the GAN design while improving performance.\n- The model surpasses StyleGAN2 on datasets like FFHQ, ImageNet, CIFAR, and Stacked MNIST and compares favorably to state-of-the-art GANs and diffusion models.\n- The authors demonstrate how a well-behaved loss enables the integration of modern backbone architectures, leading to a simpler and more efficient GAN.",
        "classification": [
            "Unconditional Image Generation",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/brownvc/R3GAN"
        ],
        "huggingface_urls": [],
        "date": "2025-01-10"
    },
    {
        "title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives",
        "authors": "ZwwWayne, Chonghao, THUdyh, ldkong, shaoyuanxie",
        "link": "https://arxiv.org/abs/2501.04003",
        "github_repo": null,
        "summary": "- DriveBench, a new benchmark designed to assess the reliability of Vision-Language Models (VLMs) in autonomous driving, is introduced.\n- The benchmark comprises 19,200 images, 20,498 question-answer pairs, and covers four driving tasks (perception, prediction, planning, behavior) under 17 settings, including clean, corrupted, and text-only inputs, to evaluate VLM robustness and visual grounding.\n- Evaluations of 12 popular VLMs reveal that they often generate plausible but fabricated responses based on general knowledge rather than visual cues, especially with missing or degraded visual inputs.\n- This behavior poses risks in safety-critical scenarios like autonomous driving, and is masked by dataset imbalances and inadequate metrics.\n- The study emphasizes the need for refined evaluation metrics that focus on multi-modal understanding and robust visual grounding, and highlights the potential of using VLMs' awareness of corruptions to enhance their reliability.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/drive-bench"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/drive-bench/arena"
        ],
        "date": "2025-01-10"
    },
    {
        "title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis",
        "authors": "Yingyu Liang, Xiaoyu Li, Zhenmei, JamesSand, keyekun",
        "link": "https://arxiv.org/abs/2501.04377",
        "github_repo": null,
        "summary": "- This paper analyzes the computational limits and efficiency of Visual Autoregressive (VAR) models for image generation, focusing on achieving faster than the current O(n^4) time complexity.\n- A key contribution is identifying a critical threshold for the norm of input matrices in VAR attention mechanisms, above which a sub-quartic time algorithm is proven impossible assuming the Strong Exponential Time Hypothesis (SETH).\n- The paper presents efficient construction leveraging low-rank approximations that satisfy the derived criteria for sub-quartic time complexity.\n- Specifically, when the bound of input matrices R is o(\u221alog n), an O(n^(2+o(1))) time algorithm exists that approximates the VAR model output with 1/poly(n) additive error.\n- Conversely, when R is \u03a9(\u221alog n), no truly sub-quartic time algorithm can achieve such approximation, establishing a fundamental computational limit for VAR models under SETH.",
        "classification": [
            "Text-to-Image",
            "Computer Vision",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-10"
    },
    {
        "title": "Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models",
        "authors": "Ece Elif Adak, tcTHEBESTMAN, fatihburakkaragoz, temretiras, sbozates",
        "link": "https://arxiv.org/abs/2501.04828",
        "github_repo": null,
        "summary": "- This paper introduces the first Named Entity Recognition (NER) dataset (HisTR) and Universal Dependencies treebank (OTA-BOUN) for historical Turkish, alongside a cleaned text corpus (OTC) and transformer-based models for NER, dependency parsing, and part-of-speech tagging.\n- HisTR consists of 812 manually annotated sentences, while OTA-BOUN contains 514 sentences annotated with part-of-speech tags and dependency relations. \n- The models were trained using BERTurk, mBERT, and TURNA architectures. \n- Experimental results show that BERTurk outperforms mBERT in NER and dependency parsing of historical Turkish, and fine-tuning with a combination of modern and historical Turkish data improves performance.\n- The resources and models are publicly available, establishing a baseline for future research in historical Turkish NLP.",
        "classification": [
            "Natural Language Processing",
            "Token Classification",
            "Text Classification",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/UniversalDependencies/UD_Ottoman_Turkish-BOUN/tree/dev",
            "https://github.com/Ottoman-NLP/ottominer-public"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/bucolin/HisTR",
            "https://huggingface.co/datasets/bucolin/OTA-BOUN_UD_Treebank",
            "https://huggingface.co/datasets/bucolin/OTC-Corpus",
            "https://huggingface.co/bucolin"
        ],
        "date": "2025-01-10"
    },
    {
        "title": "Entropy-Guided Attention for Private LLMs",
        "authors": "Brandon Reagen, nandan523",
        "link": "https://arxiv.org/abs/2501.03489",
        "github_repo": "https://github.com/Nandan91/entropy-guided-attention-llm",
        "summary": "- This research introduces an entropy-guided attention mechanism for enhancing the privacy of large language models (LLMs) during inference.\n- Researchers discovered that removing nonlinearities in LLMs can cause training instability due to entropy collapse in deeper layers and entropic overload in earlier layers.\n- The study presents a novel entropy regularization technique and proposes PI-friendly alternatives to layer normalization.\n- Experimental results show that the proposed methods reduce communication overhead by 3.94x and improve inference speed by 1.72x in a private setting.\n- The work bridges information theory and architectural design, utilizing entropy dynamics to guide the development of efficient privacy-preserving LLM architectures.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Nandan91/entropy-guided-attention-llm"
        ],
        "huggingface_urls": [],
        "date": "2025-01-10"
    },
    {
        "title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model",
        "authors": "Radu Timofte, Chris Biemann, Carolin Holtermann, Florian Schneider, Gregor Geigle",
        "link": "https://arxiv.org/abs/2501.05122",
        "github_repo": null,
        "summary": "- This paper introduces Centurio, a massively multilingual Large Vision-Language Model (LVLM) supporting 100 languages, trained by machine-translating high-quality English data and benchmarked across 13 downstream vision-language tasks covering 43 diverse languages.\n- The study investigates optimal language distributions of pre-training and instruction-tuning data, finding that including up to 100 languages with as little as 25-50% non-English data improves multilingual performance while maintaining strong English performance. \n- The research also introduces a new benchmark, SMPQA (Synthetic Multilingual Plot Question Answering), for evaluating multilingual text-in-image understanding and finds that non-English OCR data in training is crucial for this task. \n- Centurio achieves state-of-the-art results on 14 tasks covering 56 languages, matching popular models' performance on English while outperforming them on low-resource languages. \n- One limitation is the heavy reliance on machine-translated data and the comparatively small image input resolution which affects performance on text-heavy tasks.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-10"
    },
    {
        "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
        "authors": "Wenlong Gao, Tianshu Wu, Ergogogogo, JiyaoZhang, pmj110119",
        "link": "https://arxiv.org/abs/2501.03841",
        "github_repo": null,
        "summary": "- OmniManip is an open-vocabulary robotic manipulation method that bridges the gap between high-level reasoning of Vision-Language Models (VLMs) and the low-level precision needed for manipulation by introducing object-centric interaction primitives as spatial constraints.\n- These primitives, defined within an object's canonical space, translate VLM reasoning into actionable 3D constraints, enabling precise manipulation.\n- The system uses a dual closed-loop approach: one for planning through primitive resampling, interaction rendering, and VLM checking, and another for execution via 6D pose tracking.\n- The method is evaluated on diverse robotic manipulation tasks and demonstrates strong zero-shot generalization capabilities without requiring VLM fine-tuning.\n- It also shows promise for automating large-scale simulation data generation for robotic manipulation.",
        "classification": [
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [
            "https://omnimanip.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
        "authors": "Sung Ju Hwang, jinheon, KangsanKim71, starsuzi",
        "link": "https://arxiv.org/abs/2501.05874",
        "github_repo": null,
        "summary": "- VideoRAG, a novel framework for Retrieval-Augmented Generation (RAG) over video corpora, is introduced, addressing the limitations of existing RAG approaches that primarily focus on text or static images and overlook the rich multimodal information in videos.\n- VideoRAG dynamically retrieves relevant videos based on their relevance to user queries and integrates both visual and textual information from these videos into the answer generation process using Large Video Language Models (LVLMs).\n-  For videos lacking textual annotations (like subtitles), VideoRAG utilizes automatic speech recognition to generate auxiliary text, enabling the use of both visual and textual modalities even when explicit textual data is absent.\n- Experimental results on the WikiHowQA and HowTo100M datasets demonstrate that VideoRAG significantly outperforms relevant RAG baselines, including text-based RAG and video-based RAG that only uses textual video descriptions.\n- Ablation studies highlight the importance of video content and both visual and textual modalities in improving the quality and informativeness of generated responses.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
        "authors": "qiaozc, zyh, HelloJiang, Niujunbo2002, JoeLeelyf",
        "link": "https://arxiv.org/abs/2501.05510",
        "github_repo": "https://github.com/JoeLeelyf/OVO-Bench",
        "summary": "- This paper introduces OVO-Bench (Online-VideO-Benchmark), a novel benchmark designed to evaluate the online video understanding capabilities of Video-LLMs.\n- OVO-Bench focuses on evaluating temporal awareness by assessing models' abilities in Backward Tracing, Real-Time Visual Perception, and Forward Active Responding.\n- The benchmark comprises 12 tasks, 644 videos, and ~2800 human-curated meta-annotations with precise timestamps, covering diverse domains and video lengths.\n- Evaluation results reveal that current Video-LLMs struggle with online video understanding, showing a substantial gap compared to human performance, especially in tasks requiring temporal reasoning and dynamic adaptation.\n- The authors suggest that a more powerful LLM backbone and better temporal prioritization mechanisms are crucial for improving online video understanding capabilities in Video-LLMs.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Question Answering",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/JoeLeelyf/OVO-Bench"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
        "authors": "Dinura Dissanayake, hishamcholakkal, ahmedheakl, Ritesh-hf, omkarthawakar",
        "link": "https://arxiv.org/abs/2501.06186",
        "github_repo": null,
        "summary": "- This paper introduces LlamaV-01, a novel multimodal visual reasoning model trained using a multi-step curriculum learning approach, along with a new benchmark called Visual Reasoning-Chain (VRC-Bench) designed to evaluate step-by-step reasoning and a novel metric to assess reasoning quality at the granularity of individual steps.\n- VRC-Bench includes over 1,000 samples and 4,173 reasoning steps across eight diverse categories, including visual reasoning, math and logic, and scientific reasoning.\n- The proposed LlamaV-01 model leverages curriculum learning, progressively training on tasks of increasing complexity to enhance reasoning abilities and combines Beam search with multi-step curriculum learning to manage complexity, improve logical coherence, and generalize to challenging scenarios.\n- The model outperforms existing open-source models and performs favorably against closed-source models, achieving an average score of 67.3 with a 3.8% absolute gain over Llava-CoT across six benchmarks, while also being 5x faster during inference.\n- The model's code and benchmark are publicly available.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/mbzuai-oryx/LlamaV-01"
        ],
        "huggingface_urls": [
            "https://huggingface.co/omkarthawakar/LlamaV-01",
            "https://huggingface.co/datasets/omkarthawakar/VRC-Bench"
        ],
        "date": "2025-01-13"
    },
    {
        "title": "Enabling Scalable Oversight via Self-Evolving Critic",
        "authors": "Losin94, Benyou, yeshoubaizi, ziniuli, tangzhy",
        "link": "https://arxiv.org/abs/2501.05727",
        "github_repo": null,
        "summary": "- This paper introduces SCRIT (Self-evolving CRITic), a framework for enhancing the critique abilities of Large Language Models (LLMs) without external supervision.\n- SCRIT leverages a contrastive critique technique where the model analyzes student solutions by referencing correct solutions, along with a self-validation mechanism that ensures critique quality.\n- Implemented with Qwen2.5-72B-Instruct, SCRIT achieves up to a 10.3% improvement on critique-correction and error identification benchmarks and shows performance improvement across eight datasets in three scenarios.\n- The paper presents analyses showing that SCRIT's performance scales positively with data and model size, outperforms alternative approaches (Direct Critic and Bug-Injection Critic), and significantly benefits from its self-validation component.\n- It demonstrates consistent improvements across various problem domains, difficulties, and solution generation models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning",
        "authors": "Ruimao, Xintao, Qiulin, ziyangy, Yuzhou914",
        "link": "https://arxiv.org/abs/2501.04698",
        "github_repo": null,
        "summary": "- ConceptMaster is a novel framework for Multi-Concept Video Customization (MCVC) that allows personalized video generation using multiple user-defined concepts without test-time tuning.\n- It addresses the identity decoupling problem in MCVC by learning decoupled multi-concept embeddings and injecting them into diffusion transformer models in a standalone manner using a Multi-Concept Injector (MC-Injector).\n- A dedicated data collection pipeline was created to build a dataset of over 1.3 million high-quality MCVC samples, which overcomes the scarcity of suitable training data.\n- A Multi-Concept Benchmark (MC-Bench) was introduced to evaluate concept fidelity, identity decoupling, and video generation quality across six concept composition scenarios.\n- Extensive experiments demonstrate ConceptMaster's superior performance over existing naive solutions and tuning-based methods, achieving high-quality video generation with accurate representation of multiple concepts.",
        "classification": [
            "Text-to-Video",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [
            "https://yuzhou914.github.io/ConceptMaster/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "Multi-subject Open-set Personalization in Video Generation",
        "authors": "universome, studyfang, willi-menapace, aliaksandr-siarohin, tschen",
        "link": "https://arxiv.org/abs/2501.06187",
        "github_repo": null,
        "summary": "- Video Alchemist, a latent diffusion transformer model, is introduced for multi-subject, open-set video personalization, allowing customization of both foreground subjects and backgrounds without requiring test-time optimization.\n- It leverages a novel Diffusion Transformer module that integrates conditional reference images and subject-level text prompts via cross-attention.\n- A new data construction pipeline with augmentations mitigates overfitting on training video frames by focusing on subject identity.\n- MSRVTT-Personalization, a new benchmark for multi-subject video personalization with varied conditioning modes, is introduced for evaluation.\n- Experiments on the MSRVTT-Personalization benchmark demonstrate superior performance over state-of-the-art methods in subject fidelity, text alignment, and video dynamics.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/snap-research/MSRVTT-Personalization"
        ],
        "date": "2025-01-13"
    },
    {
        "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding",
        "authors": "danielpaulroth, jw2yang, zyang39, mqliu, Fiaa",
        "link": "https://arxiv.org/abs/2501.05452",
        "github_repo": null,
        "summary": "- REFOCUS is a framework that improves multimodal Large Language Models (LLMs) ability to perform visual reasoning on structured images like tables and charts by enabling them to generate Python code to edit the input image.\n- REFOCUS guides the LLM's attention through visual edits such as drawing boxes, highlighting sections, and masking areas, simulating a visual chain of thought.\n- Experiments on table and chart VQA datasets show significant performance improvements over GPT-40 without visual editing, with average gains of 11.0% on table tasks and 6.8% on chart tasks.\n- A 14k training set created using REFOCUS and GPT-40 demonstrates that visual chain-of-thought supervision leads to better performance compared to training on standard VQA data or chain-of-thought data, with an 8.0% average gain over QA pairs and a 2.6% gain over CoT when fine-tuning a Phi-3.5-vision model.\n- Analysis suggests that REFOCUS enhances the LLM's visual grounding, OCR accuracy, and reduces hallucinations through selective attention.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Table Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/microsoft/Phi-3-vision-128k-instruct"
        ],
        "date": "2025-01-13"
    },
    {
        "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
        "authors": "Shuang Li, Joshua B. Tenenbaum, Antoniotorralbaborruel, yilundu, vsub851",
        "link": "https://arxiv.org/abs/2501.05707",
        "github_repo": null,
        "summary": "- This paper introduces multiagent finetuning, a novel approach to improve large language models (LLMs) by leveraging multiagent interaction and specialization.\n- Instead of training a single model, the approach trains multiple LLMs from the same base model and specializes each model to different functionalities like generating initial responses (generation agents) and critiquing/refining those responses (critic agents).\n- Each model is independently trained using data generated through multiagent debate between the models, fostering specialization and promoting response diversification.\n- Experiments across open-source and proprietary LLMs on a suite of reasoning tasks demonstrate significant performance gains and the ability to improve over more finetuning rounds compared to single-agent self-improvement methods.\n- The finetuned models exhibit better generalization capabilities to new datasets in a zero-shot setting.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://llm-multiagent-ft.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "Infecting Generative AI With Viruses",
        "authors": "fgmckee, dnoever",
        "link": "https://arxiv.org/abs/2501.05542",
        "github_repo": null,
        "summary": "- This paper introduces a novel approach to penetration testing for large language models (LLMs), focusing on their handling of image files containing embedded malware.\n- The researchers successfully embedded the EICAR test file, a harmless string used to test antivirus software, within JPEG images and uploaded them to several LLMs, including GPT-40, Microsoft Copilot, Google Gemini 1.5 Pro, and Anthropic Claude 3.5 Sonnet.\n- They demonstrated the LLMs' ability to process and even extract the embedded malware using Python scripts within their environments, raising concerns about potential vulnerabilities.\n- This research highlights the need for improved security measures in LLMs, especially in detecting and preventing the execution of potentially malicious code hidden within seemingly benign files.\n- The study also suggests further research into automated LLM file inspection, standardized security testing frameworks for LLMs, and investigation of cross-platform vulnerabilities.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
        "authors": "RunjiLin, BeichenZhang, wuyangzhen, chujiezheng, Zhenru",
        "link": "https://arxiv.org/abs/2501.07301",
        "github_repo": null,
        "summary": "- This paper introduces two new process reward models (PRMs) for mathematical reasoning, Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B, focusing on enhancing the process supervision capabilities.\n- The study demonstrates that conventional Monte Carlo (MC) estimation for training data synthesis is less effective for training PRMs compared to LLM-as-a-judge or human annotation methods.\n- Furthermore, they point out the bias of using Best-of-N (BoN) evaluation alone and advocate for incorporating step-level metrics, such as PROCESSBENCH, for a more comprehensive assessment.\n- A new consensus filtering mechanism which integrates MC estimation with LLM-as-a-judge is proposed to improve both model performance and data efficiency.\n- The proposed models and training mechanisms significantly improve error identification in mathematical reasoning, exceeding the capabilities of existing open-source models and providing guidelines for future PRM development.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/Qwen/Qwen2.5-Math-PRM-7B",
            "https://hf.co/Qwen/Qwen2.5-Math-PRM-72B"
        ],
        "date": "2025-01-14"
    },
    {
        "title": "Tensor Product Attention Is All You Need",
        "authors": "Huizhuo Yuan, Yifeng Liu, thughost, zhenqincn, yifAI",
        "link": "https://arxiv.org/abs/2501.06425",
        "github_repo": "https://github.com/tensorgi/T6",
        "summary": "- This paper introduces Tensor Product Attention (TPA), a novel attention mechanism designed to reduce memory overhead in large language models (LLMs) during inference.\n- TPA leverages tensor decomposition to create compact representations of queries, keys, and values, thereby decreasing the size of key-value caches, a major memory consumer in LLMs.\n- Based on TPA, the authors create a new model architecture, Tensor ProducT ATTenTion Transformer (T6), and show through experiments that it improves performance on language modeling tasks, achieving lower perplexity and higher accuracy on various benchmarks compared to standard transformer models.\n- TPA's memory efficiency facilitates handling longer sequences under fixed resource constraints, directly addressing a key scalability challenge in current LLMs.\n- Additionally, TPA is shown to integrate seamlessly with Rotary Position Embedding (RoPE), simplifying its application in existing architectures like LLaMA and Gemma.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/tensorgi/T6"
        ],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
        "authors": "tyj2022, edoarc, lfsm",
        "link": "https://arxiv.org/abs/2501.06252",
        "github_repo": null,
        "summary": "- This paper introduces Transformer\u00b2, a novel self-adaptation framework for Large Language Models (LLMs) that adapts to unseen tasks in real-time.\n- It employs a two-pass mechanism: first identifying task properties through a dispatch system and then dynamically mixing task-specific \"expert\" vectors, trained using reinforcement learning.\n- The \"expert\" vectors are generated by Singular Value Fine-tuning (SVF), a new parameter-efficient fine-tuning (PEFT) method that modifies singular values within weight matrices.\n- This method outperforms LoRA, a popular existing PEFT method, with fewer parameters and greater efficiency across tasks and models. \n- The approach demonstrates versatility across different LLM architectures and modalities, including vision-language tasks, where it uses knowledge from language tasks to improve performance in visual question answering.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/SakanaAI/self-adaptive-llms"
        ],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "VideoAuteur: Towards Long Narrative Video Generation",
        "authors": "Jiepeng Cen, Liangke Gui, Lu Qi, Feng Cheng, lambertxiao",
        "link": "https://arxiv.org/abs/2501.06173",
        "github_repo": null,
        "summary": "- This paper introduces VideoAuteur, a two-stage auto-regressive pipeline for generating long-form narrative videos, consisting of a long narrative director and a visual-conditioned video generation model.\n- The long narrative director generates visual embeddings or keyframes along with captions and actions which capture the narrative flow using an interleaved auto-regressive model.\n- A novel cooking video dataset, CookGen, is created consisting of approximately 200,000 video clips sourced from existing video datasets (YouCook2 and HowTo100M) annotated with captions, actions, and visual states, which allows benchmarking long narrative video generation.\n- Experiments demonstrate that the generated videos contain improved semantic consistency and visual fidelity compared to existing methods.\n- Using CLIP embeddings for visual regression outperforms VAE embeddings.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [
            "https://videoauteur.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "authors": "zhoudeyu, Runnaning, ZekunXi, wzl0228, callanwu",
        "link": "https://arxiv.org/abs/2501.07572",
        "github_repo": null,
        "summary": "- This paper introduces WebWalkerQA, a new benchmark designed to evaluate the web traversal capabilities of Large Language Models (LLMs).\n- WebWalkerQA focuses on information-seeking question-answering tasks that require navigating through website subpages, often involving multiple steps.\n- The benchmark contains 680 question-answer pairs across over 1373 webpages from diverse domains including conference, organization, education, and games, and is available in both English and Chinese.\n- A novel multi-agent framework called WebWalker, employing an explorer-critic paradigm, is proposed as a strong baseline for mimicking human-like web navigation and memory management.\n- Experimental results demonstrate that WebWalkerQA is challenging for LLMs, highlighting the need for better integration of LLMs with web traversal strategies.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning",
        "authors": "Gui Geng, Pengfei, alanyoung058, ZhenHuang, zongzi",
        "link": "https://arxiv.org/abs/2501.06458",
        "github_repo": null,
        "summary": "- This paper explores inference-time scaling in Large Language Models (LLMs) for medical reasoning tasks, including diagnosis and treatment planning.\n- Experiments on medical benchmarks (MedQA, Medbullets, JAMA Clinical Challenges) reveal that increasing inference time improves performance, with a 6-11% improvement observed using a modest training set of 500 samples.\n- Task complexity correlates with required reasoning chain length, and the model's differential diagnoses adhere to hypothetico-deductive principles.\n- The study utilizes a knowledge distillation approach from GPT-series models to enable journey learning during inference.\n- Findings highlight the potential of combining inference-time scaling and journey learning to improve real-world clinical reasoning in LLMs.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/SPIRAL-MED/Ophiuchus"
        ],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction",
        "authors": "langgz, gaoruize, zhihaodu, Yingda, chenmengzhe",
        "link": "https://arxiv.org/abs/2501.06282",
        "github_repo": null,
        "summary": "- MinMo, an 8-billion parameter multimodal large language model, is introduced for seamless voice interaction, addressing limitations of prior aligned multimodal models by training on 1.4 million hours of diverse speech data and a broad range of speech tasks.\n- MinMo achieves state-of-the-art performance in voice comprehension and generation benchmarks, maintains text LLM capabilities, and facilitates full-duplex conversations, outperforming models like Moshi, Freeze-Omni, and GLM-4-Voice across ASR, S2TT, SQA, VSC, SER, and LID tasks (Figure 1).\n- A novel voice decoder balancing simplicity and performance is proposed, utilizing a streaming Transformer mixing LLM hidden states with speech tokens.\n- MinMo demonstrates enhanced instruction-following, controlling speech generation with nuances like emotions, dialects, speaking rates, and voice mimicking with 98.4% accuracy.\n- MinMo supports full-duplex interaction with low latency (100ms for speech-to-text and 600ms theoretical/800ms practical for full-duplex) using a prediction module leveraging the text LLM's semantic understanding.",
        "classification": [
            "Multimodal",
            "Audio",
            "Text-to-Speech",
            "Automatic Speech Recognition"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct"
        ],
        "date": "2025-01-14"
    },
    {
        "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
        "authors": "Zhangyang Wang, Lu Liu, Gaojie Jin, Ziquan Zhu, Tianjin Huang",
        "link": "https://arxiv.org/abs/2501.06842",
        "github_repo": "https://github.com/TianjinYellow/SPAM-Optimizer.git",
        "summary": "- This paper introduces SPAM (Spike-Aware Adam with Momentum Reset), a novel optimizer designed to enhance the stability and efficiency of Large Language Model (LLM) training by mitigating the negative impact of gradient spikes.\n- SPAM incorporates two key innovations: periodic momentum reset and spike-aware gradient clipping to counteract the harmful accumulation of spiked gradients and preserve valuable directional information.\n- Extensive experiments demonstrate that SPAM surpasses Adam and its variants across various tasks, including LLM pre-training, quantization-aware training, reinforcement learning, and time series forecasting.\n- SPAM also facilitates memory-efficient training through sparse momentum, outperforming state-of-the-art memory-efficient optimizers like GaLore and Adam-Mini under memory constraints.\n- The analysis reveals that gradient spikes, often overlooked, coincide with subtle loss bumps during training and can reach up to 1000 times the magnitude of typical gradients, significantly impacting performance across different architectures, model sizes, and datasets.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Time Series Forecasting"
        ],
        "github_urls": [
            "https://github.com/TianjinYellow/SPAM-Optimizer.git"
        ],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature",
        "authors": "yeunglevy, yuhuizhang, jnirschl, minwoosun, lozanoe",
        "link": "https://arxiv.org/abs/2501.07171",
        "github_repo": null,
        "summary": "- This paper introduces BIOMEDICA, a scalable open-source framework for creating a large-scale, deep-learning-ready biomedical image-caption dataset derived from scientific literature, along with a suite of associated CLIP-style models (BMCA-CLIP) pretrained on this data.\n- The BIOMEDICA dataset contains over 24 million image-text pairs from over 6 million open-access articles with rich metadata and expert annotations, significantly larger and more diverse than existing biomedical vision-language datasets.\n- BMCA-CLIP models are trained using continual pretraining on the BIOMEDICA dataset via streaming.\n- Evaluation across 40 standardized biomedical tasks demonstrates state-of-the-art zero-shot performance, with a 6.56% average improvement in classification (up to +29.8% on dermatology and +17.5% on ophthalmology tasks) and superior retrieval performance compared to previous methods, while using 10x less compute.\n- The authors release the code, dataset, and pretrained models to promote reproducibility and further research in biomedical vision-language modeling.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Text-to-Image",
            "Zero-Shot Classification",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/minwoosun/biomedica-etl",
            "https://github.com/Ale9806/open_clip_with_biomedica"
        ],
        "huggingface_urls": [
            "https://huggingface.co/BIOMEDICA"
        ],
        "date": "2025-01-14"
    },
    {
        "title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
        "authors": "Wangchunshu, siruo2, super-dainiu, CamelH, RTT1",
        "link": "https://arxiv.org/abs/2501.06590",
        "github_repo": "https://github.com/gersteinlab/chemagent",
        "summary": "- ChemAgent, a new framework, enhances Large Language Models (LLMs) for chemical reasoning tasks using a dynamic self-updating library, improving performance by up to 46% (GPT-4).\n- The library compiles sub-tasks and solutions from decomposed chemical tasks, facilitating task decomposition and solution generation for new problems.\n- ChemAgent integrates three memory types: Planning Memory for high-level strategies, Execution Memory for specific solutions, and Knowledge Memory for fundamental chemistry principles, stored externally for efficient retrieval.\n- Experimental results on SciBench datasets demonstrate significant improvements over existing methods, including a 46% gain for GPT-4 and a 10% average improvement over StructChem.\n- The self-updating library system allows continuous refinement of problem-solving strategies and solutions over time, analogous to human learning from past experiences.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/gersteinlab/chemagent"
        ],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "UnCommon Objects in 3D",
        "authors": "EarlGr, Jiali, zarzarj, JianyuanWang, wenchang05",
        "link": "https://arxiv.org/abs/2501.07574",
        "github_repo": null,
        "summary": "- Introduced uCO3D, a novel object-centric 3D dataset designed for 3D deep learning and generative AI. \n- uCO3D contains 170k 360\u00b0 videos of objects across 1,070 categories from the LVIS taxonomy, offering significantly greater diversity than existing datasets like MVImgNet and CO3Dv2. \n- uCO3D includes high-quality 3D annotations: camera poses, depth maps, point clouds, captions, and 3D Gaussian Splat reconstructions, enabling realistic renders from canonical viewpoints. \n- Demonstrated uCO3D's superior quality by training popular 3D models (LRM, CAT3D) and achieving better novel-view synthesis results than alternatives. \n- Showcased uCO3D's potential for photorealistic text-to-3D generation by training an Instant3D-like model using its 3DGS reconstructions rendered from canonical viewpoints.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/uco3d"
        ],
        "huggingface_urls": [],
        "date": "2025-01-14"
    },
    {
        "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
        "authors": "Bangwei Gong, Aonian Li, MiniMax, Hannnnnxd, enochzhang",
        "link": "https://arxiv.org/abs/2501.08313",
        "github_repo": "https://github.com/MiniMax-AI",
        "summary": "- This paper introduces the MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which leverage \"lightning attention\" and Mixture of Experts (MoE) to handle long contexts (up to 4 million tokens for MiniMax-Text-01).\n- MiniMax-Text-01 is a 456 billion parameter model, with 45.9 billion parameters activated per token and 32 experts, designed to match leading commercial models while offering a significantly larger context window.\n- MiniMax-VL-01 is a vision-language model trained on 512 billion vision-language tokens.\n- The authors claim their models match the performance of state-of-the-art models like GPT-4 and Claude-3.5-Sonnet on standard benchmarks and outperform them in long context scenarios (200k+ tokens).\n- They also highlight superior prefill latency due to their novel architecture.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering",
            "Natural Language Processing",
            "Question Answering",
            "Translation",
            "Summarization",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/MiniMax-AI"
        ],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models",
        "authors": "Yoad Tewel, Rinon Gal, Hadas Orgad, Ido Galil, Michael Toker",
        "link": "https://arxiv.org/abs/2501.06751",
        "github_repo": null,
        "summary": "- This paper investigates the role of padding tokens in text-to-image (T2I) diffusion models, a practice used to standardize input lengths.\n- Two causal intervention methods, Intervention in Text Encoder Output (ITE) and Intervention in the Diffusion Process (IDP), are introduced to analyze how padding tokens influence image generation.\n- Findings reveal that frozen text encoders often ignore padding tokens, but trained or fine-tuned encoders utilize them to encode semantic information.\n- Multi-modal attention architectures, like those in Stable Diffusion 3 and FLUX, can use padding tokens as \"registers\" to store and recall information during the diffusion process.\n- This deeper understanding of padding mechanisms may inform future model design and training in T2I systems.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
        "authors": "Hao Ouyang, Jie Xiao, Xi Chen, Ka Leong Cheng, Zhiheng Liu",
        "link": "https://arxiv.org/abs/2501.08332",
        "github_repo": null,
        "summary": "- MangaNinja is a novel reference-based line art colorization model derived from diffusion models, specializing in precise colorization guided by reference images.\n- It uses a dual-branch architecture, with a Reference U-Net for encoding reference image features and a Denoising U-Net for colorizing the line art, facilitated by cross-attention between the branches.\n- A progressive patch shuffling module enhances local matching capabilities by dividing the reference image into patches and randomly shuffling them during training, forcing the model to learn finer-grained correspondences.\n- A point-driven control scheme powered by PointNet allows users to specify matching point pairs between the reference and line art, enabling fine-grained control over colorization and handling complex scenarios such as varying poses, missing details, and multi-reference colorization.\n- Quantitative and qualitative evaluations on a self-collected benchmark demonstrate MangaNinja's superior performance over existing methods in terms of visual fidelity and identity preservation.",
        "classification": [
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
        "authors": "Jingyang Qian, Kangwei Liu, Xinle Deng, Ningyu, Fangyinfff",
        "link": "https://arxiv.org/abs/2501.08187",
        "github_repo": "https://github.com/zjunlp/Instructcell;",
        "summary": "- INSTRUCTCELL, a multimodal AI copilot, is introduced for enhanced single-cell analysis, integrating natural language instructions with single-cell RNA sequencing (scRNA-seq) data. \n- The model architecture comprises a Q-Former for embedding gene expression profiles, a pretrained language model (LM) for text processing, and a cell reconstruction block for generating gene expression profiles. \n- INSTRUCTCELL excels in tasks such as conditional pseudo-cell generation, cell type annotation, and drug sensitivity prediction, adapting to diverse experimental settings. \n- Evaluation across multiple scRNA-seq datasets shows INSTRUCTCELL performs on par with or surpasses existing models like scBERT, scGPT, and Geneformer, demonstrating robustness and efficiency. \n- Through this unified approach, INSTRUCTCELL streamlines complex single-cell data exploration, lowering technical barriers and revealing deeper biological insights.",
        "classification": [
            "Multimodal",
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/zjunlp/InstructCell"
        ],
        "huggingface_urls": [
            "https://huggingface.co/zjunlp/InstructCell-chat",
            "https://huggingface.co/zjunlp/InstructCell-instruct"
        ],
        "date": "2025-01-15"
    },
    {
        "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
        "authors": "Xuefeng Xiao, Ceyuan Yang, Yuxi Ren, Xin Xia, PeterL1n",
        "link": "https://arxiv.org/abs/2501.08316",
        "github_repo": null,
        "summary": "- This paper introduces Adversarial Post-Training (APT), a novel approach for generating high-resolution images and videos in a single step.\n- APT utilizes a pre-trained diffusion transformer model (DiT) and continues training it with an adversarial objective against real data.\n- To stabilize the adversarial training process, the authors introduce several key improvements, including discriminator enhancements (architectural changes, ensemble across timesteps, approximated R1 regularization) and a generator initialized through deterministic distillation.\n- The resulting model, Seaweed-APT, generates 2-second, 1280x720, 24fps videos and 1024px images in a single evaluation step.\n- User studies indicate that Seaweed-APT achieves comparable quality to state-of-the-art one-step image generation methods regarding visual fidelity and structural integrity, while showing improvements in realism and details compared to the original diffusion model.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/black-forest-labs/FLUX.1-dev",
            "https://huggingface.co/black-forest-labs/FLUX.1-schnell"
        ],
        "date": "2025-01-15"
    },
    {
        "title": "PokerBench: Training Large Language Models to become Professional Poker Players",
        "authors": "Zhengyu Li, Aniket Rahane, Richard Yang, Richard Zhuang, akshat57",
        "link": "https://arxiv.org/abs/2501.08328",
        "github_repo": "https://github.com/pokerllm/pokerbench",
        "summary": "- This paper introduces PokerBench, a benchmark and dataset for evaluating and training large language models (LLMs) to play the strategic game of poker.\n- PokerBench includes 11,000 scenarios (1,000 pre-flop and 10,000 post-flop) designed to test LLMs on decision-making and game theory optimal (GTO) play.\n- The authors evaluate several prominent LLMs including GPT-4, ChatGPT 3.5, and various Llama and Gemma models, finding that pre-trained models underperform compared to fine-tuned models.\n- Fine-tuning Llama-3-8B significantly improves its performance, exceeding that of GPT-4 on PokerBench.\n- Head-to-head comparisons between checkpoints with varying PokerBench scores show a correlation between benchmark performance and actual win rates, demonstrating the efficacy of PokerBench as an evaluation tool.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/pokerllm/pokerbench"
        ],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors",
        "authors": "Hui Li, Hang Xu, Yihan Zeng, Xinpeng Zhou, Yabo Zhang",
        "link": "https://arxiv.org/abs/2501.08225",
        "github_repo": "https://github.com/YBYBZhang/FramePainter",
        "summary": "- FramePainter is an innovative image editing model that leverages video diffusion priors by reformulating the task as an image-to-video generation problem.\n- The model is initialized with Stable Video Diffusion and incorporates a lightweight sparse control encoder to integrate user edits, ensuring temporal consistency and reducing training costs.\n- To address limitations in handling large motion between frames, FramePainter introduces matching attention, an auxiliary branch that complements spatial attention and fosters dense correspondence between edited and source image tokens.\n- This matching attention mechanism is further optimized using tracking results from CoTracker-v3 to achieve finer visual detail preservation.\n- Experimental results show that FramePainter outperforms existing training-free and training-based methods with significantly less training data, demonstrating its effectiveness in various editing tasks and generalization to out-of-domain scenarios.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/YBYBZhang/FramePainter"
        ],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens",
        "authors": "Xiaohui Shen, Chenglin Yang, Qihang Yu, Dongwon Kim, turkeyju",
        "link": "https://arxiv.org/abs/2501.07730",
        "github_repo": null,
        "summary": "- This paper introduces TA-TiTok, a novel text-aware 1D tokenizer, and MaskGen, a family of text-to-image masked generative models.\n- TA-TiTok improves upon previous 1D tokenizers by incorporating textual information during detokenization, using a simplified one-stage training process, and supporting both discrete and continuous tokens.\n- MaskGen leverages TA-TiTok to efficiently generate images from text, supporting both discrete and continuous token representations and utilizing CLIP for text encoding.\n-  Evaluated on MJHQ-30K and GenEval benchmarks, MaskGen achieves comparable performance to models trained on private data, despite being trained exclusively on open data.\n- Notably, MaskGen-XL (1.1B parameters) achieves an FID of 6.53 on MJHQ-30K and an overall score of 0.57 on GenEval, outperforming larger models trained on private datasets, while exhibiting faster inference.",
        "classification": [
            "Text-to-Image",
            "Image Feature Extraction",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/laion/laion2B-en-aesthetic",
            "https://huggingface.co/datasets/laion/laion-art",
            "https://huggingface.co/datasets/laion/laion-pop",
            "https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions"
        ],
        "date": "2025-01-15"
    },
    {
        "title": "Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks",
        "authors": "Subhashree Radhakrishnan, Sifei Liu, De-An Huang, Min-Hung Chen, Miran Heo",
        "link": "https://arxiv.org/abs/2501.08326",
        "github_repo": null,
        "summary": "- Omni-RGPT, a multimodal large language model, is introduced for region-level understanding in both images and videos.\n- It uses \"Token Mark,\" a set of learned tokens embedded into visual features and text prompts using region prompts (boxes or masks) to represent target regions.\n- An auxiliary \"Temporal Region Guide Head\" is introduced to improve region consistency in videos without relying on tracklets.\n- A new large-scale region-level video instruction dataset, RegVID-300k, is also introduced, containing 98k videos and 294k instruction samples.\n- Omni-RGPT achieves state-of-the-art performance on benchmarks like Causal-VidQA and VCR, demonstrating its effectiveness in region-level understanding tasks.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Video-Text-to-Text",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training",
        "authors": "Ran Chen, Wei Wang, Zekun Wang, Ziyun Dai, yuyijiong",
        "link": "https://arxiv.org/abs/2501.08197",
        "github_repo": "https://github.com/yuyijiong/fineweb-edu-chinese",
        "summary": "- This paper introduces the OpenCSG Chinese Corpus, a series of high-quality datasets designed for Chinese LLM pre-training, post-training, and fine-tuning.\n- The corpus comprises four distinct datasets: Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with unique characteristics catering to diverse training needs.\n- The datasets leverage automated scoring modules, synthetic text generation, and domain-focused curation, ensuring scalability, diversity, and openness.\n- Experimental results on a 2B-level LLM demonstrate significant performance improvements in tasks like C-Eval when pre-trained on Fineweb-edu-chinese compared to a baseline dataset.\n- This work addresses the scarcity of high-quality Chinese datasets and promotes advancements in Chinese NLP research by providing open-access resources.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/yuyijiong/fineweb-edu-chinese"
        ],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding",
        "authors": "Yuan Lin, Yuchen Zhang, Haomiao Sun, Jiawei Wang, Liping Yuan",
        "link": "https://arxiv.org/abs/2501.07888",
        "github_repo": null,
        "summary": "- Tarsier2, a 7B parameter Large Vision-Language Model (LVLM), focuses on generating detailed video descriptions and demonstrates superior general video understanding.\n- It utilizes a simple architecture comprising a vision encoder, vision adapter, and LLM and undergoes three training stages: pre-training on 40M video-text pairs, supervised fine-tuning with fine-grained temporal alignment, and Direct Preference Optimization (DPO) with automatically generated preference data.\n- Evaluation results show Tarsier2-7B outperforming proprietary models like GPT-40 and Gemini 1.5 Pro in detailed video description and achieving state-of-the-art performance on 15 public benchmarks, including video question answering, grounding, and hallucination tests.\n- A key contribution is a new recaptioning dataset, Tarsier2-Recap-585K, used to enhance existing LVLMs for video description and general video understanding.\n- Ablation studies confirm the effectiveness of scaling the pre-training data, fine-grained temporal alignment, and DPO training.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Visual Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Tarsier-LLM/Tarsier2-Recap-585K"
        ],
        "date": "2025-01-15"
    },
    {
        "title": "Enhancing Automated Interpretability with Output-Centric Feature Descriptions",
        "authors": "Mor Geva, Chen Agassy, Roy Mayan, Yoav Gur-Arieh, atticusg",
        "link": "https://arxiv.org/abs/2501.08319",
        "github_repo": null,
        "summary": "- Proposes two output-centric methods, VocabProj and TokenChange, for generating natural language descriptions of features in Large Language Models (LLMs).\n- Introduces a two-faceted evaluation framework for feature descriptions, considering both input-based and output-based metrics.\n- Shows that output-centric methods outperform input-centric methods (like MaxAct) on output-based evaluations, and are often only slightly worse on input-based evaluations.\n- Demonstrates that combining input- and output-centric methods leads to more comprehensive and accurate feature descriptions.\n- Reveals that output-centric methods can be used to find inputs that activate \"dead\" features, which were previously thought to be inactive.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data",
        "authors": "Satya Kapoor, Sreyoshi Bhaduri, Natalie Perez, Rewina Bedemariam, amanchadha",
        "link": "https://arxiv.org/abs/2501.08167",
        "github_repo": null,
        "summary": "- This research explores the effectiveness of Large Language Models (LLMs) as judges for evaluating the thematic alignment of summaries generated by other LLMs, focusing on open-text survey data.\n- The study uses an Anthropic Claude model to generate thematic summaries and employs Amazon's Titan Express, Nova Pro, and Meta's Llama as LLM judges, comparing their performance to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha.\n- Findings reveal that while LLMs can offer a scalable solution comparable to human raters in judging thematic alignment, humans may still excel at detecting subtle, context-specific nuances.\n- The study highlights the potential of LLMs as judges in organizational settings while emphasizing the need for careful consideration of their limitations.\n- Recommendations for future research include addressing potential biases in LLM evaluations and developing more comprehensive assessment methods that capture nuanced thematic understanding.",
        "classification": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
        "authors": "Yejin Choi, David Wadden, Shrusti Ghela, Abhilasha Ravichander",
        "link": "https://arxiv.org/abs/2501.08292",
        "github_repo": null,
        "summary": "- This paper introduces HALoGEN, a benchmark for evaluating hallucinations in large language models (LLMs).\n- HALoGEN consists of 10,923 prompts across nine domains, including programming, scientific attribution, and summarization, along with automatic verifiers.\n- The benchmark evaluates ~150,000 generations from 14 LLMs and finds that even top-performing models exhibit frequent hallucinations (4%-86% of generated atomic facts).\n- A novel error classification categorizes hallucinations as Type A (incorrect recollection of training data), Type B (incorrect or out-of-context training data), or Type C (fabrication).\n- The framework aims to facilitate research into why LLMs hallucinate and promote the development of trustworthy language models.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://halogen-hallucinations.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages",
        "authors": "Ibrahim Said Ahmad, David Ifeoluwa Adelani, Abinew Ali Ayele, Idris Abdulmumin, Shamsuddeen Hassan Muhammad",
        "link": "https://arxiv.org/abs/2501.08284",
        "github_repo": "https://github.com/AfriHate/AfriHate",
        "summary": "- This paper introduces AfriHate, a multilingual dataset of hate speech and abusive language in 15 African languages.\n- The dataset consists of tweets annotated by native speakers into three categories: hate, abusive/offensive, and neutral, with further labeling of hate speech targets based on attributes like ethnicity, religion, and gender.\n- Baseline experiments using Africa-centric pre-trained language models and prompted LLMs were conducted, revealing performance variations across languages and demonstrating that multilingual training often yields better results.\n- The study finds that while multilingual models generally perform better, LLMs show potential for improved hate speech detection in low-resource languages.\n- The datasets, scripts, models, and lexicons are publicly released to facilitate further research on hate speech and offensive language in African languages.",
        "classification": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [
            "https://github.com/AfriHate/AfriHate"
        ],
        "huggingface_urls": [],
        "date": "2025-01-15"
    },
    {
        "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
        "authors": "Ruiming Tang, Dexun Li, Xin Deik Goh, Yujing Chang, daviddongdong",
        "link": "https://arxiv.org/abs/2501.08828",
        "github_repo": null,
        "summary": "- This paper introduces MMDocIR, a novel benchmark for multi-modal document retrieval focusing on page-level and layout-level retrieval tasks.\n- MMDocIR comprises an evaluation set with 313 documents and 1,658 expert-annotated questions and a training set with 6,878 documents and 73,843 automatically annotated questions.\n- Experimental results demonstrate that visual retrievers outperform text-based methods, and models trained on MMDocIR exhibit superior performance.\n- The benchmark addresses limitations of existing datasets by focusing on retrieval granularity, offering complete page contexts, and improved question quality.\n- The benchmark also highlights the effectiveness of VLM-based text representations over OCR for multi-modal document retrieval.",
        "classification": [
            "Multimodal",
            "Document Question Answering",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/MMDocIR"
        ],
        "date": "2025-01-16"
    },
    {
        "title": "CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities",
        "authors": "liuziwei7, hongfz16, FrozenBurning, hzxie",
        "link": "https://arxiv.org/abs/2501.08983",
        "github_repo": null,
        "summary": "- CityDreamer4D is a novel compositional generative model designed for creating unbounded 4D cities, separating dynamic elements (vehicles) from static ones (buildings, roads) and employing distinct neural fields for different object types.\n- The model uses a compact BEV representation, enhanced with a bottom-up height map for hollow structures, and generates dynamic traffic scenarios with vehicles on high-fidelity maps derived from city layouts.\n- The authors introduce a new dataset, CityTopia, comprising 37,500 high-fidelity street-view and drone-view images with precise 2D and 3D semantic and instance annotations, alongside OSM and Google Earth datasets for layout and visual details.\n- CityDreamer4D achieves state-of-the-art performance in generating realistic and diverse 4D cities, outperforming baselines in FID, KID, VBench, Depth Error, and Camera Error metrics, and demonstrates superior performance in a user study focusing on perceptual quality, 4D realism, and view consistency.\n- The model supports applications like urban simulation and localized editing, offering advancements in creating large-scale, dynamic, and customizable virtual city environments.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-16"
    },
    {
        "title": "Towards Best Practices for Open Datasets for LLM Training",
        "authors": "jending12, ayahbdeir, avi-skowron, stellaathena, stefan-baack",
        "link": "https://arxiv.org/abs/2501.08365",
        "github_repo": null,
        "summary": "- This paper discusses the challenges and best practices for creating open datasets for LLM training, focusing on sourcing, processing, governing, and releasing data.\n- It emphasizes the importance of dataset transparency for accountability and innovation in AI, particularly given the increasing criticism of data practices by large AI companies.\n- The authors recommend prioritizing community resources, providing thorough documentation, adhering to preference signals, and promoting diversity in data sources.\n- They also outline the need for clear legal frameworks and ethical considerations in data governance and release.\n- The paper emerged from a convening hosted by Mozilla and EleutherAI and builds on case studies from prominent open datasets.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/r-three/common-pile"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/HuggingFaceH4/c-pile"
        ],
        "date": "2025-01-16"
    },
    {
        "title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation",
        "authors": "liuziwei7, Ziqi, cszy98, weepiess2383, ChenyangSi",
        "link": "https://arxiv.org/abs/2501.08994",
        "github_repo": null,
        "summary": "- RepVideo, a novel framework designed for text-to-video generation, enhances video diffusion models by leveraging enriched intermediate representations.\n- It employs a feature cache module to aggregate features from adjacent transformer layers and a gating mechanism to combine these aggregated features with the original input, improving spatial detail and temporal consistency.\n- RepVideo addresses the issue of fragmented spatial semantics and reduced temporal coherence in existing transformer-based video diffusion models.\n- Experimental results on VBench show that RepVideo-2B outperforms the baseline CogVideoX-2B and other state-of-the-art methods in various metrics, including motion smoothness, object class, multiple objects, and spatial relationship.\n- Both automated and human evaluations demonstrate RepVideo's superiority in generating high-quality videos with enhanced temporal coherence, spatial fidelity, and alignment with text prompts.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-16"
    },
    {
        "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
        "authors": "Wenjie Zhu, Wei Tan, Wei Yuan, Can Zhang, Sida Tian",
        "link": "https://arxiv.org/abs/2501.08809",
        "github_repo": null,
        "summary": "- XMusic is a novel framework for generating symbolic music from various prompt types, including images, videos, text, tags, and humming.\n- The framework consists of two main components: XProjector, which parses prompts into symbolic music elements (emotions, genres, rhythms, notes), and XComposer, which generates music based on these elements and selects high-quality outputs.\n- XComposer utilizes a Transformer Decoder as its generative model and incorporates a multi-task learning Selector for quality assessment, emotion recognition, and genre recognition.\n- XMusic is trained on a new large-scale symbolic music dataset, XMIDI, containing over 108,000 MIDI files with detailed emotion and genre annotations.\n- Both objective and subjective evaluations demonstrate XMusic\u2019s superior performance in terms of music quality and controllability compared to existing methods across various prompt types, including video, text and image conditioned generation.",
        "classification": [
            "Audio",
            "Text-to-Audio",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-16"
    },
    {
        "title": "Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography",
        "authors": "Sarah Meiklejohn, Ilia Shumailov, bballe, fhartmann, danrama",
        "link": "https://arxiv.org/abs/2501.08970",
        "github_repo": null,
        "summary": "- This paper introduces Trusted Capable Model Environments (TCMEs), a novel framework for secure multi-party computation leveraging capable machine learning models as trusted third parties.\n- TCMEs address privacy concerns in scenarios where traditional cryptographic solutions are computationally infeasible due to the complexity or unstructured nature of the data or computation.\n- The core principles of a TCME include statelessness of the model, explicit information flow control, and the use of trustworthy and capable models.\n- The paper presents several practical examples, such as multi-agent non-competition in research and confidential audits, showcasing the potential of TCMEs in diverse applications.\n- While acknowledging current limitations in model capabilities and the need for further research in areas like model verification, the paper positions TCMEs as a promising direction for private inference in complex scenarios.",
        "classification": [
            "Other"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-16"
    },
    {
        "title": "Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding",
        "authors": "douwh, Changyao, favor123, Einsiedler, wzk1015",
        "link": "https://arxiv.org/abs/2501.07783",
        "github_repo": "https://github.com/OpenGVLab/PIIP",
        "summary": "- PIIP (Parameter-Inverted Image Pyramid Networks) is proposed as a novel architecture for visual perception and multimodal understanding tasks.\n- It processes multi-scale images with different sized models: smaller models for higher resolutions and larger models for lower resolutions, which makes it more efficient than traditional image pyramids.\n- Cross-branch feature interaction and branch merging components allow information exchange and feature fusion between levels for enhanced performance.\n- PIIP-LLaVA, built on PIIP, adapts the architecture for efficient and effective high-resolution multimodal understanding.\n- PIIP demonstrates performance improvements of 1-2% with 40-60% less computation on object detection and semantic segmentation tasks, achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K with InternViT-6B.",
        "classification": [
            "Computer Vision",
            "Object Detection",
            "Image Segmentation",
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/OpenGVLab/PIIP"
        ],
        "huggingface_urls": [],
        "date": "2025-01-16"
    },
    {
        "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
        "authors": "Vincentchang, Ruixiang",
        "link": "https://arxiv.org/abs/2501.09012",
        "github_repo": "https://github.com/songrise/MLLM4Art",
        "summary": "- This paper investigates the ability of Multimodal Large Language Models (MLLMs) to evaluate the aesthetic quality of artworks, focusing on artistic stylization.\n- It introduces MM-StyleBench, a new large-scale dataset with diverse content and style instances, and develops a method for modeling human aesthetic preferences for benchmarking.\n- The study reveals a hallucination issue in MLLMs' art evaluation, tied to response subjectivity, and proposes ArtCoT, a prompting method with explicit task decomposition to mitigate this.\n- ArtCoT enhances MLLMs' reasoning ability, leading to increased alignment with human preferences, by encouraging concrete language and reducing subjective interpretations.\n- The findings offer insights into MLLMs' application in art evaluation and suggest potential benefits for downstream tasks like style transfer and image generation.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/songrise/MLLM4Art"
        ],
        "huggingface_urls": [],
        "date": "2025-01-16"
    },
    {
        "title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
        "authors": "Jie An, GiantBision, qiudavy, FireCRT, jchensteve",
        "link": "https://arxiv.org/abs/2501.09019",
        "github_repo": null,
        "summary": "- Ouroboros-Diffusion, a novel video denoising framework, is proposed for tuning-free long video generation, addressing the limitations of current methods like FIFO-Diffusion in maintaining temporal consistency.\n- The framework incorporates three key components: coherent tail latent sampling to improve structural consistency, Subject-Aware Cross-Frame Attention (SACFA) to enhance subject consistency within short segments, and self-recurrent guidance to leverage past subject information for long-range coherence.\n- Ouroboros-Diffusion outperforms state-of-the-art methods like FIFO-Diffusion and FreeNoise on the VBench benchmark, achieving higher scores in Subject Consistency, Background Consistency, Motion Smoothness, and Temporal Flickering.\n- The coherent tail latent sampling utilizes the low-frequency component of the second-to-last frame latent along with high-frequency noise for the new tail latent, ensuring structure similarity while introducing dynamics.\n- Experimental results on VBench demonstrate substantial improvements in generating consistent long videos, particularly in reducing temporal flickering and maintaining subject and background coherence across frames.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-16"
    },
    {
        "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
        "authors": "Ningyu, Runnaning, callanwu, JizhanFang, ZekunXi",
        "link": "https://arxiv.org/abs/2501.09751",
        "github_repo": null,
        "summary": "- OmniThink, a novel machine writing framework, enhances knowledge density in generated long-form articles by emulating human-like iterative expansion and reflection.\n- It simulates the cognitive process of learners progressively deepening their knowledge, iteratively adjusting retrieval strategies for thorough information exploration.\n- This framework incorporates expansion and reflection, outline structuring, and article composition stages, utilizing search engines and LLMs to generate nuanced, original content.\n- Evaluation on WildSeek dataset with GPT-40 and Qwen-Plus demonstrates improved knowledge density and overall quality compared to baselines like RAG, ORAG, STORM, and Co-STORM.\n- Human evaluations confirm enhanced breadth and depth, though automated and human novelty assessments diverge, suggesting areas for future evaluation refinement.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
        "authors": "mingdazhang, ycsu, hexianghu, S8T, willllis",
        "link": "https://arxiv.org/abs/2501.09732",
        "github_repo": null,
        "summary": "- This paper proposes a framework for scaling diffusion models at inference time by searching for optimal noise vectors for image generation, improving image quality beyond simply increasing denoising steps.\n- The framework introduces two key components: verifiers, which provide feedback during the search process, and search algorithms to explore the noise space.\n- Experiments on ImageNet and larger-scale text-conditioned image generation benchmarks show that increasing inference compute dedicated to search leads to substantial quality improvements across different model sizes and tasks.\n- The study demonstrates that different verifiers, such as CLIP, DINO, and ImageReward, possess different biases, and the optimal search configurations vary across tasks, suggesting the need for task-specific verifiers and search setups.\n- Random search, zero-order search, and search over paths algorithms are explored for efficiently scaling compute at inference time",
        "classification": [
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators",
        "authors": "Quan Tu, hsaest, ShizhengLi, sdujq, zhaocheng",
        "link": "https://arxiv.org/abs/2501.09484",
        "github_repo": "https://github.com/LIO-H-ZEN/PatientSimulator",
        "summary": "- This paper introduces a novel patient simulator trained on synthetic doctor-patient dialogue data generated using real patient dialogue strategies and medical records.\n- The simulator aims to address the limitations of prompt engineering in accurately representing patient behavior in online medical consultations (OMCs).\n- Experiments demonstrate that the simulator exhibits a lower hallucination rate and improved anthropomorphism compared to baselines, although the irrelevant response rate is slightly higher.\n- The study investigates the relationship between inquiry and diagnosis in OMCs and finds that they adhere to Liebig's law: poor inquiry limits effective diagnosis, and vice-versa.\n- By categorizing inquiries into four types, the research analyzes inquiry differences among models and reveals the importance of effective inquiry allocation within limited consultation rounds.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/LIO-H-ZEN/PatientSimulator"
        ],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces",
        "authors": "Jingyuan Liu, Yannick Hold-Geoffroy, Sumit Chaturvedi, zhixinshu, mengweir",
        "link": "https://arxiv.org/abs/2501.09756",
        "github_repo": null,
        "summary": "- SynthLight, a novel diffusion-based model, relights portraits by learning to re-render synthetic faces using a physically-based rendering engine and environment map lighting, bypassing explicit inverse rendering.\n- Trained on a synthetic dataset of 3D head renders under varied lighting, the model leverages multi-task training with real images (using a text-to-image task) and inference-time adaptation with classifier-free guidance to bridge the domain gap between synthetic and real images.\n- Quantitative evaluations on synthetic and Light Stage data reveal performance comparable to or exceeding state-of-the-art relighting methods.\n- User studies confirm that SynthLight delivers superior lighting accuracy, identity preservation, and overall image quality.\n- Qualitative results showcase unprecedented lighting effects like specular highlights, cast shadows, catch lights, subsurface scattering, and inter-reflections, generalizing effectively to complex portraits and unseen scenarios like half-body shots.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
        "authors": "oier-mees, dannydriess, brianichter, kylestach, KarlP",
        "link": "https://arxiv.org/abs/2501.09747",
        "github_repo": null,
        "summary": "- This paper introduces FAST (Frequency-space Action Sequence Tokenization), a novel compression-based tokenization scheme for robot actions, utilizing Discrete Cosine Transform (DCT) and Byte Pair Encoding (BPE) to improve the training of Vision-Language-Action (VLA) models, especially with high-frequency data.\n- It addresses the limitations of per-dimension binning, which struggles with high-frequency, correlated action sequences by compressing redundant data into fewer, high-information tokens.\n- Based on FAST, they introduce FAST+, a universal pre-trained tokenizer effective across different robot morphologies, action spaces, and control frequencies, offering a strong default for robot action tokenization.\n- Combining FAST with the \u03c00 VLA model, they demonstrate performance comparable to state-of-the-art diffusion-based VLAs on long-horizon, dexterous manipulation tasks while achieving up to 5x faster training speeds.\n- The \u03c00-FAST model trained with the proposed tokenization also successfully learns a generalist manipulation policy that generalizes to unseen environments in a zero-shot setting based on natural language prompts, the first of its kind on the DROID dataset.",
        "classification": [
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/physical-intelligence/fast"
        ],
        "date": "2025-01-17"
    },
    {
        "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
        "authors": "David Yan, Philippe Hansen-Estruch, endernewton, Tingbo, orrzohar",
        "link": "https://arxiv.org/abs/2501.09755",
        "github_repo": null,
        "summary": "- This paper introduces ViTok, a Vision Transformer-based tokenizer for image and video generation, which replaces traditional convolutional neural networks (CNNs) with an enhanced Vision Transformer (ViT) architecture combined with Llama, to improve scalability.\n- The study explores the impact of scaling the bottleneck size, encoder, and decoder of the auto-encoder on reconstruction and generation performance, finding that total floating points in the latent code are crucial for reconstruction, while scaling the decoder improves reconstruction but has limited generative benefits.  Scaling the encoder yielded minimal if any improvements to reconstruction or generation.\n- ViTok achieves competitive or state-of-the-art performance in image and video reconstruction on ImageNet-1K, COCO, and UCF-101 benchmarks while using significantly fewer FLOPs than existing methods.\n- Integrated with Diffusion Transformers, ViTok achieves competitive results in image generation and sets new benchmarks for class-conditional video generation on UCF-101.\n- The paper analyzes the trade-off between different loss functions in the decoder, suggesting its role as an extension of the generative model.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Image-to-Image",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://vitok.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
        "authors": "Jaime Fern\u00e1ndez Fisac, Thomas L. Griffiths, Ryan Liu, Haimin Hu, kaiquliang",
        "link": "https://arxiv.org/abs/2501.08617",
        "github_repo": null,
        "summary": "- This paper introduces Reinforcement Learning from Hindsight Simulation (RLHS), a new alignment algorithm to mitigate misalignment in Reinforcement Learning from Human Feedback (RLHF).\n- RLHS decouples human feedback on the outcomes of an interaction from the prediction of these outcomes by simulating plausible consequences and then eliciting feedback, reducing the AI's incentive to influence predictions and promoting better alignment with human utility.\n- The authors apply RLHS to both online (Proximal Policy Optimization - PPO) and offline (Direct Preference Optimization - DPO) preference optimization methods and show empirically that it reduces misalignment in both.\n- Through a human user study, RLHS outperforms RLHF, leading to improved user goal achievement and higher satisfaction ratings, despite training solely with simulated feedback.\n- These findings underscore the importance of incorporating long-term consequences, even if simulated, for enhancing alignment between AI and human values in RLHF.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
        "authors": "Ouyangtj, zhazhahui7, berserkerko, zzfoutofspace, haohao11",
        "link": "https://arxiv.org/abs/2501.09686",
        "github_repo": null,
        "summary": "- This paper surveys recent advancements in Large Language Model (LLM) reasoning, focusing on reinforced learning methods and prompting techniques.\n- The survey explores how \"thought\" sequences, representing intermediate reasoning steps, enhance LLM's reasoning abilities, moving beyond simple token generation.\n- It reviews techniques like Chain-of-Thought prompting, Tree-of-Thoughts, and reinforcement learning methods using Process Reward Models (PRMs) for training and test-time scaling.\n- The paper analyzes OpenAI's o1 series and open-source projects like OpenR, LLaMA-Berry, and Journey Learning, showcasing their approaches to achieving strong reasoning capabilities.\n- Finally, it discusses open challenges and future research directions, including refining test-time scaling, developing more advanced reasoning models, and exploring potential applications in diverse domains.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation",
        "authors": "Junjie He, Liefeng, gengyifeng, ashui, tuoyuxiang",
        "link": "https://arxiv.org/abs/2501.09503",
        "github_repo": null,
        "summary": "- AnyStory, a novel framework, is introduced to address the challenge of generating personalized images from text, focusing on single and multiple subjects personalization.\n- It leverages an \"encode-then-route\" paradigm, using a simplified ReferenceNet combined with a CLIP vision encoder for enhanced subject encoding, capturing both high-fidelity details and semantic concepts.\n- AnyStory uses a decoupled instance-aware subject router to guide subject condition injection, mitigating subject blending issues common in multi-subject generation.\n- Experimental results highlight AnyStory's effectiveness in preserving subject details, adhering to text descriptions, and personalizing for single and multiple subjects, showcasing advancements in personalized text-to-image generation.\n- The router's behavior exhibits similarities to image instance segmentation, suggesting potential applications in reference-prompted image segmentation using denoising U-Nets and trained routers.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/RealVisXL/RealVisXL-V4.0"
        ],
        "date": "2025-01-17"
    },
    {
        "title": "CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation",
        "authors": "Junyoung Choi, Jeong A Wi, Seongyeong Lee, Hwan Heo, longshiine",
        "link": "https://arxiv.org/abs/2501.09433",
        "github_repo": null,
        "summary": "- CaPa is a two-stage carve-and-paint framework for generating high-fidelity, textured 3D meshes from text or image inputs, which decouples geometry and texture generation.\n- Geometry generation uses a multi-view guided 3D latent diffusion model, ensuring structural consistency, followed by the generation of high-resolution textures (up to 4K) using a novel, model-agnostic Spatially Decoupled Attention method, which resolves multi-view inconsistencies like the Janus problem.\n- A 3D-aware occlusion inpainting algorithm further enhances texture completeness by filling untextured regions guided by a specialized UV map respecting surface locality.\n- Experimental results show CaPa produces higher fidelity textures and geometry as compared to existing methods like DreamCraft3D, Unique3D and SF3D, while also significantly reducing generation time to under 30 seconds.\n- CaPa's model-agnostic architecture in texture synthesis allows for direct integration with large pre-trained 2D generative models like SDXL and ControlNet, enhancing scalability and avoiding the need for extensive retraining.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "Do generative video models learn physical principles from watching videos?",
        "authors": "Priyank Jaini, Laura Culp, rgeirhos, kswersky, sam-motamed",
        "link": "https://arxiv.org/abs/2501.09038",
        "github_repo": "https://github.com/google-deepmind/physics-IQ-benchmark",
        "summary": "- This paper introduces Physics-IQ, a benchmark dataset designed to evaluate the understanding of physical principles in generative video models. \n- The dataset consists of 396 real-world videos covering various physical phenomena like fluid dynamics, optics, and magnetism, filmed from three different perspectives. \n- The models are tasked to predict the continuation of a video after observing a short initial sequence, and their predictions are evaluated against ground truth using metrics such as spatial and temporal intersection over union (IoU), and mean squared error (MSE). \n- Across a range of generative video models like Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet, the study finds a significant gap between visual realism and physical understanding, with even the best-performing models showing limited grasp of physical principles. \n- The research also finds no correlation between visual realism, assessed by a multimodal large language model (MLLM), and performance on the Physics-IQ benchmark, further highlighting the dissociation between generating realistic visuals and understanding underlying physics.",
        "classification": [
            "Computer Vision",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/google-deepmind/physics-IQ-benchmark"
        ],
        "huggingface_urls": [],
        "date": "2025-01-17"
    },
    {
        "title": "Evolving Deeper LLM Thinking",
        "authors": "Shumeet Baluja, Dave Marwood, Yueh-Hua Wu, Ian Fischer, Kuang-Huei Lee",
        "link": "https://arxiv.org/abs/2501.09891",
        "github_repo": null,
        "summary": "- This paper introduces Mind Evolution, a novel evolutionary search strategy for Large Language Models (LLMs) designed to enhance their problem-solving capabilities by efficiently utilizing inference-time compute.\n- Mind Evolution employs a genetic algorithm that generates, refines, and recombines candidate solutions in natural language, guided by an evaluator that provides feedback without needing explicit formalization of the underlying problem.\n- In experiments on TravelPlanner, Trip Planning, and Meeting Planning benchmarks, Mind Evolution with Gemini 1.5 Flash significantly outperformed Best-of-N and Sequential Revision, achieving success rates exceeding 95%, 96%, and 85%, respectively.\n- A two-stage approach using Gemini 1.5 Pro for unsolved instances further boosted performance to near-perfect scores on TravelPlanner and Meeting Planning, matching or exceeding state-of-the-art results achieved with formal solvers.\n- The authors also introduce StegPoet, a new challenging benchmark for stenographic encoding of hidden messages in creative text, where Mind Evolution achieved a success rate of 87% using Gemini 1.5 Pro, demonstrating the method's applicability to less formalized tasks.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-20"
    },
    {
        "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
        "authors": "Yuchen Zhang, Yuan Lin, Peiyuan Feng, Guanhua Huang, Yichen He",
        "link": "https://arxiv.org/abs/2501.10120",
        "github_repo": "https://github.com/bytedance/pasa",
        "summary": "- PaSa is a novel Large Language Model (LLM) agent designed for comprehensive academic paper searches that mimics human researcher behaviour by autonomously making decisions such as invoking search tools, reading papers and selecting relevant references.\n- PaSa consists of two LLM agents, the Crawler and the Selector.\n- The Crawler collects relevant papers by using search tools or extracting citations from the current paper and adding them to a growing paper queue, which the Selector then reads each paper in to determine whether it meets the requirements of the user query.\n- The authors build a synthetic but high-quality academic search dataset, AutoScholarQuery, based on fine-grained scholar queries and their corresponding relevant papers from ICLR 2023, ICML 2023, NeurIPS 2023, ACL 2024, and CVPR 2024.\n- PaSa-7b significantly outperforms all baselines on the RealScholarQuery dataset including Google, Google Scholar, Google paired with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4), GPT-3.5-Turbo, and PaSa-GPT-4, achieving 37.78% improvement in Recall@20 and 39.90% improvement in Recall@50 compared to the strongest Google based baseline (Google with GPT-4).",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/bytedance/pasa"
        ],
        "huggingface_urls": [],
        "date": "2025-01-20"
    },
    {
        "title": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions",
        "authors": "Liefeng Bo, Jianqiang Ren, Chao He",
        "link": "https://arxiv.org/abs/2501.10020",
        "github_repo": null,
        "summary": "- Textoon is a novel method for generating animatable 2D cartoon characters in the Live2D format from text descriptions, leveraging large language and vision models.\n- The system parses complex text descriptions using a fine-tuned LLM to identify character features like hair, eyes, clothing, and shoes, achieving over 90% accuracy.\n- It employs Stable Diffusion XL for controllable appearance generation, ensuring high-quality images and precise text pattern generation while maintaining model driving performance.\n- Textoon addresses component completion challenges by using a template-based approach for pixel extraction and image-to-image control generation for refining occluded areas.\n- For animation, Textoon enhances facial expressiveness by integrating ARKit's face blend shape capabilities into Live2D, enabling more detailed and lively lip-sync and facial movements.",
        "classification": [
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-20"
    },
    {
        "title": "Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong",
        "authors": "Pedro Reviriego, Gonzalo Mart\u00ednez, Javier Conde, Tairan Fu, mariagrandury",
        "link": "https://arxiv.org/abs/2501.09775",
        "github_repo": null,
        "summary": "- This paper investigates how Large Language Models (LLMs) self-confidence changes when they are asked to provide reasoning before answering multiple-choice questions (MCQs).\n- The study uses the Massive Multitask Language Understanding (MMLU) benchmark and evaluates seven different LLMs, including models from Meta, Mistral, Google, 01.AI, and OpenAI.\n- Results show that LLMs exhibit higher confidence in their selected answers when they provide reasoning, regardless of answer correctness. \n- This increased confidence is more pronounced for incorrect answers and is attributed to the autoregressive nature of LLMs and the influence of reasoning on predicted probabilities.\n- This behavior is consistent with human behavior, suggesting potential limitations of using confidence estimates for LLM evaluation and raising questions about the effectiveness of reasoning for certain question types.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/aMa2210/LLM_MCQ_LogProbs"
        ],
        "huggingface_urls": [],
        "date": "2025-01-20"
    },
    {
        "title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution",
        "authors": "Chong Zhang, Yukun Ma, Zexu Pan, Kun Zhou, Shengkui Zhao",
        "link": "https://arxiv.org/abs/2501.10045",
        "github_repo": "https://github.com/modelscope/ClearerVoice-Studio",
        "summary": "- HiFi-SR, a unified transformer-convolutional generative adversarial network (GAN), is proposed for high-fidelity speech super-resolution.\n- The model uses a transformer network as an encoder to convert low-resolution mel-spectrograms into latent representations, and a convolutional network (based on HiFi-GAN generator) upscales these representations into high-resolution waveforms.\n- A multi-band, multi-scale time-frequency discriminator and a multi-scale mel-reconstruction loss are incorporated to enhance high-frequency fidelity during adversarial training.\n- HiFi-SR can upscale any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate.\n- Experimental results on VCTK, EXPRESSO, and VocalSet datasets demonstrate that HiFi-SR significantly outperforms existing methods in both objective metrics (LSD) and subjective listening tests (ABX).",
        "classification": [
            "Audio",
            "Audio-to-Audio"
        ],
        "github_urls": [
            "https://github.com/modelscope/ClearerVoice-Studio"
        ],
        "huggingface_urls": [],
        "date": "2025-01-20"
    },
    {
        "title": "X-Dyna: Expressive Dynamic Human Image Animation",
        "authors": "Zhengfei Kuang, Yipeng Gao, You Xie, Hongyi Xu, Boese0601",
        "link": "https://arxiv.org/abs/2501.10021",
        "github_repo": "https://github.com/bytedance/X-Dyna",
        "summary": "- X-Dyna is a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, generating realistic, context-aware dynamics for both the subject and the surrounding environment.\n- It addresses key shortcomings of prior pose control-centered approaches, enhancing the lifelike qualities of human video animations through a Dynamics-Adapter.\n- This lightweight module integrates reference appearance context into the diffusion backbone's spatial attentions while preserving motion modules' ability to synthesize dynamic details.\n- Beyond body pose, it employs a local control module for identity-disentangled facial expression transfer, enhancing realism. \n- Evaluations demonstrate X-Dyna outperforms state-of-the-art methods, creating lifelike and expressive animations, shown by improved metrics and user preference over existing models on dynamic texture generation.",
        "classification": [
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/bytedance/X-Dyna"
        ],
        "huggingface_urls": [
            "https://x-dyna.github.io/xdyna.github.io/"
        ],
        "date": "2025-01-20"
    },
    {
        "title": "GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor",
        "authors": "Yuan Liu, Qi Zhang, Heng Li, Kunming Luo, Xiangyue Liu",
        "link": "https://arxiv.org/abs/2501.09978",
        "github_repo": null,
        "summary": "- GaussianAvatar-Editor is a novel framework for text-driven editing of animatable 3D Gaussian head avatars, enabling modifications to expression, pose, and viewpoint.\n- It addresses challenges like motion occlusion and 4D inconsistency by introducing the Weighted Alpha Blending Equation (WABE), prioritizing visible Gaussians during editing and preserving non-visible ones.\n- Conditional adversarial learning is incorporated to improve editing quality and ensure consistency during animation.\n- The method is evaluated on a multi-view video dataset, demonstrating superior performance compared to relevant baselines in novel view synthesis, self-reenactment, and cross-identity reenactment.\n- The results showcase consistent, high-quality edits across different viewpoints, expressions, and even when transferring edits to new actors.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-20"
    },
    {
        "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario",
        "authors": "Jie Tang, Haiyi Hu, Xiaohan Zhang, Zhengxiao Du, Lucen Zhong",
        "link": "https://arxiv.org/abs/2501.10132",
        "github_repo": "https://github.com/THUDM/ComplexFuncBench",
        "summary": "- ComplexFuncBench, a new benchmark for evaluating complex function calling in LLMs, focusing on multi-step and constrained scenarios within a 128k long context, is introduced.\n- Unlike existing benchmarks, it incorporates multi-step calls, user constraints, parameter value reasoning from implicit info, long parameter values, and a 128k context.\n- ComplexEval, an automatic evaluation framework, uses multi-dimensional matching (rule-based, response-based, and LLM-based) to overcome limitations of traditional exact matching.\n- Experiments on various LLMs reveal that closed-source models outperform open-source models, and parameter value errors are a significant challenge.\n- Different models exhibit specific weaknesses in handling various parameter types and planning function call steps.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/THUDM/ComplexFuncBench"
        ],
        "huggingface_urls": [],
        "date": "2025-01-20"
    },
    {
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "authors": "Yiran Qin, XihuiLiu, di-zhang-fdu, Xintao, VictorYuki",
        "link": "https://arxiv.org/abs/2501.08325",
        "github_repo": null,
        "summary": "- GameFactory is a new framework for creating novel games using generative interactive videos by leveraging pre-trained video diffusion models and introducing a new action-controllable dataset, GF-Minecraft.\n- It employs a multi-phase training strategy that decouples the learning of game style from action control, addressing the challenge of scene generalization in game video generation.\n- GameFactory utilizes a specialized action control module that incorporates distinct mechanisms for continuous mouse movements (concatenation) and discrete keyboard inputs (cross-attention).\n- It extends the model to enable autoregressive long video generation, crucial for practical game applications.\n- Experimental results demonstrate GameFactory\u2019s ability to effectively generate open-domain, diverse, and action-controllable videos, representing a significant advancement in game generation technology.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/KwaiVGI/GF-Minecraft"
        ],
        "date": "2025-01-21"
    },
    {
        "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
        "authors": "Bingyi Kang, Yao Zhao, Xun Guo, Yunchao Wei, maverickrzw",
        "link": "https://arxiv.org/abs/2501.09781",
        "github_repo": null,
        "summary": "- VideoWorld, an auto-regressive video generation model trained on unlabeled video data, is proposed to explore knowledge learning from visual input.\n- The model architecture consists of a VQ-VAE for encoding video frames into discrete tokens, a transformer for next-token prediction, and a Latent Dynamics Model (LDM) to represent multi-step future visual changes.\n- Experiments on video-based Go and robotic control tasks show that video-only training allows for knowledge acquisition, including rules, reasoning, and planning capabilities.\n- VideoWorld reached a 5-dan professional level in Video-GoBench without search or reward mechanisms, showing strong performance on complex planning tasks.\n- The model shows promising generalization in robotic control tasks across multiple environments, highlighting the potential of video-based knowledge learning.",
        "classification": [
            "Computer Vision",
            "Robotics",
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-21"
    },
    {
        "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
        "authors": "Zhengyin Du, Zhiheng Xi, Junjie-Ye, lovesnowbest, siyuyuan",
        "link": "https://arxiv.org/abs/2501.11425",
        "github_repo": null,
        "summary": "- This paper introduces Agent-R, a novel iterative self-training framework designed to improve the error correction capabilities of large language model (LLM) agents in interactive environments.\n- Agent-R leverages Monte Carlo Tree Search (MCTS) to dynamically construct training samples, enabling agents to learn from their mistakes by revising erroneous trajectories.\n- The framework includes a model-guided critique construction mechanism where the actor model pinpoints the first error in a failed trajectory and splices it with the adjacent correct path, facilitating timely error correction.\n- Experimental results across three interactive and agentic environments (WebShop, SciWorld, and TextCraft) demonstrate that Agent-R surpasses baseline methods and agents trained on expert trajectories, achieving superior performance (+5.59%).\n- Agent-R also equips agents with the ability to more effectively identify and correct erroneous actions in real time while avoiding loops, addressing a key limitation of previous methods.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/bytedance/Agent-R"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered"
        ],
        "date": "2025-01-22"
    },
    {
        "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
        "authors": "Lujing Xie, Yilun Zhao, Phil-01, entropyhu, freesky",
        "link": "https://arxiv.org/abs/2501.12380",
        "github_repo": null,
        "summary": "- MMVU, a new expert-level multi-discipline benchmark, is introduced for evaluating foundation models in video understanding, comprising 3,000 expert-annotated questions across 27 subjects in Science, Healthcare, Humanities & Social Sciences, and Engineering.\n- MMVU emphasizes domain-specific knowledge application and complex reasoning for specialized-domain video analysis, going beyond basic visual perception common in current video benchmarks.\n- Each MMVU example includes expert annotations from scratch with stringent quality control, enriched by expert-written reasoning rationales and domain knowledge.\n- In evaluations of 32 prominent models, advanced System-2 models like o1 and Gemini 2.0 Flash Thinking achieved top performance, though still below human expertise.\n- This work provides valuable insights for enhancing expert-level, knowledge-intensive video understanding in specialized domains.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "github.com/yale-nlp/MMVU"
        ],
        "huggingface_urls": [
            "huggingface.co/datasets/yale-nlp/MMVU"
        ],
        "date": "2025-01-22"
    },
    {
        "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
        "authors": "Kaiyue Wen, Bo Zheng, Zeyu Huang, Zihan Qiu, Losin94",
        "link": "https://arxiv.org/abs/2501.11873",
        "github_repo": null,
        "summary": "- This paper proposes a global-batch load balancing loss (LBL) strategy for training Mixture-of-Experts (MoE) models, addressing the limitations of the commonly used micro-batch LBL.\n- The micro-batch LBL enforces load balancing at the sequence level, hindering expert specialization, particularly in domain-specific tasks, whereas the global-batch LBL promotes load balancing at the corpus level, encouraging specialization.\n- The global-batch LBL involves synchronizing expert selection frequencies across parallel groups, introducing minimal computational overhead.\n- Experiments on various MoE model sizes (up to 42.8B parameters) trained on up to 400B tokens show that global-batch LBL significantly improves both pre-training perplexity and downstream task performance.\n- Analysis reveals that global-batch LBL leads to more interpretable expert specialization, aligning routing decisions with the language modeling task.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
        "authors": "Shihao Liang, Haoming Wang, Junjie Fang, Yining Ye, Yujia Qin",
        "link": "https://arxiv.org/abs/2501.12326",
        "github_repo": null,
        "summary": "- UI-TARS is a native GUI agent model that perceives screenshots and performs human-like interactions, such as keyboard and mouse operations, outperforming current agent frameworks.\n- It incorporates enhanced perception through a large-scale dataset of GUI screenshots for context-aware understanding, unified action modeling for multi-step execution across platforms, and system-2 reasoning for deliberate decision-making.\n- UI-TARS addresses the data bottleneck in end-to-end agent training by automatically collecting, filtering, and refining interaction traces on virtual machines, along with reflection tuning to recover from errors.\n- In experiments on 10+ GUI agent benchmarks, UI-TARS achieved SOTA performance in perception, grounding, and GUI task execution, surpassing models like GPT-40 and Claude.\n- Notably, UI-TARS achieved 24.6 on OSWorld (50 steps) and 46.6 on AndroidWorld, exceeding Claude's 22.0 and GPT-40's 34.5, respectively.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/bytedance/UI-TARS"
        ],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
        "authors": "Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy, mikewang",
        "link": "https://arxiv.org/abs/2501.11733",
        "github_repo": null,
        "summary": "- This paper introduces Mobile-Agent-E, a novel hierarchical multi-agent framework for mobile task automation, featuring self-evolution capabilities through learning and applying reusable *Shortcuts* and *Tips* from past experiences.\n- Mobile-Agent-E consists of a *Manager*, *Perceptor*, *Operator*, *Action Reflector*, and *Notetaker* agents to handle planning, visual perception, action execution, error verification, and information aggregation respectively.\n- It also proposes Mobile-Eval-E, a new benchmark focusing on complex, long-horizon, multi-app mobile tasks, along with a *Satisfaction Score* metric based on human-written rubrics for evaluating open-ended tasks.\n- Experimental results on Mobile-Eval-E demonstrate that Mobile-Agent-E outperforms previous state-of-the-art approaches by a significant margin, achieving a 22.1% absolute improvement in Satisfaction Score with GPT-40.\n- The inclusion of a self-evolution module shows further performance gains and improved efficiency.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/x-plug/MobileAgent"
        ],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
        "authors": "Zilong Huang, Feihu Zhang, Shengnan Zhu, Hengkai Guo, Sili Chen",
        "link": "https://arxiv.org/abs/2501.12375",
        "github_repo": null,
        "summary": "- Video Depth Anything, a new feed-forward video transformer model based on Depth Anything V2, excels in temporally consistent video depth estimation for arbitrarily long videos.\n- This model substitutes the DPT head with a spatial-temporal head incorporating temporal attention layers and introduces a novel temporal gradient matching loss to ensure temporal consistency.\n- Trained jointly on video depth data and unlabeled images, the model demonstrates superior performance without relying on geometric or generative priors.\n- It utilizes a key-frame-based inference strategy for processing super-long videos.\n- Evaluation across various benchmarks reveals state-of-the-art results in zero-shot video depth estimation, surpassing baselines in accuracy and consistency while maintaining computational efficiency.",
        "classification": [
            "Depth Estimation",
            "Computer Vision",
            "Video Classification"
        ],
        "github_urls": [
            "videodepthanything.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
        "authors": "Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Zibo Zhao",
        "link": "https://arxiv.org/abs/2501.12202",
        "github_repo": "https://github.com/Tencent/Hunyuan3D-2",
        "summary": "- Hunyuan3D 2.0 is a large-scale 3D synthesis system that generates high-resolution textured 3D assets from images or text prompts using two foundational models: Hunyuan3D-DiT (shape generation) and Hunyuan3D-Paint (texture synthesis).\n- Hunyuan3D-DiT uses a scalable flow-based diffusion transformer on a latent space generated by an autoencoder (Hunyuan3D-ShapeVAE), enabling the creation of detailed meshes aligned with given image conditions.\n- Hunyuan3D-Paint utilizes a mesh-conditioned multi-view image generation pipeline along with image pre-processing and baking techniques to synthesize high-resolution texture maps consistent with the generated or user-provided mesh and the input prompt.\n- Evaluations show that Hunyuan3D 2.0 outperforms existing open-source and commercial models in geometry details, image alignment, and texture quality across various metrics.\n- A user study confirmed these results, indicating a preference for Hunyuan3D 2.0 generated assets and highlighting its superior alignment with image prompts.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/Tencent/Hunyuan3D-2"
        ],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
        "authors": "Tao Yu, Pengcheng Yin, Jinsung Yoon, Ruoxi Sun, Hongjin Su",
        "link": "https://arxiv.org/abs/2501.10893",
        "github_repo": null,
        "summary": "- LEARN-BY-INTERACT is a data-centric framework designed to enable Large Language Model (LLM) agents to self-adapt to new environments without human annotations by synthesizing trajectories of agent-environment interactions based on documentation.\n- It constructs instructions by summarizing or abstracting interaction histories (backward construction) and uses these in training-based and training-free in-context learning scenarios with retrieval approaches optimized for agents.\n- Experiments across coding, web, and desktop environments (SWE-bench, WebArena, OSWorld, Spider2-V) show LEARN-BY-INTERACT improves baseline results, with up to 12.2% for in-context learning with Claude-3.5 and 19.5% for training with Codestral-22B.\n- Backward construction contributes significantly to performance, improving results by up to 14%.\n- The framework's agentic retrieval pipeline demonstrates superiority over conventional retrieval-augmented generation.",
        "classification": [
            "Reinforcement Learning",
            "Robotics",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "Reasoning Language Models: A Blueprint",
        "authors": "Afonso Catarino, Ales Kubicek, Eric Schreiber, Julia Barth, Maciej Besta",
        "link": "https://arxiv.org/abs/2501.11223",
        "github_repo": null,
        "summary": "- This paper introduces a blueprint for Reasoning Language Models (RLMs), providing a modular framework for their design and analysis.\n- The blueprint incorporates various reasoning structures, strategies, and training schemes, unifying diverse RLM approaches like MCTS, reinforcement learning, and structured prompting.\n- A modular implementation, x1, is presented for rapid RLM prototyping and experimentation, along with insights like multi-phase training and the importance of familiar training distributions.\n- Analysis of existing RLMs like LLaMA-Berry and QwQ demonstrates the blueprint's versatility and unifying potential.\n- The work aims to democratize advanced reasoning capabilities, fostering innovation and bridging the gap between \"rich AI\" and \"poor AI\".",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/spcl/x1"
        ],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
        "authors": "Chuyu Zhang, Mo Li, Taolin Zhang, Maosong Cao, zsytony",
        "link": "https://arxiv.org/abs/2501.12273",
        "github_repo": "https://github.com/InternLM/Condor",
        "summary": "- Condor, a two-stage framework for synthetic data generation, enhances Large Language Model (LLM) alignment by leveraging a World Knowledge Tree and self-reflection refinement.\n- The first stage, Condor Void, uses a knowledge inspiration strategy with the World Knowledge Tree to create diverse questions and initial responses, forming the Dv dataset.\n- The second stage, Condor Refine, applies a self-reflection mechanism allowing the model to iteratively refine Dv responses based on self-generated critiques, generating a higher-quality DR dataset.\n- Experiments using various LLMs, including Qwen, InternLM, and Llama, demonstrate that Condor-generated data significantly improves performance on subjective chat benchmarks compared to officially released and RLHF-trained models, even without RLHF incorporated in the Condor training pipeline.\n- Additional experiments on knowledge-based benchmarks reveal that Condor maintains the models' knowledge QA capabilities while improving conversational ability.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/InternLM/Condor"
        ],
        "huggingface_urls": [
            "https://hf.co/datasets/internlm/Condor-SFT-20K"
        ],
        "date": "2025-01-22"
    },
    {
        "title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
        "authors": "Liefeng Bo, Bang Zhang, Qi Wang, Siqi Hu, Linrui Tian",
        "link": "https://arxiv.org/abs/2501.10687",
        "github_repo": null,
        "summary": "- EMO2 is a novel two-stage audio-driven talking head method that generates expressive facial expressions and synchronized hand gestures from a single reference image and audio input.\n- The first stage uses a motion diffusion model to generate hand poses from audio, leveraging the strong correlation between audio and hand movements.\n- The second stage employs a diffusion-based model with a ReferenceNet backbone to synthesize video frames, incorporating the generated hand poses to produce realistic facial expressions and body movements, guided by \"pixels prior IK\".\n- Experimental results demonstrate that EMO2 outperforms state-of-the-art methods, such as CyberHost and Vlogger, in terms of visual quality, synchronization accuracy, and motion diversity, particularly in generating more vivid and expressive hand motions.\n- The method addresses the challenge of weak correspondence between audio and full-body gestures by focusing on hand motion generation and leveraging the implicit IK knowledge within 2D generative models.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "GPS as a Control Signal for Image Generation",
        "authors": "Andrew Owens, Alexei A. Efros, Aleksander Holynski, Ziyang Chen, chfeng",
        "link": "https://arxiv.org/abs/2501.12390",
        "github_repo": null,
        "summary": "- This paper introduces a novel approach to image generation using GPS coordinates as a control signal, enabling the generation of images that reflect the visual characteristics of specific locations within a city.\n- The proposed GPS-to-image diffusion model is conditioned on both GPS coordinates and text prompts, enabling compositional generation and fine-grained control over the generated images.\n- The model can also be used to extract 3D models from images by conditioning a NeRF on GPS coordinates and utilizing a score distillation sampling technique, avoiding the need for explicit camera pose estimation or feature matching.\n- Evaluations on image and 3D datasets from New York City and Paris demonstrate that the model captures subtle location-based variations in images and improves 3D structure estimation.\n- This approach opens up possibilities for applications such as location-based image editing, generation of location-specific content, and 3D reconstruction from geotagged photo collections.",
        "classification": [
            "Text-to-Image",
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://cfeng16.github.io/gps-gen/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space",
        "authors": "Shiran Zada, Omer Tov, Roni Paiss, Shahar Yadin, Daniel Garibi",
        "link": "https://arxiv.org/abs/2501.12224",
        "github_repo": null,
        "summary": "- TokenVerse is a novel method for multi-concept personalization in text-to-image generation using diffusion transformers (DiTs).\n- It leverages the modulation space of DiTs, learning personalized modulation vectors for each token in a given text prompt, allowing for disentangled control over various visual concepts, including objects, poses, materials, and lighting.\n-  Unlike previous methods, TokenVerse handles multiple images with multiple concepts each, enabling the combination of learned concepts from different images into new generated images.\n- The method employs a two-stage optimization process to learn personalized modulation vectors and incorporates a concept isolation loss to prevent interference between concepts from different images.\n-  Evaluations show that TokenVerse outperforms existing methods in terms of concept preservation and prompt fidelity, both qualitatively and quantitatively, including a user study.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "MSTS: A Multimodal Safety Test Suite for Vision-Language Models",
        "authors": "Alicia Parrish, Janis Goldzycher, Felix Friedrich, Giuseppe Attanasio, Paul R\u00f6ttger",
        "link": "https://arxiv.org/abs/2501.10057",
        "github_repo": null,
        "summary": "- This paper introduces MSTS, a Multimodal Safety Test Suite for Vision-Language Models (VLMs).\n- MSTS comprises 400 unsafe multimodal English-language prompts across 40 fine-grained hazard categories and is designed to test the safety of VLMs in a structured manner.\n- MSTS test prompts consist of both a textual and visual component designed to be safe individually but unsafe when combined.\n- Commercial VLMs generally respond safely to MSTS while open VLMs have clear safety issues often responding unsafely or failing to interpret the multimodal input correctly.\n- This research highlights the need for further research into VLM safety and the importance of multimodal inputs in safety evaluation.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/paul-rottger/msts-multimodal-safety"
        ],
        "huggingface_urls": [],
        "date": "2025-01-22"
    },
    {
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "authors": "AS-7, haha-point, freesky, DejianYang, guoday",
        "link": "https://arxiv.org/abs/2501.12948",
        "github_repo": null,
        "summary": "- This paper introduces DeepSeek-R1, a large language model designed for enhanced reasoning capabilities, along with DeepSeek-R1-Zero, a model trained purely via reinforcement learning (RL) without supervised fine-tuning.\n- DeepSeek-R1-Zero showcases emergent reasoning abilities but suffers from readability and language mixing issues, prompting the development of DeepSeek-R1, which incorporates a multi-stage training pipeline with cold-start data and RL fine-tuning.\n- DeepSeek-R1 achieves comparable performance to OpenAI-01-1217 on several reasoning benchmarks and outperforms DeepSeek-V3.\n- Through knowledge distillation from DeepSeek-R1 to smaller models, even a 7B model surpasses existing open-source models on several benchmarks, and 32B and 70B models set new performance records.\n- The paper releases a series of distilled models based on Qwen and Llama.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
        "authors": "Senbao Shi, Li-Zhouyi, PigCatchingExpert, longyuewang, imryanxu",
        "link": "https://arxiv.org/abs/2501.12909",
        "github_repo": null,
        "summary": "- FilmAgent, a novel LLM-based multi-agent collaborative framework, automates end-to-end virtual film production, encompassing idea development, scriptwriting, cinematography, and actor actions within pre-built Unity 3D environments.\n- Employing two novel multi-agent collaboration strategies, Critique-Correct-Verify and Debate-Judge, the framework leverages LLMs as virtual crew members (director, screenwriter, actors, cinematographer) to enhance communication and refine film elements iteratively.\n- Human evaluations across 15 film ideas demonstrate FilmAgent's superiority, achieving an average score of 3.98 out of 5, significantly outperforming single-agent baselines and even surpassing OpenAI's larger reasoning model, o1, in a multi-agent setting.\n- Compared with OpenAI's text-to-video model Sora, FilmAgent exhibits stronger storytelling and coherence in longer videos, owing to the pre-designed 3D spaces and character interactions, while Sora showcases greater adaptability and stylistic flexibility but lacks consistency and physics compliance.\n- The framework addresses limitations in existing automated film production methods by incorporating communication-driven collaboration, offering a promising approach to end-to-end film automation using AI agents.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
        "authors": "Yu Cheng, linjieli222, Xiaoye08, huxy912, yaful",
        "link": "https://arxiv.org/abs/2501.12895",
        "github_repo": "https://github.com/yafuly/TPO",
        "summary": "- Test-Time Preference Optimization (TPO) is introduced, a framework that aligns Large Language Model (LLM) outputs with human preferences during inference without retraining.\n- TPO translates reward signals into textual critiques and utilizes them as rewards to iteratively refine LLM responses, unlike methods relying solely on numerical rewards.\n- Evaluations across various NLP tasks demonstrate that TPO progressively improves alignment with human preferences, and in some cases, the unaligned LLM with TPO surpasses its aligned counterpart.\n- TPO efficiently scales with both search width and depth during inference.\n- Case studies illustrate TPO's ability to exploit LLMs' capacity to interpret and act on reward signals, making it a practical, lightweight alternative for on-the-fly preference optimization.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/yafuly/TPO"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
        "authors": "Sicong, Guanzheng, Zhiqiang007, ClownRat, CausalLi",
        "link": "https://arxiv.org/abs/2501.13106",
        "github_repo": "https://github.com/DAMO-NLP-SG/VideoLLaMA3",
        "summary": "- VideoLLaMA3 is a multimodal foundation model for image and video understanding that leverages a vision-centric training paradigm and framework design. \n- The model architecture incorporates an any-resolution vision tokenization (AVT) technique, enabling it to handle varying input resolutions, and a differential frame pruner (DiffFP) for efficient video compression.\n- VideoLLaMA3 is trained in four stages: vision encoder adaptation, vision-language alignment, multi-task fine-tuning, and video-centric fine-tuning. \n- The model outperforms existing state-of-the-art models on a variety of benchmarks, demonstrating strong abilities in image and video comprehension, including chart and document understanding, mathematical reasoning, temporal reasoning, and grounding. \n- Notably, VideoLLaMA3 demonstrates significant improvements in chart understanding and vision-related math problem solving in image understanding, as well as achieving state-of-the-art performance in multiple video understanding benchmarks.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Video-Text-to-Text",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/DAMO-NLP-SG/VideoLLaMA3"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
        "authors": "ChonghuaLiao, DuChenZhuang, shelowize, xingbowei, KbsdJames",
        "link": "https://arxiv.org/abs/2501.12599",
        "github_repo": null,
        "summary": "- Kimi k1.5 is a multimodal large language model (LLM) trained using reinforcement learning (RL) with a focus on scaling context length for improved reasoning abilities.\n- The model architecture is based on a Transformer decoder and incorporates techniques like partial rollouts, improved policy optimization, and length penalty for efficient RL training.\n- It achieves state-of-the-art results on multiple benchmarks, including 77.5 on AIME, 96.2 on MATH 500, and 94-th percentile on Codeforces, matching OpenAI's GPT-01 on MathVista.\n- Long2short methods are introduced to improve short-context models by transferring knowledge from long-context models, resulting in significantly improved token efficiency, for example, k1.5-short w/ rl achieves a Pass@1 score of 60.8 on AIME2024 while utilizing only 3,272 tokens on average.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Autonomy-of-Experts Models",
        "authors": "Yining Qian, kangzhanhui, shwu, Ruobing-Xie, AngLv",
        "link": "https://arxiv.org/abs/2501.13074",
        "github_repo": null,
        "summary": "- This paper introduces Autonomy-of-Experts (AoE), a novel Mixture-of-Experts (MoE) paradigm for large language models.\n- AoE allows experts to autonomously decide whether to process inputs based on the scale of their internal activations, eliminating the need for a separate router.\n- By pre-computing and ranking internal activation norms, only the top-activated experts process each token, while others abort, thereby improving efficiency.\n- The overhead of pre-computing activations is reduced through low-rank weight factorization.\n- Experimental results demonstrate that AoE outperforms traditional MoE models on downstream tasks with comparable efficiency across various model sizes up to 4 billion parameters.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament",
        "authors": "Yixin Cao, Rui Min, Zijun Yao, Yantao Liu, juanli",
        "link": "https://arxiv.org/abs/2501.13007",
        "github_repo": null,
        "summary": "- This paper introduces Pairwise Reward Model (Pairwise RM), a novel approach for Best-of-N (BoN) sampling in Large Language Models (LLMs), particularly for math reasoning tasks.\n- Instead of assigning absolute scores, Pairwise RM evaluates two candidate solutions simultaneously, using a knockout tournament to select the best solution through pairwise comparisons.\n- This method addresses limitations of traditional reward models by eliminating arbitrary scoring and enabling cross-validation.\n- A new dataset, PAIRWISE-443K, with 443K pairwise comparisons is created for model training.\n- Experimental results on MATH-500 and Olympiad Bench demonstrate significant improvements over baseline models, showing a 40% to 60% relative improvement on the top 50% challenging problems in MATH-500.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/THU-KEG/PairwiseRM/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
        "authors": "Yibo Wang, Haiying He, Li Shen, cxc361461518, iNk233",
        "link": "https://arxiv.org/abs/2501.12570",
        "github_repo": "https://github.com/StarDewXXX/O1-Pruner",
        "summary": "- This paper introduces Length-Harmonizing Fine-Tuning (O1-Pruner), a novel method to optimize long-thought reasoning in Large Language Models (LLMs) for mathematical problem-solving.\n- O1-Pruner addresses the issue of reasoning length disharmony in LLMs, where models generate solutions of varying lengths with shorter, yet accurate, solutions often available, leading to computational redundancy.\n- The method utilizes a Reinforcement Learning (RL)-style fine-tuning approach, incorporating an accuracy constraint to ensure that optimizing for shorter reasoning processes does not compromise problem-solving accuracy.\n- Experimental results on benchmark datasets like MATH, GSM8k, and GaoKao demonstrate that O1-Pruner achieves a better balance between solution length and accuracy compared to baseline and other competing methods like SFT and DPO, leading to improved inference efficiency.\n- The proposed method is evaluated using both accuracy and a new metric called Accuracy-Efficiency Score (AES), showing consistent improvements in reducing solution length while maintaining or improving accuracy.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/StarDewXXX/O1-Pruner"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
        "authors": "Ilankad23, Eladlev",
        "link": "https://arxiv.org/abs/2501.11067",
        "github_repo": "https://github.com/plurai-ai/intellagent",
        "summary": "- This paper introduces IntellAgent, a multi-agent framework for evaluating conversational AI systems.\n- IntellAgent leverages a novel approach by automating the generation of diverse, synthetic scenarios that test conversational AI agents across various aspects including multi-turn dialogues, policy adherence, and API usage.\n- IntellAgent uses a graph-based policy model to represent the relationships and complexities of policy interactions, enabling fine-grained diagnostics.\n- Experimental results demonstrate a strong correlation between model performance on the IntellAgent benchmark and the T-bench, despite IntellAgent relying entirely on synthetic data.\n- The findings indicate a decrease in model performance with increasing complexity and variations in capabilities across different policy categories, highlighting IntellAgent's ability to provide detailed insights for targeted optimization.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/plurai-ai/intellagent",
            "https://github.com/langchain-ai/langgraph"
        ],
        "date": "2025-01-23"
    },
    {
        "title": "SRMT: Shared Memory for Multi-agent Lifelong Pathfinding",
        "authors": "Yuri Kuratov, mbur, alsu-sagirova",
        "link": "https://arxiv.org/abs/2501.13200",
        "github_repo": "https://github.com/Aloriosa/srmt",
        "summary": "- This paper introduces the Shared Recurrent Memory Transformer (SRMT), a novel architecture extending memory transformers to multi-agent settings for improved coordination in pathfinding tasks.\n- SRMT pools and globally broadcasts individual recurrent memory vectors, allowing agents to implicitly exchange information and coordinate actions without explicit communication, utilizing an attention block inspired by the Huggingface GPT-2 model.\n- Evaluated on a Partially Observable Multi-Agent Pathfinding (PO-MAPF) Bottleneck task and POGEMA benchmark, SRMT outperforms reinforcement learning baselines, particularly under sparse rewards, and generalizes to longer corridors.\n- In the Bottleneck task, SRMT demonstrates superior Cooperative Success Rate (CSR), Individual Success Rate (ISR), and Sum-of-Costs (SoC) compared to other methods, especially in challenging scenarios.\n- On POGEMA, SRMT is competitive with state-of-the-art MARL, hybrid, and planning-based algorithms, showing its potential as a robust solution for decentralized multi-agent pathfinding.",
        "classification": [
            "Reinforcement Learning",
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/Aloriosa/srmt"
        ],
        "huggingface_urls": [
            "https://huggingface.co/docs/transformers/model_doc/gpt2"
        ],
        "date": "2025-01-24"
    },
    {
        "title": "Improving Video Generation with Human Feedback",
        "authors": "Ziyang Yuan, Jiajun Liang, Gongye Liu, Xintao, jieliu",
        "link": "https://arxiv.org/abs/2501.13918",
        "github_repo": null,
        "summary": "- This paper introduces a new framework for aligning text-to-video (T2V) generation models with human preferences using reinforcement learning from human feedback (RLHF).\n- A new 182k-example multi-dimensional human preference dataset focused on modern video generation models is constructed, along with VideoReward, a multi-dimensional video reward model.\n- Three new alignment algorithms for flow-based video generation models are derived: Flow-DPO, Flow-RWR (training-time algorithms), and Flow-NRG (inference-time algorithm). \n- Flow-DPO outperforms standard supervised fine-tuning and Flow-RWR on automatic and human preference evaluations when the KL-divergence parameter \\(\\beta\\) is fixed. \n- Flow-NRG enables personalized video generation by allowing users to adjust weights for multiple alignment objectives during inference.",
        "classification": [
            "Text-to-Video",
            "Reinforcement Learning",
            "Multimodal"
        ],
        "github_urls": [
            "https://gongyeliu.github.io/videoalign"
        ],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models",
        "authors": "hanglics, yegong, lx865712528, tzh94588, Lin0",
        "link": "https://arxiv.org/abs/2501.13629",
        "github_repo": null,
        "summary": "- SIGMA, a new large language model specializing in the system domain, is introduced, featuring DiffQKV attention for enhanced inference efficiency.\n- DiffQKV differentially optimizes Query, Key, and Value components: using compressed K and V and augmented Q, balancing performance and efficiency.\n- SIGMA is pre-trained on 6 trillion tokens, including 19.5 billion system domain data and 1 trillion synthesized/rewritten data.\n- In general domains, SIGMA's performance is comparable to state-of-the-art models.\n- On AIMICIUS, a new system domain benchmark, SIGMA significantly outperforms existing models, including GPT-4, with up to a 52.5% absolute improvement.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Temporal Preference Optimization for Long-Form Video Understanding",
        "authors": "Zeyu Wang, yeunglevy, yuhuizhang, nicholswang, ruili0",
        "link": "https://arxiv.org/abs/2501.13919",
        "github_repo": null,
        "summary": "- This paper introduces Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video Large MultiModal Models (LMMs).\n- TPO leverages preference learning at two granularities: localized temporal grounding, focusing on specific video segments, and comprehensive temporal grounding, addressing broader temporal dependencies.\n- By curating preference data at these two levels, TPO trains video-LMMs to differentiate between temporally grounded and ungrounded responses, improving their ability to capture nuanced temporal relationships in videos.\n- Experiments on LongVideoBench, MLVU, and Video-MME benchmarks demonstrate significant performance improvements with TPO across two state-of-the-art video-LMMs (LongVA-7B and LLaVA-Video-7B).\n- Notably, LLaVA-Video with TPO achieves state-of-the-art results on Video-MME among 7B models, highlighting the effectiveness of TPO in enhancing long-form video understanding.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "DiffuEraser: A Diffusion Model for Video Inpainting",
        "authors": "Haolan Xue, Liefeng, lyraestar, asLKHFksasak",
        "link": "https://arxiv.org/abs/2501.10018",
        "github_repo": null,
        "summary": "- DiffuEraser, a novel video inpainting model based on stable diffusion, is introduced. \n- It addresses three key challenges: propagation of known pixels, generation of unknown pixels, and temporal consistency. \n- The model incorporates priors for initialization and weak conditioning, mitigating noisy artifacts and hallucinations, and leverages a motion module and expanded temporal receptive fields for enhanced temporal consistency during long-sequence inference. \n- Experimental results demonstrate that DiffuEraser outperforms state-of-the-art techniques in content completeness and temporal consistency. \n- The model achieves this while maintaining acceptable efficiency by using only two steps to generate samples via Phased Consistency Models (PCM).",
        "classification": [
            "Computer Vision",
            "Video Classification",
            "Text-to-Video"
        ],
        "github_urls": [
            "https://github.com/lixiaowen-xw/DiffuEraser.git"
        ],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step",
        "authors": "Renrui Zhang, hsli-cuhk, gaopenghigh, zhizhengzhao, ZiyuG",
        "link": "https://arxiv.org/abs/2501.13926",
        "github_repo": "https://github.com/ZiyuGuo99/Image-Generation-CoT",
        "summary": "- This paper investigates the application of Chain-of-Thought (CoT) reasoning to enhance autoregressive image generation, focusing on test-time verification and preference alignment.\n- The authors introduce two specialized reward models, Potential Assessment Reward Model (PARM) and PARM++, designed for autoregressive image generation. PARM performs adaptive step-wise potential assessment, while PARM++ incorporates a reflection mechanism for self-correction of generated images.\n- Using these strategies, the baseline model Show-o achieves a significant +24% improvement on the GenEval benchmark, outperforming Stable Diffusion 3 by +15%.\n- The research demonstrates the effectiveness of integrating CoT reasoning with autoregressive image generation through tailored reward models and strategic alignment techniques.\n- The findings pave a new path for enhancing image generation quality and provide valuable insights for future advancements in the field.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/ZiyuGuo99/Image-Generation-CoT"
        ],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models",
        "authors": "lzyhha, JackyZhuo, RuoyiDu, Afeng-x, jyjyjyjy",
        "link": "https://arxiv.org/abs/2501.13920",
        "github_repo": "https://github.com/jylei16/Imagine-e",
        "summary": "- IMAGINE-E, a comprehensive evaluation framework, is introduced to benchmark text-to-image (T2I) models across five key domains.\n- Six prominent models, including FLUX.1 and Ideogram2.0, were evaluated on tasks related to structured output generation, realism and physical consistency, specific domain generation, challenging scenarios, and multi-style creation.\n- FLUX.1 and Ideogram2.0 show superior performance, especially in structured and specific domain tasks.\n- The evaluation reveals that existing evaluation frameworks need to be improved to better assess these advanced models and that T2I models show progress towards becoming foundational AI tools.\n- The evaluation also highlights ongoing limitations in complex areas like 3D and code generation.",
        "classification": [
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/jylei16/Imagine-e"
        ],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion",
        "authors": "Renjie Chen, Boyuan Liu, Shiyue Yan, Jiangchuan Wei, linwf",
        "link": "https://arxiv.org/abs/2501.13452",
        "github_repo": null,
        "summary": "- EchoVideo is an identity-preserving text-to-video model based on Diffusion Transformers (DiT) that maintains high identity similarity while mitigating \"copy-paste\" artifacts.\n- It leverages an Identity Image-Text Fusion (IITF) module to integrate text semantics, image semantics, and facial identity, effectively capturing clean identity representations and resolving semantic conflicts between modalities. \n- A two-stage training strategy, incorporating a stochastic method in the second phase, reduces over-reliance on shallow facial information while enhancing fidelity. \n-  EchoVideo preserves both facial identity and full-body consistency, including attributes like clothing and hairstyle, controlled solely through text prompts. \n- Experimental results demonstrate superior performance in generating high-quality, controllable, and high-fidelity videos compared to existing methods, addressing challenges like rigid expressions and semantic mismatches.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback",
        "authors": "spermwhale, yunhe, sainbar, jindi, yentinglin",
        "link": "https://arxiv.org/abs/2501.10799",
        "github_repo": null,
        "summary": "- This paper introduces Step-KTO (Stepwise Kahneman-Tversky-inspired Optimization), a novel training framework designed to enhance the mathematical reasoning capabilities of Large Language Models (LLMs).\n- Step-KTO integrates both process-level and outcome-level binary feedback signals to guide LLMs in generating not only correct final answers but also logically sound intermediate reasoning steps.\n- By incorporating a Kahneman-Tversky-inspired value function, Step-KTO prioritizes correctness and coherence in the reasoning process.\n- Experimental results on benchmark mathematical reasoning datasets demonstrate that Step-KTO surpasses existing state-of-the-art methods, achieving a notable improvement in accuracy (e.g., 63.2% Pass@1 on MATH-500 vs. 53.4% for the baseline model) alongside producing more reliable intermediate solutions.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Debate Helps Weak-to-Strong Generalization",
        "authors": "Yongbin-Li, hzhwcmhf, langnick",
        "link": "https://arxiv.org/abs/2501.13124",
        "github_repo": null,
        "summary": "- This paper proposes a novel approach to improve weak-to-strong generalization in natural language processing by leveraging debate between two large language models (LLMs).\n- The debate mechanism extracts trustworthy information from the LLMs, which is then used to train a better weak supervisor.\n- An ensemble of weak models is employed to process the long arguments generated during the debate, leading to more robust supervision estimates.\n- The proposed combination of scalable oversight and weak-to-strong generalization approaches results in improved alignment on OpenAI's weak-to-strong NLP benchmarks.\n- Experimental results show that the debate-enhanced weak supervision significantly outperforms baseline approaches in terms of performance gap recovered (PGR) and test accuracy on various question-answering datasets, including SciQ, BoolQ, CosmosQA, and AnthropicHH.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Evolution and The Knightian Blindspot of Machine Learning",
        "authors": "Tarin Ziyaee, Kenneth O. Stanley, Tarek El-Gaaly, ekmeyerson, jal278",
        "link": "https://arxiv.org/abs/2501.13075",
        "github_repo": null,
        "summary": "- This paper argues that machine learning (ML), and reinforcement learning (RL) in particular, overlooks the critical aspect of robustness to Knightian uncertainty (KU), or unknown unknowns, which is essential for general intelligence in open worlds.\n- By contrasting RL with biological evolution, the authors highlight how evolution's mechanisms, such as diversification, adaptation to novelty, and persistence as a filter for robustness, enable it to thrive in open-ended, unpredictable environments, unlike current RL agents, which struggle with out-of-distribution scenarios.\n- The paper identifies specific limitations in RL's core formalisms, including closed-world assumptions in MDPs, fixed time horizons in reward functions, episodic boundaries, and the treatment of training data as timeless, arguing that these limitations contribute to RL's blindness to KU. \n- The authors suggest that incorporating principles from evolution, such as open-endedness, artificial life, and revisiting core RL formalisms, might help address KU and lead to more robust AI.\n- The implications of KU for foundation models, RLHF, AI safety, and potential pathways to integrate KU into RL algorithms are discussed.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos",
        "authors": "ZhangYuanhan, wangxiao1208, pufanyi, craigwu, KairuiHu",
        "link": "https://arxiv.org/abs/2501.13826",
        "github_repo": null,
        "summary": "- Introduces Video-MMMU, a benchmark designed to evaluate large multimodal models' (LMMs) ability to acquire and apply knowledge from professional educational videos.\n- The benchmark includes 300 videos spanning six disciplines, each accompanied by three question-answer pairs aligned with Bloom\u2019s Taxonomy: Perception, Comprehension, and Adaptation.\n- A new metric, called *\u0394knowledge*, is proposed to quantify the models' performance improvement on practice exam questions after viewing a video.\n- Evaluation results of several LMMs revealed a progressive decline in model performance as the cognitive level increases from perception to comprehension to adaptation.\n- Analysis reveals that even top-performing models like Claude-3.5-Sonnet exhibit significant performance decline in complex scenarios, highlighting the limitations of current models in adapting video-based knowledge to solve real-world problems.",
        "classification": [
            "Multimodal",
            "Question Answering",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "GSTAR: Gaussian Surface Tracking and Reconstruction",
        "authors": "Jie Song, Juan Zarate, Chengwei Zheng, lxxue",
        "link": "https://arxiv.org/abs/2501.10283",
        "github_repo": null,
        "summary": "- GSTAR, a novel method for dynamic surface reconstruction and tracking from multi-view captures, is introduced, addressing topology changes like surface emergence, disappearance, and splitting.\n- It combines meshes with Gaussians, tracking consistent topology surfaces via mesh tracking and generating new surfaces from unbound Gaussians in topology-changing areas.\n- A surface-based scene flow method using 2D optical flow and depth enhances frame tracking initialization, handling significant deformations.\n- Evaluations show GSTAR outperforms state-of-the-art in appearance and geometry metrics, demonstrated through novel view synthesis and mesh comparisons, respectively.\n- It excels in tracking accuracy, even with topology changes, based on experiments using AprilTags attached to a moving human.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://eth-ait.github.io/GSTAR/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-24"
    },
    {
        "title": "Humanity's Last Exam",
        "authors": "Josephina Hu, Nathaniel Li, Ziwen Han, Alice Gatti, Long Phan",
        "link": "https://arxiv.org/abs/2501.14249",
        "github_repo": null,
        "summary": "- This paper introduces HUMANITY'S LAST EXAM (HLE), a challenging multi-modal benchmark designed to assess the advanced academic capabilities of large language models (LLMs).\n- HLE comprises 3,000 multi-modal, multiple-choice, and exact-match questions across various subjects, emphasizing complex mathematics problems, and aims to be the final closed-ended academic benchmark of its kind.\n- The benchmark creation involved a rigorous process of expert contribution, LLM difficulty checks, and a multi-stage review process to ensure high quality and difficulty.\n- Initial evaluations demonstrate that state-of-the-art LLMs perform poorly on HLE and exhibit poor calibration, indicating significant room for improvement.\n- HLE's public release aims to provide a robust tool for researchers and policymakers to evaluate AI progress and inform discussions about AI development and governance.",
        "classification": [
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-27"
    },
    {
        "title": "Redundancy Principles for MLLMs Benchmarks",
        "authors": "Chunyi Li, Xiangyu Zhao, Zicheng Zhang, KennyUTC, nebulae09",
        "link": "https://arxiv.org/abs/2501.13953",
        "github_repo": null,
        "summary": "- This paper introduces a framework for evaluating redundancy in Multimodal Large Language Model (MLLM) benchmarks, addressing the issue of numerous benchmarks with overlapping capabilities.\n- The framework quantifies redundancy across three key perspectives: dimensions (intra-benchmark), instances (intra-benchmark), and cross-benchmarks within specific domains.\n- It uses performance correlation to measure redundancy, leveraging data from VLMEvalKit, a comprehensive dataset containing results from diverse benchmarks and over 100 MLLMs.\n- The study reveals significant redundancy in many existing benchmarks, suggesting opportunities for optimization by reducing redundant dimensions and instances.\n- The paper concludes with recommendations for constructing more efficient and effective MLLM benchmarks, including guidance on redundancy checks during the benchmark design process.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/VLMEval/OpenVLMRecords"
        ],
        "date": "2025-01-27"
    },
    {
        "title": "Chain-of-Retrieval Augmented Generation",
        "authors": "Zhicheng Dou, Xiaolong Huang, Nan Yang, Haonan Chen, Liang Wang",
        "link": "https://arxiv.org/abs/2501.14342",
        "github_repo": null,
        "summary": "- This paper introduces CoRAG (Chain-of-Retrieval Augmented Generation), a novel approach for training RAG models that retrieve and reason over information step-by-step before generating answers.\n- Unlike conventional RAG methods that perform a single retrieval step, CoRAG allows dynamic query reformulation based on the model's evolving state, enhancing its effectiveness in handling complex queries.\n- CoRAG is trained effectively using rejection sampling to generate intermediate retrieval chains, augmenting existing RAG datasets and improving the model's ability to learn effective retrieval strategies.\n- Experiments across multiple benchmarks show CoRAG outperforming strong baselines, particularly in multi-hop question answering tasks, where it achieves more than 10 points improvement in EM score.\n- CoRAG establishes a new state-of-the-art performance on the KILT benchmark across various knowledge-intensive tasks, demonstrating its superior ability to handle diverse knowledge-intensive tasks.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-27"
    },
    {
        "title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques",
        "authors": "Ruoyu Sun, Tian Ding, Zhenyang Xiao, Ziniu Li, Zhengyang Tang",
        "link": "https://arxiv.org/abs/2501.14492",
        "github_repo": "https://github.com/tangzhy/RealCritic",
        "summary": "- The paper introduces RealCritic, a novel benchmark designed to assess the critique capabilities of large language models (LLMs) in a closed-loop manner.\n- Unlike existing benchmarks that evaluate critiques in isolation, RealCritic evaluates critique quality based on the effectiveness of the generated corrections.\n- RealCritic incorporates various critique settings, including self-critique, cross-critique, and iterative critique, providing a comprehensive evaluation.\n- The benchmark is implemented using eight challenging reasoning tasks across mathematical reasoning and multiple-choice question domains.\n- Experiments show that reasoning-based models outperform classical LLMs, particularly in self-critique settings, emphasizing the value of a closed-loop evaluation approach.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/tangzhy/RealCritic"
        ],
        "huggingface_urls": [],
        "date": "2025-01-27"
    },
    {
        "title": "Relightable Full-Body Gaussian Codec Avatars",
        "authors": "Timur Bagautdinov, Igor Santesteban, Tomas Simon, Shaofei Wang, psyth",
        "link": "https://arxiv.org/abs/2501.14726",
        "github_repo": null,
        "summary": "- This paper introduces Relightable Full-Body Gaussian Codec Avatars, a novel method for creating and animating high-fidelity, relightable 3D human avatars from light stage data.\n- The model uses 3D Gaussian Splatting (3DGS) for geometry and appearance representation, combined with a learned radiance transfer function that leverages zonal harmonics for diffuse lighting and deferred shading for specular highlights.\n- A dedicated shadow network predicts non-local shadows caused by body articulation, improving realism.\n- The method outperforms a physically-based rendering (PBR) baseline and ablation studies demonstrate the importance of zonal harmonics, shadow networks, and deferred shading.\n- This allows for efficient, high-quality rendering of human avatars under various lighting and poses.",
        "classification": [
            "Computer Vision",
            "Text-to-3D",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-27"
    }
]