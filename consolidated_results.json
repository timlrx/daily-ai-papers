[
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": " - This research introduces MaskLLM, a novel learnable pruning method that generates semi-structured (N:M) sparsity in Large Language Models (LLMs) for enhanced inference efficiency.\n- MaskLLM distinguishes itself from previous methods by directly learning the distribution of N:M sparsity patterns using Gumbel Softmax sampling, enabling end-to-end training on large datasets and addressing limitations of hand-crafted importance criteria.\n-  Empirical evaluations on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, demonstrate that MaskLLM surpasses state-of-the-art techniques, achieving a perplexity of 6.72 on LLaMA2-7B compared to SparseGPT's 10.42.\n- The research underscores the efficacy of learning sparsity patterns directly from data, leading to more accurate and efficient compression of LLMs without compromising performance.\n- The adaptability of MaskLLM to downstream tasks and its ability to achieve lossless compression in certain scenarios highlight its potential for practical applications.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "This research proposes LLaVA-3D, a novel framework for building 3D-aware Large Multimodal Models (LMMs) by adapting the existing 2D LLaVA model.  LLaVA-3D introduces the concept of \"3D Patches,\" which inject 3D positional embeddings into 2D image features, enhancing the model's spatial understanding without complex 3D processing pipelines.  Evaluations demonstrate LLaVA-3D's state-of-the-art performance on various 3D tasks, including question answering, dense captioning, and visual grounding, surpassing existing 3D LMMs while maintaining comparable 2D image understanding capabilities to its 2D counterpart. The research highlights the advantages of leveraging pre-trained 2D LMMs for 3D scene understanding and the benefits of integrating 3D spatial information into 2D visual features.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Image-to-Text",
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "This paper introduces EMOVA, a novel end-to-end multimodal large language model capable of perceiving and generating images, text, and speech with emotional expressiveness. EMOVA utilizes a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for end-to-end speech processing.  The model achieves state-of-the-art performance on both vision-language and speech benchmarks, outperforming models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks and surpassing the speech LLM Mini-Omni in ASR tasks. EMOVA also enables emotional spoken dialogue by explicitly predicting speech style labels (emotions and pitches) and leveraging a lightweight style module for controllable speech synthesis. This is achieved through a novel text-centric multimodal alignment approach, which leverages publicly available bimodal data and eliminates the reliance on scarce trimodal data.",
        "classification": [
            "Multimodal",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Image-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": "\u2022 This paper introduces Lotus, a novel diffusion-based visual foundation model designed for high-quality dense prediction tasks, specifically focusing on depth and surface normal estimation.\n\u2022 Lotus leverages the rich visual priors acquired from pre-trained text-to-image diffusion models, enabling strong zero-shot generalization capabilities.\n\u2022 The authors systematically analyze the diffusion formulation and identify that the standard parameterization type and multi-step diffusion process are not optimal for dense prediction, proposing a more effective adaptation protocol.\n\u2022 Through extensive experiments, Lotus demonstrates state-of-the-art performance on various benchmark datasets for zero-shot depth and normal estimation with significantly less training data compared to previous methods. \n\u2022 Notably, Lotus achieves superior efficiency due to its single-step diffusion formulation, making it hundreds of times faster than many existing diffusion-based methods.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": "This research paper introduces GemFilter, a novel approach to reduce the computational cost and latency of processing long context inputs with Large Language Models (LLMs). GemFilter leverages the ability of early LLM layers to identify relevant tokens and compresses the input sequence by a factor of 1000x for subsequent processing by the full model. Empirical evaluations show that GemFilter achieves a 2.4x speedup and 30% reduction in GPU memory consumption compared to state-of-the-art methods, while maintaining comparable performance on benchmarks like LongBench and outperforming them on the Needle in a Haystack task. GemFilter is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting the selected input sequence.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": " - This paper proposes a novel post-training objective function for Latent Diffusion Models (LDMs) by incorporating a pixel-space loss alongside the standard latent-space loss during fine-tuning.  \n- This method enhances the generation of high-frequency details and reduces visual flaws in generated images.\n- Human evaluation demonstrates that this approach significantly improves both supervised fine-tuning and preference-based post-training on DiT and U-Net based LDMs. \n- For example, it boosts visual appeal win rate by 12.9% in DiT and 5.9% in Emu during preference-based fine-tuning,  and achieves a 32.8% vs 9.3% win rate on visual flaws with supervised fine-tuning on DiT. \n- The paper also shows the benefit of using SimPO over DPO for reward modeling in diffusion models.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": " - This research paper investigates implicit instruction tuning, demonstrating that instruction following can emerge without explicit instruction-response training. \n- The authors show that training solely on responses (response tuning) and on narrow-domain data (single-task finetuning) leads to broad instruction-following abilities in language models. \n- For instance, response-tuned models achieve a 43% win rate against explicitly instruction-tuned models in head-to-head evaluations. \n- Furthermore, they introduce a simple rule-based language model that, when combined with a pretrained model, exhibits instruction-following behavior. \n- These findings highlight that adaptation methods not explicitly designed for instruction following can implicitly induce such capabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This research paper introduces a novel technique named \"TOKEN POOLING\" for enhancing the efficiency of multi-vector retrieval models, especially focusing on ColBERT, without significantly affecting performance.  The method uses clustering techniques to group together similar token representations and then applies mean pooling to create a single, representative vector, effectively reducing the overall storage footprint. Experiments show that this approach reduces the required vector count by 50% without compromising accuracy, and a 66% reduction still yields strong performance. The paper also demonstrates that TOKEN POOLING can be effectively combined with existing quantization methods, leading to even more significant compression rates while maintaining reasonable retrieval performance. ",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": " - This paper introduces Disco4D, a novel Gaussian Splatting framework that generates animatable 3D clothed human avatars from a single image. \n- Disco4D disentangles clothing from the human body, representing the former with Gaussian models and the latter with the SMPL-X model, enhancing generation detail and flexibility.\n- Disco4D utilizes diffusion models to refine textures during 3D generation and extrapolate unseen views during 4D animation, surpassing existing methods in fidelity and view consistency.\n- Disco4D enables fine-grained editing and animation of generated avatars, including direct manipulation of clothing Gaussians and pose-driven animation.\n- The authors highlight Disco4D's limitations, such as reliance on robust SMPL-X estimations and limitations in multi-layered clothing modeling, pointing to future research directions.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": " - This paper presents a comprehensive review of the emerging field of Conversation Analysis (CA), a process designed to extract critical information from conversational data and leverage it for system optimization and decision-making.  - The paper systematically defines CA as a four-step procedure: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, and discusses the challenges and trends within each step. - The paper further argues that while previous CA efforts focused on atomic tasks with limited business impact, the rise of Large Language Models (LLMs) enables deeper, more insightful analysis and strategic decision-making from conversations.  - The authors compile and categorize existing benchmark datasets for CA but highlight a significant gap in comprehensive benchmarks containing fine-grained conversation elements and long-context modeling capabilities.  - The paper concludes by outlining future research directions, including the development of LLM-based conversation simulators, fine-grained CA benchmarks, long-context conversation modeling, in-depth attribution analysis, and advanced goal-directed optimization and evaluation methods.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging Knowledge Graphs (KGs) and graph-based architectures. The authors demonstrate the effectiveness of their framework by applying it to the SoccerNet dataset, a large dataset of soccer videos. Their findings show that Structured-GraphRAG significantly improves query processing efficiency, reduces response times, and enhances accuracy compared to traditional RAG methods. The structured nature of KGs reduces hallucinations in LLMs, making the responses more consistent and reliable. The authors highlight that their framework can be applied to a broad range of applications due to its flexible design.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": " - This paper introduces Robot See Robot Do (RSRD), a novel approach for enabling robots to imitate articulated object manipulation from a single monocular RGB human demonstration. \n- Given a static multi-view object scan and a monocular human interaction video, RSRD reconstructs the 3D motion trajectories of object parts using a method called 4D Differentiable Part Models (4D-DPM). \n- RSRD achieves an average success rate of 87% for each phase (object pose registration, trajectory planning, grasping, and motion execution) and an end-to-end success rate of 60% across 90 trials involving 9 different objects. \n- The method leverages DINO feature fields and an analysis-by-synthesis paradigm for robust 3D motion recovery from monocular video. \n- RSRD demonstrates the potential of object-centric learning from human demonstrations for enabling robots to acquire new manipulation skills in a user-friendly and efficient manner.",
        "classification": [
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-27"
    },
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": " - This paper introduces MaskLLM, a novel learnable pruning method designed to induce Semi-structured (N:M) Sparsity in Large Language Models (LLMs), thereby reducing computational overhead during inference.\n - Unlike conventional one-shot pruning techniques, MaskLLM models N:M patterns as a learnable distribution using Gumbel Softmax sampling, facilitating end-to-end training on large-scale datasets and enabling the learning of accurate masks.\n -  Evaluations on various LLMs (LLaMA-2, Nemotron-4, GPT-3) with 2:4 sparsity demonstrate MaskLLM's superiority over existing methods, achieving a significantly lower perplexity of 6.72 on Wikitext compared to 10.42 achieved by state-of-the-art techniques.\n -  MaskLLM supports the transfer learning of sparsity across domains or tasks, enabling the generation of customized masks for specific downstream applications and achieving lossless compression in certain cases.\n -  Through this learnable approach, MaskLLM effectively addresses the limitations of traditional pruning methods, such as the reliance on small calibration sets and the use of inaccurate importance criteria.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "This paper introduces EMOVA, a novel end-to-end multimodal Large Language Model (LLM) capable of processing visual, textual, and audio data. EMOVA utilizes a continuous vision encoder and a discrete semantic-acoustic disentangled speech tokenizer for seamless multimodal alignment and diverse speech style control. The paper demonstrates that publicly available image-text and speech-text datasets are sufficient for training EMOVA, achieving state-of-the-art results on vision-language and speech benchmarks, including surpassing proprietary models like GPT-4V and Gemini Pro 1.5 on 10 out of 14 vision-language tasks. Additionally, EMOVA outperforms the most recent multimodal model VITA on both visual-language and speech tasks, demonstrating the effectiveness of the proposed architecture and training approach.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Any-to-Any"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "This research introduces LLaVA-3D, a novel framework that extends the capabilities of existing 2D large multimodal models (LMMs) to handle 3D scene understanding tasks.  LLaVA-3D leverages 3D patches, integrating 2D visual features with 3D positional embeddings, to effectively capture 3D spatial information within a 2D LMM architecture.  Experimental results demonstrate that LLaVA-3D significantly outperforms existing approaches on various 3D benchmarks, including 3D question answering, 3D dense captioning, and 3D visual grounding, showcasing its superiority in 3D scene understanding. Notably, LLaVA-3D achieves state-of-the-art performance on these benchmarks while maintaining comparable capabilities to its 2D counterpart in 2D image understanding and reasoning tasks.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Visual Question Answering",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": "This research paper introduces GemFilter, a novel approach designed to accelerate inference and reduce memory consumption in large language models (LLMs) dealing with long context inputs.  GemFilter leverages the observation that LLMs identify crucial information in early layers by utilizing these layers as filters to select and compress input tokens, thereby reducing the context length for subsequent processing. The paper provides evidence of GemFilter's efficacy by demonstrating a 2.4x speed improvement and a 30% reduction in GPU memory usage compared to state-of-the-art methods. Additionally, GemFilter exhibits superior performance on the Needle in a Haystack benchmark, showcasing its capability to efficiently process lengthy input sequences. The paper emphasizes that GemFilter is straightforward, doesn't require training, and can be applied to various LLMs. Finally, GemFilter enhances interpretability by enabling the examination of the selected input sequence.",
        "classification": [
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": " - This paper introduces Lotus, a novel diffusion-based visual foundation model specifically designed for high-quality dense prediction tasks, including depth and normal estimation.\n - By employing a novel adaptation protocol that includes direct annotation prediction, a single-step diffusion formulation, and a detail preserver, Lotus effectively leverages visual priors from pre-trained text-to-image diffusion models for enhanced zero-shot generalization.\n - Lotus outperforms existing methods in zero-shot depth and normal estimation benchmarks, achieving state-of-the-art results with minimal training data, as evidenced by its superior performance on datasets such as NYUv2, KITTI, ETH3D for depth and NYUv2, ScanNet, iBims-1, and Sintel for normal estimation.\n - Moreover, Lotus exhibits significant efficiency improvements compared to previous diffusion-based methods, being hundreds of times faster for inference, as demonstrated in a comparison of inference times at different image resolutions.\n - The model's adaptability and performance make it suitable for various downstream applications like 3D reconstruction and scene understanding.",
        "classification": [
            "Computer Vision",
            "Depth Estimation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": "- This paper introduces a novel post-training method for Latent Diffusion Models (LDMs) that enhances image generation quality by incorporating a pixel-space loss function alongside the conventional latent-space loss.\n- The authors demonstrate that this approach significantly improves both visual appeal and reduces visual flaws, as evidenced by a 32.8% vs 9.3% win rate in visual flaws and a 34.8% vs 16.6% win rate in visual appeal during supervised fine-tuning of a DiT model.\n-  Furthermore, the method proves effective for both supervised fine-tuning and preference-based post-training, showing consistent improvements across different LDM architectures like DiT and U-Net.\n- The paper also explores variations in reward modeling during preference-based fine-tuning, finding that combining SimPO with the proposed pixel-space loss yields the most substantial enhancements.\n- The authors highlight the simplicity and general applicability of their method, allowing for seamless integration into existing LDM pipelines and potentially benefiting future post-training techniques.",
        "classification": [
            "Computer Vision",
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": "This research paper explores alternative training methods for language models to exhibit instruction-following behavior without explicit instruction tuning. - The authors demonstrate that \"response tuning,\" which involves training solely on the responses without corresponding instructions, can lead to instruction following, suggesting an implicit instruction-response mapping learned during pretraining. - Additionally, the study reveals that \"single-task finetuning,\"  training on narrow-domain data like poetry generation, yields broad instruction-following capabilities, indicating that models learn more than just the specific task. -  The paper provides evidence that a simple 3-rule rule-based adapter can achieve comparable performance to instruction-tuned models, highlighting the potential for simplified approaches to instruction following. - These findings suggest that instruction following might be a more fundamental property of language models acquired through various adaptation methods, even those not explicitly designed for this purpose.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This paper introduces TOKEN POOLING, a novel technique for reducing storage requirements in multi-vector retrieval models like ColBERT by employing clustering methods to merge similar token representations. Experiments demonstrate that reducing the vector count by 50% results in negligible performance degradation and even a 66% reduction maintains minimal degradation across most datasets, significantly shrinking ColBERT index sizes.  This method is compatible with ColBERT's quantization process, enabling even greater compression, and exhibits similar positive results when applied to a Japanese ColBERT model, indicating its generalizability.  The paper encourages further research into understanding the significance of individual tokens in multi-vector retrieval to develop enhanced compression methods.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": " - This paper introduces Disco4D, a novel framework that generates and animates 3D clothed human avatars from a single image using disentangled Gaussian representations.\n - Disco4D utilizes SMPL-X for body representation and separate Gaussian models for clothing and accessories, leading to improved detail and flexibility compared to existing methods that merge body and clothing into a single mesh. \n - The paper demonstrates Disco4D's superior performance in fidelity and geometry through quantitative comparisons on standard benchmarks (Synbody and CloSe), outperforming competing methods like DreamGaussian, LGM, and SHERF.\n -  Beyond static 3D generation, Disco4D enables animation through SMPL-X pose sequences and learns clothing dynamics from monocular videos, achieving more realistic and nuanced clothing movements.\n - Disco4D's disentanglement approach allows for fine-grained, localized editing of individual clothing items, enabling color changes, object removal, and swapping clothing items without impacting other parts of the avatar.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": "\u2022 This survey paper provides the first technical overview of Conversation Analysis (CA), analyzing existing research and techniques related to the field.\n\u2022 The paper segments the field of CA into four key components: Scene Reconstruction, Causality Analysis, Skill Enhancement, and Conversation Generation, each playing a crucial role in achieving specific goals within CA.\n\u2022 The authors highlight the significant gap between current research, which focuses on relatively shallow aspects of conversation analysis, and the genuine needs of businesses.\n\u2022 The paper provides a comprehensive overview of existing benchmarks and metrics used in CA, categorizing them based on task and technical approach.\n\u2022 The authors conclude by outlining potential future directions for CA research, emphasizing the need for more sophisticated and in-depth analysis, particularly in light of the capabilities of Large Language Models (LLMs).",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "This paper introduces Structured-GraphRAG, a novel framework designed to enhance data retrieval from structured datasets by leveraging knowledge graphs (KGs) and graph-based architectures. Structured-GraphRAG enhances the accuracy and efficiency of answering natural language queries related to large datasets by converting them into KG queries. Experimental results using the SoccerNet dataset show that compared to a baseline method, Structured-GraphRAG improves accuracy from 36% to 64% and demonstrates significantly faster query processing and reduced response times. The framework's design is generic and can be applied to other structured datasets, making it a valuable tool for various applications.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": "- This paper presents Robot See Robot Do (RSRD), a novel approach to teach robots how to manipulate articulated objects with moving parts through a single monocular human demonstration. \n- RSRD constructs a 4D Differentiable Part Model (4D-DPM) from a multi-view static object scan and uses it to track object part motion from the monocular video. \n- RSRD outperforms photometric-based tracking approaches, achieving a mean average point distance of 7.5mm on tracking manipulated object part poses. \n- The robot then leverages the recovered part motions to plan its own motion to achieve the same object configuration change, demonstrating a 60% end-to-end success rate across various objects and re-orientations. \n- The proposed method is particularly notable for its zero-shot capability, requiring no task-specific training data or annotations.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-28"
    },
    {
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "authors": "wxcTest, gheinrich, srvm, yinhongxu, Vinnnf",
        "link": "https://arxiv.org/abs/2409.17481",
        "github_repo": "https://github.com/NVlabs/MaskLLM",
        "summary": "\n- MaskLLM, a new learnable pruning method, introduces semi-structured (N:M) sparsity to Large Language Models (LLMs) to reduce computational overhead during inference.\n- Unlike traditional methods that rely on importance criteria, MaskLLM learns N:M patterns as a distribution, using Gumbel Softmax for differentiable sampling, and training these distributions end-to-end.\n- Evaluation on LLMs such as LLaMA-2, Nemotron-4, and GPT-3 shows MaskLLM achieves better perplexity than existing techniques. For example, on Wikitext, MaskLLM achieves a 6.72 perplexity with frozen weights compared to 10 or higher from state-of-the-art methods and 5.12 PPL with dense models.\n- MaskLLM's learnable masks enable transfer learning of sparsity across domains or tasks and can even be customized for lossless application of sparsity for specific downstream tasks.\n- The method successfully scales to large datasets, enabling effective mask learning while leveraging the vast knowledge embedded in LLMs.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/NVlabs/MaskLLM"
        ],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Wenwei Zhang, XihuiLiu, Jiangmiao, taiwang, ChaimZhu",
        "link": "https://arxiv.org/abs/2409.18125",
        "github_repo": null,
        "summary": "-\nLLaVA-3D, a novel framework built upon the 2D large multimodal model (LMM) LLaVA, empowers LMMs with 3D spatial understanding by introducing 3D Patches, integrating 2D patch features with 3D positional embeddings.\n- This model achieves state-of-the-art performance on various 3D tasks, including 3D question answering, captioning, and visual grounding, as demonstrated by its superior results on ScanQA, SQA3D, MMScan QA, Scan2Cap, and ScanRefer benchmarks.\n- LLaVA-3D converges 3.5 times faster than other existing 3D LMMs and maintains strong 2D capabilities by employing joint instruction tuning on 2D and 3D vision-language datasets.\n- The model utilizes efficient 3D pooling strategies like voxelization and farthest point sampling to handle multiple input views effectively, and introduces a novel 2D click-based interaction for 3D understanding and reasoning tasks.\n- Experimental analysis demonstrates the efficacy of 3D patches, the advantage of using pre-trained 2D LMMs, and the impact of different components, such as pooling strategies and multi-view image sampling.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Visual Question Answering",
            "Image-to-Text",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "vikyzeng2, 17day, zhili-liu, gyhdog, KaiChen1998",
        "link": "https://arxiv.org/abs/2409.18042",
        "github_repo": null,
        "summary": "- EMOVA, an end-to-end omni-modal Large Language Model (LLM), is introduced, integrating vision, speech, and text modalities with emotional spoken dialogue capabilities.\n- It leverages a continuous vision encoder for detailed visual understanding and a semantic-acoustic disentangled speech tokenizer/detokenizer for speech processing and emotional control.\n- The model employs a text-centric omni-modal alignment strategy, using text as a bridge to connect different modalities, thus eliminating the need for scarce omni-modal data.\n- EMOVA achieves state-of-the-art performance on both vision-language and speech benchmarks, surpassing existing open-source and some proprietary models.\n- A lightweight style module is incorporated, enabling control over speech styles like emotions and pitches, adding vividness to spoken dialogue.",
        "classification": [
            "Multimodal",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Text-to-Audio",
            "Audio-to-Audio",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Leheng Li, Yixun Liang, Wei Yin, Jing He, haodongli",
        "link": "https://arxiv.org/abs/2409.18124",
        "github_repo": null,
        "summary": " - Lotus, a diffusion-based visual foundation model, is introduced for high-quality dense prediction, specializing in depth and normal map estimation.\n- It utilizes a novel single-step diffusion process with direct annotation prediction for improved performance and a detail preserver to enhance predictions in intricate areas.\n- Evaluation on standard datasets like NYUv2, KITTI, ETH3D, ScanNet, iBims-1, and Sintel shows that Lotus achieves state-of-the-art zero-shot results, outperforming competitors trained on much larger datasets, especially Marigold trained with 74K vs 59K images used for training Lotus, achieving an avg. rank between 1.0 - 7.0 vs 1.5 - 2.5 on Lotus across all datasets and metrics.\n- Lotus offers significant efficiency gains, being hundreds of times faster than existing diffusion-based methods.\n- The efficiency and quality enable various applications like joint estimation and 3D reconstruction.",
        "classification": [
            "Computer Vision",
            "Depth Estimation",
            "Image-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "authors": "Shafiq Joty, Yingyu Liang, Xuan-Phi Nguyen, Zhenmei Shi, alvinming",
        "link": "https://arxiv.org/abs/2409.17422",
        "github_repo": "https://github.com/SalesforceAIResearch/GemFilter",
        "summary": " - This research introduces GemFilter, a novel algorithm to accelerate Large Language Model (LLM) inference and reduce GPU memory consumption for long context inputs. \n- It leverages the observation that LLMs identify crucial information in early layers by using those layers as filters to select relevant input tokens before full model inference. \n- This approach achieves a 2.4x speedup and 30% reduction in GPU memory usage compared to state-of-the-art methods like SnapKV. \n- Evaluation on Needle in a Haystack and LongBench benchmarks demonstrates GemFilter\u2019s superior performance in information retrieval tasks with long contexts and effectiveness similar to SnapKV and H2O. \n- Moreover, the algorithm is simple, training-free, applicable across diverse LLMs, and offers enhanced interpretability.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [
            "https://github.com/SalesforceAIResearch/GemFilter"
        ],
        "huggingface_urls": [
            "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
            "https://huggingface.co/mistralai/Mistral-Nemo-Base-2407",
            "https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
        ],
        "date": "2024-09-29"
    },
    {
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "authors": "Felix Juefei-Xu, Ji Hou, Matthew Yu, Simran Motwani, Christina Zhang",
        "link": "https://arxiv.org/abs/2409.17565",
        "github_repo": null,
        "summary": " - This research proposes a novel pixel-space post-training method for Latent Diffusion Models (LDMs) to enhance the generation of high-frequency details and complex compositions, which are often imperfect in LDMs.\n- This method addresses the limitations of latent space training by adding pixel-space supervision during post-training, thereby preserving details lost in the compression of the latent space. \n- Human evaluations on a DiT transformer model demonstrate a significant improvement of 18.2% in visual appeal and 23.5% in reduction of visual flaws with supervised fine-tuning, and 17.8% and 11.3% with preference-based fine-tuning using this method compared to a latent-space baseline. \n- This improvement is also validated on U-Net diffusion models, showing a 32.8% improvement on visual flaws with the same fine-tuning dataset. \n- This simple method can be easily integrated into any existing LDM, offering advancements in both supervised and preference-based post-training.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "authors": "Christopher D. Manning, Percy Liang, Nelson F. Liu, John Hewitt",
        "link": "https://arxiv.org/abs/2409.14254",
        "github_repo": null,
        "summary": "\n- This paper introduces the concept of implicit instruction tuning, where language models exhibit instruction-following behavior through training methods not explicitly designed for this purpose. \n- Two forms of implicit instruction tuning are explored: response tuning (training only on responses without corresponding instructions), and single-task fine-tuning (training on narrow-domain data). \n- Experiments show that response-tuned models achieve competitive win rates against instruction-tuned models in AlpacaEval, suggesting a pre-existing instruction-response mapping within pretrained models. \n- Single-task fine-tuning on diverse datasets also yields general instruction-following behavior, demonstrating that learning the distribution of desirable responses can generalize beyond the narrow training domain. \n- A rule-based language model with three simple rules is introduced, which, when combined with a pretrained model, exhibits instruction following, providing evidence for the simplicity of the mapping from pretrained to instruction-following distributions.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/john-hewitt/implicit-ins"
        ],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "authors": "Longze Chen, Minzheng Wang, Yongbin Li, Haiyang Yu, Xinghua Zhang",
        "link": "https://arxiv.org/abs/2409.14195",
        "github_repo": null,
        "summary": " - This paper surveys Conversation Analysis (CA) tasks, techniques, and trends, focusing on extracting actionable insights from conversation data in the Large Language Model (LLM) era.\n- It defines CA as a four-step process: scene reconstruction, causality analysis, skill enhancement, and conversation generation, aimed at continuous goal-directed optimization of conversations.\n- The paper reviews existing CA datasets and metrics, highlighting the lack of comprehensive datasets with detailed scene elements and the gap between shallow analysis results and business needs.\n- It also discusses the shift towards deeper semantic understanding, more flexible task formulations, and first-person interactive simulation modeling with the rise of LLMs.\n-  Finally, it outlines future directions, including LLM conversation simulators, fine-grained benchmarks, long-context modeling, in-depth attribution analysis, goal-directed optimization and evaluation, cross-session KV cache, and conversation security.",
        "classification": [
            "Natural Language Processing",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "authors": "Griffin Adams, Benjamin Clavi\u00e9, NohTow",
        "link": "https://arxiv.org/abs/2409.14683",
        "github_repo": null,
        "summary": "This paper introduces TOKEN POOLING, a method to reduce storage and memory costs for ColBERT multi-vector retrieval method using clustering and average pooling of token representations.\n- Using hierarchical clustering based pooling approach, the method can reduce the vector count by 50% with almost no performance impact on various evaluation datasets.\n- It can achieve even further reduction of vector count by 66% with less than 3% performance degradation.\n- This approach requires no change in architecture and no query-time processing and therefore can be used with any existing ColBERT models.\n- The method is tested on various datasets including BEIR and LoTTe, and with both unquantized and quantized vectors.\n- The result shows that the method consistently reduces storage requirements with minimal impact on performance and can also be used with Japanese ColBERT models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/colbert-ir/colbertv2.0"
        ],
        "date": "2024-09-29"
    },
    {
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "authors": "P\u00e5l Halvorsen, Michael A. Riegler, Cise Midoglu, Sushant Gautam, Zahra Sepasdar",
        "link": "https://arxiv.org/abs/2409.17580",
        "github_repo": null,
        "summary": "\n- This paper introduces Structured-GraphRAG, a framework designed to enhance information retrieval across structured datasets using knowledge graphs (KGs) and retrieval-augmented generation (RAG).\n- It leverages the structured relationships and rich semantics within KGs to improve retrieval accuracy and context awareness.\n- Compared to traditional RAG and direct data analysis methods on a SoccerNet dataset, Structured-GraphRAG shows improvements in both accuracy and query processing time.\n- The framework's design enables the creation of KGs without requiring deep expertise in graph theory and also effectively reduces the occurence of hallucinations in LLMs.\n- While the demonstration focuses on soccer data, the framework is adaptable to other structured data, offering a powerful tool for diverse applications.",
        "classification": [
            "Question Answering",
            "Graph Machine Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Qianqian Wang, Brent Yi, Mingxuan Wu, Chung Min Kim, Justin Kerr",
        "link": "https://arxiv.org/abs/2409.18121",
        "github_repo": null,
        "summary": "-\"Robot See Robot Do (RSRD)\" is introduced, a method for robots to imitate articulated object manipulation from a single monocular RGB human demonstration, given a static multi-view object scan.\n- RSRD uses 4D Differentiable Part Models (4D-DPM) to recover 3D part motion from monocular video using part-centric feature fields and iterative optimization with geometric regularizers.\n- The robot replicates demonstrated object trajectories by planning bimanual arm motions inducing the same part motion, focusing on the intended behavior rather than mimicking human hand motions.\n- RSRD achieves an average 87% success rate in each phase (registration, planning, grasping, execution), resulting in a 60% total end-to-end success rate across 90 trials with 9 objects.\n- This is achieved using feature fields from pre-trained vision models without task-specific training, fine-tuning, data collection, or annotation.",
        "classification": [
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "authors": "Tianwei Zhang, Lei Yang, Zhongang Cai, Shuai Liu, Hui En Pang",
        "link": "https://arxiv.org/abs/2409.17280",
        "github_repo": null,
        "summary": "- Disco4D is a novel Gaussian Splatting framework that generates and animates 4D clothed humans from a single image, outperforming existing methods by disentangling clothing from the human body.\n- Disco4D uses the SMPL-X model for body representation and Gaussian models for clothing, allowing for detailed generation and flexibility.\n- It leverages diffusion models to enhance 3D generation, particularly for occluded parts, and includes an identity encoding for each clothing Gaussian for asset separation.\n- Disco4D supports 4D human animation with vivid dynamics, enabling virtual try-on and avatar customization.\n- User studies confirm that Disco4D generates higher-fidelity outputs and aligns better with original image content compared to competing methods.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-29"
    },
    {
        "title": "MIO: A Foundation Model on Multimodal Tokens",
        "authors": "Jiaheng Liu, Wangchunshu Zhou, Chunpu Xu, King Zhu, Zekun Wang",
        "link": "https://arxiv.org/abs/2409.17692",
        "github_repo": null,
        "summary": " - MIO is a novel any-to-any foundation model, built upon multimodal tokens, that integrates understanding and generation across four modalities: text, image, speech, and video.\n- It supports generating multimodal interleaved sequences and is trained in four stages: alignment pre-training, interleaved pre-training, speech-enhanced pre-training, and supervised fine-tuning.\n- Experimental results show MIO performs competitively against other dual-modal and any-to-any models and surpasses some modality-specific baselines.\n- It boasts advanced any-to-any capabilities, such as interleaved video-text generation and chain-of-visual-thought reasoning.\n- MIO's design addresses limitations of existing multimodal LLMs by handling diverse modalities in a unified framework and enabling more complex multimodal outputs.",
        "classification": [
            "Multimodal",
            "Any-to-Any",
            "Text-to-Image",
            "Image-to-Text",
            "Text-to-Speech",
            "Automatic Speech Recognition",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
        "authors": "Li Lyna Zhang, Shengyu Ye, Jicheng Wen, Yifei Liu, yangwang92",
        "link": "https://arxiv.org/abs/2409.17066",
        "github_repo": null,
        "summary": " - This paper introduces Vector Post-Training Quantization (VPTQ), a novel approach for extremely low-bit quantization of Large Language Models (LLMs) using Vector Quantization.\n- VPTQ leverages second-order optimization to guide the design of its quantization algorithm and employs channel-independent second-order optimization for a granular vector quantization.\n- The authors claim that VPTQ achieves state-of-the-art accuracy on extremely low-bit LLMs, reducing perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, and 4.41-7.34 on LLaMA-3 over existing methods at 2-bit quantization.\n- They also report an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, and 11-22% on LLaMA-3 on question answering tasks.\n- VPTQ offers a lightweight and efficient approach with low quantization overhead, utilizing only 10.4-18.6% of the quantization algorithm execution time compared to SOTA and resulting in a 1.6-1.8x increase in inference throughput.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/microsoft/VPTQ"
        ],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult",
        "authors": "fetong",
        "link": "https://arxiv.org/abs/2409.17545",
        "github_repo": null,
        "summary": "This research paper introduces Modulated Intervention Preference Optimization (MIPO), a novel algorithm designed for preference optimization in large language models (LLMs).\n- MIPO modulates the influence of the reference model during training based on the alignment between the reference model and the given preference pair, allowing for more effective learning.\n- Experimental results demonstrate that MIPO consistently outperforms Direct Preference Optimization (DPO) across various benchmarks, including AlpacaEval 2.0 and MT-Bench, using both Mistral-7B and Llama3-8B models.\n- On AlpacaEval 2.0, MIPO shows significant improvements over DPO, achieving gains of approximately 9 points with Llama3-8B and 8 points with Mistral-7B.\n- MIPO simplifies hyperparameter tuning by using only a single parameter, \u03b2, exhibiting robustness across different model architectures and datasets within a specific range.\n- MIPO effectively maintains performance on well-aligned pairs while substantially improving poorly aligned pairs, thereby efficiently enhancing the alignment of the policy model with given preferences.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback"
        ],
        "date": "2024-09-30"
    },
    {
        "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
        "authors": "Guanting Dong, Che Jiang, Yihuai Gao, Biqing Qi, Dayuan Fu",
        "link": "https://arxiv.org/abs/2409.16686",
        "github_repo": null,
        "summary": "- The paper introduces MSI-Agent, an embodied agent designed to enhance the planning and decision-making abilities of Large Language Models (LLMs) by effectively summarizing and utilizing insights at multiple scales.\n- MSI-Agent leverages a three-part pipeline consisting of an experience selector, insight generator, and insight selector to generate, store, and utilize task-specific and high-level insights.\n- Experimental results demonstrate that MSI-Agent outperforms other insight strategies when used with GPT-3.5 for planning tasks in the TEACh TfD benchmark and Alfworld environment.\n- The paper investigates different strategies for selecting seed experiences and insights, showing that MSI-Agent exhibits improved robustness in domain-shifting scenarios.\n- MSI-Agent effectively addresses the challenges of irrelevant insights and the lack of general insights, which can hinder the performance of LLM-based agents.",
        "classification": [
            "Robotics",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-09-30"
    },
    {
        "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
        "authors": "nm-w, pdufter, zhegan27, fly6464, haotiz",
        "link": "https://arxiv.org/abs/2409.20566",
        "github_repo": null,
        "summary": " - MM1.5, a new family of Multimodal Large Language Models (MLLMs), enhances capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning.\n- MM1.5 excels at understanding text-rich images by incorporating high-quality OCR data and synthetic captions during continual pre-training.\n- It outperforms existing open-source models in the 1B and 3B parameter range, showing competitive performance across benchmarks.\n- MM1.5 introduces specialized variants for video understanding (MM1.5-Video) and mobile UI understanding (MM1.5-UI).\n-  A data-centric approach and optimized mixtures for supervised fine-tuning contribute to MM1.5's enhanced multimodal understanding and reasoning capabilities.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "DiaSynth -- Synthetic Dialogue Generation Framework",
        "authors": "Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl",
        "link": "https://arxiv.org/abs/2409.19020",
        "github_repo": null,
        "summary": " - DiaSynth, a synthetic dialogue generation framework, produces high-quality, contextually rich dialogues using Large Language Models (LLMs) and Chain of Thought (CoT) reasoning.\n- It simulates personas, subtopics, and diverse conversational characteristics to generate realistic, domain-specific dialogues.\n- Models fine-tuned on synthetic data from DiaSynth outperformed base models by 16.47% on dialogue summarization tasks.\n- The synthetic data captured 90.48% of the performance achieved by models fine-tuned on in-domain data.\n- DiaSynth's data quality scales with LLM size, offering a robust alternative to traditional data collection.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Hyper-Connections",
        "authors": "banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder",
        "link": "https://arxiv.org/abs/2409.19606",
        "github_repo": null,
        "summary": "This research paper introduces hyper-connections as an effective alternative to residual connections in deep learning architectures, particularly transformers, addressing common drawbacks like the seesaw effect between gradient vanishing and representation collapse.\n- Hyper-connections allow the network to dynamically adjust the strength of connections between features at different depths and rearrange layers, improving performance with negligible increases in computation and parameters.\n- Experiments on large language models, both dense and sparse, demonstrated significant performance improvements compared to residual connections.\n- Hyper-connections are also effective in vision tasks.\n- Pre-Norm and Post-Norm residual connection variants can be considered specific cases of non-trainable hyper-connections.\n- The authors anticipate this method's broad applicability across various AI problems.",
        "classification": [
            "Natural Language Processing",
            "Computer Vision",
            "Image Classification",
            "Text Generation",
            "Image-to-Text",
            "Unconditional Image Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
        "authors": "yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li",
        "link": "https://arxiv.org/abs/2409.18943",
        "github_repo": "https://github.com/Geaming2002/Ruler",
        "summary": "- RULER, a model-agnostic method to enhance LLMs' ability to generate responses matching specified lengths by introducing Meta Length Tokens (MLTs).\n- Introduces the Target Length Generation (TLG) task and metrics Precise Match (PM) and Flexible Match (FM) for evaluating length-controlled generation.\n- RULER improves PM and FM scores by an average of 27.97 and 29.57, respectively, across various LLMs.\n- Shows RULER's effectiveness in controlling response length through multi-MLT generation and self-generated MLT experiments. \n- RULER maintains overall performance on various other benchmarks without affecting non-length based generation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Geaming2002/Ruler"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Cottention: Linear Transformers With Cosine Attention",
        "authors": "Eric C. Larson, TrevorDohm, gmongaras",
        "link": "https://arxiv.org/abs/2409.18747",
        "github_repo": null,
        "summary": "This study introduces \"Cottention,\" a novel attention mechanism using cosine similarity instead of softmax, achieving linear memory complexity concerning sequence length. Cottention maintains performance comparable to softmax attention while significantly reducing memory needs, validated on bidirectional BERT and causal GPT tasks. It is reformulated as a recurrent neural network (RNN) with a finite hidden state, enabling constant memory usage during inference. Results show Cottention as a promising alternative for handling longer sequences without performance loss due to its native linear memory complexity and constant memory footprint during inference.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/gmongaras/Cottention_Transformer"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Image Copy Detection for Diffusion Models",
        "authors": "Yi Yang, Zhentao Tan, Yifan Sun, WenhaoWang",
        "link": "https://arxiv.org/abs/2409.19952",
        "github_repo": null,
        "summary": "-\"ICDiff\", a new Image Copy Detection (ICD) model specialized for diffusion-generated replicas, is introduced, addressing the challenge of content originality in AI-generated images.\n- A novel Diffusion-Replication (D-Rep) dataset comprising 40,000 image-replica pairs, annotated with six replication levels, is created using Stable Diffusion V1.5 and LAION-Aesthetics V2.\n- A new PDF-Embedding method transforms replication levels into probability density functions (PDFs) for supervision, improving performance by leveraging the continuous and smooth nature of replication level probabilities.\n- Experimental results demonstrate PDF-Embedding outperforms protocol-driven and non-PDF methods on D-Rep, highlighting its effectiveness in detecting diffusion-based replication.\n- Analysis reveals replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%, indicating a significant prevalence of content replication in AI-generated images.",
        "classification": [
            "Computer Vision",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://icdiff.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Can Models Learn Skill Composition from Examples?",
        "authors": "Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu",
        "link": "https://arxiv.org/abs/2409.19808",
        "github_repo": null,
        "summary": "This paper investigates whether smaller language models can learn compositional generalization, the ability to combine learned skills in novel ways, through fine-tuning on a dataset generated by GPT-4.\n- Fine-tuning on text combining 2 or 3 skills leads to improved composition of 4 and 5 skills.\n- Fine-tuning on training skills enhances the composition of held-out skills, suggesting acquisition of a higher-order meta-skill.\n- The study shows that incorporating skill-rich synthetic text improves compositional capabilities.\n- Models fine-tuned on data with more skills (larger k) learn faster, showcasing data efficiency.\n- Results are validated using Claude 3 Opus as a grader to address potential GPT-4 bias.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
        "authors": "Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae",
        "link": "https://arxiv.org/abs/2409.19715",
        "github_repo": null,
        "summary": "COFFEE-GYM, a comprehensive reinforcement learning (RL) environment designed for training feedback models to refine code editing. COFFEE-GYM incorporates COFFEE, a dataset containing human code edit traces with machine feedback, addressing data scarcity issues. The environment also introduces COFFEEEVAL, a unit-test driven reward model directly measuring feedback's helpfulness. Experiments show COFFEEEVAL provides more accurate reward compared to the SOTA G-Eval with GPT-4.  Feedback models trained with COFFEE-GYM generates helpful feedback and achieve closed-source models' performance in code editing tasks.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym"
        ],
        "date": "2024-10-01"
    },
    {
        "title": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding",
        "authors": "Jianzong Wang, Jing Xiao, zhangxulong, Pechola",
        "link": "https://arxiv.org/abs/2409.19627",
        "github_repo": null,
        "summary": "-\nIDEAW, a novel dual-stage invertible neural network model, is introduced for robust audio watermarking, addressing the issue of high overhead in watermark localization.\n- It employs a dual-embedding strategy to embed watermark messages and locating codes separately, enabling faster and more efficient watermark locating.\n- A balance block is introduced to mitigate the asymmetry caused by the attack layer in the invertible neural network during robustness training and maintain training stability.\n- IDEAW demonstrates superior performance in terms of higher capacity and more efficient locating compared to existing neural audio watermarking methods.\n- Experimental results show its ability to withstand various attacks while maintaining good imperceptibility.",
        "classification": [
            "Audio",
            "Audio-to-Audio"
        ],
        "github_urls": [
            "https://github.com/PecholaL/IDEAW"
        ],
        "huggingface_urls": [],
        "date": "2024-10-01"
    },
    {
        "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
        "authors": "xwhan, ruihou16, xwwang, astonzhang, MingZhong",
        "link": "https://arxiv.org/abs/2409.19951",
        "github_repo": "https://github.com/facebookresearch/llm-cross-capabilities",
        "summary": " - This research paper explores the intersection of multiple abilities, termed \"cross capabilities,\" in Large Language Models (LLMs), which are essential for real-world tasks but often overlooked in current evaluations that focus on individual capabilities.\n- It introduces CROSSEVAL, a benchmark with 1,400 human-annotated prompts and 8,400 human ratings, designed to evaluate both individual and cross capabilities, revealing that current LLMs underperform in cross-capability tasks.\n- The study finds that LLM cross-capability performance adheres to the \"Law of the Weakest Link,\" being significantly limited by the weakest individual capability, regardless of improvements in other areas.\n- The results highlight that tool use is a major challenge for LLMs and suggest that prioritizing the enhancement of weaker capabilities is more crucial for improving overall performance than focusing on already strong ones.\n-  The work emphasizes the importance of shifting focus towards cross-capability evaluation and development to improve LLM effectiveness in complex, real-world scenarios rather than just on individual capabilities.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/llm-cross-capabilities"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices",
        "authors": "Hongfang Yu, Mohsen Guizani, Jiaoshen, LIKirin",
        "link": "https://arxiv.org/abs/2410.00531",
        "github_repo": "https://github.com/Lizonghang/TPI-LLM",
        "summary": "TPI-LLM is a tensor parallel inference system designed for serving 70B-scale LLMs efficiently on low-resource edge devices.\n- It addresses memory limitations by introducing a sliding window memory scheduler that dynamically manages layer weights during inference, overlapping disk I/O with computation and communication.\n- TPI-LLM prioritizes tensor parallelism over pipeline parallelism for single-user scenarios on edge devices and implements a star-based allreduce algorithm to minimize link latency.\n- Experimental results show significant reductions in time-to-first-token, token latency, and peak memory footprint compared to benchmarks like Transformers, Accelerate, and Galaxy.\n- TPI-LLM successfully runs Llama 2-70B with a peak memory footprint of 3.1GB across 8 low-resource devices, enabling larger models to run on edge devices while preserving user privacy.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/Lizonghang/TPI-LLM"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect",
        "authors": "imomayiz, amr-mohamed, khoubrane-yousef, habdine, guokan-shang",
        "link": "https://arxiv.org/abs/2409.17912",
        "github_repo": null,
        "summary": "Atlas-Chat introduces the first Large Language Models (LLMs) for Moroccan Arabic, a low-resource dialectal Arabic (DA) variant also known as Darija.\n- A new instruction dataset, Darija-SFT-Mixture, was created by combining existing and new manually and synthetically created Darija resources, as well as translated English instructions.\n- Atlas-Chat-9B and 2B models, fine-tuned on this dataset, outperform existing LLMs, including Arabic-specific and state-of-the-art models like LLaMa, Jais, and AceGPT, achieving a 13% improvement over a 13B model on a new Darija benchmark.\n- A new evaluation suite, including DarijaMMLU, DarijaHellaSwag, and DarijaBench, was developed for comprehensive LLM assessment in Darija, focusing on discriminative and generative tasks. \n- An experimental analysis was conducted on fine-tuning strategies and base model choices, finding that instruction-tuned Gemma 2 models with LoRA performed optimally.",
        "classification": [
            "Natural Language Processing",
            "Translation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/MBZUAI-Paris/Atlas-Chat-9B",
            "https://hf.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
        "authors": "sebgao, wangpichao, meihaiyang, tonghe, ZechenBai",
        "link": "https://arxiv.org/abs/2409.19603",
        "github_repo": "https://github.com/showlab/VideoLISA",
        "summary": "-\nVideoLISA, a video-based multimodal large language model, is introduced for language-instructed reasoning segmentation in videos. \n- It leverages the reasoning capabilities of large language models and the Segment Anything Model to generate temporally consistent segmentation masks.\n- VideoLISA employs a Sparse Dense Sampling strategy, balancing temporal context and spatial detail, and a One-Token-Seg-All approach using a <TRK> token for object tracking.\n- Evaluations on various benchmarks, including the newly introduced ReasonVOS, demonstrate its superior performance in video object segmentation with complex reasoning.\n- While optimized for videos, it generalizes well to image segmentation, showing potential as a foundation model for language-instructed object segmentation.",
        "classification": [
            "Computer Vision",
            "Image Segmentation",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/showlab/VideoLISA"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Illustrious: an Open Advanced Illustration Model",
        "authors": "Junha Lee, leehg57, mhy9910, solbon1212, andyp-nvidia",
        "link": "https://arxiv.org/abs/2409.19946",
        "github_repo": null,
        "summary": "\n- Illustrious, a state-of-the-art, open-source anime image generation model, leverages a large dataset and detailed prompt guidance to generate high-resolution, dynamic images with anatomical integrity.\n- The model focuses on three key improvements: batch size and dropout control for faster concept activation learning, increased training resolution for accurate anatomy depiction, and refined multi-level captions covering tags and natural language for enhanced model development.\n- Illustrious outperforms existing models in animation style and allows for easier customization, as shown in Figure 17, which demonstrates diverse image generation using various prompts.\n- Evaluation using Elo Rating, TrueSkill 2, and Character-wise Image Comparison (CCIP) demonstrates Illustrious's superior performance compared to other models.\n- The model addresses limitations of existing datasets and text encoders within the illustration/animation domain by employing techniques like No Dropout Token and Quasi-Register Tokens.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0",
            "https://huggingface.co/datasets/nyanko7/danbooru2023"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation",
        "authors": "Filippos Kokkinos, Andrea Vedaldi, philiptorr, JianyuanWang, Junlinh",
        "link": "https://arxiv.org/abs/2410.00890",
        "github_repo": null,
        "summary": "Flex3D is a novel two-stage framework for generating high-quality 3D content from text, single images, or sparse view images.\n- The first stage generates a pool of candidate views using fine-tuned multi-view image and video diffusion models, followed by a selection process to filter these views based on quality and consistency.\n- The second stage introduces FlexRM (Flexible Reconstruction Model), a transformer-based architecture that reconstructs detailed 3D Gaussian points from the selected views using a tri-plane representation.\n- Flex3D also employs a novel training strategy simulating imperfect input views by injecting noise into the generated 3D Gaussian points to enhance robustness for generation tasks. \n- Experimental results show that Flex3D achieves state-of-the-art performance in both 3D generation and reconstruction tasks, with a user study win rate exceeding 92% in generation tasks. \n- FlexRM outperforms other baselines in 3D reconstruction across various input view settings, demonstrating its flexibility and efficiency.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer",
        "authors": "Jingren, chenweix7, chaojiemao, jingfengzhang, jiangzeyinzi",
        "link": "https://arxiv.org/abs/2410.00086",
        "github_repo": null,
        "summary": " - ACE, a unified framework based on a Diffusion Transformer, supports a wide range of visual generation and editing tasks through natural language instructions, including text-guided generation, low-level visual analysis, controllable generation, semantic editing, element editing, repainting, layer editing, and reference generation.\n- ACE introduces the Long-context Condition Unit (LCU) to incorporate historical information from previous generation rounds, enabling multi-turn and long-context generation.\n- A meticulous data collection workflow is established to construct a 0.7 billion-scale dataset covering various generation and editing tasks.\n- Evaluation on benchmarks such as MagicBrush and a user study on a manually curated benchmark demonstrates ACE\u2019s superior performance in various visual generation tasks.\n- ACE can be easily integrated into a multimodal chat system to streamline image creation and editing, avoiding cumbersome pipelines typically employed in visual agents.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/runwayml/stable-diffusion-v1-5",
            "https://huggingface.co/runwayml/stable-diffusion-inpainting"
        ],
        "date": "2024-10-02"
    },
    {
        "title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models",
        "authors": "Xiaolong Wang, Xuxin Cheng, Zipeng Fu, Qi Wu, cbfinn",
        "link": "https://arxiv.org/abs/2410.00231",
        "github_repo": null,
        "summary": "-\"Helpful DoggyBot\" is introduced, a quadrupedal robot system for open-world object fetching in indoor environments, integrating a 1-DoF gripper, a learned whole-body controller, and vision-language models (VLMs).\n-The system uses a two-phase training process for the controller, focusing on whole-body control and agility, leveraging privileged information in simulation and distilling it to a deployable policy using egocentric depth.\n-VLMs are used for zero-shot generalization to unseen environments and objects, enabling open-vocabulary object detection, efficient navigation, and precise grasping.\n-Real-world experiments show that the system achieves a 60% first-attempt success rate in fetching objects from beds and sofas, outperforming baselines and approaching teleoperation performance in terms of average time to completion.\n-The system demonstrates the potential of integrating learned controllers and VLMs for complex mobile manipulation tasks in unstructured indoor settings.",
        "classification": [
            "Robotics",
            "Computer Vision",
            "Object Detection"
        ],
        "github_urls": [
            "https://helpful-doggybot.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
        "authors": "Shubham Tulsiani, Donglai Xiang, Jeff Tan, gengshan-y, devakramanan",
        "link": "https://arxiv.org/abs/2409.20563",
        "github_repo": null,
        "summary": "DressRecon reconstructs temporally consistent 4D human body models from monocular RGB videos, focusing on challenging scenarios with loose clothing and object interactions.\n- It leverages a hierarchical \"bag-of-bones\" motion model, disentangling body and clothing deformations as separate layers.\n- The method uses image-based priors such as human body pose, surface normals, and optical flow to optimize the model and improve reconstruction quality.\n- The resulting neural fields can be converted to meshes or further optimized as explicit 3D Gaussians for interactive rendering.\n- On datasets with challenging clothing deformations, DressRecon shows superior performance over prior art in terms of reconstruction fidelity and rendering quality.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Visual Context Window Extension: A New Perspective for Long Video Understanding",
        "authors": "Zhenzhong Chen, hcwei",
        "link": "https://arxiv.org/abs/2409.20018",
        "github_repo": null,
        "summary": "This research paper proposes a novel approach to enhance long video understanding by extending the visual context window of Large Multimodal Models (LMMs).\n- It redefines the context window in LMMs as two distinct windows: visual and language, addressing the discrepancies between these modalities.\n- The study introduces a method to extend positional embeddings within the visual context window, enabling LMMs to handle lengthy videos without retraining on large video-text datasets.\n- A progressive pooling strategy is implemented to reduce memory consumption by selectively adjusting the spatial resolution of frame embeddings.\n- Experimental results on benchmarks like MLVU, VideoMME, and LongVideoBench demonstrate consistent performance improvements with increasing video frames, outperforming models like GPT-40 and achieving memory savings of approximately 45%.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs",
        "authors": "Qing Lian, Xu Yan, Yingjie Cai, Weichao Qiu, Leheng Li",
        "link": "https://arxiv.org/abs/2410.00337",
        "github_repo": null,
        "summary": "-\nSyntheOcc, a novel image generation framework, achieves fine-grained 3D geometric control by conditioning on 3D occupancy labels, enabling applications like 3D editing, dataset generation, and long-tailed scene generation.\n- It leverages 3D semantic Multi-Plane Images (MPIs) as conditional input, offering precise spatial alignment with generated images.\n- An MPI encoder and reweighing strategies enhance image quality and recognizability.\n- SyntheOcc outperforms existing methods in generating realistic and controllable street view images, as demonstrated by its superior performance on the nuScenes dataset.\n- The synthetic data generated by SyntheOcc effectively augments perception models for 3D occupancy prediction.",
        "classification": [
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-02"
    },
    {
        "title": "Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration",
        "authors": "Michael Elad, Michato, ohayonguy",
        "link": "https://arxiv.org/abs/2410.00418",
        "github_repo": null,
        "summary": "This paper introduces Posterior-Mean Rectified Flow (PMRF), a new algorithm for photo-realistic image restoration.\n- PMRF aims to minimize the mean squared error (MSE) under the constraint of a perfect perceptual index, unlike methods that sample from the posterior or optimize a weighted sum of distortion and perceptual losses.\n- PMRF first predicts the posterior mean and then uses a rectified flow model to transport the result to a high-quality image distribution, approximating the optimal estimator for minimal MSE under perfect perceptual index.\n- PMRF consistently outperforms existing methods on various image restoration tasks, including blind face restoration, as demonstrated by improved FID, KID, PSNR, and SSIM scores on the CelebA-Test benchmark and lower IndRMSE on real-world datasets, while maintaining competitive performance on other perceptual and distortion metrics.\n- PMRF's effectiveness is attributed to its novel framework, which directly targets the optimal MSE estimator under a perfect perceptual index constraint, as shown by its superior performance compared to alternative flow-based methods in controlled experiments.\n- The codes are available at https://github.com/ohayonguy/PMRF.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/ohayonguy/PMRF"
        ],
        "huggingface_urls": [],
        "date": "2024-10-02"
    }
]