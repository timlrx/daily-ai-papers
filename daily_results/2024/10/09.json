[
    {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "authors": "Peijie Dong, wenxinsiju, xuminghui, Dominic789654",
        "link": "https://arxiv.org/abs/2410.04199",
        "github_repo": null,
        "summary": "-\nLongGenBench, a synthetic benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs), focusing on consistency and logical flow.\n-\nIt redesigns question formats, requiring LLMs to provide single, cohesive long-context answers encompassing multiple questions within a single query.\n-\nEvaluation on LongGenBench reveals performance degradation across both API-accessed and open-source LLMs in long-context scenarios, ranging from 1.2% to 47.1%.\n-\nDifferent LLM series show varying degradation trends, with Gemini-1.5-FLASH exhibiting minimal degradation among API-accessed models, and QWEN2 series showing minimal degradation among open-source models.\n-\nModel size influences performance decline, with larger models within a series generally demonstrating less degradation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization",
        "authors": "Francois Charton, Justin Wang, shizhuo2",
        "link": "https://arxiv.org/abs/2410.04717",
        "github_repo": null,
        "summary": "-\nThis paper investigates the impact of instruction diversity on the generalization ability of Large Language Models (LLMs), focusing solely on instruction-following capabilities and isolating them from reasoning and knowledge retrieval.\n- Through controlled string rewriting experiments inspired by the Turing-complete Markov algorithm and mathematical deduction tasks, the study demonstrates that generalization to unseen instructions emerges only when training data is sufficiently diverse across semantic domains.\n- Findings reveal that diversifying data within limited domains does not guarantee robust generalization, while cross-domain diversification significantly enhances adaptability to new instructions.\n- The research further shows that increasing the diversity of training data can lead to performance improvements in real-world scenarios, including code generation and reasoning tasks with both specialized and generalist models. \n- The results underscore the importance of strategic data diversification over simply increasing data size, offering guidelines for improving instruction-tuning datasets and enhancing model performance across various domains.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
        "authors": "lifengshang, YuxinJiang, Tiezheng, yufeiwang201217a, DonJoey",
        "link": "https://arxiv.org/abs/2410.05193",
        "github_repo": null,
        "summary": "\u2022 REVISEVAL, a novel evaluation paradigm, leverages the revision capabilities of Large Language Models (LLMs) to generate response-adapted references for evaluating text generation quality. \n\u2022 It revises the generated response based on the given instruction and evaluation rubric, then uses the revised text as a reference for subsequent evaluation by either LLM-as-a-Judge or classic text evaluation metrics.\n\u2022 REVISEVAL outperforms reference-free and reference-based evaluation methods across various NLG and instruction-following tasks using both open-source and proprietary LLMs. \n\u2022 Response-adapted references enhance the performance of classic metrics, sometimes even rivaling LLM-as-a-Judge. \n\u2022 REVISEVAL effectively reduces bias in evaluation, such as verbosity and positional biases, and its effectiveness is linked to the relevance of the generated references.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
        "authors": "Sinan Tan, Jinze, JustinLin610, ZefanCai, leonardPKU",
        "link": "https://arxiv.org/abs/2410.01912",
        "github_repo": "https://github.com/chenllliang/DnD-Transformer",
        "summary": "-\nThe paper introduces the 2-Dimensional Autoregression (DnD) Transformer, a novel autoregressive model for image generation that addresses the information loss bottleneck of vector quantization (VQ) by predicting more codes for an image through a new autoregression direction (depth) alongside the traditional sequence length.\n- The DnD-Transformer inserts multiple prediction heads into the backbone transformer decoder to predict depth codes within a single forward pass, enhancing image quality without increasing model size or sequence length.\n- On ImageNet 256x256 generation, DnD-Transformer achieves up to 1.54 FID and 82.6 IS improvements without increased model size or sequence length, surpassing the larger LlamaGen model.\n- It demonstrates an emergent vision-language intelligence by generating images with rich text and graphical elements in a self-supervised manner, solely trained on images, even outperforming diffusion models on rich-text image datasets.\n- It opens up a new optimization perspective in autoregressive image generation by introducing a new autoregression direction that reduces information loss and efficiently reconstructs images.",
        "classification": [
            "Text-to-Image",
            "Unconditional Image Generation",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/chenllliang/DnD-Transformer"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
        "authors": "Haocheng Shen, Peize Sun, Shoufa Chen, Tianheng Cheng, Zongming Li",
        "link": "https://arxiv.org/abs/2410.02705",
        "github_repo": "https://github.com/hustvl/ControlAR",
        "summary": " - ControlAR, a new framework, allows autoregressive models to perform controllable image generation from spatial control inputs like edges, depth maps, and segmentation masks.\n- ControlAR uses a lightweight control encoder, based on a Vision Transformer, to transform control images into sequential control tokens.\n- It employs a conditional decoding strategy, where the next image token is predicted based on both previous image tokens and the corresponding control token, demonstrating better performance and efficiency than pre-filling methods.\n- ControlAR extends autoregressive models to arbitrary-resolution image generation by conditioning on control token inputs of varying sizes.\n- Experiments show that ControlAR achieves highly competitive performance with state-of-the-art diffusion-based methods and strong control capability across various tasks, even surpassing ControlNet++ in some cases.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/hustvl/ControlAR"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
        "authors": "Yu Sun, Shuohuan Wang, Huang Fang, Haoran Sun, Yekun Chai",
        "link": "https://arxiv.org/abs/2410.02743",
        "github_repo": "https://github.com/ernie-research/MA-RLHF",
        "summary": "\n- MA-RLHF, a new Reinforcement Learning from Human Feedback (RLHF) framework, is introduced to improve large language model alignment with human preferences.  It leverages \"macro actions\" which are sequences of tokens or higher-level language constructs. \n- This approach reduces the temporal distance between actions and rewards, addressing the credit assignment problem in token-level RLHF, and facilitates faster and more accurate credit assignment. \n- The model achieves substantial performance improvements across various tasks, including up to a 30% gain in summarization, an 18% gain in dialogue, and an 8% gain in question answering, while demonstrating a 1.7x-2x faster convergence compared to standard RLHF. \n- MA-RLHF's robustness is highlighted through experiments conducted with different model sizes (2B to 27B) on various tasks, such as text summarization with the TL;DR dataset and dialogue generation with the HH-RLHF dataset. \n- Further analysis explores termination strategies for macro actions, demonstrating the effectiveness of n-gram and parsing-based approaches in improving model performance.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Summarization",
            "Text2Text Generation",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/ernie-research/MA-RLHF"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Dahoas/full-hh-rlhf"
        ],
        "date": "2024-10-09"
    },
    {
        "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
        "authors": "Yufan Zhou, Shizhe Diao, Yu Cheng, Zhiyang Xu, WHB139426",
        "link": "https://arxiv.org/abs/2410.03290",
        "github_repo": null,
        "summary": "**-** This paper introduces Grounded-VideoLLM, a novel Video Large Language Model (Video-LLM) designed for fine-grained temporal grounding in videos. \n**-** Grounded-VideoLLM uses a two-stream architecture, encoding spatial information from keyframes and temporal dynamics from multiple frames using a video encoder, to create a temporally-aware video representation.\n**-**  It introduces discrete temporal tokens into the LLM's vocabulary for representing timestamps efficiently, avoiding tokenization of numerical text and integrating time representations directly into the LLM.  \n**-** A multi-stage training approach is employed, progressing from video-caption alignment to temporal token alignment and finally multi-task instruction tuning on datasets incorporating temporal grounding tasks.\n**-** Experimental results demonstrate that Grounded-VideoLLM achieves state-of-the-art performance on various fine-grained temporal grounding tasks including Temporal Sentence Grounding, Dense Video Captioning and Grounded VideoQA, as well as general video understanding benchmarks.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/WHB139426/Grounded-Video-LLM"
        ],
        "huggingface_urls": [],
        "date": "2024-10-09"
    },
    {
        "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
        "authors": "yuyijiong",
        "link": "https://arxiv.org/abs/2410.04422",
        "github_repo": null,
        "summary": " - This paper investigates the underlying reasons why Long Context Language Models (LCLMs) struggle with complex tasks, despite their ability to handle extensive text. \n- Through experiments with synthetic datasets, the study identifies \"multi-matching retrieval\" (retrieving multiple items simultaneously) and \"logic-based retrieval\" (using logic within retrieval criteria) as the core challenges, and further defines them as \"hyper-multi-step\" problems.\n- \"Hyper-multi-step\" implies that these seemingly simple tasks actually comprise a large number of indivisible sub-steps, which increases with context length and exceeds the processing capacity of current LCLMs. \n- The paper provides empirical evidence through linear probing of hidden states and analysis of attention weights, demonstrating that these problems are more akin to complex arithmetic tasks, rather than traditional retrieval, and are therefore not adequately addressed by existing techniques such as Retrieval-Augmented Generation (RAG) or Chain-of-Thought (CoT) prompting. \n- The study concludes that simply increasing the context window size of LCLMs may not suffice; instead, future research should focus on addressing the numerous steps involved and explore alternative solutions, such as using external tools.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-09"
    }
]