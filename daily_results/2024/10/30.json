[
    {
        "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
        "authors": "Denis Bobkov, Boris Mikheev, Alexey Zhavoronkin, Dmitrii Korzh, therem",
        "link": "https://arxiv.org/abs/2410.18057",
        "github_repo": null,
        "summary": "- Introduces CLEAR, a multimodal benchmark for evaluating machine unlearning (MU) in textual and visual modalities, focusing on removing information about specific individuals.\n- The benchmark includes a synthetic dataset of 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs, along with real-world face and visual question answering datasets for evaluating model retention.\n- Evaluates 10 existing MU methods adapted for multimodal unlearning, revealing that current state-of-the-art algorithms struggle in multimodal settings.\n- Demonstrates that simple L1 regularization on LoRA adapter weights during unlearning significantly mitigates catastrophic forgetting, improving the preservation of model performance on retained data.\n- Makes the dataset publicly available to encourage further research in the field.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Natural Language Processing",
            "Image-to-Text",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/therem/CLEAR"
        ],
        "date": "2024-10-30"
    },
    {
        "title": "SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization",
        "authors": "Chuang Gan, Donglai Wei, Jiawei Zhou, zmeng0116, EthanTaylor",
        "link": "https://arxiv.org/abs/2410.21411",
        "github_repo": "https://github.com/Mengzibin/SocialGPT",
        "summary": "- SocialGPT is a novel modular framework that leverages Vision Foundation Models (VFMs) and Large Language Models (LLMs) for social relation reasoning.\n- It employs VFMs to translate image content into a textual \"social story\" and utilizes LLMs for reasoning based on the generated story and provided bounding boxes.\n- This framework incorporates systematic design principles to enhance the collaboration between VFMs and LLMs, including comprehensive and domain-specific visual information extraction and a structured reasoning prompt named SocialPrompt.\n- SocialGPT achieves competitive zero-shot performance on PIPA and PISC datasets, outperforming previous state-of-the-art supervised methods on PIPA by 1.4%.\n- The framework also introduces Greedy Segment Prompt Optimization (GSPO) for automatic prompt tuning, resulting in significant performance improvements across various LLMs.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Zero-Shot Classification"
        ],
        "github_urls": [
            "https://github.com/Mengzibin/SocialGPT"
        ],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization",
        "authors": "Hongming Zhang, Wenhao Yu, Kaixin Ma, Wenlin Yao, Hongliang He",
        "link": "https://arxiv.org/abs/2410.19609",
        "github_repo": null,
        "summary": "- This paper introduces OpenWebVoyager, an open-source framework for building multimodal web agents that can explore real-world websites, receive feedback, and iteratively optimize their performance.\n- The agent architecture adapts the Idefics2-8b-instruct model, processing observations consisting of webpage screenshots and accessibility trees.\n-  OpenWebVoyager combines imitation learning from a GPT-40 powered web agent with an exploration-feedback-optimization cycle, where GPT-40 evaluates the agent's trajectory success.\n- Across multiple iterations on the WebVoyager and Mind2Web datasets, the agent shows improvement in task success rate, starting from 19.9% to 25.8% on the WebVoyager test set and 6.3% to 19.6% on the Mind2Web cross-task test set.\n- The results indicate that the iterative real-world exploration and optimization method is an effective way to improve the agent's real-world performance.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/MinorJerry/OpenWebVoyager"
        ],
        "date": "2024-10-30"
    },
    {
        "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning",
        "authors": "Paul Mineiro, ydeng9",
        "link": "https://arxiv.org/abs/2410.22304",
        "github_repo": null,
        "summary": "- This paper introduces Flow-DPO, a novel approach to improve Large Language Model (LLM) mathematical reasoning by generating high-quality reasoning traces through online multi-agent learning flows.\n- The method employs an incremental output production flow composed of multiple LLMs that iteratively communicate to construct solutions, similar to a multi-agent system.\n- The flow is trained using online Direct Preference Optimization (DPO) with rollouts, generating DPO pairs for each training example during answer chunk generation and updating the models in real-time.\n- Experimental results on MetaMath, GSM8K, and MATH datasets demonstrate that Flow-DPO generates higher-quality reasoning traces compared to direct model inference, leading to improved performance in mathematical reasoning tasks after supervised fine-tuning.\n- This improvement is particularly significant for the Llama-3-8B-instruct model, achieving a 20% improvement in validation accuracy on mathematical reasoning tasks during training.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
        "authors": "Ningxin Zheng, Size Zheng, Wenlei Bao, Li-Wen Chang, preminstrel",
        "link": "https://arxiv.org/abs/2410.21465",
        "github_repo": "https://github.com/bytedance/ShadowKV",
        "summary": "- SHADOWKV is a novel LLM inference system designed for enhanced throughput in long-context scenarios by storing a low-rank representation of the key cache on the GPU and offloading the value cache to the CPU.\n- It employs a precise KV selection strategy during decoding, utilizing landmarks and static outliers to minimize the sparse KV cache budget while maintaining accuracy.\n- Evaluations on benchmarks like RULER, LongBench, and Needle in a Haystack with various LLMs (Llama, GLM, Yi, Phi, Qwen) show that SHADOWKV can handle contexts up to 1M tokens.\n- It achieves up to a 6x increase in batch size and a 3.04x boost in throughput compared to full attention on an A100 GPU.\n- SHADOWKV's performance even surpasses the theoretical throughput of infinite batch size with full attention, assuming infinite GPU memory.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/bytedance/ShadowKV"
        ],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset",
        "authors": "Yongyuan Liang, Huanyu Li, Tao Huang, Yifei Sun, Guangqi Jiang",
        "link": "https://arxiv.org/abs/2410.22325",
        "github_repo": null,
        "summary": "- This paper introduces Manipulation Centric Representation (MCR), a framework for learning robotic visual representations that prioritize manipulation-relevant information, such as robot end-effectors and task-relevant objects.\n- MCR leverages large-scale robot datasets (e.g. DROID), and introduces a novel contrastive loss that aligns visual observations with robot proprioceptive state-action dynamics.\n- It uses two new training objectives: dynamics alignment loss and action prediction loss. A time contrastive loss is also used.\n- MCR outperforms previous state-of-the-art by 14.8% across four simulated robotic manipulation domains and also achieves a 76.9% improvement on three real-world robot tasks.\n- The paper also introduces \"manipulation centricity,\" a metric demonstrating a strong correlation with downstream policy performance in robotic manipulation tasks.",
        "classification": [
            "Robotics",
            "Image Feature Extraction",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-10-30"
    },
    {
        "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning",
        "authors": "Sergey Levine, Jeffrey Wu, charlesxu0124, jianlanluo",
        "link": "https://arxiv.org/abs/2410.21845",
        "github_repo": null,
        "summary": "- This paper introduces a human-in-the-loop vision-based reinforcement learning (RL) system called HIL-SERL for dexterous robotic manipulation tasks.\n- HIL-SERL integrates human demonstrations and corrections, pretrained vision backbones, and a sample-efficient off-policy RL algorithm.\n- The system achieves near-perfect success rates and surpasses human cycle times on diverse manipulation tasks like dynamic object flipping, precise assembly, and dual-arm coordination, often within 1-2.5 hours of real-world training.\n- It demonstrates a significant performance improvement compared to imitation learning methods (e.g., 101% average success rate increase and 1.8x faster cycle time).\n- The approach trains policies that exhibit both reactive and predictive behaviors, adapting to the specific requirements of each task, from precise insertions to dynamic motions like Jenga whipping.",
        "classification": [
            "Reinforcement Learning",
            "Robotics",
            "Computer Vision"
        ],
        "github_urls": [
            "https://hil-serl.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-10-30"
    }
]