[
    {
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
        "authors": "Weipeng Chen, Mang Wang, Wen Wang, Zhicheng Dou, Jiejun Tan",
        "link": "https://arxiv.org/abs/2411.02959",
        "github_repo": null,
        "summary": "- HtmlRAG is a novel Retrieval-Augmented Generation (RAG) system that utilizes HTML, instead of plain text, as the format for retrieved knowledge, aiming to preserve structural and semantic information often lost during HTML-to-text conversion.\n- HtmlRAG incorporates HTML cleaning, compression, and a two-step pruning process, involving an embedding model for coarse-grained pruning and a generative model for fine-grained pruning, to address challenges of long input sequences and noisy content.\n- This approach outperforms existing RAG systems, utilizing text-based and Markdown-based post-retrieval processes on six QA datasets, including ASQA, HotpotQA, NQ, TriviaQA, MuSiQue, and ELI5 datasets. \n- Specifically, it improves exact match by up to 4.5% on the NQ Dataset and 8.7% on the MuSiQue dataset when using Llama 3.1 70B instruct model.\n- The results demonstrate the effectiveness of utilizing HTML for knowledge modeling in RAG systems, particularly with powerful LLMs capable of handling complex HTML structures.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/plageon/HtmlRAG"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "LLaMo: Large Language Model-based Molecular Graph Assistant",
        "authors": "Hyunwoo J. Kim, Dohwan Ko, Minseong Bae, Jinyoung Park",
        "link": "https://arxiv.org/abs/2411.00871",
        "github_repo": "https://github.com/mlvlab/LLaMo",
        "summary": "- LLaMo, a Large Language Model-based Molecular graph assistant, integrates a molecular graph encoder and a large language model for instruction-following response generation in the molecular domain.\n- LLaMo uses a multi-level graph projector to abstract representations of each GNN layer and motif representations, bridging the graph encoder and language model.\n- Machine-generated molecular graph instruction data, created through a multi-turn conversation format from molecular descriptions and IUPAC names, are used for instruction tuning.\n- Experimental results demonstrate LLaMo's superior performance in molecular description generation, property prediction, and IUPAC name prediction, outperforming LLM-based models like GPT-4.\n- Ablation studies validate the contribution of the multi-level graph projector and the instruction tuning process.",
        "classification": [
            "Graph Machine Learning",
            "Multimodal",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/mlvlab/LLaMo"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
        "authors": "Shenzhi Wang, Yizeng Han, Bingyi Kang, Yulin Wang, Yang Yue",
        "link": "https://arxiv.org/abs/2411.02359",
        "github_repo": "https://github.com/yueyang130/DeeR-VLA",
        "summary": "- DeeR-VLA dynamically adjusts the size of activated Multimodal Large Language Models (MLLMs) based on situation complexity, improving computational efficiency for robotic tasks.\n- DeeR leverages a multi-exit MLLM architecture allowing early termination of processing once sufficient model capacity is reached for a given input, avoiding redundant computation.\n- The framework includes algorithms to set early-exit criteria based on predefined computational budgets (average/peak FLOPs, GPU memory), enabling adaptability to resource constraints.\n- A tailored training method integrates temporal information within the multi-exit architecture to ensure reasonable action predictions.\n- Evaluation on the CALVIN benchmark shows 5.2-6.5x reduction in LLM computational costs and 2-6x reduction in LLM GPU memory usage without compromising task performance.",
        "classification": [
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/yueyang130/DeeR-VLA"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "Sample-Efficient Alignment for LLMs",
        "authors": "Min Lin, Wee Sun Lee, Chao Du, Changyu Chen, Zichen Liu",
        "link": "https://arxiv.org/abs/2411.01493",
        "github_repo": null,
        "summary": "- This paper introduces SEA (Sample-Efficient Alignment), a Thompson sampling-based algorithm, for aligning Large Language Models (LLMs) with human preferences efficiently, addressing the bottleneck of extensive human feedback requirements in current alignment methods.\n- The approach frames LLM alignment as a contextual dueling bandit problem and emphasizes two key properties for sample efficiency: online interaction and active exploration.\n- SEA leverages an epistemic reward model (deep ensemble of reward models) for posterior sampling, policy-guided search for efficient response selection, and mixed preference learning (combining online user feedback and synthetic feedback from the reward model) to update the LLM policy online.\n- Experimental results across various model scales (1B, 2.8B, 6.9B parameters) and direct preference optimization methods (DPO, IPO, SLiC) show SEA achieves higher win rates against reference responses and significantly better sample efficiency compared to existing baselines, including passive online learning and other active exploration methods. \n- The authors release `oat`, an open-source, distributed learning system designed for online LLM alignment research, aiming to facilitate further studies and fair comparisons in the field.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/sail-sg/oat"
        ],
        "huggingface_urls": [
            "https://huggingface.co/docs/trl/main/en/online_dpo_trainer",
            "https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B"
        ],
        "date": "2024-11-06"
    },
    {
        "title": "DreamPolish: Domain Score Distillation With Progressive Geometry Generation",
        "authors": "Shiyu Huang, Wendi Zheng, Ming Ding, Yean Cheng, GhostCai",
        "link": "https://arxiv.org/abs/2411.01602",
        "github_repo": null,
        "summary": "- DreamPolish is a text-to-3D generation model that produces refined geometry and high-quality textures by leveraging multiple neural representations and a novel score distillation objective called Domain Score Distillation (DSD).\n- It uses a progressive geometry construction approach, starting with NeRF and transitioning to NeuS and DMTet for detailed surface information, and incorporates a surface polishing stage guided by a normal estimation prior.\n- DSD guides neural representations towards a domain in the latent space of pretrained text-to-image models that balances photorealism and stability during texture generation, addressing limitations of previous score distillation methods like SDS, VSD, and BSD.\n- Experimental results demonstrate that DreamPolish outperforms existing state-of-the-art methods in generating 3D objects with polished surfaces and photorealistic textures, as measured by PSNR, SSIM, LPIPS, and CLIP Score.\n- A user study further confirms the superior performance of DreamPolish, with users consistently preferring its generated 3D models over those produced by baseline methods.",
        "classification": [
            "Text-to-3D"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge",
        "authors": "Lashaw Salta, Chinmay Agrawal, Catalina Villouta, Andrew Langdon, ksoman",
        "link": "https://arxiv.org/abs/2411.02657",
        "github_repo": null,
        "summary": "- Zebra-Llama, a context-aware large language model specializing in Ehlers-Danlos Syndrome (EDS) information, was developed using a novel context-aware fine-tuning methodology.\n- The model leverages Retrieval-Augmented Generation (RAG) and is trained on a diverse dataset comprising medical literature, patient forums, and clinical resources, structured as question-context-answer triplets.\n- Evaluation on real-world questions from EDS patients and clinicians demonstrated Zebra-Llama's superior performance compared to the base Llama model across thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%), and citation reliability (70.6% vs. 52.3%).\n- A custom RAG API and Jupyter Notebook demo are also released.\n- The model and code are open-sourced to democratize expert-level knowledge in rare disease management.",
        "classification": [
            "Question Answering",
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/karthiksoman/zebra-llama"
        ],
        "huggingface_urls": [
            "https://huggingface.co/zebraLLAMA/zebra-Llama-v0.2"
        ],
        "date": "2024-11-06"
    },
    {
        "title": "Controlling Language and Diffusion Models by Transporting Activations",
        "authors": "Nicholas Apostoloff, Luca Zappella, Michal Klein, Arno Blaas, Pau Rodriguez",
        "link": "https://arxiv.org/abs/2410.23054",
        "github_repo": null,
        "summary": "- This paper introduces Activation Transport (ACT), a framework to steer activations in generative models (GMs) using optimal transport theory.\n- ACT generalizes existing activation steering methods by applying univariate maps to activations while preserving target distributions, improving controllability and robustness. \n- Linear-ACT, an inference-time intervention based on ACT, matches or outperforms other methods in toxicity mitigation, concept induction, and truthfulness in LLMs.\n- ACT effectively controls text-to-image diffusion models for fine-grained style control and concept negation. \n- The authors adapt ITI (Li et al., 2024) for text-to-image and find that ACT with a strength parameter of 1 consistently achieves strong conditioning across tasks and models.",
        "classification": [
            "Natural Language Processing",
            "Text-to-Image",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details",
        "authors": "Zirong Jin, Wanghao Du, Chenghong Li, Haolin Liu, Zhongjin Luo",
        "link": "https://arxiv.org/abs/2411.03047",
        "github_repo": null,
        "summary": "- This paper introduces GarVerseLOD, a new dataset and framework for high-fidelity 3D garment reconstruction from single in-the-wild images.\n- GarVerseLOD contains 6,000 high-quality, hand-crafted 3D garment models with detailed geometry, categorized into five common types, and structured with levels of detail (LOD) to improve learning and inference.\n- The method uses a hierarchical approach, starting with coarse shape estimation and progressively adding pose-induced global deformations and fine-scale local details.\n- A novel data labeling paradigm using conditional diffusion models generates realistic paired images for training.\n- Experimental results demonstrate GarVerseLOD's ability to reconstruct garments with various shapes and intricate deformations, outperforming existing methods in quality and robustness.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-11-06"
    },
    {
        "title": "Correlation of Object Detection Performance with Visual Saliency and Depth Estimation",
        "authors": "Dylan Seychell, mbar0075",
        "link": "https://arxiv.org/abs/2411.02844",
        "github_repo": "https://github.com/mbar0075/Object-Detection-Correlation-Saliency-vs-Depth",
        "summary": "- This paper investigates the correlation between object detection accuracy and two visual tasks: depth prediction and visual saliency prediction.\n- Experiments using models like DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model on COCO and Pascal VOC datasets reveal that visual saliency has a stronger correlation with object detection accuracy (mAp up to 0.459 on Pascal VOC) than depth prediction (mAp up to 0.283).\n- The correlation varies across object categories, with larger objects showing stronger correlations than smaller objects (up to three times higher).\n- The findings suggest that incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories.\n- The category-specific variations also offer insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.",
        "classification": [
            "Object Detection",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/mbar0075/Object-Detection-Correlation-Saliency-vs-Depth"
        ],
        "huggingface_urls": [],
        "date": "2024-11-06"
    }
]