[
    {
        "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
        "authors": "Zhongyuan Wang, Zhizheng Zhang, Qi Su, chengchi, Zhoues",
        "link": "https://arxiv.org/abs/2412.04455",
        "github_repo": null,
        "summary": "This paper introduces Code-as-Monitor (CaM), a novel paradigm that leverages vision-language models (VLMs) for both reactive and proactive robotic failure detection. CaM formulates both reactive and proactive failure detection as spatio-temporal constraint satisfaction problems.  Experimental results across three simulators and a real-world setting demonstrate that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% compared to baselines under severe disturbances. CaM is also shown to be integrated with open-loop control policies to form closed-loop systems enabling long-horizon tasks in cluttered scenes with dynamic environments.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://zhoues.github.io/Code-as-Monitor/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
        "authors": "tianbaoxiexxx, ludunjie, ZeonLap, kugwzk, ranpox",
        "link": "https://arxiv.org/abs/2412.04454",
        "github_repo": null,
        "summary": "- AGUVIS, a unified pure vision-based framework, is introduced for building generalizable GUI agents that operate with vision-based observations and a plugin-enabled action system, enhancing cross-platform adaptability.\n- A two-stage training process is employed: first for GUI grounding, followed by planning and reasoning.\n- The model leverages vision-based grounding to improve generalization and reduce inference costs while employing a standardized action space with a plugin system to facilitate consistent learning.\n- Through experiments, AGUVIS surpasses previous state-of-the-art methods on benchmarks like ScreenSpot, Multimodal-Mind2Web, and AndroidControl, achieving the first fully autonomous pure vision GUI agent capable of performing tasks independently.\n- This model demonstrates its efficiency by considerably reducing USD costs and input tokens compared to GPT-40 on Mind2Web-Live.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "A Noise is Worth Diffusion Guidance",
        "authors": "Minjae Kim, Sanghyun Lee, Jiwon Kang, Donghoon Ahn, Min-Jaewon",
        "link": "https://arxiv.org/abs/2412.03895",
        "github_repo": null,
        "summary": " - The paper introduces NoiseRefine, a novel method that enhances the quality of images generated by diffusion models without using guidance techniques like classifier-free guidance (CFG).\n - NoiseRefine refines the initial noise input to the diffusion model by learning a mapping from standard Gaussian noise to a 'guidance-free noise space'.\n - The model uses a multistep score distillation technique to train efficiently and avoids the high computational cost of backpropagation through multiple steps.\n - Experiments show that NoiseRefine achieves comparable image quality to CFG while being significantly faster.\n - The paper analyzes how refined noise contributes to generating high-quality images by investigating the role of different frequency components and demonstrating the effectiveness of low-frequency components in forming layouts.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://cvlab-kaist.github.io/NoiseRefine/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Evaluating Language Models as Synthetic Data Generators",
        "authors": "Seongyun Lee, Vijay Viswanathan, Xiang Yue, Juyoung Suk, seungone",
        "link": "https://arxiv.org/abs/2412.03679",
        "github_repo": null,
        "summary": "- This paper introduces AGORABENCH, a benchmark for evaluating the effectiveness of large language models (LLMs) as synthetic data generators for training other LMs.\n- The benchmark uses standardized settings and a new metric, Performance Gap Recovered (PGR), to compare the quality of synthetic data generated by different LLMs across various tasks and data generation methods.\n- The study finds that an LLM's ability to generate high-quality training data does not necessarily correlate with its problem-solving abilities, but rather with intrinsic properties of the data such as response quality, perplexity, and instruction difficulty.\n- Strategic choices like output format and cost-conscious model selection can significantly impact the effectiveness of data generation, with generating larger datasets from cheaper models sometimes outperforming smaller datasets from more expensive models.\n- The code and data for AGORABENCH are publicly available.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/neulab/data-agora"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "MV-Adapter: Multi-view Consistent Image Generation Made Easy",
        "authors": "Ran Yi, Haoran Wang, pookiefoof, bennyguo, huanngzh",
        "link": "https://arxiv.org/abs/2412.03632",
        "github_repo": null,
        "summary": "- This research presents MV-Adapter, a novel plug-and-play adapter designed to enhance text-to-image (T2I) diffusion models for generating multi-view consistent images. \n- MV-Adapter duplicates self-attention layers to decouple multi-view learning from original model training, preserving prior knowledge and using a parallel attention architecture for integrating multi-view attention and image cross-attention. \n- It incorporates a unified condition guider that encodes camera and geometry information, enabling versatile applications such as text/image-guided 3D generation and texturing. \n- Experimental results demonstrate MV-Adapter's high efficiency, adaptability, and versatility in generating 768-resolution multi-view images on Stable Diffusion XL (SDXL). \n- It achieves a new quality standard, outperforming existing methods in text-to-multiview and image-to-multiview generation tasks in terms of visual fidelity and consistency with conditions.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Text-to-3D",
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/black-forest-labs/flux"
        ],
        "huggingface_urls": [
            "https://huggingface.co/xinsir/controlnet-openpose-sdxl-1.0",
            "https://huggingface.co/xinsir/controlnet-scribble-sdxl-1.0",
            "https://huggingface.co/xinsir/controlnet-tile-sdxl-1.0",
            "https://huggingface.co/TencentARC/t2i-adapter-sketch-sdxl-1.0",
            "https://huggingface.co/h94/IP-Adapter",
            "https://huggingface.co/cagliostrolab/animagine-xl-3.1",
            "https://huggingface.co/goofyai/3d_render_style_xl",
            "https://huggingface.co/JerryOrbachJr/Chalk-Sketch-SDXL",
            "https://huggingface.co/ming-yang/sdxl_chinese_ink_lora",
            "https://huggingface.co/TheLastBen/Papercut_SDXL",
            "https://huggingface.co/ByteDance/SDXL-Lightning",
            "https://huggingface.co/latent-consistency/lcm-sdxl"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "Negative Token Merging: Image-based Adversarial Feature Guidance",
        "authors": "Yejin Choi, Ranjay Krishna, Weijia Shi, Lindsey Li, Jaskirat Singh",
        "link": "https://arxiv.org/abs/2412.01339",
        "github_repo": null,
        "summary": "- NegToMe introduces a training-free method for adversarial guidance in diffusion models using reference images instead of text prompts, enabling finer control over visual concepts.\n- NegToMe enhances output diversity (e.g., race, gender) by minimizing feature similarity between generated images and improves visual dissimilarity to copyrighted material by 34.57% when guided with copyrighted references.\n- The method involves merging tokens in transformer blocks during reverse diffusion, pushing generated image features away from matched features in the reference image.\n- It shows improved diversity while maintaining image quality and prompt alignment across various classifier-free guidance scales and diffusion models, including those lacking native negative prompt support like Flux.\n- It incurs a marginal increase (<4%) in inference time.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://negtome.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Densing Law of LLMs",
        "authors": "Xu Han, Guoyang Zeng, Weilin Zhao, Jie Cai, xcjthu",
        "link": "https://arxiv.org/abs/2412.04315",
        "github_repo": null,
        "summary": "- This paper introduces \"capacity density\" to evaluate the training quality of Large Language Models (LLMs) across different scales, considering both effectiveness and efficiency.\n- Capacity density is calculated as the ratio of a model's effective parameter size (the size a reference model would need to achieve equivalent performance) to its actual parameter size.\n- The paper proposes a two-step process to predict downstream task performance: 1) estimate the relationship between parameter size and language modeling loss and 2) estimate the relationship between loss and downstream task performance using a sigmoid function.\n- An empirical law, the \"Densing Law,\" is revealed, showing that the maximum capacity density of open-source base LLMs exhibits exponential growth, doubling approximately every three months.\n- This trend suggests that future LLM development should prioritize improving capacity density rather than solely increasing parameter size, enabling optimal performance with minimal computational overhead.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
        "authors": "Dianqi Li, Haiping Wu, Jianwei Yang, Jiuhai Chen, zhoutianyi",
        "link": "https://arxiv.org/abs/2412.04424",
        "github_repo": "https://github.com/JiuhaiChen/Florence-VL",
        "summary": "- Florence-VL, a new family of Multimodal Large Language Models (MLLMs), leverages the generative vision foundation model Florence-2 as its visual encoder, enabling it to capture diverse visual features at different levels of detail and under various prompts.\n- A novel Depth-Breadth Fusion (DBFusion) mechanism concatenates visual features from different layers (depth) and under multiple prompts (breadth), providing a rich visual representation to the language model.\n- This model is trained with a two-stage process: end-to-end pretraining on a large image captioning dataset followed by fine-tuning on a diverse set of instruction-tuning datasets.\n- Quantitative analysis and visualization demonstrate improved vision-language alignment compared to models using CLIP or SigLIP encoders. \n- Florence-VL achieves state-of-the-art results across 25 multimodal and vision-centric benchmarks, including VQA, OCR, Chart understanding, and knowledge-based reasoning tasks, outperforming other advanced MLLMs like Cambrian.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/JiuhaiChen/Florence-VL"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
        "authors": "Yuqi Zhang, Bin Yan, Yi Jiang, Jinlai Liu, Jian Han",
        "link": "https://arxiv.org/abs/2412.04431",
        "github_repo": null,
        "summary": "- Infinity is a bitwise visual autoregressive model for high-resolution image synthesis.\n- It uses a bitwise token prediction framework with an infinite-vocabulary tokenizer and classifier and bitwise self-correction mechanism.\n- It outperforms top-tier diffusion models like SD3-Medium and SDXL on benchmarks like GenEval, ImageReward, and HPSv2.1, achieving a win rate of 66%.\n- Infinity generates a 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium.\n- The model exhibits strong scaling capabilities by increasing the image tokenizer vocabulary size and the corresponding transformer size.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/FoundationVision/Infinity"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Towards Universal Soccer Video Understanding",
        "authors": "Yanfeng Wang, Ya Zhang, Hao Jiang, haoningwu, Homie0609",
        "link": "https://arxiv.org/abs/2412.01820",
        "github_repo": null,
        "summary": "- This paper introduces MatchVision, a novel visual-language foundation model designed for comprehensive soccer video understanding.\n- The model leverages a spatiotemporal attention mechanism inspired by TimeSformer, which allows it to effectively capture dynamic information within soccer videos.\n- MatchVision is trained on SoccerReplay-1988, a new dataset containing 1988 full soccer matches with rich annotations, alongside existing datasets like SoccerNet, that is significantly larger and more diverse.\n- The paper benchmarks MatchVision on various downstream tasks such as event classification, commentary generation, and foul recognition.\n- Experimental results demonstrate state-of-the-art performance across multiple benchmarks, highlighting the model's effectiveness and the value of the new dataset.",
        "classification": [
            "Computer Vision",
            "Video Classification",
            "Text Generation",
            "Multimodal"
        ],
        "github_urls": [
            "https://jyrao.github.io/UniSoccer/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing",
        "authors": "Juncheng Li, Xiangtai Li, Ling Yang, WeiChow, BryanW",
        "link": "https://arxiv.org/abs/2412.04280",
        "github_repo": "https://github.com/viiika/HumanEdit",
        "summary": "- This paper introduces HumanEdit, a new high-quality dataset for instruction-guided image editing created with extensive human input to improve alignment with user preferences, addressing a gap in current datasets that rely heavily on automatic generation.\n- HumanEdit consists of 5,751 high-resolution image pairs, each with an editing instruction, description, and mask, covering six instruction types (Action, Add, Counting, Relation, Remove, and Replace).\n- A four-stage annotation pipeline involving tutorials, image selection, instruction/edit generation, and administrator review with human feedback ensures data quality.\n- HumanEdit supports both masked and mask-free editing and has superior diversity and image resolution compared to existing datasets.\n- Benchmark results on HumanEdit across various baselines reveal that while methods generally perform well on Add and Remove tasks, improvements are needed for more complex instructions and consistency in aligning with human preferences.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/viiika/HumanEdit"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/BryanW/HumanEdit"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "Personalized Multimodal Large Language Models: A Survey",
        "authors": "Zhehao Zhang, Yu Xia, Hanjia Lyu, Junda Wu, Franck-Dernoncourt",
        "link": "https://arxiv.org/abs/2412.02142",
        "github_repo": null,
        "summary": "- This paper presents a comprehensive survey of personalized multimodal large language models (MLLMs), examining their architectures, training methods, and applications.\n- The authors propose a taxonomy for categorizing techniques used to personalize MLLMs, focusing on instruction, alignment, generation, and fine-tuning methods.\n- The survey also summarizes applications of personalized MLLMs, including text generation, image generation, recommendation, and retrieval tasks.\n- They also present an overview of commonly used datasets and evaluation metrics for personalized MLLMs.\n- Finally, the authors highlight key open challenges in the field, including benchmarking, evaluation metrics, diverse modalities, modality fusion, and theoretical foundations.",
        "classification": [
            "Multimodal",
            "Text Generation",
            "Image-to-Text",
            "Image-to-Image",
            "Text2Text Generation",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality",
        "authors": "Hong Zhou, Shaoxuan He, Yuanyu He, Feng Chen, Yefei He",
        "link": "https://arxiv.org/abs/2412.04062",
        "github_repo": null,
        "summary": "- ZipAR is a training-free, plug-and-play parallel decoding framework designed to accelerate auto-regressive visual generation by exploiting the spatial locality in images and decoding spatially adjacent tokens in parallel.\n- Unlike traditional next-token prediction, which processes tokens sequentially, ZipAR introduces \"next-set prediction\" to decode tokens from different rows concurrently based on a local window size that defines the spatial adjacency.\n- This reduces the number of forward passes required, significantly improving generation efficiency without needing additional training or model modifications.\n- Experiments demonstrate up to a 91% reduction in forward passes on Emu3-Gen while maintaining image quality.\n- ZipAR is compatible with various auto-regressive visual generation models, including LlamaGen and Lumina-mGPT, showing consistent improvements across different models and tasks.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities",
        "authors": "Yanfeng Wang, Weidi Xie, Ya Zhang, Ziheng Zhao, haoningwu",
        "link": "https://arxiv.org/abs/2412.04106",
        "github_repo": null,
        "summary": "- MRGen, a diffusion-based data engine, synthesizes medical images for unannotated modalities, enabling training of segmentation models on these modalities. \n- MRGen leverages MedGen-1M, a large-scale curated dataset containing CT and MRI images with modality labels, attributes, regions, organ information, and a subset of masks. \n- MRGen is trained in two stages: text-guided pretraining on MedGen-1M and mask-conditioned fine-tuning on a subset with masks.\n- MRGen takes text prompts and organ masks to generate synthetic medical images for segmentation training.\n- Experimental results demonstrate MRGen effectively generates high-quality MR images and boosts segmentation performance on unannotated modalities, outperforming existing image generation techniques like CycleGAN and DualNorm.",
        "classification": [
            "Image Segmentation",
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
        "authors": "Jian Gang Ngui, David I. Adelani, Cl\u00e9mentine Fourrier, Angelika Romanou, Shivalika Singh",
        "link": "https://arxiv.org/abs/2412.03304",
        "github_repo": null,
        "summary": "- This paper introduces Global-MMLU, a 42-language multilingual multi-domain question-answering dataset designed to address cultural and linguistic biases in MMLU.\n- It includes culturally sensitive (CS) and culturally agnostic (CA) subsets, allowing for more nuanced evaluations.\n- The creation involved professional and community annotators for translation and post-editing, expanding language coverage and improving translation quality.\n- The paper also quantifies the impact of cultural biases, with analysis revealing that 28% of MMLU questions require culturally specific knowledge and a disproportionate focus on Western cultures.\n- State-of-the-art model evaluations on Global-MMLU highlight the impact of these biases on performance rankings, advocating for reporting performance on CS and CA subsets separately to provide a more comprehensive understanding of model capabilities across cultures.",
        "classification": [
            "Question Answering",
            "Translation",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/datasets/CohereForAI/Global-MMLU"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "Monet: Mixture of Monosemantic Experts for Transformers",
        "authors": "Jaewoo Kang, Kee-Eung Kim, Young Jin Ahn, affjljoo3581",
        "link": "https://arxiv.org/abs/2412.04139",
        "github_repo": "https://github.com/dmis-lab/Monet",
        "summary": " - This paper introduces MONET, a novel Mixture-of-Experts (MoE) architecture designed to enhance the mechanistic interpretability of large language models (LLMs) by addressing the issue of polysemanticity.\n- MONET incorporates sparse dictionary learning directly into end-to-end MoE pretraining, enabling the scaling of the expert count to 262,144 per layer while maintaining parameter efficiency.\n- The model's performance is evaluated across various benchmarks, showing competitive results with dense LLMs while offering superior knowledge manipulation capabilities.\n- Through qualitative and quantitative analyses, MONET demonstrates mutual exclusivity of knowledge across experts, enabling robust knowledge manipulation without performance degradation.\n- The code and pretrained checkpoints for MONET are publicly available.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/dmis-lab/Monet"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
        "authors": "Yusuke Kato, Zichun Liao, Akash Gokul, Konstantinos Kallidromitis, Shufan Li",
        "link": "https://arxiv.org/abs/2412.01169",
        "github_repo": "https://github.com/jacklishufan/OmniFlows",
        "summary": "- OmniFlow is a novel generative model designed for any-to-any generation tasks, using a modular architecture inspired by Stable Diffusion 3's MMDiT.\n- It extends the rectified flow (RF) framework to handle multiple modalities (text, image, audio) jointly and introduces a novel guidance mechanism for flexible control over modality interaction in generated outputs.\n- OmniFlow outperforms previous any-to-any models on various tasks, achieving competitive performance with state-of-the-art specialist models in text-to-image and text-to-audio generation.\n- Its modular design allows individual component pretraining and merging with pretrained single-task models, reducing training resource requirements compared to training from scratch.\n- Evaluations show significant improvements over existing any-to-any models in image quality, text-image alignment, and CLIP scores, particularly on the GenEval benchmark.",
        "classification": [
            "Any-to-Any",
            "Multimodal",
            "Text-to-Image",
            "Text-to-Audio"
        ],
        "github_urls": [
            "https://github.com/jacklishufan/OmniFlows"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "Discriminative Fine-tuning of LVLMs",
        "authors": "Ioannis Maniadis Metaxas, Anestis Zaganidis, Alexandros Xenos, Adrian Bulat, Yassine Ouali",
        "link": "https://arxiv.org/abs/2412.04378",
        "github_repo": null,
        "summary": "- This paper introduces VladVA, a novel training approach for discriminative fine-tuning of Large Vision-Language Models (LVLMs).\n- VladVA converts a generative LVLM into a discriminative one by employing both contrastive and next-token prediction losses on image-text pairs with varying lengths and granularities.\n- The approach uses a parameter-efficient adaptation method that involves soft prompting and LoRA, thereby allowing for effective training on smaller datasets with limited compute.\n- On standard image-text retrieval benchmarks, VladVA shows significant improvement, achieving gains from +4.7% to +7.0% in absolute terms over similarly sized state-of-the-art CLIP-like models.\n- Additionally, the model demonstrates notable gains in compositionality tasks, showing improved language understanding over the standard two-tower image-text models.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Text-to-Image",
            "Zero-Shot Image Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-06"
    },
    {
        "title": "KV Shifting Attention Enhances Language Modeling",
        "authors": "Weipeng Chen, Bingning Wang, Wei Cheng, xumingyu16",
        "link": "https://arxiv.org/abs/2411.19574",
        "github_repo": null,
        "summary": "- This paper introduces KV shifting attention, a novel attention mechanism designed to improve the efficiency of induction heads in large language models (LLMs).\n- KV shifting attention decouples keys and values in the attention mechanism, reducing the depth and width requirements for induction heads, enabling single-layer transformers to perform induction tasks effectively.\n- Theoretical analysis and empirical validation demonstrate that KV Shifting attention achieves comparable or superior performance to conventional multi-layer transformers in language modeling tasks.\n- The authors apply KV shifting attention to large language pre-training models with up to 19B parameters and show improved performance and faster convergence compared to baseline models using standard attention mechanisms.\n- KV shifting attention introduces a bias towards learning induction, which is beneficial for language modeling across diverse model scales.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/xumingyu16/Baseline_2.9B",
            "https://huggingface.co/xumingyu16/KV_shifting_2.9B"
        ],
        "date": "2024-12-06"
    },
    {
        "title": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models",
        "authors": "Zhichao Liao, Fulong Ye, Pengze Zhang, Qichao Sun, Crayon-Shinchan",
        "link": "https://arxiv.org/abs/2412.04146",
        "github_repo": null,
        "summary": "- AnyDressing is a novel multi-garment virtual dressing method that generates customized character images based on any combination of garments and personalized text prompts.\n- The model architecture consists of two primary networks: GarmentsNet, which extracts detailed clothing features using a Garment-Specific Feature Extractor, and DressingNet, which integrates these features for virtual dressing using a Dressing-Attention module and an Instance-Level Garment Localization Learning strategy.\n- A Garment-Enhanced Texture Learning strategy is also incorporated to improve fine-grained texture details of garments in synthesized images.\n- AnyDressing outperforms existing methods by demonstrating superior consistency in clothing style and texture, better text fidelity, and effective handling of background contamination and garment confusion in multi-garment dressing scenarios.\n- The model can also be used as a plugin, compatible with other extensions such as ControlNet, IP-Adapter, and LoRA, enhancing its versatility in diverse applications.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/crayon-shinchan/AnyDressing"
        ],
        "huggingface_urls": [],
        "date": "2024-12-06"
    }
]