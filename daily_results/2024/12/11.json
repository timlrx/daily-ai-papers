[
    {
        "title": "Evaluating and Aligning CodeLLMs on Human Preference",
        "authors": "JustinLin610, huybery, misakamage, instro, jx-yang",
        "link": "https://arxiv.org/abs/2412.05210",
        "github_repo": null,
        "summary": "- This paper introduces CodeArena, a new human-curated benchmark for evaluating code large language models (LLMs) based on human preference, addressing the gap between code correctness and user satisfaction.\n- CodeArena contains 397 diverse samples across 40 coding categories and 44 programming languages, focusing on real-world coding scenarios.\n- In addition, the authors create SynCode-Instruct, a large-scale (20B token) synthetic instruction dataset generated by scaling instructions from web sources and generating corresponding code snippets with unit tests where applicable. Fine-tuning Qwen2.5-Coder using SynCode-Instruct leads to state-of-the-art performance for open-source code LLMs. \n- Evaluations of 40+ LLMs on CodeArena show significant differences compared to execution-based benchmarks and a large performance gap between open-source and closed-source LLMs.\n- This work also shows that fine-tuning with larger synthetic instruction data sets improves code generation and benchmark performance in code LLMs.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://codearenaeval.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation",
        "authors": "Chao Tang, LXT, zengyh1900, JingboWang, jianzongwu",
        "link": "https://arxiv.org/abs/2412.07589",
        "github_repo": null,
        "summary": "- DiffSensei, a novel framework for generating customized manga panels, bridges Multi-Modal Large Language Models (MLLMs) with diffusion-based image generators, enabling dynamic multi-character control and narrative consistency.\n- The framework uses an MLLM as a text-compatible identity adapter, allowing character features to adjust based on panel captions and ensuring layout control through masked cross-attention injection and dialog embedding.\n- A new large-scale dataset, MangaZero, containing 43,264 manga pages and 427,147 annotated panels, supports the training and evaluation of models on customized manga generation.\n- Experimental results show DiffSensei outperforms existing story visualization models in character consistency, layout controllability, and text adherence, marking a significant advancement in manga generation by enabling text-adaptable character customization.\n- A human preference study further validates DiffSensei\u2019s superior performance, highlighting its capability to create coherent and expressive manga panels.",
        "classification": [
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "STIV: Scalable Text and Image Conditioned Video Generation",
        "authors": "jefflai, JesseAllardice, tsujuifu, wenzehu, Jiasenlu",
        "link": "https://arxiv.org/abs/2412.07730",
        "github_repo": null,
        "summary": "- STIV (Scalable Text and Image Conditioned Video Generation) is a new text- and image-conditioned video generation model based on a Diffusion Transformer (DiT) architecture, integrating image conditions via frame replacement and text conditions through a joint image-text conditional classifier-free guidance.\n- This design enables STIV to handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks concurrently, and it's adaptable to various applications like video prediction and frame interpolation.\n- The 8.7B parameter STIV model achieves state-of-the-art performance on VBench benchmarks, scoring 83.1 on T2V and 90.1 on TI2V at 512x512 resolution, outperforming existing models like CogVideoX-5B, Pika, Kling, and Gen-3.\n- A progressive training approach involving text-to-image and text-to-video stages, alongside techniques like QK-norm, sandwich-norm, and MaskDiT, enhances stability and efficiency during model scaling.\n- The model utilizes flow matching as a training objective and incorporates rotary positional embeddings, micro-conditions, and a novel joint image-text classifier-free guidance strategy to improve performance and address motion-related issues.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Hidden in the Noise: Two-Stage Robust Watermarking for Images",
        "authors": "Niv Cohen, chegde, rtealwitter, penfever, kasraarabi",
        "link": "https://arxiv.org/abs/2412.04653",
        "github_repo": null,
        "summary": "- This paper introduces WIND, a two-stage robust watermarking method for images generated by diffusion models.\n- WIND leverages the initial noise of diffusion models as a distortion-free watermark and augments it with generated Fourier patterns to embed group identifiers, enabling efficient detection.\n- The two-stage framework reduces the search space during detection, increasing efficiency while maintaining robustness.\n- Experimental results demonstrate that WIND achieves state-of-the-art robustness to forgery and removal attacks compared to existing methods like Tree-Ring and RingID.\n- Additionally, the paper outlines a strategy to apply the watermarking method to non-synthetic images using diffusion inpainting.",
        "classification": [
            "Computer Vision",
            "Text-to-Image",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics",
        "authors": "Yuqian Zhou, He Zhang, Zhifei Zhang, jimmie33, xichenhku",
        "link": "https://arxiv.org/abs/2412.07774",
        "github_repo": null,
        "summary": "- UniReal is a unified framework based on a diffusion transformer model, designed for various image generation and editing tasks using a \"discontinuous\" frame generation approach, effectively treating diverse input and output images as pseudo video frames.\n- It leverages a hierarchical prompting scheme with context and image prompts to guide image generation and editing under a shared text prompt, enabling better coordination between tasks and reducing ambiguity from mixed data sources.\n- The model utilizes universal supervision from large-scale video data by treating pairs of discontinuous video frames with captions as instructive editing data, and automatically constructing data for other image tasks from video datasets. \n- UniReal surpasses state-of-the-art methods in various tasks such as instructive image editing and customized image generation, demonstrating its effectiveness in following instructions and maintaining image fidelity.\n- Evaluations on DreamBench, EMU Edit, and MagicBrush demonstrate UniReal\u2019s superior capabilities in detail preservation, alignment with user intent, and overall image quality across a wide range of image generation and editing tasks.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Image Segmentation",
            "Image-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
        "authors": "conghui, friskit, Liam-Liu, wanderkid, ouyanglinke",
        "link": "https://arxiv.org/abs/2412.07626",
        "github_repo": "https://github.com/opendatalab/OmniDocBench",
        "summary": "- OmniDocBench, a new multi-source benchmark for automated document content extraction, addresses limitations in existing methods regarding document diversity and comprehensive evaluation.\n- The benchmark includes a high-quality evaluation dataset with nine diverse document types and a flexible framework with 19 layout and 14 attribute labels for multi-level assessments.\n- An exhaustive comparison of modular pipelines and end-to-end methods highlights their limitations in handling document diversity.\n- Evaluations demonstrate that pipeline tools outperform general VLMs on common documents, while VLMs show better generalization on specialized data and robustness on pages with challenging attributes.\n- OmniDocBench sets a robust and fair standard, enabling fairer comparisons and guiding future development in document parsing.",
        "classification": [
            "Computer Vision",
            "Document Question Answering"
        ],
        "github_urls": [
            "https://github.com/opendatalab/OmniDocBench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models",
        "authors": "myownskyW7, guandao, Dubhe-zmc, justimyhxu, tongwu2020",
        "link": "https://arxiv.org/abs/2412.07674",
        "github_repo": null,
        "summary": "- This research introduces FiVA (Fine-grained Visual Attribute Dataset), a novel dataset designed to improve controllable image generation with text-to-image diffusion models by focusing on fine-grained visual attributes.\n- The FiVA dataset features a well-organized taxonomy of visual attributes and contains around 1 million generated images with corresponding attribute annotations.\n- FiVA-Adapter, a fine-grained visual attribute adaptation framework is introduced, which enables transferring visual attributes from multiple source images into a generated target image.\n- FiVA-Adapter includes a multimodal encoder (Q-former) to handle attribute tag instructions within the diffusion process, allowing for more precise control and customization of visual elements.\n- Experimental results demonstrate the superior performance of this method in extracting specific visual attributes, high textual alignment, and the capability of combining different attributes, as compared to baseline methods.",
        "classification": [
            "Text-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/FiVA/FiVA"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
        "authors": "Dongping Chen, Ethan Shen, Cheng-Yu Hsieh, Zelun Luo, Mahtab Bigverdi",
        "link": "https://arxiv.org/abs/2412.03548",
        "github_repo": null,
        "summary": "- This research introduces Perception Tokens, intrinsic image representations designed to improve visual reasoning abilities in Multimodal Language Models (MLMs).\n- Perception tokens, such as depth maps and bounding boxes, are incorporated as intermediate reasoning steps in the MLM's chain-of-thought process using a novel training method called AURORA.\n- AURORA leverages a VQVAE to transform visual representations into tokens and trains the MLM in a multi-task framework with a curriculum learning strategy.\n- The proposed LLaVA-AURORA model demonstrates significant improvement over fine-tuning baselines on benchmarks including BLINK, CVBench, and SEED-Bench for depth estimation and object counting tasks.\n- Experimental results exhibit the effectiveness of Perception Tokens, resulting in a 6.4% boost for relative depth estimation on BLINK and up to 11.3% improvement on object counting across datasets.",
        "classification": [
            "Multimodal",
            "Computer Vision",
            "Depth Estimation",
            "Object Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Video Motion Transfer with Diffusion Transformers",
        "authors": "Sergey Tulyakov, fabvio, philiptorr, aliaksandr-siarohin, alexpondaven",
        "link": "https://arxiv.org/abs/2412.07776",
        "github_repo": null,
        "summary": "- DiTFlow is a novel method for transferring motion from a reference video to a newly synthesized one specifically designed for Diffusion Transformers (DiTs).\n- It leverages the global attention mechanism of DiTs to extract motion patterns across video frames, which are represented as Attention Motion Flows (AMFs).\n- DiTFlow guides the latent denoising process of the DiT model by optimizing the latent representation of the video such that its AMF matches the AMF of the reference video. \n- The method also enables zero-shot motion transfer by optimizing positional embeddings within the DiT.\n- Experimental results on the DAVIS dataset using CogVideoX demonstrate that DiTFlow outperforms existing motion transfer methods across multiple metrics, including motion fidelity and image quality, and also in human evaluation.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/snap-research/DiTFlow"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Pondaven/DiTFlow"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "EMOv2: Pushing 5M Vision Model Frontier",
        "authors": "Zhucun Xue, Teng Hu, Jiangning Zhang, LXT, hhy724",
        "link": "https://arxiv.org/abs/2412.06674",
        "github_repo": "https://github.com/zhangzjn/EMOv2",
        "summary": "- EMOv2, a family of lightweight vision models, introduces an Improved Inverted Residual Mobile Block (i2RMB) and a parameter-sharing spanning attention mechanism for enhanced efficiency and performance in dense prediction tasks.\n- The i2RMB builds upon the Meta Mobile Block (MMBlock), a unified abstraction of Inverted Residual Blocks (IRBs), Multi-Head Self-Attention (MHSA), and Feed-Forward Networks (FFNs), simplifying architecture design.\n- EMOv2 achieves state-of-the-art accuracy on various tasks, including image recognition, dense prediction, and image generation, outperforming models with similar parameter counts.\n- EMOv2-5M achieves 79.4 Top-1 accuracy on ImageNet-1K, exceeding comparable CNN- and attention-based models, and a 41.5 mAP on object detection surpassing previous EMO-5M by +2.6. \n- With a robust training strategy, EMOv2-5M attains 82.9 Top-1 accuracy, establishing a new benchmark for 5M magnitude models.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Object Detection",
            "Image Segmentation",
            "Video Classification",
            "Unconditional Image Generation"
        ],
        "github_urls": [
            "https://github.com/zhangzjn/EMOv2"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Granite Guardian",
        "authors": "Tejaswini Pedapati, Subhajit Chaudhury, Manish Nagireddy, Inkit Padhi, Giandomenico",
        "link": "https://arxiv.org/abs/2412.07724",
        "github_repo": "https://github.com/ibm-granite/granite-guardian",
        "summary": "- This paper introduces Granite Guardian, a suite of safeguard models (2B and 8B parameter sizes) designed for comprehensive risk detection in Large Language Models, addressing prompt and response risks related to social biases, security (jailbreaking), and RAG-specific issues like context relevance, groundedness, and answer relevance.\n- The models are trained on a combined dataset of human-annotated data from diverse sources and synthetic data generated to specifically cover adversarial attacks and RAG hallucinations. \n- Evaluation on standard benchmarks like the OpenAI Moderation Evaluation Dataset, HarmBench, and ToxicChat show that Granite Guardian outperforms existing open-source models on key metrics like AUC, F1, and recall.\n- On RAG hallucination benchmarks (groundedness), Granite Guardian achieves an average AUC of 0.854 on the TRUE dataset and performs competitively with dedicated models explicitly trained for groundedness detection.\n- The models are released open-source to encourage community adoption and promote responsible development of safer LLM applications.",
        "classification": [
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [
            "https://github.com/ibm-granite/granite-guardian"
        ],
        "huggingface_urls": [
            "https://huggingface.co/ibm-granite/granite-guardian-hap-38m"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
        "authors": "Jianhua Han, Runhui Huang, Junwei Yang, Guansong Lu, Chunwei Wang",
        "link": "https://arxiv.org/abs/2412.06673",
        "github_repo": null,
        "summary": "- ILLUME, a unified multimodal large language model (MLLM), seamlessly integrates understanding and generation capabilities using a next-token prediction approach.\n- It incorporates a semantic vision tokenizer and a progressive multi-stage training to enhance data efficiency, requiring only 15M image-text pairs for pretraining.\n- ILLUME introduces a self-enhancing multimodal alignment scheme where the model assesses consistency between generated images and text descriptions for synergistic improvement.\n- Experiments show ILLUME competing with state-of-the-art unified and specialized MLLMs across visual understanding, generation, and editing.\n- It outperforms previous best models on several benchmarks by significant margins, like a 25% improvement on MMMU and 14% on SEED.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "ObjCtrl-2.5D: Training-free Object Control with Camera Poses",
        "authors": "Chen Change Loy, Shangchen Zhou, Yushi Lan, Zhouxia Wang",
        "link": "https://arxiv.org/abs/2412.07721",
        "github_repo": null,
        "summary": "- ObjCtrl-2.5D is a training-free object motion control method for text-to-video generation that leverages 3D trajectories derived from 2D trajectories and depth maps, converting them into camera poses to guide object motion.\n- The approach models object movement as camera motion to leverage existing Camera Motion Control T2V (CMC-T2V) models without requiring additional training. \n- A Layer Control Module isolates object and background layers to apply distinct camera poses, enabling object-specific motion control.\n- Shared Warping Latent enhances control precision by sharing low-frequency latent information across frames within the warped object regions.\n- ObjCtrl-2.5D outperforms existing training-free methods in control accuracy and supports complex object motion like rotation, showing potential for advanced control in 3D video generation, but its performance is still behind training-based methods.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation",
        "authors": "Menghan Xia, Sida Peng, Xintao Wang, Xian Liu, lemonaddie",
        "link": "https://arxiv.org/abs/2412.07759",
        "github_repo": null,
        "summary": "- 3DTrajMaster is a novel approach for manipulating multi-entity 3D motions in video generation using entity-specific 6DoF pose sequences as input, leveraging a plug-and-play 3D-motion grounded object injector.\n- The object injector fuses entity descriptions and trajectories into latent embeddings, which are then combined and fed into a gated self-attention layer for motion fusion, preserving the video diffusion prior and generalizing to diverse entities and trajectories.\n- A new 360\u00b0-Motion Dataset is constructed using UE rendering, correlating 3D human and animal assets with GPT-generated trajectories and capturing motion with 12 cameras, addressing the lack of suitable training data.\n- A domain adaptor and annealed sampling strategy mitigate video quality degradation during training and inference, respectively.\n- Experimental results demonstrate state-of-the-art accuracy and generalization for controlling multi-entity 3D motions, outperforming existing methods in pose accuracy and handling complex scenarios like 3D occlusions.",
        "classification": [
            "Text-to-Video",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation",
        "authors": "Kazuhiro Fukui, Erica K. Shimomoto, Lincon S. Souza, Pedro H. V. Valois",
        "link": "https://arxiv.org/abs/2412.07334",
        "github_repo": "https://github.com/phvv-me/frame-representation-hypothesis.git",
        "summary": "- This paper introduces the Frame Representation Hypothesis (FRH), a new framework for interpreting and controlling Large Language Models (LLMs) by modeling multi-token words as frames, which are ordered sequences of independent vectors.\n- FRH extends the Linear Representation Hypothesis (LRH) which was limited to single-token words, and makes it applicable to multi-token words and thus any textual data.\n- The paper proposes Concept Frames, which are centroids of a set of word frames that share a common concept, and shows that over 99% of words composed of several tokens are composed of linearly independent token vectors.\n- A Top-k Concept-Guided Decoding method is introduced for steering text generation using chosen concepts, which helps to expose and potentially remediate biases present in LLMs.\n- Experiments on Llama 3.1, Gemma 2, and Phi 3 demonstrate FRH's ability to expose biases and steer text generation, showing its applicability to enhance the transparency and controllability of LLMs.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/phvv-me/frame-representation-hypothesis.git"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation",
        "authors": "Umberto Michieli, Pietro Zanuttigh, Mete Ozay, obohdal, donaldssh",
        "link": "https://arxiv.org/abs/2412.05148",
        "github_repo": null,
        "summary": "- LoRA.rar introduces a novel method for merging subject and style LoRAs for personalized image generation using a small (0.5M parameter) hypernetwork.\n- The hypernetwork is trained on a dataset of content and style LoRA pairs and predicts merging coefficients in real-time for new, unseen pairs, eliminating the need for computationally expensive optimization used by other methods like ZipLoRA.\n- A new evaluation metric, MARS2, based on Multimodal Large Language Models (MLLMs) is introduced to address the limitations of existing metrics for evaluating content-style fidelity.\n- LoRA.rar outperforms existing merging strategies and ZipLoRA in subject-style personalization as measured by MARS2 and human evaluations.\n- The method achieves a significant speedup of over 4000x in the merging process compared to optimization-based methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Fully Open Source Moxin-7B Technical Report",
        "authors": "Sung-En Chang, Yixin Shen, Zhenglun Kong, Xuan Shen, Pu Zhao",
        "link": "https://arxiv.org/abs/2412.06845",
        "github_repo": null,
        "summary": "- Moxin-7B is a fully open-source large language model (LLM) based on the Mistral architecture, extended to 36 blocks from 32, and trained on over 2 trillion tokens.\n- It incorporates grouped-query attention (GQA), sliding window attention (SWA), and a rolling buffer cache for efficient long-context handling (up to 32K tokens).\n- The model was trained in three phases: initial pre-training with 2k and 4k context lengths and a final capability enhancement phase using curated data from Hugging Face and evaluation benchmarks.\n- Evaluation on standard benchmarks like ARC, HellaSwag, MMLU, Winogrande, and PIQA demonstrates superior zero-shot performance against other 7B models and competitive results in few-shot settings.\n- The chat model variant, Moxin-7B-chat, outperforms baselines on MTBench, showcasing strong alignment capabilities.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/moxin-org/Moxin-LLM"
        ],
        "huggingface_urls": [
            "https://huggingface.co/moxin-org/moxin-llm-7b",
            "https://huggingface.co/moxin-org/moxin-chat-7b"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation",
        "authors": "Felice Dell'Orletta, Marco Avvenuti, Amaury Trujillo, Alessio Miaschi, Lorenzo Cima",
        "link": "https://arxiv.org/abs/2412.07338",
        "github_repo": null,
        "summary": "- This paper proposes and evaluates strategies for generating contextualized and personalized counterspeech using a LLaMA2-13B model.\n- The model is instructed to generate counterspeech by leveraging contextual information such as community, conversation details, and user history.\n- The study finds that contextualized counterspeech significantly outperforms generic counterspeech in adequacy and persuasiveness.\n-  A mixed-design crowdsourcing experiment revealed a poor correlation between quantitative evaluation metrics and human evaluations. \n- This highlights the importance of human evaluation in assessing nuanced aspects of generated content like artificiality.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/dfurman/Llama-2-13B-Instruct-v0.2"
        ],
        "date": "2024-12-11"
    },
    {
        "title": "Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment",
        "authors": "Jitendra Malik, Masayoshi Tomizuka, Chenfeng Xu, Yilin Wu, Ran Tian",
        "link": "https://arxiv.org/abs/2412.04835",
        "github_repo": null,
        "summary": "- This paper introduces Representation-Aligned Preference-based Learning (RAPL), a method for efficiently aligning visuomotor robot policies with end-user preferences using minimal human feedback.\n- RAPL focuses on aligning the robot's visual representation with the human's, rather than relying on costly reinforcement learning from human feedback.\n- In simulation, RAPL achieves comparable performance to ground truth and outperforms baselines on both standard and cluttered manipulation tasks. \n- Hardware experiments show RAPL aligns a diffusion-based policy with 5x less real human preference data than traditional RLHF. \n- RAPL also demonstrates strong zero-shot generalization across different robot embodiments.",
        "classification": [
            "Robotics",
            "Reinforcement Learning",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "Chimera: Improving Generalist Model with Domain-Specific Experts",
        "authors": "Renrui Zhang, Renqiu Xia, Hongbin Zhou, Mingsheng Li, Tianshuo Peng",
        "link": "https://arxiv.org/abs/2412.05983",
        "github_repo": null,
        "summary": "- Chimera, a new multimodal pipeline, enhances Large Multi-modal Models (LMMs) with domain-specific experts to improve performance on specialized tasks like multimodal reasoning and visual content extraction.\n- The model integrates multiple expert encoders into a single LMM using a progressive training strategy and a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism to address optimization imbalances.\n- Chimera achieves state-of-the-art performance on MathVista and MathVerse, outperforming comparable-scale LMMs and specialized expert models.\n- The model also excels in visual structural extraction tasks for charts, tables, and documents, achieving near-specialist-level results on benchmarks like ChartQA-SE, Table-SE, and Doc-SE.\n- The authors plan to open-source Chimera and the training datasets to facilitate future research on LMMs.",
        "classification": [
            "Multimodal",
            "Question Answering",
            "Table Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-11"
    },
    {
        "title": "A New Federated Learning Framework Against Gradient Inversion Attacks",
        "authors": "Weihong Ren, Xiaodan Zhang, Wenhao Chen, Shuang Zeng, gpx333",
        "link": "https://arxiv.org/abs/2412.07187",
        "github_repo": "https://github.com/Pengxin-Guo/HyperFL",
        "summary": "- This paper introduces HyperFL, a novel federated learning framework designed to protect against gradient inversion attacks (GIA) without relying on existing defense mechanisms like SMC, HE, or DP.\n- HyperFL employs a dual-pronged approach: network decomposition (splitting the model into a shared feature extractor and a private classifier) and hypernetwork sharing (using a hypernetwork to generate the feature extractor's parameters, with only the hypernetwork's parameters being shared).\n- Two configurations of HyperFL are presented: the main configuration for simpler tasks and HyperFL-LPM, which utilizes pre-trained models as fixed feature extractors and generates trainable adapter parameters via a hypernetwork for complex tasks.\n- Theoretical analysis demonstrates the convergence of HyperFL, and experimental results showcase its privacy-preserving capability and competitive performance compared to FedAvg.\n- HyperFL outperforms DP-based methods in terms of accuracy while providing comparable privacy protection, indicating a more favorable privacy-utility trade-off.",
        "classification": [
            "Computer Vision",
            "Image Classification"
        ],
        "github_urls": [
            "https://github.com/Pengxin-Guo/HyperFL"
        ],
        "huggingface_urls": [],
        "date": "2024-12-11"
    }
]