[
    {
        "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
        "authors": "Rui Qian, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Pan Zhang",
        "link": "https://arxiv.org/abs/2412.09596",
        "github_repo": "https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive",
        "summary": "- InternLM-XComposer2.5-OmniLive (IXC2.5-OL) is a multimodal system designed for real-time interaction with streaming video and audio inputs, addressing the limitations of current Multimodal Large Language Models (MLLMs) in continuous perception, memory, and reasoning.\n- IXC2.5-OL consists of three modules: a Streaming Perception Module processing multimodal information, a Multi-modal Long Memory Module integrating and retrieving short-term and long-term memories, and a Reasoning Module handling queries.\n- The system simulates human-like cognition by disentangling streaming perception, reasoning, and memory mechanisms, allowing simultaneous processing of information.\n- Evaluation across audio and video benchmarks demonstrates IXC2.5-OL's superior performance, achieving state-of-the-art results on StreamingBench for real-time video interactions and competitive results on other benchmarks like MLVU and Video-MME.\n- IXC2.5-OL excels in real-time video interactions while demonstrating competitive results among open-source models on several audio and video understanding benchmarks.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Audio",
            "Automatic Speech Recognition"
        ],
        "github_urls": [
            "https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Phi-4 Technical Report",
        "authors": "Ronen Eldan, S\u00e9bastien Bubeck, Harkirat Behl, Jyoti Aneja, Marah Abdin",
        "link": "https://arxiv.org/abs/2412.08905",
        "github_repo": null,
        "summary": "- Phi-4 is a 14-billion parameter language model trained with an emphasis on data quality, incorporating synthetic data generated by diverse techniques like multi-agent prompting and instruction reversal.\n- Phi-4 surpasses its teacher model (GPT-4) on STEM-focused QA, demonstrating that the data generation and post-training techniques provide capabilities beyond distillation.\n- The training recipe focuses on three pillars: synthetic data generation, curation of high-quality organic data, and innovative post-training techniques like rejection sampling and a new approach to Direct Preference Optimization (DPO).\n- Phi-4's performance on reasoning tasks is comparable to or surpasses larger models, exceeding Llama-3.1 on benchmarks like GPQA and MATH, and scoring high on the November 2024 AMC math competitions, indicating robust reasoning abilities and lack of overfitting.\n- Post-training includes supervised fine-tuning (SFT), pivotal token search-based DPO, and judge-guided DPO, refining the model's alignment with human preferences, improving reasoning, safety, robustness, and mitigating hallucinations.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
        "authors": "Willie Neiswanger, Jinyi Hu, Tianyu Yu, Ollie Liu, jrzhang",
        "link": "https://arxiv.org/abs/2412.08737",
        "github_repo": null,
        "summary": "- This paper introduces Euclid, a family of Multimodal Large Language Models (MLLMs) specifically optimized for enhanced low-level visual perception (LLVP) in geometric tasks.\n- Euclid employs a curriculum learning strategy with synthetically generated high-fidelity visual descriptions of geometric shapes and their relationships, addressing the limitations of existing MLLMs in accurately perceiving detailed geometric information. \n-  A new benchmark dataset called *Geoperception*, derived from the Geometry-3K corpus, is introduced to evaluate the model\u2019s ability to precisely transcribe 2D geometric information from images.\n-  Euclid outperforms leading open-source and closed-source models on the Geoperception benchmark, demonstrating strong generalization capabilities to novel geometry shapes. \n- For instance, Euclid surpasses the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain tasks and 10.65% on average.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision"
        ],
        "github_urls": [
            "github.com/euclid-multimodal/Euclid"
        ],
        "huggingface_urls": [
            "huggingface.co/euclid-multimodal"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "Multimodal Latent Language Modeling with Next-Token Diffusion",
        "authors": "Li Dong, Zhiliang Peng, Wenhui Wang, Hangbo Bao, Yutao Sun",
        "link": "https://arxiv.org/abs/2412.08635",
        "github_repo": null,
        "summary": "- LatentLM, a novel multimodal generative model, is introduced, which seamlessly integrates continuous and discrete data using causal transformers and next-token diffusion.\n- The model employs a variational autoencoder (VAE) to represent continuous data as latent vectors and next-token diffusion for autoregressive generation of these vectors, overcoming variance collapse issues with the use of \u03c3-VAE.\n- LatentLM shows superior performance in image generation, surpassing Diffusion Transformers in both performance and scalability, as evidenced by its favorable FID and IS scores on ImageNet.\n- When integrated into multimodal large language models, LatentLM provides a unified interface, outperforming existing methods in language modeling, text-to-image generation, and vision-language understanding tasks, as demonstrated by its better perplexity scores and FID scores on MS-COCO.\n- In text-to-speech synthesis, LatentLM achieves state-of-the-art results, outperforming VALL-E 2 in speaker similarity and robustness while requiring 10x fewer decoding steps.",
        "classification": [
            "Multimodal",
            "Text-to-Image",
            "Image-to-Text",
            "Text-to-Speech"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/DiT"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
        "authors": "Zhennan Shen, Dunjie Lu, Yiheng Xu, cxiong, ZeonLap",
        "link": "https://arxiv.org/abs/2412.09605",
        "github_repo": null,
        "summary": "- AgentTrek is a novel data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials.\n- It automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment.\n- A VLM-based evaluator ensures the correctness of the generated trajectories, resulting in a dataset with 10,398 trajectories.\n- Experiments demonstrate that training GUI agents with the synthesized trajectories significantly improves their grounding and planning performance compared to current models.\n- The proposed method is more cost-efficient than traditional human annotation methods, paving the way for large-scale GUI agent training.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://agenttrek.github.io"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM",
        "authors": "Hao Shao, Guanglu Song, Bingqi Ma, Dongzhi Jiang, Zhuofan Zong",
        "link": "https://arxiv.org/abs/2412.09618",
        "github_repo": null,
        "summary": "- EasyRef is a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and text prompts.\n- It leverages the multi-image comprehension and instruction-following capabilities of a multimodal large language model (MLLM) to capture consistent visual elements within multiple images.\n- EasyRef introduces an efficient reference aggregation strategy and a progressive training scheme to mitigate computational costs and enhance fine-grained detail preservation.\n- Experimental results demonstrate that EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA in terms of aesthetic quality and zero-shot generalization.\n- A new multi-reference image generation benchmark, MRBench, is introduced to facilitate the evaluation of multi-reference image generation methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://easyref-gen.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion",
        "authors": "Ziwei Liu, Xingang Pan, Xin Huang, Tengfei Wang, Zexin He",
        "link": "https://arxiv.org/abs/2412.09593",
        "github_repo": null,
        "summary": "- Neural LightRig is a novel framework that enhances object normal and PBR material estimation from single images by leveraging multi-light conditions generated via a diffusion prior.\n- It utilizes a two-stage process involving a multi-light diffusion model, built upon a stable diffusion model, which synthesizes images of the input object under varying directional light sources.\n- A large G-buffer model, with a U-Net architecture, subsequently processes these multi-light images to produce high-resolution surface normals and PBR material maps.\n- Neural LightRig is trained on a new synthetic relighting dataset named LightProp and employs data augmentation strategies to bridge the domain gap between synthetic and generated multi-light images.\n- Extensive evaluations demonstrate that Neural LightRig outperforms previous state-of-the-art methods in surface normal estimation, PBR material estimation, and single-image relighting.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [
            "https://projects.zxhezexin.com/neural-lightrig"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations",
        "authors": "Eunbyung Park, Youngjoon Hong, Jaemin Oh, kangnamgyu27",
        "link": "https://arxiv.org/abs/2412.05994",
        "github_repo": null,
        "summary": "- This paper proposes Physics-Informed Gaussians (PIGs), a novel method for approximating solutions to Partial Differential Equations (PDEs) that combines learnable Gaussian feature embeddings with a lightweight neural network.\n- PIG dynamically adjusts the positions and shapes of Gaussians during training, enabling it to effectively capture high-frequency components and complex solution behaviors, addressing the limitations of traditional Physics-Informed Neural Networks (PINNs) that rely on Multi-Layer Perceptrons (MLPs) which suffer from spectral bias. \n- PIG maintains the same optimization framework as PINNs, using numerous collocation points to compute PDE residuals and gradient-based optimization, but requires fewer parameters and demonstrates faster convergence.\n- Experimental results on various PDEs, including Allen-Cahn, Helmholtz, Klein-Gordon, Flow Mixing, and Nonlinear Diffusion equations, show that PIG achieves competitive accuracy compared to existing methods, often outperforming methods using large MLPs or high-resolution parametric grids.\n- The adaptability of Gaussians allows PIG to efficiently approximate solutions by concentrating computational resources in regions with high residual losses or singularities, offering advantages over fixed grid structures.",
        "classification": [
            "Other"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training",
        "authors": "Arpit Sahni, Huseyin Coskun, Xijie Huang, Jierun Chen, Dongting Hu",
        "link": "https://arxiv.org/abs/2412.09619",
        "github_repo": null,
        "summary": "- This paper introduces SnapGen, a novel text-to-image model designed for mobile devices that generates high-resolution images.\n- SnapGen employs efficient network architectures, including a thinner and shorter UNet, and improved training techniques such as flow matching and multi-level knowledge distillation.\n- The model achieves competitive performance on ImageNet-1K and T2I benchmarks, surpassing large-scale models while using significantly fewer parameters (e.g., 7x smaller than SDXL).\n- SnapGen demonstrates high-resolution image generation (1024x1024) on mobile devices within approximately 1.4 seconds, showcasing its efficiency and speed.\n- The authors conduct extensive experiments to validate the model's performance, including quantitative benchmarks and a user study comparing its image quality with existing state-of-the-art methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Learned Compression for Compressed Learning",
        "authors": "Neeraja J. Yadwadkar, Dan Jacobellis",
        "link": "https://arxiv.org/abs/2412.09405",
        "github_repo": null,
        "summary": "- This paper introduces WaLLoC (Wavelet Learned Lossy Compression), a novel neural codec architecture for compressed-domain learning.\n- WaLLoC combines linear transform coding with nonlinear dimensionality-reducing autoencoders, placing a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform.\n- This design enables WaLLoC to achieve computationally efficient encoding, high compression ratios, and uniform dimensionality reduction\u2014key requirements for effective compressed learning.\n- Experiments demonstrate WaLLoC's superior performance compared to existing autoencoders used in state-of-the-art latent diffusion models across several metrics, including image classification, colorization, document understanding, and music source separation.\n- WaLLoC achieves up to 20x dimensionality reduction, making it an effective drop-in replacement for resolution reduction in accelerating downstream models without sacrificing accuracy.",
        "classification": [
            "Computer Vision",
            "Image Classification",
            "Image-to-Image",
            "Audio-to-Audio",
            "Audio Classification"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://ut-sysml.org/walloc/"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition",
        "authors": "Longxiang Tang, Senqiao Yang, Yuqi Liu, Chengyao Wang, Zhisheng Zhong",
        "link": "https://arxiv.org/abs/2412.09501",
        "github_repo": null,
        "summary": "- Lyra, an efficient Multimodal Large Language Model (MLLM), enhances multimodal abilities with a focus on speech, including long speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction.\n- Lyra leverages existing open-source large models and a multi-modality LoRA to reduce training costs, uses a latent multi-modality regularizer and extractor to strengthen relationships between modalities (especially speech), and introduces a high-quality dataset with 1.5M multimodal samples and 12K long speech samples.\n- Compared to other omni-models, Lyra achieves state-of-the-art performance on vision-language, vision-speech, and speech-language benchmarks.\n- Lyra demonstrates superior efficiency with fewer computational resources, less training data, and faster training and inference speed across speech, image, and video tasks.\n- The model supports sound and speech understanding and generation, handles complex long speech inputs, and exhibits enhanced omni-comprehension capabilities.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Text-to-Speech",
            "Text-to-Audio",
            "Automatic Speech Recognition"
        ],
        "github_urls": [
            "https://github.com/dvlab-research/Lyra"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios",
        "authors": "Xiaobao Wu, Sitao Cheng, Liangming Pan, Wenyue Hua, Ruiwen Zhou",
        "link": "https://arxiv.org/abs/2412.08972",
        "github_repo": "https://github.com/skyriver-2000/RuleArena",
        "summary": "- RULEARENA, a benchmark designed to evaluate LLMs' ability to follow complex real-world rules in reasoning across three domains: airline baggage fees, NBA transactions, and tax regulations.\n- It assesses proficiency in handling intricate instructions requiring long-context understanding, logical reasoning, and accurate mathematical computation.\n- Two key distinctions from existing benchmarks: extends beyond first-order logic and grounds tasks in authentic scenarios.\n- Findings reveal LLM limitations: difficulty applying rules, inaccurate math, and overall poor performance.\n- Highlights challenges in rule-guided reasoning for LLMs in real-world applications.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/skyriver-2000/RuleArena"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
        "authors": "Judy Hoffman, Daniel Bolya, Sangmin Lee, Ajay Bati, Fiona Ryan",
        "link": "https://arxiv.org/abs/2412.09586",
        "github_repo": "http://github.com/fkryan/gazelle",
        "summary": "- Gaze-LLE, a novel transformer-based model, streamlines gaze target estimation by leveraging features from a frozen DINOv2 encoder, eliminating the need for complex, handcrafted multi-branch architectures.\n- The model utilizes a single feature representation for the scene and incorporates a person-specific positional prompt to decode gaze with a lightweight module, significantly reducing the number of learnable parameters compared to prior works.\n- Gaze-LLE achieves state-of-the-art performance across multiple gaze estimation benchmarks, including GazeFollow, VideoAttentionTarget, and ChildPlay, demonstrating the effectiveness of leveraging pretrained visual feature representations from foundational models.\n- The model exhibits strong cross-dataset performance without fine-tuning, highlighting its generalizability to diverse domains.\n-  Gaze-LLE is computationally efficient, achieving state-of-the-art results in under 1.5 GPU hours.",
        "classification": [
            "Computer Vision",
            "Keypoint Detection"
        ],
        "github_urls": [
            "http://github.com/fkryan/gazelle"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
        "authors": "Lilach Eden, Roy Bar-Haim, Yotam Perlitz, Odellia Boni, Ariel Gera",
        "link": "https://arxiv.org/abs/2412.09569",
        "github_repo": null,
        "summary": "- This paper introduces JuStRank, a novel benchmark designed to assess the efficacy of Large Language Models (LLMs) in performing system-level ranking of other LLMs.\n- The benchmark employs a dataset of responses generated by various LLMs to a set of instructions and evaluates the judges' ability to rank the systems based on the quality of their responses, using correlation with a human-generated ranking as the primary metric.\n-  The study evaluates 48 state-of-the-art judges, encompassing both general-purpose LLMs and specialized reward models, and reveals that several smaller reward models can perform comparably to much larger LLMs in this ranking task.\n- The authors also investigate the influence of various judge realizations (e.g., absolute numeric scores, Likert-scale ratings, comparative judgments) on ranking accuracy and observe a significant impact, with absolute scores generally outperforming comparative judgments.\n-  The analysis identifies two key emergent properties of system-level judges: decisiveness, characterized by the tendency to amplify differences between strong and weak systems, and system-specific bias.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation",
        "authors": "Jianwei Yang, Jianfeng Gao, Humphrey Shi, Zhengyuan Yang, Jitesh Jain",
        "link": "https://arxiv.org/abs/2412.09585",
        "github_repo": "https://github.com/SHI-Labs/OLA-VLM",
        "summary": "- OLA-VLM introduces a novel approach to enhance visual representations within Multimodal Large Language Models (MLLMs) by distilling knowledge from specialized teacher vision encoders into the LLM's intermediate layers during pretraining.\n- This method involves a coupled optimization of predictive visual embedding and next text-token prediction, incorporating embedding losses at specific LLM layers to align with target visual features from encoders trained on tasks like segmentation, depth estimation, and image generation.\n- The approach also utilizes specialized tokens enriched with target-specific information, creating an implicit visual chain of thought within the LLM's input sequence.\n- OLA-VLM demonstrates improved performance over single and multi-encoder baselines on various benchmarks, including up to an 8.7% boost on the Depth task in CV-Bench, showcasing its superior visual understanding capabilities.\n- The embedding optimization strategy is hypothesized to yield better projector initialization for the instruction fine-tuning stage, leading to efficient and accurate visual processing within the MLLM while using only a single vision encoder during inference.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Computer Vision",
            "Depth Estimation",
            "Image Segmentation"
        ],
        "github_urls": [
            "https://github.com/SHI-Labs/OLA-VLM"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective",
        "authors": "David Samuel, Freddy Wetjen, Lemei Zhang, Vladislav Mikhailov, Javier de la Rosa",
        "link": "https://arxiv.org/abs/2412.09460",
        "github_repo": null,
        "summary": "- This research investigates the impact of copyrighted material on Norwegian Large Language Models (LLMs) by training models using various datasets, including copyrighted and non-copyrighted materials.\n- The study uses Mistral 7B v0.1 architecture as base and creates different versions by pre-training from scratch and also warm-starting and fine-tuning with copyrighted materials and instructions.\n- Evaluations are conducted using a new benchmarking suite with 28 NLP tasks designed for Norwegian, demonstrating that copyrighted material leads to performance improvements across diverse tasks, particularly specialized ones.\n- The paper finds that warm-starting with other languages reduces the impact of adding Norwegian copyrighted data.\n- The findings highlight ethical and legal considerations regarding the use of copyrighted materials in LLM development and provide empirical evidence for policy discussions on author compensation and copyright in the digital age.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/mistralai/Mistral-7B-v0",
            "https://huggingface.co/datasets/mimir-project/mimir-bias",
            "https://huggingface.co/datasets/ltg/nortruthfulqa_mc",
            "https://huggingface.co/datasets/ltg/nortruthfulqa_gen",
            "https://huggingface.co/datasets/ltg/noropenbookqa",
            "https://huggingface.co/datasets/ltg/nrk",
            "https://huggingface.co/datasets/ltg/norcommonsensega",
            "https://huggingface.co/datasets/mimir-project/noridiom",
            "https://huggingface.co/datasets/SamiaT/NorSumm"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "Word Sense Linking: Disambiguating Outside the Sandbox",
        "authors": "Roberto Navigli, Alberte Fern\u00e1ndez-Castro, Luigi Procopio, Edoardo Barba, Andrei Stefan Bejgu",
        "link": "https://arxiv.org/abs/2412.09370",
        "github_repo": null,
        "summary": "- This paper introduces Word Sense Linking (WSL), a new task that aims to bridge the gap between Word Sense Disambiguation (WSD) research and its practical application in downstream tasks.\n- WSL requires systems to identify and disambiguate all spans in a given text using only a reference sense inventory, without relying on pre-identified spans or candidate senses.\n- A novel retriever-reader architecture is proposed for WSL, which inverts the traditional concept detection and candidate generation steps of WSD to overcome limitations in handling unannotated spans.\n- The model outperforms state-of-the-art WSD systems adapted for the WSL setting by a significant margin, demonstrating its robustness and efficiency.\n- A new WSL benchmark dataset, built by expanding the existing WSD evaluation datasets, facilitates comprehensive evaluation of both precision and recall.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Babelscape/WSL"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction",
        "authors": "Ying Shan, Shenghua Gao, Jiale Xu",
        "link": "https://arxiv.org/abs/2412.09573",
        "github_repo": null,
        "summary": "- FreeSplatter is a feed-forward framework that generates 3D Gaussians and estimates camera parameters from uncalibrated sparse-view images.\n- It uses a transformer architecture with self-attention blocks to process multi-view image tokens and decode them into pixel-wise 3D Gaussian primitives.\n- FreeSplatter outperforms state-of-the-art baselines on object-centric and scene-level datasets in reconstruction quality and pose estimation accuracy, even surpassing pose-dependent models on object-centric tasks.\n- Two variants, FreeSplatter-O and FreeSplatter-S, are trained for object and scene reconstruction, showcasing its adaptability.\n- It enhances downstream applications like text/image-to-3D, simplifying the process by eliminating the need for explicit camera pose alignment.",
        "classification": [
            "Image-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "DisPose: Disentangling Pose Guidance for Controllable Human Image Animation",
        "authors": "Zhihong Zhu, Junjie Cao, Yuhang Yang, Yaowei Li, Hongxiang Li",
        "link": "https://arxiv.org/abs/2412.09349",
        "github_repo": "https://github.com/lihxxx/DisPose",
        "summary": "- DisPose, a plug-and-play module, enhances controllable human image animation by disentangling pose guidance into motion field guidance and keypoint correspondence, thus improving quality and consistency without additional dense inputs.\n- It generates a dense motion field from a sparse motion field derived from skeleton poses and the reference image, providing region-level guidance while maintaining generalization.\n- DisPose extracts diffusion features from reference image keypoints and transfers them to the target pose based on motion trajectories for enhanced appearance consistency.\n- Implemented as a hybrid ControlNet, it integrates seamlessly with existing models, improving quality without modifying their parameters.\n-  Quantitative and qualitative evaluations demonstrate DisPose's superior performance compared to current methods, including improvements in metrics like FID-FVD and VBench scores on a TikTok dataset and an unseen dataset.",
        "classification": [
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/lihxxx/DisPose"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "LoRACLR: Contrastive Adaptation for Customization of Diffusion Models",
        "authors": "Pinar Yanardag, Federico Tombari, Thomas Hofmann, enisimsar",
        "link": "https://arxiv.org/abs/2412.09622",
        "github_repo": null,
        "summary": "- LoRACLR is a novel approach for multi-concept image generation that merges multiple pre-trained LoRA models into a single unified model using a contrastive objective, eliminating the need for individual fine-tuning or access to original training data.\n- This method adapts pre-existing LoRA models to function cohesively by aligning their weight spaces, ensuring each concept retains high fidelity while remaining compatible in joint compositions.\n- The contrastive objective enforces distinct boundaries between concepts, preventing feature interference and preserving individual identities.\n- LORACLR demonstrates superior performance compared to state-of-the-art methods in both qualitative and quantitative evaluations, exhibiting enhanced image quality, compositional coherence, and identity preservation, particularly in complex scenarios with multiple concepts.\n- This approach provides a practical and scalable solution for personalized image synthesis, enabling efficient and high-quality generation of complex scenes without retraining or relying on heavy computational overhead.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/windwhinny/chilloutmix"
        ],
        "date": "2024-12-13"
    },
    {
        "title": "SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts",
        "authors": "Mohit Bansal, Chongyang Zhao, Zun Wang, Yicong Hong, Gengze Zhou",
        "link": "https://arxiv.org/abs/2412.05552",
        "github_repo": null,
        "summary": "- This paper proposes State-Adaptive Mixture of Experts (SAME), a novel model for versatile language-guided visual navigation that can interpret and execute instructions with varying levels of granularity.\n- SAME utilizes a mixture of expert networks specialized in different navigation skills, such as exploration and instruction-following, routed based on the agent's current state (attended language and visual observation).\n- The model effectively addresses the challenge of conflicting learning objectives in multi-task training by selectively activating experts based on the input state, promoting shared knowledge learning while maintaining task-specific capabilities.\n- Experimental results on seven navigation tasks including R2R, RxR-EN, REVERIE, OBJECTNAV, CVDN, SOON, and R2R-CE, demonstrate that SAME outperforms or achieves comparable performance to task-specific agents, showing the efficacy of the state-adaptive expert routing mechanism.\n- Further analysis reveals that applying MoE on visual queries in the cross-attention layer of the navigation policy yields superior results compared to applying it on feed-forward networks, highlighting the importance of cross-modal attention in action selection.",
        "classification": [
            "Computer Vision",
            "Reinforcement Learning",
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/GengzeZhou/SAME"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Arbitrary-steps Image Super-resolution via Diffusion Inversion",
        "authors": "Chen Change Loy, Kang Liao, Zongsheng Yue",
        "link": "https://arxiv.org/abs/2412.09013",
        "github_repo": "https://github.com/zsyOAOA/InvSR",
        "summary": "- This paper introduces InvSR, a novel image super-resolution (SR) technique based on diffusion inversion, which leverages a pre-trained diffusion model (SD-Turbo) without modifying its architecture.\n- InvSR employs a Partial Noise Prediction (PnP) strategy to determine an optimal noise map for a given low-resolution (LR) image, enabling efficient initialization of the reverse sampling process.\n-  A deep noise predictor network is trained to estimate this noise map, enabling flexible sampling with arbitrary steps ranging from one to five.\n- Experimental results on ImageNet-Test and real-world datasets demonstrate that InvSR with a single sampling step achieves comparable or superior results to state-of-the-art one-step SR methods and competitive results compared to multi-step methods.\n- InvSR provides a flexible and efficient sampling mechanism adaptable to various degradation types, making it practical for real-world applications.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/zsyOAOA/InvSR"
        ],
        "huggingface_urls": [],
        "date": "2024-12-13"
    },
    {
        "title": "Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages",
        "authors": "Srinivasan Umesh, rumourscape",
        "link": "https://arxiv.org/abs/2412.09025",
        "github_repo": null,
        "summary": "- This paper introduces Shiksha, a dataset and model for translating technical content into Indian languages.\n- The dataset comprises 2.8 million high-quality, parallel translation pairs across 8 Indian languages, extracted from NPTEL lecture transcriptions.\n- A 3.3B parameter NLLB model is fine-tuned using LoRA on this dataset, demonstrating improvements in both in-domain and out-of-domain translation tasks.\n- Evaluation on a held-out test set and the Flores+ benchmark shows improved performance over baseline NLLB and comparable results to IndicTrans2.\n- The models are integrated into a tool called Translingua, which aids human annotators in translating NPTEL lectures.",
        "classification": [
            "Translation",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/SPRINGLab"
        ],
        "date": "2024-12-13"
    }
]