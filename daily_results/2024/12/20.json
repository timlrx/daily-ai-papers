[
    {
        "title": "Qwen2.5 Technical Report",
        "authors": "Losin94, bowenYu, bzheng, huybery, Baosong",
        "link": "https://arxiv.org/abs/2412.15115",
        "github_repo": null,
        "summary": "- Qwen2.5 is a series of large language models (LLMs) trained on 18 trillion tokens of data, improving upon its predecessor Qwen2 through enhanced pre-training and post-training techniques.\n- The models range from 0.5B to 72B parameters in open-weight offerings and include Mixture-of-Experts (MoE) models, Qwen2.5-Turbo and Qwen2.5-Plus, for hosted solutions.\n- Qwen2.5-72B-Instruct demonstrates competitive performance against Llama-3-405B-Instruct, a model five times its size.\n- Qwen2.5-Turbo and Qwen2.5-Plus exhibit superior cost-effectiveness while competing with GPT40-mini and GPT40 respectively.\n- Qwen2.5 also serves as a foundation for specialized models like Qwen2.5-Math and Qwen2.5-Coder, broadening its applicability to specific domains.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/QwenLM/Qwen2.5"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Qwen"
        ],
        "date": "2024-12-20"
    },
    {
        "title": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval",
        "authors": "BoZhaoHuggingFace, yzwang, Shitao, zl101, JUNJIE99",
        "link": "https://arxiv.org/abs/2412.14475",
        "github_repo": null,
        "summary": "- MegaPairs, a novel data synthesis method that uses vision-language models (VLMs) and open-domain images with a massive synthetic dataset for universal multimodal retrieval is proposed.\n- This method constructs heterogeneous KNN triplets using three similarity models (CLIP vision encoder, DINO vision encoder, and CLIP text encoder) to sample correlated image pairs.\n- It then utilizes MLLM and LLM annotators for relationship description and pseudo retrieval instruction generation resulting in triplets (Image query, Text instruction, Image target). \n- MMRet models trained on MegaPairs demonstrate SOTA zero-shot results on 4 Composed Image Retrieval benchmarks and MMEB's 36 datasets, outperforming baselines by 8.1% on CIRCO using 70x less data. \n- Further downstream fine-tuning shows that the model maintains leading performance on the benchmarks mentioned above.",
        "classification": [
            "Multimodal",
            "Image-to-Text",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Progressive Multimodal Reasoning via Active Retrieval",
        "authors": "douzc, yutaozhu94, dengmengjie, Snow-Nation, dongguanting",
        "link": "https://arxiv.org/abs/2412.14835",
        "github_repo": null,
        "summary": "- This paper introduces AR-MCTS, a framework designed to improve multi-step multimodal reasoning in Multimodal Large Language Models (MLLMs) by combining Active Retrieval (AR) and Monte Carlo Tree Search (MCTS).\n- AR-MCTS employs a unified retrieval module to gather key insights from a hybrid-modal corpus, aiding in problem-solving.\n- It utilizes MCTS with active retrieval to automatically generate step-wise annotations, enhancing the diversity and reliability of the reasoning process.\n- A process reward model (PRM) is progressively aligned through step-wise Direct Preference Optimization (DPO) and Supervised Fine-tuning (SFT) for automated verification.\n- Experimental results across various MLLMs and benchmarks show AR-MCTS's effectiveness in boosting performance, optimizing sampling diversity and accuracy, and demonstrating improvement in complex reasoning scenarios, particularly on WE-MATH's S3 metrics and general reasoning tasks like GAOKAO-MM.",
        "classification": [
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
        "authors": "wangxz098, haopeng01, NeoZ123, tsq2000, bys0318",
        "link": "https://arxiv.org/abs/2412.15204",
        "github_repo": null,
        "summary": "- Introduces LongBench v2, a challenging benchmark designed to evaluate the deep understanding and reasoning capabilities of Large Language Models (LLMs) in long-context scenarios across diverse real-world tasks.\n- The benchmark consists of 503 multiple-choice questions spanning six major task categories, with contexts ranging from 8k to 2M words, focusing on complex reasoning rather than simple information retrieval.\n- Data collection involves nearly 100 highly educated individuals and employs rigorous automated and manual review processes, resulting in a high-quality dataset where even human experts achieve only 53.7% accuracy under time constraints.\n- Evaluation shows that the best-performing model achieves 57.7% accuracy, surpassing the human baseline by 4% when leveraging chain-of-thought prompting during inference.\n- The results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle long-context challenges and call for further exploration in this direction.",
        "classification": [
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/THUDM/LongBench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "How to Synthesize Text Data without Model Collapse?",
        "authors": "XingtaiHF, iseesaw, Hengli, daixuancheng, xuekai",
        "link": "https://arxiv.org/abs/2412.14689",
        "github_repo": null,
        "summary": "- This paper proposes ToEdit (Token Editing), a novel technique for synthesizing text data that mitigates model collapse, a degenerative process where language models overfit to synthetic data distributions.\n- ToEdit employs token-level editing on human-produced data, guided by a pre-trained language model's probability distribution, to create semi-synthetic data.\n- This method theoretically constrains the test error within a fixed upper bound, preventing the error accumulation observed in iterative training on synthetic data.\n- Experimental results across pre-training, continual pre-training, and supervised fine-tuning demonstrate that ToEdit enhances model performance compared to using purely synthetic or mixed synthetic and human-produced data.\n- Statistical analyses reveal that synthetic data suffers from coverage collapse and over-concentration of n-gram features, issues addressed by ToEdit's approach.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Flowing from Words to Pixels: A Framework for Cross-Modality Evolution",
        "authors": "Andrew Brown, Alan Yuille, Xi Yin, mannatsingh, QHL067",
        "link": "https://arxiv.org/abs/2412.15213",
        "github_repo": null,
        "summary": "- CrossFlow, a novel framework for cross-modal flow matching, leverages variational encoders and a novel classifier-free guidance technique to directly map one modality's distribution to another's.\n- For text-to-image generation, CrossFlow uses a vanilla transformer without cross-attention, unlike existing methods that rely on complex architectures and conditioning mechanisms.\n- Demonstrating improved scaling, CrossFlow slightly outperforms standard flow matching baselines in zero-shot FID-30K and achieves comparable CLIP scores, given the same data, model size, and training budget.\n- CrossFlow exhibits semantic latent space arithmetic, enabling meaningful output edits through latent manipulation.\n- Its generalizability is showcased by comparable or superior performance in image captioning, depth estimation, and image super-resolution compared to state-of-the-art techniques.",
        "classification": [
            "Text-to-Image",
            "Image-to-Text",
            "Depth Estimation",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "https://cross-flow.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis",
        "authors": "lmwang, cqf, felixcheng97, qiuyuu, hlwang06",
        "link": "https://arxiv.org/abs/2412.15214",
        "github_repo": null,
        "summary": "- LeviTor is a novel method for 3D trajectory control in image-to-video synthesis that combines depth information with K-means clustered points of object masks without explicit 3D trajectory tracking.\n- It leverages a high-quality Video Object Segmentation (VOS) dataset (SA-V) for training and a user-friendly inference pipeline that simplifies 3D trajectory input.\n- LeviTor achieves state-of-the-art performance on standard video generation metrics (FID, FVD) and motion control metrics (ObjMC) compared to existing approaches like DragNUWA and DragAnything.\n- The model effectively manages the proximity changes of objects and produces video results with complex motions (like orbiting) and object occlusions that aren't possible with 2D trajectory controls.\n- Ablation studies validate the importance of the combined depth and instance information, along with the number of control points for effective 3D motion representation.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion",
        "authors": "Ye Liu, hpfister, dwei, EthanTaylor, Kakituken",
        "link": "https://arxiv.org/abs/2412.14462",
        "github_repo": "https://github.com/KaKituken/affordance-aware-any",
        "summary": "- This paper introduces Mask-Aware Dual Diffusion (MADD), a novel dual-stream diffusion model for affordance-aware object insertion into images. \n- MADD utilizes a DINOv2 encoder for foreground guidance and a frozen VAE encoder for background encoding, combined with a unified position prompt encoder.\n- It simultaneously denoises both the RGB image and a mask of the inserted object, facilitating better object placement consistent with real-world affordances.\n- The authors claim state-of-the-art performance on a new dataset, SAM-FB, derived from SA-1B and consisting of over 3 million image-object pairs across more than 3,000 categories, achieving a FID score of 13.53 and CLIP score of 0.8727 using mask prompts.\n-  MADD also shows strong generalization ability on in-the-wild images, adjusting the inserted object's position, size, and view for realistic compositions, even with ambiguous or null position prompts.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/KaKituken/affordance-aware-any"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation",
        "authors": "Yuejiang Dong, yshan2u, bluestyle97, pookiefoof, thuzhaowang",
        "link": "https://arxiv.org/abs/2412.15200",
        "github_repo": null,
        "summary": "- DI-PCG introduces a novel, efficient approach to Inverse Procedural Content Generation (I-PCG) using a diffusion transformer model.\n- This lightweight model (7.6M parameters) treats PCG parameters as the denoising target and uses observed images as conditions to control 3D asset generation.\n- DI-PCG efficiently recovers accurate parameters within seconds, generalizing well to diverse, real-world images without needing external datasets, thanks to features from a pre-trained visual foundation model.\n- Experiments show superior performance on I-PCG and image-to-3D tasks, validated qualitatively and quantitatively against state-of-the-art methods including Shap-E, Michelangelo, and InstantMesh.\n- The method offers a promising step toward a 3D generation pipeline focused on learning construction parameters, rather than modeling the 3D object directly.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling",
        "authors": "wping, ctnzr, shoeybi, ychenNLP, zihanliu",
        "link": "https://arxiv.org/abs/2412.15084",
        "github_repo": null,
        "summary": "- AceMath, a suite of large language models (LLMs) designed for complex math problem-solving and featuring specialized reward models for solution evaluation, is introduced.\n- The instruction-tuned math models are developed through a two-stage supervised fine-tuning (SFT) process, starting with general domain SFT and followed by targeted math domain fine-tuning using curated prompts and synthetically generated responses.\n- AceMath-72B-Instruct outperforms existing open-weight and proprietary LLMs, including Qwen2.5-Math-72B-Instruct, GPT-40, and Claude-3.5 Sonnet, on a variety of math reasoning benchmarks.\n- A new comprehensive benchmark, AceMath-RewardBench, is introduced for evaluating math reward models; the associated AceMath-72B-RM reward model achieves state-of-the-art performance.\n- Combining AceMath-72B-Instruct with AceMath-72B-RM yields the highest average rm@8 score across math reasoning benchmarks.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency",
        "authors": "Federico Tombari, Yongqin Xian, thofmann, Alessiot, enisimsar",
        "link": "https://arxiv.org/abs/2412.15216",
        "github_repo": null,
        "summary": "- UIP2P, an unsupervised model for instruction-based image editing, eliminates the need for ground-truth edited images during training by introducing Cycle Edit Consistency (CEC).\n- CEC enforces consistency by applying forward and backward edits in one training step, leveraging alignment between text and images in the CLIP embedding space and ensuring coherence in image and attention spaces.\n- This approach allows training on datasets with real image-caption pairs or image-caption-edit triplets, outperforming supervised methods across a broader range of edits.\n- Empirically, UIP2P shows better performance in qualitative comparisons and user studies across various datasets for diverse tasks like color modification, object removal, and structural changes.\n- Ablation studies demonstrate the importance of loss functions and the efficiency of UIP2P requiring fewer steps than other models like InstructPix2Pix for high-quality edits.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception",
        "authors": "Ke Zhu, Jing Hao, FuNz, cloud913, syp115",
        "link": "https://arxiv.org/abs/2412.14233",
        "github_repo": "https://github.com/syp2ysy/DCE",
        "summary": "- This paper introduces DCE (Descriptive Caption Enhancement), a novel image captioning engine that leverages off-the-shelf visual specialist models to extract detailed object attributes and relationships from images.\n- These attributes, combined with LLM-generated region captions and relational information, produce richer and more comprehensive descriptions than existing methods relying solely on LLMs or human annotation.\n- Experimental results demonstrate that DCE-generated captions significantly improve the performance of Large Multimodal Models (LMMs) across 14 visual question answering and multimodal benchmarks, exceeding human and other LLM-generated captions.\n- DCE utilizes open-source models for caption generation, reducing the costs and improving the efficiency compared to methods using expensive models like GPT-4V.\n- The authors plan to release the DCE source code and pipeline to promote further research and enable easy integration of other visual specialists into multimodal models.",
        "classification": [
            "Multimodal",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/syp2ysy/DCE"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
        "authors": "Qing Li, Yunqing Liu, Jiatong Li, schrodingers-tiger, Duke-de-Artois",
        "link": "https://arxiv.org/abs/2412.14642",
        "github_repo": "https://github.com/phenixace/TOMG-Bench",
        "summary": "- This paper introduces TOMG-Bench, the first benchmark designed to evaluate the ability of Large Language Models (LLMs) to perform open-domain text-based molecule generation.\n- TOMG-Bench comprises three core tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom), each with three subtasks containing 5,000 samples.\n- It includes an automated evaluation system assessing generated molecules' quality and accuracy, alongside an instruction-tuning dataset OpenMolIns, extracted from PubChem, to enhance LLM performance.\n- The benchmarking of 25 LLMs showcases the current limitations and potential in this field; with OpenMolIns, Llama-3.1-8B outperforms open-source general LLMs, even surpassing GPT-3.5-turbo on the TOMG-Bench by 46.5%.\n- The researchers identified the challenge of open molecule generation for existing LLMs and constructed the corresponding benchmark and the instructional dataset.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/phenixace/TOMG-Bench"
        ],
        "huggingface_urls": [],
        "date": "2024-12-20"
    },
    {
        "title": "Move-in-2D: 2D-Conditioned Human Motion Generation",
        "authors": "Feng Liu, Difan Liu, Jui-Hsien Wang, Yang Zhou, hsinh",
        "link": "https://arxiv.org/abs/2412.13185",
        "github_repo": null,
        "summary": "- Move-in-2D introduces a novel approach to generate human motion sequences conditioned on a 2D scene image and a text prompt, using a diffusion model with a transformer architecture.\n- The model architecture consists of CLIP and DINO encoders for text and image inputs, respectively, which are incorporated into the model via in-context conditioning and AdaLN.\n- A new large-scale dataset, HiC-Motion, is collected from open-domain internet videos and annotated with 3D human motion, text prompts, and scene images to train and evaluate the model.\n- Experimental results demonstrate that Move-in-2D generates human motion that aligns with the scene image and text prompt and improves the quality of human motion in video synthesis tasks, outperforming existing methods on several metrics, including FID, accuracy, diversity, and multimodality.\n- The generated motion is shown to be compatible with existing video generation frameworks and significantly enhance the generation of scene-consistent human actions and dynamics, overcoming limitations of methods that depend on existing motion sequences.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-20"
    }
]