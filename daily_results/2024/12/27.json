[
    {
        "title": "YuLan-Mini: An Open Data-efficient Language Model",
        "authors": "Jie Chen, Jiapeng Wang, Jia Deng, Huatong Song, Yiwen Hu",
        "link": "https://arxiv.org/abs/2412.17743",
        "github_repo": "https://github.com/RUC-GSAI/YuLan-Mini",
        "summary": "- This paper introduces YuLan-Mini, a 2.42B parameter open-source language model trained on 1.08T tokens.\n- The model uses a decoder-only transformer architecture with grouped-query attention, SwiGLU activation, rotary positional embeddings, and embedding tying.\n- Key innovations for pre-training include a robust optimization method to improve training stability, a focused data pipeline with data cleaning and scheduling, and an annealing approach for data selection and long context training.\n- The model is evaluated on benchmarks for math, code generation, reasoning, and language understanding, showing comparable performance to larger industry models trained on significantly more data.\n- YuLan-Mini achieves state-of-the-art results for similar-sized models and demonstrates significant improvements in training efficiency.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/RUC-GSAI/YuLan-Mini"
        ],
        "huggingface_urls": [],
        "date": "2024-12-27"
    },
    {
        "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
        "authors": "Xinting Huang, Shuaiyi Li, Kelong Mao, Zhisong Zhang, ChenlongDeng",
        "link": "https://arxiv.org/abs/2412.17483",
        "github_repo": null,
        "summary": "- This paper investigates gist token-based context compression methods for improving long-context processing in large language models (LLMs).\n- The study evaluates different gist-based model architectures categorized by memory location (recurrent or key-value cache) and gist granularity (coarse or fine-grained).\n- Experiments across language modeling, reasoning, and long-context tasks show that fine-grained key-value cache models achieve near-lossless performance on some tasks, while struggling with others like reranking.\n- Three failure patterns are identified: \"lost by the boundary,\" where generation quality degrades near segment boundaries; \"lost if surprise,\" where unexpected details are lost; and \"lost along the way,\" where models struggle to recover exact rehearsals. \n- Two mitigation strategies, fine-grained autoencoding and segment-wise token importance estimation, show improvements, especially in addressing boundary effects and precise recall.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2024-12-27"
    }
]