[
    {
        "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis",
        "authors": "Yian Wang, Chuanyang Jin, Kanzhi Cheng, heroding77, QiushiSun",
        "link": "https://arxiv.org/abs/2412.19723",
        "github_repo": null,
        "summary": "- OS-Genesis is a novel GUI data synthesis pipeline that automates the construction of high-quality and diverse agent trajectories without human supervision or predefined tasks.\n- Instead of relying on predefined tasks, OS-Genesis allows agents to explore environments freely and interact step-wise, retroactively deriving tasks from interactions. This interaction-driven approach reverses the conventional trajectory collection process and enables more effective exploration.\n- The pipeline incorporates a trajectory reward model (TRM) to filter and prioritize the synthesized trajectories for more effective utilization.\n- Evaluations on challenging online mobile benchmarks like AndroidWorld show that OS-Genesis doubles the performance of existing task-driven methods and outperforms other data synthesis approaches on unseen, out-of-distribution apps.\n-  Analysis suggests that OS-Genesis significantly increases the diversity of both generated instructions and trajectories, more closely mirroring human-like interactions with digital environments than existing synthetic data generation methods.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-02"
    },
    {
        "title": "Xmodel-2 Technical Report",
        "authors": "Jiang Ling, Qu Zhijiu, Lin Qingquan, Liu Yang, valeriaWong",
        "link": "https://arxiv.org/abs/2412.19638",
        "github_repo": "https://github.com/XiaoduoAILab/Xmodel-2",
        "summary": "- Xmodel-2 is a 1.2 billion parameter large language model specialized for reasoning tasks, using a unified set of hyperparameters across different model scales for efficient experimentation and configuration transfer.\n- Trained on 1.5 trillion tokens, Xmodel-2 employs the Warmup-Stable-Decay (WSD) learning rate scheduler from MiniCPM for enhanced training efficiency and stability.\n- It achieves state-of-the-art performance on complex reasoning and agent-based tasks compared to other models in the 1-2 billion parameter range, demonstrating the effectiveness of its design and training approach.\n- The model architecture is similar to Llama 2 and incorporates techniques like embedding sharing, deep-and-thin structure and grouped-query attention for optimized training and inference.\n- The model exhibits good calibration properties with predicted confidence closely aligned with actual correctness probabilities.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation",
            "Text2Text Generation",
            "Fill-Mask",
            "Sentence Similarity",
            "Feature Extraction",
            "Summarization",
            "Translation"
        ],
        "github_urls": [
            "https://github.com/XiaoduoAILab/Xmodel-2"
        ],
        "huggingface_urls": [],
        "date": "2025-01-02"
    }
]