[
    {
        "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
        "authors": "yingtai, zhenheny, chenzhao, yinhongliu, SherryX",
        "link": "https://arxiv.org/abs/2501.02976",
        "github_repo": null,
        "summary": "- STAR, a novel Spatial-Temporal Augmentation approach, leverages text-to-video (T2V) diffusion priors for real-world video super-resolution, enhancing spatial details and temporal consistency.\n- It introduces a Local Information Enhancement Module (LIEM) before global self-attention in the T2V model to mitigate artifacts caused by complex real-world degradations.\n- A Dynamic Frequency (DF) Loss guides the model to prioritize different frequency information during diffusion steps, improving fidelity.\n- Experiments demonstrate STAR outperforms state-of-the-art methods on both synthetic and real-world video super-resolution benchmarks in clarity (DOVER scores) while maintaining robust temporal consistency.\n- Ablation studies show the effectiveness of proposed LIEM, DF Loss and demonstrate improvement when scaling to larger T2V models like CogVideoX.",
        "classification": [
            "Computer Vision",
            "Text-to-Video",
            "Image-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/THUDM/CogVideoX-5b"
        ],
        "date": "2025-01-07"
    },
    {
        "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
        "authors": "lindahua, yhcao, KennyUTC, yuhangzang, BeichenZhang",
        "link": "https://arxiv.org/abs/2501.03226",
        "github_repo": "https://github.com/beichenzbc/BoostStep",
        "summary": "- BoostStep, a novel method, enhances the mathematical reasoning capabilities of Large Language Models (LLMs) by improving single-step reasoning through refined in-context learning.\n- It addresses the granularity mismatch and negative-effect noise within traditional in-context learning examples by providing step-level guidance with a \"first-try\" strategy.\n- This strategy retrieves highly related in-context examples based on the model's initial reasoning attempt for each step, improving reasoning quality.\n- BoostStep improves the performance of GPT-40 and Qwen2.5-Math-72B by 3.6% and 2.0% respectively on various mathematical benchmarks, and by 7.5% when combined with Monte Carlo Tree Search (MCTS).\n- It seamlessly integrates with MCTS, improving both candidate generation and decision-making processes.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/beichenzbc/BoostStep"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
        "authors": "myownskyW7, lindahua, yhcao, yuhangzang, Mar2Ding",
        "link": "https://arxiv.org/abs/2501.03218",
        "github_repo": "https://github.com/Mark12Ding/Dispider",
        "summary": "- Dispider is a novel system designed for active, real-time interaction with streaming videos, disentangling perception, decision, and reaction into asynchronous modules.\n- It features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction using scene-based features and historical interactions.\n- An asynchronous interaction module provides detailed responses without interrupting video processing, ensuring real-time performance and multi-step reasoning capabilities.\n- Experiments on StreamingBench and a subset of ETBench show Dispider significantly outperforms existing online video LLM models in temporal grounding and proactive response generation.\n- It also maintains strong performance on conventional video QA tasks across long-video benchmarks such as EgoSchema, VideoMME, and MLVU, demonstrating effectiveness in handling long video lengths and complex interactions.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Mark12Ding/Dispider"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Personalized Graph-Based Retrieval for Large Language Models",
        "authors": "Franck-Dernoncourt, namyongp, Ojasmitha17, Tobilee, StevenAu",
        "link": "https://arxiv.org/abs/2501.02157",
        "github_repo": null,
        "summary": "- This paper introduces PGraphRAG, a framework leveraging user-centric knowledge graphs for personalized text generation with LLMs.\n- PGraphRAG enhances personalization by augmenting prompts with user-relevant context retrieved from the knowledge graph, improving contextual understanding and output quality.\n- A new benchmark, the Personalized Graph-based Benchmark for Text Generation, is presented to evaluate the effectiveness of PGraphRAG.\n- Experimental results demonstrate that PGraphRAG significantly outperforms state-of-the-art personalization methods, especially in cold-start scenarios with limited user history.\n- The integration of structured user knowledge graphs through PGraphRAG allows for richer, contextually appropriate personalized responses.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/PGraphRAG-benchmark/PGR-LLM"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Test-time Computing: from System-1 Thinking to System-2 Thinking",
        "authors": "Jia Xu, Kaixin Wu, Hai Ye, douvleplus, Yisam",
        "link": "https://arxiv.org/abs/2501.02497",
        "github_repo": null,
        "summary": "- This paper surveys test-time computing methods, categorizing them into System-1 (for perceptual tasks) and System-2 (for cognitive tasks) models.\n- For System-1, test-time adaptation methods like parameter updates, input modification, representation editing, and output calibration are discussed, focusing on enhancing robustness and generalization.\n- For System-2, techniques such as repeated sampling, self-correction, and tree search are explored, aiming to improve reasoning and planning abilities.\n- The paper traces the evolution from System-1 to System-2 thinking, emphasizing test-time computing's role in this transition, and suggests potential future directions.\n- The o1 model is used as an example of the test-time computing scaling effect where increased computational effort at inference leads to improved performance in complex reasoning.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/Dereck0602/Awesome_Test_Time_LLMs"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
        "authors": "willieneis, oliu-io, upup-ashton-wang, Johannes, oliu-io",
        "link": "https://arxiv.org/abs/2501.02045",
        "github_repo": null,
        "summary": "- METAGENE-1 is a 7-billion parameter autoregressive transformer model, pre-trained on a novel corpus of 1.5 trillion base pairs of metagenomic DNA and RNA sequences from wastewater.\n- The model utilizes byte-pair encoding (BPE) tokenization tailored for metagenomic sequences and a decoder-only architecture.\n- In benchmarks, METAGENE-1 achieves state-of-the-art results on genomic tasks like pathogen detection and sequence embedding, outperforming models trained on curated species genomes.\n- It demonstrates potential for pandemic monitoring and early detection of emerging health threats through wastewater analysis.\n- The model's open-source release aims to accelerate research in genomic anomaly detection while acknowledging safety considerations for future model development.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "github.com/metagene-ai"
        ],
        "huggingface_urls": [
            "huggingface.co/metagene-ai"
        ],
        "date": "2025-01-07"
    },
    {
        "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
        "authors": "Yijun Li, yingcongchen, HeZhang, zhifeichen097, wileewang",
        "link": "https://arxiv.org/abs/2501.03006",
        "github_repo": null,
        "summary": "- TransPixar introduces a novel approach for generating RGBA videos from text, extending pre-trained Diffusion Transformer (DiT) based text-to-video models.\n- The method incorporates alpha-specific tokens and LoRA-based fine-tuning for joint RGB and alpha channel generation.\n- By optimizing the attention mechanism, specifically the RGB-attend-to-Alpha component, TransPixar achieves strong alignment between RGB and alpha channels.\n- This approach effectively addresses the limitations of existing methods, which often struggle with transparency and complex visual effects due to limited RGBA training data.\n- Experimental results demonstrate TransPixar's ability to generate diverse and consistent RGBA videos, as showcased in qualitative comparisons and user studies that indicate superior performance compared to alternative methods.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://wileewang.github.io/TransPixar/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Ingredients: Blending Custom Photos with Video Diffusion Transformers",
        "authors": "Di Qiu, MichaelFan, Changqian, Debang, onion",
        "link": "https://arxiv.org/abs/2501.01790",
        "github_repo": "https://github.com/feizc/Ingredients",
        "summary": "- Ingredients, a training-free framework, is introduced for customizing video generation with video diffusion transformers by incorporating multiple user-provided ID photos.\n- The framework includes a facial extractor, a multi-scale projector, and an ID router to handle ID features, embed them into the video diffusion transformer context, and allocate the ID embeddings to corresponding regions, respectively.\n- Ingredients supports multi-ID customization without prompt constraints, offering flexibility and precision in video synthesis.\n- Evaluations show Ingredients' superior performance in generating high-quality, editable videos with consistent multi-human customization, exceeding baseline methods quantitatively and qualitatively.\n- The data, code, and model weights are publicly available for research.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/feizc/Ingredients"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct",
            "https://huggingface.co/openai/clip-vit-base-patch32"
        ],
        "date": "2025-01-07"
    },
    {
        "title": "DepthMaster: Taming Diffusion Models for Monocular Depth Estimation",
        "authors": "Ruijie Zhu, Hao Zhang, Bo Li, Zerong Wang, Ziyang Song",
        "link": "https://arxiv.org/abs/2501.02576",
        "github_repo": null,
        "summary": "- DepthMaster, a novel single-step diffusion model, is proposed for monocular depth estimation, enhancing generalization and detail preservation by adapting generative features for the discriminative task.\n- A Feature Alignment module integrates semantic information by aligning feature distributions of the diffusion model with a high-quality external encoder, mitigating overfitting to texture details.\n- A Fourier Enhancement module refines high-frequency details by operating in the frequency domain, balancing low-frequency structure and high-frequency details to improve visual quality, simulating multi-step refinement in a single pass.\n- A two-stage training strategy first focuses on global scene structure using the Feature Alignment module and then refines fine-grained details using the Fourier Enhancement module.\n- DepthMaster achieves state-of-the-art zero-shot performance and superior detail preservation, outperforming other diffusion-based methods on various datasets, demonstrating a 17.2% improvement over Marigold on KITTI in terms of absolute relative error.",
        "classification": [
            "Depth Estimation",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
        "authors": "Yaniv Taigman, Shelly Sheynin, Amit Zohar, Yuval Kirstain, GuyYariv",
        "link": "https://arxiv.org/abs/2501.03059",
        "github_repo": null,
        "summary": "- This paper introduces THROUGH-THE-MASK, a two-stage image-to-video generation model that leverages mask-based motion trajectories. \n- The first stage generates mask-based motion trajectories conditioned on the input image, initial segmentation mask, and a motion-specific text prompt. \n- The second stage generates the video, conditioned on the reference image, generated mask-based motion trajectories, a global text prompt, and object-specific prompts using masked cross-attention and masked self-attention mechanisms. \n- The model achieves state-of-the-art performance on the Image-Animation-Bench and a newly introduced SA-V-128 benchmark, outperforming existing methods in various metrics, including temporal coherence and text faithfulness. \n- Ablation studies demonstrate the effectiveness of the masked attention mechanism and the advantage of using mask-based trajectories over optical flow.",
        "classification": [
            "Image-to-Video",
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
        "authors": "Yijin Li, Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, wangfuyun",
        "link": "https://arxiv.org/abs/2501.02690",
        "github_repo": null,
        "summary": "- GS-DiT, a novel video generation framework, introduces pseudo 4D Gaussian fields constructed via efficient dense 3D point tracking to enable 4D control in videos, including multi-camera shooting and dolly zoom effects, without requiring multi-view training data.\n- A novel efficient Dense 3D Point Tracking (D3D-PT) method is proposed, outperforming SpatialTracker in accuracy and accelerating inference speed significantly.\n- The framework constructs pseudo 4D Gaussian fields from 3D point trajectories, rendering novel view videos to guide a fine-tuned pre-trained Diffusion Transformer (DiT) model, dubbed GS-DiT.\n- GS-DiT generates videos conditioned on rendered videos from the pseudo 4D Gaussian field and effectively generalizes to real-world videos due to its training strategy that leverages monocular videos.\n- The model enables advanced cinematic effects by manipulating Gaussian field and camera intrinsics, showcasing 4D control capabilities beyond camera poses, as demonstrated by its superior performance in multi-camera shooting compared to GCD and MonST3R.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models",
        "authors": "Weiqiang Wang, Huijia Zhu, Yaojie Lu, Shuhen Zhou, Yanjiang Liu",
        "link": "https://arxiv.org/abs/2501.01830",
        "github_repo": null,
        "summary": "- AUTO-RT, a reinforcement learning framework, automatically explores and optimizes complex attack strategies to uncover security vulnerabilities in Large Language Models (LLMs) through malicious queries.\n- It introduces Early-terminated Exploration to accelerate exploration by focusing on high-potential attack strategies and a Progressive Reward Tracking algorithm to dynamically refine the search trajectory towards successful vulnerability exploitation.\n- It operates in a black-box setting, requiring only access to a model's textual outputs, making it adaptable to diverse LLMs.\n- Experiments across various LLMs demonstrate that AUTO-RT detects a broader range of vulnerabilities with a faster detection speed and 16.63% higher success rate compared to existing methods.\n- AUTO-RT improves exploration efficiency and automatically optimizes attack strategies.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/icip-cas/Auto-RT"
        ],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "Samba-asr state-of-the-art speech recognition leveraging structured state-space models",
        "authors": "Kartik-angadi, kruthika, SyedAbdul",
        "link": "https://arxiv.org/abs/2501.02832",
        "github_repo": null,
        "summary": "- Samba-ASR is a novel Automatic Speech Recognition (ASR) model employing the Mamba architecture for both encoding and decoding, built upon state-space models (SSMs).\n- Unlike transformer-based ASR models, Samba-ASR utilizes efficient state-space dynamics to model dependencies, resulting in significant performance gains and linear scaling with input length.\n- Across benchmarks such as Gigaspeech, LibriSpeech, and SPGISpeech, Samba-ASR demonstrates state-of-the-art performance, surpassing existing open-source transformer-based ASR models.\n- Notably, Samba-ASR achieves a Word Error Rate (WER) of 1.17% on LibriSpeech Clean, 2.48% on LibriSpeech Other, 9.12% on Gigaspeech, and 1.84% on SPGISpeech.\n- The inherent efficiency of the Mamba architecture translates to reduced training time and inference latency, making Samba-ASR a scalable and robust solution for diverse ASR tasks.",
        "classification": [
            "Automatic Speech Recognition"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-07"
    },
    {
        "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use",
        "authors": "Yufei Xu, Xuesong Yao, Zhengyin Du, Junjie Ye, maverick1994",
        "link": "https://arxiv.org/abs/2501.02506",
        "github_repo": null,
        "summary": "- ToolHop, a new dataset with 995 multi-hop queries and 3,912 associated tools, is introduced to evaluate large language models (LLMs) in multi-hop tool use scenarios.\n- ToolHop employs a query-driven data construction approach involving tool creation, document refinement, and code generation.\n- An evaluation of 14 LLMs across five model families (LLaMA, Qwen, Gemini, Claude, and GPT) reveals that even the top-performing model (GPT-4) only achieves 49.04% accuracy.\n- Analysis suggests that providing LLMs with tools significantly improves their performance, but there are still significant challenges.\n- Different LLM families exhibit distinct tool-use patterns, with Qwen tending towards parallel calls that result in hallucinations, while GPT leverages tool feedback effectively to improve tool usage.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/bytedance-research/ToolHop"
        ],
        "date": "2025-01-07"
    }
]