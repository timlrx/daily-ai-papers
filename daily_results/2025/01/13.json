[
    {
        "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
        "authors": "Wenlong Gao, Tianshu Wu, Ergogogogo, JiyaoZhang, pmj110119",
        "link": "https://arxiv.org/abs/2501.03841",
        "github_repo": null,
        "summary": "- OmniManip is an open-vocabulary robotic manipulation method that bridges the gap between high-level reasoning of Vision-Language Models (VLMs) and the low-level precision needed for manipulation by introducing object-centric interaction primitives as spatial constraints.\n- These primitives, defined within an object's canonical space, translate VLM reasoning into actionable 3D constraints, enabling precise manipulation.\n- The system uses a dual closed-loop approach: one for planning through primitive resampling, interaction rendering, and VLM checking, and another for execution via 6D pose tracking.\n- The method is evaluated on diverse robotic manipulation tasks and demonstrates strong zero-shot generalization capabilities without requiring VLM fine-tuning.\n- It also shows promise for automating large-scale simulation data generation for robotic manipulation.",
        "classification": [
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [
            "https://omnimanip.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
        "authors": "Sung Ju Hwang, jinheon, KangsanKim71, starsuzi",
        "link": "https://arxiv.org/abs/2501.05874",
        "github_repo": null,
        "summary": "- VideoRAG, a novel framework for Retrieval-Augmented Generation (RAG) over video corpora, is introduced, addressing the limitations of existing RAG approaches that primarily focus on text or static images and overlook the rich multimodal information in videos.\n- VideoRAG dynamically retrieves relevant videos based on their relevance to user queries and integrates both visual and textual information from these videos into the answer generation process using Large Video Language Models (LVLMs).\n-  For videos lacking textual annotations (like subtitles), VideoRAG utilizes automatic speech recognition to generate auxiliary text, enabling the use of both visual and textual modalities even when explicit textual data is absent.\n- Experimental results on the WikiHowQA and HowTo100M datasets demonstrate that VideoRAG significantly outperforms relevant RAG baselines, including text-based RAG and video-based RAG that only uses textual video descriptions.\n- Ablation studies highlight the importance of video content and both visual and textual modalities in improving the quality and informativeness of generated responses.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
        "authors": "qiaozc, zyh, HelloJiang, Niujunbo2002, JoeLeelyf",
        "link": "https://arxiv.org/abs/2501.05510",
        "github_repo": "https://github.com/JoeLeelyf/OVO-Bench",
        "summary": "- This paper introduces OVO-Bench (Online-VideO-Benchmark), a novel benchmark designed to evaluate the online video understanding capabilities of Video-LLMs.\n- OVO-Bench focuses on evaluating temporal awareness by assessing models' abilities in Backward Tracing, Real-Time Visual Perception, and Forward Active Responding.\n- The benchmark comprises 12 tasks, 644 videos, and ~2800 human-curated meta-annotations with precise timestamps, covering diverse domains and video lengths.\n- Evaluation results reveal that current Video-LLMs struggle with online video understanding, showing a substantial gap compared to human performance, especially in tasks requiring temporal reasoning and dynamic adaptation.\n- The authors suggest that a more powerful LLM backbone and better temporal prioritization mechanisms are crucial for improving online video understanding capabilities in Video-LLMs.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Question Answering",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/JoeLeelyf/OVO-Bench"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
        "authors": "Dinura Dissanayake, hishamcholakkal, ahmedheakl, Ritesh-hf, omkarthawakar",
        "link": "https://arxiv.org/abs/2501.06186",
        "github_repo": null,
        "summary": "- This paper introduces LlamaV-01, a novel multimodal visual reasoning model trained using a multi-step curriculum learning approach, along with a new benchmark called Visual Reasoning-Chain (VRC-Bench) designed to evaluate step-by-step reasoning and a novel metric to assess reasoning quality at the granularity of individual steps.\n- VRC-Bench includes over 1,000 samples and 4,173 reasoning steps across eight diverse categories, including visual reasoning, math and logic, and scientific reasoning.\n- The proposed LlamaV-01 model leverages curriculum learning, progressively training on tasks of increasing complexity to enhance reasoning abilities and combines Beam search with multi-step curriculum learning to manage complexity, improve logical coherence, and generalize to challenging scenarios.\n- The model outperforms existing open-source models and performs favorably against closed-source models, achieving an average score of 67.3 with a 3.8% absolute gain over Llava-CoT across six benchmarks, while also being 5x faster during inference.\n- The model's code and benchmark are publicly available.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/mbzuai-oryx/LlamaV-01"
        ],
        "huggingface_urls": [
            "https://huggingface.co/omkarthawakar/LlamaV-01",
            "https://huggingface.co/datasets/omkarthawakar/VRC-Bench"
        ],
        "date": "2025-01-13"
    },
    {
        "title": "Enabling Scalable Oversight via Self-Evolving Critic",
        "authors": "Losin94, Benyou, yeshoubaizi, ziniuli, tangzhy",
        "link": "https://arxiv.org/abs/2501.05727",
        "github_repo": null,
        "summary": "- This paper introduces SCRIT (Self-evolving CRITic), a framework for enhancing the critique abilities of Large Language Models (LLMs) without external supervision.\n- SCRIT leverages a contrastive critique technique where the model analyzes student solutions by referencing correct solutions, along with a self-validation mechanism that ensures critique quality.\n- Implemented with Qwen2.5-72B-Instruct, SCRIT achieves up to a 10.3% improvement on critique-correction and error identification benchmarks and shows performance improvement across eight datasets in three scenarios.\n- The paper presents analyses showing that SCRIT's performance scales positively with data and model size, outperforms alternative approaches (Direct Critic and Bug-Injection Critic), and significantly benefits from its self-validation component.\n- It demonstrates consistent improvements across various problem domains, difficulties, and solution generation models.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning",
        "authors": "Ruimao, Xintao, Qiulin, ziyangy, Yuzhou914",
        "link": "https://arxiv.org/abs/2501.04698",
        "github_repo": null,
        "summary": "- ConceptMaster is a novel framework for Multi-Concept Video Customization (MCVC) that allows personalized video generation using multiple user-defined concepts without test-time tuning.\n- It addresses the identity decoupling problem in MCVC by learning decoupled multi-concept embeddings and injecting them into diffusion transformer models in a standalone manner using a Multi-Concept Injector (MC-Injector).\n- A dedicated data collection pipeline was created to build a dataset of over 1.3 million high-quality MCVC samples, which overcomes the scarcity of suitable training data.\n- A Multi-Concept Benchmark (MC-Bench) was introduced to evaluate concept fidelity, identity decoupling, and video generation quality across six concept composition scenarios.\n- Extensive experiments demonstrate ConceptMaster's superior performance over existing naive solutions and tuning-based methods, achieving high-quality video generation with accurate representation of multiple concepts.",
        "classification": [
            "Text-to-Video",
            "Multimodal",
            "Computer Vision"
        ],
        "github_urls": [
            "https://yuzhou914.github.io/ConceptMaster/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "Multi-subject Open-set Personalization in Video Generation",
        "authors": "universome, studyfang, willi-menapace, aliaksandr-siarohin, tschen",
        "link": "https://arxiv.org/abs/2501.06187",
        "github_repo": null,
        "summary": "- Video Alchemist, a latent diffusion transformer model, is introduced for multi-subject, open-set video personalization, allowing customization of both foreground subjects and backgrounds without requiring test-time optimization.\n- It leverages a novel Diffusion Transformer module that integrates conditional reference images and subject-level text prompts via cross-attention.\n- A new data construction pipeline with augmentations mitigates overfitting on training video frames by focusing on subject identity.\n- MSRVTT-Personalization, a new benchmark for multi-subject video personalization with varied conditioning modes, is introduced for evaluation.\n- Experiments on the MSRVTT-Personalization benchmark demonstrate superior performance over state-of-the-art methods in subject fidelity, text alignment, and video dynamics.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/snap-research/MSRVTT-Personalization"
        ],
        "date": "2025-01-13"
    },
    {
        "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding",
        "authors": "danielpaulroth, jw2yang, zyang39, mqliu, Fiaa",
        "link": "https://arxiv.org/abs/2501.05452",
        "github_repo": null,
        "summary": "- REFOCUS is a framework that improves multimodal Large Language Models (LLMs) ability to perform visual reasoning on structured images like tables and charts by enabling them to generate Python code to edit the input image.\n- REFOCUS guides the LLM's attention through visual edits such as drawing boxes, highlighting sections, and masking areas, simulating a visual chain of thought.\n- Experiments on table and chart VQA datasets show significant performance improvements over GPT-40 without visual editing, with average gains of 11.0% on table tasks and 6.8% on chart tasks.\n- A 14k training set created using REFOCUS and GPT-40 demonstrates that visual chain-of-thought supervision leads to better performance compared to training on standard VQA data or chain-of-thought data, with an 8.0% average gain over QA pairs and a 2.6% gain over CoT when fine-tuning a Phi-3.5-vision model.\n- Analysis suggests that REFOCUS enhances the LLM's visual grounding, OCR accuracy, and reduces hallucinations through selective attention.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Table Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/microsoft/Phi-3-vision-128k-instruct"
        ],
        "date": "2025-01-13"
    },
    {
        "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
        "authors": "Shuang Li, Joshua B. Tenenbaum, Antoniotorralbaborruel, yilundu, vsub851",
        "link": "https://arxiv.org/abs/2501.05707",
        "github_repo": null,
        "summary": "- This paper introduces multiagent finetuning, a novel approach to improve large language models (LLMs) by leveraging multiagent interaction and specialization.\n- Instead of training a single model, the approach trains multiple LLMs from the same base model and specializes each model to different functionalities like generating initial responses (generation agents) and critiquing/refining those responses (critic agents).\n- Each model is independently trained using data generated through multiagent debate between the models, fostering specialization and promoting response diversification.\n- Experiments across open-source and proprietary LLMs on a suite of reasoning tasks demonstrate significant performance gains and the ability to improve over more finetuning rounds compared to single-agent self-improvement methods.\n- The finetuned models exhibit better generalization capabilities to new datasets in a zero-shot setting.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://llm-multiagent-ft.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-01-13"
    },
    {
        "title": "Infecting Generative AI With Viruses",
        "authors": "fgmckee, dnoever",
        "link": "https://arxiv.org/abs/2501.05542",
        "github_repo": null,
        "summary": "- This paper introduces a novel approach to penetration testing for large language models (LLMs), focusing on their handling of image files containing embedded malware.\n- The researchers successfully embedded the EICAR test file, a harmless string used to test antivirus software, within JPEG images and uploaded them to several LLMs, including GPT-40, Microsoft Copilot, Google Gemini 1.5 Pro, and Anthropic Claude 3.5 Sonnet.\n- They demonstrated the LLMs' ability to process and even extract the embedded malware using Python scripts within their environments, raising concerns about potential vulnerabilities.\n- This research highlights the need for improved security measures in LLMs, especially in detecting and preventing the execution of potentially malicious code hidden within seemingly benign files.\n- The study also suggests further research into automated LLM file inspection, standardized security testing frameworks for LLMs, and investigation of cross-platform vulnerabilities.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-13"
    }
]