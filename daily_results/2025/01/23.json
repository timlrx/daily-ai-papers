[
    {
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "authors": "AS-7, haha-point, freesky, DejianYang, guoday",
        "link": "https://arxiv.org/abs/2501.12948",
        "github_repo": null,
        "summary": "- This paper introduces DeepSeek-R1, a large language model designed for enhanced reasoning capabilities, along with DeepSeek-R1-Zero, a model trained purely via reinforcement learning (RL) without supervised fine-tuning.\n- DeepSeek-R1-Zero showcases emergent reasoning abilities but suffers from readability and language mixing issues, prompting the development of DeepSeek-R1, which incorporates a multi-stage training pipeline with cold-start data and RL fine-tuning.\n- DeepSeek-R1 achieves comparable performance to OpenAI-01-1217 on several reasoning benchmarks and outperforms DeepSeek-V3.\n- Through knowledge distillation from DeepSeek-R1 to smaller models, even a 7B model surpasses existing open-source models on several benchmarks, and 32B and 70B models set new performance records.\n- The paper releases a series of distilled models based on Qwen and Llama.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
        "authors": "Senbao Shi, Li-Zhouyi, PigCatchingExpert, longyuewang, imryanxu",
        "link": "https://arxiv.org/abs/2501.12909",
        "github_repo": null,
        "summary": "- FilmAgent, a novel LLM-based multi-agent collaborative framework, automates end-to-end virtual film production, encompassing idea development, scriptwriting, cinematography, and actor actions within pre-built Unity 3D environments.\n- Employing two novel multi-agent collaboration strategies, Critique-Correct-Verify and Debate-Judge, the framework leverages LLMs as virtual crew members (director, screenwriter, actors, cinematographer) to enhance communication and refine film elements iteratively.\n- Human evaluations across 15 film ideas demonstrate FilmAgent's superiority, achieving an average score of 3.98 out of 5, significantly outperforming single-agent baselines and even surpassing OpenAI's larger reasoning model, o1, in a multi-agent setting.\n- Compared with OpenAI's text-to-video model Sora, FilmAgent exhibits stronger storytelling and coherence in longer videos, owing to the pre-designed 3D spaces and character interactions, while Sora showcases greater adaptability and stylistic flexibility but lacks consistency and physics compliance.\n- The framework addresses limitations in existing automated film production methods by incorporating communication-driven collaboration, offering a promising approach to end-to-end film automation using AI agents.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
        "authors": "Yu Cheng, linjieli222, Xiaoye08, huxy912, yaful",
        "link": "https://arxiv.org/abs/2501.12895",
        "github_repo": "https://github.com/yafuly/TPO",
        "summary": "- Test-Time Preference Optimization (TPO) is introduced, a framework that aligns Large Language Model (LLM) outputs with human preferences during inference without retraining.\n- TPO translates reward signals into textual critiques and utilizes them as rewards to iteratively refine LLM responses, unlike methods relying solely on numerical rewards.\n- Evaluations across various NLP tasks demonstrate that TPO progressively improves alignment with human preferences, and in some cases, the unaligned LLM with TPO surpasses its aligned counterpart.\n- TPO efficiently scales with both search width and depth during inference.\n- Case studies illustrate TPO's ability to exploit LLMs' capacity to interpret and act on reward signals, making it a practical, lightweight alternative for on-the-fly preference optimization.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/yafuly/TPO"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
        "authors": "Sicong, Guanzheng, Zhiqiang007, ClownRat, CausalLi",
        "link": "https://arxiv.org/abs/2501.13106",
        "github_repo": "https://github.com/DAMO-NLP-SG/VideoLLaMA3",
        "summary": "- VideoLLaMA3 is a multimodal foundation model for image and video understanding that leverages a vision-centric training paradigm and framework design. \n- The model architecture incorporates an any-resolution vision tokenization (AVT) technique, enabling it to handle varying input resolutions, and a differential frame pruner (DiffFP) for efficient video compression.\n- VideoLLaMA3 is trained in four stages: vision encoder adaptation, vision-language alignment, multi-task fine-tuning, and video-centric fine-tuning. \n- The model outperforms existing state-of-the-art models on a variety of benchmarks, demonstrating strong abilities in image and video comprehension, including chart and document understanding, mathematical reasoning, temporal reasoning, and grounding. \n- Notably, VideoLLaMA3 demonstrates significant improvements in chart understanding and vision-related math problem solving in image understanding, as well as achieving state-of-the-art performance in multiple video understanding benchmarks.",
        "classification": [
            "Multimodal",
            "Image-Text-to-Text",
            "Visual Question Answering",
            "Document Question Answering",
            "Video-Text-to-Text",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/DAMO-NLP-SG/VideoLLaMA3"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
        "authors": "ChonghuaLiao, DuChenZhuang, shelowize, xingbowei, KbsdJames",
        "link": "https://arxiv.org/abs/2501.12599",
        "github_repo": null,
        "summary": "- Kimi k1.5 is a multimodal large language model (LLM) trained using reinforcement learning (RL) with a focus on scaling context length for improved reasoning abilities.\n- The model architecture is based on a Transformer decoder and incorporates techniques like partial rollouts, improved policy optimization, and length penalty for efficient RL training.\n- It achieves state-of-the-art results on multiple benchmarks, including 77.5 on AIME, 96.2 on MATH 500, and 94-th percentile on Codeforces, matching OpenAI's GPT-01 on MathVista.\n- Long2short methods are introduced to improve short-context models by transferring knowledge from long-context models, resulting in significantly improved token efficiency, for example, k1.5-short w/ rl achieves a Pass@1 score of 60.8 on AIME2024 while utilizing only 3,272 tokens on average.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning",
            "Question Answering",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Autonomy-of-Experts Models",
        "authors": "Yining Qian, kangzhanhui, shwu, Ruobing-Xie, AngLv",
        "link": "https://arxiv.org/abs/2501.13074",
        "github_repo": null,
        "summary": "- This paper introduces Autonomy-of-Experts (AoE), a novel Mixture-of-Experts (MoE) paradigm for large language models.\n- AoE allows experts to autonomously decide whether to process inputs based on the scale of their internal activations, eliminating the need for a separate router.\n- By pre-computing and ranking internal activation norms, only the top-activated experts process each token, while others abort, thereby improving efficiency.\n- The overhead of pre-computing activations is reduced through low-rank weight factorization.\n- Experimental results demonstrate that AoE outperforms traditional MoE models on downstream tasks with comparable efficiency across various model sizes up to 4 billion parameters.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament",
        "authors": "Yixin Cao, Rui Min, Zijun Yao, Yantao Liu, juanli",
        "link": "https://arxiv.org/abs/2501.13007",
        "github_repo": null,
        "summary": "- This paper introduces Pairwise Reward Model (Pairwise RM), a novel approach for Best-of-N (BoN) sampling in Large Language Models (LLMs), particularly for math reasoning tasks.\n- Instead of assigning absolute scores, Pairwise RM evaluates two candidate solutions simultaneously, using a knockout tournament to select the best solution through pairwise comparisons.\n- This method addresses limitations of traditional reward models by eliminating arbitrary scoring and enabling cross-validation.\n- A new dataset, PAIRWISE-443K, with 443K pairwise comparisons is created for model training.\n- Experimental results on MATH-500 and Olympiad Bench demonstrate significant improvements over baseline models, showing a 40% to 60% relative improvement on the top 50% challenging problems in MATH-500.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/THU-KEG/PairwiseRM/"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
        "authors": "Yibo Wang, Haiying He, Li Shen, cxc361461518, iNk233",
        "link": "https://arxiv.org/abs/2501.12570",
        "github_repo": "https://github.com/StarDewXXX/O1-Pruner",
        "summary": "- This paper introduces Length-Harmonizing Fine-Tuning (O1-Pruner), a novel method to optimize long-thought reasoning in Large Language Models (LLMs) for mathematical problem-solving.\n- O1-Pruner addresses the issue of reasoning length disharmony in LLMs, where models generate solutions of varying lengths with shorter, yet accurate, solutions often available, leading to computational redundancy.\n- The method utilizes a Reinforcement Learning (RL)-style fine-tuning approach, incorporating an accuracy constraint to ensure that optimizing for shorter reasoning processes does not compromise problem-solving accuracy.\n- Experimental results on benchmark datasets like MATH, GSM8k, and GaoKao demonstrate that O1-Pruner achieves a better balance between solution length and accuracy compared to baseline and other competing methods like SFT and DPO, leading to improved inference efficiency.\n- The proposed method is evaluated using both accuracy and a new metric called Accuracy-Efficiency Score (AES), showing consistent improvements in reducing solution length while maintaining or improving accuracy.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/StarDewXXX/O1-Pruner"
        ],
        "huggingface_urls": [],
        "date": "2025-01-23"
    },
    {
        "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
        "authors": "Ilankad23, Eladlev",
        "link": "https://arxiv.org/abs/2501.11067",
        "github_repo": "https://github.com/plurai-ai/intellagent",
        "summary": "- This paper introduces IntellAgent, a multi-agent framework for evaluating conversational AI systems.\n- IntellAgent leverages a novel approach by automating the generation of diverse, synthetic scenarios that test conversational AI agents across various aspects including multi-turn dialogues, policy adherence, and API usage.\n- IntellAgent uses a graph-based policy model to represent the relationships and complexities of policy interactions, enabling fine-grained diagnostics.\n- Experimental results demonstrate a strong correlation between model performance on the IntellAgent benchmark and the T-bench, despite IntellAgent relying entirely on synthetic data.\n- The findings indicate a decrease in model performance with increasing complexity and variations in capabilities across different policy categories, highlighting IntellAgent's ability to provide detailed insights for targeted optimization.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/plurai-ai/intellagent",
            "https://github.com/langchain-ai/langgraph"
        ],
        "date": "2025-01-23"
    }
]