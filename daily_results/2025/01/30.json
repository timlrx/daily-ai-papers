[
    {
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "authors": "Xiang Yue, wenhu, ubowang",
        "link": "https://arxiv.org/abs/2501.17703",
        "github_repo": null,
        "summary": "- This paper introduces Critique Fine-Tuning (CFT), a novel training strategy for Large Language Models (LLMs) that focuses on learning to critique responses rather than simply imitating correct ones.\n- CFT leverages the power of advanced LLMs such as GPT-40 to generate critiques for noisy responses, guiding the model towards deeper analysis and understanding.\n- CFT models consistently outperform SFT models across several benchmarks, demonstrating a 4-10% improvement on mathematical reasoning tasks while using significantly less data (50K samples vs. 2M+).\n- The effectiveness of CFT is shown across a variety of base models (Qwen2.5, DeepSeek-Math-7B) and datasets, showcasing its potential as an efficient training strategy.\n- CFT highlights that learning to critique offers a more effective way to train LLMs, promoting critical thinking and nuanced understanding often overlooked by standard SFT methods.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-30"
    },
    {
        "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
        "authors": "Simon Gosset, Caroline Vateau, Louis Ladan, Neyri56, clementdesroches",
        "link": "https://arxiv.org/abs/2501.14334",
        "github_repo": null,
        "summary": "- This paper proposes a methodology to estimate the environmental impact of a company's AI portfolio, focusing on energy consumption and its multi-factor impacts like GHG emissions, water usage, and resource depletion.\n- The study finds that large generative AI models consume significantly more energy (up to 4600x) than traditional models and that energy consumption scales substantially with model size and workflow complexity.\n- The paper projects AI electricity use up to 2030 under various scenarios, with a potential 24.4x increase in a high-adoption scenario driven by widespread Generative AI and agents.\n- The authors advocate for standardized environmental assessment frameworks, greater transparency from AI providers, and a \"Return on Environment\" metric to align AI development with sustainability goals.\n- The paper explores the feasibility of achieving a 90% GHG reduction by 2030 through improvements in PUE, energy mix decarbonization, and hardware efficiency and proposes an environmental scoring label for AI similar to eco-scores in other industries.",
        "classification": [
            "Natural Language Processing",
            "Other"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-01-30"
    },
    {
        "title": "Atla Selene Mini: A General Purpose Evaluation Model",
        "authors": "Kyle Dai, Jackson Golden, Henry Broomfield, Andrei Alexandru, NinaCalvi",
        "link": "https://arxiv.org/abs/2501.17195",
        "github_repo": null,
        "summary": "- Atla Selene Mini is a new small language model as a judge (SLMJ), designed for general purpose evaluation tasks.\n- It outperforms existing SLMJ and GPT-40-mini on 11 out-of-distribution benchmarks covering absolute scoring, classification, and pairwise preference tasks.\n- The model leverages a curated dataset augmented with synthetic critiques and a combined Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) training approach.\n- Selene Mini excels in real-world scenarios, demonstrating strong zero-shot agreement with human expert evaluations on financial and medical datasets and robustness to variations in prompt format. \n- It also leads in a live community-driven Judge Arena benchmark, further highlighting its robust evaluation capabilities.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B"
        ],
        "date": "2025-01-30"
    },
    {
        "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
        "authors": "Miriam Ugarte, ssegura, japarejo, pablovalle, aitorarrieta",
        "link": "https://arxiv.org/abs/2501.17749",
        "github_repo": null,
        "summary": "- This research paper details the pre-deployment safety testing of OpenAI's o3-mini large language model (LLM) using the ASTRAL tool.\n- ASTRAL automatically generates unsafe test inputs categorized by style and persuasion technique across 14 safety areas.\n- A total of 10,080 tests were generated and executed, with 87 confirmed instances of unsafe behavior after manual verification.\n- The o3-mini model demonstrated improved safety compared to older OpenAI models, possibly due to integrated policy violation detection.\n- Key areas of concern still exist, particularly surrounding recent controversial topics and specific categories like terrorism and child abuse.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/Trust4AI/ASTRAL"
        ],
        "huggingface_urls": [],
        "date": "2025-01-30"
    },
    {
        "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
        "authors": "ling1119, sftekin25, tawreos, SihaoHu, TianshengHuang",
        "link": "https://arxiv.org/abs/2501.17433",
        "github_repo": "https://github.com/git-disl/Virus",
        "summary": "- This paper introduces \"Virus,\" a novel attack method targeting Large Language Models (LLMs) that bypasses guardrail moderation systems designed to prevent harmful fine-tuning.\n- Virus utilizes a dual-objective optimization approach to craft harmful training data, minimizing detectability by the guardrail while maximizing the degradation of the LLM's safety alignment.\n- Experimental results demonstrate Virus's effectiveness, achieving up to 100% leakage ratio (bypassing the guardrail) and increasing harmful scores by up to 21.8% compared to standard harmful fine-tuning attacks.\n- The key finding is that relying solely on guardrail moderation is insufficient for mitigating harmful fine-tuning risks, highlighting the need for more robust defense mechanisms.\n- The optimized datasets generated by Virus are publicly available for further research and analysis.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/git-disl/Virus"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/anonymous4486/Virus"
        ],
        "date": "2025-01-30"
    }
]