[
    {
        "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
        "authors": "Liam-Liu, kangz, aaabiao, BingliW, mkj69",
        "link": "https://arxiv.org/abs/2502.14739",
        "github_repo": null,
        "summary": "- This paper introduces SuperGPQA, a comprehensive benchmark designed to evaluate graduate-level knowledge and reasoning capabilities of Large Language Models (LLMs) across 285 academic disciplines.\n- SuperGPQA contains 26,529 multiple-choice questions with an average of 9.67 options per question, sourced from reference books and collaboratively refined using human-LLM filtering techniques to ensure question quality and difficulty.\n- Experimental results on state-of-the-art LLMs reveal significant room for improvement in diverse knowledge domains, with the best performing models (reasoning-focused) achieving around 61.82% accuracy.\n- The benchmark demonstrates the importance of reasoning capabilities, the effectiveness of instruction tuning, and varying performance across disciplines. \n- The paper emphasizes the need for comprehensive cross-domain evaluation in LLMs and offers methodological guidance for building similar benchmarks",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
        "authors": "Xiao Wang, talfanevans, ibomohsin, AlexeyG, mitsch",
        "link": "https://arxiv.org/abs/2502.14786",
        "github_repo": "https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md",
        "summary": "- SigLIP 2, a family of multilingual vision-language encoders, builds upon the original SigLIP by incorporating techniques like captioning-based pretraining, self-supervised losses, and online data curation.\n- The model architecture uses a ViT architecture and supports multiple resolutions, preserving the input's native aspect ratio in a NaFlex variant.\n- SigLIP 2 outperforms its predecessor and other baselines in zero-shot classification, image-text retrieval, and transfer learning tasks for Vision-Language Models (VLMs), as demonstrated by improved performance on benchmarks like ImageNet, COCO, and Flickr.\n- Additionally, it shows significant improvements in localization and dense prediction tasks, leading to better performance in applications such as referring expression comprehension and open-vocabulary segmentation.\n- Trained on a diverse, de-biased dataset, SigLIP 2 improves fairness and multilingual understanding compared to the original SigLIP.",
        "classification": [
            "Multimodal",
            "Image Feature Extraction",
            "Zero-Shot Classification",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md"
        ],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
        "authors": "Nikolay Bashlykov, Nicholas Roberts, Lovish Madaan, rraileanu, dnathani",
        "link": "https://arxiv.org/abs/2502.14499",
        "github_repo": null,
        "summary": "- Meta introduces MLGYM, a new Gym environment and framework designed for developing and evaluating Large Language Model (LLM) agents for diverse, open-ended AI research tasks.\n- MLGYM-Bench, an accompanying benchmark, features 13 tasks spanning computer vision, natural language processing, reinforcement learning, and game theory, requiring agents to exhibit research skills like hypothesis generation and experimental design.\n- Initial evaluations of frontier LLMs (Claude, Llama, GPT, Gemini) reveal that while current models can improve upon given baselines (typically by optimizing hyperparameters), they struggle with generating novel hypotheses or significant improvements.\n- MLGYM's flexible design enables research on diverse training algorithms (e.g., reinforcement learning, curriculum learning) and accepts diverse artifacts (model weights, algorithms) for evaluation, unlike existing frameworks.\n- A new metric adapted from optimization and AutoML literature is introduced to compare LLM agent performance across tasks with differing performance metrics.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Computer Vision",
            "Other"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/MLGym"
        ],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "S*: Test Time Scaling for Code Generation",
        "authors": "Shangyin Tan, Xiuyu Li, Chengkun Cao, Dacheng Li, eva98",
        "link": "https://arxiv.org/abs/2502.14382",
        "github_repo": "https://github.com/NovaSky-AI/SkyThought",
        "summary": "- S* is a novel hybrid test-time scaling framework for code generation that improves both the coverage and selection accuracy of generated code.\n- It extends parallel scaling with sequential scaling using iterative debugging based on execution feedback, and introduces a new adaptive selection mechanism grounded in execution where LLM generate distinguishing inputs for pairwise comparison.\n- The method is evaluated on 12 large language and reasoning models across various model sizes and families and two benchmarks (LiveCodeBench and CodeContests).\n- Results show that S* consistently boosts performance across different types of LLMs.\n- In particular, S* enables smaller models to outperform larger models in the same family, non-reasoning models to outperform reasoning models, and open models to be competitive with closed models.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/NovaSky-AI/SkyThought"
        ],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
        "authors": "Vasily Konovalov, Daniil Moskovskiy, Maria Marina, msalnikov, memyprokotow",
        "link": "https://arxiv.org/abs/2502.14502",
        "github_repo": null,
        "summary": "- This paper investigates the extent to which new factual knowledge can be integrated into Large Language Models (LLMs) using Low-Rank Adaptation (LoRA) without compromising pre-existing knowledge.\n- Experiments fine-tuning Llama-3.1-8B-instruct with varying amounts of new knowledge reveal that optimal performance is achieved with a mixture of known and new facts in the training data.\n- However, over-reliance on new information can negatively impact performance on external question-answering benchmarks and introduce biases towards overrepresented entities.\n- Two fine-tuning techniques are introduced to counter these effects: incorporating paraphrased new facts and adding facts already known to the model, with analysis indicating that training with HighlyKnown samples is a preferable strategy.\n- Analysis of knowledge shifts show that positive shifts arise when the model correctly answer questions it couldn't answer before, while negative shifts occur due to the model's convergence on over-represented answers from the training set.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/AIRI-Institute/knowledge-packing"
        ],
        "huggingface_urls": [
            "https://hf.co/meta-llama/Meta-Llama-3-8B-Instruct",
            "https://hf.co/meta-llama/Meta-Llama-3-70B-Instruct"
        ],
        "date": "2025-02-21"
    },
    {
        "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
        "authors": "Jaewoo Kang, Minbyul Jeong, Jungwoo Park, Chanwoong Yoon, Yein Park",
        "link": "https://arxiv.org/abs/2502.14258",
        "github_repo": null,
        "summary": "- This paper introduces the concept of \"Temporal Heads\" in large language models (LLMs), which are specialized attention heads responsible for processing time-dependent information.\n- Through circuit analysis and targeted ablation experiments, the researchers demonstrate that these heads play a crucial role in recalling time-sensitive facts.\n- Disabling these heads selectively impairs the model's ability to retrieve temporally accurate information while leaving other functionalities intact.\n- The researchers further show that these heads are activated by both numerical and textual time cues and that their activations can be manipulated to edit temporal knowledge within the model.\n- The findings shed light on the internal mechanisms of temporal reasoning in LLMs and pave the way for more refined control over their ability to represent and utilize time-dependent knowledge.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/dmis-lab/TemporalHead"
        ],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
        "authors": "Jifan Yu, Yushi Bai, Daniel Zhang-Li, Yucheng Wang, Shangqing Tu",
        "link": "https://arxiv.org/abs/2502.14834",
        "github_repo": "https://github.com/THU-KEG/LongWriter-V",
        "summary": "- LongWriter-V introduces a new approach to enable ultra-long and high-fidelity generation in vision-language models (VLMs).\n- A new dataset, LongWriter-V-22k, comprising 22,158 examples with multiple input images, instructions, and long outputs (0-10,000 words) was created using a plan-and-write approach with GPT-4.\n- Iterative Direct Preference Optimization (IterDPO) was proposed to improve the quality of long outputs by breaking them into segments and using iterative corrections to form preference pairs, enabling efficient utilization of human feedback.\n- A benchmark, MMLongBench-Write, featuring six tasks designed to evaluate the long-generation capabilities of VLMs was developed to show current VLMs struggle to generate coherent outputs exceeding 1,000 words. \n- The proposed 7B parameter model, trained with the new dataset and IterDPO, achieved impressive performance, outperforming larger proprietary models such as GPT-40 on this benchmark.",
        "classification": [
            "Image-Text-to-Text",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/THU-KEG/LongWriter-V"
        ],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
        "authors": "Yuqian Hong, Haoming Luo, Qingnan Ren, Zitian Gao, Tian Xie",
        "link": "https://arxiv.org/abs/2502.14768",
        "github_repo": null,
        "summary": "- This paper introduces Logic-RL, a rule-based reinforcement learning framework designed to improve the reasoning abilities of large language models (LLMs). \n- The framework trains LLMs on a synthetic dataset of Knights and Knaves (K&K) logic puzzles, utilizing a carefully designed reward system and the REINFORCE++ algorithm and incorporates modifications like KL loss and unbiased KL estimation for enhanced training stability and convergence.\n- The 7B model trained with Logic-RL demonstrates a remarkable ability to generalize to challenging math benchmarks (AIME and AMC) after training on just 5K logic problems, showing a 125% improvement on AIME and 38% on AMC compared to the base model.\n- The paper explores several research questions related to RL algorithm comparison, the impact of thinking tokens, emergence of reasoning behaviors, generalization to OOD tasks, and the role of curriculum learning.\n- The study provides valuable insights into the dynamics of RL training for reasoning in LLMs and opens avenues for future research.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
        "authors": "Junyang Wang, Yuyang Wanyan, Haiyang Xu, Xi Zhang, Haowei Liu",
        "link": "https://arxiv.org/abs/2502.14282",
        "github_repo": null,
        "summary": "- PC-Agent, a hierarchical multi-agent framework, is proposed for automating complex tasks on PCs, addressing the challenges of intricate interactive environments and inter-app workflows.\n- The framework incorporates an Active Perception Module (APM) to enhance fine-grained perception and operation on screen elements and text, leveraging accessibility trees and an MLLM-driven intention understanding agent with OCR.\n- A hierarchical structure decomposes decision-making into instruction, subtask, and action levels, managed by Manager, Progress, and Decision agents, respectively, enabling efficient handling of inter-subtask dependencies.\n- A Reflection agent provides feedback on action outcomes, allowing for dynamic adjustments and error correction, thus improving task completion rates.\n- PC-Agent significantly outperforms previous methods on PC-Eval, a new benchmark with real-world complex PC tasks, demonstrating a 32% improvement in task success rate over state-of-the-art methods.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
        "authors": "Jiaqi Chen, Xingyan Liu, Cheng Liu, Peisong Wang, Ruotian Ma",
        "link": "https://arxiv.org/abs/2502.12853",
        "github_repo": "https://github.com/NineAbyss/S2R",
        "summary": "- S2R, a new framework, enhances LLM reasoning by teaching models to self-verify and self-correct during inference, focusing on iterative self-verification and self-correction behaviors.\n- It initializes LLMs with these behaviors through supervised fine-tuning on curated data and further strengthens them with outcome and process-level reinforcement learning.\n- Qwen2.5-math-7B, trained with S2R using only 3.1k samples, shows significant improvement, achieving accuracy from 51.0% to 81.6% on Math500 and outperforming models trained on equivalent long-CoT distilled data (80.2%).\n- S2R effectively employs both outcome and process-level reinforcement learning to boost self-verification and self-correction capabilities and explores more flexible and effective test-time scaling.\n- Experiments across various base models and benchmarks, including out-of-domain general tasks like MMLU-PRO, demonstrate the effectiveness and generalizability of the learned self-verifying and self-correcting abilities.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/NineAbyss/S2R"
        ],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
        "authors": "Zi-Wen Liu, basil2115",
        "link": "https://arxiv.org/abs/2502.14372",
        "github_repo": null,
        "summary": "- This paper introduces a reinforcement learning (RL) approach for designing quantum error-correcting codes with low check weights, a critical parameter for efficient fault tolerance.\n- The RL agent interacts with a stabilizer code's Tanner graph, adding or removing edges to minimize check node degrees while preserving code distance, utilizing Proximal Policy Optimization (PPO) with action masking for efficient exploration within weight constraints.\n- The method demonstrates substantial qubit overhead reduction compared to existing techniques (up to 73x improvement), achieving distances up to 4x larger than prior RL code design methods, reaching d\u224835 for weight 6, degree 3 codes, thereby making them practical for near-future devices.\n- The approach reveals that optimizing weight with distance constraints is significantly more effective for RL than optimizing distance with weight constraints, due to the high complexity of computing code distance.\n- The approach shows the potential of RL for discovering improved finite-size quantum error-correcting codes, potentially paving the way for faster realization of fault-tolerant quantum computation.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "Dynamic Concepts Personalization from Single Videos",
        "authors": "Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Or Patashnik, Rameen Abdal",
        "link": "https://arxiv.org/abs/2502.14844",
        "github_repo": null,
        "summary": "- This paper introduces Set-and-Sequence, a novel framework for personalizing diffusion transformer-based text-to-video models with dynamic concepts (entities defined by appearance and motion).\n- The two-stage framework first learns an identity basis from an unordered set of video frames to capture appearance, followed by motion residual encoding on the full video sequence to capture motion dynamics.\n- This approach enables unprecedented editability and compositionality, allowing for tasks like blending dynamic concepts (e.g. fire and water), editing camera motion and expressions, and introducing localized text-driven changes. \n- Evaluating on a human-centric video dataset, the model exhibits higher fidelity in appearance preservation, motion coherence, and text adherence compared to baselines like DreamVideo, NewMove, and DreamMix.\n- The two-stage approach, including dropout regularization, outperforms single-stage methods in an ablation study, achieving a better reconstruction-editability trade-off.",
        "classification": [
            "Text-to-Video",
            "Video Classification",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
        "authors": "Luca Weihs, Tanmay Gupta, Matt Deitke, Ajay Patel, Yue Yang",
        "link": "https://arxiv.org/abs/2502.14846",
        "github_repo": null,
        "summary": "- This paper introduces CoSyn, a framework that leverages large language models (LLMs) to generate synthetic text-rich multimodal data for training vision-language models (VLMs).\n- CoSyn uses LLMs to generate code in various formats (Python, HTML, LaTeX) that renders synthetic text-rich images, and then uses the code as context to generate corresponding textual instructions, creating instruction-tuning data.\n- Using CoSyn, the authors created CoSyn-400k, a dataset with 400k images and 2.7M instruction-tuning examples, and demonstrated state-of-the-art performance on 7 text-rich VQA benchmarks, outperforming open-source models like Llama 3.2 and proprietary models like GPT-4V and Gemini 1.5 Flash.\n-  CoSyn also generates synthetic pointing data to enable VLMs to ground information within images, achieving state-of-the-art performance on the ScreenSpot click prediction benchmark.\n- Further analysis demonstrates CoSyn's ability to improve zero-shot generalization on novel tasks, mitigate dataset biases, and enable efficient data generation for chain-of-thought reasoning in VLMs.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Document Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/yyupenn/DocPointQA",
            "https://huggingface.co/datasets/yyupenn/NutritionQA"
        ],
        "date": "2025-02-21"
    },
    {
        "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
        "authors": "Dinh Bach Vu, Alan Dao",
        "link": "https://arxiv.org/abs/2502.14669",
        "github_repo": null,
        "summary": "- AlphaMaze, a novel two-stage training framework, enhances the visual spatial reasoning abilities of Large Language Models (LLMs) for maze navigation by combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO).\n- The model uses a tokenized representation of the maze as input, which allows the LLM to \"see\" the structure and plan its movements.\n- The model significantly outperforms a baseline model, achieving a 93% success rate on MazeBench, a proposed benchmark for evaluating maze-solving capabilities, while the baseline model has 0% accuracy.\n- GRPO refines the model's decision-making, leading to emergent chain-of-thought behaviors and self-correction capabilities, similar to those observed in DeepSeek-R1.\n- The research demonstrates the potential of combining SFT and GRPO to enhance visual reasoning in LLMs and suggests broader applications in robotics and other visual AI domains that involve spatial understanding and sequential decision-making.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild",
        "authors": "Goran Glava\u0161, Anne Lauscher, saadob12",
        "link": "https://arxiv.org/abs/2502.12769",
        "github_repo": null,
        "summary": "- This paper investigates the hallucination rates of Large Language Models (LLMs) across 30 languages in a knowledge-intensive, long-form question answering setting, representative of real-world LLM usage.\n- A multilingual hallucination detection model is trained using a translate-train approach, where an English hallucination dataset is machine-translated into 30 languages.\n- The study uses a new benchmark, MFAVA, containing both silver (synthetic) and gold (human-annotated) hallucination evaluation datasets for a subset of languages.\n- It is found that while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, length-normalized hallucination rates do not correlate with a language's digital representation.\n- Smaller LLMs exhibit higher hallucination rates compared to larger models and models supporting more languages tend to hallucinate more.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/WorldHellow/mHallucinations-LLM"
        ],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
        "authors": "Zeyu Zhang, Jonathan Tonglet, Yuan Huang, Jingpu Yang, Ziruibest",
        "link": "https://arxiv.org/abs/2502.13759",
        "github_repo": null,
        "summary": "- This paper introduces GeoComp, a large-scale dataset for geolocation with 25 million metadata entries and 3 million geo-tagged locations collected from a geolocation game with 740K users.\n- It proposes GeoCoT, a novel multi-step reasoning framework that mimics human geolocation strategies by integrating contextual and spatial cues.\n- GeoCoT significantly boosts geolocation accuracy by up to 25% compared to existing large vision models (LVMs).\n- GeoEval, a new evaluation metric, assesses geolocation performance in terms of reasoning quality.\n-  The dataset includes diverse global locations, human performance labels to gauge task difficulty, and fills critical gaps in current models.",
        "classification": [
            "Computer Vision",
            "Image-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
        "authors": "Zhanjie Zhang, Jiasong Feng, Ao Ma, Jing Wang, Ke Cao",
        "link": "https://arxiv.org/abs/2502.14377",
        "github_repo": null,
        "summary": "- RelaCtrl, a Relevance-Guided Efficient Controllable Generation framework for Diffusion Transformers, is introduced, enabling efficient integration of control signals for optimized resource allocation.\n- The framework assesses the relevance of each transformer layer to control information using a \"ControlNet Relevance Score,\" tailoring control layer placement and capacity based on relevance strength.\n- A Two-Dimensional Shuffle Mixer (TDSM) replaces self-attention and FFN in copy blocks for efficient token and channel mixing, further enhancing efficiency.\n- Experimental results show RelaCtrl achieves superior performance with only 15% of the parameters and computational complexity of PixArt-d.\n- RelaCtrl demonstrates consistent improvements in control accuracy, image quality, and text-image consistency across various control tasks, indicating its effectiveness in controlled image generation.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    },
    {
        "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization",
        "authors": "David Jurgens, Isabelle Augenstein, Lu Wang, Zain Muhammad Mujahid, dwright37",
        "link": "https://arxiv.org/abs/2502.14409",
        "github_repo": null,
        "summary": "- This paper proposes a new task: long-context query-focused summarization with unstructured evidence citation, aiming to improve transparency and reliability of LLM-generated summaries.\n- It introduces SUnsET (Summaries with Unstructured Evidence Text), a synthetic dataset created through an inductive pipeline to address challenges like positional bias (lost-in-the-middle) and lack of diverse evidence.\n- The pipeline generates diverse titles, outlines, queries, summaries, and evidence passages, refines them, and validates accuracy.\n- Two fine-tuning schemes using LoRA (low-rank adaptation) are employed: standard (concatenated sections) and shuffled (randomized section order) to mitigate positional bias and enhance citation quality.\n- Experiments across five LLMs and four datasets show that fine-tuning with SUnsET improves evidence extraction, citation accuracy, and summary quality, mitigating lost-in-the-middle effects and bridging the performance gap with stronger models.",
        "classification": [
            "Summarization"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-02-21"
    }
]