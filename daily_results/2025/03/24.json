[
    {
        "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
        "authors": "Xinyu Zhang, Zhangqi Wang, Zhiyuan Wang, Qika, VentureZJ",
        "link": "https://arxiv.org/abs/2503.16905",
        "github_repo": null,
        "summary": "- This paper introduces MAPS, a Multi-Agent framework based on the Big Seven Personality theory and Socratic questioning, for solving Multimodal Scientific Problems (MSPs).\n- The framework employs seven distinct agents with specific skills to perform problem-solving in a cooperative manner.\n- A progressive four-agent solving strategy is proposed where each agent focuses on a specific stage of the problem-solving process (Interpreter, Aligner, Scholar, and Solver), and a Critic agent provides feedback to refine reasoning, simulating human reflection.\n- Experiments conducted on EMMA, Olympiad, and MathVista datasets demonstrates that MAPS outperforms current state-of-the-art models by 15.84% across all tasks and even surpasses human expert performance by 3.58%.",
        "classification": [
            "Multimodal",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/exoskeletonzj/MAPS"
        ],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization",
        "authors": "Jun Liu, Haiping Zhu, Zhangqi Wang, Qika, VentureZJ",
        "link": "https://arxiv.org/abs/2503.16874",
        "github_repo": null,
        "summary": "- This paper introduces MARS (Multi-Agent framework Incorporating Socratic guidance), a novel multi-agent framework designed for Automated Prompt Optimization (APO).\n- MARS utilizes a seven-agent architecture, including a Planner to devise flexible optimization paths and a Teacher-Critic-Student module for iterative prompt refinement through Socratic dialogue.\n- MARS addresses the limitations of existing APO methods, such as the inflexibility of fixed templates and inefficient search in prompt spaces.\n- Experimental results on 12 general tasks and 5 domain-specific datasets show that MARS surpasses previous state-of-the-art methods and significantly improves performance compared to original prompts and Chain-of-Thought (CoT) prompts.\n- An efficiency analysis demonstrates MARS effectively balances resource consumption and performance improvement.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints",
        "authors": "Xiaohong Liu, Zhenfei Yin, Xiufeng Song, FACEONG, IranQin",
        "link": "https://arxiv.org/abs/2503.16408",
        "github_repo": null,
        "summary": "- RoboFactory, a framework for automated data collection in multi-agent embodied systems, is introduced, focusing on generating safe and efficient training data.\n- The framework uses compositional constraints (logical, spatial, and temporal) to govern agent behavior and ensure collaboration effectiveness.\n- RoboBrain, a large language model (e.g., GPT-40), generates subgoals and constraints based on task instructions and feedback, while RoboChecker utilizes constraint interfaces to validate trajectories and prevent violations.\n- A new benchmark for embodied multi-agent manipulation, also named RoboFactory, featuring 11 tasks with varying agent numbers and environment settings within the ManiSkill simulator, is proposed.\n- Evaluation using Diffusion Policy on this benchmark showed promising results, highlighting the importance of sufficient data and the framework's ability to generate high-quality datasets, but also revealed performance degradation with increasing agent numbers and limitations in long-term temporal dependency learning.",
        "classification": [
            "Robotics",
            "Reinforcement Learning",
            "Multimodal"
        ],
        "github_urls": [
            "https://iranqin.github.io/robofactory/"
        ],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation",
        "authors": "Andrey Kuznetsov, Elizaveta Goncharova, Eduard Allakhverdov",
        "link": "https://arxiv.org/abs/2503.16660",
        "github_repo": null,
        "summary": "- This paper introduces a novel method for reducing the number of visual tokens generated by vision encoders in multimodal models, aiming to improve efficiency without compromising performance.\n- The method employs an autoencoder with a Gumbel-Softmax selection mechanism to identify and retain the most informative visual tokens, allowing for the reconstruction of less valuable features from more valuable ones.\n- Experiments on LLaVA-NeXT and LLaVA-OneVision models show that up to 50% of the visual context can be discarded with minimal performance loss on OCR-based tasks using the proposed selection method, outperforming random selection.\n- In general domain tasks, the performance is less affected by the visual token reduction, where retaining as little as 30% of the original tokens often yields performance on par with utilizing the full set. \n- This adaptive token reduction approach provides a promising direction for more scalable and efficient multimodal inference without significant performance degradation.",
        "classification": [
            "Multimodal",
            "Image Feature Extraction"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
        "authors": "Yuanzhi Zhu, Yao Teng, Zhijie Lin, ShuhuaiRen, Epiphqny",
        "link": "https://arxiv.org/abs/2503.16430",
        "github_repo": null,
        "summary": "- TokenBridge, a novel approach for autoregressive visual generation, bridges continuous and discrete token representations, maintaining the representational capacity of continuous tokens while preserving the modeling simplicity of discrete tokens.\n- It utilizes post-training quantization to directly discretize pre-trained VAE features, eliminating optimization instabilities associated with training-based discrete tokenizers.\n- A dimension-wise quantization and prediction strategy handles the exponentially large token space, eliminating the need for large codebooks and enabling computationally feasible autoregressive prediction.\n- Experiments show TokenBridge achieves reconstruction quality comparable to continuous VAEs and generation quality on par with continuous methods while using standard categorical prediction.\n- On ImageNet 256x256, TokenBridge achieves state-of-the-art results, demonstrating the effectiveness of bridging discrete and continuous paradigms for high-quality visual generation.",
        "classification": [
            "Text-to-Image",
            "Image-to-Text",
            "Unconditional Image Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
        "authors": "Wei Wang, Nanyun Peng, Fan Yin, Hritik Bansal, Yihe Deng",
        "link": "https://arxiv.org/abs/2503.17352",
        "github_repo": "https://github.com/yihedeng9/OpenVLThinker",
        "summary": "- OpenVLThinker-7B, a large vision-language model (LVLM), is introduced, trained using iterative self-improvement involving supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance complex reasoning abilities.\n- The model leverages a warm-start approach where SFT establishes initial reasoning structure and RL drives performance gains and generalization, utilizing distilled reasoning from text-based models in a vision-language context.\n- OpenVLThinker iteratively refines through RL, specifically Group Relative Policy Optimization (GRPO), using the improved model to generate refined SFT datasets for subsequent iterations.\n- Evaluation on visual reasoning benchmarks, including MathVista, MathVerse, and MathVision, demonstrates consistent improvement, surpassing or matching the performance of existing models like GPT-4 and Qwen2.5-VL-7B.\n- The work suggests the potential of combining SFT and RL for complex reasoning in LVLMs and provides early evidence for integrating R1-style reasoning into multimodal contexts.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/yihedeng9/OpenVLThinker"
        ],
        "date": "2025-03-24"
    },
    {
        "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
        "authors": "Max Kreminski, Yuqian Sun, Melissa Roemmele, Vishakh Padmakumar, John Joon Young Chung",
        "link": "https://arxiv.org/abs/2503.17126",
        "github_repo": null,
        "summary": "- This paper introduces diversified versions of Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO) for training large language models (LLMs) to generate diverse and high-quality creative writing.\n- The core idea is to incorporate deviation, a measure of how different a training instance is from others with the same prompt, into the training objective.\n- This approach encourages the model to learn from rarer, high-quality examples and promotes diversity in generated text.\n- Experiments show that the diversified methods achieve comparable quality to state-of-the-art models while significantly improving diversity, even surpassing human-generated text in some metrics.\n- The approach is robust to variations in dataset size and outperforms existing diversification methods like DivPO.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/mj-storytelling/DiversityTuning"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/euclaise/WritingPrompts_preferences"
        ],
        "date": "2025-03-24"
    },
    {
        "title": "Single Image Iterative Subject-driven Generation and Editing",
        "authors": "Idan Schwartz, Gal Chechik, yairshp",
        "link": "https://arxiv.org/abs/2503.16025",
        "github_repo": null,
        "summary": "- SISO (Single Image Subject Optimization) is a novel training-free method for personalizing text-to-image generation and image editing using only a single reference image of a subject.\n- SISO iteratively optimizes a subject similarity score between generated images and the single subject image during inference time, using pre-trained DINO and IR feature-based metrics.\n- The method adapts to any image generator and doesn't require training an encoder, avoiding limitations in training data distribution.\n- It demonstrates significant improvements in image quality, subject fidelity, and background preservation in both generation and editing tasks, evaluated on the ImageHub benchmark.\n- Human evaluations further validate its improved performance in terms of prompt alignment and naturalness.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems",
        "authors": "Jun Cen, Tao Feng, Yunqiu Xu, Felix Chen, JacobYuan",
        "link": "https://arxiv.org/abs/2503.16549",
        "github_repo": "https://github.com/MathFlow-zju/MathFlow",
        "summary": "- This paper introduces MathFlow, a modular problem-solving pipeline designed to improve Multimodal Large Language Models (MLLMs) performance on visual mathematical problems by decoupling perception and inference stages.\n- A novel benchmark called FlowVerse is proposed which categorizes information into four components: Descriptive, Essential, Reasoned Property, and Question.  \n- MathFlow-P-7B, a dedicated perception model trained using a two-stage approach is developed to extract key information from diagrams.\n- Results on FlowVerse and MathVerse show that MathFlow-P-7B significantly improves performance when integrated with various inference models, achieving state-of-the-art results with some combinations. \n- The results highlight the importance of strong perception capabilities in visual mathematical problem-solving and the effectiveness of the modular design.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/MathFlow-zju/MathFlow"
        ],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
        "authors": "Wei Liu, Peng Zhang, Yuchong Sun, Zhengfeng Lai, Guan123",
        "link": "https://arxiv.org/abs/2503.16867",
        "github_repo": null,
        "summary": "- This paper introduces ETVA, a novel evaluation method for assessing text-to-video alignment, addressing limitations of existing metrics by employing fine-grained question generation and answering.\n- ETVA uses a multi-agent system to parse prompts into scene graphs for generating atomic questions and a knowledge-augmented multi-stage reasoning framework with auxiliary LLMs and video LLMs for question answering. \n- A comprehensive ETVA-Bench benchmark featuring 2k diverse prompts and 12k atomic questions across 10 categories is also presented.\n- Evaluation results on ETVA-Bench demonstrate that ETVA achieves significantly higher correlation with human judgment (Spearman's correlation of 58.47) compared to existing metrics like VideoScore which achieves 31.0.\n- Systematic evaluation of 15 T2V models reveals their limitations, especially in handling temporal dynamics and physics, highlighting areas for improvement in next-generation text-to-video models.",
        "classification": [
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "Enabling Versatile Controls for Video Diffusion Models",
        "authors": "Jiaxing Yan, Xiaobin Lu, Haoming Qin, Hao Zhou, Xu Zhang",
        "link": "https://arxiv.org/abs/2503.16983",
        "github_repo": "http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl",
        "summary": "- Introduces VCtrl (PP-VCtrl), a novel framework for enabling fine-grained control over pre-trained video diffusion models using various control signals like Canny edges, segmentation masks, and human keypoints.\n- Employs a unified control signal encoding process and a sparse residual connection mechanism to integrate control signals efficiently without modifying the underlying video diffusion model's architecture. \n- Experimental results on tasks such as style transfer, video editing, and character animation demonstrates superior visual quality and control precision compared to existing state-of-the-art methods like Control-A-Video and Text2Video-Zero. \n- Proposes novel evaluation metrics specifically designed for controllability assessment in video generation, complementing traditional video quality metrics like FVD. \n- The framework demonstrates strong scalability and generalizability, making it adaptable to diverse video generation tasks.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl"
        ],
        "huggingface_urls": [
            "https://pp-vctrl.github.io"
        ],
        "date": "2025-03-24"
    },
    {
        "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
        "authors": "Donghao Luo, Kai Hu, Chengming Xu, Chen Liu, Lingfan Zhang",
        "link": "https://arxiv.org/abs/2503.16921",
        "github_repo": null,
        "summary": "- This paper proposes Adaptive-DPO, a novel preference learning framework to address the negative impact of minority samples in preference datasets on diffusion model training.\n- Adaptive-DPO introduces a minority-instance-aware metric combining intra-annotator confidence and inter-annotator stability to distinguish between majority and minority preferences.\n- An adaptive DPO objective is presented to make minority samples less important, while amplifying supervision from majority samples.\n- Experimental results on benchmarks Pick-a-Pic and HPDv2 with SD1.5 and SDXL as backbones demonstrate that Adaptive-DPO significantly outperforms Diffusion-DPO and other baselines. \n- The method effectively handles real-world data and generalizes to other preference optimization methods, improving image quality and mitigating negative impacts of minority samples.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
        "authors": "Xuan Luo, Wenjie Yang, Zheng Li, Mao Zheng, Mingyang Song",
        "link": "https://arxiv.org/abs/2503.17287",
        "github_repo": null,
        "summary": "- FASTCURL is a Curriculum Reinforcement Learning approach with a context window extension strategy designed for efficient training of R1-like reasoning models.\n- It involves length-aware training data segmentation and context window extension training.\n- The former splits training data by input prompt length into three levels, and the latter uses these segmented datasets with progressively increasing context window length to train the model.\n- FASTCURL-1.5B-Preview outperforms DeepScaleR-1.5B-Preview on five datasets (MATH 500, AIME 2024, AMC 2023, Minerva Math, and OlympiadBench) using only 50% of the training steps.\n- All training for FASTCURL-1.5B-Preview is completed on a single 8-GPU node.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/nick7nlp/FastCuRL"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/Nickyang/FastCuRL",
            "https://huggingface.co/Nickyang/FastCuRL-1.5B-Preview"
        ],
        "date": "2025-03-24"
    },
    {
        "title": "PVChat: Personalized Video Chat with One-Shot Learning",
        "authors": "Yuchen Li, Yumeng Li, Gang Xu, Weilong Yan, Master-Shi",
        "link": "https://arxiv.org/abs/2503.17069",
        "github_repo": null,
        "summary": "- PVChat is a personalized Video Large Language Model (ViLLM) that enables subject-aware question answering from a single video using a one-shot learning approach.\n- It utilizes a Mixture-of-Heads (MoH) enhanced ViLLM architecture with ReLU Routing and two novel training objectives: Smooth Proximity Regularization and Head Activation Enhancement, to facilitate personalized learning.\n- A systematic data augmentation pipeline generates identity-preserving positive and hard negative samples, along with question-answer pairs across existence, appearance, action, and location categories.\n- A two-stage training strategy, transitioning from image pre-training to video fine-tuning, progressively develops the model's capacity from static attributes to dynamic representations.\n- Experimental results on diverse video datasets demonstrate that PVChat achieves state-of-the-art performance in personalized video understanding and question answering.",
        "classification": [
            "Multimodal",
            "Video-Text-to-Text",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration",
        "authors": "Yu Cheng, Jiawei Zhou, Xiaoye Qu, hitsmy",
        "link": "https://arxiv.org/abs/2503.12821",
        "github_repo": null,
        "summary": "- This paper introduces the Adaptive Data Refinement (ADR) framework, a method designed to improve Large Vision-Language Models (LVLMs) by addressing the long-tail problem in training data, where certain concepts are overrepresented while others are underrepresented.\n- ADR consists of two stages: Data Rebalancing (DR) and Data Synthesis (DS).  The DR stage filters redundant data based on entity distributions to mitigate overfitting, while the DS stage utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic data for underrepresented concepts.\n- Evaluation across eleven benchmarks shows ADR improves the average performance of LLaVA 1.5 by 4.36% relative to the baseline without increasing the training data volume.  The improvement is observed not only in overall performance but also specifically on tail data, indicating effective mitigation of the long-tail problem.\n- The framework is model-agnostic and data-agnostic, allowing for easy integration with existing LVLMs and datasets.\n- Analysis of the long-tail problem identifies four key perspectives contributing to the imbalance: tokens, objects, co-occurrences, and interrogations.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "Implicit Bias-Like Patterns in Reasoning Models",
        "authors": "Calvin K. Lai, l048596",
        "link": "https://arxiv.org/abs/2503.11572",
        "github_repo": null,
        "summary": "- This paper introduces the Reasoning Model Implicit Association Test (RM-IAT), a method for studying implicit bias-like patterns in AI reasoning models, focusing on the model's processing rather than just its outputs.\n- The RM-IAT adapts the human Implicit Association Test (IAT) by measuring the number of reasoning tokens used by LLMs like OpenAI's 03-mini when categorizing words related to social groups and attributes in association-compatible and -incompatible pairings.\n- Results reveal that 03-mini requires significantly more tokens for association-incompatible pairings in 9 out of 10 RM-IATs, indicating greater computational effort and mirroring human implicit bias in processing efficiency.\n- This bias in computational effort has potential implications for real-world applications, as it suggests that reasoning models may unintentionally perpetuate social stereotypes due to their reliance on pre-existing patterns in data.\n- Further research on how RLHF and other alignment techniques interact with these implicit-like processing patterns in reasoning models is necessary to address the concerns raised.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model",
        "authors": "Junlin Han, Runjia Li, Yun Liu, Guolei Sun, Zhaochong An",
        "link": "https://arxiv.org/abs/2503.16282",
        "github_repo": "https://github.com/ZhaochongAn/GFS-VL",
        "summary": "- This paper introduces GFS-VL, a generalized few-shot 3D point cloud segmentation framework that leverages the open-world knowledge of 3D Vision-Language Models (3D VLMs) to enhance the performance of few-shot learning.\n- GFS-VL addresses the noisy nature of 3D VLM predictions by using a prototype-guided pseudo-label selection method, filtering noisy regions and adaptively infilling unlabeled areas using contextual information and few-shot samples.\n- To further enhance learning, a novel-base mix strategy is employed, integrating support samples into training scenes while preserving context.\n- The authors introduce two new benchmarks with increased diversity and quantity of novel classes for a comprehensive evaluation.\n- Experiments demonstrate state-of-the-art performance across existing and new benchmarks, validating the efficacy of GFS-VL in generalizing to diverse novel classes.",
        "classification": [
            "Image Segmentation",
            "Computer Vision",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/ZhaochongAn/GFS-VL"
        ],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
        "authors": "Junyong Noh, Hangyeul Shin, Chaelin Kim, Kwan Yun",
        "link": "https://arxiv.org/abs/2503.17095",
        "github_repo": null,
        "summary": "- FFaceNeRF is a novel NeRF-based few-shot face editing method that allows for flexible 3D-aware face editing using customized segmentation masks.\n- It utilizes a geometry adapter with feature injection, enabling adaptation to new mask layouts with as few as 10 training samples.\n- The method employs Latent Mixing for Triplane Augmentation (LMTA) to enhance training robustness and an overlap-based optimization process for precise editing, particularly in small regions.\n- Experimental results demonstrate that FFaceNeRF surpasses existing NeRF-based face editing methods in terms of flexibility and generated image quality.\n- FFaceNeRF outperforms state-of-the-art methods in a user study, with participants rating it higher in faithfulness, source retention, and visual quality.",
        "classification": [
            "Image-to-Image",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-03-24"
    },
    {
        "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting",
        "authors": "Tiansong Zhou, Zhonghua Jiang, Gaige Wang, Jingchuan Hu, Jianchuan Chen",
        "link": "https://arxiv.org/abs/2503.17032",
        "github_repo": null,
        "summary": "- TaoAvatar, a novel teacher-student framework, generates realistic, 3D, full-body talking avatars from multi-view sequences by leveraging 3D Gaussian Splatting.\n- The approach employs a personalized, clothed parametric template (SMPLX++) which binds Gaussians to mesh triangles as textures.\n- A StyleUnet-based teacher network learns pose-dependent deformations, which are then distilled into a lightweight MLP-based student network for real-time performance on mobile devices.\n-  Two lightweight blend shapes are introduced to enhance the appearance details.\n- Experiments on the proposed TalkBody4D dataset and ActorHQ dataset demonstrate superior rendering quality and speed, achieving 90 FPS on stereo devices like Apple Vision Pro.",
        "classification": [
            "Computer Vision",
            "Text-to-3D",
            "Image-to-3D"
        ],
        "github_urls": [
            "https://PixelAI-Team.github.io/TaoAvatar"
        ],
        "huggingface_urls": [],
        "date": "2025-03-24"
    }
]