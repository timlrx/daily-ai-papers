[
    {
        "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
        "authors": "Jiaxu Zhang, Xianfang Zeng, Yiying Yang, CH3COOK, wchengad",
        "link": "https://arxiv.org/abs/2504.06263",
        "github_repo": null,
        "summary": "- OmniSVG is a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal complex SVG generation.\n- It parameterizes SVG commands and coordinates into discrete tokens, decoupling structural logic from low-level geometry, and mitigating the \"coordinate hallucination\" problem.\n- OmniSVG introduces MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks.\n- Experimental results demonstrate that OmniSVG outperforms existing methods across various SVG generation tasks, including Text-to-SVG, Image-to-SVG, and Character-Reference SVG generation.\n- OmniSVG shows potential for integration into professional SVG design workflows due to its ability to generate high-quality, complex, and editable SVG content from diverse modalities.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "http://omnisvg.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-04-09"
    },
    {
        "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
        "authors": "Jiangbo Pei, Yichen Wei, Xiaokun Wang, Chris, Yi Peng",
        "link": "https://arxiv.org/abs/2504.05599",
        "github_repo": null,
        "summary": "- Skywork R1V, a 38B parameter multimodal reasoning model, extends the R1-series Large Language Models (LLMs) to visual modalities using a lightweight visual projector and a hybrid optimization strategy combining Iterative Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO).\n- The model employs an adaptive-length Chain-of-Thought distillation approach for efficient reasoning and outperforms other similarly sized models, achieving scores of 69.0 on MMMU and 67.5 on MathVista.\n- It also maintains robust textual reasoning performance, with scores of 72.0 on AIME and 94.0 on MATH500.\n- The model architecture involves a staged approach, first training an MLP adapter to align a vision encoder with a substitute language model and then transferring this adapter to bridge the encoder with the reasoning-capable R1 LLM.\n-  The adaptive-length Chain-of-Thought distillation dynamically optimizes the reasoning chain length, improving inference efficiency.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-Text-to-Text",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/Skywork/Skywork-R1V-38B"
        ],
        "date": "2025-04-09"
    },
    {
        "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
        "authors": "Vage Egiazarian, George Yakushev, Alina Shutova, Roman Garipov, Gleb Rodionov",
        "link": "https://arxiv.org/abs/2504.06261",
        "github_repo": null,
        "summary": "- This paper introduces Hogwild! Inference, a novel parallel LLM inference method designed to enhance reasoning and generation speed by enabling multiple LLM instances to collaborate dynamically during inference.\n- The core idea is to allow concurrent access to a shared key-value cache, which eliminates the need for recomputing key-value representations for each worker and facilitates real-time interaction between instances using Rotary Position Embeddings (ROPE).\n- The authors experiment with three cache layouts: contiguous (token-wise), interleaved (step-wise), and combined, with their preliminary results demonstrating improved performance on mathematical reasoning tasks compared to single-thread baselines.\n- Through prompting, the LLMs are encouraged to collaborate by devising and adjusting their own strategies for problem-solving, rather than relying on pre-defined frameworks.\n- The initial experiments suggest that reasoning-capable LLMs like QwQ and DeepSeek-R1 can effectively leverage this shared cache approach to improve efficiency and potentially quality on tasks requiring long-chain reasoning, such as those in the LIMO dataset.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/eqimp/hogwild_llm"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Qwen/QwQ-32B",
            "https://huggingface.co/datasets/GAIR/LIMO"
        ],
        "date": "2025-04-09"
    },
    {
        "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
        "authors": "Fei Ding, Yufeng Cheng, Mengqi Huang, wuwx, fenfan",
        "link": "https://arxiv.org/abs/2504.02160",
        "github_repo": "https://github.com/bytedance/UNO",
        "summary": "- This paper introduces UNO (Universal Network for Customization), a novel subject-driven image generation model built upon Diffusion Transformers (DiTs).\n- UNO employs a progressive cross-modal alignment training strategy and incorporates Universal Rotary Position Embedding (UnoPE) to enhance subject fidelity and controllability.\n- A new data synthesis pipeline leveraging DiTs\u2019 in-context generation capabilities produces high-quality, high-resolution, and multi-subject image pairs.\n- UNO demonstrates state-of-the-art performance on DreamBench and other multi-subject generation benchmarks achieving the highest DINO and CLIP-I scores.\n- Extensive qualitative and quantitative analyses, including a user study, showcases UNO\u2019s superior performance in various image generation tasks such as virtual try-on and stylized generation.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/bytedance/UNO"
        ],
        "huggingface_urls": [],
        "date": "2025-04-09"
    },
    {
        "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
        "authors": "Ming-Hsuan Yang, Mike Zheng Shou, Yuchao Gu, Lan Chen, Qi Mao",
        "link": "https://arxiv.org/abs/2504.05594",
        "github_repo": "https://github.com/CUC-MIPG/UnifyEdit",
        "summary": "- Introduces UnifyEdit, a tuning-free framework for text-based image editing that balances fidelity and editability using a unified latent diffusion optimization approach.\n- Employs two attention-based constraints: a self-attention preservation constraint for structural fidelity and a cross-attention alignment constraint for enhanced text alignment.\n- Develops an adaptive time-step scheduler to dynamically adjust the influence of these constraints, preventing over- or under-editing issues.\n- Introduces Unify-Bench, a diverse dataset for evaluating text-based image editing across various scopes and types of edits.\n- Demonstrates superior performance compared to state-of-the-art methods through extensive quantitative and qualitative experiments, showing a robust balance between structure preservation and text alignment.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/CUC-MIPG/UnifyEdit"
        ],
        "huggingface_urls": [],
        "date": "2025-04-09"
    },
    {
        "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
        "authors": "Baizhou Huang, Ruilin Yan, Xiangyu Wang, YitaoLiang, pkuHaowei",
        "link": "https://arxiv.org/abs/2504.02810",
        "github_repo": null,
        "summary": "- This paper introduces KUMO, a generative evaluation framework for assessing reasoning in Large Language Models (LLMs).\n- KUMO generates diverse, multi-turn reasoning tasks across various domains,  using LLMs combined with symbolic engines to create partially observable environments with adjustable difficulty.\n- The authors evaluated 23 state-of-the-art LLMs on 5,000 KUMO tasks and found that reasoning-scaled LLMs perform comparably to university students on complex reasoning challenges, while even standard LLMs outperform humans on simpler tasks.\n- The paper shows strong correlation between LLM performance on KUMO and other reasoning benchmarks like MMLU-Pro and LiveBench-Reason, and demonstrates KUMO's resistance to data contamination by showing that fine-tuned LLMs overfit to specific domains.\n- An analysis of parsing errors and token consumption revealed that some LLMs struggle with KUMO's format and generate excessively long or irrelevant outputs.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/linhaoweil/kumo"
        ],
        "huggingface_urls": [],
        "date": "2025-04-09"
    },
    {
        "title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\n  Capabilities in Multimodal Large Language Models",
        "authors": "Alex Jinpeng Wang, Ping Yu, Zhengyuan Yang, Linjie Li, Fengx1nn",
        "link": "https://arxiv.org/abs/2504.06148",
        "github_repo": "https://github.com/CSU-JPG/V-MAGE",
        "summary": " - V-MAGE is a novel game-based benchmark designed to evaluate visual-centric capabilities of Multimodal Large Language Models (MLLMs).\n - It features five diverse games with over 30 handcrafted levels, testing various visual reasoning skills such as positioning, tracking, timing, and visual memory.\n - V-MAGE reveals significant challenges in MLLMs' visual perception and reasoning, with top-performing models exhibiting substantial performance gaps compared to humans.\n - The benchmark uses an Elo-based ranking system for comparing model performance and facilitates iterative model improvements.\n - The findings highlight crucial limitations of current MLLMs and suggest avenues for improvement from an agent-centric perspective.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/CSU-JPG/V-MAGE"
        ],
        "huggingface_urls": [],
        "date": "2025-04-09"
    },
    {
        "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
        "authors": "William W. Cohen, Bill Yuchen Lin, Langlin Huang, Chengsong Huang, Jixuan Leng",
        "link": "https://arxiv.org/abs/2504.00043",
        "github_repo": null,
        "summary": "- CrossWordBench, a novel benchmark for evaluating the reasoning capabilities of LLMs and LVLMs using crossword puzzles, is introduced.\n- The benchmark features controllable puzzle generation in multiple formats (text and image) and offers various evaluation strategies.\n- Evaluations on over 20 models demonstrate that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints.\n- LVLMs struggle with the task, showing a strong correlation between their performance and grid-parsing accuracy.\n- Findings offer insights into the limitations of current LLMs and LVLMs and provide an effective approach for creating multimodal constrained tasks.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/SeanLeng1/CrossWordBench"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/HINT-lab/CrossWordBench"
        ],
        "date": "2025-04-09"
    },
    {
        "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\n  Guidance",
        "authors": "Tong Wu, Pan Zhang, Yujie Zhou, Pengyang Ling, Jiazi Bu",
        "link": "https://arxiv.org/abs/2504.06232",
        "github_repo": null,
        "summary": "- HiFlow, a training-free and model-agnostic framework, enhances high-resolution text-to-image synthesis in Rectified Flow models by aligning the high-resolution sampling flow with a virtual reference flow.\n- This flow-aligned guidance, implemented through initialization, direction, and acceleration alignment, ensures low-frequency consistency, structure preservation, and detailed fidelity.\n- HiFlow demonstrates enhanced visual quality and semantic coherence in high-resolution image generation, outperforming existing training-free methods.\n- This technique is broadly applicable to various model architectures, including U-Net and DiT, due to its model-agnostic design.\n- HiFlow exhibits competitive image quality and image-text alignment compared to leading training-based models and closed-source commercial models, while requiring less compute than existing training-free methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/Bujiazi/HiFlow"
        ],
        "huggingface_urls": [],
        "date": "2025-04-09"
    },
    {
        "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One\n  Sequence",
        "authors": "Yijiong Yu",
        "link": "https://arxiv.org/abs/2503.20533",
        "github_repo": "https://github.com/yuyijiong/parallel-decoding-in-one-sequence",
        "summary": "- This paper introduces \"Parallel Decoding in One Sequence,\" a novel decoding method designed to accelerate the reasoning process of Large Language Models (LLMs) for parallelizable tasks.\n- The method modifies the attention mask and position IDs, enabling parallel decoding of multiple tokens within a single sequence, thereby improving efficiency without requiring additional memory or KV cache recomputation.\n- Experiments on retrieval, multi-document QA, and planning tasks demonstrate a significant speedup in decoding time (over 100% in some cases) while maintaining answer quality.\n- The method is generally applicable across different LLMs without requiring additional training or modules.\n- Evaluation shows improved decoding speed by approximately 70% and 40% for Qwen2.5-14b and Qwen2.5-7b on the planning task with little to no reduction in answer quality as measured by GPT-4o rating scores.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/yuyijiong/parallel-decoding-in-one-sequence"
        ],
        "huggingface_urls": [],
        "date": "2025-04-09"
    }
]