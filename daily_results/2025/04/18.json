[
    {
        "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
        "authors": "Dan Su, Xin Dong, Yonggan Fu, Yu Yang, shizhediao",
        "link": "https://arxiv.org/abs/2504.13161",
        "github_repo": null,
        "summary": "- CLIMB, a novel framework for automating the search for optimal data mixtures during pre-training of large language models (LLMs), is introduced.\n- CLIMB consists of three steps: embedding and clustering large-scale datasets, constructing mixture-performance pairs by training proxy models, and fitting a predictor to estimate performance.\n- When trained on 400 billion tokens with an optimized data mixture, a 1 billion parameter model trained using CLIMB exceeds the performance of the state-of-the-art Llama-3.2-1B model by 2.0%.\n- Optimizing for a specific domain (e.g., Social Sciences) results in a 5% performance improvement compared to random sampling.\n- ClimbLab, a 1.2 trillion token corpus with 20 clusters, and ClimbMix, a 400 billion token dataset, are introduced as new datasets for pre-training research.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/ClimbMix",
            "https://huggingface.co/ClimbLab"
        ],
        "date": "2025-04-18"
    },
    {
        "title": "Antidistillation Sampling",
        "authors": "Avi Schwarzschild, Zhili Feng, Asher Trockman, arobey1, yashsavani",
        "link": "https://arxiv.org/abs/2504.13146",
        "github_repo": null,
        "summary": "- This paper introduces \"antidistillation sampling,\" a method designed to mitigate the effectiveness of model distillation by poisoning the reasoning traces generated by large language models (LLMs).\n- Antidistillation sampling strategically modifies the model's next-token probability distribution during the generation of reasoning traces, striking a balance between maintaining high likelihood under the original distribution and maximizing the negative impact on the performance of distilled models.\n- The approach focuses on making the generated traces less useful for training student models while preserving the teacher model's performance on downstream tasks.\n- Experiments using DeepSeek-R1-Distill-Qwen-7B, Qwen2.5-3B, and Llama-3.2-3B on GSM8K and MATH benchmarks demonstrate the effectiveness of antidistillation sampling.\n- The results show that the approach significantly reduces the performance of distilled student models relative to those trained on traces from temperature sampling, for the same teacher performance level.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
        "authors": "Honglin Lin, Yu Li, Zinan Tang, Qizhi Pei, GX-XinGao",
        "link": "https://arxiv.org/abs/2504.12322",
        "github_repo": "https://github.com/GX-XinGao/GRA",
        "summary": "- This paper introduces GRA, a novel framework leveraging multiple small language models (LLMs) in a collaborative approach for data synthesis, aiming to match or exceed the quality achieved by single large LLMs.\n- GRA assigns distinct roles to small LLMs (Generator, Reviewer, and Adjudicator) inspired by the peer-review process, promoting iterative refinement and quality control, thus achieving data-level parity with large LLM-based distillation.\n- Across multiple benchmarks, GRA-produced data matches or exceeds the quality of single large LLM outputs like Qwen-2.5-72B-Instruct, demonstrating the potential of strategically coordinating smaller LLMs for high-quality data synthesis.\n- The framework reduces computational resource requirements significantly compared to large LLM distillation.\n- This study demonstrates the potential of smaller, collaborative LLMs in enhancing efficiency and accessibility of data synthesis",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/GX-XinGao/GRA"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
        "authors": "Maneesh Agrawala, Lvmin Zhang",
        "link": "https://arxiv.org/abs/2504.12626",
        "github_repo": "https://github.com/lllyasviel/FramePack",
        "summary": "- FramePack, a novel neural network structure, enhances video generation by addressing the \"forgetting\" problem in next-frame prediction, where models struggle to maintain long-range temporal consistency.\n- FramePack compresses input frames based on their relative importance, allowing models to process a larger number of frames without increasing computational burden.\n- The structure employs progressive compression by manipulating the transformer's patchify kernel size, ensuring a fixed context length regardless of video duration.\n- The paper also introduces anti-drifting sampling methods that incorporate bi-directional context to enhance visual quality.\n- Experiments demonstrate that FramePack can improve existing video diffusion models, allowing higher training batch sizes and yielding more balanced diffusion schedules for enhanced results.",
        "classification": [
            "Text-to-Video",
            "Image-to-Video",
            "Video-Text-to-Text",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/lllyasviel/FramePack"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
        "authors": "Trevor Darrell, Joseph E. Gonzalez, Jiaxin Ge, Heekyung Lee, tsunghanwu",
        "link": "https://arxiv.org/abs/2504.13169",
        "github_repo": null,
        "summary": "- REVERSE, a novel framework for reducing hallucinations in vision-language models (VLMs), is introduced, unifying generation adjustment and online post-hoc verification within a single VLM architecture.\n- The approach uses a 1.3M semi-synthetic training dataset with hallucination phrases tagged by special tokens, enabling the VLM to identify potential hallucinations.\n- Retrospective resampling, a new inference technique, allows the VLM to act as its own verifier, backtracking and revising hallucinations through rejection sampling and query rewriting.\n- Evaluations show REVERSE achieves state-of-the-art hallucination reduction, improving CHAIR scores by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest compared to existing methods.\n- The framework demonstrates significant improvements on hallucination-sensitive captioning and open-ended VQA tasks, while also maintaining performance on discriminative question formats.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Image-to-Text"
        ],
        "github_urls": [
            "https://github.com/tsunghan-wu/reverse_vlm"
        ],
        "huggingface_urls": [
            "https://reverse-vlm.github.io"
        ],
        "date": "2025-04-18"
    },
    {
        "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
        "authors": "Shuai Yang, Wenqi Ouyang, Yifan Zhou, Yushi Lan, Zeqi Xiao",
        "link": "https://arxiv.org/abs/2504.12369",
        "github_repo": null,
        "summary": "- WORLDMEM is a new framework for long-term consistent video generation that uses a memory bank to store past frames and states (poses and timestamps).\n- It employs a memory attention mechanism to extract relevant information from these memory frames, enabling accurate reconstruction of previously observed scenes even under significant viewpoint or temporal gaps.\n- WORLDMEM is built upon a Conditional Diffusion Transformer (CDIT) and uses Diffusion Forcing (DF) for autoregressive generation. \n- The model maintains a memory bank composed of memory units - each containing a past frame and its corresponding state. \n- Extensive experiments in both virtual (Minecraft) and real (RealEstate10K) scenarios show that WORLDMEM significantly improves 3D spatial consistency and enables robust viewpoint reasoning, high-fidelity scene reconstructions, and accurate tracking of evolving events.",
        "classification": [
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
        "authors": "Meng Luo, Haojian Huang, scofield7419, ChocoWu, Harold328",
        "link": "https://arxiv.org/abs/2504.13122",
        "github_repo": "https://github.com/HaroldChen19/VistaDPO",
        "summary": "- VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization, improves text-video preference alignment in Large Video Models (LVMs) across three hierarchical levels: instance, temporal, and perceptive.\n- It introduces VistaDPO-7k, a dataset of 7.2K QA pairs with chosen/rejected responses and spatiotemporal grounding information.\n- VistaDPO addresses video-language misalignment and hallucination by aligning video content with responses at the instance level, video temporal semantics with event descriptions at the temporal level, and spatial objects with language tokens at the perceptive level.\n- Experiments on benchmarks including VideoHallucination, Video QA, and Captioning demonstrate significant performance improvements in existing LVMs (e.g., 26.42% over PLLaVA, 53.92% over Video-LLaVA).\n- The hierarchical spatiotemporal alignment and the high-quality VistaDPO-7k dataset contribute to enhanced video understanding and reduced hallucination in LVMs.",
        "classification": [
            "Video-Text-to-Text",
            "Visual Question Answering",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/HaroldChen19/VistaDPO"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
        "authors": "Chao Du, Zijian Wu, Jinjie Ni, Xiangyan Liu, dreamerdeo",
        "link": "https://arxiv.org/abs/2504.13055",
        "github_repo": null,
        "summary": "- NoisyRollout, a novel reinforcement learning (RL) method, is introduced to enhance visual reasoning in vision-language models (VLMs) by improving rollout diversity and addressing perceptual limitations.\n- It incorporates a hybrid rollout strategy, using trajectories from both clean and distorted images during training to improve exploration and expose perceptual discrepancies.\n- A noise annealing schedule gradually reduces distortion strength to maintain training stability.\n- Experiments demonstrate state-of-the-art performance on five out-of-domain benchmarks, outperforming open-source RL-tuned models and even some large-scale supervised fine-tuning (SFT) + RL models with only 2.1K training samples.\n- This method improves generalization without requiring additional training overhead or changes to the RL objective (GRPO).",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/John-AI-Lab/NoisyRollout"
        ],
        "huggingface_urls": [
            "hf.co/collections/xyliu6/noisyrollout"
        ],
        "date": "2025-04-18"
    },
    {
        "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
        "authors": "Firoz Kabir, Aayush Bajaj, Mahir Ahmed, 38saidul, ahmed-masry",
        "link": "https://arxiv.org/abs/2504.05506",
        "github_repo": "https://github.com/vis-nlp/ChartQAPro",
        "summary": "- This paper introduces ChartQAPro, a new benchmark dataset for Chart Question Answering (CQA) that aims to address the limitations of existing datasets like ChartQA.\n- ChartQAPro includes 1,341 charts from 157 diverse sources, featuring 1,948 questions with diverse types such as multiple-choice, conversational, hypothetical, and unanswerable questions.\n- The dataset emphasizes visual diversity with the inclusion of infographics and dashboards and linguistic diversity with more complex and varied questions and accompanying paragraphs.\n- Evaluations with 21 models show substantial performance drops for large vision-language models (LVLMs) on ChartQAPro; for example, Claude Sonnet 3.5 accuracy drops from 90.5% on ChartQA to 55.81% on ChartQAPro.\n- Error analysis reveals challenges in visual perception, instruction following, and math reasoning, providing directions for future LLM development in CQA.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/vis-nlp/ChartQAPro"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "Exploring Expert Failures Improves LLM Agent Tuning",
        "authors": "Ruochen Wang, Minhao Cheng, Andrew Bai, Li-Cheng Lan, zhoutianyi",
        "link": "https://arxiv.org/abs/2504.13145",
        "github_repo": null,
        "summary": "- This paper proposes Exploring Expert Failures (EEF), a novel method for fine-tuning Large Language Model (LLM) agents that leverages beneficial actions from failed expert trajectories, which are often overlooked by existing methods like Rejection Sampling Fine-Tuning (RFT).\n- EEF identifies and incorporates valuable actions and plans from unsuccessful expert demonstrations, enhancing agent exploration efficiency and skill acquisition, while carefully filtering out potentially detrimental actions to prevent negative learning.\n- EEF outperforms RFT and expert models in challenging environments such as WebShop and SciWorld, achieving a 62% win rate in WebShop, exceeding RFT (53.6%) and GPT-4 (35.6%).\n-  Furthermore, EEF sets new state-of-the-art performance in WebShop (first to score above 0.81) and SciWorld (first to exceed 81). \n- The method's effectiveness is demonstrated by its ability to solve previously unsolvable subtasks and its efficient utilization of weaker but more cost-effective experts like GPT-3.5 Turbo.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework",
        "authors": "Yiji Cheng, Qixun Wang, Yanbing Zhang, Jiale Tao, wanghaofan",
        "link": "https://arxiv.org/abs/2504.12395",
        "github_repo": "https://github.com/Tencent/InstantCharacter",
        "summary": "- InstantCharacter is a scalable framework for character customization built upon a foundation diffusion transformer.\n- It introduces a scalable adapter with stacked transformer encoders that effectively process open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers.\n- A large-scale character dataset containing 10-million-level samples is constructed for training.\n- This framework enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways using paired and unpaired data.\n- Qualitative experiments demonstrate InstantCharacter's advanced capabilities in generating high-fidelity, text-controllable, and character-consistent images, outperforming existing methods.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/Tencent/InstantCharacter"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy",
        "authors": "Seon Joo Kim, Michael S. Brown, Dongyun Kim, Mahmoud Afifi, dongyong2",
        "link": "https://arxiv.org/abs/2504.07959",
        "github_repo": null,
        "summary": "- CCMNet is a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining, leveraging pre-calibrated Color Correction Matrices (CCMs) from camera ISPs.\n- The method transforms predefined illuminant colors into the test camera's raw space using CCMs, creating a camera fingerprint embedding (CFE) that guides a hypernetwork to generate a camera-specific model.\n- A novel data augmentation technique interpolates between cameras and CCMs to create imaginary camera images, which enhances the model's generalization capabilities.\n- Experimental results across multiple datasets and backbones show that CCMNet achieves state-of-the-art cross-camera color constancy while remaining lightweight.\n- Unlike previous methods, CCMNet does not require additional images from the test camera for adaptation, relying only on data readily available in camera ISPs.",
        "classification": [
            "Computer Vision",
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "FocusedAD: Character-centric Movie Audio Description",
        "authors": "Liangcheng Li, Sheng Zhou, Yiren Song, Chun Wang, Xiaojun Ye",
        "link": "https://arxiv.org/abs/2504.12157",
        "github_repo": "https://github.com/Thorin215/FocusedAD",
        "summary": "- FocusedAD is a novel framework for generating character-centric movie audio descriptions, addressing the need for plot-relevant and character-referential narration for blind and visually impaired audiences.\n- The model architecture consists of a Character Perception Module (CPM) to track character regions and link them to names, a Dynamic Prior Module (DPM) to integrate contextual cues from prior descriptions and subtitles, and a Focused Caption Module (FCM) to generate detailed, character-focused narrations.\n- It introduces an automated pipeline for building character query banks to improve character identification and incorporates a dynamic soft prompt mechanism in the DPM to handle scenes with varying numbers of characters.\n- FocusedAD outperforms existing methods, achieving state-of-the-art results, including in zero-shot settings, on MAD-eval-Named and a newly proposed Cinepile-AD dataset.\n- The model's focus on narrative-salient regions and explicit character identification makes it particularly suitable for automated audio description generation, surpassing methods that rely on global scene features and ambiguous character references.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/Thorin215/FocusedAD"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "Retrieval-Augmented Generation with Conflicting Evidence",
        "authors": "Mohit Bansal, Elias Stengel-Eskin, Archiki Prasad, HanNight",
        "link": "https://arxiv.org/abs/2504.13079",
        "github_repo": "https://github.com/HanNight/RAMDocs",
        "summary": "- This paper introduces MADAM-RAG, a multi-agent debate approach for Retrieval-Augmented Generation (RAG) designed to handle conflicting evidence from multiple sources, including ambiguity, misinformation, and noise.\n- MADAM-RAG assigns each retrieved document to an LLM agent, which generates an intermediate response. These agents then engage in a multi-round debate, allowing them to refine responses and challenge each other\u2019s claims.\n- An aggregator summarizes the debate into a coherent final response, filtering out misinformation and presenting multiple valid answers when appropriate.\n- The authors also introduce RAMDocs, a new dataset that simulates complex scenarios for conflicting evidence.\n- Experimental results on FaithEval and AmbigDocs demonstrate that MADAM-RAG outperforms existing RAG baselines by up to 15.80% and 11.40%, respectively.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/HanNight/RAMDocs"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
        "authors": "Sarah Wooders, Charles Packer, Yu Wang, Charlie Snell, Kevin Lin",
        "link": "https://arxiv.org/abs/2504.13171",
        "github_repo": "https://github.com/letta-ai/sleep-time-compute",
        "summary": "- This paper introduces \"sleep-time compute,\" a technique that allows large language models (LLMs) to pre-compute useful information from a given context before a query is presented, reducing test-time compute requirements.\n- The authors demonstrate that sleep-time compute improves the Pareto frontier of test-time compute vs. accuracy, achieving similar accuracy with 5x less compute on modified versions of Stateful GSM-Symbolic and Stateful AIME reasoning tasks. \n- By scaling sleep-time compute and amortizing it across multiple related queries (using Multi-Query GSM-Symbolic), they further boost accuracy and reduce average cost per query by up to 18% and 2.5x, respectively.\n- Analysis shows that the efficacy of sleep-time compute correlates with the predictability of user queries from the context.\n- Finally, they apply the technique to a realistic agentic software engineering task, demonstrating its potential in real-world stateful LLM applications.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [
            "https://github.com/letta-ai/sleep-time-compute"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
        "authors": "Adams Wai-Kin Kong, Yan Ren, Leyang Li, Shilin-LU",
        "link": "https://arxiv.org/abs/2504.12782",
        "github_repo": "https://github.com/lileyang1210/ANT",
        "summary": "- This paper introduces ANT (Automatically guides deNoising Trajectories), a novel finetuning framework for text-to-image diffusion models that aims to prevent the generation of unwanted or harmful content.\n- ANT leverages a key insight: reversing the classifier-free guidance condition direction during the mid-to-late denoising stages allows for precise content modification without sacrificing early-stage structural integrity.\n- For single-concept erasure, ANT employs an augmentation-enhanced weight saliency map to pinpoint and modify critical parameters contributing to the unwanted concept.\n- In multi-concept erasure scenarios, ANT's objective function acts as a plug-and-play enhancement for existing frameworks like MACE, boosting their performance.\n- Experimental results demonstrate that ANT achieves state-of-the-art performance in both single and multi-concept erasure tasks, effectively removing targeted concepts while preserving image quality and the integrity of unrelated content.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [
            "https://github.com/lileyang1210/ANT"
        ],
        "huggingface_urls": [],
        "date": "2025-04-18"
    },
    {
        "title": "Perception Encoder: The best visual embeddings are not at the output of\n  the network",
        "authors": "Andrea Madotto, Jang Hyun Cho, Peize Sun, Po-Yao Huang, Daniel Bolya",
        "link": "https://arxiv.org/abs/2504.13181",
        "github_repo": null,
        "summary": "- This paper introduces Perception Encoder (PE), a vision encoder trained with contrastive vision-language learning.\n- PE achieves state-of-the-art zero-shot performance in image and video classification and retrieval, outperforming models trained on larger, proprietary datasets like JFT-3B.\n-  It uses a novel training process involving a robust image pretraining recipe and a video data engine that generates aligned captions for video clips.\n- The authors also release a new dataset of 1M videos with 120K human-refined annotations called the PE Video Dataset (PVD).\n- Through two novel alignment methods, language and spatial, internal representations within PE are identified that excel in various tasks exceeding models with other pretraining techniques.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Zero-Shot Image Classification",
            "Video Classification",
            "Zero-Shot Classification",
            "Image Feature Extraction"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/perception_models"
        ],
        "huggingface_urls": [
            "https://ai.meta.com/datasets/pe-video/"
        ],
        "date": "2025-04-18"
    }
]