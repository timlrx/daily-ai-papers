[
    {
        "title": "Learning to Reason under Off-Policy Guidance",
        "authors": "Zhi Wang, ganqu, huzican, yaful, Elliott",
        "link": "https://arxiv.org/abs/2504.14945",
        "github_repo": null,
        "summary": "- LUFFY (Learning to reason Under oFF-policy guidance) is introduced, a framework that enhances zero-shot reinforcement learning by incorporating off-policy demonstrations (e.g., from DeepSeek-R1) alongside on-policy rollouts for training large reasoning models (LRMs).\n- Policy shaping via regularized importance sampling is proposed to mitigate entropy collapse and encourage exploration, thereby emphasizing crucial, low-probability actions during mixed-policy training.\n- LUFFY achieves a substantial improvement of over +7.0 average gain across six mathematical reasoning benchmarks and demonstrates superior generalization on out-of-distribution tasks with an advantage of over +6.2 points compared to supervised fine-tuning.\n- Analysis of training dynamics reveals that LUFFY effectively balances imitation and exploration, strategically assimilating off-policy behaviors while preserving autonomous exploration within its own reasoning space.\n- This approach offers a scalable and effective strategy to train generalizable reasoning models by leveraging off-policy guidance within the zero-shot reinforcement learning paradigm.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/ElliottYan/LUFFY"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
            "https://github.com/huggingface/Math-Verify"
        ],
        "date": "2025-04-22"
    },
    {
        "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
        "authors": "P2333, bhooi, dreamerdeo, yueliu1999, HongchengGao",
        "link": "https://arxiv.org/abs/2504.15257",
        "github_repo": "https://github.com/sail-sg/FlowReasoner",
        "summary": "- This paper introduces FLOWREASONER, a query-level meta-agent designed to automate the creation of multi-agent systems tailored to individual user queries, enhancing adaptability compared to traditional task-level approaches.\n- FLOWREASONER employs a reasoning-driven approach, leveraging external execution feedback and reinforcement learning to optimize workflows without relying on complex search algorithms or predefined search sets.\n- A multi-purpose reward function guides the RL training, focusing on performance, complexity, and efficiency of the generated multi-agent systems.\n- Experimental results on code generation benchmarks show FLOWREASONER-14B outperforming all baselines, including manually designed and existing automated workflow methods, achieving a 5 percentage point improvement over the strongest baseline (MaAS) and a 10.52% improvement over its underlying worker model (o1-mini) across three benchmarks.\n- The model demonstrates generalization capability by effectively adapting its planning strategies to different worker models and exhibits flexibility in workflow structure and granularity based on task complexity.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/sail-sg/FlowReasoner"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
        "authors": "WonminByeon, deahuang, lulidong, RealZhiqiLi, cg1177",
        "link": "https://arxiv.org/abs/2504.15271",
        "github_repo": null,
        "summary": "- Eagle 2.5 is a family of frontier vision-language models (VLMs) designed for long-context multimodal learning, addressing challenges in video and high-resolution image understanding.\n- It uses a generalist architecture based on LLaVA, using an MLP projection to align vision embeddings from SigLIP with the LLM representation space, and employs image tiling for any-resolution image inputs.\n-  Eagle 2.5 introduces an information-first sampling strategy (including Image Area Preservation and Automatic Degradation Sampling) and progressive mixed post-training.\n- It leverages a diverse data recipe, combining open-source data with the new Eagle-Video-110K dataset designed for long video understanding, featuring hierarchical story-level and clip-level annotations.\n- Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching top commercial models like GPT-40 and open-source models such as Qwen2.5-VL-72B and InternVL2.5-78B, showing robust scaling with increased frames.",
        "classification": [
            "Multimodal",
            "Visual Question Answering",
            "Video-Text-to-Text",
            "Image-Text-to-Text"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "ToolRL: Reward is All Tool Learning Needs",
        "authors": "Cheng Qian, Gokhantur, XtremSup, Merlin-Hongru, emrecanacikgoz",
        "link": "https://arxiv.org/abs/2504.13958",
        "github_repo": null,
        "summary": "- This paper introduces ToolRL, a novel approach for enhancing tool-integrated reasoning in Large Language Models (LLMs) using reinforcement learning, specifically Group Relative Policy Optimization (GRPO).\n- The authors propose a principled reward design framework tailored for tool use tasks, incorporating both structural (format) and semantic (correctness) components.\n- ToolRL consistently outperforms supervised fine-tuning and other RL baselines by 17% and 15% respectively across multiple tool use and question-answering benchmarks.\n- The trained model exhibits strong generalization to unseen scenarios and task objectives, along with emergent behaviors such as proactiveness and metacognitive reasoning.\n- Comprehensive analysis of different reward strategies reveals the importance of dynamic reward scaling and fine-grained reward decomposition for effective tool learning.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/qiancheng0/ToolRL"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
        "authors": "joyfull78, sungwon95, YeolJoo, TaewoongKang, mpark",
        "link": "https://arxiv.org/abs/2504.14396",
        "github_repo": "https://github.com/pmh9960/SphereDiff",
        "summary": "- SphereDiff is a novel tuning-free framework for generating seamless 360-degree panoramic images and videos using pretrained diffusion models.\n- It introduces a spherical latent representation paired with spherical coordinates and extends the MultiDiffusion framework to this spherical latent space for denoising.\n- A dynamic latent sampling method discretizes projected spherical latents onto a 2D grid, compatible with existing diffusion models, and mitigates distortion.\n- Distortion-aware weighted averaging further refines the projection process, enhancing visual quality.\n- Experiments demonstrate SphereDiff's superior performance over existing methods in generating high-quality, distortion-free, and continuous panoramic content.",
        "classification": [
            "Text-to-Image",
            "Text-to-Video",
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/pmh9960/SphereDiff"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
        "authors": "Cailin Zhuang, Yiying12, unpackableorange, wchengad, xuanyangz",
        "link": "https://arxiv.org/abs/2504.15281",
        "github_repo": null,
        "summary": "- StyleMe3D is a novel framework for stylizing 3D Gaussian Splatting (3D GS) representations using both text and image prompts, preserving geometric details while applying diverse artistic styles.\n- It leverages a multi-expert approach with four key components: Dynamic Style Score Distillation (DSSD) for semantic alignment with Stable Diffusion's latent space, Contrastive Style Descriptor (CSD) for localized texture transfer, Simultaneously Optimized Scale (SOS) for detail preservation, and 3D Gaussian Quality Assessment (3DG-QA) for aesthetic enhancement.\n- Evaluated on NeRF synthetic (objects) and tandt db (scenes) datasets, StyleMe3D demonstrates superior performance in preserving geometric details and stylistic consistency across complex scenes compared to state-of-the-art methods.\n- It effectively handles both non-photorealistic art styles (e.g., cartoon, sketch) and state-based styles (e.g., fire, cloud), capturing physical properties and artistic expressions.\n- This framework bridges photorealistic 3D GS and artistic stylization with quantitative results showing improvements in SSIM, PSNR, and LPIPS metrics, indicating enhanced fidelity and perceptual quality.",
        "classification": [
            "Image-to-3D",
            "Text-to-3D",
            "Computer Vision"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
        "authors": "hamidpalangi, mparvez, genglinliu, liweijiang, salmannyu",
        "link": "https://arxiv.org/abs/2504.13203",
        "github_repo": null,
        "summary": "- Introduces X-Teaming, a multi-turn jailbreaking framework that utilizes collaborative agents for planning, executing, and optimizing attacks against language models (LMs).\n- Achieves state-of-the-art multi-turn jailbreak success rates (up to 98.1%) across diverse LMs, including a 96.2% success rate against Claude 3.7 Sonnet.\n- Generates XGuard-Train, a 30K multi-turn safety training dataset 20x larger than previous resources, enabling improved multi-turn safety alignment for LMs.\n- Shows that models fine-tuned on XGuard-Train exhibit a 28.3% improvement in multi-turn attack resistance while maintaining single-turn safety and general capabilities.\n- Open-sources the framework, dataset, and trained models to facilitate the development of robust multi-turn defenses for conversational AIs.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/marslabucla/XGuard-Train"
        ],
        "date": "2025-04-22"
    },
    {
        "title": "UFO2: The Desktop AgentOS",
        "authors": "rujiawang, liqul, duchao, shilhe, vyokky",
        "link": "https://arxiv.org/abs/2504.14603",
        "github_repo": "https://github.com/microsoft/UFO/",
        "summary": "- UFO\u00b2 is a novel multi-agent operating system for Windows desktops designed for robust and practical computer-using agent (CUA) automation.\n- It features a centralized HostAgent for task coordination and specialized AppAgents for application-specific interactions, using native APIs and a hybrid GUI-API action layer for improved control and efficiency.\n- UFO\u00b2 incorporates a hybrid control detection pipeline combining Windows UI Automation (UIA) with visual grounding, continuous knowledge integration from documentation and execution logs, and speculative multi-action planning for reduced latency.\n- Evaluation across 20 real-world Windows applications shows UFO\u00b2 outperforms existing CUAs in robustness and accuracy, achieving a 10% higher task completion rate than the best-performing CUA, Operator, with a 50% relative improvement using deeper OS-level integration.\n- A Picture-in-Picture (PiP) interface allows for non-disruptive automation within an isolated virtual desktop.",
        "classification": [
            "Multimodal",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/microsoft/UFO/"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
        "authors": "Shengbang Tong, yubei, chengtim, ch-chenyu, danielchyeh",
        "link": "https://arxiv.org/abs/2504.15280",
        "github_repo": null,
        "summary": "- This paper introduces All-Angles Bench, a new benchmark designed to evaluate the multi-view understanding capabilities of Multimodal Large Language Models (MLLMs).\n- The benchmark consists of over 2,100 human-annotated multi-view question-answer pairs across 90 diverse real-world scenes, categorized into six tasks: counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation.\n- Experiments on 27 representative MLLMs reveal a substantial performance gap between current models and human-level proficiency, particularly in tasks involving cross-view correspondence for partially occluded views and establishing coarse camera poses.\n- An in-depth analysis suggests that existing MLLMs struggle with identifying the same object across multiple viewpoints, and that current chain-of-thought prompting techniques are insufficient to address these limitations.\n- The findings highlight the need for domain-specific refinements or specialized modules to enhance multi-view awareness in MLLMs.",
        "classification": [
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/danielchyeh/All-Angles-Bench"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
        "authors": "Yan Wang, Yunhui Xia, chuyi777, jasonkleinlove, Swtheking",
        "link": "https://arxiv.org/abs/2504.14655",
        "github_repo": null,
        "summary": "- This paper introduces LeetCodeDataset, a benchmark for evaluating and training code-generation LLMs. \n- The dataset consists of curated LeetCode Python problems with rich metadata, 100+ test cases per problem, and temporal splits for contamination-free evaluation and efficient supervised fine-tuning (SFT).\n- Experiments reveal that reasoning models significantly outperform non-reasoning counterparts on this dataset.\n- SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts.\n- This demonstrates the high-quality and efficiency of the LeetCodeDataset for training code LLMs.",
        "classification": [
            "Natural Language Processing",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/newfacade/LeetCodeDataset"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/newfacade/LeetCodeDataset"
        ],
        "date": "2025-04-22"
    },
    {
        "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
        "authors": "Xavier Hu, Yuhang Liu, xiaotianhan, xieck13, pengxiang",
        "link": "https://arxiv.org/abs/2504.14239",
        "github_repo": "https://github.com/Reallm-Labs/InfiGUI-R1",
        "summary": "- This paper introduces InfiGUI-R1, a Multimodal Large Language Model (MLLM)-based GUI agent trained with the novel Actor2Reasoner framework, designed to enhance GUI agent reasoning capabilities.\n- The two-stage framework first injects spatial reasoning into the base MLLM through Spatial Reasoning Distillation, leveraging a teacher model, to transition from a Reactive Actor to a Basic Reasoner. \n- The second stage, Deliberation Enhancement, refines the Basic Reasoner into a Deliberative Reasoner using Reinforcement Learning with Sub-goal Guidance and Error Recovery Scenario Construction to bolster planning and reflection. \n- InfiGUI-R1 achieves state-of-the-art performance on cross-platform GUI grounding (87.5% on ScreenSpot) and strong results on complex, long-horizon tasks (71.1% on AndroidControl-High), outperforming existing models with comparable or even larger parameters.\n- This demonstrates the framework's effectiveness in improving agent performance, particularly in tasks requiring complex reasoning and planning.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/Reallm-Labs/InfiGUI-R1"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
        "authors": "Linear-Matrix-Probability, HaomingXu, xukewei, Saberlve, xzwnlp",
        "link": "https://arxiv.org/abs/2504.15133",
        "github_repo": "https://github.com/zjunlp/EasyEdit",
        "summary": "- EasyEdit2 is a framework for controlling Large Language Model (LLM) behavior at test time without modifying model parameters.\n- It features a new architecture with key modules like the steering vector generator and applier for seamless model steering.\n- EasyEdit2 supports various interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features, and allows control with a single example.\n- Experimental results demonstrate EasyEdit2's effectiveness across different LLMs and dimensions, showing improvements over baseline methods and prior work.\n- The framework is open-sourced with an online demo and video tutorial for easy use and experimentation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/zjunlp/EasyEdit"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
        "authors": "dkeeeee, Yuxiang007, zhimingc, Pengxiangzhao, lgy0404",
        "link": "https://arxiv.org/abs/2504.13805",
        "github_repo": null,
        "summary": "- LearnAct, a novel multi-agent framework, enhances mobile GUI agent performance through few-shot demonstration learning, addressing long-tail scenarios and personalization gaps.\n- LearnGUI, the first comprehensive dataset for demonstration-based learning in mobile GUI agents, includes 2,252 offline and 101 online tasks with human demonstrations, enabling personalized adaptation.\n- LearnAct incorporates three agents: DemoParser extracts knowledge, KnowSeeker retrieves relevant knowledge, and ActExecutor combines instructions, GUI context, and demonstrations for task completion.\n- Evaluations demonstrate substantial improvements: Gemini-1.5-Pro accuracy increases from 19.3% to 51.7% (198.9% relative improvement) offline, and UI-TARS-7B-SFT online task success rate rises from 18.1% to 32.8% (+14.7%).\n- The results establish demonstration-based learning as a promising direction for adaptable and personalized mobile GUI agents, offering personalized assistance in diverse scenarios.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
        "authors": "Somayeh Sojoudi, Jonah Casebeer, Njb, Bai-YT",
        "link": "https://arxiv.org/abs/2504.15217",
        "github_repo": null,
        "summary": "- DRAGON, a novel framework for fine-tuning generative models using distributional rewards, is introduced, allowing optimization of instance-wise, instance-to-distribution, and distribution-to-distribution rewards, unlike traditional RLHF or pairwise preference methods.\n- Novel reward functions are designed by selecting an encoder (e.g., CLAP) and reference examples, enabling optimization of metrics like FAD and enhancing generations by comparing them to an exemplar distribution, even across modalities.\n- Evaluated on an audio-domain text-to-music diffusion model with 20 reward functions, including custom aesthetics, CLAP, Vendi, and FAD, DRAGON achieved an average 81.45% win rate across targets.\n- Human evaluation shows a 60.95% win rate in perceived music quality without human preference annotations, solely based on appropriate exemplar sets.\n- Example generations can be found at https://ml-dragon.github.io/web.",
        "classification": [
            "Text-to-Audio",
            "Audio"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
        "authors": "Katerina Fragkiadaki, Bowei Zhang, aharley, lkeab",
        "link": "https://arxiv.org/abs/2504.14717",
        "github_repo": "https://github.com/zbw001/TAPIP3D",
        "summary": "- TAPIP3D is a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos that represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information.\n- It uses a Local Pair Attention mechanism to manage irregular 3D point distributions, enabling effective spatial relationship exploitation and precise trajectory estimation.\n- This 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to traditional methods when depth information is available.\n- TAPIP3D supports inference in both camera and world coordinates, with results showing improved tracking performance by compensating for camera motion. \n- Experimental results on benchmarks such as TAPVid3D, LSFOdyssey, Dynamic Replica, and DexYCB demonstrate that TAPIP3D achieves state-of-the-art performance in 3D point tracking when ground-truth or sensor depth is available, and competitive performance with estimated depth.",
        "classification": [
            "Computer Vision",
            "Video Classification"
        ],
        "github_urls": [
            "https://github.com/zbw001/TAPIP3D"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes",
        "authors": "Yuan Yao, Ji Qi, chuats, acharkq, bys0318",
        "link": "https://arxiv.org/abs/2504.15270",
        "github_repo": null,
        "summary": "- Quicksviewer is a novel Large Multimodal Model (LMM) designed for efficient video understanding that uses a reinforced compression technique.\n- The model partitions videos into nonuniform cubes based on temporal information density, followed by a unified resampling approach to achieve efficient compression and training.\n- Quicksviewer outperforms existing baselines on various video understanding benchmarks, using fewer tokens per frame and less training data.\n- The model demonstrates high efficiency, enabling training with lengthy videos (420 seconds/1fps) and achieving state-of-the-art results on Video-MME and MLVU benchmarks.\n- The paper also introduces a novel annealing strategy for Gumbel Softmax to enhance training stability and effectiveness.",
        "classification": [
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://quicksviewer.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
        "authors": "Truong-Son Hy, tnngo2, quyanh",
        "link": "https://arxiv.org/abs/2504.15047",
        "github_repo": "https://github.com/knoveleng/rainbowplus",
        "summary": "- RAINBOWPLUS, a novel red-teaming framework, enhances adversarial prompt generation for Large Language Models (LLMs) using an adaptive quality-diversity search.\n- It employs a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, overcoming limitations of prior methods like Rainbow Teaming.\n- Experiments across various datasets and LLMs demonstrate superior attack success rate (ASR) and diversity compared to Rainbow, generating up to 100 times more unique prompts.\n- On HarmBench with 12 LLMs, RAINBOWPLUS achieves 81.1% average ASR, outperforming AutoDAN-Turbo by 3.9% and running 9 times faster.\n- The open-source implementation facilitates further research and development in LLM red-teaming and safety assessment.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/knoveleng/rainbowplus"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
        "authors": "yejinchoinka, ericnyberg, ekmb, shrimai19, SieraL",
        "link": "https://arxiv.org/abs/2504.13941",
        "github_repo": null,
        "summary": "- NEMOTRON-CROSSTHINK is a novel framework that uses multi-domain corpora for reinforcement learning (RL) training of large language models (LLMs), improving generalization across diverse reasoning tasks.\n- It addresses the challenge of verifiable reward modeling for non-deterministic domains by employing templates on curated data to limit answer space diversity, enabling scalable RL training.\n- The framework incorporates diverse data sources (synthetic and real-world question-answer pairs), applies structured templates (multiple-choice and open-ended), filters for verifiable answers, and optimizes data blending strategies.\n- Experimental results show significant accuracy improvements on math (MATH-500: +30.1%, AMC23: +27.5%) and non-math (MMLU-PRO: +12.8%, AGIEVAL: +15.1%) benchmarks.\n- NEMOTRON-CROSSTHINK also improves response efficiency, using 28% fewer tokens for correct answers compared to math-only training.",
        "classification": [
            "Reinforcement Learning",
            "Question Answering",
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-04-22"
    },
    {
        "title": "CoMotion: Concurrent Multi-person 3D Motion",
        "authors": "Stephan R. Richter, Alejandro Newell, vkoltun, lahavl, peiyun-hu-apple",
        "link": "https://arxiv.org/abs/2504.12186",
        "github_repo": "https://github.com/apple/ml-comotion",
        "summary": "- CoMotion is a novel model for detecting and tracking 3D human poses of multiple people in video using a single, monocular camera in an online fashion.\n- It employs a recurrent, tracking-by-attention model which updates poses directly from each new frame, enabling online tracking even through occlusions, rather than matching individual detections across frames.\n- CoMotion is trained on a diverse mix of datasets, including single-image and video datasets, both synthetic and real-world, using pseudo-labeling for challenging scenarios.\n- Evaluation on PoseTrack21 demonstrates state-of-the-art performance with a 14% improvement in MOTA and a 12% increase in IDF1 compared to previous methods. \n- CoMotion also achieves competitive results on pose estimation benchmarks while being significantly faster than competing models.",
        "classification": [
            "Computer Vision",
            "Keypoint Detection"
        ],
        "github_urls": [
            "https://github.com/apple/ml-comotion"
        ],
        "huggingface_urls": [],
        "date": "2025-04-22"
    }
]