[
    {
        "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
        "authors": "VityaVitalich, nakrayko, VirVen, zlatamaria, memyprokotow",
        "link": "https://arxiv.org/abs/2505.21115",
        "github_repo": null,
        "summary": "- This paper introduces EverGreenQA, the first multilingual question answering dataset with labels indicating whether questions are evergreen (answers remain stable over time) or mutable (answers change).\n- The dataset supports both evaluation and training of models for evergreen question classification.\n- They benchmark 12 modern LLMs on EverGreenQA to assess their ability to encode question temporality and train EG-E5, a lightweight multilingual classifier that achieves state-of-the-art performance on this task.\n- They demonstrate the practical utility of evergreen question classification in three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4's retrieval behavior.\n- The dataset and trained model are publicly released to facilitate further research.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/collections/s-nlp/evergreen-683465909575cb89d6b904fe"
        ],
        "date": "2025-06-09"
    },
    {
        "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
        "authors": "Owen Lee, Liyan Zhao, Zheshu Chen, Shunian Chen, SatsukiVie",
        "link": "https://arxiv.org/abs/2506.01111",
        "github_repo": "https://github.com/satsuki2486441738/FusionAudio",
        "summary": "- This paper introduces FusionAudio-1.2M, a novel large-scale dataset comprising 1.2 million detailed audio captions and 6 million QA pairs.\n- A two-stage automated pipeline for fine-grained audio caption generation is proposed. This pipeline first uses pretrained models to extract contextual cues (speech, music, general sounds, and visual information) and then synthesizes these cues using an LLM to generate detailed captions.\n- The proposed method demonstrates enhanced accuracy and detail by leveraging visual and comprehensive auditory cues, outperforming baselines in terms of caption detail and accuracy according to a manual evaluation.\n- FusionAudio-1.2M improves audio-text alignment and instruction following, achieving better performance in audio-text retrieval tasks compared to existing datasets.\n- Ablation studies confirm the contributions of diverse modalities and the effectiveness of multimodal contextual fusion.",
        "classification": [
            "Audio"
        ],
        "github_urls": [
            "https://github.com/satsuki2486441738/FusionAudio"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
        "authors": "Yu Su, Muhao Chen, Kai Zhang, DarthZhu",
        "link": "https://arxiv.org/abs/2506.01872",
        "github_repo": null,
        "summary": "- This paper investigates the effects of modality extension, a common technique for training multimodal models, on achieving true omni-modality.\n- The researchers analyze the trade-offs between extending modalities and preserving core language abilities, exploring model merging as an alternative approach.\n- Three key research questions are addressed: whether modality extension compromises core language abilities, whether model merging effectively integrates independently fine-tuned modality-specific models, and whether omni-modality extension leads to better generalization.\n- Experiments reveal that modality extension can enhance certain capabilities but may compromise others; weighted model merging outperforms standard average merging by preserving more crucial attributes.\n- The findings suggest that omni-modality fine-tuning is less efficient than modality-specific fine-tuning for specific tasks.",
        "classification": [
            "Multimodal",
            "Any-to-Any",
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/DarthZhu/lm-extend"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
        "authors": "Linjie Li, Kevin Lin, Chung-Ching Lin, xiaofei-wang, dcml0714",
        "link": "https://arxiv.org/abs/2506.05984",
        "github_repo": null,
        "summary": "- This paper explores using audio-aware large language models (ALLMs) as automatic judges for evaluating the speaking styles of speeches generated by spoken language models (SLMs).\n- Two tasks, voice style instruction following and role-playing, were designed to evaluate the SLMs' ability to control speaking styles, including emotion, volume, pace, and emphasis.\n- Human evaluations and ALLM evaluations were compared, showing that the Gemini ALLM's agreement with human judges was comparable to the agreement between human evaluators.\n- The results demonstrated that ALLMs can serve as effective automatic judges for speaking styles, while also highlighting areas where current SLMs need improvement in speaking style control.\n- The paper contributes two new evaluation tasks for SLMs and demonstrates the feasibility and effectiveness of using ALLMs as automatic judges for speaking styles.",
        "classification": [
            "Audio",
            "Text-to-Speech"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
        "authors": "sambaran, abhi1nandy2, ananthmuppidi",
        "link": "https://arxiv.org/abs/2506.05629",
        "github_repo": null,
        "summary": "- This paper introduces a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) for parameter-efficient fine-tuning of large language models.\n- ID-SPAM generates soft prompts based on input tokens, attending to different tokens with varying importance, and keeping the number of trainable parameters small.\n- The proposed approach is compared to state-of-the-art techniques on various tasks, demonstrating improved zero-shot domain transfer capability.\n- ID-SPAM outperforms several baselines on multiple benchmarks, including GLUE and SuperGLUE, showcasing better performance with fewer parameters.\n- Ablation studies highlight the importance of the self-attention mechanism in ID-SPAM, indicating improved efficiency and effectiveness.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "null"
        ],
        "huggingface_urls": [
            "null"
        ],
        "date": "2025-06-09"
    },
    {
        "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
        "authors": "Yuyang Wang, Huangjie Zheng, David Berthelot, Tianrong Chen, Jiatao Gu",
        "link": "https://arxiv.org/abs/2506.06276",
        "github_repo": null,
        "summary": "The paper introduces STARFlow, a generative model based on normalizing flows that achieves strong performance on high-resolution image synthesis.  STARFlow uses Transformer Autoregressive Flow (TARFlow) as its main building block, combining normalizing flows with autoregressive Transformer architectures. The model incorporates a deep-shallow design for improved scalability, learns in the latent space of pre-trained autoencoders, and utilizes a novel guidance algorithm for enhanced sample quality.  STARFlow achieves competitive results in both class- and text-conditional image generation, with sample quality approaching that of state-of-the-art diffusion models.  The authors demonstrate the model's success across various tasks including image inpainting and interactive editing.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/stabilityai/sd-vae-ft-mse"
        ],
        "date": "2025-06-09"
    },
    {
        "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
        "authors": "Yiqiang Feng, Honglei Yan, Panwang Pan, Yuchen Lin, chenguolin",
        "link": "https://arxiv.org/abs/2506.05573",
        "github_repo": null,
        "summary": "- PARTCRAFTER is a novel structured 3D generative model that jointly generates multiple semantically meaningful and geometrically distinct 3D meshes from a single RGB image.\n- The model adopts a unified, compositional generation architecture that utilizes a compositional latent space where each 3D part is represented by a set of disentangled latent tokens and a hierarchical attention mechanism.\n- PARTCRAFTER outperforms existing two-stage methods (HoloPart and MIDI) in generating decomposable 3D meshes on both part-level and scene-level object generation tasks, demonstrating superior performance in terms of generation quality and efficiency.\n- The model is trained on a new dataset curated by mining part-level annotations from large-scale 3D object datasets (Objaverse, ShapeNet, and ABO), enabling part-level supervision during training.\n- PARTCRAFTER demonstrates the strength of part-aware generative priors for 3D understanding and synthesis, as it can generate parts that are not directly visible in the input images.",
        "classification": [
            "Image-to-3D"
        ],
        "github_urls": [
            "https://wgsxm.github.io/projects/partcrafter"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
        "authors": "Hyunwoo Jae, Ankit Nakhawa, Anirudh Satheesh, Andrew Wang, Zikui",
        "link": "https://arxiv.org/abs/2506.05523",
        "github_repo": null,
        "summary": " - This paper introduces MORSE-500, a new video benchmark dataset designed to stress-test multimodal reasoning capabilities. \n - The dataset comprises 500 fully-scripted video clips with embedded questions, programmatically generated using deterministic Python scripts. \n - MORSE-500 spans six reasoning categories (mathematical, abstract, spatial, temporal, physical, and planning), allowing fine-grained control over visual complexity and temporal dynamics. \n - Initial experiments on MORSE-500 reveal substantial performance gaps across all categories for state-of-the-art models, highlighting the need for improved temporal memory, compositionality, and generalization. \n - The authors provide the full dataset, generation scripts, and evaluation harness to facilitate further research in multimodal reasoning.",
        "classification": [
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/morse-benchmark/morse-500-code"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/video-reasoning/morse-500",
            "https://huggingface.co/datasets/video-reasoning/morse-500-view"
        ],
        "date": "2025-06-09"
    },
    {
        "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
        "authors": "Baoqi Pei, Lidong Lu, Yifei Huang, Yuping He, cg1177",
        "link": "https://arxiv.org/abs/2506.06253",
        "github_repo": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
        "summary": "This survey paper does not introduce any new models or datasets.  - It provides a comprehensive overview of video understanding techniques from both egocentric and exocentric viewpoints. - It systematically analyzes recent advancements in three main research directions: leveraging egocentric data to enhance exocentric understanding; utilizing exocentric data to improve egocentric analysis; and joint learning frameworks that unify both perspectives. - The paper also discusses benchmark datasets and proposes promising future research directions.  - A key contribution is the identification and organization of key research challenges and opportunities for future development in cross-view collaborative intelligence.",
        "classification": [
            "Computer Vision",
            "Video Classification",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
        "authors": "Quanxi Wu, Yubo Dong, Siyuan Zhou, Peihao Chen, Hoyard",
        "link": "https://arxiv.org/abs/2506.06199",
        "github_repo": null,
        "summary": "- This paper introduces 3DFlowAction, a novel approach to robotic manipulation that leverages a 3D flow world model to generate 3D optical flow for action guidance.\n- The model is trained on a large-scale dataset, ManiFlow-110k, synthesized from human and robot manipulation videos using a moving object auto-detection pipeline.\n- 3DFlowAction incorporates a flow-guided rendering mechanism and a task-aware grasp pose generation module for closed-loop planning.\n- Extensive experiments demonstrate 3DFlowAction's strong generalization across various manipulation tasks and its reliable cross-embodiment adaptation without hardware-specific training.\n- The proposed method outperforms existing world models and imitation learning methods on several complex robotic manipulation tasks.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/Hoyyyaard/3DFlowAction/"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
        "authors": "Junxian Cai, Longteng Guo, Yepeng Tang, Tongtian Yue, Zikang Liu",
        "link": "https://arxiv.org/abs/2506.05433",
        "github_repo": "https://github.com/johncaged/PrefixGrouper",
        "summary": "- This paper introduces Prefix Grouper, a novel training algorithm for Group Relative Policy Optimization (GRPO) that significantly improves training efficiency.\n- Prefix Grouper addresses the computational redundancy in GRPO by employing a Shared-Prefix Forward strategy, which encodes shared input prefixes only once.\n- Theoretical and empirical evidence demonstrates that Prefix Grouper achieves equivalent training results to standard GRPO while drastically reducing computational cost and memory usage.\n- The method is fully compatible with existing GRPO architectures, requiring minimal modifications to input construction and attention computations.\n- Experiments confirm that Prefix Grouper consistently outperforms the baseline approach, demonstrating its superior efficiency and scalability in long-context reinforcement learning scenarios.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/johncaged/PrefixGrouper"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data",
        "authors": "Zhibin Li, Tom Erez, Steven Bohez, Mauro Comi, Ben Moran",
        "link": "https://arxiv.org/abs/2506.04120",
        "github_repo": null,
        "summary": "- This paper introduces a novel real-to-sim framework for creating accurate physical simulations directly from real-world robot motion, which is a challenging task due to noisy data and dynamic scenes.\n- The framework uses a hybrid scene representation called \"SplatMesh\", which combines the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation.\n- It proposes an end-to-end optimization pipeline that jointly refines all scene components (object geometry and appearance, robot poses, and physical parameters) from raw robot trajectories using differentiable rendering and physics within MuJoCo.\n- The effectiveness of the approach is demonstrated on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, showcasing its capability to achieve high-fidelity object mesh reconstruction, photorealistic novel views, and annotation-free robot pose calibration.\n- The proposed method outperforms existing techniques, particularly in terms of accuracy and robustness in the case of noisy data and dynamic scenes.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
        "authors": "Harshil Patel, helloparthshah, guineapig",
        "link": "https://arxiv.org/abs/2506.04255",
        "github_repo": "https://github.com/HASHIRU-AI/HASHIRU",
        "summary": "- HASHIRU is a novel multi-agent system framework designed to enhance flexibility, resource efficiency, and adaptability in handling complex tasks.\n- It features a hierarchical structure with a \"CEO\" agent dynamically managing specialized \"employee\" agents, leveraging a hybrid intelligence model that prioritizes smaller, cost-effective local LLMs while flexibly integrating external APIs and larger models when necessary.\n- The system incorporates an economic model with hiring/firing costs, promoting efficient resource allocation and team stability, and includes autonomous API tool creation.\n- Evaluations demonstrate HASHIRU's capabilities in academic paper review, safety assessments, and complex reasoning tasks, often outperforming existing models.\n- Through case studies, it showcases self-improvement via autonomous cost model generation, tool integration, and budget management.",
        "classification": [
            "Natural Language Processing",
            "Question Answering",
            "Text Generation",
            "Reinforcement Learning",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/HASHIRU-AI/HASHIRU"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming",
        "authors": "Kai Shen, Hongyan Li, Yang Sun, Siyao Liu, zhwang01",
        "link": "https://arxiv.org/abs/2506.05817",
        "github_repo": null,
        "summary": " - This paper introduces CodeContests+, a new dataset for competitive programming that uses an LLM-based agent system to generate high-quality test cases.\n - The agent system consists of a generator agent and a validator agent, which work together to create test cases that are both comprehensive and correct.\n - CodeContests+ achieves significantly higher accuracy than CodeContests, especially in terms of True Positive Rate (TPR).\n - Experiments in LLM Reinforcement Learning (RL) show that improvements in test case quality lead to substantial advantages in RL training.\n - The CodeContests+ dataset includes a total of 11,690 problems and millions of test cases, making it one of the largest and most comprehensive datasets for competitive programming.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/bytedance/SandboxFusion"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/ByteDance-Seed/Code-Contests-Plus"
        ],
        "date": "2025-06-09"
    },
    {
        "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning",
        "authors": "Chong Peng, Hao Yang, Lei Wang, Kaiyuan Deng, Shenshen Li",
        "link": "https://arxiv.org/abs/2506.04755",
        "github_repo": "https://github.com/Leo-ssl/RAP",
        "summary": "- This paper introduces a novel data selection paradigm, Reasoning Activation Potential (RAP), to improve the efficiency of multi-modal reasoning in large language models (LLMs).\n- RAP identifies high-value cognitive samples by using two estimators: Causal Discrepancy Estimator (CDE) and Attention Confidence Estimator (ACE), which remove samples that overly rely on language priors and those with attention on irrelevant tokens, respectively.\n- The Difficulty-aware Replacement Module (DRM) replaces easy samples with more challenging ones to ensure complexity in the training process.\n- Experimental results show RAP achieves superior performance using only 9.3% of the training data and reduces computational costs by over 43%, outperforming existing methods on six datasets.\n- The effectiveness of RAP is demonstrated across different model architectures and RL algorithms, showcasing its robustness and broad applicability.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/Leo-ssl/RAP"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    },
    {
        "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information\n  Extraction",
        "authors": "Eneko Agirre, Iker Garc\u00eda-Ferrero, OSainz, neildlf",
        "link": "https://arxiv.org/abs/2506.00649",
        "github_repo": null,
        "summary": "- This paper introduces GUIDEX, a novel method for automatically defining domain-specific schemas, inferring guidelines, and generating synthetically labeled instances for improved zero-shot information extraction.\n- GUIDEX achieves state-of-the-art results across seven zero-shot Named Entity Recognition benchmarks, surpassing previous methods by up to 7 F1 points without human-labeled data and nearly 2 F1 points higher when combined with it.\n- The method involves four steps: document summarization, structured representation, guideline generation, and instance extraction, all leveraging LLMs.\n- GUIDEX demonstrates enhanced comprehension of complex, domain-specific annotation schemas and produces high-quality synthetic data by using executable Python code for validation and ensuring consistency.\n- The generated synthetic dataset exhibits diverse labels across domains like medicine, economics, history, music, and education, showcasing its versatility and broad applicability.",
        "classification": [
            "Natural Language Processing",
            "Zero-Shot Classification",
            "Feature Extraction",
            "Text Generation"
        ],
        "github_urls": [
            "https://neilus03.github.io/guidex.com"
        ],
        "huggingface_urls": [],
        "date": "2025-06-09"
    }
]