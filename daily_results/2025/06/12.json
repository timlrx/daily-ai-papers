[
    {
        "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
        "authors": "Ivan Oseledets, Andrey Kuznetsov, Alexander Zubrey, Matvey Skripkin, LiPengyi29",
        "link": "https://arxiv.org/abs/2506.06395",
        "github_repo": null,
        "summary": "- This paper introduces Reinforcement Learning via Self-Confidence (RLSC), a novel few-shot reinforcement learning method for fine-tuning language models that uses the model's own confidence as a reward signal.\n- RLSC eliminates the need for human annotations, external reward models, or manual reward shaping, making it efficient and scalable.\n- When applied to the Qwen2.5-Math-7B model, RLSC achieved significant accuracy improvements (+13.4% on AIME2024, +21.2% on MATH500, and other datasets) with only 16 samples per question and 10 or 20 training steps.\n- The method is based on mode sharpening, which implicitly improves the model's confidence and generalisation ability.\n- RLSC is suitable for resource-constrained settings due to its minimal data and computational requirements.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "authors": "Lu Jiang, Weilin Huang, Tuyen Hoang, Haoyuan Guo, Yu Gao",
        "link": "https://arxiv.org/abs/2506.09113",
        "github_repo": null,
        "summary": "- Seedance 1.0 is a high-performance and inference-efficient video generation model that integrates several core technical improvements, including multi-source data curation, efficient architecture design, and carefully optimized post-training approaches.\n- The model architecture employs a variational autoencoder (VAE) for efficient compression and a diffusion transformer with decoupled spatial and temporal layers for effective video generation.\n- Seedance 1.0 achieves over 10x inference speedup through multi-stage distillation strategies and system-level optimizations, generating a 5-second 1080p video in 41.4 seconds.\n- Compared to state-of-the-art models, Seedance 1.0 excels in high-quality and fast video generation with superior spatiotemporal fluidity and precise instruction adherence, particularly in complex multi-subject contexts.\n- The model's superior performance is validated through extensive evaluations on the Artificial Analysis Arena and SeedVideoBench 1.0, where it achieves top rankings across both text-to-video and image-to-video tasks.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "PlayerOne: Egocentric World Simulator",
        "authors": "Fan Wang, Xiang Bai, Xi Chen, Hao Luo, Yuanpeng Tu",
        "link": "https://arxiv.org/abs/2506.09995",
        "github_repo": null,
        "summary": "- PlayerOne is the first egocentric realistic world simulator that generates egocentric videos aligned with user motion captured by an exocentric camera.\n- The model employs a coarse-to-fine training pipeline, first pre-training on large-scale egocentric text-video data and then fine-tuning on motion-video data.\n- A part-disentangled motion injection scheme is used to enable precise control of part-level movements, enhancing the realism of generated videos.\n- A joint reconstruction framework progressively models both 4D scenes and video frames, ensuring scene consistency during long-form video generation.\n- Experimental results demonstrate PlayerOne's superior performance in generating world-consistent videos with precise motion control compared to existing methods.",
        "classification": [
            "Text-to-Video"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
        "authors": "Yuxi Ren, Jianwen Jiang, Hao He, Ceyuan Yang, Shanchuan Lin",
        "link": "https://arxiv.org/abs/2506.09350",
        "github_repo": null,
        "summary": "- This paper introduces Autoregressive Adversarial Post-Training (AAPT), a novel method that transforms pre-trained latent video diffusion models into real-time interactive video generators.\n- AAPT employs a single neural function evaluation (1NFE) to autoregressively generate latent frames, enabling real-time streaming and interactive control.\n- The model architecture utilizes a causal transformer with block causal attention and a KV cache for efficient one-step generation.\n- Experimental results demonstrate that the proposed 8B parameter model achieves real-time 24fps video generation at 736x416 resolution on a single H100 GPU, outperforming existing state-of-the-art methods.\n- The effectiveness of the adversarial training paradigm and long-video training is showcased through experiments on pose-conditioned virtual human generation and camera-controlled world exploration.",
        "classification": [
            "Image-to-Video",
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
        "authors": "Weihua Luo, Longyue Wang, Xue Yang, Yiyu Wang, Zhenran Xu",
        "link": "https://arxiv.org/abs/2506.09790",
        "github_repo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
        "summary": "- ComfyUI-R1 is a large reasoning model for automated workflow generation in ComfyUI, achieving a 97% format validity rate and surpassing previous state-of-the-art methods.\n- The model uses a two-stage training framework: (1) cold-start CoT fine-tuning and (2) reinforcement learning with a rule-metric hybrid reward function.\n- ComfyUI-R1 outperforms existing methods such as GPT-40 and Claude series in terms of format validity rate, node-level and graph-level F1 scores and pass rate on ComfyBench.\n- The model leverages a code-based workflow representation for superior performance compared to a JSON-based representation.\n- Qualitative comparison demonstrates ComfyUI-R1's capability in handling complex workflows and ensuring alignment with user instructions.",
        "classification": [
            "Image-to-Image",
            "Text-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/AIDC-AI/ComfyUI-Copilot"
        ],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
        "authors": "Yu Cheng, Yuqing Xia, Shijie Cao, Shuming Guo, Yizhao Gao",
        "link": "https://arxiv.org/abs/2506.08889",
        "github_repo": "https://github.com/microsoft/SeerAttention",
        "summary": "- SeerAttention-R is a novel sparse attention framework designed to enhance the efficiency of long decoding in reasoning models.\n- It extends SeerAttention by removing query pooling and introducing modifications to support autoregressive decoding, improving efficiency.\n- The model uses a lightweight plug-in gating mechanism that can be easily integrated into existing pretrained models without modifying original parameters.\n- Experiments demonstrate that SeerAttention-R achieves near-lossless accuracy with large sparse attention blocks (64/128), outperforming existing methods.\n- An optimized sparse decoding kernel achieves near-theoretical speedups, demonstrating the efficiency and scalability of the proposed framework.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/microsoft/SeerAttention"
        ],
        "huggingface_urls": [
            "string"
        ],
        "date": "2025-06-12"
    },
    {
        "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
        "authors": "Mouxiang Chen, Jian Yang, Min Yang, Jiaxi Yang, Lei Zhang",
        "link": "https://arxiv.org/abs/2506.09003",
        "github_repo": "https://github.com/Hambaobao/SWE-Flow",
        "summary": "SWE-Flow is a novel data synthesis framework that leverages Test-Driven Development (TDD) to automatically generate software engineering data.  The core of SWE-Flow is the construction of a Runtime Dependency Graph (RDG) which captures function interactions, enabling the generation of a structured development schedule. At each step, SWE-Flow produces a partial codebase, corresponding unit tests, and code modifications, resulting in fully verifiable TDD tasks.  SWE-Flow generated 16,061 training and 2,020 test instances from real-world projects, creating the SWE-Flow-Bench benchmark.  Experiments showed fine-tuning on this dataset significantly improved performance in TDD-based coding tasks.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://github.com/Hambaobao/SWE-Flow"
        ],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
        "authors": "Gaojie Lin, Chao Liang, Jianwen Jiang, Jiaqi Yang, Zhenzhi Wang",
        "link": "https://arxiv.org/abs/2506.09984",
        "github_repo": null,
        "summary": "- InterActHuman is a novel framework for multi-concept human animation that uses layout-aligned audio conditions.\n- It addresses the limitations of existing methods by introducing an attention module to explicitly predict spatial locations of concepts and bind audio conditions to corresponding regions.\n- The model is trained using a large dataset of human-centric videos with over two million video-entity pairs.\n- InterActHuman outperforms existing methods in terms of lip synchronization, motion diversity, and overall video quality, as demonstrated in both qualitative and quantitative evaluations.\n- The framework introduces a novel iterative mask prediction strategy to achieve accurate spatial alignment of multi-modal conditions during inference.",
        "classification": [
            "Text-to-Video",
            "Multimodal",
            "Audio"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
        "authors": "Haruki Nishimura, Igor Gilitschenski, Shengxiang Sun, Yuanliang Ju, Qiao Gu",
        "link": "https://arxiv.org/abs/2506.09937",
        "github_repo": null,
        "summary": "This paper introduces a novel multitask failure detection method, SAFE, for generalist vision-language-action (VLA) models.  SAFE utilizes internal VLA features to predict the likelihood of task failure, demonstrating strong generalization to unseen tasks and outperforming existing baselines.  SAFE is compatible with diverse policy architectures and achieves state-of-the-art results in both simulation and real-world environments by using conformal prediction to determine failure thresholds.  The model's architecture includes an LSTM and a multi-layer perceptron to effectively process VLA features and generate failure predictions.  Results demonstrate improved accuracy and faster detection times compared to existing methods.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://vla-safe.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
        "authors": "Bernhard Sch\u00f6lkopf, Maximilian Dax, Tim Z. Xiao, Simon Buchholz, Zeju Qiu",
        "link": "https://arxiv.org/abs/2506.08001",
        "github_repo": null,
        "summary": " - This paper introduces POET, a novel reparameterized training algorithm for LLMs that uses orthogonal equivalence transformation to optimize neurons.\n - POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix, which preserves the spectral properties of weight matrices.\n - The proposed method demonstrates improved generalization and stability compared to standard AdamW optimization, achieving state-of-the-art validation perplexity results in several large-scale LLaMA model experiments.\n - POET introduces two levels of approximations for efficient training: stochastic primitive optimization and approximate orthogonality via Cayley-Neumann parameterization.\n - Extensive experiments demonstrate POET's effectiveness and scalability in training LLMs.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/jiaweizzhao/GaLore"
        ],
        "huggingface_urls": [
            "https://huggingface.co/"
        ],
        "date": "2025-06-12"
    },
    {
        "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
        "authors": "Taha Emre, Ronald Fecso, Emese S\u00fckei, Botond Fazekas, Jos\u00e9 Morano",
        "link": "https://arxiv.org/abs/2506.08900",
        "github_repo": "https://github.com/j-morano/MIRAGE",
        "summary": " - The paper introduces MIRAGE, a novel multimodal foundation model for retinal OCT/SLO image analysis, trained using a paired multimodal Masked Autoencoder (MAE) approach. \n- MIRAGE significantly outperforms state-of-the-art foundation models on both classification and segmentation tasks, showcasing its robustness and generalization capabilities. \n- A new comprehensive evaluation benchmark is also proposed for validating foundation models in retinal OCT/SLO analysis, including classification and segmentation tasks across multiple datasets. \n- The MIRAGE model is based on a Vision Transformer encoder with modality-specific linear projection layers and Transformer decoders, effectively utilizing complementary information from OCT and SLO images. \n- The superior performance of MIRAGE across multiple datasets highlights its suitability for developing robust AI systems for retinal image analysis, making it a valuable tool for clinicians.",
        "classification": [
            "Image Segmentation",
            "Image Classification",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/j-morano/MIRAGE"
        ],
        "huggingface_urls": [],
        "date": "2025-06-12"
    },
    {
        "title": "Branched Schr\u00f6dinger Bridge Matching",
        "authors": "Pranam Chatterjee, Alexander Tong, Yinuo Zhang, Sophia Tang",
        "link": "https://arxiv.org/abs/2506.09007",
        "github_repo": null,
        "summary": " - This paper introduces Branched Schr\u00f6dinger Bridge Matching (BranchSBM), a novel framework for learning branched trajectories from an initial distribution to multiple target distributions. \n- BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, allowing it to model population-level divergence. \n- The model architecture consists of several neural networks that parameterize the velocity and growth rates of multiple branches, optimizing for energy and mass conservation. \n- Experiments on various datasets demonstrate that BranchSBM outperforms existing single-branch methods, particularly in scenarios with branched dynamics. \n- These improvements are particularly evident in tasks involving multi-path surface navigation, modeling cell fate bifurcations, and predicting heterogeneous cellular responses.",
        "classification": [
            "Other"
        ],
        "github_urls": [
            "None"
        ],
        "huggingface_urls": [
            "https://huggingface.co/ChatterjeeLab/BranchSBM"
        ],
        "date": "2025-06-12"
    }
]