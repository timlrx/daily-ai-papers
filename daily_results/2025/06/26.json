[
    {
        "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
        "authors": "Ke Ji, Shunian Chen, Zhenyang Cai, Junying Chen, cppppppc",
        "link": "https://arxiv.org/abs/2506.18095",
        "github_repo": null,
        "summary": "- This paper introduces ShareGPT-40-Image, the first publicly available dataset containing 45K text-to-image and 46K image-to-image pairs generated using GPT-40's image generation capabilities.\n- Leveraging this dataset, a new multimodal large language model called Janus-40 is developed, demonstrating significant improvements in text-to-image and text-and-image-to-image generation.\n- Janus-40 achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8xA800 GPU machine.\n- The experiments show that Janus-40 outperforms other open-source models on several benchmarks, including improvements in image quality and instruction following.\n- This work contributes to the democratization of advanced image generation techniques by providing a large-scale, high-quality dataset and a powerful open-source model.",
        "classification": [
            "Text-to-Image",
            "Image-to-Image",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/FreedomIntelligence/ShareGPT-40-Image"
        ],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
        "authors": "Jaewoo Kang, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, affjljoo3581",
        "link": "https://arxiv.org/abs/2506.19697",
        "github_repo": "https://github.com/dmis-lab/Outlier-Safe-Pre-Training",
        "summary": " - This paper introduces Outlier-Safe Pre-training (OSP), a novel method for preventing outlier formation during the pre-training of large language models (LLMs).\n - OSP combines three key innovations: the Muon optimizer, Single-Scale RMSNorm, and a learnable embedding projection. \n -  The proposed method achieves a 35.7 average score across 10 benchmarks under aggressive 4-bit quantization, significantly outperforming an Adam-trained model (26.5). \n -  OSP models exhibit near-zero excess kurtosis (0.04), compared to extreme values (1818.56) in standard models, demonstrating a fundamental alteration of LLM quantization behavior. \n - The study trained a 1.4B-parameter model on 1 trillion tokens without outliers, showcasing the scalability and effectiveness of the proposed method.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/dmis-lab/Outlier-Safe-Pre-Training"
        ],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
        "authors": "Hang Xu, Siyuan He, Boyu Li, WizardTY, tellarin",
        "link": "https://arxiv.org/abs/2506.16012",
        "github_repo": "https://github.com/ds199895/DualTHOR.git",
        "summary": "- The paper introduces DualTHOR, a physics-based simulation platform for dual-arm humanoid robots designed for contingency-aware planning.\n- DualTHOR extends AI2-THOR by incorporating real-world robot assets, a dual-arm task suite, inverse kinematics solvers, and a contingency mechanism that simulates real-world uncertainties.\n- The platform allows for a more comprehensive evaluation of the robustness and generalization of Vision-Language Models (VLMs) in household environments.\n- Experiments reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies.\n- The authors highlight the importance of using DualTHOR to develop more capable VLMs for embodied tasks.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://github.com/ds199895/DualTHOR.git"
        ],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
        "authors": "Pengfei Liu, Xuefeng Li, Fan Zhou, Zengzhi Wang",
        "link": "https://arxiv.org/abs/2506.20512",
        "github_repo": null,
        "summary": " - OctoThinker is a novel two-stage mid-training strategy that enhances the compatibility of Llama language models with reinforcement learning (RL). \n - The study reveals that high-quality mathematical corpora significantly boost both base model and RL performance, surpassing existing alternatives. \n - The method introduces a \"Stable-then-Decay\" approach, first training with a constant learning rate and then decaying it across multiple CoT-focused branches, improving downstream RL performance.\n - OctoThinker models demonstrate strong RL compatibility, significantly reducing the performance gap with more RL-friendly models like Qwen. \n - The authors release the open-source OctoThinker models along with a curated math reasoning corpus of over 70 billion tokens.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
        "authors": "Jing Shao, Zhe Zhang, Lehan He, lsheng2024, zx55",
        "link": "https://arxiv.org/abs/2506.18315",
        "github_repo": null,
        "summary": "- The paper introduces Property-Generated Solver (PGS), a novel framework that leverages Property-Based Testing (PBT) to validate LLM-generated code.\n- PGS uses two LLM-based agents: a Generator for code generation and refinement, and a Tester for managing the PBT lifecycle and providing feedback.\n- Experimental results on multiple code generation benchmarks demonstrate that PGS achieves substantial improvements (23.1% to 37.3% relative gains) over established TDD methods.\n- The framework addresses the limitations of traditional TDD approaches by focusing on high-level program properties instead of relying on specific input-output examples.\n- PGS provides a robust mechanism for steering LLMs towards more correct and generalizable code by effectively decoupling code generation from its validation.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
        "authors": "Yibin Liu, Zijian Cai, Baijun Chen, Zanxin Chen, TianxingChen",
        "link": "https://arxiv.org/abs/2506.18088",
        "github_repo": null,
        "summary": "- This paper introduces RoboTwin 2.0, a scalable framework for generating diverse and realistic dual-arm robot manipulation data, addressing limitations in existing datasets.\n- RoboTwin 2.0 incorporates an automated expert data generation pipeline that uses multimodal large language models (MLLMs) and simulation-in-the-loop refinement for high-quality trajectory synthesis.\n- The framework employs comprehensive domain randomization to improve sim-to-real transfer, enhancing policy robustness to unseen real-world conditions.\n- Experiments demonstrate significant improvements in policy generalization and robustness with RoboTwin 2.0 compared to existing methods. \n- The framework, benchmark, and dataset are publicly available, facilitating broader research in robust bimanual manipulation.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://robotwin-platform.github.io"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/RoboTwin2_dataset"
        ],
        "date": "2025-06-26"
    },
    {
        "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
        "authors": "Pedro Reviriego, Gonzalo Mart\u00ednez, Javier Conde, Raquel Ferrando",
        "link": "https://arxiv.org/abs/2506.18674",
        "github_repo": null,
        "summary": "- This paper investigates the potential benefits of optimizing tokenizers specifically for conversational applications in large language models (LLMs).\n- The authors retrain several popular tokenizers using a chatbot conversation dataset and compare their performance to the original tokenizers on both the conversation dataset and the original LLM training dataset.\n- Results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, leading to energy savings in the range of 5% to 10%, with minimal or positive impact on tokenization efficiency for the training corpus.\n- Although further research is needed to confirm the results and evaluate the impact on training costs, the findings suggest that customizing tokenizers for conversation can improve LLM efficiency.\n- The study highlights the importance of considering tokenization efficiency, often overlooked, in optimizing LLMs for real-world applications.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/RaquelFerrando/conversational_tokenizers.git"
        ],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
        "authors": "Sara Hooker, Julia Kreutzer, Ye Shen, Daniel D'souza, ammar-cohere",
        "link": "https://arxiv.org/abs/2506.20544",
        "github_repo": null,
        "summary": "This paper introduces novel sampling and selection strategies for improving the inference-time compute of multilingual LLMs. The proposed methods significantly improve performance across multiple languages and tasks, particularly in underrepresented languages. Compared to existing methods, the proposed approach yields notable gains in win-rates on various benchmarks, even against strong commercial models. These strategies involve adapting sampling and selection to account for diversity in domains and languages, leading to an average +6.8 jump in win-rates for 8B models and +9.0 for 111B models.  The researchers also provide a detailed experimental setup and extensive analysis of existing methods, highlighting the need for language- and task-aware approaches. Overall, the findings underscore the potential for democratizing performance improvements across various languages and tasks through efficient inference-time compute strategies.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
        "authors": "Ningyu Zhang, Huajun Chen, Wenhao Yu, Yunzhi Yao, Haoze Wu",
        "link": "https://arxiv.org/abs/2506.20495",
        "github_repo": "https://github.com/zjunlp/ReCode",
        "summary": "- This paper introduces ReCode, a novel framework that uses reinforcement learning to help large language models (LLMs) adapt to frequent updates in external library APIs.\n- ReCode constructs a dataset of approximately 2,000 data entries to train LLMs to perform version migration based on updated information and uses a modified string similarity metric for code evaluation as the reward for reinforcement learning.\n- Experimental results show that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task, and has less impact on LLMs' general code generation abilities compared to supervised fine-tuning.\n- ReCode is applied to various LLMs and reinforcement learning algorithms, all achieving consistent improvements, with Qwen2.5-Coder-7B outperforming a 32B parameter code instruction-tuned model and a reasoning model with the same architecture.\n- This work demonstrates that ReCode provides a promising solution to the challenge of adapting LLMs to the dynamically evolving nature of software development environments.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/zjunlp/ReCode"
        ],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling",
        "authors": "Farnood Salehi, Tobias Vontobel, RMW, msadat97",
        "link": "https://arxiv.org/abs/2506.20452",
        "github_repo": null,
        "summary": "- HiWave is a novel training-free method for high-resolution image generation that uses pretrained diffusion models.\n- It employs a two-stage pipeline: first generating a base image and then enhancing it using a patch-wise DDIM inversion strategy and a wavelet-based detail enhancer.\n- HiWave significantly reduces common artifacts like object duplication, improving both visual quality and structural coherence.\n- In user studies, HiWave outperformed state-of-the-art methods in over 80% of comparisons, demonstrating its effectiveness in generating high-quality, artifact-free images.\n- The method successfully extends the capabilities of pretrained diffusion models to ultra-high resolutions (e.g., 4096x4096) without any additional training.",
        "classification": [
            "Text-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency\n  Models",
        "authors": "Aibek Alanov, Andrey Kuznetsov, Ilia Beletskii",
        "link": "https://arxiv.org/abs/2506.19103",
        "github_repo": "https://github.com/ControlGenAI/Inverse-and-Edit",
        "summary": "- This paper introduces a novel framework that enhances image inversion using cycle consistency models for high-quality image editing.\n- The proposed method improves reconstruction accuracy and enables a controllable trade-off between editability and content preservation using a cycle-consistency optimization strategy.\n- The framework achieves state-of-the-art performance across various image editing tasks and datasets, surpassing full-step diffusion models while maintaining efficiency.\n- The method is computationally efficient, requiring only four steps for high-quality editing, unlike full-step approaches.\n- The code for the proposed method is publicly available on GitHub.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/ControlGenAI/Inverse-and-Edit"
        ],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
        "authors": "Carlos C. N. Kuhn, adnaan525",
        "link": "https://arxiv.org/abs/2506.18403",
        "github_repo": null,
        "summary": "- This paper introduces the Debugging Decay Index (DDI), a novel metric to quantify the effectiveness of debugging in large language models (LLMs) for code generation.\n- The DDI is based on an exponential decay model that captures the diminishing returns of successive debugging attempts.\n- The authors propose a \"fresh start\" strategy to mitigate debugging decay by strategically restarting the code generation process when the effectiveness drops below a certain threshold.\n- Empirical results across multiple models demonstrate that this fresh start strategy can improve overall code generation accuracy compared to continuing debugging.\n- The DDI framework offers a multi-dimensional evaluation of debugging effectiveness, providing insights into initial performance, decay rate, optimal intervention points, and model fit quality.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
        "authors": "Eric de la Clergerie, Nathan Godey, rntc",
        "link": "https://arxiv.org/abs/2506.20331",
        "github_repo": null,
        "summary": "- This paper introduces Biomed-Enriched, a biomedical dataset created through a two-stage annotation process using LLMs.\n- The dataset contains 2 million clinical case paragraphs, including 450K high-quality ones from articles with commercial-use licenses, addressing the scarcity of publicly available clinical text.\n- Experiments show that strategically combining quality filtering and domain upsampling significantly improves data efficiency and targeted model performance.\n- The curated subsets in Biomed-Enriched enabled targeted improvements in continual pretraining experiments, with clinical upsampling boosting performance by 5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by 1%.\n- The results demonstrate the potential for more efficient and effective biomedical pretraining strategies.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-06-26"
    },
    {
        "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
        "authors": "Paul Laban, Matt Laing, AleksandrAlgazinov",
        "link": "https://arxiv.org/abs/2506.19502",
        "github_repo": "https://github.com/AlgazinovAleksandr/Multi-Agent-MATE",
        "summary": "- The paper introduces MATE, a multimodal accessibility MAS that performs modality conversions based on user needs, outperforming other LLMs and statistical models in experiments.\n- MATE supports multiple model types, from LLM API calling to custom ML classifiers, and runs locally to ensure privacy and security.\n- It includes ModCon-Task-Identifier, a model that extracts modality conversion tasks from user input, showing consistent improvement over other LLMs and models.\n- The system is adaptable to various needs and compatible with a wide range of hardware, designed to assist individuals with disabilities in interacting with digital environments.\n- The code and data for MATE are publicly available, promoting further research and development in the field of AI for accessibility.",
        "classification": [
            "Multimodal",
            "Any-to-Any",
            "Text-to-Speech",
            "Text-to-Image",
            "Image-to-Text",
            "Image-to-Video",
            "Text-to-Audio",
            "Image-to-3D",
            "Text-to-Video",
            "Audio-to-Audio",
            "Automatic Speech Recognition",
            "Natural Language Processing",
            "Text Classification"
        ],
        "github_urls": [
            "https://github.com/AlgazinovAleksandr/Multi-Agent-MATE"
        ],
        "huggingface_urls": [],
        "date": "2025-06-26"
    }
]