[
    {
        "title": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D\n  Worlds from Words or Pixels",
        "authors": "Junta Wu, Zhenwei Wang, HunyuanWorld Team, nightkiller, LeoLau",
        "link": "https://arxiv.org/abs/2507.21809",
        "github_repo": null,
        "summary": "- This paper introduces HunyuanWorld 1.0, a novel framework for generating immersive, explorable, and interactive 3D worlds from text or image inputs.\n- The framework uses a semantically layered 3D mesh representation with panoramic images as world proxies, enabling coherent 360\u00b0 views and seamless integration with existing graphics pipelines.\n- HunyuanWorld 1.0 features disentangled object representations for enhanced interactivity, supporting applications in virtual reality, physical simulation, and game development.\n- Extensive experiments demonstrate state-of-the-art performance compared to existing methods in terms of generating coherent, explorable, and interactive 3D worlds.\n- The model supports versatile applications in virtual reality, physical simulation, game development, and interactive content creation.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0"
        ],
        "huggingface_urls": [],
        "date": "2025-07-30"
    }
]