[
    {
        "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
        "authors": "Xinyu Ye, Yingzhe Peng, Zhou Ziheng, Yizhou Zhou, Yongliang Wu",
        "link": "https://arxiv.org/abs/2508.05629",
        "github_repo": "https://github.com/yongliang-wu/DFT",
        "summary": "- This paper introduces Dynamic Fine-Tuning (DFT), a novel method that improves the generalization capabilities of Supervised Fine-Tuning (SFT) for Large Language Models (LLMs).\n- DFT addresses the limited generalization of SFT by rectifying the problematic reward structure implicitly encoded in standard SFT gradients.\n- The method dynamically rescales the objective function for each token using its probability, resulting in more stable gradient updates and enhanced generalization.\n- Empirical results demonstrate that DFT significantly outperforms standard SFT across multiple challenging benchmarks and base models, exhibiting improved generalization.\n- DFT also shows competitive results in offline RL settings, offering a simpler alternative to existing RL approaches.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/yongliang-wu/DFT"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data",
        "authors": "Zongxia Li, Hongming Zhang, Xiaoyang Wang, Wenhao Yu, Chengsong Huang",
        "link": "https://arxiv.org/abs/2508.05004",
        "github_repo": null,
        "summary": "- This paper introduces R-Zero, a novel self-evolving framework for training reasoning LLMs from scratch, without any pre-existing tasks or human labels.\n- R-Zero initializes two independent models, a Challenger and a Solver, which co-evolve through interaction: the Challenger proposes challenging tasks, and the Solver attempts to solve them.\n- The Challenger is rewarded for creating tasks at the edge of the Solver's capabilities, while the Solver is rewarded for successfully solving increasingly difficult challenges.\n- Experiments demonstrate that R-Zero substantially improves reasoning capabilities across different backbone LLMs, achieving significant gains on math and general-domain reasoning benchmarks.\n- R-Zero is a model-agnostic and fully autonomous framework, eliminating the need for vast human-curated data which is a major bottleneck for existing self-evolving LLM methods.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/Chengsong-Huang/R-Zero"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning",
        "authors": "Ziming Wang, B\u00f6rje F. Karlsson, Ye Wang, Pi Bu, Xinrun Xu",
        "link": "https://arxiv.org/abs/2508.05405",
        "github_repo": null,
        "summary": " - DeepPHY is a novel benchmark framework designed to systematically evaluate Vision-Language Models' (VLMs) understanding of fundamental physical principles through challenging simulated environments.\n - It integrates multiple physical reasoning environments of varying difficulty and incorporates fine-grained evaluation metrics, finding that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise control.\n - DeepPHY systematically integrates six challenging physics-based simulation environments: PHYRE, I-PHYRE, Kinetix, Pooltool, Angry Birds, and Cut the Rope.\n -  The benchmark utilizes a unified framework and standardized metrics to transform diverse physics simulators into a rigorous and accessible testbed, evaluating VLMs and collecting interaction data.\n - DeepPHY reveals the boundaries and core shortcomings of current VLMs, highlighting their limitations in complex physical interaction, long-horizon planning, and dynamic adaptation.",
        "classification": [
            "Reinforcement Learning",
            "Robotics",
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/XinrunXu/DeepPHY"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation",
        "authors": "Shengcong Chen, Donglin Yang, Siyuan Huang, Pengfei Zhou, Yue Liao",
        "link": "https://arxiv.org/abs/2508.05635",
        "github_repo": null,
        "summary": "- This paper introduces Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework.\n- At its core is GE-Base, a large-scale instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions.\n- GE-Act maps latent representations to executable action trajectories, enabling precise and generalizable policy inference.\n- GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development.\n- The platform is equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment, and is shown to outperform existing methods.",
        "classification": [
            "Robotics"
        ],
        "github_urls": [
            "https://genie-envisioner.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity",
        "authors": "Zhibing Li, Tong Wu, Ziyang Chu, Long Zhuo, Yuhan Zhang",
        "link": "https://arxiv.org/abs/2508.05609",
        "github_repo": null,
        "summary": " - Hi3DEval is a new hierarchical evaluation framework for 3D generation that assesses object-level and part-level quality, along with material evaluation via reflectance cues. \n- It introduces a large-scale benchmark (Hi3DBench) with diverse 3D generative models and human-aligned annotations generated via a multi-agent, multi-modal pipeline. \n- Hi3DEval uses a hybrid automated scoring system integrating video-based and naive 3D-based representations to enhance evaluations of 3D structure. \n- Experiments demonstrate that Hi3DEval outperforms existing image-based metrics in modeling 3D characteristics and shows superior alignment with human preference. \n- This framework provides a scalable alternative to manual evaluations and detailed diagnostic analysis capabilities.",
        "classification": [
            "Text-to-3D",
            "Image-to-3D",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/datasets/anonymous-mY2nG5/H3DBench"
        ],
        "date": "2025-08-08"
    },
    {
        "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented\n  Generation?",
        "authors": "Junjie Yang, Dongping Chen, Yaochen Wang, Mingjia Wang, Wenxuan Shen",
        "link": "https://arxiv.org/abs/2508.03644",
        "github_repo": null,
        "summary": "This paper introduces DOUBLE-BENCH, a new large-scale, multilingual, and multimodal benchmark designed for evaluating Document Retrieval-Augmented Generation (RAG) systems.  It addresses shortcomings of previous benchmarks by using a large corpus of real-world documents and human-verified queries. DOUBLE-BENCH includes queries in six languages and covers various document types such as PDFs and scanned documents.  Experimental results reveal a narrowing gap between text and visual embedding models, highlighting limitations of current RAG frameworks. The authors hope that DOUBLE-BENCH will foster future research in improving document RAG systems.",
        "classification": [
            "Document Question Answering"
        ],
        "github_urls": [
            "https://double-bench.github.io"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?",
        "authors": "Huan Liu, Chengshuai Zhao, Zhen Tan, Dawei Li, Bohan Jiang",
        "link": "https://arxiv.org/abs/2508.03990",
        "github_repo": null,
        "summary": "- This paper introduces a novel large-scale dataset containing 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs.\n- A principle-guided LLM-as-a-judge evaluation framework is proposed, employing dual judges to assess explanation quality, which aligns well with human evaluations.\n- Fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) significantly improves the quality of generated explanations.\n- The study reveals that explanation quality varies significantly across models, audiences, and categories, with DPO- and SFT-finetuned models outperforming their larger counterparts.\n- This research pioneers a large-scale, systematic investigation of LLMs' capabilities in explaining well-being concepts and provides insights into the strengths and weaknesses of current LLMs in this specific task.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text2Text Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "Can Large Multimodal Models Actively Recognize Faulty Inputs? A\n  Systematic Evaluation Framework of Their Input Scrutiny Ability",
        "authors": "Yuan Wu, Yi Chang, Gengxu Li, Jinzhe Li, Haiqi Yang",
        "link": "https://arxiv.org/abs/2508.04017",
        "github_repo": "https://github.com/MLGroupJLU/LMM_ISEval",
        "summary": "- This paper introduces ISEval, a novel framework for evaluating the input scrutiny ability of Large Multimodal Models (LMMs).\n- ISEval uses seven categories of flawed premises and three evaluation metrics to assess LMMs' ability to proactively identify and report errors in inputs.\n- The evaluation of ten advanced LMMs reveals that most struggle to autonomously detect flawed inputs, relying heavily on explicit prompts.\n- Error type significantly impacts performance, with models excelling at logical fallacies but struggling with surface-level linguistic errors.\n- Cross-modal inconsistencies reveal modality preferences, with most models favoring visual input when conflicts arise.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/MLGroupJLU/LMM_ISEval"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities",
        "authors": "Zhijie Sang, Kejing Yang, Qi Zhou, Su Lu, Shuo Cai",
        "link": "https://arxiv.org/abs/2508.05496",
        "github_repo": null,
        "summary": "- InfiAlign is a novel post-training framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) in a scalable and sample-efficient manner.\n- It integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) and a robust data selection pipeline that automatically curates high-quality alignment data using multi-dimensional quality metrics.\n- When evaluated on the Qwen-2.5-Math-7B-Base model, InfiAlign achieves performance on par with DeepSeek-R1-Distill-Qwen-7B while using only approximately 12% of the training data.\n- Further improvements are achieved through DPO, resulting in an average improvement of 3.89% on AIME 24/25 benchmarks.\n- The results highlight the effectiveness of combining principled data selection with a multi-stage post-training approach for aligning LLMs to enhance reasoning capabilities.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [
            "https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT"
        ],
        "date": "2025-08-08"
    },
    {
        "title": "Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation",
        "authors": "Feng Chen, Lifan Guo, Junhui Li, Huaixia Dou, Jie Zhu",
        "link": "https://arxiv.org/abs/2508.04423",
        "github_repo": "https://github.com/aliyun/qwen-dianjin",
        "summary": "This paper introduces the task of Customer Support Conversation (CSC) and proposes a structured framework grounded in COPC guidelines, defining five conversational stages and twelve strategies.  A new evaluation dataset, CSConv (1,855 real-world conversations), and a training dataset, RoleCS (synthetic data using LLMs), are constructed. Experiments demonstrate that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses. Human evaluations further confirm gains in problem resolution.  The contribution of this work lies in the novel framework and datasets for a previously underexplored task in NLP.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/aliyun/qwen-dianjin"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning\n  Models",
        "authors": "Fangzhou Yao, Weibo Gao, Yizhi Wang, Yichao Du, Linan Yue",
        "link": "https://arxiv.org/abs/2508.02120",
        "github_repo": null,
        "summary": " - This paper surveys efficient reasoning methods for R1-style Large Reasoning Models (LRMs), categorizing existing works into single-model optimization and model collaboration. \n-  The authors propose a novel taxonomy to organize existing efficient reasoning methods for R1-style LRMs.\n-  The survey covers various efficient reasoning techniques including early exit, CoT compression, adaptive reasoning, representation engineering, and model collaboration strategies. \n-  The paper also discusses future research directions such as multimodal reasoning, tool-integrated reasoning, multi-agent systems and truthful and efficient reasoning.\n- A public GitHub repository is maintained to track the latest progress in efficient reasoning methods.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/yuelinan/Awesome-Efficient-R1-style-LRMS"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "MOSEv2: A More Challenging Dataset for Video Object Segmentation in\n  Complex Scenes",
        "authors": "Xudong Jiang, Shuting He, Chang Liu, Kaining Ying, Henghui Ding",
        "link": "https://arxiv.org/abs/2508.05630",
        "github_repo": null,
        "summary": " - MOSEv2 is a new, more challenging dataset for video object segmentation that significantly surpasses existing datasets in complexity.\n - It features 5,024 videos with over 701,976 high-quality masks for 10,074 objects across 200 categories.\n - MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, adverse weather, low-light scenes, and multi-shot sequences.\n - Benchmarking 20 representative video object segmentation methods on MOSEv2 reveals consistent performance drops, highlighting the challenges posed by real-world conditions.\n - The dataset is publicly available at https://MOSE.video.",
        "classification": [
            "Computer Vision",
            "Image Segmentation",
            "Video Classification",
            "Mask Generation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "CoAct-1: Computer-using Agents with Coding as Actions",
        "authors": "Taiwei Shi, Jieyu Zhang, Viraj Prabhu, Yutong Dai, Linxin Song",
        "link": "https://arxiv.org/abs/2508.03923",
        "github_repo": null,
        "summary": "- CoAct-1 is a novel multi-agent system that uses coding as an enhanced action for computer-using agents, combining GUI-based control with direct programmatic execution.\n- It features an Orchestrator that dynamically delegates subtasks to a GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts.\n- This hybrid approach significantly outperforms prior methods on the OSWorld benchmark, achieving a new state-of-the-art success rate of 60.76% and reducing the average number of steps required to complete a task to 10.15.\n- The system's efficiency is attributed to its ability to strategically bypass inefficient GUI action sequences by using code for tasks like file management and data processing.\n- The results demonstrate the effectiveness and scalability of integrating coding as a core action for generalized computer automation.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/salesforce/CoAct-1"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "Marco-Voice Technical Report",
        "authors": "Qingjuan Li, Haoqin Sun, Xuanfan Ni, Chenyang Lyu, Fengping Tian",
        "link": "https://arxiv.org/abs/2508.02038",
        "github_repo": "https://github.com/AIDC-AI/Marco-Voice",
        "summary": "- This paper introduces Marco-Voice, a novel multifunctional speech synthesis system that integrates voice cloning and emotion control within a unified framework.\n- The model architecture employs a speaker-emotion disentanglement mechanism with in-batch contrastive learning and a rotational emotion embedding integration method for smooth emotion control.\n- Marco-Voice achieves substantial improvements in both objective and subjective metrics compared to existing systems, showcasing competitive performance in terms of speech clarity and emotional richness.\n- The authors contribute CSEMOTIONS, a high-quality Mandarin emotional speech dataset with 10 hours of speech from six professional speakers across seven emotional categories, supporting comprehensive training and evaluation.\n- The code and dataset are publicly available, furthering research and development in the field of expressive neural speech synthesis.",
        "classification": [
            "Text-to-Speech"
        ],
        "github_urls": [
            "https://github.com/AIDC-AI/Marco-Voice"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS"
        ],
        "date": "2025-08-08"
    },
    {
        "title": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance",
        "authors": "Xiaobin Hu, Han Feng, Chengming Xu, Moran Li, Na Zhang",
        "link": "https://arxiv.org/abs/2508.01650",
        "github_repo": "https://github.com/fighting-Zhang/StrandDesigner",
        "summary": "- This paper introduces StrandDesigner, the first sketch-based hair strand generation model, addressing the limitations of text or image-based methods.\n- The model architecture uses a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads.\n- Experiments on benchmark datasets show that StrandDesigner outperforms existing approaches in realism and precision.\n- The model addresses challenges such as modeling complex strand interactions and diverse sketch patterns.\n- Code will be released at GitHub.",
        "classification": [
            "Image-to-3D"
        ],
        "github_urls": [
            "https://github.com/fighting-Zhang/StrandDesigner"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during\n  Multi-Hop Analysis",
        "authors": "Reshmi Ghosh, Yashwanth Babu, Srujana Pillarichety, Isha Nalawade, Anushka Yadav",
        "link": "https://arxiv.org/abs/2508.04699",
        "github_repo": null,
        "summary": "- This paper introduces a novel diagnostic framework for analyzing reasoning failures in multi-hop question answering models.\n- The framework categorizes errors across three dimensions: hops, coverage, and overthinking, providing a nuanced understanding of model limitations.\n- The authors manually annotated model traces from six language models across three datasets, revealing common reasoning issues such as incomplete reasoning and unnecessary steps.\n- An LLM-as-a-Judge framework was developed to scale the analysis, demonstrating strong agreement with human annotations.\n- Findings highlight the prevalence of overthinking and its negative impact on answer accuracy, especially in complex datasets.",
        "classification": [
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction",
        "authors": "Prajit Das, Lavanya Elluri, Aritran Piplai, Anantaa Kotal, Leon Garza",
        "link": "https://arxiv.org/abs/2508.05545",
        "github_repo": null,
        "summary": "- This paper introduces PRvL, an open-source suite of fine-tuned models and evaluation tools for general-purpose PII redaction, built on open-source LLMs and supporting multiple inference settings.\n- PRvL addresses the limitations of rule-based and NER-based methods by leveraging the contextual understanding capabilities of LLMs for accurate and privacy-aware PII redaction across domains and languages.\n- The research evaluates a range of LLM architectures and training strategies, including fine-tuning, instruction-tuning, and retrieval-augmented generation, measuring redaction performance, semantic preservation, and PII leakage.\n- The empirical results provide practical guidance for configuring LLM-based redactors and demonstrate the effectiveness of instruction-tuning for high-accuracy, generalizable PII redaction.\n- The open-source nature of PRvL, along with its flexibility and compliance features, enables data owners to perform redactions within their own secure environments without relying on third-party services.",
        "classification": [
            "Natural Language Processing",
            "Token Classification",
            "Text2Text Generation"
        ],
        "github_urls": [
            "https://anonymous.4open.science/r/PRvL-C1BF"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/ai4privacy/pii-masking-300k"
        ],
        "date": "2025-08-08"
    },
    {
        "title": "REINA: Regularized Entropy Information-Based Loss for Efficient\n  Simultaneous Speech Translation",
        "authors": "Xiao Yu, Mahesh Kumar Nandwana, Joseph Liu, Nameer Hirschkind",
        "link": "https://arxiv.org/abs/2508.04946",
        "github_repo": null,
        "summary": "- This paper introduces REINA, a novel loss function for training efficient simultaneous speech translation (SimulST) models by adapting existing non-streaming translation models.\n- REINA is derived from information theory principles and aims to optimize the trade-off between translation quality and latency by only waiting for more input if it yields information gain.\n- The proposed approach achieves state-of-the-art (SOTA) streaming results on various language pairs, outperforming existing methods based on the proposed normalized streaming efficiency metric.\n- REINA is trained on open-source and synthetic data, demonstrating its effectiveness without reliance on proprietary datasets.\n- The model architecture includes an acoustic encoder, a text decoder, and a policy network using a transformer architecture.  The policy network makes READ/WRITE decisions to balance quality and latency.",
        "classification": [
            "Automatic Speech Recognition",
            "Translation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating\n  Linguistic Shibboleth Detection in LLM Hiring Evaluations",
        "authors": "Chirag Shah, Aman Chadha, Tanya Roosta, Julia Kharchenko",
        "link": "https://arxiv.org/abs/2508.04939",
        "github_repo": null,
        "summary": " - This paper introduces a new benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths, which are subtle linguistic markers that can reveal demographic attributes.\n - The benchmark uses 100 validated question-response pairs in simulated interview scenarios to measure how LLMs systematically penalize certain linguistic patterns.\n -  The paper demonstrates that hedged responses receive significantly lower ratings than confident responses despite equivalent content quality.\n - The benchmark's effectiveness is validated by identifying model-specific biases in various LLMs.\n - The proposed framework provides a foundation for detecting and measuring linguistic discrimination in AI systems.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation",
        "authors": "Jian Yang, Yixuan Ding, Tianfang Zhang, Yimian Dai, fengyiwu",
        "link": "https://arxiv.org/abs/2508.04190",
        "github_repo": null,
        "summary": "*- The paper introduces RPCANet++, a novel deep unfolding network for sparse object segmentation that integrates the interpretability of Robust Principal Component Analysis (RPCA) with the efficiency of deep learning architectures.\n- The model architecture comprises three modules: a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM).  The BAM includes a Memory-Augmented Module (MAM) to mitigate inter-stage transmission loss, and the OEM incorporates a Deep Contrast Prior Module (DCPM) to expedite object extraction.\n- Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance in various segmentation tasks, including infrared small target detection, vessel segmentation, and defect detection.\n- The interpretability of the model is enhanced through visual and numerical measurements of low-rankness and sparsity across different stages of the network.\n- The results show that RPCANet++ outperforms existing methods on multiple benchmarks, demonstrating its effectiveness and generalizability for sparse object segmentation tasks.",
        "classification": [
            "Image Segmentation"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-08-08"
    },
    {
        "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking",
        "authors": "Chao Wang, Tong Ruan, Kaiwen Li, Junwen Li, Ziyan Liu",
        "link": "https://arxiv.org/abs/2508.02243",
        "github_repo": "https://github.com/ziyan-xiaoyu/I2CR/",
        "summary": "- This paper introduces I2CR, a novel framework for multimodal entity linking that prioritizes text information and uses visual clues iteratively when necessary.\n- The I2CR framework employs a multi-round iterative strategy, integrating key visual features from various aspects of the image to support reasoning and enhance accuracy.\n- Experiments on three widely used public datasets demonstrate that I2CR outperforms existing state-of-the-art methods, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively.\n- The framework addresses the challenges of unnecessary image data incorporation and reliance on one-time visual feature extraction present in previous methods.\n- The core of the framework is a fine-tuned Large Language Model that makes initial entity selections, and then employs intra- and inter-modal consistency checks and visual feedback loops before returning a final result.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/ziyan-xiaoyu/I2CR/"
        ],
        "huggingface_urls": [],
        "date": "2025-08-08"
    }
]