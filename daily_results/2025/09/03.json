[
    {
        "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
        "authors": "Hejia Geng, Guibin Zhang, henggg, Artemis0430, JeremyYin",
        "link": "https://arxiv.org/abs/2509.02547",
        "github_repo": null,
        "summary": "This survey paper examines the paradigm shift from conventional reinforcement learning for LLMs (LLM RL) to agentic reinforcement learning (Agentic RL).  Agentic RL treats LLMs as autonomous agents embedded in dynamic environments instead of passive text generators. The study establishes a comprehensive taxonomy of Agentic RL, categorizing its key components (planning, tool use, memory, etc.) and its applications. A key finding is that RL is the critical mechanism for transforming static heuristic modules into adaptable and robust behavior. The paper also provides a practical compendium of open-source environments, benchmarks, and frameworks to support future research.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning",
        "authors": "Haoyang Zou, zhwang4ai, JoeYing, jzfeng, MingComplex",
        "link": "https://arxiv.org/abs/2509.02544",
        "github_repo": null,
        "summary": " - UI-TARS-2 is a novel GUI agent model that uses a unified framework for advanced GUI interaction, integrating a data flywheel, a stabilized multi-turn RL framework, a hybrid GUI environment, and a unified sandbox platform.\n - The model significantly improves on its predecessor, UI-TARS-1.5, achieving state-of-the-art results on several GUI benchmarks such as Online-Mind2Web (88.2), OSWorld (47.5), WindowsAgentArena (50.6), and AndroidWorld (73.3), and outperforming strong baselines like Claude and OpenAI agents.\n - UI-TARS-2's training methodology includes a data flywheel that continually refines model and data quality through iterative cycles of continual pre-training, supervised fine-tuning, and reinforcement learning.\n - A hybrid GUI environment is utilized that expands beyond GUI-only operations by integrating file systems and terminals to broaden the scope of tasks the model can solve, including those in long-horizon information-seeking and software engineering domains.\n - The model demonstrates strong generalization to long-horizon tasks and robustness across diverse agent tasks, highlighting its potential to advance the state of GUI agents and its applicability to real-world interactive scenarios.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/bytedance/ui-tars",
            "https://github.com/bytedance/UI-TARS-desktop"
        ],
        "huggingface_urls": [
            "https://seed-tars.com/showcase/ui-tars-2"
        ],
        "date": "2025-09-03"
    },
    {
        "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning",
        "authors": "Qian Liu, Longtao Zheng, Zhenghai Xue, xszheng2020, R1ch0rd",
        "link": "https://arxiv.org/abs/2509.02479",
        "github_repo": null,
        "summary": "- SimpleTIR is a novel plug-and-play algorithm designed to stabilize multi-turn tool-integrated reasoning (TIR) training by filtering out trajectories with void turns, which significantly improves the learning dynamics.\n- It addresses the challenge of distributional drift from external tool feedback, preventing the accumulation of low-probability tokens that cause catastrophic gradient explosions.\n- SimpleTIR achieves state-of-the-art performance on challenging mathematical reasoning benchmarks, outperforming existing methods notably on the AIME24 score.\n- Extensive experiments demonstrate the effectiveness of SimpleTIR in stabilizing multi-turn TIR training and achieving superior performance.\n- By avoiding supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long\n  Video Understanding",
        "authors": "Xuanyu Zheng, Ruohui Wang, Mercury7353, datamonkey, HLSv",
        "link": "https://arxiv.org/abs/2508.21496",
        "github_repo": null,
        "summary": "- The paper introduces ELV-Halluc, a benchmark dataset for evaluating semantic aggregation hallucinations in long videos.\n- ELV-Halluc focuses on Semantic Aggregation Hallucinations (SAH), where models produce outputs with correct frame-level semantics but incorrect event-level aggregation.\n- The dataset comprises 8K adversarial data pairs created by modifying ground truth captions with hallucinated content, evaluating models' ability to distinguish correct and incorrect information.\n- Experiments on 14 open-source and 2 closed-source models show that SAH increases with semantic complexity, and that positional encoding strategies help mitigate SAH.\n- The paper demonstrates improvements in both ELV-Halluc and Video-MME, with a 27.7% reduction in SAH ratio using positional encoding and DPO strategies.",
        "classification": [
            "Video-Text-to-Text"
        ],
        "github_urls": [
            "https://github.com/hlsv02/ELV-Halluc"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model",
        "authors": "Jianwei Yang, Chunyuan Li, Benjamin-eecs, drogozhang, russwang",
        "link": "https://arxiv.org/abs/2509.00676",
        "github_repo": null,
        "summary": " - This paper introduces LLaVA-Critic-R1, a multimodal model trained via reinforcement learning on a critic dataset. Unlike typical critic models that only score responses, LLaVA-Critic-R1 directly produces responses, achieving competitive performance against specialized reasoning models.\n - LLaVA-Critic-R1 outperforms existing methods on 26 visual reasoning and understanding benchmarks by an average of +5.7%, achieving state-of-the-art results on some benchmarks.\n -  The improved critic capabilities enhance inference: test-time self-critique boosts performance by an average of +13.8% on five reasoning tasks without additional training.\n - The model's enhanced critic capability comes from RL training and two factors: improved visual perception, and structured reasoning through a \"think-then-answer\" generation pattern.\n - The study validates the effectiveness of RL critic training on other strong reasoning models and shows that the dual ability benefits test-time scaling.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/LLaVA-VL/LLaVA-NeXT/LLaVA-Critic-R1"
        ],
        "huggingface_urls": [
            "https://huggingface.co/collections/lmms-lab/llava-critic-r1"
        ],
        "date": "2025-09-03"
    },
    {
        "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models\n  for Document Conversion",
        "authors": "Haicheng Wang, Le Tian, Zhongyin Zhao, YxxxB, YuanLiuuuuuu",
        "link": "https://arxiv.org/abs/2509.01215",
        "github_repo": "https://github.com/Tencent/POINTS-Reader",
        "summary": " - The paper introduces POINTS-Reader, a novel distillation-free framework for document conversion.  \n - POINTS-Reader consists of two stages: a uniform format warm-up stage that generates large-scale synthetic data, and an iterative self-improvement stage that uses the model to generate and refine real-world data. \n - The model achieves state-of-the-art performance on multiple benchmarks, exceeding even some larger models without knowledge distillation. \n - The framework addresses the limitations of existing distillation-based approaches, which often lack accuracy and may inherit biases from teacher models. \n - The proposed self-improvement approach is fully automated, reducing reliance on labor-intensive manual annotations.",
        "classification": [
            "Image-Text-to-Text",
            "Document Question Answering",
            "Table Question Answering"
        ],
        "github_urls": [
            "https://github.com/Tencent/POINTS-Reader"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
        "authors": "Zhiheng Lyu, Zhuofeng Li, Yi Lu, JasperHaozhe, DongfuJiang",
        "link": "https://arxiv.org/abs/2509.01055",
        "github_repo": "https://github.com/TIGER-AI-Lab/verl-tool",
        "summary": " - VERLTOOL is a modular and efficient framework for Agentic Reinforcement Learning with Tool Use (ARLT). \n - It addresses the limitations of existing ARLT approaches, such as fragmentation, synchronous execution bottlenecks, and limited extensibility. \n - VERLTOOL provides a unified tool management system via standardized APIs, supports diverse modalities, and enables asynchronous rollout execution. \n - Experiments on six ARLT domains demonstrate competitive performance compared to specialized systems. \n - The modular plugin architecture allows for rapid tool integration with only lightweight Python definitions.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/TIGER-AI-Lab/verl-tool"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Baichuan-M2: Scaling Medical Capability with Large Verifier System",
        "authors": "Jayok6, yuanshuai, sdujq, anselcmy, fairyang",
        "link": "https://arxiv.org/abs/2509.02208",
        "github_repo": null,
        "summary": " - Baichuan-M2 is a 32B parameter medical augmented reasoning model that outperforms other open-source and many closed-source models on HealthBench, achieving a score above 32 on the challenging HealthBench Hard benchmark.\n - The model is trained using a novel dynamic verification framework that simulates real-world clinical scenarios with a Patient Simulator and a Clinical Rubrics Generator, moving beyond static answer verification.\n - The framework incorporates multi-stage reinforcement learning with an improved Group Relative Policy Optimization algorithm, enhancing various capabilities such as medical knowledge, reasoning, and patient interaction.\n - The model achieves a new Pareto front in the performance-parameter trade-off for medical AI deployment, making it more feasible in resource-constrained healthcare settings.\n - The results underscore the critical role of a robust validation system in integrating model capabilities with practical applications.",
        "classification": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Kwai Keye-VL 1.5 Technical Report",
        "authors": "SXxtyz, Chengru, bhsc24, dingboyang, biaoYang",
        "link": "https://arxiv.org/abs/2509.01563",
        "github_repo": "https://github.com/Kwai-Keye/Keye",
        "summary": " -  This paper introduces Keye-VL-1.5, an 8-billion parameter multimodal foundation model that improves video understanding capabilities. \n- The model architecture uses a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity. \n- Keye-VL-1.5 employs a four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens. \n-  The model also incorporates a post-training pipeline focusing on reasoning enhancement and human preference alignment. \n- Keye-VL-1.5 demonstrates significant improvements over existing models across various benchmarks, particularly excelling in video understanding tasks.",
        "classification": [
            "Video-Text-to-Text",
            "Multimodal",
            "Visual Question Answering"
        ],
        "github_urls": [
            "https://github.com/Kwai-Keye/Keye"
        ],
        "huggingface_urls": [
            "https://huggingface.co/Kwai-Keye"
        ],
        "date": "2025-09-03"
    },
    {
        "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR",
        "authors": "Lu Wang, Yukun Chen, Ze Gong, Longze Chen, Geaming",
        "link": "https://arxiv.org/abs/2509.02522",
        "github_repo": "https://github.com/ritzz-ai/PACS",
        "summary": "- The paper introduces PACS, a novel reinforcement learning framework for RLVR that achieves implicit Actor-Critic coupling through a supervised learning approach.\n- PACS reformulates the RLVR problem as a supervised learning task, using the outcome reward as a predictable label and training a score function parameterized by the policy model.\n- A detailed gradient analysis demonstrates that this formulation implicitly couples actor and critic roles, leading to more stable and efficient training.\n- Experiments on challenging mathematical reasoning tasks show that PACS outperforms strong RLVR baselines like PPO and GRPO, achieving significant improvements in reasoning performance.\n- The code and data for PACS are available as open source.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/ritzz-ai/PACS"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "DCPO: Dynamic Clipping Policy Optimization",
        "authors": "Kai Lu, Chengfeng Dou, sdujq, GuoPD, yangshui",
        "link": "https://arxiv.org/abs/2509.02333",
        "github_repo": null,
        "summary": "- This paper introduces Dynamic Clipping Policy Optimization (DCPO), a novel approach for reinforcement learning from verifiable rewards (RLVR) that addresses the limitations of existing methods such as GRPO and DAPO.\n- DCPO introduces a dynamic clipping strategy that adaptively adjusts clipping bounds based on token-specific prior probabilities and a smooth advantage standardization technique that standardizes rewards across cumulative training steps.\n- Experiments on four benchmarks using four different models show that DCPO achieves state-of-the-art performance, surpassing both DAPO and GRPO in most cases.\n- DCPO significantly reduces token clipping ratio and increases the average nonzero advantage compared to GRPO and DAPO, demonstrating improved efficiency and data utilization.\n- The results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://github.com/lime-RL/DCPO"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task\n  Arithmetic",
        "authors": "Bernard Ghanem, Mohammad Zbeeb, hammh0a",
        "link": "https://arxiv.org/abs/2509.01363",
        "github_repo": null,
        "summary": "- This paper introduces the concept of reasoning vectors, which are compact representations of reasoning capabilities learned by large language models (LLMs).\n- Reasoning vectors are extracted by comparing two identically initialized LLMs: one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO).\n- The reasoning vector is then added to compatible instruction-tuned models using simple arithmetic to enhance their reasoning capabilities, which is demonstrated on various reasoning benchmarks, showing consistent improvements.\n- This approach allows for transferring reasoning abilities between models without requiring additional training, making it a cost-effective method for enhancing LLMs.\n- The paper also demonstrates the robustness and generalizability of the reasoning vector through several experiments and ablation studies.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
        "authors": "Lingen Li, Guangzhi Wang, Xiaodong Cun, Xiaoyu521, Ysz2022",
        "link": "https://arxiv.org/abs/2509.02460",
        "github_repo": null,
        "summary": "- The paper introduces GenCompositor, a novel generative video compositing method that uses a Diffusion Transformer (DiT) to seamlessly integrate dynamic foreground elements into background videos.\n- GenCompositor consists of three main components: a lightweight DiT-based background preservation branch, a DiT fusion block with full self-attention, and a novel position embedding method called Extended Rotary Position Embedding (EROPE).\n- The model is trained on a new dataset called VideoComp, containing 61K video sets with high-quality target videos and complete dynamic elements.\n- Experiments demonstrate that GenCompositor outperforms existing solutions in terms of fidelity and consistency, achieving state-of-the-art results on video harmonization and trajectory-controlled generation tasks.\n- The authors also showcase the model's generalizability by applying it to video inpainting and object removal tasks.",
        "classification": [
            "Image-to-Video",
            "Video Classification",
            "Text-to-Video",
            "Multimodal"
        ],
        "github_urls": [
            "https://gencompositor.github.io/"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations",
        "authors": "Tianlu, jcklcn, spermwhale, danyaljj, dogtooth",
        "link": "https://arxiv.org/abs/2509.02534",
        "github_repo": null,
        "summary": "This paper introduces DARLING, a novel framework that jointly optimizes for response quality and semantic diversity in language model generations.  DARLING uses a learned partition function to measure semantic diversity and combines this signal with a quality reward during online reinforcement learning. Experiments show that DARLING consistently outperforms existing quality-only RL baselines across multiple model families and sizes on both non-verifiable (instruction following, creative writing) and verifiable (competition math) tasks.  Most strikingly, optimizing for diversity improves exploration, which leads to higher-quality responses. The code is available on GitHub.",
        "classification": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/facebookresearch/darling"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for\n  Multimodal Learning",
        "authors": "Zirui Wang, Letian Zhang, Xianhang Li, Yanqing Liu, cihangxie",
        "link": "https://arxiv.org/abs/2509.01644",
        "github_repo": null,
        "summary": "- This paper introduces OpenVision 2, a simplified version of OpenVision that uses only a captioning loss for training, removing the text encoder and contrastive loss.\n- OpenVision 2 achieves competitive performance with the original OpenVision model while significantly reducing training time and memory consumption (e.g., 1.5x faster training time and 1.8x less memory usage with ViT-L/14).\n- The model's superior training efficiency allows for scaling beyond the largest vision encoders previously used in OpenVision, reaching over 1 billion parameters.\n- OpenVision 2's performance is evaluated on a range of multimodal benchmarks, consistently matching or exceeding the performance of the original OpenVision and other CLIP variants.\n- The results demonstrate that a purely generative, caption-only training objective can rival contrastive methods in multimodal performance while substantially lowering computational and memory requirements.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [
            "https://github.com/UCSC-VLAA/OpenVision"
        ],
        "huggingface_urls": [
            "https://huggingface.co/UCSC-VLAA"
        ],
        "date": "2025-09-03"
    },
    {
        "title": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via\n  Self-Supervision",
        "authors": "Yan-Jie Zhou, Heng Guo, Chengyu Fang, Zheng Jiang, Che Liu",
        "link": "https://arxiv.org/abs/2509.01360",
        "github_repo": null,
        "summary": "- This paper introduces M\u00b3Ret, a unified visual encoder for zero-shot multimodal medical image retrieval, trained on a large-scale hybrid-modality dataset comprising 867,653 medical images.\n- M\u00b3Ret uses both generative (MAE) and contrastive (SimDINO) self-supervised learning paradigms, achieving state-of-the-art results in zero-shot image-to-image retrieval across various modalities (X-rays, ultrasound, endoscopy, and CT scans).\n- The model demonstrates strong cross-modal alignment without paired data and generalizes to unseen MRI tasks, showcasing the effectiveness of purely visual self-supervision.\n- Comprehensive analyses validate the scalability of M\u00b3Ret across different model and data sizes, demonstrating its robustness and potential as a foundation model for visual self-supervised learning in multimodal medical image understanding.\n- The paper highlights three key contributions: unified training across diverse medical modalities without modality-specific modifications, superior zero-shot image retrieval performance across various datasets, and analysis validating that performance gains are primarily driven by scaling data, model capacity, and compute.",
        "classification": [
            "Image-to-Image",
            "Image Feature Extraction",
            "Zero-Shot Image Classification",
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm\n  Simulators for Conditional Synthetic Data Generation",
        "authors": "Xiaolei Huang, Weisi Liu, kwangju",
        "link": "https://arxiv.org/abs/2509.02040",
        "github_repo": null,
        "summary": "- This paper introduces Genetic Prompt, a novel framework that uses genetic algorithms with LLMs to generate high-quality and diverse synthetic data for NLP tasks.\n- The framework treats semantic text attributes as gene sequences and leverages LLMs to simulate crossover and mutation operations, enhancing data quality and diversity.\n- Experiments show that Genetic Prompt significantly outperforms state-of-the-art baselines on multiple NLP tasks, demonstrating robustness and scalability across various generator model sizes.\n- The approach also integrates an active learning scheme to optimize parent selection and expand the offspring search space.\n- The results validate that Genetic Prompt is an effective method for producing high-quality synthetic data for NLP applications.",
        "classification": [
            "Natural Language Processing",
            "Text Generation"
        ],
        "github_urls": [
            "https://github.com/trust-nlp/Genetic-Prompt"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Benchmarking Optimizers for Large Language Model Pretraining",
        "authors": "mjaggi, MatPag, Andron00e",
        "link": "https://arxiv.org/abs/2509.01440",
        "github_repo": null,
        "summary": "This paper presents a comprehensive benchmarking of 11 optimization methods across various LLM pretraining scenarios.  The study systematically varies model size, batch size, and training duration to provide guidance on which optimizer is best suited for each scenario.  A key finding is that AdEMAMix and MARS consistently outperform other optimizers across different scales and training lengths, challenging the long-standing dominance of AdamW.  Furthermore, the authors open-source their full benchmarking toolkit, including training scripts and evaluation pipelines, enabling fully reproducible research. Finally, the paper provides a set of best practices for LLM pretraining.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/epfml/llm-optimizer-benchmark"
        ],
        "huggingface_urls": [
            "https://huggingface.co/datasets/HuggingFaceFW/fineweb"
        ],
        "date": "2025-09-03"
    },
    {
        "title": "The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in\n  LLMs with Camlang",
        "authors": "Solomon Tsai, Zhujun Jin, Yixuan Liu, Fenghua Liu, yulongchen",
        "link": "https://arxiv.org/abs/2509.00425",
        "github_repo": null,
        "summary": " - This paper introduces Camlang, a novel constructed language designed to evaluate metalinguistic reasoning in LLMs.\n - Camlang includes explicit resources like a grammar book and a bilingual dictionary, enabling a controlled assessment of LLM competence.\n - Experiments reveal a significant performance gap between LLMs and humans on Camlang tasks, highlighting limitations in metalinguistic reasoning.\n - Human verification shows that most LLM successes are from shallow lexical alignment, not systematic grammatical mastery.\n - Camlang establishes a cognitively grounded evaluation paradigm for assessing the fundamental gaps between current LLMs and human capabilities.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Fantastic Pretraining Optimizers and Where to Find Them",
        "authors": "Percy Liang, Tengyu Ma, David Hall, Kaiyue Wen",
        "link": "https://arxiv.org/abs/2509.02046",
        "github_repo": null,
        "summary": " - This paper conducts a systematic study of ten deep learning optimizers for large language model pretraining, addressing previous methodological shortcomings.\n - It finds that fair comparisons require rigorous hyperparameter tuning and end-of-training evaluations across various model scales and data-to-model ratios.\n - The study reveals that the speedup of many optimizers over well-tuned AdamW baselines is lower than previously claimed and diminishes with model size.\n - Matrix-based optimizers consistently outperform scalar-based optimizers, although the speedup decreases with model scale.\n - The optimal choice of optimizer depends on data-to-model ratios, highlighting the complexity of optimizer selection for large language model training.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/marin-community/marin/tree/kaiyue/optimizers"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Universal Deep Research: Bring Your Own Model and Strategy",
        "authors": "Pavlo Molchanov, Peter Belcak",
        "link": "https://arxiv.org/abs/2509.00244",
        "github_repo": null,
        "summary": "- The paper introduces Universal Deep Research (UDR), a generalist agentic system that allows users to create custom deep research strategies using any language model without requiring additional training or fine-tuning.\n- UDR addresses limitations of existing deep research tools by enabling users to define resource hierarchies, automate cross-validation, and manage search expenses.\n- The system converts user-defined research strategies into executable code, ensuring transparent and deterministic behavior. \n- UDR employs language models for localized tasks like summarization and ranking, while maintaining control logic in generated code for efficiency and reliability.\n- The research demonstrates UDR's flexibility and capability through various examples, highlighting its potential to automate complex research workflows.",
        "classification": [
            "Natural Language Processing",
            "Question Answering"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games",
        "authors": "Dongmin Park, Jaehyeon Son, Heeseung Yun, Junseo Kim, ahnpersie",
        "link": "https://arxiv.org/abs/2509.01052",
        "github_repo": null,
        "summary": " - This paper introduces FlashAdventure, a benchmark of 34 Flash-based adventure games designed to evaluate GUI agents' ability to solve full story arcs.\n - It proposes CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework that leverages long-term clue memory to improve planning and problem-solving in adventure games.\n - Experiments show that current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap.\n - The benchmark includes diverse subgenres (mystery/detective, hidden object, room escape, visual novel, simulation) to evaluate agents' generalizability.\n -  The results highlight a significant performance gap between humans and state-of-the-art GUI agents, suggesting opportunities for future research.",
        "classification": [
            "Multimodal",
            "Reinforcement Learning"
        ],
        "github_urls": [
            "https://ahnjaewoo.github.io/flashadventure"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing",
        "authors": "Amin Heyrani Nobar, Ngan Hoai Nguyen, Ligong Han, Xiaoxiao He, quandao10",
        "link": "https://arxiv.org/abs/2509.01984",
        "github_repo": null,
        "summary": "- This paper introduces VARIN, a novel noise inversion-based editing technique for Visual Autoregressive (VAR) models that enables prompt-guided image editing without retraining.\n- VARIN leverages a pseudo-inverse function called Location-aware Argmax Inversion (LAI) to generate inverse Gumbel noises, which facilitates precise image reconstruction and controllable edits.\n- Experiments demonstrate that VARIN effectively modifies source images according to textual prompts while preserving original background and structural details, outperforming baseline methods like Regeneration.\n- VARIN achieves comparable editing quality to more complex optimization-based approaches, while offering significantly faster inference times (approximately 1 second per image).\n- The method addresses the limitations of previous autoregressive editing techniques that struggled with precise control over edits and preservation of original image details.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "MobiAgent: A Systematic Framework for Customizable Mobile Agents",
        "authors": "Wangbo Gong, Yisheng Zhao, Xi Zhao, fengerhu, sjtuzc",
        "link": "https://arxiv.org/abs/2509.00531",
        "github_repo": null,
        "summary": "- The paper introduces MobiAgent, a comprehensive mobile agent system for customizable mobile agents, addressing accuracy and efficiency challenges in real-world tasks.\n- MobiAgent consists of three core components: MobiMind-series agent models (Planner, Decider, and Grounder), the AgentRR acceleration framework, and the MobiFlow benchmarking suite.\n- It achieves state-of-the-art performance compared to general-purpose LLMs and specialized GUI agent models in real-world mobile scenarios, demonstrated by experimental results on the MobiFlow benchmark.\n- The AgentRR framework significantly improves efficiency and accuracy through a record-replay mechanism leveraging multi-level experiences and a latent memory model.\n- MobiFlow, a novel benchmarking framework based on Directed Acyclic Graphs (DAGs), provides more accurate and fine-grained evaluations of agent performance in real-world scenarios.",
        "classification": [
            "Reinforcement Learning"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?",
        "authors": "Xiaofeng Yang, Yuheng Li, wy20030128, yuxianglai117, mcl0222",
        "link": "https://arxiv.org/abs/2509.02379",
        "github_repo": "https://github.com/ricklisz/MedDINOv3",
        "summary": "- MedDINOv3 is a novel framework that adapts the DINOv3 foundation model for medical image segmentation.  It uses a refined ViT architecture with multi-scale token aggregation to improve performance.\n- The model undergoes domain-adaptive pre-training on a large-scale CT dataset (CT-3M) using a three-stage process (global/local self-distillation, gram anchoring, high-resolution adaptation).\n- MedDINOv3 achieves state-of-the-art performance across four public CT/MRI segmentation benchmarks (AMOS22, BTCV, KiTS23, LiTS), surpassing or matching strong CNN and transformer baselines.\n- The improvements are particularly notable in organ-at-risk segmentation, exceeding the performance of nnU-Net.\n- The code for MedDINOv3 is publicly available on GitHub, facilitating further research and development in the field.",
        "classification": [
            "Image Segmentation"
        ],
        "github_urls": [
            "https://github.com/ricklisz/MedDINOv3"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with\n  Knowledge Augmentation for Robust Constitutional Alignment of Language Models",
        "authors": "Rahul Karthikeyan, Shivam Dubey, Aryan Kasat, Snehasis Mukhopadhyay, amanchadha",
        "link": "https://arxiv.org/abs/2509.02133",
        "github_repo": null,
        "summary": "This paper introduces AMBEDKAR, a novel framework for mitigating bias in Large Language Models (LLMs).  AMBEDKAR employs a two-stage approach, using a smaller, potentially biased model to generate text and a larger, constitutionally-aligned verifier model to evaluate fairness.  It uses a speculative decoding algorithm that prioritizes outputs aligned with fairness principles and reduces casteist and communal bias by up to 26.41%. This framework operates at inference time and reduces the computational overhead compared to retraining. The results demonstrate improved fairness without significantly sacrificing text quality.",
        "classification": [
            "Natural Language Processing",
            "Text Generation",
            "Text Classification"
        ],
        "github_urls": [
            "https://anonymous.4open.science/r/AMBEDKAR-983B/"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Improving Large Vision and Language Models by Learning from a Panel of\n  Peers",
        "authors": "Simon Jenni, Jing Shi, Jefferson Hernandez, kushalkafle, vicenteor",
        "link": "https://arxiv.org/abs/2509.01610",
        "github_repo": null,
        "summary": "- This paper introduces a novel Panel-of-Peers (PoP) learning framework for improving Large Vision and Language Models (LVLMs).\n- The PoP framework leverages a panel of LVLMs that iteratively evaluate and learn from each other's outputs through a simulated peer-review system.\n- Experiments demonstrate that PoP enhances model performance across multiple benchmarks without extensive human-labeled data, increasing average scores on fifteen benchmarks from 48% to 57%.\n- The methodology is shown to enable knowledge transfer between models with different capabilities, such as those with and without OCR abilities.\n- Ablation studies analyze the impact of various design choices on PoP's performance.",
        "classification": [
            "Multimodal"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association",
        "authors": "Daniel Cremers, Xi Wang, Shenhan Qian, zhangganlin",
        "link": "https://arxiv.org/abs/2509.01584",
        "github_repo": "https://github.com/zhangganlin/vista-slam",
        "summary": "- This paper introduces ViSTA-SLAM, a real-time monocular visual SLAM system that operates without requiring camera intrinsics.\n- The core of ViSTA-SLAM is a lightweight symmetric two-view association (STA) model that simultaneously estimates relative camera poses and regresses local pointmaps from two RGB images.\n- ViSTA-SLAM utilizes a Sim(3) pose graph optimization and loop closures to address accumulated drift, achieving superior performance in both camera tracking and dense 3D reconstruction quality compared to other state-of-the-art methods.\n- The STA model in ViSTA-SLAM is significantly smaller than comparable state-of-the-art models (35% of the size).\n- Extensive experiments on challenging datasets demonstrate the effectiveness of ViSTA-SLAM in achieving real-time performance and high-quality results.",
        "classification": [
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/zhangganlin/vista-slam"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Towards More Diverse and Challenging Pre-training for Point Cloud\n  Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
        "authors": "Junchi Yan, Shaofeng Zhang, Xiangdong Zhang",
        "link": "https://arxiv.org/abs/2509.01250",
        "github_repo": "https://github.com/aHapBean/Point-PQAE",
        "summary": "- Point-PQAE, a novel self-supervised cross-reconstruction generative paradigm for point cloud learning, is proposed, which first generates two decoupled point clouds and then reconstructs one from the other.\n- The model architecture consists of a crop mechanism for point cloud view generation, a novel positional encoding to represent the 3D relative position between decoupled views, and a cross-attention mechanism.\n- Point-PQAE outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN.\n- The proposed method achieves state-of-the-art performance on several benchmarks, including object classification, few-shot learning, and part segmentation.\n- Ablation studies demonstrate the effectiveness of the proposed modules and the robustness of the framework.",
        "classification": [
            "Computer Vision"
        ],
        "github_urls": [
            "https://github.com/aHapBean/Point-PQAE"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction",
        "authors": "bindsch, amanchadha, shollercoaster",
        "link": "https://arxiv.org/abs/2509.00581",
        "github_repo": null,
        "summary": "- This paper introduces SQL-of-Thought, a novel multi-agent framework for Natural Language to SQL (NL2SQL) that uses a chain-of-thought prompting approach for query planning and guided error correction.\n- The framework decomposes the NL2SQL task into schema linking, subproblem identification, query planning, SQL generation, and a guided correction loop, using specialized agents for each subtask.\n- Unlike previous methods that rely on execution-based feedback alone, SQL-of-Thought incorporates a taxonomy-guided dynamic error modification mechanism, informed by in-context learning, which leads to state-of-the-art results on the Spider dataset and its variants.\n- The paper demonstrates that the combination of reasoning-based query planning and taxonomy-guided error correction is superior to methods that rely solely on execution feedback.\n- Experimental results show that SQL-of-Thought achieves state-of-the-art execution accuracy (91.59% on Spider, 90.16% on Spider-Realistic) and a high valid SQL generation rate (94%-99%).",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Object Detection",
        "authors": "Vito Ren\u00f3, Abdenour Hadid, Bekhouche, xkruvox, ldb0071",
        "link": "https://arxiv.org/abs/2509.00578",
        "github_repo": null,
        "summary": " - C-DiffDet+ is a novel object detection model that fuses global scene context with generative denoising for high-fidelity object detection, particularly addressing challenges in fine-grained domains. \n- The model architecture consists of an Adaptive Channel Enhancement (ACE) block to improve feature quality, a Global Context Encoder (GCE) to capture environmental information, a Context-Aware Fusion (CAF) module using cross-attention mechanisms to integrate GCE with local features, and an enhanced Multi-Modal Fusion module. \n- Experiments on the CarDD benchmark demonstrate that C-DiffDet+ achieves state-of-the-art performance, outperforming existing methods by a significant margin in various metrics, particularly AP at higher IoU thresholds.\n- The improvement is particularly notable for challenging object categories such as scratches and cracks, where global context is crucial for disambiguation.\n- Ablation studies validate the effectiveness of each component in C-DiffDet+, emphasizing the synergistic impact of global context integration and adaptive feature refinement.",
        "classification": [
            "Object Detection"
        ],
        "github_urls": [],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
        "authors": "Hengjie Cao, wenzi001, ZhouJixian, cnyangyifeng, ChenMengyi",
        "link": "https://arxiv.org/abs/2509.00404",
        "github_repo": "https://github.com/typename-yyf/Metis-quantization",
        "summary": "- This paper introduces Metis, a novel training framework designed to enhance the training of large language models (LLMs) using low-bit quantization.\n- Metis addresses the inherent anisotropy in parameter distributions, a key obstacle in low-bit quantization, by combining spectral decomposition with random embedding to create quantization-friendly narrow distributions.\n- The framework incorporates adaptive learning rates within the spectral domain to amplify underrepresented directions and a dual-range regularizer that jointly constrains numerical precision and parameter range distribution for stable training.\n- Metis demonstrates significant improvements in training stability and accuracy, achieving FP8 performance that surpasses FP32 baselines and FP4 accuracy comparable to FP32.\n- The proposed method is validated on moderately sized models (1B parameters) and shows potential to scale to larger models, paving the way for efficient and scalable LLM training under advanced low-bit quantization.",
        "classification": [
            "Natural Language Processing"
        ],
        "github_urls": [
            "https://github.com/typename-yyf/Metis-quantization"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    },
    {
        "title": "FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable\n  Diffusion Models",
        "authors": "Zhen Wang, Zhuandi He, Shiyue Zhang, Yanwei Lei, zhengchong",
        "link": "https://arxiv.org/abs/2508.20586",
        "github_repo": null,
        "summary": "- This paper introduces FastFit, a novel framework for high-speed multi-reference virtual try-on that utilizes a cacheable diffusion architecture.\n- The model architecture employs a Semi-Attention mechanism and substitutes traditional timestep embeddings with class embeddings to decouple reference feature encoding from the denoising process.\n- This technique allows reference features to be computed only once and reused across all steps, resulting in an average 3.5x speedup over comparable methods.\n- FastFit introduces a new large-scale dataset, DressCode-MR, comprising 28,179 image sets of high-quality, paired images spanning five key categories.\n- Experiments on various datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while maintaining its significant advantage in inference efficiency.",
        "classification": [
            "Image-to-Image"
        ],
        "github_urls": [
            "https://github.com/Zheng-Chong/FastFit"
        ],
        "huggingface_urls": [],
        "date": "2025-09-03"
    }
]